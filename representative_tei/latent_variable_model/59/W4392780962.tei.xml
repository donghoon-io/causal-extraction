<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Proxy Methods for Domain Adaptation</title>
				<funder>
					<orgName type="full">Gatsby Charitable Foundation</orgName>
				</funder>
				<funder ref="#_8SVWVb7 #_yhCHTux #_T3r9EBB">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Google Inc.</orgName>
				</funder>
				<funder>
					<orgName type="full">Alfred P. Sloan Foundation</orgName>
				</funder>
				<funder ref="#_FX7jYQx">
					<orgName type="full">Google LLC</orgName>
				</funder>
				<funder ref="#_jbEk4sx">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder ref="#_GRmcuHh">
					<orgName type="full">NIFA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-03-12">12 Mar 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Katherine</forename><surname>Tsai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana</orgName>
								<address>
									<country>Champaign</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><forename type="middle">R</forename><surname>Pfohl</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Olawale</forename><surname>Salaudeen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana</orgName>
								<address>
									<country>Champaign</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicole</forename><surname>Chiou</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University College London</orgName>
								<address>
									<addrLine>5 Google DeepMind</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>D'amour</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sanmi</forename><surname>Koyejo</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Gatsby Computational Neuroscience Unit</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Proxy Methods for Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-03-12">12 Mar 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2403.07442v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of domain adaptation under distribution shift, where the shift is due to a change in the distribution of an unobserved, latent variable that confounds both the covariates and the labels. In this setting, neither the covariate shift nor the label shift assumptions apply. Our approach to adaptation employs proximal causal learning, a technique for estimating causal effects in settings where proxies of unobserved confounders are available. We demonstrate that proxy variables allow for adaptation to distribution shift without explicitly recovering or modeling latent variables. We consider two settings, (i) Concept Bottleneck: an additional "concept" variable is observed that mediates the relationship between the covariates and labels; (ii) Multi-domain: training data from multiple source domains is available, where each source domain exhibits a different distribution over the latent confounder. We develop a two-stage kernel estimation approach to adapt to complex distribution shifts in both settings. In our experiments, we show that our approach outperforms other methods, notably those which explicitly recover the latent confounder.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of domain adaptation is to transfer an accurate model from a labeled source domain to an unlabeled target domain, which has a different but related distribution <ref type="bibr" target="#b31">(Pan et al., 2010;</ref><ref type="bibr" target="#b17">Koh et al., 2021;</ref><ref type="bibr" target="#b23">Malinin et al., 2021)</ref>. It is motivated by the fact that labeling data is often labor intensive, and sometimes requires domain expertise. For example, the distribution of patients diagnosed with a condition from hospital A and hospital B may differ due to patients' socioeconomic status, demographics, and other factors. However, labeled data might be only be available at hospital A and not at hospital B (e.g., due to less funding). As a result, an accurate model for patients from hospital A may perform poorly for patients from hospital B.</p><p>In order to provide guarantees on the accuracy of a transferred model, one of two classical assumptions have been made: label shift or covariate shift. Label shift <ref type="bibr" target="#b4">(Buck et al., 1966;</ref><ref type="bibr" target="#b19">Lipton et al., 2018)</ref> assumes that the distribution of a label P pY q shifts between source and target domains, but the conditional distribution P pX | Y q does not. Conversely, covariate shift <ref type="bibr">(Shimodaira, 2000)</ref> assumes that the covariate distribution P pXq shifts between domains, but the distribution P pY | Xq stays the same. Each assumption provides theoretical guarantees on the generalization of a transferred classifier. In fact, without any assumptions, the source and target domains could differ arbitrarily, making guarantees impossible. However, these assumptions are often too restrictive to apply in real-world settings <ref type="bibr">(Zhang et al., 2015;</ref><ref type="bibr" target="#b38">Schrouff et al., 2022)</ref>. For instance, if covariates X and labels Y are confounded by a third variable U , it is possible for neither P pX | Y q or P pY | Xq to be equal across domains. For example, demographic information U could confound the relationship between a diagnosis Y and a radiological image X. In this example, if two hospitals have different distributions over demographics, both label shift and covariate shift adaptation methods will fail to transfer a classifier across hospitals.</p><p>To address this, recent work has introduced a latent shift assumption: the distribution of U , an unobserved latent confounder of X and Y , shifts between the source and target domain <ref type="bibr" target="#b0">(Alabdulmohsin et al., 2023)</ref>. In this setting, all distributions of X and Y (without conditioning on U ) may differ across the domains, violating label and covariate shift assumptions.</p><p>Contributions. We propose techniques for domain adaptation under the latent shift assumption that are guaranteed to identify the optimal predictor ErY | xs in the target domain. We make use of proxy methods <ref type="bibr">(Miao et al., 2018)</ref>, which are a recently developed framework for causal effect estimation in the presence of a hidden confounder U , given indirect proxy information on U . Compared to prior work <ref type="bibr" target="#b0">(Alabdulmohsin et al., 2023)</ref>, our techniques do not require: identifying the distribution of the latent variable U , that U be discrete, or further linear independence assumptions. We consider two settings: (1) Concept Bottleneck: we observe in both domains a proxy W of the unobserved confounder U and a concept C that mediates the direct relationship between X and Y <ref type="bibr" target="#b0">(Alabdulmohsin et al., 2023)</ref>, or (2) Multi-Domain: we do not observe C in either domain, but have access to observations from multiple source domains. For both settings, we provide guarantees for identifying ErY | xs without observing Y in the target domain. When ErY | xs is identifiable, we develop practical two-stage kernel estimators to perform adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The development of techniques for learning robust models and adapting to distribution shift has a long history in machine learning, but recently has received increased attention <ref type="bibr" target="#b40">(Shen et al., 2021;</ref><ref type="bibr" target="#b52">Zhou et al., 2022;</ref><ref type="bibr" target="#b49">Wang et al., 2022)</ref>.</p><p>Causality for domain adaptation. Our work is inspired by techniques that formulate the covariate/label shift settings as assumptions on the causal structure for domain adaptation and distributional robustness (e.g, <ref type="bibr" target="#b37">Schölkopf et al. (2012)</ref>; <ref type="bibr" target="#b33">Peters et al. (2015)</ref>; <ref type="bibr">Zhang et al. (2015)</ref>; <ref type="bibr" target="#b46">Subbaswamy et al. (2019)</ref>; <ref type="bibr" target="#b35">Rothenhäusler et al. (2021)</ref>; <ref type="bibr" target="#b48">Veitch et al. (2021)</ref>; <ref type="bibr" target="#b21">Magliacane et al. (2018)</ref>; <ref type="bibr">Arjovsky et al. (2019)</ref>; <ref type="bibr">Ganin et al. (2016)</ref>; <ref type="bibr" target="#b2">Ben-David et al. (2010)</ref>; <ref type="bibr" target="#b30">Oberst et al. (2021)</ref>).</p><p>Proximal causal inference. Our identification technique is inspired by approaches used to identify causal effects with unobserved confounding with observed proxies <ref type="bibr" target="#b18">(Kuroki and Pearl, 2014;</ref><ref type="bibr">Miao et al., 2018;</ref><ref type="bibr" target="#b6">Deaner, 2018;</ref><ref type="bibr" target="#b47">Tchetgen et al., 2020;</ref><ref type="bibr">Mastouri et al., 2021;</ref><ref type="bibr" target="#b5">Cui et al., 2023;</ref><ref type="bibr" target="#b50">Xu and Gretton, 2023)</ref>. These approaches design 'bridge functions' to connect quantities involving a proxy W with those of the label Y . The beauty of this approach is that these bridge functions are implicitly a marginalization over U . This allows these approaches to identify causal quantities without identifying distributions involving U .</p><p>Latent shift. Our work is most closely related to <ref type="bibr" target="#b0">Alabdulmohsin et al. (2023)</ref>, who introduced the setting of latent shift with proxies W and concepts C. They showed that the optimal predictor ErY | xs is identifiable in the target domain if W and C are observed in the source domain and X is observed in the target domain. To do so, they required (a) identification of distributions involving U , (b) that U is a discrete variable, (c) knowledge of the dimensionality of U , and (d) additional linear independence assumptions. In contrast, our work derives identification results for arbitrary U , The shaded circle denotes unobserved variable and the solid circle denotes observed variable. X is the covariate, Y is the response, C is the concept, W is the proxy, Z is the domain-related variable, and U is the latent variable.</p><p>and does not require any of (a)-(d). However, there is no free lunch: to achieve this, we require that proxies W are observed in the target, and either that: (i) concepts C are also observed in the target, or (ii) we observe multiple source domains. For (ii) we do not require C in either the source or the target, but for full identification we require that U is discrete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Framework</head><p>Let P p¨q and Qp¨q denote the probability distribution functions of the source domain and target domain, respectively. Let p and q indicate source and target quantities. Our goal is to study identification and estimation of the optimal target predictor E q rY | xs when Y is not observed in the target domain.</p><p>Concept Bottleneck. The first setting we study is described by the graph in Figure <ref type="figure" target="#fig_0">1c</ref>. We have two additional variables: (i) proxies W , which provide auxiliary information about U , or can be seen as a noisy version of it <ref type="bibr" target="#b18">(Kuroki and Pearl, 2014)</ref>, and (ii) concepts C, which mediate or 'bottleneck' the relationship between the covariates X and labels Y <ref type="bibr" target="#b9">(Goyal et al., 2019;</ref><ref type="bibr" target="#b16">Koh et al., 2020)</ref>. For example, <ref type="bibr" target="#b16">Koh et al. (2020)</ref> describe a setting where the concepts C are high-level clinical and morphological features of a knee X-ray X, which mediate the relationship with osteoporosis severity Y . In this example, U could describe demographic variations that alter symptoms X, C and outcome Y , and the proxies W could include patient background and clinical history (e.g., prior diagnoses, medications, procedures, etc). For the source domain we assume we observe pX, C, W, Y q " P and for the target domain we observe pX, C, W q " Q.</p><p>We formalize the notion of latent shift, as introduced in <ref type="bibr" target="#b0">Alabdulmohsin et al. (2023)</ref>.</p><p>Assumption 1 (Concept Bottleneck, <ref type="bibr" target="#b0">Alabdulmohsin et al. (2023)</ref>). The shift between P and Q is located in unobserved U , i.e., there is a latent shift P pU q ‰ QpU q, but P pV | U q " QpV | U q, where V Ď tW, X, C, Y u.</p><p>This assumption states that every variable conditioned on U is invariant across domains. However, as P pU q ‰ QpU q, none of the marginal distributions are: P pV q ‰ QpV q for V Ď tW, X, C, Y u. This assumption is a generalization of covariate shift P pY | X, U q " QpY | X, U q <ref type="bibr">(Shimodaira, 2000)</ref> and label shift P pX | Y, U q " QpX | Y, U q <ref type="bibr" target="#b4">(Buck et al., 1966)</ref>, with associated graphs in Figure <ref type="figure" target="#fig_0">1a-1b</ref>.</p><p>Assumption 2 (Structural assumption). Graphs in Figure <ref type="figure" target="#fig_0">1</ref> are faithful and Markov <ref type="bibr" target="#b45">(Spirtes et al., 2000)</ref>.</p><p>Under Assumption 2, we have the following conditional independence properties for the graph in Figure <ref type="figure" target="#fig_0">1c</ref>:</p><formula xml:id="formula_0">Y K K X | tU, Cu, W K K tX, Cu | U.</formula><p>With this conditional independence structure, tU, Cu blocks the information from X to Y and U blocks the information flow from W to tX, Cu. We will see in Section 4 that these assumptions allow us to obtain QpY | xq from QpW, C | xq in the target domain, where the latter is a function of observed quantities. Multi-domain. In the second setting, suppose we do not observe the concepts C in any domain, but instead observe data from multiple source domains, according to the graph in Figure <ref type="figure" target="#fig_0">1d</ref>. For instance, we may want to learn a classifier for a target hospital that has only unlabelled data, using data from several source hospitals with labelled data. Here, let Z be a random variable in Z denoting a prior over the source domains, and let P pU |Zq be the distribution of U given Z. We make k Z draws from Z, indexed by r P t1, . . . , k Z u, and write tz 1 , . . . , z k Z u ": Z p Ď Z. For each source domain z r , we observe pX, W, Y q " P pX, W, Y |z r q :" P r pX, W, Y q. For the target, we denote it with index k Z `1 and only observe pX, W q " P pX, W |z k Z `1q :" QpX, W q. In general let P r pV q :" P pV |z r q and QpV q :" P pV |z k Z `1q for any V Ď tW, X, Y, U u. For this setting we replace Assumption 1 with the following shift assumption.</p><p>Assumption 3 (Multi-Domain). For each z, z 1 P Z p such that z ‰ z 1 , we have P pU |zq ‰ P pU |z 1 q ‰ QpU q.</p><p>Note that Assumption 2 implies the following the conditional independence property in Figure <ref type="figure" target="#fig_0">1d</ref>:</p><formula xml:id="formula_1">tY, X, W u K K Z | U.</formula><p>Under Assumption 3, we allow all joint distributions to be different</p><formula xml:id="formula_2">P pW, X, U, Y |zq ‰ P pW, X, U, Y |z 1 q ‰ QpW, X, U, Y q for z ‰ z 1 P Z p .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Identification under Latent Shifts</head><p>Our identification techniques are inspired by proximal causal inference <ref type="bibr" target="#b47">(Tchetgen et al., 2020)</ref>. The key idea is to design so-called "bridge" functions to identify distributions confounded by unobserved variables. We first show that with additional proxies and concepts, E q rY | xs is identifiable under any latent shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Identification with Concepts</head><p>To prove identifiability, we need certain assumptions to hold for the shift. The first is a regularity assumption, also known as a completeness condition, and is commonly used to identify causal estimands (D <ref type="bibr">'Haultfoeuille, 2011;</ref><ref type="bibr">Miao et al., 2018)</ref>.</p><p>Assumption 4 (Informative variables). Let g be any mean squared integrable function. Both the source domain and the target domain, pf, F q P tpp, P q, pq, Qqu, satisfy E f rgpU q | x, cs " 0 for all x P X , c P C if and only if gpU q " 0 almost surely with respect to F pU q.</p><p>At a high level, completeness states that the X must have sufficient variability related to the change of U . This is a common assumption made in proximal causal inference (cf. Condition (ii) in <ref type="bibr">Miao et al. (2018)</ref> and Assumption 3 in <ref type="bibr">Mastouri et al. (2021)</ref>). For more details on the justification of completeness assumption, see the supplementary material of <ref type="bibr" target="#b28">Miao et al. (2022)</ref>.</p><p>Second, we need a guarantee on the support of u P U. Intuitively, if a u P U has non-zero probability in the target domain, it should have non-zero probability in the source domain as well.</p><p>Otherwise, it is impossible to adjust to certain shifts (as we never see these regimes in the source domain). This is similar to the positivity assumption commonly made in causality literature <ref type="bibr" target="#b12">(Hernán and Robins, 2006)</ref>.</p><p>Assumption 5 (Positivity). For any u P U, if Qpuq ą 0 then P puq ą 0.</p><p>If data are generated according to Figure <ref type="figure" target="#fig_0">1c</ref>, and the regularity conditions 8-10 hold (see Appendix A.2), <ref type="bibr">Miao et al. (2018)</ref> first showed the existence of the solutions h p 0 pw, cq, h q 0 pw, cq of the following equations:</p><formula xml:id="formula_3">E p rY | c, xs " ż W h p 0 pw, cqdP pw | c, xq (4.1) E q rY | c, xs " ż W h q 0 pw, cqdQpw | c, xq.</formula><p>The terms h p 0 pw, cq, h q 0 pw, cq are called 'bridge' functions as they connect the proxy W to the label Y . If we are able to identify h q 0 pw, cq then we can identify E q rY | xs, by using eq. ( <ref type="formula" target="#formula_7">4</ref>.1) to obtain E q rY | C, xs and marginalizing over QpC | xq.</p><p>We show that it is possible to connect identification of h q 0 pw, cq with that of h p 0 pw, cq, leading directly to identification of E q rY | xs.</p><p>Theorem 4.1. Assume that h p 0 and h q 0 exist (i.e., regularity Assumptions 8-10 hold). Then given Assumptions 1, 2, 4, 5 we have that, for any c P C,</p><formula xml:id="formula_4">ż W h p 0 pw, cqdP pw | uq " ż W h q 0 pw, cqdQpw | uq,</formula><p>almost surely with respect to QpU q. This implies that</p><formula xml:id="formula_5">E q rY | xs " ż WˆC h p 0 pw, cqdQpw, c | xq.</formula><p>The proof is given in Appendix B.1. Hence, given h p 0 and pW, X, Cq from the target Q, we are able to adapt to arbitrary distribution shifts in unobserved U . The advantage of this approach is that it will not require estimating any distributions involving U . We demonstrate this in Section 5.</p><p>While concepts can ensure identifiability, they may not be available in practice. In this case, a natural question is whether the optimal target predictor E q rY | xs is still identifiable. In the next section we show that if we instead have access to data from multiple source domains, E q rY | xs may again be identifiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Blessings of Multiple Domains</head><p>We now turn to the multi-domain setting. The graphical structure in Figure <ref type="figure" target="#fig_0">1d</ref> is similar to the structure in Figure <ref type="figure" target="#fig_0">1c</ref> with C replaced by X, X replaced by Z, and the arrow between U and Z flipped. Although the bridge function proposed by <ref type="bibr">Miao et al. (2018)</ref> assumes an edge from U to Z, changing the direction from Z to U does not change the conditional independence structure <ref type="bibr" target="#b32">(Pearl, 2009)</ref>. The main difference is we will only be able to guarantee full identification when U is discrete. We start by demonstrating this, and then give an example of the inherent difficulty of identification when U is continuous.</p><p>To begin, for simplicity, assume U and W are discrete (with dimensionalities k U and k W ). We have finitely many samples from Z, denoted as z 1 , . . . , z k Z , corresponding to our training domains. We seek a bridge function (in this case, a matrix M 0 pw i , xq) satisfying</p><formula xml:id="formula_6">E r rY | xs " kw ÿ i"1 M 0 pw i , xqP r pw i | xq, (4.2)</formula><p>for all r " 1, . . . , k Z , where E r rY | xs is the conditional expectation obtained in domain r, and P r pW | xq " P pW | x, z r q.</p><p>In order to identify M 0 pw i , xq, and then E q rY | xs, we need enough source domains to capture the variability of U . The following result describes how many we need.</p><p>Proposition 4.2. Suppose that we have k Z source domains and W , U have k W and k U categories respectively. Then, if k W , k Z ě k U and subject to appropriate rank conditions (see proof in Appendix B.2), the bridge function is identifiable and does not depend on the specific z.</p><p>This result generalizes the identification analysis developed in <ref type="bibr">Miao et al. (2018)</ref>. If the number of observed source domains k Z is greater than the dimension of the latent U , then subject to appropriate identifiability requirements (detailed in Appendix B.2), we can recover the bridge M 0 pw i , xq. Now, consider the case where U is discrete but all observed variables W, X, Y are continuous. In this case we have the following system</p><formula xml:id="formula_7">E r rY | xs " ż W m 0 pw, xqdP r pw | xq,<label>(4.3)</label></formula><p>for r " 1, . . . , k Z . The proof of existence of m 0 is a modification of Proposition A.2, as shown in Proposition A.3. In order to identify target E q rY | xs, we need the following assumption.</p><p>Assumption 6. Let g be a square integrable function on U . For each x P X and for all z P Z p , ErgpU q | x, zs " 0 if and only if gpU q " 0, P pU q almost surely.</p><p>Given this assumption we can prove identifiability.</p><p>Proposition 4.3. Given that Assumptions 1-3, 6 hold; that m 0 exists; that pW, X, Y q are observed for the sources z P Z p , and pW, Xq is observed from the target domain. Then E q rY | xs is identifiable, and for any x P X , we can write</p><formula xml:id="formula_8">E q rY | xs " ż W m 0 pw, xqdQpw | xq. (4.4)</formula><p>The proof is given in Appendix B.3. Crucially, this result is valid only when Assumptions 6 holds, and it remains unclear when it is expected to hold. Proposition 4.2 suggests that Assumptions 6 is not vacuous when U is finite dimensional. We plan to investigate further this in future work. Now let us consider the case where U is continuous. In this case, unfortunately, Assumption 6 may not hold, preventing identification of E q rY | xs. This is illustrated in the following example. For every x, Eqs. (4.5) and (4.6) represent projections onto P pu | x, z r q, r P 1, . . . , k z . Consider U :" r´π, πs with periodic boundary conditions, and for a given x define P pu | x, z r q " p2πq ´1p1 cospruqq, @r P N `(note that cosines form an orthonormal basis). We now construct an example where (4.5) holds for some z but not for others. Define the difference</p><formula xml:id="formula_9">E p rY | x, us ´żW m 0 pw, xqdP pw | uq (4.7)</formula><p>" cosppk z `1quq ": gpuq.</p><p>In this case, gpuq ‰ 0, and in particular, (4.5) holds for all r ď k z , but not for P pu | x, z kz`1 q.</p><p>This example illustrates a larger point: that for continuous U , no finite set of projections will suffice to completely characterize the square integrable functions on U. That said, as more projections are employed, and subject to appropriate assumptions on the smoothness of (4.7), the error will reduce as more domains are observed. The characterization of this convergence will be the topic of future work. In experiments, we show that the adaptation can still be effective even when the latent variable U |z r is continuous valued and follows different Beta distributions for each distinct r, given just two training source domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Kernel Bridge Function Estimation</head><p>We introduce kernel methods to estimate the bridge functions and subsequently leverage the estimates to adapt to distribution shifts. Section 4 shows that bridge functions for both settings can be adapted to the target domain, so we drop the domain specific indices and use h 0 and m 0 to denote the bridge functions. We begin by introducing the notation.</p><p>Notation. Let b be the tensor product, b be the columnwise Khatri-Rao product and d be the Hadamard product. For any space V P tX , C, W, Yu, let k : V ˆV Ñ R be a positive semidefinite kernel function and ϕpvq " kpv, ¨q for any v P V be the feature map. We denote H V to be the RKHS on V associated with kernel function k. The RKHS has two properties: (i) f P H V , f pvq " xf, kpv, ¨qy for all v P V and (ii) kpv, ¨q P H V . We denote x¨, ¨y as the inner product and ||| ¨||| H V as the induced norm. For notation simplicity, we denote the product space</p><formula xml:id="formula_10">H V ˆHV 1 associated with operation H V b H V 1 as H VV 1 .</formula><p>We define the kernel mean embedding as µ V " ErϕpV qs " ş kpv, ¨qppvqdv <ref type="bibr" target="#b43">(Smola et al., 2007)</ref> and the conditional mean embedding as µ V |y " ş kpv, ¨qppv | yqdv <ref type="bibr">(Song et al., 2009;</ref><ref type="bibr" target="#b42">Singh et al., 2019)</ref>. For V P tW, X, Cu, we denote the a-th batch of i.i.d. samples as V a " tv a,i u na i"1 . Define the Gram matrices as</p><formula xml:id="formula_11">K Va " " kpv a,i , v a,j q ‰ i,j P R naˆna , K V ab " " kpv a,i , v b,j q ‰ i,j P R naˆn b . Let Φ Va " " ϕpv a,1 q, . . . , ϕpv a,na q ‰ J P H na V be the vectorized feature map such that Φ Va pv 1 q " " kpv a,1 , v 1 q, . . . , kpv a,na , v 1 q ‰ J P R na .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Adaptation with Concepts</head><p>Suppose that for the bridge function h 0 P H WC , where H WC is a RKHS. It follows from Theorem 4.1 that</p><formula xml:id="formula_12">E q rY | X " xs " E q rh 0 pW, Cq | xs " E q rxh 0 , ϕpW q b ϕpCqy | xs " xh 0 , µ q W C|x y. (5.1)</formula><p>To adapt to the distribution shifts, we estimate the bridge function h 0 in the source domain and the conditional mean embedding µ q W C|x " E q rϕpW q b ϕpCq | xs in the target domain. The empirical estimate of the conditional mean embedding along with the consistency proof have been provided in <ref type="bibr">(Song et al., 2009;</ref><ref type="bibr" target="#b11">Grünewälder et al., 2012)</ref> thus we focus on the estimation procedure of the bridge function h 0 .</p><p>To estimate the bridge function h 0 , we employ the regression method developed in <ref type="bibr">Mastouri et al. (2021)</ref>. Recall ErY | c, xs " Erh 0 pW, cq | c, xs. We define the population risk function in the source domain as: Rph 0 q " E p rpY ´Gh 0 pC, Xqq 2 s;</p><p>(5.2)</p><formula xml:id="formula_13">G h 0 px, cq " xh 0 , µ p W |c,x b ϕpcqy.</formula><p>The procedure to optimize (5.2) involves two stages. In the first stage, we estimate the conditional mean embedding µ p W |c,x " E p rϕpW q | c, xs, which we will use as a plug-in estimator to estimate h 0 in the second step. Given</p><formula xml:id="formula_14">n 1 i.i.d. samples pX 1 , W 1 , C 1 q " tpx 1,i , w 1,i , c 1,i qu n 1</formula><p>i"1 from the source distribution p and a regularizing parameter λ 1 ą 0, we denote</p><formula xml:id="formula_15">K X 1 P R n 1 ˆn1 , K C 1 P R n 1 ˆn1 as the Gram matrices and Φ X 1 P H n 1 X , Φ C 1 P H n 1 C</formula><p>as n 1 -dimensional vectorized feature maps of X 1 , C 1 respectively. Following the procedure developed in <ref type="bibr">Song et al. (2009)</ref>, the estimate of µ p W |x,c is</p><formula xml:id="formula_16">µ p W |c,x " n 1 ÿ i"1 b i px, cqϕpw 1,i q; (5.3) bpx, cq " pK X 1 d K C 1 `λ1 n 1 Iq ´1 pΦ X 1 pxq d Φ C 1 pcqq .</formula><p>In the second stage, we replace µ p W |x,c with µ p W |x,c in (5.2) and define the empirical risk. Consider</p><formula xml:id="formula_17">n 2 i.i.d. samples pX 2 , Y 2 , C 2 q " tpx 2,i , y 2,i , c 2,i qu n 2</formula><p>i"1 from the source distribution and a regularization parameter λ 2 ą 0, we want to minimize</p><formula xml:id="formula_18">argmin h 0 PH WC 1 2n 2 n 2 ÿ i"1 ´y2,i ´xh 0 , ϕpc 2,i q b µ p W |c 2,i ,x 2,i y ¯2 `λ2 |||h 0 ||| 2 H WC .</formula><p>(5.4)</p><p>We follow the same analysis procedure derived in <ref type="bibr">Mastouri et al. (2021)</ref>. The solution to (5.4) is shown in the following.</p><p>Proposition 5.1.</p><formula xml:id="formula_19">Let K W 1 P R n 1 ˆn1 , K C 2 P R n 2 ˆn2</formula><p>be the Gram matrices of W 1 and C 2 , respectively. Let K X 12 P R n 1 ˆn2 , K C 12 P R n 1 ˆn2 be the cross Gram matrices of pX 1 , X 2 q and pC 1 , C 2 q, respectively. For any λ 2 ą 0, there exists a unique optimal solution to (5.4) of the form</p><formula xml:id="formula_20">h 0 " n 1 ÿ i"1 n 2 ÿ j"1 α ij ϕpw 1,i q b ϕpc 2,j q; vecpαq " pIbΓqpλ 2 n 2 I `Σq ´1y 2 , where Σ " pΓ J K W 1 Γq d K C 2 , Γ " pK X 1 d K C 1 `λ1 n 1 Iq ´1pK X 12 d K C 12 q, and y 2 " " y 2,1 , . . . , y 2,n 2 ‰ J .</formula><p>Proposition 5.1 is an application of the Representer theorem <ref type="bibr">(Schölkopf et al., 2001)</ref> -the optimal estimate of the infinite dimensional operator is a finite rank operator spanned by the feature space of W 1 and C 2 .</p><p>Finally, given estimate µ q W C|x and a new sample x new , we can construct the empirical predictor of (5.1) as y pred " x h 0 , µ q W C|xnew y.</p><p>This completes the full adaptation procedure. On classification tasks. For classification tasks, where the label is Y P t1, . . . , k Y u, we treat the multi-task regressor as a classifier. We encode Y by a one-hot encoder and then regress on the encoded Y P t0, 1u k Y . Each label ℓ has a corresponding bridge function h 0,ℓ for ℓ P t1, . . . , k Y u. For i " 1, . . . , n 2 , let the encoded y 2,i be y 2,i " " y 2,i,1 , . . . , y 2,i,k Y ‰ J P t0, 1u k Y . Then for each ℓ, we can estimate h 0,ℓ by replacing y 2,i in (5.4) with y 2,i,ℓ P t0, 1u. For each new sample x new , the predicted score of label ℓ is y pred,ℓ " x h 0,ℓ , µ q W C|xnew y, and we select the label that has the highest prediction score: argmax ℓ y pred,ℓ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Adaptation with Multiple Domains</head><p>In the multiple source domain setting, the estimation of m 0 follows similarly to that of h 0 . Assuming that m 0 P H WX , then (4.3) can be written as</p><formula xml:id="formula_21">E r rY | xs " E p rxm 0 , µ W |x,r b ϕpxqy | xs, for r " 1, . . . , k Z .</formula><p>The task is to estimate m 0 from the source domain and then apply it to the target domain. We can define the population risk function as</p><formula xml:id="formula_22">Rpm 0 q " k Z ÿ r"1 E r rpY ´Gm 0 pr, Xqq 2 s;</formula><p>(5.5)</p><formula xml:id="formula_23">G m 0 pr, xq " xm 0 , µ W |r,x b ϕpxqy.</formula><p>We employ the two-stage estimation procedure as we did for estimating h 0 : (i) we first estimate µ W |r,x and then (ii) plug the estimate µ W |r,x to estimate m 0 . At the r-th domain, we observe the samples: tpw r,i , x r,i , rqu nr i"1 . As with (5.3), we learn a conditional mean embedding µ W |r,x " ř nr i"1 d r,i pxqϕpw r,i q, where d r pxq " pK Xr `λ3 Iq ´1 pΦ Xr pxqq P R nr and λ 3 ą 0 for r " 1, . . . , k Z . In the second stage, given another batch of independent samples: tpy r,i , x r,i , rqu nr i"1 for r " 1, . . . , k Z , we minimize:</p><formula xml:id="formula_24">1 2 ř r"1 n r k Z ÿ r"1 nr ÿ i"1 ´yr,i ´xm 0 , ϕpx r,i q b µ W |r,x r,i y ¯2 `λ4 |||m 0 ||| 2 H WX . (5.6)</formula><p>Then, m 0 yields an analytical solution in similar form to h 0 shown in Proposition 5.1 (see Appendix C.2 for details). Finally, with the estimated conditional mean embedding µ q W |x and a new sample x new from the target test set, we have</p><formula xml:id="formula_25">y pred " x m 0 , µ q W |xnew b ϕpx new qy.</formula><p>We convert the regression task with m 0 to the classification task by learning k Y bridge functions, where each bridge function m 0,ℓ corresponds to label ℓ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We verify our theory with both simulated and real data, demonstrating robustness to latent shifts and transferablility of the bridge functions.</p><p>For the setting with concept variables present, we compare our method with baselines: Empricial Risk Minimization (ERM), Covariate shift weighting (COVAR) <ref type="bibr">(Shimodaira, 2000)</ref>, Label shift weighting (LABEL) <ref type="bibr" target="#b4">(Buck et al., 1966)</ref>, and the spectral (LSA-S) and Wasserstein Autoencoder  The result on the right shows that our method is robust even when the overlapping area between two distributions is small.</p><p>Table <ref type="table">1</ref>: Multi-domain adaptation result. The values are the average AUROC of 10 independent replicates of the data. Each task has three source domains with different P r pU q and one target domain. The proposed method has outperformed other baselines and is close to the Oracle in task 2. (LSA-WAE) latent shift adaptation approaches <ref type="bibr" target="#b0">(Alabdulmohsin et al., 2023)</ref>. For the multi-domain setting, we compare our method with baselines: Simple Adaptation (SA) <ref type="bibr">(Mansour et al., 2008)</ref>, Weighted Combination of Source Classifiers (WCSC) <ref type="bibr">(Zhang et al., 2015)</ref>, and Marginal Kernel (MK) <ref type="bibr">(Blanchard et al., 2011)</ref>. We also compare with multi-domain generalization baselines <ref type="bibr">(Muandet et al., 2013)</ref>: Domain Adversarial Neural Networks (DANN) <ref type="bibr">(Ganin et al., 2016)</ref>, Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b10">(Gretton et al., 2012)</ref>. Additionally, we modify the ERM method to the multi-domain setting by concatenating the source samples to learn one ERM model (Cat-ERM) or taking the average result of each source domain ERM model (Avg-ERM). The ORACLE model is a model that is trained on target distribution samples. and evaluated on held-out target distribution samples. The tuning parameters for all models including the proposed model are selected using five-fold cross-validation. Details regarding the setups are in Appendix D.</p><p>Classification task. The task designed in Alabdulmohsin et al. ( <ref type="formula">2023</ref>) is a binary classification problem with Y P t0, 1u and the latent variable U P t0, 1u is a Bernoulli random variable. Additionally, X P R 2 , W P R are continuous random variables and C P R 3 is a discrete variable. We have one source domain with P pU " 1q " 0.1. We evaluate the models on the target distribution with QpU q shifting from QpU " 1q P t0.1, . . . , 0.9u. The goal of this task is to investigate whether the adaptation method is robust to any arbitrary shift of U .</p><p>The ORACLE and ERM model are implemented as MultiLayer Perceptrons (MLP). The kernel function used in the proposed method is the Gaussian kernel.</p><p>We compare the proposed method with the LSA-S and Wasserstein Autoencoder adaptation LSA-WAE approaches developed in <ref type="bibr" target="#b0">Alabdulmohsin et al. (2023)</ref>. While all three methods are designed to adjust shift for the same graph in Figure <ref type="figure" target="#fig_0">1c</ref>, our method takes additional W, C, X as training samples in the target domain while LSA-S and LSA-WAE only take X. For all three methods, only X is observed in the test data.</p><p>While the identification theory developed in <ref type="bibr" target="#b0">(Alabdulmohsin et al., 2023)</ref> does not require W, C in the target domain, we are aware that in practice, having more information in the target domain may improve estimation. To make the methods more directly comparable, we design an additional step to incorporate W from the target in the LSA-S algorithm. We describe this procedure in more detail in Appendix D.1.</p><p>Results are shown in Figure <ref type="figure" target="#fig_3">2a</ref>. The proposed method is more robust to the shift compared to baselines and is close to the ORACLE model. It is shown that with observed W in the target domain, LSA-S does not improve the performance compared to LSA-S without W . We also compare results under different noise levels and observe similar trends as discussed in Appendix D.</p><p>dSprites dataset regression task. We test the proposed procedure on the dSprites <ref type="bibr">(Matthey et al., 2017)</ref> dataset, an image dataset described by five latent parameters (shape, scale, rotation, posX, and posY). Motivated by <ref type="bibr">Matthey et al. (2017)</ref>'s experiments, we design a regression task where the dSprites images (64 ˆ64 = 4096-dimensional) are X P R 64ˆ64 and subject to a nonlinear confounder U P r0, 2πs which is a rotation of the image. W P R and C P R are continuous random variables. For this experiment, we have 7000 training samples and 3000 test samples. Further details about the procedure are in Appendix D.</p><p>In the results in Figure <ref type="figure" target="#fig_3">2b</ref>, we vary a, which controls which region of the source distribution that the target distribution concentrates. We design the experiment such that increasing a shifts the target distribution to increasingly low mass regions of the source distribution. We compute the mean squared error of each method on test examples from the target distribution.</p><p>We find that, while the baseline methods degrade as the target distributions shift increases, the proposed method adapts and maintains low error, nearly matching the error achieved by the oracle, which is trained on target distribution samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Multi-Domain Adaptation</head><p>In the multi-domain setting, we use the same classification dataset provided in <ref type="bibr" target="#b0">Alabdulmohsin et al. (2023)</ref> as Section D.6. We assume that C is not observed in any domain and generate multiple datasets drawn with different distributions on U .</p><p>Classification task. We construct three different tasks with different settings of P pU q over the source and target domains. For each task, we construct three source domains and one target domain, drawing 3200 random training samples for the each source domain and 9600 random training samples for the target domain. The set of source domains of of Task 1-3 have different combinations of distribution on U documented in Appendix D.3.</p><p>The backbone models for ORACLE, Cat-ERM, Avg-ERM, and SA <ref type="bibr">(Mansour et al., 2008)</ref> are simple MLPs; MK <ref type="bibr">(Blanchard et al., 2011</ref>) is a weighted kernel support vector machine; WCSC <ref type="bibr">(Zhang et al., 2015)</ref> is a re-weighted kernel density estimator. SA <ref type="bibr">(Mansour et al., 2008)</ref> assumes that QpXq is the convex combinations of P r pXq for r " 1, . . . , k Z ; WCSC <ref type="bibr">(Zhang et al., 2015)</ref> assumes that QpX | Y q is a linear mixture of P r pX|Y q for r " 1, . . . , k Z domain is an i.i.d. realization from the general distribution. from embeddings of chest X-rays over five replicates of a sampling procedure that introduces a shift in the prevalence of "No finding" with patient sex subgroups, where radiology report embeddings serve as concept variables C and patient age serves as the proxy W . In the concept adaptation experiment, the source domain corresponds to P pU " 1q " P pY " 1 | Sex " Femaleq " P pY " 0 | Sex " Maleq " 0.1. In the multi-domain adaptation experiment, we consider two source domains P pU " 1q " t0.1, 0.2u.</p><p>The results are shown in Table <ref type="table">1</ref>. Overall, we find our approach performs better than ERM and baseline multi-domain adaptation methods. All methods perform better in the setting of Task 2 than for Task 1, informally demonstrating the effect of the closeness of the source domains to the target domain. For Task 3, while our proposed approach performs best, ERM also performs well, and substantially better than the domain adaptation baselines.</p><p>Regression task. We consider two regression tasks, where U is either a Bernoulli or a Beta random variable. We present the results in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Concept and multi-domain adaptation with MIMIC-CXR</head><p>We conduct a small-scale experiment using a sample of chest X-ray data extracted from the MIMIC-CXR dataset <ref type="bibr">(Johnson et al., 2019)</ref>. We briefly describe the experimental design and results here, and include a complete description in Appendix D.7. We consider classification of the absence of a radiological finding from low-dimensional embeddings of the X-rays <ref type="bibr">(Sellergren et al., 2022)</ref>, using the absence of a radiological finding in the radiology report as the target of prediction. This corresponds to the "No Finding" label defined by <ref type="bibr">Irvin et al. (2019)</ref>.</p><p>We consider distribution shifts similar to settings in <ref type="bibr">Makar et al. (2022)</ref>, where patient sex is considered as a possible "shortcut" in the classification of the absence of a radiological finding. We impose distribution shift through structured resampling of the data where P pU " 1q " P pY " 1 | Sex " Femaleq " P pY " 0 | Sex " Maleq and P pSex " Femaleq " P pSex " Maleq " 0.5 is held constant. We perform both concept adaptation and multi-domain adaptation experiments with the MIMIC-CXR data. For the concept adaptation experiment, we consider the concept variable C to be the embedding of a radiology report associated with the chest X-ray. We experiment with the use of patient age as a potential proxy W for U due to a hypothesized correlation between the presence of radiological findings and patient age.</p><p>The results are summarized in Figure <ref type="figure" target="#fig_4">3</ref>. For both experiments, we find that the performance of baseline models fit using only information from the source domain(s) degrades under distribution shift. In the concept adaptation experiment, adaptation is relatively successful, as much of the performance of comparator models fit using target domain data is recovered by the adaptation procedure.</p><p>However, we find that the multi-domain adaptation procedure is not successful. In this case, we find that while the multi-domain adaptation procedure marginally outperforms a model fit using the concatenated source domain data under distribution shift, it recovers substantially less of the performance of the target domain model than the concept adaptation procedure does. Furthermore, the adapted model does not outperform the kernel estimators that only leverage information from the source domains. The lack of success in this setting could potentially be explained by insufficient number or diversity of domains relative to the level of noise induced by sampling variability and limited sample size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>We propose a strategy for adaptation under distribution shift in a latent variable using a bridge function approach <ref type="bibr">(Miao et al., 2018;</ref><ref type="bibr" target="#b47">Tchetgen et al., 2020)</ref>. This approach allows for identification of the optimal predictor in the target domain without identifying the distribution of the latent variable and without distributional assumptions on the form of the latent. We require that proxies of the latent variable are present and that (i) mediating concepts are available or (ii) data from multiple source domains are present.</p><p>We argue our approach is useful for two reasons. First, the latent distribution in general is only identifiable under strict distributional assumptions <ref type="bibr" target="#b20">(Locatello et al., 2019)</ref>. Second, recovery of the latent variable may be challenging in practice even if it is identifiable <ref type="bibr" target="#b34">(Rissanen and Marttinen, 2021)</ref>. For example, because most latent variable estimation methods are designed to model the data generating process <ref type="bibr" target="#b15">(Kingma and Welling, 2013)</ref>, one might allocate substantial modeling capacity to variability in the data and the latent variable that are irrelevant to modeling the shift in the conditional distribution of Y | X. By contrast, we model only the components of the observable variables relevant to the adaptation.</p><p>Assumption 7 is the requirement for the least-squares problem to have an unique solution. Hence, by Assumption 7, we have</p><formula xml:id="formula_26">PpU | X, cq " PpW | U q : PpW | X, cq,</formula><p>where PpW | U q : is the generalized inverse of PpW | U q. Plug the above equation into (A.2), we see that PpY | X, cq " PpY | U, cqPpW | U q : looooooooooooomooooooooooooon HpY,W,cq</p><p>PpW | X, cq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Existence of the Bridge Function h 0</head><p>The sufficient conditions of existence of h 0 are originally discussed in <ref type="bibr">Miao et al. (2018)</ref>, we adapt them to our setting and provide a brief review in this section. We assume the following completeness assumption and regularity conditions. This assumption is equivalent to Condition (iii) in <ref type="bibr">Miao et al. (2018)</ref>.</p><p>Assumption 8. For any mean squared integrable function g and for c P C, ErgpXq | W, cs " 0 almost surely if and only if gpXq " 0 almost surely.</p><p>Let f be either the distribution from p or q, we consider K c : L 2 pW | cq Ñ L 2 pX | cq as the conditional expectation operator associated with the kernel function</p><formula xml:id="formula_27">kpw, x, cq " f pw, x | cq f pw | cqf px | cq .</formula><p>Then it follows that ErY | c, xs " K c h 0 :</p><formula xml:id="formula_28">ErY | c, xs " ż W h 0 pw, cqf pw | x, cqdw " ż kpw, x, cqh 0 pw, cqf pw | cqdw " K c h 0 .</formula><p>To find the solution h 0 , we assume the followings.</p><p>Assumption 9. For any c P C,</p><formula xml:id="formula_29">ş W ş X f pw | c, xqf px | c, wqdwdx ă 8</formula><p>. This is a sufficient condition to ensure that K c is a compact operator <ref type="bibr">(Carrasco et al., 2007, Example 2.3)</ref>. Hence, by the definition of a compact operator, there exists a singular system tλ c,i , ϕ c,i , ψ c,i u iPN of K c for every c P C.</p><p>Assumption 10. For fixed c P C: <ref type="bibr">Miao et al. (2018)</ref>. We adapt the results from Proposition 1 in <ref type="bibr">Miao et al. (2018)</ref> to the graph in Figure <ref type="figure" target="#fig_0">1c</ref> which replaces the node X by C and node Z by X.</p><formula xml:id="formula_30">1. ErY | X, cs P L 2 pX | cq; 2. ř iPN λ ´2 c,i |xErY | X, cs, ψ c,i y| 2 ă 8.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The above two assumptions are restatements of Conditions (v)-(vii) in</head><p>Proposition A.1 (Existence of h 0 , adapted from Proposition 1 in <ref type="bibr">Miao et al. (2018)</ref>). Under Assumption 2, 8-10, the solution to (4.1) exists.</p><p>Proof. The proof follows directly from the result of Picard's theorem. Assumption 9 implies that K c is a compact operator. Assumption 8 implies that N pK c q K " L 2 pX | cq. Therefore, under the first statement in Assumption 10, we have ErY | X, cs P N pK c q K . Along with the second statement in Assumption 10, we can apply Lemma A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Existence of Bridge Function m 0</head><p>The proof of the existence of m p 0 is similar to the analysis of h 0 . Let K  <ref type="figure">,</ref><ref type="figure">ϕ</ref> x,i , ψ x,i q is the singular system of K x .</p><formula xml:id="formula_31">x : L 2 pW | xq Ñ L 2 pZ |</formula><p>Then the solution of m p 0 exists.</p><p>The proof of Proposition A.2 is similar to the proof of Proposition 1 in <ref type="bibr">(Miao et al., 2018)</ref>, where we replace P py|z, xq in Proposition 1 of <ref type="bibr">Miao et al. (2018)</ref> with ErY | Z, xs. The proof for existence of m q 0 also follows similarly as Proposition A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Auxiliary Lemma</head><p>We introduce the Picard's theorem as follows.</p><p>Lemma A.3 (Picard's Theorem). Let K : H 1 Ñ H 2 be a compact operator with singular system tλ j , φ j , ψ j u 8 j"1 and ϕ be a given function in H 2 . Then the equation of first kind Kh " ϕ have solutions if and only if 1. ϕ P N pK ˚qK , where N pK ˚q " th : K ˚h " 0u is the null space of the adjoint operator K ˚.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>ř `8 j"1 λ ´2 j |xϕ, ψ j y| 2 ă 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Transferring Bridge Functions</head><p>In this section, we discuss the identifiability results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Proof of Theorem 4.1</head><p>For f P tp, qu, recall that</p><formula xml:id="formula_32">E f rY | c, xs " ż W h f 0 pw, cqf pw | c, xqdw " ż W ż U h f 0 pw, cqf pw | c, uqf pu | c, xqdudw " ż W ż U h f 0 pw, cqf pw | uqf pu | c, xqdudw pW K K C | U q.</formula><p>Similarly, we can write</p><formula xml:id="formula_33">E f rY | c, xs " ż U E f rY | c, usf pu | c, xqdu pY K K X | tU, Cuq.</formula><p>Under Assumption 4, we have</p><formula xml:id="formula_34">E f rY | c, U s " ż W h f 0 pw, cqf pw | U qdw (B.1)</formula><p>almost surely with respect to F pU q, F P tP, Qu. Suppose that u P U such that Qpuq ą 0. Then, by Assumption 5 , we must have P puq ą 0. Hence, conditioned on the selected u and c and under Assumption 1, we have</p><formula xml:id="formula_35">E p rY | c, us " ż W h p 0 pw, cqppw | uqdw; E q rY | c, us " ż W h q 0 pw, cqppw | uqdw pppw | uq " qpw | uq, @c P C, w P W, u P Uq.</formula><p>We then can write</p><formula xml:id="formula_36">E p rY | c, us ´Eq rY | c, us " ż W h p 0 pw, cqppw | uqdw ´żW h q 0 pw, cqqpw | uqdw.</formula><p>Note that, by Assumption 1, we have E p rY | c, us " E q rY | c, us and hence the left hand side of the above equation is 0 and we can conclude that:</p><formula xml:id="formula_37">ż W h p 0 pw, cqppw | U qdw " ż W h q 0 pw, cqqpw | U qdw</formula><p>QpU q almost surely. We complete the first part of proof.</p><p>To show the second part of the theorem, note that we can write</p><formula xml:id="formula_38">E q rY | xs " E q rE q rY | C, xs | xs " E q rE q rh q 0 pW, cq | C, xs | xs.</formula><p>Since ppw | uq " qpw | uq by Assumption 1, we can factorize the above equation as</p><formula xml:id="formula_39">E q rY | xs " ż C "ż U "ż W h q 0 pw, cqppw | uqdw * qpu | c, xqdu ȷ qpc | xqdc. Let the support of U conditioned on c, x be U 1 c,x " tu : Qpu | c, xq ą 0u and U 0 c,x " tu : Qpu | c, xq " 0u. Hence, we have U " U 0 c,x Y U 1 c,x , and U 0 c,x X U 1 c,x " H such that ş U 0 c,x qpu | c, xqdu " 0 and ş U 1 c,x qpu | c, xqdu " 1.</formula><p>Then, we can further decompose the above as</p><formula xml:id="formula_40">E q rY | xs " ż C « ż U 0 c,x "ż W h q 0 pw, cqppw | uqdw * qpu | c, xqdu ff qpc | xqdc `żC « ż U 1 c,x "ż W h q 0 pw, cqppw | uqdw * qpu | c, xqdu ff qpc | xqdc " ż C « ż U 1 c,x "ż W h q 0 pw, cqppw | uqdw * qpu | c, xqdu ff qpc | xqdc.</formula><p>Given c, x, since the support of QpU | c, xq is included in the support of QpU q, so if u P U 1 c,x , we must have Qpuq ą 0 and hence P puq ą 0 by Assumption 5, and we can swap h q 0 with h p 0 .</p><p>"</p><formula xml:id="formula_41">ż C « ż U 1 c,x "ż W h p 0 pw, cqppw | uqdw * qpu | c, xqdu ff qpc | xqdc. Since ş U 0 c,x ␣ş W h p 0 pw, cqppw | uqdw ( qpu | c,</formula><p>xqdu " 0, we can add it to the above term and arrive at</p><formula xml:id="formula_42">" ż C "ż U "ż W h p 0 pw, cqppw | uqdw * qpu | c, xqdu ȷ qpc | xqdc " ż C ż W h p 0 pw, cqqpw, c | xqdwdc. (B.2)</formula><p>Since we can identify h p 0 from the observable pW, X, Y, Cq of the source domain by solving the linear system (4.1), given observable pW, C, Xq from the target domain, we can identify E q rY | xs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Proposition 4.2</head><p>The following proof is a generalization of the proof of <ref type="bibr">Miao et al. (2018)</ref>, suited to the multidomain case. All variables besides Z are assumed to be discrete-valued and multivariate:</p><formula xml:id="formula_43">V can take k v values for V P tU, X, Y, W u. Let PpW | U q " " PpW | u 1 q . . . PpW | u k U q ‰ P R k W ˆkU . Similarly, define PpY | U, xq " " PpY | u 1 , xq . . . PpY | u k U , xq ‰ P R k Y ˆkU .</formula><p>This notation carries through to the remaining variables.</p><p>The approach we will take differs from the concept case (and standard proxy case) in the following way: we do not observe Z in the training or test domains, nor do we know its true dimension (indeed Z may be continuous valued). Rather, we assume that we have at least k Z distinct draws z r from Z in training, where r P t1, . . . , k Z u is the domain index, and that k Z ě k U . We also suppose that in test, we observe a distinct draw z k Z `1 which was not seen in training.</p><p>Our goal is to obtain a bridge function, which in the categorical case will be a bridge matrix of dimension M w,x P R k Y ˆkW . Define P r pV | xq :" P pV | x, z r q for V P tU, Y, W u. We assume that for each x,</p><formula xml:id="formula_44">rank pP 1:k Z pU | xqq " k U , P 1:k Z pU | xq :" " P 1 pU | xq . . . P k Z pU | xq ‰</formula><p>which implies that P pU | x, z r q varies with z r , and that we see a sufficient diversity of domains to span the space of vectors on U . The graphical model supports the conditional independence relation tY, X, W u K K Z | U, however we will only require the standard proxy assumptions</p><formula xml:id="formula_45">W K K X, Z | U, Y K K Z | X, U.</formula><p>Next, as in the concept case, we require</p><formula xml:id="formula_46">P pY |U, xq " M w,x P pW |U q,</formula><p>where we assume rankpP pW |U qq " k u (as in the first condition of Assumption 7). The matrix M w,x is invariant to the distribution P pU q by construction. If we can solve for M w,x , then given a novel domain corresponding to the draw z kz`1 , we have P pY |U, xqP kz`1 pU |xq " M w,x P pW |U qP kz`1 pU |xq P kz`1 pY |xq " M w,x P kz`1 pW |xq.</p><p>This allows us to compute conditional expectations under P pY | xq in the novel domain, based on observations of pW, Xq in this domain.</p><p>To solve for M w,x , we project both sides on a basis over U arising from the training domains,</p><formula xml:id="formula_47">P pY |U, xqP 1:k Z pU | xq " M w,x P pW |U qP 1:k Z pU | xq,</formula><p>where we define</p><formula xml:id="formula_48">P 1:k Z pY |xq " " P 1 pY | xq . . . P k Z pY | xq ‰</formula><p>, and likewise P 1:k Z pW | xq. Then the above becomes</p><formula xml:id="formula_49">P 1:k Z pY |xq " M w,x P 1:k Z pW | xq M w,x " P 1:k Z pY |xqP : 1:k Z pW | xq. (B.3)</formula><p>This demonstrates that we can recover the domain-invariant M w,x purely from observed data. One domain is not enough: We illustrate with an example, where we again consider the case where all variables are categorical:</p><formula xml:id="formula_50">P pY |xq " M w,x P pW |xq,<label>(B.4)</label></formula><p>where P pY | xq is a k Y ˆ1 vector of probabilities, P pW | xq is a k W ˆ1 vector of probabilities, and M is a k Y ˆkW matrix for which we wish to solve. We have too few equations for the number of unknowns.</p><p>One solution to (B.4) is the matrix of conditional probabilities M w,x " P pY |W, xq. This matrix is not invariant to changes to P pU q, however:</p><formula xml:id="formula_51">ppY |W, xq " ppY |U, xqP pU |W, xq.</formula><p>The posterior P pU |W, xq changes when the prior P pU q changes. In contrast, the solution in (B.3) is guaranteed to be domain invariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Proof of Proposition 4.3</head><p>For all r " 1, . . . , k Z , we can write</p><formula xml:id="formula_52">E r rY | xs " ErY | x, z r s " ż W m 0 pw, xqdP pw | x, z r q " ż U ż W m 0 pw, xqdP pw | uqdP pu | x, z r q; (B.5) ErY | x, z r s " ż U ErY | x, usdP pu | x, z r q. (B.6)</formula><p>By Assumption 6, the integrands of (B.5)-(B.6) have the following property</p><formula xml:id="formula_53">ErY | x, us " ż W m 0 pw, xqdP pw | uq, (B.7)</formula><p>almost surely with respect to P pU q. We will show that m 0 can be transferred to identify the distribution in the target domain. We define the support set S q pxq " tu : Qpu | xq ą 0u. Therefore, we can write</p><formula xml:id="formula_54">E q rY | xs " ż U ErY | u, xsdQpu | xq " ż Sqpxq ErY | u, xsdQpu | xq.</formula><p>Furthermore, since we have S q pxq Ď tu : P puq ą 0u, we can apply (B.7) to obtain</p><formula xml:id="formula_55">E q rY | xs " ż W ż U m 0 pw, xqdP pw | uqdQpu | xq " E q rm 0 pW, xq | xs.</formula><p>We complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Estimation Procedure</head><p>The estimation procedure of h 0 is discussed in Section C.1 and the estimation procedure of m 0 is discussed in Section C.2. In Section C.3, we discuss the case when either Z or C is a discrete variable.</p><p>C.1 Proof of Proposition 5.1</p><p>The proof of Proposition 5.1 simply follows the result in <ref type="bibr">(Mastouri et al., 2021)</ref> which extends from the representer theorem <ref type="bibr">(Schölkopf et al., 2001)</ref>. There exists a γ P R n 2 such that</p><formula xml:id="formula_56">h 0 " n 2 ÿ j"1 γ j µ W |c 2,j ,x 2,j b ϕpc 2,j q. (C.1)</formula><p>From <ref type="bibr">Song et al. (2009)</ref>, we have µ W |c 2,j ,x 2,j " ř n 1 i"1 b i pc 2,j , x 2,j qϕpw 1,i q and b i is the i-th element of b, a function on C ˆX : bpc, xq " pK</p><formula xml:id="formula_57">X 1 d K C 1 `λ1 n 1 Iq ´1 pΦ X 1 pxq d Φ C 1 pcqq. If we expand (C.1)</formula><p>with the previous expression, we have</p><formula xml:id="formula_58">h 0 " n 1 ÿ i"1 n 2 ÿ j"1 α ij ϕpw 1,i q b ϕpc 2,j q,</formula><p>where α ij " b i pc 2,j , x 2,j qγ j . Hence, the rest of the proof will focus on finding the expression of α ij . Following the proof technique developed in <ref type="bibr">(Mastouri et al., 2021)</ref>, we introduce two following lemmas that assist the analysis.</p><p>Lemma C.1. The square of the operator norm of h 0 , denoted as ||| h 0 ||| 2 H WC , can be represented as</p><formula xml:id="formula_59">||| h 0 ||| 2 H WC " vecpαq J pK C 2 b K W 1 q vecpαq.</formula><p>Apply Woodbury matrix identity, the above display is equivalent as</p><formula xml:id="formula_60">" E ´1Dpλ 2 n 2 I `DJ E ´1Dq ´1y 2 . (C.3)</formula><p>Using the fact that for matrices A, B, C, F , pA b BqpCbF q " ACbBF , we can simplify E ´1D as</p><formula xml:id="formula_61">E ´1D " ´K´1 C 2 b K ´1 W 1 ¯"K C 2 b ␣ K W 1 pK X 1 d K C 1 `λ1 n 1 Iq ´1pK X 12 d K C 12 q (‰ " IbpK X 1 d K C 1 `λ1 n 1 Iq ´1pK X 12 d K C 12 q " IbΓ.</formula><p>Hence, using the fact that pAbBq J pCbF q " pA J Cq d B J F , we have</p><formula xml:id="formula_62">D J E ´1D " pK C 2 bK W 1 Γq J pIbΓq " K C 2 d pΓ J K W 1 Γq</formula><p>Hence, we can write (C.3) as</p><formula xml:id="formula_63">vecpαq " pIbΓq ␣ λ 2 n 2 I `KC 2 d pΓ J K W 1 Γq ( ´1 y 2 .</formula><p>C.2 Proof of Kernel Bridge Function m 0</p><p>We begin with the results.</p><formula xml:id="formula_64">Proposition C.3. Let K W 3 P R n 3 ˆn3 , K X 4 P R n 4 ˆn4</formula><p>be the Gram matrices of W 3 and X 4 , respectively. Let K X 34 P R n 3 ˆn4 , K Z 34 P R n 3 ˆn4 be the cross Gram matrices of pX 3 , X 4 q and pZ 3 , Z 4 q, respectively. For any λ 4 ą 0, there exists a unique optimal solution to (5.6) of the form</p><formula xml:id="formula_65">m 0 " n 3 ÿ i"1 n 4 ÿ j"1 α ij ϕpw 3,i q b ϕpx 4,j q;</formula><p>vecpαq " pIbΓqpλ 4 n 4 I `Σq ´1y 4 , where Σ " pΓ</p><formula xml:id="formula_66">J K W 3 Γq d K X 4 , Γ " pK X 3 d K Z 3 `λ3 n 3 Iq ´1pK X 34 d K Z 34 q, and y 4 " " y 4,1 , . . . , y 4,n 4 ‰ J .</formula><p>The proof of Proposition C.3 follows exactly as the proof of Proposition 5.1, with X replaced by Z and C replaced by X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Estimation with discrete Z or C</head><p>In the case when C or Z happen to be discrete variables, a more efficient alternative to the estimator introduced in Section 5.1 which requires kernelized features of C (or Z), is to solve a separate regression of W on X for each c P C (or z P Z). Define the index set Ξ 1 pcq " ti : c 1,i " c, i " 1, . . . , n 1 u, we modify (5.3) as</p><formula xml:id="formula_67">µ p W |c,x " n 1 ÿ i"1 b i pxqϕpw 1,i q1pc 1,i " cq; bpxq " pK X 1,c `λ1 Iq ´1Φ X 1,c pxq,</formula><p>where K X 1,c " rkpx 1,i , x 1,j qs i,j and Φ X 1,c " " ϕpx 1,i q ‰ J i with i, j P Ξ 1 pcq. Alternatively, one can apply the form in (5.3) but use binary kernel on C (or Z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experiments</head><p>In this section we discuss the experimental settings and implementation details. We start with introducing the implementation details of all the baselines and proposed method. Then, we discuss the experimental settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Baselines of Adaptation with Concepts and Proxies</head><p>We introduce the baseline methods for the adaptation task with C and W . This includes the baselines methods COVARS, LABELS, ORACLE, LSA-W, LSA-S, LSA-S w/ target W and the proposed method. To select the parameters for the regression task on dSprite, we apply five-fold cross-validation with mean squared error as the metric to select the kernel length scale and the ridge regularization penalty.</p><p>COVARS. We fit a domain classifier using logistic regression, compute instance weights following <ref type="bibr">Shimodaira (2000)</ref>, and learn a weighted kernel ridge regressor with a Gaussian kernel function on the source training samples.</p><p>LABELS. The label shift baseline assumes oracle access to labels in the target domain. For the classification task, we compute instance weights qpY q{ppY q using the observed frequencies in the validation set for the source domain and the training set for the target domain. For the regression task, we compute the weights by fitting a Gaussian kernel density estimator using the source validation set and the target training set separately. We then use the fitted densities to estimate qpY q{ppY q for each sample in the source training set. Finally, we learn a sample-weighted kernel ridge regressor with a Gaussian kernel on the source training samples.</p><p>ORACLE. For regression tasks, we learn a kernel ridge regressor with a Gaussian kernel on target training samples. For the classification task, we use a standard MLP trained with sample in the target domain. Details of the model structure are documented in Section D.2.</p><p>LSA-W. The estimation procedure follows Section 6 in <ref type="bibr" target="#b0">Alabdulmohsin et al. (2023)</ref>. In this case, we discretize the values of W by applying additional transform signpwq for each sample w.</p><p>LSA-S. The estimation procedure follows Algorithm 2-5 in <ref type="bibr" target="#b0">Alabdulmohsin et al. (2023)</ref>.</p><p>LSA-S w/ target W . We briefly describe the procedure to incorporate target W to LSA-S. <ref type="bibr" target="#b0">Alabdulmohsin et al. (2023)</ref>  </p><formula xml:id="formula_68">Qp U | xq " arg min › › › › › › › » - - x QpW | xq, P pW | U " 1qy . . . x QpW | xq, P pW | U " k U qy fi ffi fl ´G » - - Qp U " 1 | xq . . . Qp U " k U | xq fi ffi fl › › › › › › › 2 F , subject to 0 ď Qp U " i | xq ď 1, i " 1, . . . , k U ; k U ÿ i"1 Qp U " i | xq " 1.</formula><p>Then, we compute the predicted conditional probability based on (D.1). Proposed Method. For the regression task using the dSprite dataset, we employ the Gaussian kernel function as the feature map for both X and W . In the classification task, we also utilize the Gaussian kernel function for X and W . Additionally, we make use of a columnwise binary kernel for C, which performs a binary kernel operation on each entry and computes the product of all function outputs. To compute h 0 , we apply one-hot encoder on Y and apply the results in Proposition 5.1 For choosing the kernel length scale for the classification task, we use the validation set with AUROC metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Baselines of Multi-Source Adaptation</head><p>For the first three baselines: Cat-ERM, Avg-ERM, and SA, we use a standard MLP model as the backbone structure. It is a single hidden layer MLP with size 100 and ReLU activation functions. The network is trained using Adam optimizer <ref type="bibr" target="#b62">(Kingma and Ba, 2014)</ref> with learning rate 10 ´3. The batch size is set to be 200 and the maximum number of iteration is set to be 300.</p><p>Cat-ERM. We concatenate all the samples across environments into one dataset. Then, we train the model with a standard MLP model as specified above.</p><p>Avg-ERM. For each environment, we train a standard MLP model. During testing, we take the average of predictions from all models.</p><p>Simple Adaptation (SA) <ref type="bibr">(Mansour et al., 2008)</ref>. To implement the method, we build kernel density estimators with Gaussian kernel function to estimate the density p r pxq for r " 1, . . . , k Z . We then reweigh the output of the classifier, a standard MLP, of each domain with the normalized weight P r px new q{ t ř r P r 1 px new qu. The kernel length scale is chosen using five-fold cross-validation with AUROC metric.</p><p>Marginal Kernel (MK) <ref type="bibr">(Blanchard et al., 2011)</ref>. This method involves a kernel SVM with a product kernel on pX , P pXqq. For any x, x 1 P X and a distribution on X, P, P 1 , the kernel function is defined as kppx, P q, px 1 , P 1 qq " k 1 px, x 1 qk 2 pP, P 1 q. Let n be the number of samples. Here k 1 is a Gaussian kernel function, and k 2 is the mean of the Gram matrix rkpx i , x 1 j qs ij P R nˆn , where x i for i " 1, . . . , n is a i.i.d. sample from P and x 1 j for j " 1, . . . , n is a i.i.d. sample from P 1 . To accommodate the large dataset, we precompute the Gram matrix and apply it to a linear classifier trained using Stochastic Gradient Descent (SGD) implemented in the package scikit-learn <ref type="bibr" target="#b72">(Pedregosa et al., 2011)</ref>. The kernel length scale is chosen using five-fold cross-validation with AUROC metric.</p><p>Weighted Combination of Source Classifiers (WCSC) <ref type="bibr">(Zhang et al., 2015)</ref>. For each source environment, we estimate the conditional probability X | y using kernel density estimator with the Gaussian kernel function. The rest of the estimation procedure follows Section 2 in <ref type="bibr">Zhang et al. (2015)</ref>. The kernel length scale is chosen using five-fold cross-validation with AUROC metric.</p><p>Proposed Method. We use columnwise Gaussian kernel function as the feature map of X, a Gaussian kernel function as the feature map of W . The conditional mean embedding µ p W |x,z is estimated using the approach introduced in Section C.3. The analytical solution of m 0 is discussed in Proposition C.3. All the kernel length scale and the regularization parameters λ 3 , λ 4 are selected using five-fold cross-validation with AUROC metric.</p><p>ORACLE. The model is x m 0 , µ q W |x y, where both the bridge function m 0 and µ q W |x are estimated using the target dataset, with the number of training samples equal to the training samples of the source domain. All the kernel length scale and the regularization parameters λ 3 , λ 4 are selected using five-fold cross-validation with AUROC metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Classification Task</head><p>The classification task discussed in Section D.6 is first introduced <ref type="bibr" target="#b0">Alabdulmohsin et al. (2023)</ref>. Let op¨q be the one-hot encoder, we follow their data generation procedure and generate samples using the following data generation process:</p><formula xml:id="formula_69">U " Categoricalpπq; W | U " u " N popuqM W |U , 1 ˘; X | U " u " N popuqM X|U , I k X q; C i | X " x, U " u " Bernoulli ´logit ´1`r xM C|X,U "u `opuqM C|U s i ˘¯; Y | C " c, U " u " Bernoulli ´logit ´1`c M Y |C,U "u `opuqM Y |U ˘¯,</formula><p>where the matrices are defined as</p><formula xml:id="formula_70">M W |U :" " ´1 1 ‰ J , M X|U :" a w " ´1 1 1 ´1ȷ , M C|U :" " ´2 2 2 ´1 1 2 ȷ ; M C|X,U "u 0 :" 3 " ´2 2 ´1 1 ´2 ´3ȷ , M C|X,U "u 1 :" 3 " 2 ´2 1 ´1 2 3 ȷ ; M Y |U :" " 2 2 ‰ J , M Y |C,U "u 0 :" " 3 ´2 ´1‰ J , M Y |C,U "u 1 :" " 3 ´1 ´2‰ J .</formula><p>The coefficient a w " 1 in Figure <ref type="figure" target="#fig_3">2a</ref>. Figure <ref type="figure" target="#fig_1">4</ref> displays additional results where a w " 2, 3. We generate 7000 training samples, 1000 validation samples, and 2000 testing samples for the classification task with concepts and proxies.</p><p>In the multi-domain case, we construct 3 different tasks: Task 1 is composed of z 1 , z 2 , z 3 such that P pU " 0 | z 1 q " 0.1, P pU " 0 | z 2 q " 0.2, P pU " 0 | z 3 q " 0.3 and a target domain with QpU " 0q " 0.9. For task 2, we select z 4 , z 5 , z 6 such that P pU " 0 | z 4 q " 0.4, P pU " 0 | z 5 q " 0.5, P pU " 0 | z 6 q " 0.6 and QpU " 0q " 0.9. For task 3, we select z 7 , z 8 , z 9 such that P pU " 0 | z 7 q " 0.7, P pU " 0 | z 8 q " 0.8, P pU " 0 | z 9 q " 0.9 and QpU " 0q " 0.4. The results are shown in Table <ref type="table">1</ref>-2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Comparison to Domain Generalization Baselines</head><p>Given that we observe multiple domains at test time, a natural question is: Does adaptation give us an advantage over generalization? In generalization, we cannot assume to have any observations in the target domain. We compare our adaptation method with multi-domain generalization baselines <ref type="bibr">(Muandet et al., 2013)</ref>: Adaptive Risk Minimization (ARM) <ref type="bibr" target="#b80">(Zhang et al., 2021)</ref>, Conditional Domain Adversarial Neural Networks (CDANN) <ref type="bibr" target="#b65">(Long et al., 2018)</ref> (CORAL) <ref type="bibr" target="#b78">(Sun and Saenko, 2016)</ref>, Domain Adversarial Neural Networks (DANN) <ref type="bibr">(Ganin et al., 2016)</ref>, Distributionally Robust Optimization for Group Shifts (GroupDRO) <ref type="bibr" target="#b73">(Sagawa et al., 2019)</ref>, Invariant Risk Minimization (IRM) <ref type="bibr">(Arjovsky et al., 2019)</ref>, Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b57">(Borgwardt et al., 2006)</ref>, and Risk Extrapolation (REx) <ref type="bibr" target="#b63">(Krueger et al., 2021)</ref>.</p><p>In Table <ref type="table" target="#tab_2">2</ref>, we show that our proposed method for domain adaptation in the multi-domain setting outperforms the state-of-the-art multi-domain generalization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Regression Tasks</head><p>We consider three tasks. We will first introduce the simulated task and then discuss about the task on dSprite data <ref type="bibr">(Matthey et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5.1 Simulated Dataset</head><p>We consider the following data generation process.</p><p>Simulated regression task 1.</p><p>U " Berpaq;</p><p>X " N p0, 1q;</p><p>Y " ´X1 pU "0q `X1 pU "1q ; (D.3)</p><p>W " N p´1, 0.01q1 pU "0q `N p1, 0.01q1 pU "1q .</p><p>There are two source domains. We set a " 0.1 for source domain z 1 and a " 0.9 for source domain z 2 . According to the data generation process (D.3), Y is mostly positively correlated with X in domain z 1 and negatively correlated with X in domain z 2 . For each domain, we synthesized 2000 training samples and 1000 testing samples. We sweep across a " t0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9u in the target domain. We run 10 replications and the results shown in Figure <ref type="figure" target="#fig_6">5</ref>. In the next task, we set U to be a continuous random variable following a Beta distribution.</p><p>In this task, we expect the Cat-ERM method to fail drastically as we anticipate that the predicted Y versus X is a flat line -the predicted result would be an average of the downward sloping line pU " 0q and upward sloping line pU " 1q. The result in Figure <ref type="figure" target="#fig_6">5</ref> supports our hypothesis, as the mean squared error remains nearly flat as we vary the target distribution QpU q.</p><p>Simulated regression task 2.</p><p>U " Betapa, bq X " N p0, 1q</p><p>Y " p2U ´1qX</p><p>W " N p´1, 0.01qp1 ´U q `N p1, 0.01qU.</p><p>There are two source domains, corresponding to two draws from P pZq which we write z r " pa, bq.</p><p>We set a " 2, b " 4 for the first source domain r " 1, and a " 4, b " 2 for the second source domain r " 2. The corresponding distributions over U are shown in Figure <ref type="figure">6</ref>. Under this setting, we test the target domain with a, b " 1, . . . , 5, with distributions shown in Figure <ref type="figure">6</ref>. For each domain, we synthesized 2000 training samples and 1000 testing samples. We run 10 replications and the results shown in Figure <ref type="figure" target="#fig_6">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6 Adaptation with Concepts and Proxies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6.1 dSprites Dataset</head><p>We test the proposed procedure on the dSprites dataset <ref type="bibr">(Matthey et al., 2017)</ref>, an image dataset described by five latent parameters (shape, scale, rotation, posX, and posY). Motivated by <ref type="bibr">Matthey et al. (2017)</ref>'s experiments, we design a regression task where the dSprites images (64 ˆ64 = 4096-dimensional) are X P R 64ˆ64 and subject to a nonlinear confounder U P r0, 2πs which is a rotation of the image (Figure <ref type="figure">7</ref>). We fix all other latent parameters -shape is heart, scale is maximized, and all others are set to their 0'th position. W P R and C P R are continuous random variables. The data generation process is defined as follows U p " 2πBetap2, 4q, U q " Uniformpa, 2πq;</p><p>X " Rotatepimage, U radsq `η, η " N p0, 0.01I 64 q;</p><p>C " ˜0.1}X T A} 2 2 ´5000 2000 ¸2 `U `γ;</p><p>A " Uniformp0, 1q, A P R 4096ˆ10 , γ " N p0, 0.5q;</p><p>Y " 1 4 C `1 20 sinpU q `ε, ε " N p0, 0.1q;</p><p>W " cospU q `ν, ν " N p0, 0.25q.</p><p>When fitting all model, both baselines and the proposed method, we project the images R 4096 to R 16 via Gaussian Random Projection using the scikit-learn implementation (Bingham and Mannila, The proposed method is close to the ORACLE method as compared all other competing methods that is vulnerable to the distribution shifts. Other figures: results of regression task 2. In each plot, we fix b and vary a. For all plots, it appears that when a " b, the mean squared error of all methods converge to a point. This is the case when the target density function of U has a peak centered around 0.5, as shown in Figure <ref type="figure">6</ref>, and hence Y " p2U ´1qX is close to zero for most samples.</p><p>2001; <ref type="bibr" target="#b72">Pedregosa et al., 2011)</ref>. Additionally, for the proposed method, we use a Gaussian kernel as the feature map for X, C. We generate 7000 training samples and 3000 test samples in our experiments. Then, we use five-fold cross-validation to select hyperparameters for baselines and proposed method for each a (U p " Uniformpa, 2πq) -hyperparameters are (i) ridge regression penalty and (ii) Gaussian kernel scaling factor. Once we select a set of hyperparameters for a value of a, we perform 10 new random data regenerations to get transfer errors with 95% confidence intervals (Figure <ref type="figure" target="#fig_3">2b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.7 Classification of radiological findings with MIMIC-CXR</head><p>We conduct a small-scale experiment with chest X-ray data extracted from the MIMIC-CXR dataset <ref type="bibr">(Johnson et al., 2019)</ref>. We consider classification of the absence of a radiological finding in a chest X-ray. For this, we use the set of labels extracted by <ref type="bibr">Irvin et al. (2019)</ref>. These labels correspond to 14 categories of radiological findings extracted based on mentions in the associated radiology reports. We specifically consider classification of the "No Finding" (Y " 1) label, corresponding to cases where no pathology was identified as positive or uncertain in the radiology report.</p><p>To define the dataset, we consider the set of 217,536 chest X-rays with defined Chexpert labels <ref type="bibr">(Irvin et al., 2019)</ref>, MIMIC-IV entries, and pretrained embeddings <ref type="bibr">(Sellergren et al., 2022)</ref>. We then filter this dataset to the 212,567 examples considered as a part of the "train" partition provided</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Causal diagrams. The shaded circle denotes unobserved variable and the solid circle denotes observed variable. X is the covariate, Y is the response, C is the concept, W is the proxy, Z is the domain-related variable, and U is the latent variable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Example 4. 4 .</head><label>4</label><figDesc>Recall the decomposition of both sides of (4.3). Under Assumption 2 and given the existence of m 0 (Proposition A.2), E p rY | x, zs " ż W m 0 pw, xqdP pw | x, zq " ż U ż W m 0 pw, xqdP pw | uqdP pu | x, zq; (4.5) E p rY | x, zs " ż U E p rY | x, usdP pu | x, zq. (4.6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Regression on the dSprites dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Adaptation results with concept and proxy. Shown is the average evaluation metric on held-out target distribution samples across 10 independent replicates of the data. The proposed method is robust to the latent shift compared to the baselines in both cases. (a) We set P pU " 1q " 0.1. Both the AUROC and accuracy remains nearly constant in various degree of shifts, while the performance of other baselines drops as QpU " 1q moves to 0.9. (b) The left figure denotes the density function of U , the overlapping area of two distribution shrinks as a moves rightward. The result on the right shows that our method is robust even when the overlapping area between two distributions is small.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Concept and multi-domain adaptation with MIMIC-CXR. Shown are the mean SD AUROC of concept (left) and multi-domain adaptation (right) for classification of "No finding"from embeddings of chest X-rays over five replicates of a sampling procedure that introduces a shift in the prevalence of "No finding" with patient sex subgroups, where radiology report embeddings serve as concept variables C and patient age serves as the proxy W . In the concept adaptation experiment, the source domain corresponds to P pU " 1q " P pY " 1 | Sex " Femaleq " P pY " 0 | Sex " Maleq " 0.1. In the multi-domain adaptation experiment, we consider two source domains P pU " 1q " t0.1, 0.2u.</figDesc><graphic coords="12,95.40,72.00,421.20,128.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>showed that QpY |xq can be decomposed as QpY | xq " a permutation of original u. Both LSA-WAE and LSA-S are multi-stage procedures to compute (a), (c), (d) individually and combine the results using formula (D.2) to obtain the predicted target distribution. Step (a) corresponds to Algorithm 5, (c) corresponds to Equation (17), and (d) corresponds to Algorithm 4 in<ref type="bibr" target="#b0">(Alabdulmohsin et al., 2023)</ref>.With the additional W from target, we can obtain (b) by slightly modifying the one estimation step in LSA-S. We test on this procedure, namely LSA-S w/ target W, with (c), (d) replaced by (b). Suppose that U takes values in 1, . . . , k U and U be a permutation of U . Define the matrix G as:G " » --x P pW | U " 1q, P pW | U " 1qy ¨¨¨x P pW | U " 1q, P pW | U " k U qy . . . . . . . . . x P pW | U " k U q, P pW | U " 1qy ¨¨¨x P pW | U " k U q, P pW | U " k U qy fi ffi fl ,where P pW | U " iq is the estimated conditional kernel density function obtained by Algorithm 3 in<ref type="bibr" target="#b0">Alabdulmohsin et al. (2023)</ref>. The step (b) is computed by solving the following least-squares:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Top left: results of regression task 1. The proposed method is close to the ORACLE method as compared all other competing methods that is vulnerable to the distribution shifts. Other figures: results of regression task 2. In each plot, we fix b and vary a. For all plots, it appears that when a " b, the mean squared error of all methods converge to a point. This is the case when the target density function of U has a peak centered around 0.5, as shown in Figure6, and hence Y " p2U ´1qX is close to zero for most samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>xq be the integral operator associated with the kernel function kpw, x, zq " ppw, z | xq{pppw | xqppz | xqq. Then, we can write E p rY | x, zs " ż kpw, x, zqppw | xqm 0 pw, xqdw " K x m 0 . Proposition A.2 (Existence of m 0 , Proposition 1 in Miao et al. (2018)). Assume that 1. for any mean squared integrable function g and for x P X , ErgpZq | W, xs " 0 almost surely if and only if gpZq " 0 almost surely; 2. For any x P X , ş f pw | x, zqf pz | x, wqdwdz ă 8; 3. For any x P X , ErY | Z, xs P L 2 pZ | xq; 4. For any x P X , ř iPN λ ´2 x,i |xErY | Z, xs, ψ x,i y| 2 ă 8, where pλ x,i</figDesc><table /><note><p>W ş Z</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>, Correlation Alignment Classification results with a w " 2, 3. The figures indicate that LSA-S and LSA-S w/ target W have comparable performance, aggregating the target W does not seem to improve the performance. Multi-domain generalization vs. (proposed) adaptation result. The values are the average AUROC of 10 independent runs drawn from the data generating process. Each task has three source domains with different P r pU q and one target domain. The proposed method has outperformed all domain generalization benchmarks across all tasks.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">a w = 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>a w = 3</cell></row><row><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell>94</cell><cell></cell><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>94</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>92</cell></row><row><cell>AUROC</cell><cell>0.75 0.80 0.85 0.90</cell><cell></cell><cell>Accuracy (%)</cell><cell>84 86 88 90</cell><cell></cell><cell>AUROC</cell><cell>0.75 0.80 0.85 0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy (%)</cell><cell>84 86 88 90</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82</cell></row><row><cell></cell><cell>0.70</cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell>0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell></row><row><cell></cell><cell>0.2</cell><cell>0.4 Q(U=1) 0.6</cell><cell>0.8</cell><cell>0.2</cell><cell>0.4 Q(U=1) 0.6</cell><cell>0.8</cell><cell></cell><cell>0.2</cell><cell cols="2">0.4 Q(U=1) 0.6</cell><cell>0.8</cell><cell>0.2</cell><cell>0.4 Q(U=1) 0.6</cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell cols="2">ORACLE</cell><cell>ERM</cell><cell>LSA-S</cell><cell cols="3">LSA-S w/ target W</cell><cell></cell><cell>LSA-WAE</cell><cell></cell><cell>Proposed</cell></row><row><cell cols="10">Figure 4: ORACLE ARM CDANN CORAL DANN GroupDRO</cell><cell cols="2">IRM</cell><cell>MMD</cell><cell>VREx Proposed</cell></row><row><cell cols="2">Task 1</cell><cell>0.9425</cell><cell>0.8065</cell><cell>0.8061</cell><cell>0.8030</cell><cell>0.8039</cell><cell></cell><cell cols="2">0.7954</cell><cell cols="2">0.7989</cell><cell>0.8055</cell><cell>0.8010</cell><cell>0.8848</cell></row><row><cell></cell><cell></cell><cell>˘0.0039</cell><cell cols="2">˘0.0247 ˘0.0252</cell><cell cols="3">˘0.0236 ˘0.0229</cell><cell cols="2">˘0.0323</cell><cell cols="3">˘0.0283 ˘0.0248 ˘0.0279</cell><cell>˘0.0120</cell></row><row><cell cols="2">Task 2</cell><cell>0.9431</cell><cell>0.9143</cell><cell>0.9159</cell><cell>0.9158</cell><cell>0.9158</cell><cell></cell><cell cols="2">0.9160</cell><cell cols="2">0.9131</cell><cell>0.9149</cell><cell>0.9136</cell><cell>0.9318</cell></row><row><cell></cell><cell></cell><cell>˘0.0061</cell><cell cols="2">˘0.0150 ˘0.0125</cell><cell cols="3">˘0.0132 ˘0.0125</cell><cell cols="2">˘0.0125</cell><cell cols="3">˘0.0135 ˘0.0135 ˘0.0124</cell><cell>˘0.0063</cell></row><row><cell cols="2">Task 3</cell><cell>0.8876</cell><cell>0.8470</cell><cell>0.8456</cell><cell>0.8473</cell><cell>0.8480</cell><cell></cell><cell cols="2">0.8487</cell><cell cols="2">0.8469</cell><cell>0.8470</cell><cell>0.8470</cell><cell>0.8569</cell></row><row><cell></cell><cell></cell><cell>˘0.0085</cell><cell cols="2">˘0.0171 ˘0.0164</cell><cell cols="3">˘0.0163 ˘0.0166</cell><cell cols="2">˘0.0185</cell><cell cols="3">˘0.0186 ˘0.0181 ˘0.0132</cell><cell>˘0.0095</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments: We thank <rs type="person">Zhu Li</rs> and <rs type="person">Dimitri Meunier</rs> for helpful discussions. AG was partly supported by the <rs type="funder">Gatsby Charitable Foundation</rs>. OS was partly supported by the <rs type="funder">UIUC Beckman Institute Graduate Research Fellowship</rs>, <rs type="grantNumber">NSF-NRT 1735252</rs>. KT was partly supported by <rs type="funder">NSF</rs> <rs type="programName">Graduate Research Fellowship Program</rs>. SK was partly supported by the <rs type="funder">NSF</rs> <rs type="grantNumber">III 2046795</rs>,  IIS 1909577, CCF 1934986, <rs type="funder">NIH</rs> <rs type="grantNumber">1R01MH116226-01A</rs>, <rs type="funder">NIFA</rs> award <rs type="grantNumber">2020-67021-32799</rs>, the <rs type="funder">Alfred P. Sloan Foundation</rs>, and <rs type="funder">Google Inc.</rs> This study was funded by <rs type="funder">Google LLC</rs> and/or a subsidiary thereof ('<rs type="projectName">Google</rs>').</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8SVWVb7">
					<idno type="grant-number">NSF-NRT 1735252</idno>
				</org>
				<org type="funding" xml:id="_yhCHTux">
					<orgName type="program" subtype="full">Graduate Research Fellowship Program</orgName>
				</org>
				<org type="funding" xml:id="_T3r9EBB">
					<idno type="grant-number">III 2046795</idno>
				</org>
				<org type="funding" xml:id="_jbEk4sx">
					<idno type="grant-number">1R01MH116226-01A</idno>
				</org>
				<org type="funding" xml:id="_GRmcuHh">
					<idno type="grant-number">2020-67021-32799</idno>
				</org>
				<org type="funded-project" xml:id="_FX7jYQx">
					<orgName type="project" subtype="full">Google</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Identification of the Distribution</head><p>In this section, we demonstrate the existence of the bridge functions h 0 and m 0 under certain regularity conditions. We first discuss the discrete case and then generalize to the continuous case.</p><p>A.1 The Discrete Case of the Bridge Function h 0</p><p>The idea of bridge function h 0 may seem abstract in the continuous setting. When every variable is discrete, however, the construction of the bridge function is demonstrated by solving series of matrix problems. This idea originates from <ref type="bibr">Miao et al. (2018)</ref> and we apply the technique to show the construction of bridge function when every variable pW, U, C, X, Y q is discrete.</p><p>Let</p><p>be a column vector, and a matrix, respectively. We define similarly</p><p>analogously. As an alternative to finding a h 0 pw, cq such that</p><p>the proxy problem is converted to finding a H 0 pY, W, cq such that</p><p>First, under the condition that W K K tX, Cu | U , we can write</p><p>Similarly, under the condition that Y K K X | tU, Cu, we have</p><p>We introduce the following assumption:</p><p>Assumption 7. Columns of PpW | U q are linearly independent. For every c P C, the columns of PpW | X, cq satisfy PpW | x, cq P N pPpW | U q ˚qK for all x P X .</p><p>Proof of Lemma C.1. Write</p><p>Using the fact that vecpABCq " pC J b Aq vecpBq, the above display can be written as</p><p>Lemma C.2. For any c P C, x P X ,</p><p>Summing over i, j, the above equation is equivalent as</p><p>With Lemma C.1-C.2, we can write (5.4) as</p><p>where</p><p>Then by setting the gradient of (C.2) with respect to vecpαq to zero, we will obtain vecpαq " `DD J `λ2 n 2 E ˘´1 Dy 2 .  To define distribution shifts, we adopt a problem formulation similar to that of <ref type="bibr">Makar et al. (2022)</ref>, where patient sex is considered as a possible "shortcut" in the classification of the absence of a radiological finding. As in <ref type="bibr">Makar et al. (2022)</ref>, we impose distribution shift through structured resampling of the data where P pU " 1q " P pY " 1 | Sex " Femaleq " P pY " 0 | Sex " Maleq. For example, when P pU " 1q " 0.1, the prevalence of P pY " 1 | Sex " Femaleq " 0.1 and P pY " 1 | Sex " Maleq " 0.9. We implement the shift through a weighted sampling procedure that maintains the label shift invariance within patient sex subgroups, i.e., preserves X | Y, A under the distribution shift, where A corresponds to patient sex. This procedure further fixes the total proportion of male and female patients in the population at 50%. For our experiments, we consider nine domains corresponding to cases where P pU " 1q P t0.1, 0.2, . . . , 0.9u.</p><p>We perform both concept adaptation and multi-domain adaptation experiments with the MIMIC-CXR data. For the concept adaptation experiment, we perform weighted sampling with replacement of 1,000 examples from each of the training, validation, and testing partitions defined previously, separately for each domain. We fix the source domain to the case where P pU " 1q " 0.1 and then adapt to each of the nine target domains. For the multi-domain adaptation experiment, we randomly sample 500 examples per domain and partition from the sets of 1,000 examples defined for the concept experiment. For this experiment, we consider a case where two source domains corresponding to P pU " 1q " 0.1 and P pU " 1q " 0.2 are available. To match the size of the aggregate source domain data with the size of the target domain, we sample 250 examples per partition for each source domain. We repeat the sampling procedure five times and report the mean ˘standard deviation of performance metrics over the five replicates.</p><p>For both experiments, we perform two-fold cross-validation for the kernel length-scale parameters using data from the source domain(s). Here, we compare to ridge logistic regression models fit in the source and target domains, with the ridge penalty fit with five-fold cross validation. We use LR-Target to refer to logistic regression models fit in a target domain, LR-SOURCE to refer to models fit in a source domain, and Cat-LR to refer to logistic regression models fit with concatenated data from the multiple source domains. We use Bridge-SOURCE to refer to the kernel estimator that leverages the bridge function (h 0 or m 0 for the concept and multi-domain adaptation settings, respectively) and conditional mean embedding (µ W C|x or µ W |z,x ) fit on the source domain data. Bridge-TARGET refers to the kernel estimator where both the bridge function and conditional mean embedding are fit on the target domain data.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adapting to latent subgroup shifts via concepts and proxies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>D'amour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Pfohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Salaudeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrouff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9637" to="9661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<title level="m">Invariant risk minimization</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generalizing from several related classification tasks to a new unlabeled sample</title>
		<author>
			<persName><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Comparison of a screening test and a reference test in epidemiologic studies. ii. a probabilistic model for the comparison of diagnostic tests</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Epidemiology</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="593" to="602" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semiparametric proximal causal inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tchetgen Tchetgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Deaner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00283</idno>
		<title level="m">Proxy controls and panel data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the completeness condition in nonparametric instrumental problems</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Haultfoeuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometric Theory</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="460" to="471" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Explaining classifiers with causal concept effect (cace)</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07165</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Grünewälder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baldassarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1205.4656</idno>
		<title level="m">Conditional mean embeddings as regressors-supplementary</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Estimating causal effects from epidemiological data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hernán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Epidemiology &amp; Community Health</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="578" to="586" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ciurea-Ilcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haghgoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shpanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>-Y. Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Horng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">317</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Concept bottleneck models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mussmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5338" to="5348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wilds: A benchmark of in-the-wild distribution shifts</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5637" to="5664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Measurement bias and effect restoration in causal inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kuroki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="437" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detecting and correcting for label shift with black box predictors</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3122" to="3130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4114" to="4124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Domain adaptation by using causal inference to predict invariant conditional distributions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Van Ommen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Claassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bongers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Versteeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Causally motivated shortcut removal using auxiliary labels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Makar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blalock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>D'amour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="739" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Shifts: A dataset of real distributional shift across multiple large-scale tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Band</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chesnokov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Noskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ploskonosov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Prokhorenkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Provilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Raina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07455</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Domain adaptation with multiple sources</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Proximal causal learning with kernels: Two-stage estimation and moment restriction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mastouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gultchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7512" to="7523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://github.com/deepmind/dsprites-dataset/" />
		<title level="m">dsprites: Disentanglement testing sprites dataset</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Identifying causal effects with proxy variables of an unmeasured confounder</title>
		<author>
			<persName><forename type="first">W</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Tchetgen Tchetgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="987" to="993" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Identifying effects of multiple treatments in the presence of unmeasured confounding</title>
		<author>
			<persName><forename type="first">W</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Ogburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regularizing towards causal invariance: Linear models with proxies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oberst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8260" to="8270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="199" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Causal inference using invariant prediction: identification and confidence intervals</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A critical look at the consistency of causal estimation with deep latent variable models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marttinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4207" to="4217" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Anchor regression: Heterogeneous data meet causality</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rothenhäusler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="246" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A generalized representer theorem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on computational learning theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="416" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sgouritsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6471</idno>
		<title level="m">On causal and anticausal learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Diagnosing failures of fairness transfer across distribution shift in real-world medical settings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schrouff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schnider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Opsahl-Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mincu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2022">19304-19318, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simplified transfer learning for chest radiography models using less data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Sellergren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Nabulsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kalidindi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Etemadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">305</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="454" to="465" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Towards out-of-distribution generalization: A survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13624</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical planning and inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Kernel instrumental variable regression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A hilbert space embedding for distributions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on algorithmic learning theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="13" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hilbert space embeddings of conditional distributions with applications to dynamical systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="961" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Causation, prediction, and search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Preventing failures due to dataset shift: Learning predictive models that transport</title>
		<author>
			<persName><forename type="first">A</forename><surname>Subbaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schulam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saria</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3118" to="3127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J T</forename><surname>Tchetgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Miao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10982</idno>
		<title level="m">An introduction to proximal causal learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Counterfactual invariance to spurious correlations in text classification</title>
		<author>
			<persName><forename type="first">V</forename><surname>Veitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>D'amour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yadlowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="16196" to="16208" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains: A survey on domain generalization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Kernel single proxy control for deterministic confounding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.04585</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation: A causal view</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Domain generalization: A survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Adapting to latent subgroup shifts via concepts and proxies</title>
		<author>
			<persName><forename type="first">I</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>D'amour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pfohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salaudeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schrouff</surname></persName>
		</author>
		<author>
			<persName><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9637" to="9661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<title level="m">Invariant risk minimization</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Random projection in dimensionality reduction: applications to image and text data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Generalizing from several related classification tasks to a new unlabeled sample</title>
		<author>
			<persName><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Integrating structured biological data by kernel maximum mean discrepancy</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Linear inverse problems in structural econometrics estimation based on spectral decomposition and regularization. Handbook of econometrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Carrasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Florens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Renault</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="5633" to="5751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ciurea-Ilcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haghgoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shpanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>-Y. Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Horng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">317</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Out-of-distribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Le</forename><surname>Priol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5815" to="5826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Causally motivated shortcut removal using auxiliary labels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Makar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blalock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>D'amour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="739" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Domain adaptation with multiple sources</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Proximal causal learning with kernels: Two-stage estimation and moment restriction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mastouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gultchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7512" to="7523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://github.com/deepmind/dsprites-dataset/" />
		<title level="m">dsprites: Disentanglement testing sprites dataset</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Identifying causal effects with proxy variables of an unmeasured confounder</title>
		<author>
			<persName><forename type="first">W</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Tchetgen Tchetgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="987" to="993" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08731</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A generalized representer theorem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on computational learning theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="416" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Simplified transfer learning for chest radiography models using less data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Sellergren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Nabulsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kalidindi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Etemadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">305</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="454" to="465" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical planning and inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Hilbert space embeddings of conditional distributions with applications to dynamical systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="961" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016 Workshops: Amsterdam</title>
		<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">October 8-10 and 15-16, 2016. 2016</date>
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III 14</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation: A causal view</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Adaptive risk minimization: Learning to adapt to domain shift</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dhawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="23664" to="23678" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
