<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Sparsity Principle for Partially Observable Causal Representation Learning</title>
				<funder>
					<orgName type="full">ISTA</orgName>
				</funder>
				<funder>
					<orgName type="full">International Max Planck Research School for Intelligent Systems</orgName>
					<orgName type="abbreviated">IS</orgName>
				</funder>
				<funder ref="#_pPRwtzN">
					<orgName type="full">Air Force Office of Scientific Research</orgName>
				</funder>
				<funder>
					<orgName type="full">IVADO</orgName>
				</funder>
				<funder ref="#_JGkxKRJ">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_hfK7Acm">
					<orgName type="full">United States Air Force</orgName>
				</funder>
				<funder>
					<orgName type="full">Amazon</orgName>
				</funder>
				<funder ref="#_fMhmjTr">
					<orgName type="full">German Federal Ministry of Education and Research (BMBF)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-15">15 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Danru</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dingling</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Perouz</forename><surname>Taslakian</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Julius</forename><surname>Von Kügelgen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sara</forename><surname>Magliacane</surname></persName>
						</author>
						<title level="a" type="main">A Sparsity Principle for Partially Observable Causal Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-15">15 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2403.08335v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causal representation learning aims at identifying high-level causal variables from perceptual data. Most methods assume that all latent causal variables are captured in the high-dimensional observations. We instead consider a partially observed setting, in which each measurement only provides information about a subset of the underlying causal state. Prior work has studied this setting with multiple domains or views, each depending on a fixed subset of latents. Here we focus on learning from unpaired observations from a dataset with an instance-dependent partial observability pattern. Our main contribution is to establish two identifiability results for this setting: one for linear mixing functions without parametric assumptions on the underlying causal model, and one for piecewise linear mixing functions with Gaussian latent causal variables. Based on these insights, we propose two methods for estimating the underlying causal variables by enforcing sparsity in the inferred representation. Experiments on different simulated datasets and established benchmarks highlight the effectiveness of our approach in recovering the ground-truth latents. tured, high-dimensional observations of a causal system. Motivated by this shortcoming, causal representation learning (CRL;<ref type="bibr" target="#b16">Schölkopf et al., 2021)</ref> aims to infer high-level causal variables from low-level data such as images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Endowing machine learning models with causal reasoning capabilities is a promising direction for improving their robustness, generalization, and interpretability <ref type="bibr" target="#b10">(Spirtes et al., 2000;</ref><ref type="bibr">Pearl, 2009;</ref><ref type="bibr">Peters et al., 2017)</ref>. Traditional causal inference methods assume that the causal variables are given a priori, but in many real-world settings, we only have unstruc-A popular approach to identify (i.e., provably recover) high-level latent variables is (nonlinear) independent component analysis (ICA) <ref type="bibr">(Hyvarinen and Morioka, 2016;</ref><ref type="bibr" target="#b15">2017;</ref><ref type="bibr">Hyvarinen et al., 2019;</ref><ref type="bibr">Khemakhem et al., 2020)</ref>, which aims to recover independent latent factors from entangled measurements. Several works generalize this setting to the case in which the latent variables can have causal relations <ref type="bibr" target="#b22">(Yao et al., 2022;</ref><ref type="bibr" target="#b5">Brehmer et al., 2022;</ref><ref type="bibr" target="#b5">Lippe et al., 2022;</ref><ref type="bibr">2023;</ref><ref type="bibr">Ahuja et al., 2023a;</ref><ref type="bibr">b;</ref><ref type="bibr">Lachapelle et al., 2022;</ref><ref type="bibr">2023;</ref><ref type="bibr">2024;</ref><ref type="bibr" target="#b16">von Kügelgen et al., 2021;</ref><ref type="bibr">2023;</ref><ref type="bibr" target="#b18">Wendong et al., 2023;</ref><ref type="bibr" target="#b11">Squires et al., 2023;</ref><ref type="bibr" target="#b6">Buchholz et al., 2023;</ref><ref type="bibr" target="#b23">Zhang et al., 2023)</ref>, establishing various identifiability results under different assumptions on the available data and the generative process. However, most existing works assume that all causal variables are captured in the high-dimensional observations. Notable exceptions include <ref type="bibr" target="#b12">Sturma et al. (2023)</ref> and <ref type="bibr" target="#b21">Yao et al. (2023)</ref> who study partially observed settings with multiple domains (datasets) or views (tuples of observations), respectively, each depending on a fixed subset of the latent variables.</p><p>In this work, we also focus on learning causal representations in such a partially observed setting, where not necessarily all causal variables are captured in any given observation. Our setting differs from prior work in two key aspects: (i) we consider learning from a dataset of unpaired partial observations; and (ii) we allow for instance-dependent partial observability patterns, meaning that each measurement depends on an unknown, varying (rather than fixed) subset of the underlying causal state. This setting is motivated by real-world applications in which we cannot at all times observe the complete state of the environment, e.g., because some objects are moving in and out of frame, or are occluded. As a motivating example, consider a stationary camera that takes pictures of a parking lot on different days as shown in Fig. <ref type="figure">1a</ref>. On different days, different cars are present in the parking lot, and the same car can be parked in different spots. Our task is to recover the position for each car that is present in a certain image.</p><formula xml:id="formula_0">C 1 C 2 C 3 C 4 X Z 4 Z 3 Z 2 Z 1 Y 4 Y 3 Y 2 Y 1 Ẑ 2 Figure 1: (a)</formula><p>Motivating example for the Unpaired Partial Observation setting: a stationary camera taking pictures of a car park. We consider x 1 the image on day 1 and x 2 the image on day 2. The latent causal variables c 1 and c 2 represent the positions of four cars on each day. In x 1 only Car2 and Car3 are visible, while in x 2 all cars except Car3 are visible. This is represented by the ones in the binary mask variables y 1 and y 2 . The combination of the values of the latent causal variables c and the masks y are the masked causal variables z, which used by the mixing function f to generate the images x. (b) Causal model of the setting, the dotted line variables are not directly observed, but they are measured only through the observation X. Our goal is to learn a representation Ẑ that identifies Z up to permutation and element-wise transformation.</p><p>In this setting, we only have one observation for a given state of the system (i.e., one image per day), and the subsets of causal variables that are measured in the observation (the parked cars), change dynamically across images. We highlight the following contributions:</p><p>• We formalize the Unpaired Partial Observations setting for CRL, where each partial observation captures only a subset of causal variables and the observations are unpaired, i.e., we do not have simultaneous partial observations of the same state of the system. • We introduce two theoretical results for identifying causal variables up to permutation and element-wise transformation under partial observability. Both results leverage a sparsity constraint. In particular, Thm. 3.1 proves identifiability for linear mixing function and without parametric assumptions on the underlying causal model. Thm. 3.4 proves identifiability for the piecewise linear mixing function, when the causal variables are Gaussian and we can group observations by their partial observability patterns. • Finally, we propose two methods that implement these theoretical results and validate their effectiveness with experiments on simulated data and image benchmarks, e.g. <ref type="bibr">Causal3DIdent (von Kügelgen et al., 2021)</ref>, that we modify to test our partial observability setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem setting</head><p>In this section, we formalize the Unpaired Partial Observation setting, in which we have a set of high-dimensional observations that are functions only of instance-dependent subsets of the true underlying causal variables. This setting consists of four sets of random variables: the causal variables C, the binary mask variables Y that represent if a variable is measured in a sample, the masked causal variables Z that combine the information from the causal variables and the masks, and the observations X. Our goal is to recover the masked causal variables Z up to permutation and element-wise transformation, solely from the observations X, despite the instance-dependent partial observability pattern. We show an example of a causal graph of the setting in This definition allows us to model also dependences between the components of Y and masking behavior that depends on the values of the underlying causal variables.</p><p>Masked causal variables Z. The masked causal variables Z = (Z 1 , ..., Z n ) ∈ Z are a combination of the causal variables and the masks, and they are the latent inputs to the mixing function f that we are trying to recover. In particular, they are the Hadamard product of the causal variables with the binary mask variable, i.e., Z = Y ⊙ C. This means that for sample i ∈ [N ] and any causal variable j ∈ [n], if the mask value y i j is 1, then the causal variable c i j is measured and z i j is c i j . Instead, if y i j = 0, then the causal variable is unmeasured and z i j takes a fixed masked value M j , which we will consider for simplicity to be 0. Note that this is not equivalent to do-interventions, since masking variables does not influence any downstream variables, as an intervention would, as explained in an example in App. A. Finally, we assume that for all s ∈ S, the probability measure P Zs|S=s has a density w.r.t. the Lebesgue measure on R |s| . Observations X. We assume that observations X ∈ X ⊆ R d are generated by mixing the masked causal variables Z with the same mixing function f : Z → X , i.e., X = f (Z). We refer to partitions of observations with the same unknown partial observability pattern, i.e., with the same unknown value of Y = y, as groups, and we assume that each observation</p><formula xml:id="formula_1">x i for i ∈ [N ] is part of a group g i ∈ G.</formula><p>Our goal is to identify the masked causal variables Z from a set of observations X. In CRL we usually cannot recover the exact value of the latent variables, but we can only identify them up to some transformation. Our results guarantee that each ground truth variable is represented by a single estimated variable up to a linear transformation. Similar notions of identifiability were used in previous works <ref type="bibr" target="#b8">(Comon, 1994;</ref><ref type="bibr">Khemakhem et al., 2020;</ref><ref type="bibr" target="#b21">Lachapelle et al., 2023)</ref>. Definition 2.1. The ground truth representation vector Z is said to be identified up to permutation and element-wise linear transformation by a learned representation vector Ẑ when there exists a permutation matrix P and an invertible diagonal matrix D such that Ẑ = PDZ almost surely.</p><p>To prove our results, we describe the sufficient support index variability, which was originally defined by This assumption avoids cases in which two variables are always missing at the same time, since then we would not be able to disentangle them from the observations. A trivial set S that satisfies this assumption is S = 1, 2, ..., n, which contains n distinct masks. We conjecture that if this assumption is not satisfied for all i, but only for blocks of variables, we would instead get block identifiability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Identifiability via a Sparsity Principle</head><p>In this section, we show how a simple sparsity constraint on the learned representations allows us to learn a disentangled representation of the ground truth variables. We first consider linear mixing functions and prove identifiability without any parametric assumption on the causal variables and while allowing for partial observability patterns that can depend on the value of the causal variables (Thm. 3.1).We then investigate if this sparsity principle can also identify variables for nonlinear mixing functions f . This is not the case in general, as we show in Example 3.1.</p><p>Our linear result hinges on the existence of a function g such that the composition of g and f is affine. Based on this intuition, we consider piecewise linear mixing functions f , since for Gaussian causal variables they can be composed with an appropriate g to obtain an affine function. In this setting, we prove that we can learn a disentangled representation of the latent variables (Def. 2.1) given that the masks are independent of the causal variables and given that we know the groups of the observations (Thm. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Linear Mixing Function</head><p>We show that for linear mixing functions under a perfect reconstruction (Eq. 1), a simple sparsity constraint on the learned representation (Eq. 2) allows us to learn a disentangled representation of the ground truth latent variables.</p><p>Theorem 3.1 (Element-wise Identifiability for Linear f ). Assume the observation X = f (Z) follows the data-generating process in Sec. 2, where f : Z → X is an injective linear function, and Ass. 2.2 holds. Let g : X → R n be an invertible linear function onto its image and let f : R n → R d be an invertible continuous function onto its image. If both of the following conditions hold,</p><formula xml:id="formula_2">E X -f (g(X)) 2 2 = 0 , and<label>(1)</label></formula><formula xml:id="formula_3">E ∥g(X)∥ 0 ≤ E ∥Z∥ 0 ,<label>(2)</label></formula><p>then Z is identified by f -1 (X) up to a permutation and element-wise linear transformations (Def. 2.1), i.e., f -1 • f is a permutation composed with element-wise invertible linear transformations on Z.</p><p>We provide a proof in App. B.1 and now give an intuitive explanation for why it holds. The zero reconstruction loss ensures that no information is lost in the encoding g(X), which implies that g(X) is not sparser than Z. Hence, incorporating Eq. ( <ref type="formula" target="#formula_3">2</ref>) as a constraint or regularization term in our methods enables our estimators to match the sparsity of the ground truth variables, which breaks indeterminacies due to rotations of the latent space.</p><p>The idea of using a sparsity constraint or regularization is similar to previous work <ref type="bibr" target="#b21">(Lachapelle et al., 2023)</ref> in the context of sparse multitask learning. In this paper we leverage these ideas in the distinct setting of partial observability. This result requires that the mixing function f , is injective, but not necessarily bijective. Notably, it does not require any parametric assumptions on the distribution of Z, thus allowing for causal relations or other statistical dependencies among the latent variables. Finally, this result also allows for mask variables that potentially depend on the values of the latent causal variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Is sparsity enough for identification for nonlinear f ?</head><p>Since linearity of f is a strong assumption that may not hold in many applications, an obvious question is whether we can extend this result to nonlinear mixing functions. Unfortunately, this is not the case without making further assumptions, as demonstrated by the following example.</p><p>Example 3.1. Consider C ∼ N (0, I 2 ), where I 2 is the identity matrix. Assume an independent mask Y with distribution p(Y = y) = 1/4 for any y ∈ {0, 1} 2 , satisfying Ass. 2.2. Let the nonlinear mixing function f :</p><formula xml:id="formula_4">R 2 → R 2 be f (z) := sinh(R π 4 z) + sinh(R -π 4 z) ,<label>(3)</label></formula><p>where R θ is a rotation matrix. Consider f and g to be the identity function, which trivially satisfy Eq. 1, since f • g(X) = X. We show in Appendix C that g satisfies the sparsity constraint (Eq. 2). However, despite satisfying all requirements of Thm 3.1 except for the linearity of f , we can show that each component of f -1 • f = f depends on both components of z; or in other words, the learned representation does not identify Z up to permutation and element-wise transformations. We refer to App. C for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Piecewise Linear Mixing Function</head><p>In light of Example 3.1, we consider the role of linearity in Thm. 3.1. We notice that it is a sufficient condition for ensuring that there exists a g such that g • f is affine on Z.</p><p>We then consider the question: even if f is not affine itself, can we consider a restricted class of f and latent variables Z such that the composition of f and an appropriate g is affine? As a first step we consider a piecewise linear f and assume that the causal variables are Gaussian and that the masks are independent from the causal variables. Assumption 3.2. We assume C follows a non-degenerate multivariate normal distribution, i.e. C ∼ N (µ, Σ), where µ ∈ R n and Σ ∈ R n×n is a positive definite matrix.</p><p>Assumption 3.3. We assume C and Y are independent from each other, i.e. the partial observability pattern does not depend on the values of the causal variables.</p><p>These assumptions represent a classical linear Gaussian Structural Causal Model setting for the causal variables and a missing-at-random assumption in terms of which variables are measured in the observation. On the other hand, in our setting, we do not directly observe the causal variables or the masks, but we only observe them mixed in an observation.</p><p>Under these assumptions, the conditional distribution of the masked causal variables Z given the binary mask vector Y is defined as a multivariate normal distribution:</p><formula xml:id="formula_5">Z | Y ∼ N (µ Y , Σ Y )</formula><p>where</p><formula xml:id="formula_6">µ Y = (µ 1 Y 1 , . . ., µ n Y n ), Σ Y(ij) = Σ ij Y i Y j</formula><p>This distribution is a degenerate multivariate normal (De-MVN), i.e., a normal with a singular covariance matrix, if at least one of the causal variables is masked by Y.</p><p>Intuitively, we can leverage the Gaussianity of Z|Y to enforce that the reconstructed Ẑ|Y = g(X) is also Gaussian. We now show that this allows us to identify the latent factors Z through our sparsity constraint on the learned representations (Eq. 5), given that we are able to partition the data according to the unknown value of Y = y, or in other words, given that we know the group g i for each observation</p><formula xml:id="formula_7">x i for i ∈ [N ]</formula><p>. The rationale of this requirement is that we do not need to know the value of the latent mask Y, but we do need to be able to separate observations that are generated by different distributions of Z, so we can effectively enforce the Gaussianity constraint (Eq. 6). Theorem 3.4 (Element-wise Identifiability for Piecewise Linear f ). Assume the observation X follows the datagenerating process in Sec 2, Ass. 2.2, 3.2 and 3.3 hold and f : Z → X is an injective continuous piecewise linear function. Let g : X → R n be a continuous invertible piecewise linear function and let f : R n → R d be a continuous invertible piecewise linear function onto its image. If all following conditions hold:</p><formula xml:id="formula_8">E X -f (g(X)) 2 2 = 0 ,<label>(4)</label></formula><formula xml:id="formula_9">E ∥g(X)∥ 0 ≤ E ∥Z∥ 0 and (5) g(X) | (Y = y) ∼ N (µ y , Σ y ) ∀y ∈ Y,<label>(6)</label></formula><p>for some µ y ∈ R n , Σ y ∈ R n×n , then Z is identified by f -1 (X), i.e., f -1 • f is a permutation composed with element-wise invertible linear transformations (Def. 2.1).</p><p>We provide the complete proof in App. B.2. We first provide some results for a weaker notion of identifiability: identifiability up to affine transformations (Def. B.4). To this end, we first extend a theorem by <ref type="bibr">Kivva et al. (2022)</ref> from the case of non-degenerate to potentially degenerate multivariate normal variables (Thm. B.5). Such variables are crucial in our setting because partial observability potentially introduces degenerate cases. The crux of our proof involves handling the case of degenerate variables that do not have probability density. We then show that given the information of the binary mask Y, we can identify the latent factors Z up to an affine function h Y (Lemma B.10). We then show that all of these affine functions can be represented by a single affine function v := f -1 (f (Z)) defined on Z.</p><p>Compared to linear case in Thm. 3.1, the additional constraint in this case is Gaussianity on both Z|Y and the estimator g(X)|Y. This ensures that the composition of two piecewise linear functions g and f remains affine on Z, extending the results from linear f to piecewise linear f .</p><p>Non-zero mask values. Our theoretical analysis assumes that we mask the unmeasured latent variables with a mask value of 0, i.e. when y i = 0 for i ∈ [N ], we set z i = 0.</p><p>One can wonder whether allowing to set z i = M when y i = 0 for some potentially nonzero constant vector M would make the model more expressive. It turns out that this is not the case, since the decoder f can always shift Z in arbitrary ways, making the specific value of M irrelevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation</head><p>We implement our two theoretical results asconstrained optimization problems in Cooper <ref type="bibr">(Gallego-Posada and Ramirez, 2022)</ref>. We approximate the sparsity constraint, i.e. Eq. 2 in Thm. 3.1 and Eq. 5 in Thm. 3.4, by replacing the L 0 norm with L 1 norm, which is differentiable except at zero. In practice, the L 1 norm of the ground truth variables Z is unknown, so we instead set a hyperparameter ϵ for the sparsity constraint E ∥g(X)∥ 1 ≤ ϵ. In our experiments, we use ϵ = 0.01 or 0.001, details are provided in App. D.2.</p><p>For linear f , we can reconstruct the latent variables directly from a dataset of observations {x i } i∈[N ] by minimizing the reconstruction error and adding the sparsity constraint (Thm 3.1). For piecewise linear f , Thm. 3.4 requires that we know how to partition the data with the same partial observability at training time, i.e., we have information about the group of each observation. At test time, we have already learned g, so we can instead use only the observations without the group information. To encourage the Gaussianity condition on g(X)|Y = y (Eq. 6) in Thm. 3.4, we add two regularization terms that push the estimated skewness of each latent variable in each group to be 0 and the estimated kurtosis to be 3, which are the values of these moments in the Gaussian distribution. We learn the encoder g ψ and the decoder fθ by solving the following optimization problem:</p><formula xml:id="formula_10">min (θ,ψ) 1 N i∈[N ] x i -fθ (g ψ (x i )) 2 2 + g∈G skew g (g ψ (x)) + kurt g (g ψ (x)) -3 subject to: 1 nN i∈[N ] g ψ (x i ) 1 ≤ ϵ,<label>(7)</label></formula><p>where skew g (g ψ (x)) and kurt g (g ψ (x)) are the estimated skewness and estimated kurtosis for group g ∈ G. We solve both problems with the extra-gradient version of Adam <ref type="bibr">(Gidel et al., 2018)</ref>. We provide all details in App. D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>We perform three sets of experiments, one with numerical data in Sec. 5.1 and two with image datasets, a dataset with multiple balls in Sec. 5.2 and PartialCausal3DIdent, a partially observable version of Causal3DIdent <ref type="bibr" target="#b16">(von Kügelgen et al., 2021)</ref> in Sec. 5.3. We provide all the code for our method and the experiments at <ref type="url" target="https://github.com/danrux/sparsity-crl">https://github.com/danrux/sparsity-crl</ref>.</p><p>Following previous work <ref type="bibr">(Hyvarinen et al., 2019;</ref><ref type="bibr">Khemakhem et al., 2020)</ref>, we report the mean coefficient of determination (MCC) to assess that the learned representations match the ground truth up to a permutation and element-wise linear transformations. This metric is based on the Pearson correlation matrix Corr n×n between the learned representations Ẑ = g(X) and ground truth masked causal variables Z. Since our results are up to permutation, we compute the MCC on the permutation π that maximizes the average of</p><formula xml:id="formula_11">|Corr i,π(i) | for each index of a ground truth variable i ∈ [n], i.e. MCC= 1 n max π∈perm([n]) n i=1 |Corr i,π(i) |.</formula><p>We denote the correlation matrix with the permutation π as Corr n×n π .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Numerical Experiments</head><p>We generate numerical data following Sec. 2. We investigate varying the partial observability patterns, the underlying causal model and the type of mixing function f . For the encoder g and the decoder f , we use 7-layer MLPs with <ref type="bibr">[10,</ref><ref type="bibr">50,</ref><ref type="bibr">50,</ref><ref type="bibr">50,</ref><ref type="bibr">50,</ref><ref type="bibr">10]</ref> × n units per layer, where n is the number of causal variables, with LeakyReLU activations for each layer except the final one in the piecewise linear case. We apply batch normalization to control the norm of g(X). For simplicity, for most experiments we set the output dimension of g to n. In an ablation in App. E.1.4 we set the output dimension of g to an overestimate of n, showing that our method learns to only use n dimensions and provides similar performance to the other experiments. In each setup, we average over 20 random seeds.</p><p>We experiment with different ratios of measured variables ρ = {1VAR, 50%, 75%}, where 1VAR is one measured variable, while 50% and 75% are the percentages of measured variables in each sample. Based on each ρ we predefine a set of K masks Y that satisfies Ass. 2.2. As discussed in Sec. 3, while the theoretical results assume a mask value of M = 0, the specific value used is inconsequential from the theoretical point of view. On the other hand, the optimization procedure improves for mask values that are out of distribution w.r.t. the unmasked distribution of each variable, since it is then easier to recover which variables are masked in each group. We investigate different masks values and consider M = µ + δ • σ, where µ, σ ∈ R n are the mean and standard deviation of Z and δ ∈ {0, 2, 3, 5, 10}.</p><p>We consider n = {3, 5, 10, 20, 40} causal variables C. In each experiment, we generate a random directed acyclic graph D from a random graph Erdös-Rényi (ER)-k model, where k ∈ {0, 1, 2, 3} and ER-k is a graph with n • k edges. In particular, ER-0 implies independent C and therefore Z.</p><p>Based on D, we consider three types of structure causal models (SCM): i) a linear Gaussian SCM where edge weights are sampled uniformly from [-2, -0.5] ∪ [0.5, 2] and we use standard Gaussian noises; ii) a linear exponential SCM, where we have a similar setup, but with exponential noises with scale 1; iii) a nonlinear SCM, where we simulate a nonlinear function with a linear layer, a sigmoid activation and a fully connected layer with 100 hidden units, where edge weights are sampled uniformly from [-2, -0.5] ∪ [0.5, 2] and we use standard Gaussian noises.</p><p>Results for linear mixing function (Thm. 3.1). We use a fully connected layer to model the linear mixing function f . We show the performance, measured in the average MCC over three random seeds in Table <ref type="table" target="#tab_1">1</ref>. In the first four rows, we investigate how the number of the latent causal variables influences the performance for a linear Gaussian SCM with an average degree k = 1 and a ratio of measured variables ρ = 50%. In this case, the method achieves excellent performances for smaller n, but these degrade as n increases. In the second group of rows, we consider the effect of k, the average degree in the causal graph in the linear Gaussian case for n = 10 causal variables and ρ = 50%. Also in this case, the performances are excellent for low k, including k = 0 which represents independent variables, but they degrade with higher k. The third and fourth group of rows show how the performance varies for different k for the linear exponential and nonlinear SCM, showing a similar performance. Finally, we show the results for varying ratio of measured variables ρ, showing a small degradation when we measure Results for piecewise linear mixing function (Thm. 3.4).</p><p>For the piecewise linear f , we use a m = {3, 10, 20}hidden-layer MLP with m-1 LeakyReLU (α = 0.2) activation functions and a final linear layer, to model the piecewise linear mixing function. The number for layers m in the f mixing function is a proxy for the complexity of the function f , a linear function has m = 0, while the higher the m the higher the non-linearity. Following <ref type="bibr">(Lachapelle et al., 2022)</ref>, the weight matrices are sampled from a standard Gaussian distribution and are orthogonalized by their columns to ensure f is injective. In this setting, we evaluate with a linear Gaussian SCM, since this satisfies Ass. 3.2 in Thm. 3.4.</p><p>We start by showing results in Fig. <ref type="figure">2a</ref> for the simple case of number of causal variables n = 3 and number of layers m = 3. Through optimization of Eq. ( <ref type="formula" target="#formula_10">7</ref>), our approach achieves good performances in these simple scenarios. However, in more complex cases, e.g. more latent variables or more complicated f , there is a decline in performance, as shown in Fig. <ref type="figure">2b-f</ref>. We hypothesize that the reason for this drop is that the estimated skewness and kurtosis cannot guarantee Gaussianity, which is crucial to ensure identifiability in Thm. 3.4. We show that this is empirically the case for n = 5 in Fig. <ref type="figure" target="#fig_3">6</ref> in App. D.3. We attribute this issue to the per-group sample variance used to calculate both sample skewness and kurtosis. We test this assumption and potential of our theoretical results, by comparing with an oracle that has access to the masks y i for each observation x i . This</p><formula xml:id="formula_12">DPDVNYDOXH 0&amp;&amp; Q P N RUDFOH RXUPHWKRG EQXPEHURIOD\HUVm 0&amp;&amp; Q = 2N RUDFOH RXUPHWKRG F(5k 0&amp;&amp; Q P = 2 RUDFOH RXUPHWKRG GUDWLRRIPHDVXUHGYDULDEOHV 0&amp;&amp; Q P = 2N RUDFOH RXUPHWKRG HPDVNYDOXH 0&amp;&amp; Q P N RUDFOH RXUPHWKRG IODWHQWVL]Hn 0&amp;&amp; P = 2N RUDFOH RXUPHWKRG</formula><p>Figure <ref type="figure">2</ref>: Results for different parameters in the piecewise linear numerical experiments. Our method implements Eq. 7 based on the group information. The oracle method implements the same loss, but with as additional information the mask y i , which it uses to assign a low variance to the masked variables in each sample for the skewness and kurtosis regularization terms. This method showcases the potential of our theoretical results with a stronger Gaussianity constraint. . information is used to set the group sample variance to a low value for the masked variables in each group. As shown in Fig. <ref type="figure">2b</ref>-f this is effective in improving the performances of our sparsity constraint.</p><p>We test the performance of our method and the oracle for various parameters in Fig. <ref type="figure">2</ref>. In particular, we see in Fig. <ref type="figure">2a</ref> and Fig. <ref type="figure">2e</ref> that when the distance δ between masked and unmasked variables increases, it enables a more distinct separation of Z, providing better identification results. Similar to the linear case, we see that an increase in complexity, e.g., in the number of causal variable n, as shown in Fig. <ref type="figure">2f</ref>, or the number of layers m, as shown in Fig. <ref type="figure">2b</ref>, lowers the performance. Similarly, as shown in Fig. <ref type="figure">2d</ref>, the performance drops when the ratio of measured variables ρ increases. Interestingly, the average degree of the causal graph k does not have an impact, as shown in Fig. <ref type="figure">2c</ref>. We provide more results and visualizations in App. E.1. In App. E.1.5 we show the results of applying standard causal discovery methods on the learned representations, assuming the causal Markov and faithfulness assumption, as well as causal sufficiency.Intuitively, the closer the learned representations are to the ground truth, the more accurate are the learned causal relations between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Image dataset: Multiple Balls</head><p>We create a new image dataset in which we render in a 2D space b moving balls, as shown in Fig. <ref type="figure" target="#fig_4">30</ref> in App. E.2. Our latent causal variables are the (x, y) position of each ball b, which we model as Gaussian. We consider two settings: i) a missing ball setting in which the balls can only move along the x-axis and they can move out of view, and ii) a masked position setting in which the balls can move freely inside the frame and each of their coordinates can be masked by being set to an unknown, but fixed, M value. Both of these settings showcase possible applications of our approach. The first setting is a simplified version of a setting with a fixed camera capturing a set of objects that might move out of view. The second setting is a simplified version of occlusion, in which an occluded object is not captured in the image, but it still interacts with other objects.</p><p>The missing ball setting represents the intuitive setting for partial observability, i.e. when an object is out of the frame or occluded. While we model each object with two causal variables, its x and y coordinates, our methods do not allow that masks for two variables are deterministically related (Ass. 2.2). Thus, we constrain the balls to move only on the x-axis, which we then identify from the observations. In order to test also the identifiability of each variable of the same object, we devise the masked position setting. In this setting, we can still use our method with a non-zero mask value for each variable, which in this case represents a specific value for one of the x or y coordinates of the ball. We generate datasets for both settings by varying the number of balls b = 2, 5, 8. We use a predefined set of K masks y that satisfies Ass. 2.2. For the missing ball setting, we generate the x-coordinates of each ball from a truncated normal distribution N (0.5, 0.1 2 ) with bounds (0.1, 0.9). For the masked position setting, we generate the x and y-coordinates</p><formula xml:id="formula_13">of each ball i ∈ [b] independently from a truncated 2-dimensional normal distribution N (µ i , Σ i )</formula><p>with bounds (0.1, 0.9) 2 , where µ i ∼ U nif (0.4, 0.6) 2 , and Σ i = ((0.01, 0.005)(0.005, 0.01)) for all groups. For both settings, we generate the masked causal variables as</p><formula xml:id="formula_14">(Z k ) K k=1 = y • C + (1 -y) • M ∈ R K×n</formula><p>, where M = 0 for the missed ball setting, and M = µ i + δ • σ i for the masked position setting. Instead of a fixed-size training dataset, we generate images online until convergence. We provide more details in App. E.2.</p><p>Results. As illustrated in Table <ref type="table" target="#tab_2">2</ref>, while the MCC decreases with an increase in the number of balls b, all MCCs remain above 0.90. The results are consistently higher in the missing ball setting, where there are b variables to reconstruct, while in the masked position setting there are 2b. Additionally, in the masked position setting, we have dependence between the x and y coordinates, which can make the problem more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Image dataset: PartialCausal3DIdent</head><p>We explore the capability of our method on a partial observability version of Causal3DIdent (von Kügelgen et al., 2021) that we create. Causal3DIdent collects images of 7 object classes rendered from 10 causal variables, including object color, object positions, spotlight positions, etc. Each latent variable is rescaled into an interval of [-1, 1]. The mixing function, i.e., the rendering process, is not piecewise linear (which violates the assumption in Thm. 3.4). We still evaluate our piecewise linear method, following the intuition that non-linear functions can be approximated up to an arbitrary precision by an adequate number of linear pieces.</p><p>Dataset generation. Since Causal3DIdent is fully observable, we sample from it to create PartialCausal3DIdent, a dataset in which some of the latent variables are masked to a predefined value. For each datum, we sample a latent vector C ∼ N (0, σ 2 I) of n = 10 independent causal variables. We apply the set of K predefined masks y ∈ {0, 1} K×n to get a set of masked latent variables</p><formula xml:id="formula_15">(Z k ) K k=1 = y • C + (1 -y) • M ∈ R K×n .</formula><p>We define the masked value as the maximum of the support area (which Results. We evaluate our method separately on each object class in PartialCausal3DIdent and show the results in Table <ref type="table" target="#tab_3">3</ref>. Although the performance fluctuates slightly across classes, our method achieves a high MCC over 80% for all classes, which verifies that our approach is empirically applicable even on highly nonlinear high dimensional data. We provide all details and ablation studies on δ in App. E.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Most closely related to our work are recent identifiability studies, which also explicitly learn causal representations in a partially observable setting. <ref type="bibr" target="#b21">Yao et al. (2023)</ref> consider learning from tuples of simultaneously observed views, which depend on different fixed (potentially overlapping) subsets of latents with modality-specific mixing functions, and prove identifiability results for different blocks of shared content variables <ref type="bibr" target="#b16">(von Kügelgen et al., 2021)</ref>. Compared to our setting, such paired multi-view data may be harder to obtain. <ref type="bibr" target="#b12">Sturma et al. (2023)</ref> study an unpaired, multi-domain setup, in which observations from each domain depend on a different fixed subset of latents, and show identifiability of the causal representation and graph in the fully linear case. This setting resembles our results for the piecewise linear case in Thm. 3.4, where we assume we have the group information for each observation, which can be seen as a single domain. On the other hand, for the linear case in Thm. 3.1, we do not need the group information, hence we also allow for mixtures of data from multiple domains.</p><p>Other works in an i.i.d. setting can be viewed as modelling partial observability implicitly by restricting the graph connecting latent and observed variables and establishing identifiability for linear <ref type="bibr" target="#b0">(Adams et al., 2021;</ref><ref type="bibr" target="#b7">Cai et al., 2019;</ref><ref type="bibr">Silva et al., 2006;</ref><ref type="bibr" target="#b19">Xie et al., 2020;</ref><ref type="bibr">2022)</ref> or discrete <ref type="bibr">(Kivva et al., 2021)</ref> settings. In our work, we consider the case in which either the causal model or the mixing are not linear and do not constrain the connectivity between Z and X.</p><p>Not considering partial observability, other works on CRL aim to also learn the causal graph from different types (hard/soft, single/multi-node) of interventions in linear <ref type="bibr" target="#b11">(Squires et al., 2023;</ref><ref type="bibr" target="#b4">Bing et al., 2023)</ref>, partially <ref type="bibr" target="#b6">(Buchholz et al., 2023;</ref><ref type="bibr">Ahuja et al., 2023a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b23">Zhang et al., 2023)</ref>, or fully nonlinear <ref type="bibr">(von Kügelgen et al., 2023;</ref><ref type="bibr" target="#b14">Varici et al., 2023)</ref> settings. We focus instead on only recovering the latent causal variables without access to interventional data.</p><p>Other works have also explored the piecewise linear setting for identifiability, including with the assumption of Gaussian causal variables. In particular, Thm. 3.4 resembles one of the identifiability results from Kivva et al. ( <ref type="formula">2022</ref>) which assumes Z is a mixture of non-degenerate Gaussians and f is a piecewise linear function. We note that, in Thm. 3.4, Z is also a mixture of Gaussians, where the "cluster index" corresponds to Y. However, this mixture contains components which are degenerate, in the sense that their covariances might be singular (this occur when Y i = 0 for some i). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions and Limitations</head><p>In this work, we focused on learning causal representations in the unpaired partial observability setting, i.e., when only an instance-dependent subset of causal variables are captured in the measurements. We first proved the identifiability with linear mixing functions f under a sparsity constraint. We then presented an example to illustrate why extending the results to nonlinear f is not possible without additional assumptions. We proved identifiability when f is piecewise linear, both the causal variables and the learned representations are Gaussian, and we know the group of each sample.</p><p>While our experiments validate our theoretical results, there are still several limitations. From the theoretical point of view, knowing the group of each sample might not always be possible, so extending our results beyond this limitation is an exciting direction. Additionally, piecewise linear functions are a limited class and there might be other classes of nonlinear functions for which our identifiability results could be extended. In particular, our results hinge on the linearity of the composition g and f on Z, which can be implied by other types of constraints on f , g and the causal variables. Finally, our Gaussianity constraint is empirically difficult to satisfy, as shown by the gap between the performances of our method and the oracle. This warrants further investigation in other ways to encourage Gaussianity of the learned representations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Relation between partial observability and do interventions</head><p>In our setting we consider two types of related variables: the causal variables C and the masked causal variables Z. The causal variables C = (C 1 , ..., C n ) represent the underlying causal system. The masked causal variables Z = (Z 1 , ..., Z n ) ∈ Z are a combination of the causal variables and the masks Y, and they are the latent inputs to the mixing function f that we are trying to recover. In particular, they are the Hadamard product of the causal variables with the binary mask variable, i.e., Z = Y ⊙ C. For sample i ∈ [N ] and any causal variable j ∈ [n], if the mask value y i j is 1, then the causal variable c i j is measured and z i j is c i j . Instead, if y i j = 0, then the causal variable is unmeasured and z i j takes a fixed masked value M j . We show an example in Fig. <ref type="figure">3a</ref>, where the binary mask Y 2 for causal variable C 2 is 0.</p><p>A superficially similar operation is a do-intervention <ref type="bibr">(Pearl, 2009)</ref>, in which we fix the value of a random variable to a specific value. For example, one could consider an intervention do(z i j = M j ), which forces the value of the variable z i j to M j . As shown in Fig. <ref type="figure">3</ref>, there are several differences between the masked variables and the intervened variables.</p><formula xml:id="formula_16">C 1 C 2 C 3 Z 3 M 2 Z 1 Y 3 Y 2 = 0 Y 1 z (b) C 1 M 2 C 3 c 1 c 3 (a) M 2 c 2 c 3 c 1 c y c | do(C 2 = M 2 ) c 1 c′ 3 0 1 1 M 2 C 1 C 3 C 2 c 2 c 3 c 1 c y 0 1 1 Z 1 / ⊥ ⊥ Z 3 After : do(C 2 = M 2 ) C 1 ⊥ ⊥ C 3 | do(C 2 = M 2 )</formula><p>Figure <ref type="figure">3</ref>: Comparison between (a) masking on C 2 , and (b) do intervention on C 2 . In the second case, there is an effect on C 3 , and the intervention cuts the link and hence the dependence between C 1 and C 3 .</p><p>Despite having the same fixed value for the masked or intervened variable, these two operations have different effects. In particular, masking variables does not influence any downstream variables, as an intervention would. For example, consider the case of three variables C 1 → C 2 → C 3 in Fig. <ref type="figure">3a</ref>. In this case, if we mask C 2 , then there will be no effect on the value of C 3 , and C 1 and C 3 will still be dependent. If we performed an intervention on C 2 , as in Fig. <ref type="figure">3b</ref>, then there would be a change in the value of C 3 , and C 1 and C 3 would be independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proofs</head><p>B.1. Proof of the results for linear mixing functions (Theorem 3.1)</p><p>We first introduce the definition of dependent inputs, which intuitively is the set of variables on which a reconstruction of a given variable depends.</p><p>Definition B.1. Let v : Z → R n be a function with variables z = (z 1 , ..., z n ) ∈ Z. For all i ∈ [n] consider N i to be the set of all variables on which v i depends, which we will call dependent inputs. Formally, we define the set of dependent inputs</p><formula xml:id="formula_17">N i ⊆ [n]</formula><p>as</p><formula xml:id="formula_18">N i := {j ∈ [n] | ∃(z j , z 0 -j ), (z ′ j , z 0 -j ) ∈ Z where z j ̸ = z ′ j s.t. v i (z 0 -j , z j ) ̸ = v i (z 0 -j , z ′ j )} ,<label>(8)</label></formula><p>where z 0 -j is any n -1 dimensional vector that represents all of the components of a vector z ∈ Z except for the index j.</p><p>This definition intuitively represents the indices of z for which the function v i is not constant, or in other words, the inputs on which v i depends. Note that if there is only one z j such that (z j , z 0 -j ) ∈ Z for an appropriate z 0 -j , then any function defined on Z is constant for that index, and hence N i does not contain j.</p><p>We now show a lemma that we will use to prove the theorem showing that for any diffeomorphism and any variable, there always exists a permutation that ensures that a variable is in the set of its dependent inputs. Intuitively, this ensures that for all variables, there always exists a permutation, such that the reconstruction of a given variable depends on the variable itself.</p><p>Lemma B.2 (Existence of permutation π s.t. i ∈ N π(i) ). Let v : Z → R n be a diffeomorphism onto its image with variables z = (z 1 , ..., z n ) ∈ Z. Assume there exists a point z 0 ∈ Z and a value ϵ &gt; 0, such that ∀i ∈ [n], e i ⊙ [-ϵ, ϵ) n + z 0 ⊂ Z, where e i is the standard basis for space R n for the i-th dimension, i.e. a n-dimensional vector in which all dimensions except i are 0, and the i-th dimension is 1. Then there exists a permutation π : [n] → [n] such that i ∈ N π(i) for all i, where N i is defined as in Def. B.1.</p><p>Proof. Since v is a diffeomorphism, its Jacobian Dv = { ∂vi ∂zj } i,j∈[n] is invertible everywhere, so it is invertible at z 0 ∈ Z. Since Dv(z 0 ) is invertible, we have that its determinant is non-zero, i.e.</p><formula xml:id="formula_19">det(Dv(z 0 )) := π∈Sn sign(π) n i=1 Dv(z 0 ) π(i),i ̸ = 0 ,<label>(9)</label></formula><p>where S n is the set of n-permutations. This equation implies that at least one term of the sum is non-zero, and that for that term, all of the terms in the product are non-zero, meaning:</p><formula xml:id="formula_20">∃π ∈ S n , ∀i ∈ [n], Dv(z 0 ) π(i),i ̸ = 0 . (<label>10</label></formula><formula xml:id="formula_21">)</formula><p>This means that, for all i ∈ [n],</p><formula xml:id="formula_22">∂v π(i) ∂zi (z 0 ) ̸ = 0, which implies that v π(i) is not constant for z i in z 0 . Then by definition of N i in Def. B.1, i ∈ N π(i) .</formula><p>As an intermediate step, we first prove an important lemma that shows that by enforcing sparsity of the transformed variables, the corresponding transformation is an element-wise linear function. Intuitively, these transformed variables will be the reconstructed masked latent variables Z.</p><p>Lemma B.3 (Element-wise Identifiability for Linear Transformation). Assume that the masked latent variables Z with support Z follow the data generating process in Sec. 2 and Ass. 2.2 holds. Let the function v : Z → R n be invertible and linear on Z , and</p><formula xml:id="formula_23">E ∥v(Z)∥ 0 ≤ E ∥Z∥ 0 . (<label>11</label></formula><formula xml:id="formula_24">)</formula><p>Then v is a permutation composed with an element-wise invertible linear transformation on Z.</p><p>Proof. We reuse the definition of the support indices S := {i ∈ [n] : Z i ̸ = 0} and analyze each side of inequality (11), starting with its right-hand side.</p><formula xml:id="formula_25">E||Z|| 0 = E n i=1 1(Z i ̸ = 0) (12) = s∈S p(s)E n i=1 1(Z i ̸ = 0) | S = s (13) = s∈S p(s) n i=1 E[1(Z i ̸ = 0) | S = s] (14) = s∈S p(s) n i=1 1(i ∈ s)<label>(15)</label></formula><p>Now analyzing the left hand side of (11), starting with similar steps as previously we get</p><formula xml:id="formula_26">E||v(Z)|| 0 = s∈S p(s) n i=1 E[1(v i (Z) ̸ = 0) | S = s] (16) = s∈S p(s) n i=1 P Z|S=s [v i (Z) ̸ = 0] (17) = s∈S p(s) n i=1 1 -P Z|S=s [v i (Z) = 0] . (<label>18</label></formula><formula xml:id="formula_27">)</formula><p>For v to be a permutation composed with an element-wise invertible linear transformation on Z , it is enough to show there exists a permutation π : [n] → [n] such that, for every i, N i = {π(i)}. To achieve this, we are going to first show that</p><formula xml:id="formula_28">P Z|S=s [v i (Z) = 0] = α i 1(N i ∩ s = ∅) ,<label>(19)</label></formula><p>where</p><formula xml:id="formula_29">α i ∈ {0, 1}. Since v i is linear on Z, we have that v i (Z) = w i • Z + c i for some w i ∈ R n . Furthermore, we show that in this case N i = {j ∈ [n] | w i j ̸ = 0}.</formula><p>In one direction, if w i j = 0, then v i is constant in dimension j, so w i j should be non-zero for j to be included in N i . For the other direction, if w i j ̸ = 0 there are two cases in which we could have that j ̸ ∈ N i : (i) if w i j Z j always cancels out with another w i k Z k , where both w i j , w i k ̸ = 0, (ii) if the variable Z j can only take a single value. We show that neither of these cases can happen because of Ass. 2.2. In particular, the first case cannot happen because two masks for two different variables cannot be the same and satisfy this assumption. The second case also cannot happen because of Ass. 2.2, since there needs to be a support index set in which i is masked and one in which it is not masked. Thus,</p><formula xml:id="formula_30">v i (Z) = w i • Z + c i = w i Ni • Z Ni + c i . Case 1: Suppose N i ∩ s = ∅. Then, P Z|S=s [v i (Z) = 0] = P Z|S=s [w i Ni • Z Ni + c i = 0] = P Z|S=s [w i Ni • 0 + c i = 0] = P Z|S=s [c i = 0] = α i .</formula><p>Note that the event P Z|S=s [c i = 0] is deterministically either true of false, hence α i ∈ {0, 1}.</p><p>Case 2: Suppose N i ∩ s ̸ = ∅. Thus, w i s ̸ = 0. Thus,</p><formula xml:id="formula_31">P Z|S=s [v i (Z) = 0] = P Z|S=s [w i • Z + c i = 0] = P Z|S=s [w i s • Z s + c i = 0] .</formula><p>Note that the event {Z s | w i s • Z s + c i = 0} corresponds to the kernel of the linear map w i s . We can thus infer its dimensionality via the rank-nullity theorem <ref type="bibr" target="#b9">(Friedberg et al., 2014)</ref> which states that rank(w i s ) + dim(Ker(w i s )) = dim(Dom(w i s )), where Ker() is nullity and Dom() is domain, which here implies that 1 + dim(Ker(w i s )) = |s|. We thus have dim({Z s | w i s • Z s + c i = 0}) = |s| -1. Since P Z|S=s has a density w.r.t. to the Lebesgue measure, we have that P Z|S=s [w i s • Z s + c i = 0] = 0 (since a density w.r.t. to Lebesgue cannot concentrate mass on a lower-dimensional linear subspace).</p><p>We thus have proved that indeed,</p><formula xml:id="formula_32">P Z|S=s [v i (Z) = 0] = α i 1(N i ∩ s = ∅).</formula><p>Putting ( <ref type="formula" target="#formula_23">11</ref>), ( <ref type="formula" target="#formula_25">15</ref>), ( <ref type="formula" target="#formula_26">18</ref>) and ( <ref type="formula" target="#formula_28">19</ref>) together, we obtain</p><formula xml:id="formula_33">s∈S p(s) n i=1 [(1 -α i ) + α i 1(N i ∩ s ̸ = ∅)] ≤ s∈S p(s) n i=1 1(i ∈ s) (20)</formula><p>We can now use Lemma B.2, because Ass. 2.2 implies the existence of the z 0 point, which in this case is (0, ..., 0). By using this lemma, we can show that there exists a permutation π such that, for all i ∈ [n], i ∈ N π(i) . We now permute the terms on the l.h.s. according to π and reorganize the terms as:</p><formula xml:id="formula_34">s∈S p(s) n i=1 [(1 -α π(i) ) + α π(i) 1(N π(i) ∩ s ̸ = ∅)] ≤ s∈S p(s) n i=1 1(i ∈ s) s∈S p(s) n i=1 [(1 -α π(i) ) + α π(i) 1(N π(i) ∩ s ̸ = ∅) -1(i ∈ s)] ≤ 0 (21) Note how, for all i, (1 -α π(i) ) + α π(i) 1(N π(i) ∩ s ̸ = ∅) -1(i ∈ s) ≥ 0, since whenever i ∈ s, we must have N π(i) ∩ s ̸ = ∅,</formula><p>because we chose a permutation such that i ∈ N π(i) . Note also that this is true irrespective of the value of α π(i) ∈ {0, 1}.</p><p>We then can notice that if i ̸ ∈ s, then α π(i) = 0. The function can have either value 0 or 1, but in any case not negative. Hence the inequality in ( <ref type="formula">21</ref>) is actually an equality and hence for all s ∈ S and all i ∈ [n],</p><p>(1</p><formula xml:id="formula_35">-α π(i) ) + α π(i) 1(N π(i) ∩ s ̸ = ∅) -1(i ∈ s) = 0 . (22)</formula><p>The first thing we conclude is that if i ̸ ∈ s, then α π(i) = 1, since otherwise Equ.( <ref type="formula">22</ref>) is violated. Under Ass. 2.2, we have that, for all i ∈ [n], there exists an s ∈ S such that i ̸ ∈ s. We thus conclude that α i = 1 for all i ∈ [n], which allows us to write</p><formula xml:id="formula_36">1(N π(i) ∩ s ̸ = ∅) = 1(i ∈ s) (23) 1(N π(i) ∩ s = ∅) = 1(i ̸ ∈ s) . (24) Importantly, this means ∀i ∈ [n], ∀s ∈ S, i ̸ ∈ s =⇒ N π(i) ∩ s = ∅ =⇒ N π(i) ⊆ s c , (<label>25</label></formula><formula xml:id="formula_37">)</formula><p>which can be rewritten as</p><formula xml:id="formula_38">∀i ∈ [n], N π(i) ⊆ s∈S|i̸ ∈s s c . (<label>26</label></formula><formula xml:id="formula_39">)</formula><p>We now rewrite Assumption 2.2 below and take the complement on both sides:</p><formula xml:id="formula_40">∀i ∈ [n], s∈S|i̸ ∈s s = [n] \ {i} (27) s∈S|i̸ ∈s s c = {i}<label>(28)</label></formula><p>Combining ( <ref type="formula" target="#formula_38">26</ref>) with ( <ref type="formula" target="#formula_40">28</ref>) implies that N π(i) = {i} for all i, which concludes the proof.</p><p>We now prove identifiability up to permutation and element-wise linear transformations for the case of a linear mixing function, given the assumption of sufficient support index variability.</p><p>Theorem 3.1 (Element-wise Identifiability for Linear f ). Assume the observation X = f (Z) follows the data-generating process in Sec. 2, where f : Z → X is an injective linear function, and Ass. 2.2 holds. Let g : X → R n be an invertible linear function onto its image and let f : R n → R d be an invertible continuous function onto its image. If both of the following conditions hold,</p><formula xml:id="formula_41">E X -f (g(X)) 2 2 = 0 , and<label>(1)</label></formula><formula xml:id="formula_42">E ∥g(X)∥ 0 ≤ E ∥Z∥ 0 , (<label>2</label></formula><formula xml:id="formula_43">)</formula><p>then Z is identified by f -1 (X) up to a permutation and element-wise linear transformations (Def. 2.1), i.e., f -1 • f is a permutation composed with element-wise invertible linear transformations on Z.</p><p>Proof. Since X = f (Z), we can rewrite Equation (49) (perfect reconstruction) as</p><formula xml:id="formula_44">E||f (Z) -f (g(f (Z)))|| 2 2 = 0 . (<label>29</label></formula><formula xml:id="formula_45">)</formula><p>This means f and f • g • f are equal P Z -almost everywhere. Both of these functions are continuous, f by assumption and f • g • f because f is continuous, and f , g are linear. Since they are continuous and equal P Z -almost everywhere, this means that they must be equal over the support of Z, Z, i.e.,</p><formula xml:id="formula_46">f (z) = f • g • f (z) , ∀z ∈ Z . (<label>30</label></formula><formula xml:id="formula_47">)</formula><p>This can be easily shown by considering any point z ′ ∈ Z on which f and f</p><formula xml:id="formula_48">• g • f are different, i.e. f • g • f (z ′ ) ̸ = f (z ′ ), This would imply that (f -f • g • f )</formula><p>, which is also a continuous function, is non-zero in z ′ , and in its neighbourhood. This would contradict the assumption that f and f • g • f are the same almost everywhere. We can now apply the inverseon both sides to obtain</p><formula xml:id="formula_49">f -1 • f (z) = g • f v:= (z) , ∀z ∈ Z . (31)</formula><p>Since both f is an injective linear function from Z to X and g is an invertible linear function from X onto its image, then, f -1 • f is an invertible linear function on Z. As v is equal to f -1 • f on Z, then, we have v is also an invertible linear function on Z. By Lemma B.3, we can derive v is a permutation composed with an element-wise linear transformation on Z.</p><p>B.2. Proof of the results for the piecewise linear functions( Theorem 3.4)</p><p>For piecewise linear mixing functions, we first prove an intermediate result (Thm B.5) for a weaker form of identifiability that does not imply a disentanglement, but an affine correspondence between the ground truth and the learned variables.</p><p>Definition B.4 (Identifiability up to affine transformation <ref type="bibr">(Khemakhem et al., 2020;</ref><ref type="bibr">Lachapelle et al., 2022)</ref>). The ground truth representation vector Z (n-dimensional random vector) is identified up to affine transformations by a learned representation vector Ẑ (also n-dimensional random vector) when there exists an invertible linear transformation h such that, Ẑ = h(Z) almost surely.</p><p>We then introduce Lemma B.10, which proves that, when Y is provided, we can identify the distribution of latent variables by linear transformations that may vary across Y. It is crucial to emphasize that in this work, having Y = y does not imply knowledge of the exact value of y; instead, we simply need the information on grouping, i.e. the partitioning of the dataset based on mask values. Finally, we use these intermediate results to conclude the prove of the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1. LINEAR IDENTIFIABILITY FOR (DE)-MVNS WITH PIECEWISE AFFINE f</head><p>In what follows, we let ≡ denote equality in distribution.</p><p>Theorem B.5 (Linear Identifiability for (De)-MVNs with Piecewise Affine f ). Assume f , f : R n → R d are injective and piecewise affine. We assume Z and Ẑ follow a (degenerate) multivariate normal distribution. If f (Z) ≡ f ( Ẑ), then there exists an invertible affine transformation h : R n → R n such that h(Z) ≡ Ẑ (Def. B.4).</p><p>Intuitively, Thm B.5 states that if we can align two (De)-MVN-distributed random vectors to the same distribution through piecewise affine transformations, then their distributions can be interchanged via an affine transformation. This finding is initially inspired by <ref type="bibr">Kivva et al. (2022)</ref>, who established the linear identifiability for latent variables following nondenegerate Multivariate Normal (MVN) distributions. Our primary focus revolves around addressing two key issues to extend the findings from MVN to de-MVN: i) determining the identifiability of de-MVN distributions, which do not have a probability density function, and ii) constructing an affine function when the domain changes from R n to Z.</p><p>We start by proving an useful lemma about (degenerate) multivariate normal distributions.</p><p>Lemma B.6. (Degenerate) Multivariate Normals, or (De)MVNs, are close under affine transformation. More formally, if Z ∼ N (µ, Σ) with µ ∈ R and |Σ| ≥ 0, is a potentially degenerate multivariate normal variable, then AZ, where A ∈ R n×n is also a potentially degenerate multivariate normal variable.</p><p>Proof. Let Ẑ = AZ, then P Ẑ = AN (µ, Σ) = N (Aµ, AΣA T ) where the determinant of the covariance, |AΣA T | ≥ 0. Therefore, Ẑ is a potentially degenerate multivariate normal variable.</p><p>We now summarize the results on the identifiability of non-degenerate multivariate normal variables by <ref type="bibr">Kivva et al. (2022)</ref>.</p><p>We report an adapted version of Theorem C. </p><formula xml:id="formula_50">P ′ = N (µ ′ , Σ ′ ),<label>(32)</label></formula><p>Proof. Let the rank of Σ be k ≤ n and consider the spectral decomposition of Σ:</p><formula xml:id="formula_51">Σ = QDQ T ,<label>(38)</label></formula><p>where Q is an orthogonal n × n matrix and D a the diagonal matrix. If the rank k &lt; n, i.e. X is a degenerate multivariate normal, we consider D to have n diagonal entries σ 2 1 , σ 2 2 , ..., σ 2 k , 0, ..., 0, where</p><formula xml:id="formula_52">σ i for i ∈ [k] are the eigenvalues. Let Y = Q T X and Y ′ = Q T X ′ . This means that Y ∼ N (Q T µ, Q T QDQ T Q) = N (Q T µ, D)<label>(39)</label></formula><formula xml:id="formula_53">Y ′ ∼ N (Q T µ ′ , Q T Σ ′ Q) (40) Since we know X ≡ X ′ in B(x 0 , δ), then we can derive that Y ≡ Y ′ in B(Q T x 0 , δ) for an appropriate δ &gt; 0. We project B(Q T x 0 , δ) into two subspaces, B(Q T x 0 , δ) 1:k and B(Q T x 0 , δ) k+1:n .</formula><p>The first captures the first k dimensions of the ball, and the second the last (nk) dimensions.</p><p>We can pick the first k dimensions of Y and Y ′ , and denote them as Y 1:k and Y ′ 1:k respectively. The first k dimensions of both variables are still the same, i.e., Y</p><formula xml:id="formula_54">1:k ≡ Y ′ 1:k in B(Q T x 0 , δ) 1:k . We can show that Y 1:k is a non-degenerate multivariate normal, because its covariance matrix D 1:k,1:k is full rank. So by Lemma B.8 we have Y 1:k ≡ Y ′ 1:k , i.e. (Q T Σ ′ Q) 1:k,1:k = D 1:k,1:k .</formula><p>For the other (nk) dimensions of Y and Y ′ , i.e., Y k+1:n and Y ′ k+1:n , we can also show that</p><formula xml:id="formula_55">Y k+1:n ≡ Y ′ k+1:n in B(Q T x 0 , δ) k+1:n . For Y k+1:n , since x 0 is contained in B(x 0 , δ), we can derive that Q T x 0 is contained in B(Q T x 0 , δ).</formula><p>Since the covariance matrix of Y k+1:n is D k+1:n,k+1:n , which is a zero matrix, the distribution of Y k+1:n is a point mass with all of the probability on a single value (Q T x 0 ) k+1:n . From (39), we know that</p><formula xml:id="formula_56">Y k+1:n ∼ N ((Q T µ) k+1:n , D k+1:n,k+1:n ) = N ((Q T µ) k+1:n , 0), so we can derive (Q T µ) k+1:n = (Q T x 0 ) k+1:n . Since Y k+1:n ≡ Y ′ k+1:n in B(Q T x 0 , δ) k+1:n , and Y k+1:n is a point mass on (Q T µ) k+1:n ∈ B(Q T x 0 , δ) k+1</formula><p>:n , then we can derive that Y ′ k+1:n should be a point mass on the same point (Q T µ) k+1:n . Therefore, (Q T Σ ′ Q) k+1:n,k+1:n is a zero matrix, which is equal to D k+1:n,k+1:n , and (Q T µ) k+1:n = (Q T µ ′ ) k+1:n . This means that Y k+1:n ≡ Y ′ k+1:n . We can now exploit that X = QY and X ′ = QY ′ due to the orthogonality of Q, and we can write:</p><formula xml:id="formula_57">X = QY = Q(Y 1:k T , Y k+1:n T )<label>(41)</label></formula><formula xml:id="formula_58">X ′ = QY ′ = Q(Y ′ 1:k T , Y ′ k+1:n T ),<label>(42)</label></formula><p>which together with</p><formula xml:id="formula_59">Y 1:k ≡ Y ′ 1:k and Y k+1:n ≡ Y ′ k+1:n implies that X ≡ X ′ .</formula><p>Theorem B.5 (Linear Identifiability for (De)-MVNs with Piecewise Affine f ). Assume f , f : R n → R d are injective and piecewise affine. We assume Z and Ẑ follow a (degenerate) multivariate normal distribution. If f (Z) ≡ f ( Ẑ), then there exists an invertible affine transformation h : R n → R n such that h(Z) ≡ Ẑ (Def. B.4).</p><p>Proof. By the definition of (De) MVN, we can write Z = Aε + b, where ε ∼ N (0, I) with dimension d ε ≤ n and support R dε , and A ∈ R n×dε has full column rank. The support of Z, Z ⊆ R n , is Z = colsp(A) + b, where colsp(A) is the column space of matrix A.</p><p>Since f (Z) and f ( Ẑ) are equally distributed, they have the same image space. Therefore, we can define</p><formula xml:id="formula_60">h 0 := f -1 • f on Z, which is a piecewise affine function. Since we assumed that f (Z) is equally distributed to f ( Ẑ), then Ẑ ≡ f -1 • f (Z) = h 0 (Z).</formula><p>We can now define another function that takes as domain the whole space R n :</p><formula xml:id="formula_61">h(x) := h 0 (A(A T A) -1 A T (x -b) + b),<label>(43)</label></formula><p>where x ∈ R n . This function is the same as h 0 on Z We first proof that h and h 0 agree on Z. In particular, ∀z 0 ∈ Z, ∃ε 0 ∈ R dε , s.t. z 0 = Aε 0 + b. Thus, by definition of h, we have ∀z 0 ∈ Z,</p><formula xml:id="formula_62">h(z 0 ) = h 0 (A(A T A) -1 A T (z 0 -b) + b) (44) = h 0 (A(A T A) -1 A T (Aε 0 + b -b) + b) (45) = h 0 (A (A T A) -1 A T A =I ε 0 + b) (46) = h 0 (Aε 0 + b) (47) = h 0 (z 0 ).<label>(48)</label></formula><p>So we have h 0 and h agree on Z. This implies that Ẑ ≡ h(Z).</p><p>Then, we will complete the proof by two steps:</p><p>i There exists a z 0 ∈ Z and δ &gt; 0 s.t. h is affine on the ball B(z 0 , δ) ⊂ R n (potentially not completely contained in Z). ii We can define an affine function h on Z such that h and h agree on the ball B. Then we show h(Z) ≡ Ẑ.</p><p>(i). We now first show that ∃z 0 ∈ Z, s.t. h is differentiable at point z 0 .</p><p>As shown in Eq. 47, h(z 0 ) can be written as the composition of the functions h 0 (Aε 0 + b) and</p><formula xml:id="formula_63">(A T A) -1 A T (z 0 -b).</formula><p>The function h 0 (Aε 0 + b) is piecewise affine, because it is a composition of (piecewise) affine maps. Hence, there must exists a point ϵ 0 ∈ R dϵ s.t. h 0 (Aε + b) is differentiable at that point. Moreover, we have that (A T A) -1 A T (xb) is differentiable at z 0 := Aε 0 + b, because it is an affine function, and (A T A) -1 A T (xb) evaluated at z 0 yields ε 0 , so the composition h is differentiable at z 0 . Since h is piecewise affine and differentiable in z 0 , then we can construct a ball B(z 0 , δ) (not necessarily completely contained in Z) which contains one single linear piece of h.</p><p>(ii) Let h : R n → R n be an invertible affine function such that h coincides with h on B(z 0 , δ). This means h(Z) and h(Z) coincide on B(z 0 , δ) ∩ Z. Since we have shown that h(Z) ≡ Ẑ are equal in distribution on Z, then h</p><formula xml:id="formula_64">(Z) = h(Z) ≡ Ẑ on B(z 0 , δ) ∩ Z.</formula><p>Since we assume Z is a (De-)MVN, by Lemma B.6, h(Z) is a (De-)MVN as well, because h is affine. Moreover, we know Ẑ is a (De-)MVN by assumption. We leverage the fact that they are equal on the intersection of the ball B(z 0 , δ) and the support of Z and use Lemma B.9 to prove that h(Z) ≡ Ẑ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.2. LINEAR IDENTIFIABILITY GIVEN Y = y FOR PIECEWISE LINEAR f</head><p>We now show that given the information of the binary mask Y, we can identify the latent factors Z up to an affine transformation (Def. B.4). It is crucial to emphasize that in this paper, having Y = y does not imply knowledge of the exact value of y; instead, it simply needs the information on grouping, i.e. the partitioning of the dataset based on mask values. Lemma B.10 (Linear Identifiability given Y = y for Piecewise Linear f ). Assume the observation X = f (Z) follows the data-generating process in Sec. 2, Ass. 3.2 and3.3 hold, and f : Z → X is an injective piecewise linear function. Let g : X → R n be a continuous piecewise linear function and f : R n → R d be an injective piecewise linear function. If both following conditions hold,</p><formula xml:id="formula_65">E X -f (g(X)) 2 2 = 0 , and<label>(49)</label></formula><formula xml:id="formula_66">g(X) | (Y = y) ∼ N (µ y , Σ y ),<label>(50)</label></formula><p>for some Proof. We start by introducing some additional notation.</p><formula xml:id="formula_67">µ y ∈ R n , Σ y ∈ R n×n , then Z | (Y = y) is identified by f -1 (X) | (Y =</formula><p>Definition B.11. Let v : Z → R n be a function with variables z = (z 1 , ..., z n ) ∈ Z. Let y ∈ [0, 1] n be a n-dimensional binary mask. We define the sub-support space as</p><formula xml:id="formula_68">Z y = {z ∈ Z : 1(z i = 0) ≥ 1(y i = 0), ∀i ∈ [n]}.<label>(51)</label></formula><p>By Lemma B.12, we can derive the function</p><formula xml:id="formula_69">(A T A) -1 A T [f (Aε + b) -b]</formula><p>is affine over R dε . Therefore, there exists M ∈ R dε×dε , and c ∈ R dε such that for every ε 0 ∈ R dε :</p><formula xml:id="formula_70">(A T A) -1 A T [f (Aε 0 + b) -b] = Mε 0 + c (62) A(A T A) -1 A T [f (Aε 0 + b) -b] = A(Mε 0 + c)<label>(63)</label></formula><p>Since A(A T A) -1 A T is a projection and its image space is colsp(A), then, it is the identity operator I on colsp(A).</p><p>Since f (Z) ≡ Z, then, the support of f (Z) should be the same as the support of Z, which is colsp(A) + b. This implies f (Aε 0 + b)b ∈ colsp(A). Then, we have</p><formula xml:id="formula_71">f (Aε 0 + b) -b = A(Mε 0 + c) (64) f (Aε 0 + b) = A(Mε 0 + c) + b. (<label>65</label></formula><formula xml:id="formula_72">)</formula><p>By definition, ε 0 = (A T A) -1 A T (z 0b) for every z 0 ∈ Z. We substitute this into Equation <ref type="formula" target="#formula_71">65</ref>, and we get</p><formula xml:id="formula_73">f (z 0 ) = A[M(A T A) -1 A T (z 0 -b) + c] + b.<label>(66)</label></formula><p>Hence, we can conclude f is a linear function over Z.</p><p>With these results, we can now prove the following Theorem 3.4. Theorem 3.4 (Element-wise Identifiability for Piecewise Linear f ). Assume the observation X follows the data-generating process in Sec 2, Ass. 2.2, 3.2 and 3.3 hold and f : Z → X is an injective continuous piecewise linear function. Let g : X → R n be a continuous invertible piecewise linear function and let f : R n → R d be a continuous invertible piecewise linear function onto its image. If all following conditions hold:</p><formula xml:id="formula_74">E X -f (g(X)) 2 2 = 0 ,<label>(4)</label></formula><p>E ∥g(X)∥ 0 ≤ E ∥Z∥ 0 and (5)</p><formula xml:id="formula_75">g(X) | (Y = y) ∼ N (µ y , Σ y ) ∀y ∈ Y,<label>(6)</label></formula><p>for some µ y ∈ R n , Σ y ∈ R n×n , then Z is identified by f -1 (X), i.e., f -1 • f is a permutation composed with element-wise invertible linear transformations (Def. 2.1).</p><p>Proof. Since X = f (Z), we can rewrite Equation (4) (perfect reconstruction) as</p><formula xml:id="formula_76">E||f (Z) -f (g(f (Z)))|| 2 2 = 0 .<label>(67)</label></formula><p>This means f and f • g • f are equal P Z -almost everywhere.</p><p>Since f , g and f are continuous, we can derive f and f • g • f must be equal over the support of Z, Z, i.e.,</p><formula xml:id="formula_77">f (z) = f • g • f (z) , ∀z ∈ Z . (<label>68</label></formula><formula xml:id="formula_78">)</formula><p>We denote v := g • f : R n → R n , an invertible continuous piecewise affine function. Take the left inverse of f on both sides, then we have,</p><formula xml:id="formula_79">f -1 • f (z) = v(z) , ∀z ∈ Z .<label>(69)</label></formula><p>From Lemma B.10, we know that for all y ∈ Y, given the mask Y = y, there exists an invertible affine transformation h y : R n → R n such that v(Z) ≡ h y (Z) over Z y .</p><p>Since we know Z|Y = y follows a (De)MVN and h y is affine, by Lemma B.6, we can derive that h y (Z) follows a (De-)MVN distribution as well. Then, we can rewrite v(Z) ≡ h y (Z) in distribution as</p><formula xml:id="formula_80">h y (Z) ≡ v • h y -1 • h y (Z).<label>(70)</label></formula><p>Since h y is affine, its inverse is continuous. In addition, v is continuous as it is the composition of continuous functions f and g. Therefore, v • h y -1 is continuous piecewise affine. By using Lemma B.13, we get that v • h y -1 is affine over h y (Z y ), i.e. there exists an invertible affine map h 1 such that</p><formula xml:id="formula_81">v • h y -1 (z) = h 1 (z) ⇐⇒ v(z) = h 1 • h y (z) ∀z ∈ Z y . (<label>71</label></formula><formula xml:id="formula_82">)</formula><p>Since v is a composition of affine maps, it is also affine on Z y , ∀y ∈ Y.</p><p>We define Z s := n i=1 (R • 1{i ∈ s}), which is the space of the supports for each dimension given a value of the support index s. For the dimensions which are included in s i , this is R, while for the others it is 0. By this definition and by the sufficient variability assumption Ass. 2.2, Z = s∈S Z s .</p><p>Since S is a finite set, which implies countable, we can find a way to order the elements in S, denoted as {s 1 , ..., s |S| }.</p><p>Thus, Z = |S| i=1 Z si . While we have already proven that v is affine over each subspace Z si , we now show that v is a linear function on Z, i.e. v(z) = wz, ∀z ∈ Z, where w ∈ R n×n .</p><p>i We first consider two index sets s 1 and s 2 . Without loss of generality, we assume the index in s 1 is from 1 to |s 1 |, and the index in s 2 is from m to m + |s 2 |, where m ∈ {1, 2, ..., n -|s 2 | + 1}. Since v is affine on Z s1 and Z s2 individually, we have </p><formula xml:id="formula_83">v(z) = z 1 a 1 + ... + z |s1| a |s1| + c 1 ∀z ∈ Z s1 (72) v(z) = z m b m + ... + z |s2|+m b |s2|+m + c 2 ∀z ∈ Z s2 . (<label>73</label></formula><formula xml:id="formula_84">) Since Z s1 ∩ Z s2 = {0}, then we can get c 1 = c 2 . Case 1. s 1 ∩ s 2 = ∅. Then, we have ∀z ∈ Z s1 ∪ Z s2 v(z) = z 1 a 1 + ... + z |s1| a |s1| + z m b m + ... + z |s2|+m b |s2|+m + c 1<label>(74)</label></formula><formula xml:id="formula_85">v(z) = z m a m + ... + z m+t a m+t + c 1 (76) v(z) = z m b m + ... + z m+t b m+t + c 1 .<label>(77)</label></formula><p>This implies a i = b i , i = m, ..., m + t. Therefore, we can derive ∀z ∈ Z s1 ∪ Z s2 ,</p><formula xml:id="formula_86">v(z) = z 1 a 1 + ... + z |s1| a |s1| + z m+t+1 b m+t+1 + ... + z |s2|+m b |s2|+m + c 1 .<label>(78)</label></formula><p>By considering both cases, we can now proof that v is a linear function on Z s1 ∪ Z s2 . ii We can iterate this strategy by iteratively adding new Z si to the union, until Z |S| . Finally, we have that v is a linear function on s∈S Z s = Z.</p><p>Then, the rest proof immediately follows from Lemma B.3, where we have proven element-wise identifiability for the linear transformation.</p><p>C. Example: sparsity is not enough for identifiability in the non-linear case.</p><p>In Theorem 3.1 we prove that we can achieve element-wise identifiability for an invertible linear mixing function f in the Partially Observable Causal Representation Learning setting, assuming sufficient support index variability (Ass. 2.2) and under the condition of perfect reconstruction and a sparsity principle. The cold color scheme corresponds to the level curves of ( f -1 • f ) 1 (z) while the warm color scheme corresponds to ( f -1 • f ) 2 (z). The example gives a concrete case where all assumptions of Theorem 3.1 hold except for the linearity of f . We can see that f -1 • f is not a permutation composed with an element-wise invertible transformation, since along the vertical dashed line, we can see that both components of f -1 • f change.</p><p>An obvious extension of this result might be considering the identifiability for non-linear mixing functions. In this section we show a counter-example that describes why this is not possible in general without any further assumption (e.g. assuming both a piecewise linear mixing function and a Gaussian causal model, as shown by our results in Thm. 3.4).</p><p>Example C.1. Assume we have 2 latent and 2 observed variables, i.e. n = 2 and d = 2. Furthermore, assume that the domain of the causal variables C = R 2 and that Ass. 3.2 and 3.3 hold. For example, consider C ∼ N (0, I 2 ) with an independent mask Y with distribution p(Y = y) = 1/4 for any y ∈ {0, 1} 2 , which trivially satisfies Ass. 3.2 and 3.3. In this case, the support of</p><formula xml:id="formula_87">Z = Y ⊙ C is Z = R 2 . Assume further that f : R 2 → R 2 is defined by f (z) := sinh(R π 4 z) + sinh(R -π 4 z) ,<label>(79)</label></formula><p>where sinh(x) := e x -e -x 2 (applied element-wise above) and R θ is a rotation matrix defined by</p><formula xml:id="formula_88">R θ := cos θ -sin θ sin θ cos θ . (<label>80</label></formula><formula xml:id="formula_89">)</formula><p>Consider f and g to be the identity function. We now show that all the assumptions of Theorem 3.1 except the linearity of f are satisfied. First, notice how</p><formula xml:id="formula_90">E||X -f (g(X))|| 2 = E||X -X|| 2 = 0 .<label>(81)</label></formula><p>Since X = f (Z) by definition and g(X) = X, since g is the identity function, then f (Z) = g(X) and thus E||g(X)|| 0 = E||f (Z)|| 0 .</p><p>We now show that E||f (Z)|| 0 ≤ E||Z|| 0 , which we will then use to prove the sparsity condition on E||g(X)|| 0 .</p><p>(87)</p><p>This counter-example shows that the results of Theorem 3.1 do not apply in general if f is nonlinear, but only with additional assumptions, as shown in Theorem 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation details</head><p>This section provides further details about the experiment implementation in Section 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Sensitivity analysis on ϵ</head><p>We conduct a sensitivity analysis on ϵ. As shown in Figure <ref type="figure">5</ref>, we observe that the linear case exhibits heightened sensitivity ϵ, whereas the piecewise linear case does not display such sensitivity. We attribute this difference to the additional group and mask information given in the training phase for piecewise linear case, enhancing the method's robustness to variations in this hyperparameter. Figure <ref type="figure">5</ref>: Sensitivity analysis on ϵ varies from 1e -4 to 1e4. The left graph is for the linear case, and right-hand side is for the piecewise linear case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Oracle method</head><p>In Sec. 4, to encourage Gaussianity of learned representations, we add a regularization term to force the sample skewness and kurtosis to match the Gaussian distribution. However, as we mention in Sec. 4, estimated skewness and kurtosis cannot guarantee Gaussianity. In Figure <ref type="figure" target="#fig_3">6</ref>, we empirically show even by adding the two penalty terms into the loss function, the estimator we obtain is still highly non-gaussian.</p><p>In the Oracle method, we adopt the assumption of knowing masks in train phases. Instead of directly estimating the unmeasured part as a constant value, we provide less information by replacing g ψ (x i ) with a low log standard deviation -10 in Eq. 7; for the measured latent variables, we assign 1 to g ψ (x i ). After training, we obtain the encoder function g. In the test phase, data in distinct groups can be mixed together. There is no longer a requirement for mask-related information.</p><p>In addition, we find empirically that if we replace μg with a constant value, e.g., 2, it also enhances the performance for most setups except for the case when δ = 0. When δ = 0, set the constant value to be 0 can obtain better results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Predefined masks</head><p>In order to satisfy Ass. 2.2, i.e. for each latent variable C i i ∈ [n], all the other latent variables should be measured in at least one of the groups where C i is unmeasured, the minimum number of distinct masks is equal to the latent size n, and the maximum number is up to 2 n . In this paper, we consider 3 different strategies to generate masks. i Consider all 2 n possible masks as a mask set Y. For each data c i , i ∈ [N ], we uniformly sample one mask y i from the mask set Y. This strategy is used in numerical experiments with linear f since we do not need group-wise masks. ii Define kn possible masks {y 1 , .., y n } with the same ratio of measured variables ρ. Define y g , g = [n] by this way: randomly set ρn elements as 1, the rest are 0s. This strategy is used in numerical experiments with piecewise linear f (k = 5) and multiple balls experiment (k = 1) due to the necessity of having a specific number of samples from the same group in each batch for the computation of sample skewness and kurtosis-something strategy i) is incapable of achieving. iii Define n possible masks {y 1 , .., y n } with different ratio of measured variables ρ g , g = [n] from 1 var to 100%. Define y g this way: set 1-th to (g + ρ g n)-th element as 1, the rest are 0s. This is employed in the PartialCausal3DIdent experiment.  Our theoretical result assume that we know the number of causal variables in advance, but in practice (and as often also the case in other CRL methods), we can show empirically that if we overestimate the true number of latent variables, we will still be able to identify the true causal variables, while some of them will be unused. We provide ablation study below, which seem to confirm this empirically.</p><p>In the experiments below we consider n = 10 ground truth causal variables, and nn latent variables that we estimate, where nn ≥ n. For all experiments, we consider an average over 3 random seeds. We use a causal graph ER-k, where ER-k is a graph with n • k edges, δ is the distance between mask value and mean of causal variables and ρ denotes the ratio of measured variables. For the piecewise linear case m denotes the number of hidden layers in the MLP. We choose one representative setting for the linear case and one representative setting for the piecewise linear case, and report how the MCC varies when we increase nn from the original size nn = n to double the size nn = 2n. We report the average MCC over 3 random seeds and a heatmap of the correlation matrix for each of the settings below.  <ref type="table" target="#tab_15">12</ref>, our method seems to perform similarly well, even if we do not know the true number of causal variables, but we overestimate them. We provide the heatmaps of the correlation matrix between Z and Ẑ for one of the seeds with different nn's in Figure <ref type="figure">27</ref> and Figure <ref type="figure">28</ref>. For the missing ball setting, we generate the x-coordinates of each ball C j for j ∈ [b] from a truncated normal distribution N (0.5, 0.1 2 ) with bounds (0.1, 0.9). For the masked position setting, we generate the x and y-coordinates of each ball C i for j ∈ [b] independently from a truncated 2-dimensional normal distribution N (µ j , Σ j ) with bounds (0.1, 0.9) 2 , where µ i is generated from Unif(0.4, 0.6) 2 , and Σ j = ((0.01, 0.005)(0.005, 0.01)) for all groups. All the aforementioned parameters, including mean and variance/covariance above, are configured to ensure the majority of the samples are located in interval [0, 1]. This configuration aims to maintain the truncated distribution as close to a Gaussian distribution as feasible.</p><p>For both settings, we generate the masked causal variables as (Z k ) K k=1 = y • C + (1y) • M ∈ R K×n . For the missing ball setting, the latent size is equal to the number of balls b. Therefore, we predefine b masks via strategy ii) in App. D.4. The mask value is set as 0. For the masked position setting, the latent size is 2b since both x and y-coordinates of each ball are considered. The masked positions for distinct balls are different; avoiding overlap happens if two balls are masked in the same group. The mask value of each latent is predefined by one of these values {0.05, 0.1, 0.9, 0.95}. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Image dataset: Causal3DIdent</head><p>A qualitative visualization of the result for one object class is presented in Fig. <ref type="figure">32</ref> where estimated and ground truth latent are well aligned on the diagonal.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig 1b and discuss each component in the following. Causal variables C. We define our latent causal variables as a random vector C = (C 1 , ..., C n ) that takes values in C = C 1 ×...×C n ⊆ R n , which is an open, simply connected latent space. The causal variables follow a distribution with density p(C), which allows for causal relations between them. We assume that p(c) ̸ = 0 for all c ∈ C. Mask variables Y. We use a binary mask random variable Y = (Y 1 , . . . , Y n ) with domain Y ⊆ {0, 1} n to represent the dynamic partial observability patterns, i.e., the causal variables that are measured in each of the samples. If Y i = 1 then we consider the variable C i measured, i.e. captured in the observation, otherwise it is considered unmeasured. We assume Y follows p(Y). Further, we define the support index random set S as the index of non-zero components of Y, i.e., S := {i ∈ [n] : Y i ̸ = 0}. The support index set has a probability mass function p(s) and support S defined as: = {s ⊆ [n] | p(s) &gt; 0}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>y) up to affine transformation, i.e., there exists an invertible affine functionh Y : Z Y → R n , such that h y (Z) | (Y = y) ≡ g(f (Z)) | (Y = y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Level curves of the function f -1 • f of Example C.1. The cold color scheme corresponds to the level curves of( f -1 • f ) 1 (z) while the warm color scheme corresponds to ( f -1 • f ) 2 (z).The example gives a concrete case where all assumptions of Theorem 3.1 hold except for the linearity of f . We can see that f -1 • f is not a permutation composed with an element-wise invertible transformation, since along the vertical dashed line, we can see that both components of f -1 • f change.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Histogram of estimator for our method for n = 5, m = 10, independent Z, ρ=50%,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 30 :</head><label>30</label><figDesc>Figure 30: Example images of two settings in multiple ball dataset, based on the rendering code provided by (Ahuja et al., 2022). (left) Missing ball setting: balls can move along gray paths only and are not visible when they move out of view. (right) Masked position setting: balls can move freely inside the frame and their coordinates are masked when they have a specific value M; e.g., the left-bottom position represents a masked version of the x and y coordinate for the olive ball.</figDesc><graphic coords="43,203.24,503.58,91.54,90.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results for the numerical experiments for linear mixing functions with δ = 0. The bold font indicates which parameters are varying in each block of rows.</figDesc><table><row><cell>n</cell><cell>k</cell><cell>SCM</cell><cell>ρ</cell><cell>MCC</cell></row><row><cell>5</cell><cell>1</cell><cell>LIN. GAUSS</cell><cell>50 %</cell><cell>0.997±0.002</cell></row><row><cell cols="2">10 1</cell><cell>LIN. GAUSS</cell><cell>50 %</cell><cell>0.996±0.001</cell></row><row><cell cols="2">20 1</cell><cell>LIN. GAUSS</cell><cell>50 %</cell><cell>0.987±0.029</cell></row><row><cell cols="2">40 1</cell><cell>LIN. GAUSS</cell><cell>50 %</cell><cell>0.714±0.153</cell></row><row><cell cols="4">10 0 INDEP. GAUSS 50 %</cell><cell>0.998±0.001</cell></row><row><cell cols="2">10 1</cell><cell>LIN. GAUSS</cell><cell>50 %</cell><cell>0.996±0.001</cell></row><row><cell cols="2">10 2</cell><cell>LIN. GAUSS</cell><cell>50 %</cell><cell>0.904±0.113</cell></row><row><cell cols="2">10 3</cell><cell>LIN. GAUSS</cell><cell>50 %</cell><cell>0.793±0.142</cell></row><row><cell cols="2">10 0</cell><cell>INDEP. EXP</cell><cell>50 %</cell><cell>0.998±0.001</cell></row><row><cell cols="2">10 1</cell><cell>LIN. EXP</cell><cell>50 %</cell><cell>0.998±0.002</cell></row><row><cell cols="2">10 2</cell><cell>LIN. EXP</cell><cell>50 %</cell><cell>0.910±0.108</cell></row><row><cell cols="2">10 3</cell><cell>LIN. EXP</cell><cell>50 %</cell><cell>0.825±0.123</cell></row><row><cell cols="2">10 1</cell><cell>NONLINEAR</cell><cell>50 %</cell><cell>0.997±0.001</cell></row><row><cell cols="2">10 2</cell><cell>NONLINEAR</cell><cell>50 %</cell><cell>0.997±0.001</cell></row><row><cell cols="2">10 3</cell><cell>NONLINEAR</cell><cell>50 %</cell><cell>0.996±0.001</cell></row><row><cell cols="2">10 1</cell><cell>LIN. GAUSS</cell><cell cols="2">1VAR 0.998 ±0.002</cell></row><row><cell cols="2">10 1</cell><cell>LIN. GAUSS</cell><cell>50 %</cell><cell>0.996±0.001</cell></row><row><cell cols="2">10 1</cell><cell>LIN. GAUSS</cell><cell cols="2">75 % 0.877 ±0.096</cell></row></table><note><p>more variables at the same time. Intuitively, measuring a smaller number of variables for each sample is the easier setting for disentangling them from the observations.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results for the multiple balls dataset.</figDesc><table><row><cell cols="3">b MCC Missing MCC Masked</cell></row><row><cell>2</cell><cell>0.963±0.012</cell><cell>0.946±0.005</cell></row><row><cell>5</cell><cell>0.950±0.011</cell><cell>0.939±0.003</cell></row><row><cell>8</cell><cell>0.928±0.004</cell><cell>0.901±0.002</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>MCC on PartialCausal3DIdent over each object class, with masking distance δ = 10 for all latent variables. After obtaining the masked latent variables, we retrieve K corresponding images from the dataset, based on the index searching scheme provided by von<ref type="bibr" target="#b16">Kügelgen et al. (2021)</ref>.</figDesc><table><row><cell>OBJECT CLASS ID</cell><cell>0</cell><cell>1</cell></row><row><cell cols="3">MCC (MEAN ± STD) 0.842 ± 0.018 0.804 ± 0.023</cell></row><row><cell>OBJECT CLASS ID</cell><cell>2</cell><cell>3</cell></row><row><cell cols="3">MCC (MEAN ± STD) 0.828 ± 0.014 0.820 ± 0.009</cell></row><row><cell>OBJECT CLASS ID</cell><cell>4</cell><cell>5</cell></row><row><cell cols="3">MCC (MEAN ± STD) 0.837 ± 0.020 0.821 ± 0.033</cell></row><row><cell>OBJECT CLASS ID</cell><cell>6</cell><cell>AVG. MEAN</cell></row><row><cell cols="3">MCC (MEAN ± STD) 0.858 ± 0.005 0.832 ± 0.016</cell></row><row><cell cols="3">is 1 for all latents). The ratio of the measured variables</cell></row><row><cell cols="3">ρ varies from 10% (only one latent is measured) to 100%</cell></row><row><cell cols="3">(all latents are measured). The average ρ is set to 50%.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Among these works leveraging sparsity, Lachapelle et al. (2023) introduce a sparsity principle that inspired our work, but for the multi-task setting, where each task depends on a subset of variables. More precisely, they assume that the connections W between tasks Y and representations f (X) are sparse, i.e. Y = W f (X) and W is a column-sparse matrix (some of the columns are filled with zeros). In our setting, the sparsity assumption is on the representation itself, i.e. each causal variable has a positive probability of being masked. While the proof strategies are similar, our result also had to address the piecewise linear case. Other work that is closely related to ours is work byLachapelle et al.   (2022; 2024), who have proposed a sparsity principle for identifiable CRL in interventional and temporal settings, motivated by the sparse mechanism shift hypothesis<ref type="bibr" target="#b16">(Schölkopf et al., 2021;</ref> Perry et al., 2022).Our work is also related to sparse component analysis(Gribonval and Lesage, 2006)  and sparse dictionary learning(Mairal et al., 2009). These unsupervised representation learning methods assume linear mixing functions f and learn a sparse representations of the input X, similar to our g(X). The identifiability of dictionary learning has been studied, e.g. byHu and Huang (2023), in the finite sample regime. The main distinction with our work is that we focus on identifiability with nonlinear mixing. Similar to our piecewise linear theorem, Liu et al. (2022) also assumes that the underlying SCM is linear Gaussian. As opposed to our work, it leverages the fact that the coefficients (or weights) of the causal relations are varying across environments, while in our case we leverage the partial observability patterns and assume that the underlying SCM is the same for all data. Similarly, Liu et al. (2024) extends the results from Liu et al. (2022) to polynomial causal models with exponential family noise variables. These works are related to ours in leveraging changes across environments, but provide a different type of results.</figDesc><table /><note><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p>This prevents us from applying the result of</p>Kivva et al.  (2022)  </p>directly to our setting. On the other hand, although we allow for degenerate components, our result assumes knowledge of the groups g, unlike</p>Kivva et al. (2022)</p>.</p>Prior work has also leveraged sparsity in representation learning, for example via a Spike and Slab prior</p><ref type="bibr" target="#b13">(Tonolini et al., 2020;</ref> Moran et al., 2022)</p>, assuming structural sparsity on the support of Jacobian of nonlinear mixing function</p><ref type="bibr" target="#b26">(Zheng et al., 2022;</ref><ref type="bibr" target="#b25">Zheng and Zhang, 2023)</ref></p>, exploiting the sparsity constraint on the linear mixing function</p>(Ng et al., 2023)</p>, or by relating the learnt representation to multiple tasks, each depending only on a small subset of latents</p><ref type="bibr" target="#b21">(Lachapelle et al., 2023;</ref> Fumero et al.,  2023)</p>. Other work that is closely related to ours is work by</p>Lachapelle et al. (2022; 2024)</p>, who have proposed a sparsity principle for identifiable CRL in interventional and temporal settings, motivated by the sparse mechanism shift hypothesis</p><ref type="bibr" target="#b16">(Schölkopf et al., 2021;</ref> Perry et al., 2022)</p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Phillip Lippe, Sara Magliacane, Sindy Löwe, Yuki M Asano, Taco Cohen, and Efstratios Gavves. Biscuit: Causal representation learning from binary interactions. Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence, 2023.</figDesc><table><row><cell>Aapo Hyvärinen and Stephen M Smith. Pairwise likelihood</cell><cell></cell></row><row><cell>ratios for estimation of non-gaussian structural equation</cell><cell></cell></row><row><cell>models. The Journal of Machine Learning Research, 14</cell><cell></cell></row><row><cell>(1):111-152, 2013.</cell><cell></cell></row><row><cell>Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Non-</cell><cell></cell></row><row><cell>linear ica using auxiliary variables and generalized con-</cell><cell>Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong,</cell></row><row><cell>trastive learning. In The 22nd International Conference</cell><cell>Biwei Huang, Anton van den Hengel, Kun Zhang, and</cell></row><row><cell>on Artificial Intelligence and Statistics, pages 859-868.</cell><cell>Javen Qinfeng Shi. Identifying weight-variant latent</cell></row><row><cell>PMLR, 2019.</cell><cell>causal models. arXiv preprint arXiv:2208.14153, 2022.</cell></row><row><cell>Markus Kalisch, Martin Mächler, Diego Colombo, Mar-loes H Maathuis, and Peter Bühlmann. Causal inference using graphical models with the r package pcalg. Journal of statistical software, 47:1-26, 2012.</cell><cell>Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton van den Hengel, Kun Zhang, and Javen Qinfeng Shi. Identifiable latent polynomial causal models through the lens of change. In The Twelfth Inter-</cell></row><row><cell>Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and</cell><cell>national Conference on Learning Representations, 2024.</cell></row><row><cell>Aapo Hyvarinen. Variational autoencoders and nonlinear</cell><cell></cell></row><row><cell>ica: A unifying framework. In International Conference</cell><cell>J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online dictionary</cell></row><row><cell>on Artificial Intelligence and Statistics, pages 2207-2217.</cell><cell>learning for sparse coding. In Proceedings of the 26th</cell></row><row><cell>PMLR, 2020.</cell><cell>annual international conference on machine learning,</cell></row><row><cell></cell><cell>pages 689-696, 2009.</cell></row><row><cell>Bohdan Kivva, Goutham Rajendran, Pradeep Ravikumar,</cell><cell></cell></row><row><cell>and Bryon Aragam. Learning latent causal graphs via</cell><cell>G Moran, D Sridhar, Y Wang, and D Blei. Identifiable deep</cell></row><row><cell>mixture oracles. Advances in Neural Information Pro-</cell><cell>generative models via sparse decoding. Transactions on</cell></row><row><cell>cessing Systems, 34:18087-18101, 2021.</cell><cell>machine learning research, 2022.</cell></row><row><cell>Bohdan Kivva, Goutham Rajendran, Pradeep Ravikumar,</cell><cell>Ignavier Ng, Yujia Zheng, Xinshuai Dong, and Kun Zhang.</cell></row><row><cell>2022. and Bryon Aragam. Identifiability of deep generative models without auxiliary information. Advances in Neu-ral Information Processing Systems, 35:15687-15701,</cell><cell>Jose Gallego-Posada and Juan Ramirez. Cooper: a toolkit On the identifiability of sparse ica without assuming non-gaussianity. Advances in Neural Information Processing Systems, 36, 2023.</cell></row><row><cell>Sébastien Lachapelle, Pau Rodriguez, Yash Sharma, Katie E Everett, Rémi Le Priol, Alexandre Lacoste, and Simon Lacoste-Julien. Disentanglement via mechanism sparsity</cell><cell>for lagrangian-based constrained optimization, 2022. Gauthier Gidel, Hugo Berard, Gaëtan Vignoud, Pascal Vin-cent, and Simon Lacoste-Julien. A variational inequality Judea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, USA, 2nd edition, 2009. ISBN 052189560X.</cell></row><row><cell>regularization: A new principle for nonlinear ica. In Conference on Causal Learning and Reasoning, pages 428-484. PMLR, 2022. Sébastien Lachapelle, Tristan Deleu, Divyat Mahajan, Ioan-nis Mitliagkas, Yoshua Bengio, Simon Lacoste-Julien,</cell><cell>perspective on generative adversarial networks. arXiv preprint arXiv:1802.10551, 2018. Ronan Perry, Julius Von Kügelgen, and Bernhard Schölkopf. Causal discovery in heterogeneous environments under the sparse mechanism shift hypothesis. Advances in Neu-Rémi Gribonval and Sylvain Lesage. A survey of sparse ral Information Processing Systems, 35:10904-10917, component analysis for blind source separation: princi-ples, perspectives, and new challenges. In ESANN'06 2022.</cell></row><row><cell>and Quentin Bertrand. Synergies between disentangle-ment and sparsity: Generalization and identifiability in multi-task learning. In International Conference on Ma-chine Learning, pages 18171-18206. PMLR, 2023.</cell><cell>proceedings-14th European Symposium on Artificial Neu-ral Networks, pages 323-330. d-side publi., 2006. Jingzhou Hu and Kejun Huang. Global identifiability of Jonas Peters, Dominik Janzing, and Bernhard Schlkopf. Elements of Causal Inference: Foundations and Learning Algorithms. The MIT Press, 2017. ISBN 0262037319.</cell></row><row><cell>Sébastien Lachapelle, Pau Rodríguez López, Yash Sharma, Katie Everett, Rémi Le Priol, Alexandre Lacoste, and Simon Lacoste-Julien. Nonparametric partial disentan-</cell><cell>$\ell_1$-based dictionary learning via matrix volume op-Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, timization. In Thirty-seventh Conference on Neural Infor-Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, mation Processing Systems, 2023. and Yoshua Bengio. Toward causal representation learn-</cell></row><row><cell>glement via mechanism sparsity: Sparse actions, inter-</cell><cell>Aapo Hyvarinen and Hiroshi Morioka. Unsupervised fea-ing. Proceedings of the IEEE, 109(5):612-634, 2021.</cell></row><row><cell>ventions and sparse temporal dependencies, 2024.</cell><cell>ture extraction by time-contrastive learning and nonlinear</cell></row><row><cell></cell><cell>ica. Advances in neural information processing systems, Pete Shinners. Pygame, 2011.</cell></row><row><cell></cell><cell>29, 2016.</cell></row></table><note><p>Marco Fumero, Florian Wenzel, Luca Zancato, Alessandro Achille, Emanuele Rodolà, Stefano Soatto, Bernhard Schölkopf, and Francesco Locatello. Leveraging sparse and shared feature activations for disentangled representation learning. arXiv preprint arXiv:2304.07939, 2023. Aapo Hyvarinen and Hiroshi Morioka. Nonlinear ica of temporally dependent stationary sources. In Artificial Intelligence and Statistics, pages 460-469. PMLR, 2017. Phillip Lippe, Sara Magliacane, Sindy Löwe, Yuki M Asano, Taco Cohen, and Stratis Gavves. Citris: Causal identifiability from temporal intervened sequences. In International Conference on Machine Learning, pages 13557-13603. PMLR, 2022. Ricardo Silva, Richard Scheines, Clark Glymour, Peter Spirtes, and David Maxwell Chickering. Learning the structure of linear latent variable models. Journal of Machine Learning Research, 7(2), 2006.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Case 2. s 1 ∩ s 2 ̸ = ∅. Without loss of generality, we assume |s 2 | ≤ |s 1 |. (a). s 2 ⊆ s 1 . Then, we can directly get ∀z ∈ Z s1 ∪ Z s2 v(z) = z 1 a 1 + ... + z |s1| a |s1| + c 1 (75) (b). s 2 ̸ ⊆ s 1 . Without loss of generality, we assume s 1 ∩ s 2 = {m, ..., m + t}, where t ∈ {1, ..., |s 2 |}. Then, ∀z ∈ Z s1 ∩ Z s2 , we have</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Parameters for experiments results in Sec. 5 and App. E.</figDesc><table><row><cell>The implementation is built upon</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Results for the numerical experiments in the piecewise linear case for simple settings.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Results for the numerical experiments in the piecewise linear case for the dependent variable case.</figDesc><table><row><cell>n</cell><cell cols="2">m k</cell><cell>ρ</cell><cell>δ</cell><cell cols="2">MCC (MEAN ± STD) MCC (MEAN ± STD) ORACLE</cell></row><row><cell>5</cell><cell cols="3">10 1 50 %</cell><cell>2</cell><cell>0.469±0.050</cell><cell>0.898±0.018</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>2</cell><cell>0.349±0.030</cell><cell>0.799±0.020</cell></row><row><cell cols="4">20 10 1 50 %</cell><cell>2</cell><cell>0.280±0.017</cell><cell>0.753±0.015</cell></row><row><cell cols="4">40 10 1 50 %</cell><cell>2</cell><cell>0.217±0.025</cell><cell>0.782±0.012</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>2</cell><cell>0.349±0.030</cell><cell>0.799±0.020</cell></row><row><cell cols="4">10 10 2 50 %</cell><cell>2</cell><cell>0.337±0.030</cell><cell>0.789±0.021</cell></row><row><cell cols="4">10 10 3 50 %</cell><cell>2</cell><cell>0.345±0.028</cell><cell>0.785±0.022</cell></row><row><cell>10</cell><cell>3</cell><cell cols="2">1 50 %</cell><cell>2</cell><cell>0.466±0.038</cell><cell>0.867±0.007</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>2</cell><cell>0.349±0.030</cell><cell>0.799±0.020</cell></row><row><cell cols="4">10 20 1 50 %</cell><cell>2</cell><cell>0.267±0.031</cell><cell>0.552±0.035</cell></row><row><cell cols="4">10 10 1 1VAR</cell><cell>2</cell><cell>0.402±0.027</cell><cell>0.908±0.013</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>2</cell><cell>0.349±0.030</cell><cell>0.799±0.020</cell></row><row><cell cols="4">10 10 1 75 %</cell><cell>2</cell><cell>0.352±0.037</cell><cell>0.695±0.026</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>0</cell><cell>0.332±0.040</cell><cell>0.185±0.036</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>3</cell><cell>0.368±0.028</cell><cell>0.856±0.015</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>5</cell><cell>0.388±0.029</cell><cell>0.930±0.010</cell></row><row><cell cols="5">10 10 1 50 % 10</cell><cell>0.405±0.027</cell><cell>0.980±0.007</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Results for the numerical experiments in the piecewise linear case for the independent variable case.For each setup in Table7, we choose one random seed to plot the heatmap of the Pearson Correlation matrix Corr n×n π with the permutation π . One figure represents one ablation study of one parameter. Ground truth Pear. Corr. matrix on the left shows the original linear correlation inside Z, compared with the estimator on the right-hand side.</figDesc><table><row><cell>n</cell><cell cols="2">m k</cell><cell>ρ</cell><cell cols="2">δ MCC (MEAN ± STD) MCC (MEAN ± STD) ORACLE</cell></row><row><cell>5</cell><cell cols="4">10 0 50 % 2</cell><cell>0.462±0.065</cell><cell>0.910±0.014</cell></row><row><cell cols="5">10 10 0 50 % 2</cell><cell>0.352±0.028</cell><cell>0.807±0.022</cell></row><row><cell cols="5">20 10 0 50 % 2</cell><cell>0.296±0.018</cell><cell>0.732±0.011</cell></row><row><cell cols="5">40 10 0 50 % 2</cell><cell>0.246±0.013</cell><cell>0.746±0.015</cell></row><row><cell>10</cell><cell>3</cell><cell cols="3">0 50 % 2</cell><cell>0.523±0.026</cell><cell>0.865±0.008</cell></row><row><cell cols="5">10 10 0 50 % 2</cell><cell>0.352±0.028</cell><cell>0.807±0.022</cell></row><row><cell cols="5">10 20 0 50 % 2</cell><cell>0.269±0.023</cell><cell>0.519±0.028</cell></row><row><cell cols="5">10 10 0 1VAR 2</cell><cell>0.412±0.027</cell><cell>0.906±0.016</cell></row><row><cell cols="5">10 10 0 50 % 2</cell><cell>0.352±0.028</cell><cell>0.807±0.022</cell></row><row><cell cols="5">10 10 0 75 % 2</cell><cell>0.340±0.028</cell><cell>0.719±0.017</cell></row><row><cell cols="5">10 10 0 50 % 0</cell><cell>0.318±0.026</cell><cell>0.236±0.040</cell></row><row><cell cols="5">10 10 0 50 % 3</cell><cell>0.366±0.029</cell><cell>0.852±0.020</cell></row><row><cell cols="5">10 10 0 50 % 5</cell><cell>0.388±0.027</cell><cell>0.926±0.014</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Results for the numerical experiments in the piecewise linear case for training on dependent variable but testing on independent variable case. .1.4. OVER-PARAMETERIZATION OF THE NUMBER OF CAUSAL REPRESENTATIONS</figDesc><table><row><cell>n</cell><cell cols="2">m k</cell><cell>ρ</cell><cell>δ</cell><cell cols="2">MCC(TABLE 7) MCC(TEST ON INDEPENDENT LATENTS)</cell></row><row><cell>5</cell><cell cols="3">10 1 50 %</cell><cell>2</cell><cell>0.898±0.018</cell><cell>0.923±0.021</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>2</cell><cell>0.799±0.020</cell><cell>0.807±0.023</cell></row><row><cell cols="4">20 10 1 50 %</cell><cell>2</cell><cell>0.753±0.015</cell><cell>0.800±0.022</cell></row><row><cell cols="4">40 10 1 50 %</cell><cell>2</cell><cell>0.782±0.012</cell><cell>0.869±0.013</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>2</cell><cell>0.799±0.020</cell><cell>0.807±0.023</cell></row><row><cell cols="4">10 10 2 50 %</cell><cell>2</cell><cell>0.789±0.021</cell><cell>0.836±0.036</cell></row><row><cell cols="4">10 10 3 50 %</cell><cell>2</cell><cell>0.785±0.022</cell><cell>0.847±0.036</cell></row><row><cell>10</cell><cell>3</cell><cell cols="2">1 50 %</cell><cell>2</cell><cell>0.867±0.007</cell><cell>0.923±0.034</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>2</cell><cell>0.799±0.020</cell><cell>0.807±0.023</cell></row><row><cell cols="4">10 20 1 50 %</cell><cell>2</cell><cell>0.552±0.035</cell><cell>0.537±0.035</cell></row><row><cell cols="4">10 10 1 1VAR</cell><cell>2</cell><cell>0.908±0.013</cell><cell>0.946±0.011</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>2</cell><cell>0.799±0.020</cell><cell>0.807±0.023</cell></row><row><cell cols="4">10 10 1 75 %</cell><cell>2</cell><cell>0.695±0.026</cell><cell>0.678±0.030</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>0</cell><cell>0.185±0.036</cell><cell>0.232±0.038</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>3</cell><cell>0.856±0.015</cell><cell>0.881±0.018</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>5</cell><cell>0.930±0.010</cell><cell>0.949±0.009</cell></row><row><cell cols="5">10 10 1 50 % 10</cell><cell>0.980±0.007</cell><cell>0.986±0.006</cell></row></table><note><p>E</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Average MCCs over 3 random seeds for linear f with n = 10, δ = 0.0σ, ρ=50, k = 1, Linear Gaussian SCM</figDesc><table><row><cell cols="2">nn(n = 10) 10(n + 0)</cell><cell>12(n + 2)</cell><cell>14(n + 4)</cell><cell>16(n + 6)</cell><cell>18(n + 8)</cell><cell>20(n + 10)</cell></row><row><cell>MCC</cell><cell cols="6">0.998±0.001 0.931±0.056 0.965±0.051 0.991±0.001 0.996±0.001 0.978±0.031</cell></row><row><cell cols="2">nn(n = 10) 10(n + 0)</cell><cell>12(n + 2)</cell><cell>14(n + 4)</cell><cell>16(n + 6)</cell><cell>18(n + 8)</cell><cell>20(n + 10)</cell></row><row><cell>MCC</cell><cell cols="6">0.796±0.026 0.798±0.012 0.804±0.015 0.809±0.012 0.814±0.014 0.816±0.012</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Average MCCs over 3 random seeds for piecewise f with n = 10, δ = 2.0σ, ρ=50, k = 1, m = 10, Linear Gaussian SCM As shown in both Table11 and Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 :</head><label>13</label><figDesc>Results of learning graph for linear mixing functions.</figDesc><table><row><cell>n</cell><cell>k</cell><cell>SCM</cell><cell>ρ</cell><cell>SHDz *</cell><cell>SHD ẑ</cell><cell>∆SHD</cell></row><row><cell>5</cell><cell cols="2">1 LIN. GAUSS</cell><cell>50 %</cell><cell>0.65±1.57</cell><cell>2.40±1.90</cell><cell>1.75</cell></row><row><cell cols="3">10 1 LIN. GAUSS</cell><cell>50 %</cell><cell>1.9±1.89</cell><cell>5.90±3.06</cell><cell>4.00</cell></row><row><cell cols="3">20 1 LIN. GAUSS</cell><cell>50 %</cell><cell>7.15±3.92</cell><cell>12.75±5.56</cell><cell>5.60</cell></row><row><cell cols="3">40 1 LIN. GAUSS</cell><cell cols="3">50 % 13.70±4.53 79.60±27.08</cell><cell>65.90</cell></row><row><cell cols="3">10 1 LIN. GAUSS</cell><cell>50 %</cell><cell>1.9±1.89</cell><cell>5.90±3.06</cell><cell>4.00</cell></row><row><cell cols="3">10 2 LIN. GAUSS</cell><cell cols="2">50 % 16.20±2.38</cell><cell>18.60±4.10</cell><cell>2.40</cell></row><row><cell cols="3">10 3 LIN. GAUSS</cell><cell cols="2">50 % 27.20±2.65</cell><cell>29.25±3.29</cell><cell>2.05</cell></row><row><cell cols="2">10 1</cell><cell>LIN. EXP</cell><cell>50 %</cell><cell>2.95±2.26</cell><cell>8.20±4.83</cell><cell>5.25</cell></row><row><cell cols="2">10 2</cell><cell>LIN. EXP</cell><cell>50 %</cell><cell>3.70±2.66</cell><cell>18.65±11.81</cell><cell>14.95</cell></row><row><cell cols="2">10 3</cell><cell>LIN. EXP</cell><cell>50 %</cell><cell>5.20±4.51</cell><cell>24.85±11.39</cell><cell>19.65</cell></row><row><cell cols="4">10 1 NONLINEAR 50 %</cell><cell>5.55±2.50</cell><cell>9±2.94</cell><cell>3.45</cell></row><row><cell cols="4">10 2 NONLINEAR 50 %</cell><cell>14.4±2.04</cell><cell>16.25±3.39</cell><cell>1.85</cell></row><row><cell cols="4">10 3 NONLINEAR 50 %</cell><cell>23.4±2.44</cell><cell>25.4±3.99</cell><cell>1</cell></row><row><cell cols="4">10 1 LIN. GAUSS 1VAR</cell><cell>2.1±1.89</cell><cell>4.65±2.89</cell><cell>2.55</cell></row><row><cell cols="4">10 1 LIN. GAUSS 50 %</cell><cell>1.9±1.89</cell><cell>5.90±3.06</cell><cell>4.00</cell></row><row><cell cols="4">10 1 LIN. GAUSS 75 %</cell><cell>2.15±1.79</cell><cell>14.70±5.25</cell><cell>12.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14 :</head><label>14</label><figDesc>Results of learning graph for piecewise linear mixing functions with our method.</figDesc><table><row><cell>n</cell><cell cols="2">m k</cell><cell>ρ</cell><cell>δ</cell><cell>SHDz *</cell><cell>SHD ẑ</cell><cell>∆SHD</cell></row><row><cell>5</cell><cell cols="3">10 1 50 %</cell><cell>2</cell><cell>0.85±1.79</cell><cell>7±1.89</cell><cell>6.15</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>2</cell><cell>2±2.08</cell><cell>19.45±2.35</cell><cell>17.45</cell></row><row><cell cols="4">20 10 1 50 %</cell><cell>2</cell><cell>6.8±3.49</cell><cell>46.75±5.09</cell><cell>39.95</cell></row><row><cell cols="4">40 10 1 50 %</cell><cell>2</cell><cell>13.3±4.44</cell><cell>113.4±6.85</cell><cell>100.1</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>2</cell><cell>2±2.08</cell><cell>19.45±2.35</cell><cell>17.45</cell></row><row><cell cols="4">10 10 2 50 %</cell><cell>2</cell><cell>15.85±2.52</cell><cell>25.3±2.32</cell><cell>9.45</cell></row><row><cell cols="4">10 10 3 50 %</cell><cell>2</cell><cell>27.8±2.33</cell><cell>30.75±2.05</cell><cell>2.95</cell></row><row><cell>10</cell><cell>3</cell><cell cols="2">1 50 %</cell><cell>2</cell><cell>2±2.08</cell><cell>19.05±2.66</cell><cell>17.05</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>2</cell><cell>2±2.08</cell><cell>19.45±2.35</cell><cell>17.45</cell></row><row><cell cols="4">10 20 1 50 %</cell><cell>2</cell><cell>2±2.08</cell><cell>19.5±2.06</cell><cell>17.5</cell></row><row><cell cols="4">10 10 1 1VAR</cell><cell>2</cell><cell>2±2.08</cell><cell>19.5±1.79</cell><cell>17.5</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>2</cell><cell>2±2.08</cell><cell>19.45±2.35</cell><cell>17.45</cell></row><row><cell cols="4">10 10 1 75 %</cell><cell>2</cell><cell>2±2.08</cell><cell>18.8±2.28</cell><cell>16.8</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>0</cell><cell>2±2.08</cell><cell>17.9±2.71</cell><cell>15.9</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>3</cell><cell>2±2.08</cell><cell>18.9±2.17</cell><cell>16.9</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>5</cell><cell>2±2.08</cell><cell>18.95±1.76</cell><cell>16.95</cell></row><row><cell cols="5">10 10 1 50 % 10</cell><cell>2±2.08</cell><cell>19.05±1.93</cell><cell>17.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 15 :</head><label>15</label><figDesc>Results of learning graph for piecewise linear mixing functions with oracle method. We use PyGame (Shinners, 2011) to render images with size 64 × 64 × 3 as shown in Figure30. The number of balls b varies from 2 to 8.</figDesc><table><row><cell>n</cell><cell cols="2">m k</cell><cell>ρ</cell><cell>δ</cell><cell>SHDz *</cell><cell>SHD ẑ</cell><cell>∆SHD</cell></row><row><cell>5</cell><cell cols="3">10 1 50 %</cell><cell>2</cell><cell>0.85±1.79</cell><cell>7.1±1.97</cell><cell>6.25</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>2</cell><cell>2±2.08</cell><cell>21.35±2.68</cell><cell>19.35</cell></row><row><cell cols="4">20 10 1 50 %</cell><cell>2</cell><cell>6.8±3.49</cell><cell>56.95±4.17</cell><cell>50.15</cell></row><row><cell cols="4">40 10 1 50 %</cell><cell>2</cell><cell>13.3±4.44</cell><cell cols="2">130.85±9.84 117.55</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>2</cell><cell>2±2.08</cell><cell>21.35±2.68</cell><cell>19.35</cell></row><row><cell cols="4">10 10 2 50 %</cell><cell>2</cell><cell>15.85±2.52</cell><cell>26.1±2.77</cell><cell>10.25</cell></row><row><cell cols="4">10 10 3 50 %</cell><cell>2</cell><cell>27.8±2.33</cell><cell>29.9±2.75</cell><cell>2.1</cell></row><row><cell>10</cell><cell>3</cell><cell cols="2">1 50 %</cell><cell>2</cell><cell>2±2.08</cell><cell>19.8±2.80</cell><cell>17.8</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>2</cell><cell>2±2.08</cell><cell>21.35±2.68</cell><cell>19.35</cell></row><row><cell cols="4">10 20 1 50 %</cell><cell>2</cell><cell>2±2.08</cell><cell>21.25±2.84</cell><cell>19.25</cell></row><row><cell cols="4">10 10 1 1VAR</cell><cell>2</cell><cell>2±2.08</cell><cell>21.15±2.72</cell><cell>19.15</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>2</cell><cell>2±2.08</cell><cell>21.35±2.68</cell><cell>19.35</cell></row><row><cell cols="4">10 10 1 75 %</cell><cell>2</cell><cell>2±2.08</cell><cell>21.3±3.24</cell><cell>19.3</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>0</cell><cell>2±2.08</cell><cell>19.55±2.87</cell><cell>17.55</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>3</cell><cell>2±2.08</cell><cell>21.2±2.74</cell><cell>19.2</cell></row><row><cell cols="4">10 10 1 50 %</cell><cell>5</cell><cell>2±2.08</cell><cell>20.35±2.74</cell><cell>18.35</cell></row><row><cell cols="5">10 10 1 50 % 10</cell><cell>2±2.08</cell><cell>19.65±2.76</cell><cell>17.65</cell></row><row><cell cols="2">E.2. Image dataset: Multiple Balls</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Data generation process</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>University of Amsterdam</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Institute of Science and Technology Austria</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Max Planck Institute for Intelligent Systems, Tübingen, Germany</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Samsung -SAIT AI Lab, Montreal</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Mila, Université de Montréal</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>ServiceNow Research</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Seminar for Statistics, ETH Zürich. Correspondence to: Danru Xu &lt;d.xu3@uva.nl&gt;. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was initiated at the Second Bellairs Workshop on Causality held at the <rs type="institution">Bellairs Research Institute</rs>, January <rs type="grantNumber">6-13</rs>, <rs type="grantNumber">2022</rs>; we thank all workshop participants for providing a stimulating research environment. The research of DX and SM was supported by the <rs type="funder">Air Force Office of Scientific Research</rs> under award number <rs type="grantNumber">FA8655-22-1-7155</rs>. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the <rs type="funder">United States Air Force</rs>. We also thank <rs type="institution">SURF</rs> for the support in using the Dutch National Supercomputer Snellius. DY was supported by an <rs type="funder">Amazon</rs> fellowship, the <rs type="funder">International Max Planck Research School for Intelligent Systems (IMPRS-IS)</rs>, and the <rs type="funder">ISTA</rs> graduate school. Work was done outside of Amazon. SL was supported by an <rs type="funder">IVADO</rs> excellence PhD scholarship and by <rs type="institution">Samsung Electronics Co.</rs>, <rs type="person">Ldt. JvK</rs> acknowledges support from the <rs type="funder">German Federal Ministry of Education and Research (BMBF)</rs> through the <rs type="grantName">Tübingen AI Center</rs> (FKZ: <rs type="grantNumber">01IS18039B</rs>).</p></div>
<div><head>Impact Statement</head><p>This paper presents conceptual work whose goal is to advance the field of <rs type="person">Machine Learning</rs>, and specifically Causal Representation Learning. There are many potential societal consequences of our work, none of which, we feel must be specifically highlighted here.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_JGkxKRJ">
					<idno type="grant-number">6-13</idno>
				</org>
				<org type="funding" xml:id="_pPRwtzN">
					<idno type="grant-number">2022</idno>
				</org>
				<org type="funding" xml:id="_hfK7Acm">
					<idno type="grant-number">FA8655-22-1-7155</idno>
				</org>
				<org type="funding" xml:id="_fMhmjTr">
					<idno type="grant-number">01IS18039B</idno>
					<orgName type="grant-name">Tübingen AI Center</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>and there exists a ball B(x 0 , δ), where x 0 ∈ R n and δ &gt; 0, such that P and P ′ induce the same measure on B(x 0 , δ), then P ≡ P ′ .</p><p>The original proof follows from the identity theorem for real analytic functions. We extend this result to the case of potentially degenerate multivariate normal variables, which we call (De)-MVNs. We first propose an intermediate result for the case in which only one of the variables is a (degenerate) multivariate normal, while the other variable is a non-degenerate multivariate normal. We then use this result to prove the general case in which both variables are potentially degenerate MVNs.</p><p>Lemma B.8 (Identifiability of a (De)-MVNs and a non-degenerate MVN). Consider a pair of random vectors X, X ′ in R n distributed as X ∼ N (µ, Σ) and</p><p>for appropriate values of µ, µ ′ and where the determinant |Σ| ≥ 0 and the determinant |Σ ′ | &gt; 0. In other words, X is a potentially degenerate MVN, while X ′ is a non-degenerate MVN.</p><p>If there exists a ball B(x 0 , δ) ⊆ R n , where x 0 ∈ R n and δ &gt; 0, such that X and X ′ follow the same distribution on B(x 0 , δ), then X ≡ X ′ , i.e.,(µ, Σ) = (µ ′ , Σ ′ ).</p><p>Proof. Let the rank of Σ be k ≤ n and consider the spectral decomposition of Σ:</p><p>where Q is an orthogonal n × n matrix and D a the diagonal matrix. If n = k we consider D to have k diagonal entries (σ 2 1 , σ 2 2 , . . . , σ 2 k ) where σ i for i ∈ [k] are the eigenvalues. Otherwise, if k &lt; n, then D has n diagonal entries (σ 2 1 , σ 2 2 , . . . , σ 2 k , 0, . . . , 0) where σ i for i ∈ [k] are the eigenvalues. Let Y = Q T X and Y ′ = Q T X ′ . Since Q is an orthogonal matrix, this means that</p><p>Since we know X ≡ X ′ in B(x 0 , δ), then we can derive that Y ≡ Y ′ in B(Q T x 0 , δ) for an appropriate δ &gt; 0. We project B(Q T x 0 , δ) into two subspaces, B(Q T x 0 , δ) 1:k and B(Q T x 0 , δ) k+1:n . The first captures the first k dimensions of the ball, and the second the last (nk) dimensions.</p><p>We can pick the first k dimensions of Y and Y ′ , and denote them as Y 1:k and Y ′ 1:k respectively. The first k dimensions of both variables are still the same, i.e., Y 1:k ≡ Y ′ 1:k in B(Q T x 0 , δ) 1:k . We can show that Y 1:k is a non-degenerate multivariate normal, because its covariance matrix D 1:k,1:k is full rank. Since both Y 1:k and Y ′ 1:k are non-degenerate multivariate normals, by Theorem B.7 by <ref type="bibr">(Kivva et al., 2022)</ref> we have Y 1:k ≡ Y ′ 1:k . We will now prove by contradiction that Y is also a non-degenerate MVN, i.e., that k = n. We consider the other (nk) dimensions of Y and Y ′ . The covariance matrix of Y k+1:n is D k+1:n,k+1:n , which is a zero matrix. However, since determinant |Σ ′ | &gt; 0, the variance of any component of Y ′ k+1:n cannot be 0. Since Y k+1:n ≡ Y ′ k+1:n in the ball B(Q T x 0 , δ) k+1:n , their covariance matrices should be the same. We now come to a contradiction, because one is supposed to be a zero matrix, while the other one is supposed to be full rank. We therefore derive that k = n, and hence</p><p>We can now exploit that X = QY and X ′ = QY ′ , due to the orthogonality of Q, and conclude that X ≡ X ′ . Lemma B.9 (Identifiability of (De)-MVNs). Consider a pair of random vectors X, X ′ in R n distributed as</p><p>for appropriate values of µ, Σ, µ ′ , Σ ′ , including also singular Σ and Σ ′ . If there exists a ball B(x 0 , δ) ⊆ R n , where x 0 ∈ X , δ &gt; 0 and X is support of X, such that X and X ′ follow the same distribution on B(x 0 , δ), then X ≡ X ′ , i.e.,(µ, Σ) = (µ ′ , Σ ′ ).</p><p>Intuitively, this means if y i = 0, this must cause z i = 0.</p><p>Alternatively, let s ⊆ [n] be the index set of nonzero elements in y, we can define</p><p>From the perfect reconstruction constraint (4), we can derive</p><p>by first substituting X = f (Z), then applying the law of total expectation and finally using the fact that the sum of squares is a positive function. Finally Y is a discrete random variable, in this case P Y -almost everywhere means everywhere on its support. We now denote v := g • f : R n → R n . Then, following (56), we have for any value y ∈ Y we have</p><p>This means that for the data that satisfy Y = y, f (Z) and f (v(Z)) are equal P Z|Y -almost everywhere, which implies f (Z) and f (v(Z)) are equally distributed. Since by assumption Z and v(Z) = g(X) are potentially degenerate MVNs, by Theorem B.5, there exists an invertible affine transformation h y : R n → R n such that</p><p>This proves that for data coming from the same mask Y = y (potentially unknown), we can identify the masked causal variables mixed through a piecewise linear function up to a linear transformation. Proof. First, for the simplest case, if rank(Σ) = 0, then, Z is a single point {µ}, and f is affine over Z.</p><p>If rank(Σ) &gt; 0, by the definition of (De) MVN, we know that Z = Aε + b, where ε ∼ N (0, I) with dimension d ε , and A ∈ R n×dε has full column rank. This implies that Z = colsp(A) + b.</p><p>The ≡ symbol denotes that</p><p>is equal to ε in distribution, which means it is not necessarily the same in each point of the support.</p><p>We can see that</p><p>where we use the fact that sin and sinh are odd, while cos is even. An analogous argument shows that</p><p>We can also easily show that f (0) = 0. The above shows that, for all z ∈ R 2 , ||f (z)|| 0 ≤ ||z|| 0 . By taking the expectation on both sides we get the desired result: E||f (Z)|| 0 ≤ E||Z|| 0 and thus E||g(X)|| 0 ≤ E||Z|| 0 .</p><p>However, we can see in in Figure <ref type="figure">4</ref> and in the computation below, f -1 • f = f is not a permutation composed with an element-wise invertible transformation on R 2 .</p><p>E. Full experimental results with the permutation π between ground truth latent variables Z and the estimator Ẑ = g(X). One figure represents one ablation study in Table <ref type="table">1</ref>. We choose one random seed to plot for each setup. Ground truth Pear. Corr. matrix on the left shows the original linear correlation inside Z, compared with the estimator on the right-hand side.  To exclude the natural dependency between latent variables that potentially enhance the value of metric MCC, we provide additional experiment results with independent latent variables using 20 random seeds in Table <ref type="table">5</ref>. In Table <ref type="table">6</ref> and Table <ref type="table">7</ref>, we provide the numerical results of averaged MCC and standard deviation over 20 random seeds as shown in Figure <ref type="figure">2</ref>. In the second block of Table <ref type="table">6</ref>, we provide two additional results of our method (training without mask information) when Z are independent. In Table <ref type="table">7</ref>, the second right column is for training without mask information, and the most right column is the oracle method, i.e., providing mask information in the training phase. Additionally, to exclude the natural dependency between latent variables that potentially enhance the value of metric MCC, we provide additional experiment results with independent latent variables in Table <ref type="table">8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1.3. TEST ON INDEPENDENT LATENT VARIABLES</head><p>To exclude the effect of the inherent causal relation, which might increase the value of metric MCC, we additionally test our estimated encoder g when the unmasked causal variables are independent with each other, and implement the same ablation study as Table <ref type="table">1</ref> for linear mixing function and Table <ref type="table">7</ref> for piecewise linear mixing function. As we can see, in Table <ref type="table">9</ref>, comparing the right two columns, there is no significant difference, which provides evidence that the MCC obtained by our method does not come from the inherent causal relation among latents.   In this section, we evaluate how can we use the latent causal variables we learn as input to causal discovery methods in order to learn the causal structure. In causal representation learning often the main difficulty is to identify the causal variables from the observations, while we can then use standard causal discovery methods to estimate an equivalence class of graphs.</p><p>In this case, intuitively we expect that the closer the estimated causal variables are to the ground truth, the closer the learned graph to the ground truth causal relations.</p><p>Since masking might also mask some causal relations in the original graph, we instead consider a group of data without masked variables and then compare the learned graph to the ground truth. For this task, we additionally assume a few standard assumptions in causal discovery <ref type="bibr" target="#b10">(Spirtes et al., 2000)</ref>, e.g. the causal Markov assumption and the causal faithfulness assumption, which together imply that conditional independences in the data correspond to d-separations in the true underlying causal graph. For simplicity, we also assume causal sufficiency, which in this case means that there are no additional latent confounders of the reconstructed latent variables, and there is no selection bias.</p><p>We consider a set of different algorithms for different underlying causal models, based on the parametric assumptions we use in each setting: While LiNGAM provides in output a Directed Acyclic Graph (DAG), which can be easily compared with the ground truth DAG, the PC algorithm instead outputs a Completely Partially Oriented DAG (CPDAG). In this case, we consider the ground truth CPDAG (the CPDAG of the Markov Equivalence Class that contains the ground truth DAG).</p><p>We evaluate the Structural Hamming distance (SHD) with respect to the ground truth causal graph for LiNGAM and to the ground truth CPDAG for PC. For each setting we consider two distances:</p><p>• SHD z * : the DAG or CPDAG learned with the ground truth causal variables z * • SHD ẑ the DAG or CPDAG learned with the estimated causal variables ẑ</p><p>Since our primary objective is to recover the causal variables, the graph learned with the ground truth Z is the optimal result we can attain. Therefore we calculate the difference between these two SHDs and we use ∆ SHD to denote it. Smaller ∆ SHD implies a closer estimated graph to the optimal one.</p><p>We provide results for our method for linear mixing functions in Table <ref type="table">13</ref> and for piecewise linear mixing functions in Table <ref type="table">14</ref>. We additionally provide results for the oracle version of our method, which uses the known masks for both settings in Table <ref type="table">15</ref>. All of these results show a negative correlation between higher MCC (and hence more accurate estimated representations) and lower ∆ SHD (and hence more accurate estimated causal graphs w.r.t. to the standard causal discovery setting). This is even clear in the scatterplot for the linear case, shown in Fig. <ref type="figure">29</ref>. This aligns with the intuition that more well-identified representations tend to lead to graph learning that closely mirrors the one learned from ground truth.  <ref type="bibr">, 2021)</ref>. In Group 1, the variable object hue is masked to a constant. In Group 2, the variable background hue is masked to a constant.</p><p>Since as we show in Table <ref type="table">3</ref>, we did not observe significant performance differences between different object classes, we conduct an ablation study of the masking value δ only on OBJECT CLASS ID 0. We show the results in Table <ref type="table">16</ref>.</p><p>Consistent with the observation in numerical experiments, when we increase the value of δ, which measures the distance between mask value and original mean, our method achieves better identification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Image credits</head><p>The images in Fig. <ref type="figure">1</ref> are adapted from a VectorPortal image that is covered by CC BY 4.0. We segmented the cars, and removed some of them or moved their position in the image. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Identification of partially observed linear causal models: Graphical conditions for the non-gaussian and heterogeneous cases</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niels</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="22822" to="22833" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised representation learning with sparse perturbations</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">S</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="15516" to="15528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interventional causal representation learning</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divyat</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="372" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multidomain causal representation learning via weak distributional invariances</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In Causal Representation Learning Workshop at NeurIPS 2023, 2023b</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Identifying linearly-mixed causal representa-tions from multi-node interventions</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmi</forename><surname>Ninad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.02695</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly supervised causal representation learning</title>
		<author>
			<persName><forename type="first">Johann</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pim</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="38319" to="38331" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bryon Aragam, Bernhard Schölkopf, and Pradeep Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goutham</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elan</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Triad constraints for learning causal structure of latent variables</title>
		<author>
			<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Independent component analysis, a new concept?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Linear Algebra</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Friedberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Insel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Spence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Pearson Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge MA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Linear causal disentanglement via interventions</title>
		<author>
			<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Seigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salil</forename><surname>Bhate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">40th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unpaired multi-domain causal representation learning</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Sturma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Variational sparse coding</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Tonolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjørn</forename><surname>Sand Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roderick</forename><surname>Murray-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="690" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Score-based causal representation learning from interventions: Nonparametric identifiability</title>
		<author>
			<persName><forename type="first">Burak</forename><surname>Varici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Acartürk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Tajer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Causal Representation Learning Workshop at NeurIPS 2023</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">kpcalg: Kernel pc algorithm for causal structure detection</title>
		<author>
			<persName><surname>Verbyla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Desgranges</surname></persName>
		</author>
		<author>
			<persName><surname>Wernisch</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=kpcalg" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>r package version</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Self-supervised learning with data augmentations provably isolates content from style. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Julius Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><surname>Locatello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="16451" to="16467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonparametric identifiability of causal representations from unknown interventions</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Julius Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendong</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Kekić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Causal component analysis</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wendong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><surname>Kekić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Julius Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalized independent noise condition for estimating latent variable causal graphs</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14891" to="14902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Identification of linear nongaussian latent hierarchical structure</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangbo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="24370" to="24387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multi-view causal representation learning with partial observability</title>
		<author>
			<persName><forename type="first">Dingling</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danru</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perouz</forename><surname>Taslakian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Martius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.04056</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning temporally causal latent processes from general temporal data</title>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuewen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changyin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Identifiability guarantees for causal disentanglement from soft interventions</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristjan</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dags with no tears: Continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Pradeep K Ravikumar</surname></persName>
		</author>
		<author>
			<persName><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalizing nonlinear ica beyond structural sparsity</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="13326" to="13355" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the identifiability of nonlinear ica: Sparsity and beyond</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="16411" to="16422" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
