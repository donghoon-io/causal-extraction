<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Monsieur THIERRY ARTIERES</title>
				<funder ref="#_Xfz8BEW">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_EF55G48">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>ECOLE CENTRALE DE MARSEILLE</roleName><forename type="first">Rapporteur</forename><surname>Professeur</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DOCTEUR-INGENIEUR</orgName>
								<orgName type="institution">CENTRE DE RECHERCHE FACEBOOK - PARIS</orgName>
								<address>
									<country>Examinatrice</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Camille</forename><surname>Madame</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DOCTEUR-INGENIEUR</orgName>
								<orgName type="institution">CENTRE DE RECHERCHE FACEBOOK - PARIS</orgName>
								<address>
									<country>Examinatrice</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Couprie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DOCTEUR-INGENIEUR</orgName>
								<orgName type="institution">CENTRE DE RECHERCHE FACEBOOK - PARIS</orgName>
								<address>
									<country>Examinatrice</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Monsieur THIERRY ARTIERES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generative modelling</term>
					<term>unsupervised learning</term>
					<term>generative adversarial networks</term>
					<term>convolutional neural networks</term>
					<term>variational autoencoders</term>
					<term>computer vision</term>
					<term>machine learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This thesis explores the topic of generative modelling of natural images, which is the task of fitting a data generating distribution. Such models can be used to generate artificial data resembling true data, or to compress images. We focus on the class of latent variable models which seek to capture the main factors of variations of an image into a variable that can be manipulated. In particular we build on two successful latent variable generative models, the generative adversarial network (GAN) and the variational autoencoder (VAE).</p><p>Recently, GAN models significantly improved the quality of images generated by deep models, obtaining very compelling samples. Unfortunately these models struggle to capture all the modes of the original distribution, i.e., they do not cover the full variability of the dataset. Conversely, likelihood based models such as VAEs typically cover the full variety of the data well and provide an objective measure of coverage. However, these models produce samples of inferior visual quality that are more easily distinguished from real ones. The work presented in this thesis strives for the best of both worlds: to obtain compelling samples while modelling the full support of the distribution. To achieve that, we focus on i) the optimisation problems, and ii) practical model limitations that hinder performance.</p><p>The first contribution of this thesis is a deep generative model that encodes global image structure into latent variables, built on VAEs, and autoregressively models low level detail. We propose a training procedure relying on an auxiliary loss function to control what information is captured by the latent variables and what information is left to an autoregressive decoder. Unlike previous approaches to such hybrid models, ours does not restrict the capacity of the autoregressive decoder to prevent degenerate models that ignore the latent variables.</p><p>The second contribution builds on the standard GAN model, which trains a discriminator network to provide feedback to a generative network. The discriminator assesses the quality of individual samples, which makes it hard to evaluate the variability of the entire set of generated data. Instead, we propose to feed the discriminator with batches that mix both true and generated samples, and train it to predict the ratio of true samples in the batch. These batches work as approximations of the distribution of generated images and allow the discriminator to approximate statistics of the data distribution. We introduce an architecture that is well suited to solve this problem efficiently, and show experimentally that our approach reduces mode collapse in GANs on two synthetic datasets, and obtains good results on the CIFAR10 and CelebA datasets.</p><p>The mutual shortcomings of VAEs and GANs can in principle be addressed by training hybrid models that use both types of objectives. In our third contribution, we show that usual parametric assumptions made in VAEs induce a conflict between them, leading to lackluster performance of hybrid models. We propose a solution based on deep invertible transformations, that learns a feature space in which usual assumptions can be made without weakening performance. Our approach provides likelihood computations in the image space while being able to take advantage of adversarial training. It obtains GAN-like samples that are competitive with fully adversarial models while improving likelihood scores over recent hybrid models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Résumé</head><p>Cette thèse explore le sujet des modèles génératifs, appliqués aux images naturelles. Cette tâche consiste a modéliser la distribution des données observées, et peut permettre de générer des données artificielles semblables aux données d'origine, où de compresser des images. Nous nous focalisons sur les modèles à variables latentes, qui cherchent a résumer les principaux facteurs de variation d'une image en une variable qui peut être manipulée. En particulier, les contributions proposées sont basées sur deux modèles génératifs a variable latentes: le modèle génératif adversarial (GAN) et l' encodeur variationel (VAE).</p><p>Récemment, les GAN ont significativement amélioré la qualité des images générées par des modèles dits 'profonds', obtenant des images très convaincantes. Malheureusement ces modèles ont du mal à capturer tous les modes de la distribution d'origine, i.e. ils ne couvrent pas les données dans toute leur diversité. A l'inverse, les modèles basés sur le maximum de vraisemblance tels que les VAEs couvrent typiquement toute la variabilité des données; en outre, ils offrent un moyen objectif de mesurer cela. Malheureusement ces modèles produisent des échantillons de qualité visuelle inférieure, qui sont plus faciles à distinguer de vraies images. Le travail présenté dans cette thèse a pour but d'obtenir le meilleur des deux mondes: des échantillons de bonne qualité tout en modélisant tout le support de la distribution. Pour arriver à cela, nous nous focalisons sur i) les problèmes d'optimisation et ii) les limitations pratiques des modèles qui heurtent leur performance.</p><p>La première contribution de ce manuscrit est un modèle génératif profond qui encode la structure globale des images dans une variable latente. Ce modèle est basé sur le VAE, et utilise un modèle autoregressif pour capturer les détails de bas niveau. Nous proposons une procédure d'entrainement qui utilise une fonction de cout auxiliaire pour contrôler quelle information est capturée par la variable latent et quelle information est laissée à un décodeur autoregressif. Au contraire des précédentes approches pour construire des modèles hybrides de ce genre, notre modèle de nécessite pas de contraindre la capacité du décodeur autoregressif pour éviter d'apprendre des modèles dégénérés qui ignorent la variable latente.</p><p>La deuxième contribution est bâtie sur le modèle du GAN standard, qui s'appuie un discriminateur pour guider le modèle génératif. Le discriminateur évalue généralement la qualité d'échantillons individuels, ce qui rend la tache d'évaluer la variabilité des données difficile. A la place, nous proposons de fournir au discriminateur des ensembles de données, par 'batches', qui mélangent des vraies images et des images générées. Nous l'entrainons à prédire le ratio de vrais et de faux éléments dans l'ensemble. Ces batches servent d'approximation de la vrai distribution des images générées et permettent au discriminateur d'approximer des statistiques sur leur distribution. Nous proposons une architecture qui est bien adaptée pour résoudre ce problème efficacement, et montrons expérimentalement que notre approche réduit l'oubli de mode des GAN sur deux jeux de données synthétiques et obtient de bons résultats sur les datasets CIFAR10 et CelebA.</p><p>Les lacunes mutuelles des VAEs et des GANs peuvent, en principe, être réglées en entrainant des modèles hybrides qui utilisent les deux types d'objectif. Dans notre troisième contribution, nous montrons que les hypothèses paramétriques habituelles faites par les VAE produisent un conflit entre les deux, menant à des performances décevantes pour les modèles hybrides. Nous proposons une solution basée sur des modèles profonds inversibles, qui entraine un espace de features dans lequel les hypothèses habituelles peuvent être faites sans poser problème. Notre approche fourni des évaluations de vraisemblance dans l'espace des images tout en étant capable de tirer profit de l'entrainement adversaire. Elle obtient des échantillons de qualité équivalente au modèle pleinement adversaires tout en obtenant une meilleure vraisemblance, comparé aux modèles hybrides récents.</p><p>Mots-clefs : Modèles génératifs, apprentissage non supervisé, réseaux génératifs adversaires, réseaux neuronaux convolutionels, vision par ordinateur, autoencoders variationels, apprentissage automatique.</p><p>Chapter 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Several landmark areas of machine learning have seen remarkable improvement in the last several years, notably in computer vision with image classification <ref type="bibr">[Krizhevsky et al., 2012, Simonyan and</ref><ref type="bibr">Zisserman, 2015]</ref>, and segmentation <ref type="bibr" target="#b86">[Long et al., 2015]</ref>, in natural language processing <ref type="bibr" target="#b5">[Bahdanau et al., 2015</ref><ref type="bibr" target="#b139">, Sutskever et al., 2014</ref><ref type="bibr">, Vaswani et al., 2017b]</ref> audio processing [van den <ref type="bibr">Oord et al., 2016a]</ref>, or game agents <ref type="bibr" target="#b134">[Silver et al., 2016</ref><ref type="bibr">, Vinyals et al., 2019]</ref>. A well recognized catalyst for this progress has been the simultaneous availability of highly expressive models and huge datasets. The former is largely due to advancements in dedicated hardware and software and the latter to the internet, which produces vast amounts of data everyday and provides the means to crowd-source its processing. For instance, more than 300 hours of video are uploaded every minute, on average, to YouTube alone<ref type="foot" target="#foot_0">foot_0</ref> .</p><p>Similarly to classical computer programs, learning based models should accept instances from some input domain and (approximately) solve a chosen task relating to those inputs. A significant difference is that they do not require an algorist to explicitly specify rules to map the input to the solution. In particular, supervised learning works by observing numerous pairs of possible inputs and desired outputs, called labels. Mappings between the two are built by iteratively modifying some internal machinery, e.g. using gradient descent based methods <ref type="bibr" target="#b77">[LeCun et al., 1998</ref>], until the model performs well. This paradigm is a potent tool to solve tasks where: i) it is impractical for an algorist to explicitly enunciate rules to solve it, such as object recognition and ii) large amounts of data and labels can be collected.</p><p>While the input data is often easy to collect -think images on the internet -training labels are almost always expensive. For instance, training an image classifier involves asking humans to manually tag images; see Figure <ref type="figure">1</ref>.1 for examples of labels required by different supervised learning tasks. This expensive work limits the scale of the datasets that can be used and constitutes a prominent performance bottleneck on many computer vision tasks. A natural question arises in this context: can something useful be learned by observing raw data alone? Figure <ref type="figure">1</ref>.1: Classical supervised-learning tasks on natural images include image classification, object detection, semantic segmentation and instance segmentation. In that order, the supervision is ranked from weakest to strongest as each signal can be deduced from the next. These annotations provide high level summaries of the data, but their necessity limits the scale of the dataset considered. Figure adapted from <ref type="bibr">Lin et al. [2014b]</ref> 1.1 Unsupervised learning Raw data often contains a lot of information. Images, videos, text or audio clips all live in high dimensional spaces, are highly structured, and therefore constitute rich and complex signal. From the point of view of Shannon information theory <ref type="bibr" target="#b131">[Shannon, 1948]</ref>, natural images contain thousands of bits of information. In contrast labels often contain only a few bits of information, for instance in image classification. This motivates the pursuit of models that are able to exploit the information in unlabelled data. In the words of Yann LeCun: "Most of human and animal learning is unsupervised learning. If intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, <ref type="bibr">[...]</ref>. We know how to make the icing [...], but we don't know how to make the cake. We need to solve the unsupervised learning problem before we can even think of getting to true AI."</p><p>-Yann LeCun, invited talk at NeurIPS 2016.</p><p>Intuition suggests it should be possible to learn something about how the data is structured without labels. For instance observing photographs of an unknown object will allow the reader to recognize it in future photographs, though not to name it. Because a virtually unlimited amount of raw data is available for the first time in human history, this is an exciting research area with many yet unexplored corners. The goal of learning from unlabelled data is, however, an ill-posed endeavour and turning it into a problem that can be tackled requires specifications. The first is how to select or improve a model in the absence of objective targets, and the second is in what sense a model obtained this way can then be useful. The work presented in this manuscript is part of the rich topic of unsupervised learning, which aims at answering these questions.</p><p>While labels contain few bits of information, these bits are critical to defining the supervised approach. They can be seen as a compressed summary of parts of the input data as perceived by humans. This high level semantic information about the input provides a target to guide iterative improvement of the models. This is missing in the context of unsupervised learning, and it is apriori unclear how to define a useful objective for optimization. A popular approach is to extract some structure from the raw data, and use it as a target label as in a supervised approach. This approach is referred to as self-supervised training. For instance, an image can be cut into pieces, and the right arrangement used as target <ref type="bibr">[Doersch et al., 2015, Noroozi and</ref><ref type="bibr" target="#b103">Favaro, 2016]</ref>.</p><p>Figure <ref type="figure">1</ref>.2: The Jigsaw-puzzle task involves cutting images to pieces and asking a model to reorder them correctly. In the exemplar-CNN task, an original image is transformed into many variants using data augmentations and a classifier must map all variants to the same target. In both cases, some target is extracted from the raw data without requiring human annotators. Figures adapted from <ref type="bibr" target="#b28">Doersch et al. [2015]</ref> and <ref type="bibr" target="#b32">Dosovitskiy et al. [2014]</ref> respectively.</p><p>Another possibility is to obtain many variants of each image using data augmentation and asking that a model classify all of them together. Both approaches are illustrated in Figure <ref type="figure">1</ref>.2. They make seemingly arbitrary choices about the target signal being extracted. It is possible to not make any such choice by defining the task as "being able to predict the data". This is the high level idea of generative modelling and is the setting of the work presented in this manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Generative modelling</head><p>In generative modelling each data point is seen as the realisation of some random experiment. A simple example is to toss a dice many times, the data being the collection of values obtained. Similarly, natural images collected over the internet by a web crawler can be seen as realisations of some more complex random phenomenon. In general, the goal is to learn a probability density model as similar as possible to the data generating distribution. This kind of model can have direct applications such as data compression <ref type="bibr" target="#b54">[Huffman, 1952]</ref> or data generation. It is also reasonable to assume that to perform well, such a model needs to build rich, abstract representations of the data.</p><p>The ability to predict the outcome of a random experiment with low uncertainty requires a form of understanding of it. For instance, a human that has good knowledge of the English language will be able to predict missing letters from a text with much better accuracy than random <ref type="bibr" target="#b132">[Shannon, 1951]</ref>. This is because high level understanding of English implicitly provides the reader with a low entropy model. A similar experiment can be to remove a few patches from an image, and to ask a human to predict their content. An accuracy much better than random will be achieved, owing to good understanding of how natural images are likely to be structured. Fitting the data generating distribution requires the model to understand the data, and build usefull representations of it. These representations can then be used to solve other tasks, for instance by first training them with huge amounts of unlabelled data, then refining them using smaller amounts of labelled data. In this case the philosophy is that the abstractions learned by the generative model should be better representations of the data than its raw representation in the input space, and it can thus be called representation learning, see e.g. <ref type="bibr" target="#b9">Bengio et al. [2012]</ref> for an overview.</p><p>The work presented in this manuscript is focused on modelling natural images. Though most Figure <ref type="figure">1</ref>.3: A simple distribution (depicted on the left by a level set of a unit Gaussian) is reshaped using a deep network into a more complex density. The generated and target distributions are then compared to each other, and this loss is used to improve the model.</p><p>of the discussion applies to other types of data, examples will be focused on images from now on. A natural image is composed of the scalar RGB values of many pixels, and the phenomena that control the relations between them are complex: for instance perspective, lighting or mechanics. These phenomena are too complex to be specified by hand and the goal is to use machine learning to model them. The probabilistic approach to this problem is to view the model as a distribution in image space. That distribution has to specify which combination of scalar values are likely to happen together in an image and which are not. This works by starting from a much simpler distribution, called a prior, over some space and using the observed data to learn a mapping that reshapes the prior into a more complex distribution as illustrated by Figure <ref type="figure">1</ref>.3.</p><p>A model of the raw data can be leveraged in other problems. Many supervised machine learning problems can be formulated probabilistically as trying to maximize the probability of observing some target outcome y, given some input data x. Denoting x as the input data and y the target label, one typically seeks to maximize the probability y given x under some model, p(y|x). Bayes' law <ref type="bibr" target="#b7">[Bayes, 1763]</ref> can be invoked to see how the predictions p(y|x) relate to generative modelling: p(y|x) = p(x|y)p(y) p(x) .</p><p>(1.1)</p><p>This yields the conditional generative modelling task of fitting p(x|y), using maximum likelihood estimation and labelled data, and the prior p(y) can also be fitted using the labels. Unlabelled data can be leveraged by optimising the marginal p(x) e.g. by gradient descent, to train p(x|y).</p><p>In terms of implementation, adding some conditioning to an unconditional generative modelling is typically easy; it can be done, for instance, by giving some extra inputs. To train the conditional model, labels are required. Thus this Bayesian view shows how to train simultaneously on labelled and unlabelled data, a problem called semi-supervised learning. Interestingly, modern deep-learning techniques enabled training of very complex priors over types of data that could not previously be considered.</p><p>Applications of generative models. Direct practical applications include information compression, which is one of the backbones of telecommunications. The intuition behind compression algorithms is that very probable events should be associated to short messages, while rare events should be transmitted with longer messages. Implementing this requires a density model of the data being transmitted and is thus a prime application of density estimation. Recent developments allow training of such model on very complex data, and thus hold the promise of compression gains, which can be very meaningful for telecommunications. Another direct and useful application is to generate real looking artificial data, which is possible if the learned generative model provides a way to sample new data from it. For instance, the goal can be to generate real looking image data to flesh out a virtual environment, after training on real scenes. This is useful in cinema and video games, and may become a crucial aspect of virtual worlds. More down to earth, generative modelling is also a good sandbox research problem for machine learning. Indeed, an almost infinite amount of low-cost data is available, and because the signal being fitted is rich and complex, over-fitting is unlikely to be an issue in the foreseeable future. This means that in this context the bottleneck will lie with model flexibility rather than data acquisition, and this is ideal for research.</p><p>Representation learning. Many self-supervised objectives can be seen as restoring the input given parts of it, so it is conceptually similar to generative modelling. With self-supervised pretext tasks, the targets can have far fewer bits of information than in generative modelling. In that sense, it appears more similar to the final regression tasks, as part of the information in the input can be discarded and more salient information kept to solve the task. However, because the pretext task in self-supervised learning is different from the final one, the information being discarded as irrelevant for the pretext task may be important for the target one. In contrast, the target of generative modelling is the data itself, and it is thus a very rich target with all the bits of information available. The model has to fit everything in the data to be optimal, and cannot discard less salient information. The representations learned may be more generic, and this forms a motivation for exploring generative modelling as a tool for representation learning. Figure <ref type="figure">1</ref>.4 provides images samples from an unsupervised generative model that demonstrate the ability of the model to capture complex dependencies in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Challenges in generative modelling</head><p>Training a model to fit a data distribution requires defining in what sense the model should match the target. A first guiding principle is that the model should be likely to generate the data that was actually observed when collecting the dataset. This is our best guess of what the model should produce and thus constitutes a reasonable target. If the model is likely to generate any point in the dataset then in some sense the model has captured the phenomenon in its entirety. A second principle is that the model should be able to generate artificial data that is realistic, but not a copy of a point in the training set, which shows that the model has learned how the data is structured. This approach requires defining a measure of quality for a sample, dependent on the observed Figure <ref type="figure">1</ref>.4: The images in the bottom grid are all generated by a generative model trained in an unsupervised manner. Samples can be conditioned on some desired style (skin colour, hair colour, hair texture) and on high level semantic information (apparent gender, head shape and pose, presence of glasses). The model is able to learn these complex notions by observing vast amounts of unlabelled data. Figure slightly adapted from <ref type="bibr" target="#b61">Karras et al. [2019]</ref> data. Both principles have their advantages and drawbacks, and are in some sense complementary. We now discuss some challenges in generative modelling that will be of core interest to the work presented in the rest of the manuscript.</p><p>Over-generalisation. Natural images are too complex for artificial models to capture their structure in all its finesse. Therefore training a model to cover all points in the observed data pushes it to accept wider sets of input than it should, in order to miss nothing. This phenomenon is illustrated in Figure <ref type="figure">1</ref>.5. The lack of flexibility can stem from problematic assumptions, in particular on the loss being used. Given a target image, the model prediction will be imperfect, so the evaluation requires a notion of distance between images. Intuitively, the most common distances define concentric spheres around training points, and consider that all points on the same sphere are equally good approximations -the closer to the center the better. Such a distance is called isotropic and leads the model to assign probability to unrealistic data points. Note that autoregressive models sequentially compare pixels in the target and prediction, and thus there is no need for isotropic distances. These models still over-generalise, because of the incentive from the training criterion to not miss any data point, but loss design is less problematic in that context. A core focus of the work presented in this manuscript is to mitigate over-generalisation. This is achieved by i) foregoing isotropic distances and ii) leveraging other training paradigms.</p><p>Mode-dropping. The second principle to train generative models is to optimize the quality of generated data. It is achieved by sampling data points from the model and comparing these Figure <ref type="figure">1</ref>.5: A model over-generalises if it covers the support of the true data distribution well, but also covers parts of the space that it should not, as depicted on the left. Conversely, a model that fails to cover the full support of the data distribution is said to mode-drop, as depicted on the right.</p><p>samples to real data. This approach is taken by the Generative Adversarial network model (GAN) <ref type="bibr" target="#b42">[Goodfellow et al., 2014]</ref>. Here the evaluation metric is trickier to design, as there is not a single candidate to which the sample should be compared. An important aspect is that the variability in the generated data must also be considered. Indeed otherwise memorising a single image of high quality solves the problem, but this solution is not desirable. A much studied pitfall of GANs is that it is hard to obtain a model that fits the full variety of the data, a problem called mode-dropping.</p><p>Posterior collapse. One way to view the mapping from the prior distribution to the generated one (see Figure <ref type="figure">1</ref>.3) is to see the variable as "causes" that determine the content of an image, and the generator as a mapping from causes to results. For tractability, it is beneficial to also have a model that performs the reverse operation, i.e. that maps an image to a possible explanation, which is referred to as an encoder. This is the core idea of the successful Variational auto-encoder model <ref type="bibr">(VAE)</ref>. A promising solution to tackle over-generalisation is to use an expressive type of decoder called an autoregressive model. These models do not require isotopic distances, and have a lot of complementaries with VAEs. However, in practice autoregressive decoders do not work well in conjunction with encoders. This is due to the fact that latent variables have a cost in terms of likelihood estimation. In the presence of sufficiently powerful autoregressive decoders it is easier and potentially even optimal not to use latent variables, a phenomenon known as posterior collapse.</p><p>Hybrid models. The two learning principles presented here, maximizing the probability of observed data and optimizing sample quality, intertwine and are complementary. Ideally, a model should simultaneously satisfy both types of evaluation: if a model fits the true distribution well, it both covers all the variety of the data, and produces high quality samples. In practice, optimizing both types of objectives together is challenging, and is another focus of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Contributions to generative modelling</head><p>The contributions presented in this manuscript focus on the problems of over-generalisation, mode-dropping, posterior collapse, and successful training of hybrid models. These problems intertwine, and they all relate to the goal of obtaining models that cover the full variety of the data while producing compelling samples. To put contributions into context, sections 2.1 to 2.8 will introduce several successful approaches in generative modelling. A discussion on some limitations of these models is then given in Section 2.9. This is followed by refinements on the basic models in sections 2.10 to 2.11, and a presentation of evaluation procedures in Section 2.12 to conclude the background section.</p><p>Variational autoencoders with autoregressive decoders. These models are among the most successful approaches to generative modelling of natural image collections. These approaches have complementary strengths as VAEs handle global structure well, while autoregressive models excel at modelling local image statistics. This motivates hybrid models that encode global image structure using an encoder while autoregressively modeling low level detail. Naive constructions of this type unfortunately yield models that are unable to use the encoder. In Chapter 3, we propose a training procedure relying on an auxiliary loss function that controls which information is captured by the encoder and what is left to the autoregressive decoder. Unlike previous approaches to such hybrid models which restricted the capacity of the autoregressive decoder, we are able to use arbitrarily expressive decoders. Our approach achieved state-of-the art quantitative performance among models with latent variables at the time of publication, and generates qualitatively convincing samples.</p><p>Training GANs by assessing batches of data. Generative adversarial networks evaluate the quality of generated samples using a discriminator network that distinguishes between real and generated images. In existing models the discriminator assesses individual samples. This prevents the discriminator from accessing global distributional statistics of generated samples and leads the generator to mode-drop. In Chapter 4 we propose to use the discriminator to assess batches of data that mix both true and fake samples. The task of the discriminator becomes to predict the proportion of real and generated images in the batch. By doing that, variability in the generated data becomes something that can be explicitly evaluated by the discriminator. We show that our approach reduces mode collapse in GANs on two synthetic datasets, and obtains good results on the CIFAR10 and CelebA datasets, both qualitatively and quantitatively.</p><p>Adversarial and maximum likelihood hybrid training. In Chapter 5, we show that parametric assumptions commonly made about the output density of maximum-likelihood based models are a source of tension with adversarial training, making successful hybrid models non trivial. We propose using an invertible model to learn an abstract feature space to which targets and predictions are mapped. In that space the distance between them can be computed without hindering hybrid training. We show that compared to existing hybrid models, our model offers improved sample quality and likelihood scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Publications</head><p>• Chapter 3 is based on the paper "Auxiliary Guided Autoregressive Variational Autoencoder", Thomas Lucas &amp; Jakob Verbeek, European Conference on Machine Learning (ECML) 2018. See <ref type="bibr">[Lucas and Verbeek, 2018b]</ref>.</p><p>• Chapter 4 is based on the paper "Mixed batches and symmetric discriminators for GAN training", Thomas Lucas, Corentin Tallec (equal contribution), Jakob Verbeek and Yann Ollivier, International Conference on Machine Learning (ICML) 2018. See <ref type="bibr">[Lucas et al., 2018b]</ref> • Chapter 5 is based on the paper "Adaptive Density Estimation for Generative Models", Thomas Lucas, Konstantin Shmelkov (equal contribution), Karteek Alahari, Cordelia Schmid, and Jakob Verbeek, conference on Neural Information Processing Systems (NeurIPS) 2019. See <ref type="bibr" target="#b93">[Lucas et al., 2019]</ref>.</p><p>• A research project initiated during my master internship and completed during the first semester of this Ph.D, led to the paper "Areas of Attention for Image Captioning", Marco Pedersoli, Thomas Lucas, Cordelia Schmid and and Jakob Verbeek, International conference on computer vision (ICCV) -see also <ref type="bibr" target="#b107">[Pedersoli et al., 2017]</ref>. It proposes an attention mechanism for image captioning -the task of describing input images with sentences -that allows the model to attend relevant parts of the image while describing it. This requires high level representations of complex image content, as with image generation. However because the topic differs from the other contributions, we left it out of the main body of the manuscript and it can be found in Appendix B.</p><p>Chapter 2</p><p>A primer on deep generative modelling In this chapter some background on generative modelling is first recalled, which also serves to set the notations for the rest of the dissertation. Latent variable generative models are introduced in sections 2.2 and 2.3, starting from linear models such as the Gaussian Mixture Model. The extension to deep generative models is presented in Section 2.4, with the main difficulties that arise when going to the deep regime. The foundations of existing approaches to deep generative modelling are discussed in sections 2.5 to 2.8. This is sufficient to discuss, in Section 2.9 some of the challenges that will be presented in the rest of the dissertation. We then go into a more detailed presentation of existing work in Section 2.10 and Section 2.11. Finally, we discuss the evaluation of generative models in Section 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Introduction to generative modelling</head><p>The goal of unconditional density modelling is to learn the distribution p * underlying samples provided in a dataset D = {x 1 , . . . , x M } and seen as realisations of a random variable X on some space X , with density at x equal to p * (X = x) 1 . In what follows, the dataset is composed of N -dimensional real valued vectors, x ∈ R N . Thus, X ⊂ R N and R N is called the ambient space. In practice, we focus on natural images datasets, and a vector x contains per pixel intensity values across colour channels. The parametric approach to density estimation is to select a family of parametric densities, P Θ = {p θ |θ ∈ Θ}, with Θ the set of admissible parameters. Learning then seeks to select the 'best' parameter θ * in Θ, which requires a measure of performance for θ.</p><p>Since the goal is to recover p * , this measure, or "loss", will typically be, or behave like, a distance between p θ and p * , and can be denoted L(p θ , p * ).</p><p>There are different approaches to designing the loss L. We will focus on unconditional density modelling, meaning that the estimator θ for θ * is obtained from unlabelled data. Intuitively, there are two things we expect from a "good" model:</p><p>• The model p θ assigns high density to samples taken from the true distribution p * :</p><formula xml:id="formula_0">x ∼ p * (x) =⇒ p θ (x) is "high".</formula><p>• Samples taken from the model p θ behave similarly to real samples from p * :</p><formula xml:id="formula_1">x ∼ p θ (x) =⇒ p * (x) is "high".</formula><p>These two properties are closely coupled through the fact that a density model is normalised.</p><p>However, focusing on one goal or the other leads to different choices for L(p θ , p * ), and different behaviours for the model p θ . This distinction coarsely separates existing models. The first type of objective, which we will refer to as "coverage driven" is more convenient to work with as it only requires samples from p * . It is the logic behind most learning algorithms, and notably leads to maximum-likelihood-estimation (MLE). This class of models is discussed in sections 2.5, 2.6 and 2.8 and is the focus of the work presented in Chapter 3. The second type of objective, which we will refer to as "quality driven"is less obvious to design, as a straightforward implementation would require access to p * . A popular instance of this approach is the Generative Adversarial Network (GAN), which is presented in Section 2.7. GANs are also the main object of the work presented in Chapter 4. Complementarities and some conflictual aspects of coverage driven and quality driven approaches are discussed in Section 2.9.2, and are a key aspect of the work presented in Chapter 5. As regards practical experiments, this presentation is focused on modelling natural images, though in theory all models presented can be applied to other types of data. Modelling of natural images is a good sandbox research problem, as the data is complex, diverse, and at the same time easy to collect in large amounts. Such generative models also have interesting applications such as compression or automatic editing of images.</p><p>Aside from the training objective and procedures, a family of parametric densities has to be specified. The data lives in a high dimensional space, is highly non-linear and can be very diverse which makes it challenging to model. Thus, "deep" approaches are very successful: P Θ is implemented by a very flexible, over-parametrised and non-convex function approximator, and θ is selected by performing gradient descent on the loss. In sections 2.2 to 2.4, deep models and the challenges they raise are introduced. In sections 2.5 to 2.8, the main successful deep models are described, followed in Section 2.9 by limitations of these models that motivate the rest of this dissertation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generative latent variable models</head><p>High-dimensional real-valued data such as images are naturally modelled by a joint probability distribution over scalar values (in the case of images, three scalars per pixel for RGB colours channels). A popular approach to define that density is to use latent variables. Suppose that an image x ∈ R N is to be sampled from a model p θ and that we wish to sample all pixels simultaneously. To obtain an image that is globally coherent, it is clear that the pixels can not be sampled independently from each other. Indeed once one pixel is sampled, this partly determines others; if the top-left pixel belongs to a car, neighbouring ones likely belong to a car as well. That means p(x) cannot simply be decomposed as a product of marginals N i=1 p(x i ), which would be the naive way of allowing simultaneous sampling.</p><p>Latent variables can be seen as a solution to this issue. In latent variable generative models, the assumption is made that "most" of the variability in the data can be explained by a certain number of factors of variations. Such factors could be object classes and locations, or the angle taken for camera projection. When generating an image, a latent representation z is chosen first, and x is generated given z. It becomes possible to sample all pixels simultaneously, as their global coherence is controlled by z:</p><formula xml:id="formula_2">p(x|z) = N i=1 p(x i |z).</formula><p>(2.1)</p><p>This assumption is called conditional independence. With this model, most of the relationship between the pixels are captured through z, and the rest of the variability in the signal is modelled as independent per-pixel "noise". This is very different from full independence: in the extreme case, p(x|z) can tend towards a deterministic function of z, i.e. there exists some f such that</p><formula xml:id="formula_3">p(x|z) ≈ δ(x -f (z)).</formula><p>Intuitively, a latent variable generative model p θ is considered representative of the data D if for all x in D, there are some probable settings of z that are likely to generate x. Formally, assume latent variable vectors z to be realisations of a random variable Z in a high<ref type="foot" target="#foot_2">foot_2</ref> dimensional space Z, with a probability density p(z) defined over Z, then one can integrate z out to measure p(x):</p><formula xml:id="formula_4">p(x) = z∈Z p(x|z)p(z)dz (2.2)</formula><p>Given Z and p(z), a deterministic function f θ : Z → R N can be used to specify the relation between z and x, and obtain a random variable in image space by computing Y = f θ (Z). In that case p Y (x|z) = δ(x -f (z)). Such a construction is hard to use directly in practice as is unlikely to pass through all points in the dataset exactly. Unless f is already perfect there exists x such that p(x) = 0 and in turn p(D) = 0. So p(x) cannot be used to train f unless f is already perfect. We say that f θ (Z) has a degenerate support. Instead, f θ is usually used to parametrize a density that has non-degenerate support. For instance using f θ (z) as the mean of a standard Gaussian density:</p><formula xml:id="formula_5">p(x|z) = N (x|f (z); I N ). (2.3)</formula><p>Now, a single fixed z is mapped to a density which is strictly greater than 0 in the entire ambient space R N , as for any fixed (x, z), p(X = x|Z = z) &gt; 0. The Gaussian density is unimodal and has light tails , so it typically cannot cover the full data-set well<ref type="foot" target="#foot_3">foot_3</ref> . When integrating z, on the other hand, a rich mixture of densities, weighted by p(z), is obtained:</p><formula xml:id="formula_6">p(x) = z N (x|f (z); I N )p(z)dz. (2.4)</formula><p>This shows how to build complex densities over high dimensional vectors using latent variables and the conditional independence assumption to model the dependencies between each scalar dimension. An other option to build a density estimator is to drop the requirements that all scalar values x i be sampled simultaneously. Intuitively, in that case, there is no longer a need for latent variables: one can sample x 1 , look at the result, then sample x 2 given x 1 and continue.</p><p>Conditioning on some global information is no longer needed. What this costs is that sampling will be slow and sequential, and that a (possibly arbitrary) ordering needs to be introduced. This construction is discussed in Section 2.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Linear latent variable models</head><p>A simple case of latent variable generative model is when f θ is a linear function of z. A classical example is the Gaussian Mixture Model (GMM). In that case the latent variable z is a discrete one-hot vector that selects a component in a mixture of K component. Denote</p><formula xml:id="formula_7">1 k = (δ 1 k , . . . , δ K k )</formula><p>where δ j i is the Kroenecker symbol, then:</p><formula xml:id="formula_8">p(z = 1 k ) = π k , and µ k = W z + µ (2.5) p(x|z) = N (x; µ k , σI D ).</formula><p>(2.6)</p><p>In this case the integration over z is a finite sum: p(x) = z p(z)p(x|z). It is illustrated in Figure <ref type="figure" target="#fig_0">2</ref>.1, and shows how simple densities can be combined into a more complex one.</p><p>A similar recipe can be followed to obtain Probabilistic Principal Component Analysis (Probabilistic PCA <ref type="bibr" target="#b122">Roweis [1997]</ref>, <ref type="bibr" target="#b142">Tipping and Bishop [1999]</ref>). In that case, z is sampled from a standard Gaussian density, and also linearly transformed:</p><formula xml:id="formula_9">p(z) = N (z; 0, I d ) (2.7) p(x|z) = N (x; µ + Wz, σI D ).</formula><p>(2.8)</p><p>In that case p(x) = z p(z)p(x|z) has a closed form solution and is likewise Gaussian with</p><formula xml:id="formula_10">x ∼ N (x; µ, W W t + σI d ).</formula><p>(2.9)</p><p>The number of columns in W corresponds to the number of principal components that will be fitted by the model. Note that for both models, the negative log-likelihood gives the 2 "reconstruction" loss of deterministic PCA and k-means:</p><formula xml:id="formula_11">-ln p(x|f θ (z)) = ||x -f θ (z)|| 2 2 , (2.10) with f θ (z) = µ + W z. Sampling z ∼ p(z)</formula><p>and mapping it to p(x|z) yields samples x ∼ p θ (x).</p><p>Note that the mode of p(x|z) is often used as reconstruction rather than taking a sample, especially with isotropic Gaussian noise or other isotropic densities that cannot capture any structure.</p><p>(a) Mixture of Gaussians (b) Probabilistic PCA Figure <ref type="figure" target="#fig_0">2</ref>.1: (a) A Gaussian Mixture Model is a convex combination of Gaussian densities, which allows for greater flexibility. The parameters of each component can be learned, as well as the weighting coefficients. This can be seen as a latent variable model, where z selects a component:</p><formula xml:id="formula_12">p(z = 1 k ) = π k which is then linearly mapped to µ k = W z + µ. (b)</formula><p>Probabilistic PCA transforms a standard Gaussian through a linear mapping. Figures adapted from <ref type="bibr" target="#b12">Bishop [2006]</ref>.</p><p>In both cases, the Gaussian noise adds volume around (points on) a linear manifold to make the support non-degenerate in the data space. This works well for many applications, but natural images are highly complex and linear manifolds may not be flexible enough to cover their support well. Therefore a lot of volume must be added, wasting density mass and yielding poor samples and poor likelihoods. Instead, it is desirable to build more flexible, non-linear manifolds f θ to better fit complex data. This is illustrated in Figure <ref type="figure" target="#fig_0">2</ref>.2, and motivates the next section.</p><p>Figure <ref type="figure" target="#fig_0">2</ref>.2: The linear model f θ maps Z to the linear manifold (in brown) that best fits p * (x) (in grey). It is necessary to add volume (in red) around f θ (z) to obtain a non-degenerate density p θ (x|z) that does cover the support of p * . Because the data manifold is non-linear, the model is far from the target in some places and a lot of volume must be added, thus wasting probability mass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Deep generative modelling</head><p>For highly complex data such as natural images, it is natural to consider models that can learn non-linear manifold and thus more closely fit the data. The idea remains similar: a simple distribution p(z) on latent variable z, e.g. a standard Gaussian, is transformed through a non-linear function x = f θ (z) that maps latent variables to data space. For this purpose, a deep neural network can be used<ref type="foot" target="#foot_4">foot_4</ref> . It will induce a complex marginal distribution p θ (x) = z p(z)p(x|f θ (z)) when integrating out z. The non-linear manifold f θ (z) is now arbitrarily flexible and can in theory approximate any function. If enough data is provided, or if proper regularization is used to avoid over-fitting the data, very expressive networks can be considered, and closely fit the data.</p><p>Using a non-linear f θ also raises a thorny issue: the evaluation of p θ (x) becomes intractable. Indeed the integral involving non-linear deep net f θ (•) can no longer be computed in closed form.</p><p>Various solutions to side-step this problem exist, and lead to different classes of models which we will introduce in the following sections. The first solution is to use a lower bound that is tractable, as in Variational Auto-Encoders (VAE), presented in sections 2.5 and 2.10. It is also possible to constrain the parametric family for f θ so that we can compute p θ (x) for instance using a flow based method as presented in Section 2.6. Another solution is to train generative models without using the integral but rather by evaluating sample quality, as in Generative Adversarial Networks (GAN) (sections 2.7 and 2.11). Finally, it is possible to design models that do not use latent variables at all but rely on chaining many uni-variate conditional distributions instead, as discussed in Section 2.8. All of these basic models are used in various parts of this manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Variational auto-encoders</head><p>Using expressive non-linear classes of functions for f θ holds the promise of more accurate models, but also makes computations challenging. For latent variable models, one problem made more complicated by the use of deep functions is the problem of inference. Given an x and a model p θ (x|z), inference consists in finding a (set of) latent variables z that are "good explanations" for x, in the sense that they are likely to generate x. Unfortunately, p θ (z|x) is completely intractable for feed-forward networks in general. We now study how to use of an inference network to compute and optimise a lower-bound on Equation <ref type="formula" target="#formula_91">2</ref>.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Deterministic auto-encoders</head><p>We have seen in Section 2.3 that PCA can interpreted as a probabilistic model. In PCA, one seeks matrices V and W that minimize the loss <ref type="bibr" target="#b6">[Baldi and Hornik, 1989]</ref>:</p><formula xml:id="formula_13">min V,W 1 2N N n=1 ||x n -V W x n || 2 .</formula><p>(2.11)</p><p>The matrix W can be seen as projecting x into a latent space and V tries to reconstruct x from z.</p><p>To go to the non-linear regime, V and W can be replaced by deep networks g φ and f θ by stacking many linear layers with non-linearities in between. The problem then becomes:</p><formula xml:id="formula_14">min φ,θ N n=1 ||x -f θ (g φ (x))|| 2 .</formula><p>(2.12)</p><p>The first network g φ maps x to a space of lower dimensionality, and is called an encoder. The second network f θ maps the latent code z = g φ (x) to reconstruction x and is called a decoder. This construction is referred to as the auto-encoder model. It is a generalization of PCA (PCA is recovered in the special case of g φ and f θ linear) that tries to capture the main factors of variations in x in a non-linear way. It does not provide a generative model that can be sampled from, or an evaluation of likelihood. However, the decoder resembles a generative model: the L 2 loss can be seen as putting isotropic Gaussian noise around f θ , yielding a latent variable model p θ (x|z). The encoder, on the other hand, tries to infer a latent variable z for a given x i.e. to extract the main factors of variations in x. This kind of inference network will prove useful in what follows.</p><p>It is important to note that some bottleneck is needed on the amount of information about x that goes into z. This can be achieved by limiting the dimension of z. Otherwise, q φ and p θ can simply both learn the identity function, which is a trivial solution to Equation 2.12 but does not learn anything useful. Note that a good inference network learns to approximately invert</p><formula xml:id="formula_15">f θ , such that f θ • f φ ≈ Id. This is different from having f φ ≈ f θ ≈ Id, which is not useful.</formula><p>A probabilistic extension can be given to the auto-encoder model, yielding a generative model called the Variational Auto-Encoder <ref type="bibr">Kingma and Welling [2014a]</ref> which we describe below.</p><p>Figure <ref type="figure" target="#fig_0">2</ref>.3: If f θ is implemented with a deep network, f θ (z) is distributed on a non-linear manifold.</p><p>It can better fit p * , and less isotropic volume is needed. The encoder q φ takes target data and maps it to latent space, and the decoder f θ maps latent variables to data space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Variational auto-encoders</head><p>To build a stochastic auto-encoder, the decoder f θ implements p θ (x|z) by mapping a latent code z to a density over observations x:</p><formula xml:id="formula_16">p θ (x|z) = N (x; f µ θ (z), f σ θ (z)) (2.13)</formula><p>The encoder g φ computes an approximation of the posterior distribution p θ (z|x) as:</p><formula xml:id="formula_17">q φ (z|x) = N (z; g µ φ (x), g σ φ (x)).</formula><p>(2.14)</p><p>In that context, another type of bottleneck can be used: a prior p θ (z) can be defined, and the amount of information going into z can be measured as:</p><formula xml:id="formula_18">D KL (q φ (z|x)||p(z)) = z q φ (z|x) log q φ (z|x) p(z) , (2.15)</formula><p>whith D KL standing for Kullback-Liebler divergence. Intuitively, the Kullback-Leibler divergence measures the difference in the expected number of bits required to encode z using q φ (z|x) rather than p θ (z), i.e. how much information (in the sense of Shannon) about z we gain by looking at x using our inference network. With this bottleneck the dimension bottleneck is no longer needed, and the model offers a probabilistic interpretation. In particular, z can be sampled from the prior p(z) and mapped to data space with f θ . This construction is detailed in what follows. The quantity of interest that we wish to optimise is the marginal likelihood, p θ (x) = z p(z)p θ (x|z), also called the "evidence". Because f θ is now non-linear, this integral can not be carried out in closed form.</p><p>Natural solutions to the intractability of the evidence are numerical approximations and Monte-Carlo estimation. The later has the benefit of spending computational budget only on likely values Figure <ref type="figure" target="#fig_0">2</ref>.4: The encoder (in red) yields a distribution q φ (z|x). Using this distribution rather than the prior (in yellow) p θ (z) to sample z incurs a cost D KL (q φ (z|x)||p θ (z|x)). Given z, a deep decoder (in blue) defines a distribution p θ (x|z).</p><p>of z rather than on a fixed grid covering R N . However, the latent space is typically very high dimensional, and obtaining an accurate approximation requires a high number of samples per the curse of dimensionality.</p><p>To alleviate this issue, a first step is to use weighted sampling. The intuition is that when computing z p θ (x|z)p(z)dz for a fixed x, most of the latent variables z are very unlikely to generate x, and most of the mass in the integral is contributed by a small subset of the entire latent space. To that end, an inference network q φ can be used. For a given x, q φ tries to guess which part of the latent space is likely to generate x and is used to reweight the integral:</p><formula xml:id="formula_19">p θ (x) = z q φ (z|x)p θ (x|z) p(z) q φ (z|x)</formula><p>dz.</p><p>(2.16)</p><p>When approximating p θ (x) with an empirical estimator pθ (x) by sampling z from q φ (z|x), one obtains an estimator that has faster convergence and lower variance than when sampling from p(z), while remaining unbiased. However, we are typically interested in computing</p><formula xml:id="formula_20">log(p θ (x)). Because log[E(p θ (x)))] = E[log(p θ (x))], the empirical estimator log(p θ (x)) is a biased estimator of log(p θ (x))</formula><p>. This brings Jensen's inequality to mind: because log is concave,</p><formula xml:id="formula_21">log(p θ (x)) = log[E(p θ (x)))] ≥ E[log(p θ (x))]</formula><p>. By replacing the expectation by Monte-Carlo sampling, we can thus obtain an unbiased empirical estimator of a lower bound on the evidence.</p><p>Deriving a variational evidence lower bound. Once again, samples used in the Monte-Carlo estimation of E[log(p θ (x))] can be taken from q φ (.|x) rather than from p θ (.). This yields a so-called variational lower bound, that can be derived by combining reweighted sampling (first line) with Jensen inequality (second line):</p><formula xml:id="formula_22">ln(p θ (x)) = ln z q φ (z|x)p θ (x|z) p(z) q φ (z|x) dz (2.17) ≥ z q φ (z|x) ln p θ (x|z) p(z) q φ (z|x) dz (2.18) = E q φ (z|x) [ln(p θ (x|z))] -D KL (q φ (z|x)||p(z)). (2.19)</formula><p>This yields an evidence lower bound (ELBO), denoted L elbo θ,φ . The ELBO offers an auto-encoder interpretation:</p><formula xml:id="formula_23">L elbo θ,φ (x) = E q φ (z|x) ln p θ (x|z) Reconstruction -D KL q φ (z|x)||p(z)</formula><p>Regularization .</p><p>(2.20)</p><p>Sampling z from the learned posterior knowing x, z ∼ q φ (.|x), can be seen as encoding x into z, while p θ (.|z) seeks to reconstruct x from z. Putting a lot of information about x in z makes reconstruction trivial, but is heavily penalised by the regularization term. Therefore, the regularization term acts as an information bottleneck, analogous to the dimension bottleneck in deterministic auto-encoders, so a balance between both terms must be found.</p><p>An other insightful form of the ELBO can be obtained. Using Bayes' rule, p(z) = p(x)p (z|x)   p <ref type="bibr">(x|z)</ref> and injecting it in Equation <ref type="formula" target="#formula_91">2</ref>.19:</p><formula xml:id="formula_24">L elbo θ,φ (x) = E q φ ln p θ (x|z) - z q φ (z|x) log q φ (z|x)p θ (x|z) p(z|x)p θ (x) (2.21) = E q φ ln p θ (x|z)p θ (x) ln p θ (x|z) -D KL q φ (z|x)||p(z|x) (2.22) = ln p θ (x) -D KL q φ (z|x)||p(z|x) (2.23)</formula><p>The Kullback-Leibler divergence, D KL is non-negative. Therefore, the second form in Equation <ref type="formula" target="#formula_91">2</ref>.23 immediately shows that the ELBO is a lower bound on the log-likelihood. It is also clearly tight if and only if q φ (z|x) = p θ (z|x). Intuitively, if the approximate posterior q φ (z|x) perfectly matches the true posterior p θ (z|x), there is no 'approximation cost' caused by improper selection of z. This form shows that maximizing L elbo φ,θ (x) can be done by either increasing p θ (x) (training the generator) or by making the bound tighter (training the inference network).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Important remarks. (i)</head><p>The second form, Equation 2.23 can not be used to train in practice as it requires computation of p θ (x) and p(z|x), and both quantities are intractable. In contrast, Equation 2.19 requires evaluating two deep feed-forward networks, q φ (.|x) and p θ (.|z), which is both tractable and efficient.</p><p>(ii) When looking at Equation 2.19, it is tempting to think that setting D KL (q φ (x|z)||p(z)) to 0 should improve the bound. That is not the case, unlike for the KL divergence in Equation 2.23. Indeed, it would mean that q φ (x|z) = p(z), so z becomes independent of x and the model does not use the encoder at all. This will strongly degrade the reconstruction term, E z∼q φ (x|z) [ln(p θ (x|z))]. The optimal setting is q φ (z|x) = p θ (z|x); this incurs a non-zero D KL (q φ (x|z)||p(z)), but in this cost is exactly compensated by the improvement of the reconstruction term due to sampling from q φ (z|x) rather than p θ (z). (iii) In practice the optimal value of D KL (q φ (x|z)||p(z)) in Equation <ref type="formula" target="#formula_91">2</ref>.19 is a balance between its two terms: the network will keep putting more information into z as long as the gain in reconstruction is greater than the cost of that information. Thus, the optimal amount of information depends on the quality of both the encoder and the decoder and cannot be predicted in general. In particular, a model can have poorer reconstructions but better likelihood performance if it uses very little information to reconstruct x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Deep invertible transformations</head><p>We have seen how to lower-bound the integral p θ (x) = z p(x|z)p(z)dz. An other approach is to restrain the class of functions F Θ that can be used to build p θ in a way that the integral is easy to compute. To see which properties can be desirable from such a class of functions, we recall the difficulties that arise when training a VAE. First, it requires an approximate posterior q φ (.|x) that matches the intractable true posterior p θ (.|x). Mistakes made by q φ have a cost: Equation <ref type="formula" target="#formula_91">2</ref>.23, is tight if and only if q φ (z|x) matches p θ (z|x) exactly. Thus, exact inference is a desirable property.</p><p>An other difficulty is that E z∼q φ (z|x) remains continuous and high dimensional despite the use q φ . This requires Monte Carlo approximations which trade-off variance of the estimator against computation efficiency. If the posterior density were to be a Dirac, the integral could be computed exactly with a single sample. Thus, deterministic inference is a desirable property. To summarize, we wish to have a function</p><formula xml:id="formula_25">f θ such that q φ (z|x) = p θ (z|x) = δ(z -f θ (x)).</formula><p>This analysis points to the class of invertible functions. Indeed, an invertible generative model offers exact inference obtained by computing it's inverse. It also maps a single input to a single output so it offers deterministic inference. In the context of natural images, <ref type="bibr" target="#b27">Dinh et al. [2017]</ref> proposed 'Non Volume Preserving' transformations (NVP). A latent space of the same dimensionality as the data space is specified, with a simple prior (e.g. unit Gaussian N (O n , I n )). The simple distribution is then progressively reshaped into a more complex one by successive invertible transformations,</p><formula xml:id="formula_26">x = f θ (z) = f θn • f θ n-1 • . . . • f θ 1 (z).</formula><p>Denote Z the random variable obtained by mapping X through f θ , the density of natural images in the training set under the reshaped distribution can be estimated using the change of variable formula:</p><formula xml:id="formula_27">p X (x) = p Z (f θ (x)) × det ∂f θ (x) ∂x . (2.24)</formula><p>which provides the training loss of the model. This formula follows from the fact that the probability mass contained by a differential area does not depend on our arbitrary parametric choices, i.e.: |p Z (z)dz| = |p X (x)dx|. To generate new images, latent variable vectors can be samples from p Z (z), and mapped to image space with the inverse transformation f -1 θ . Figure <ref type="figure" target="#fig_0">2</ref>.5 illustrates the mapping from image space to latent space. Practical construction: affine coupling layers. In practice, the class of invertible functions used must be restricted to ensure efficient computations of the quantities involved in Equation <ref type="formula" target="#formula_91">2</ref>.24: y = f (x) as well as the determinant det ∂f θ (x) ∂x . One way to ensure tractable and easy to invert blocks f θ i is to partition the dimensions of the variables in two groups, by applying a permutation σ i to x and splitting σ i (x) in two such that σ i (x) = (σ i (x) 1 , σ i (x) 2 ). To simplify notations, assume the permutation x ← σ i (x) has been performed and write x again. One group is kept unchanged, but is used to transform the other group via translation and scaling, as illustrated in Figure <ref type="figure" target="#fig_0">2</ref>.6. Then f θ i (x) = (y 1 , y 2 ) where:</p><formula xml:id="formula_28">y 1 = x 1</formula><p>(2.25)</p><formula xml:id="formula_29">y 2 = t x 1 + x 2 exp s(x 1 ) (2.26)</formula><p>This transformation is trivial to invert:</p><formula xml:id="formula_30">x 1 = y 1 (2.27) x 2 = y 2 -t x 1 exp -s(x 1 ) (2.28)</formula><p>The functions s(•) and t(•) can be arbitrarily flexible, and implemented as complex non-invertible functions, e.g. deep CNNs. There is no need to invert them when computing f -1 θ i as shown in Equation 2.28. This construction yields blocks f θ i that are easy to invert. Each block is not very expressive, being limited to an affine transformation of half the variables. To make f θ more flexible, many Figure <ref type="figure" target="#fig_0">2</ref>.6: During forward propagation, the scalar variables are separated into two groups using masking schemes. The first group x 1 (in red) is used as input to expressive functions s and t that yield translation and scaling parameters for the second group (in blue), yielding a results that depends on both groups (in purple). To invert the function, it is not necessary to invert s and t (as depicted in the centre). To avoid always modifying the same subset of the scalar variables, checkerboard partitioning patterns are used together with partitioning along the channel axis (depicted on the right). Figure adapted from <ref type="bibr" target="#b27">Dinh et al. [2017]</ref> .</p><p>such blocks are composed, and their log-determinants are summed. The permutation σ i is here to avoid always modifying the same features: because of it, the partitioning scheme changes from one block to the next. To do this, <ref type="bibr" target="#b27">Dinh et al. [2017]</ref> proposes two variable partitioning schemes, illustrated in Figure <ref type="figure" target="#fig_0">2</ref>.6. One uses a checkerboard mask, which is very natural: looking at one pixel out of two gives a good idea of what the image looks like. The second uses channel-wise masking and the two masks are used alternatively. This construction also offers efficient computations of the log-determinant of the Jacobians. Indeed the Jacobian has a triangular structure:</p><formula xml:id="formula_31">∂f θ (x) ∂x = I d O d ∂y 2 ∂x 1 diag(exp(s(x 1 ))) . (2.29)</formula><p>Therefore the determinant is given by the product of the diagonal terms of the Jacobian, ln det ∂f (x) ∂x = 1 s(x 1 ). The log-likelihood of the model is thus also easy and tractable to compute, and can be optimized by stochastic gradient descent on the negative log-likelihood:</p><formula xml:id="formula_32">-ln p X (x) = -ln p Z (f θ (x)) -1 s(x 1 ).</formula><p>(2.30)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Generative adversarial networks</head><p>We have presented two classes of deep latent variable generative models built to have tractable (approximation of) p θ (x). Another approach is to train the model without maximizing p θ (x). Indeed, while inference can be challenging in latent variable models, computing f θ (z) is simple and efficient. Thus given an image quality metric, the model can be trained by assigning scores to samples x taken from the model. Generative Adversarial Networks (GANs), introduced by <ref type="bibr" target="#b42">Goodfellow et al. [2014]</ref>, propose to use a classifier D φ to evaluate samples. A latent variable vector z is sampled from a prior p(z), and mapped to image space using a deep network: x = G θ (z), inducing an implicit density p θ (x). A second deep network, termed the 'discriminator' is used to evaluate the quality of sampled images by outputting D φ (x) ∈ [0, 1], the estimated probability of x being real. Intuitively, if D φ is well trained, it is able to identify real looking images, and it's score can be used as a reward. The objective of the discriminator is a function of the parameters of the generator θ and those of the discriminator, φ. It is the expected log-likelihood of correct classification:</p><formula xml:id="formula_33">V (φ, θ) = E x∼p * (x) [ln D φ (x)] + E x∼p(z) [ln (1 -D φ (G θ (z)))].</formula><p>(2.31)</p><p>The discriminator is trained to maximize classification accuracy for a given generator, i.e. φ is optimized to reach φ * = max φ V (φ, θ). Simultaneously, the generator is trained to degrade the classification of a given discriminator, i.e. θ is optimized to reach θ * = min θ V (φ, θ). Solving this adversarial problem corresponds to finding φ * and θ * such that:</p><formula xml:id="formula_34">V (φ * , θ * ) = min θ max φ V (φ, θ).</formula><p>(2.32)</p><p>The discriminator can be seen as a trainable loss function that can focus on the mistakes typically made by the generator. Assume a fixed generator G θ , the optimal discriminator maximizes</p><formula xml:id="formula_35">V θ (φ) = x [p * (x) ln D φ (x) + p θ (x) ln(1 -D φ (x)) ] dx. (2.33)</formula><p>For any (a, b) ∈ R 2 \ {0, 0} the function y → a ln(y) + b ln(1 -y) achieves its maximum in [0, 1] at y = a/(a + b) so for a fixed θ, the optimal discriminator D φ * (θ) is the Bayes classifier:</p><formula xml:id="formula_36">D φ * (θ) (x) = p * (x)</formula><p>p * (x)+p θ (x) . Now assuming that D φ is trained to optimality for a given θ, D φ * (θ) can be plugged in V (θ, φ):</p><formula xml:id="formula_37">V (φ * (θ), θ) + ln 4 = D KL p * p * + p θ 2 + D KL p θ p * + p θ 2 ∝ D JS (p * ||p θ ). (2.34)</formula><p>Assume the regime of infinite data, infinite model capacity, and under the assumption that the optimal discriminator is reached at each training iteration of the generator. In that context, Equation 2.34 shows, by convexity of D KL<ref type="foot" target="#foot_5">foot_5</ref> , that there is a unique global optimum for G θ , at the data distribution p θ = p * , which can be recovered by gradient descent. In practice however, the ideal assumptions made above are not met. To optimize the networks, the expectation is replaced with sample average over mini-batches, and parallel stochastic gradient descent is used on φ and θ.</p><p>GANs are known to be difficult to train, for several reasons. The mini-max objective formulation between two networks can lead to oscillations between solutions, because the optimality assumption is not met. It is also necessary to pick "compatible" generator and discriminator architectures, as training is known to fail if the discriminator is too powerful, as explained in <ref type="bibr" target="#b3">Arjovsky et al. [2017]</ref>. The GAN training objective also elicit models that produce good quality images but fail to capture the full support of the training data, a phenomenon known as mode-collapse and detailed in Section 2.9.2. If training is successful, however, GANs can produce outstanding samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Autoregressive density estimation</head><p>We have presented three approaches to train deep latent-variable generative models. A fourth important class of model is obtained by dropping the conditional independence requirement. In that case, latent variables are no longer needed to ensure global coherence of the image, as explained in Section 2.2. Instead, one can rely on the standard chain rule to factorize p θ (x) into a joint probability distribution:</p><formula xml:id="formula_38">p(x 1:D ) = p(x 1 ) D i=2 p(x i |x &lt;i ),</formula><p>(2.35)</p><p>with x &lt;i = (x 1 , . . . , x i-1 ). Latent variables are no longer needed to ensure global coherence in this case: a given pixel x i is modelled after seeing x j&lt;i , so it can adapt to previous pixels to be coherent with them. Then, following pixels x &gt;i are modelled conditioned on x i , so they can adapt to x i , and all pixels are coherent.</p><p>In the regime of deep auto-regressive models, the idea is to use (deep) neural networks to model the dependencies in this chain, p(x i |x &lt;i ). To implement this with a CNN-like architecture, masked convolutions can be used. The idea, illustrated in Figure <ref type="figure" target="#fig_0">2</ref>.7, is to compute for each scalar variable x i , features of increasing complexity f i that will be used to parametrise p(x i |x j&lt;i ). Thus, at a given layer l, a feature f l i can only depend on x j&lt;i . This can be built recursively like so: at layer 1, f 1 i is computed with a masked kernel that only looks at x j&lt;i as in Figure <ref type="figure" target="#fig_0">2</ref>.7, top left. Given the feature at a layer l, f l i i≤N , a new feature f l+1 i can be computed with a masked kernel convolution as in Figure <ref type="figure" target="#fig_0">2</ref>.7 (center top), because these features only depend on x &lt;i . Notice the central feature is available except at the first layer. This model, introduced in <ref type="bibr" target="#b105">Oord et al. [2016]</ref> is called Pixel-CNN. This yields tractable exact likelihood computations and does not require complex integral over latent variables. The main drawback of this method is that it suffers from slow sequential sampling. An other drawback is that the fact that the model cannot rely on latent variables to couple pixels makes it harder for it to model global structure, especially given the fact that stacked convolutions already devote more flexibility to modelling local structure. Finally, the absence of latent variables makes it unsuitable for representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9">Challenges in generative modelling</head><p>We have discussed the main existing families of deep generative models used in the literature. In this section, we present in greater detail two challenges in generative modelling that will be ... To predict the red pixel in the first map (top left), the model is allowed to look at the green pixels. When computing the second feature map (top centre) the yellow feature has been computed without looking at the red pixel, and therefore becomes observable. This explains why the first convolutional filter is different from the following ones. The second half of the figure shows how the receptive field of the central feature evolves through the successive layers. central in the rest of this dissertation. The first is elaborated upon in Chapter 3 and the second in Chapter 4. The two issues, which can intertwine, are tackled together in Chapter 5. Some key related work also adressing these challenges is presented in Section 2.10 and Section 2.11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9.1">Understanding the conditional independence assumption</head><p>In this section, we will take a closer look at the theoretical and practical implications of the conditional independence assumption, which we presented in Section 2.2. Tackling the shortcomings of this assumption is key to the work presented in Chapter 3 and Chapter 5. We present several complementary points of view. As we have seen the standard way of defining latent generative models is to make a conditional independence assumption, which yields a density of the form:</p><formula xml:id="formula_39">p θ (x|z) = N i=1 p θ (x i |z).</formula><p>(2.36)</p><p>Recall that in Section 2.2, we have seen that mapping a latent variable z from some latent space Z to an image space X using a non-linear function f θ yields an implicit density ρ θ (x) = P (f θ (z) = x). The support of this density belongs to a low dimensional manifold of the ambient space, as z is of low dimensionality and f θ is deterministic. Therefore ρ θ has a degenerate support in the sense that it (almost surely) puts 0 mass on the observed dataset:</p><formula xml:id="formula_40">P ({∃x ∈ D|ρ θ (x) = 0}) = 1. (2.37)</formula><p>To measure a density and train our model, it is thus necessary to add volume around this lowdimensional manifold. This is typically done by using f θ as the mean of a parametric distribution and adding isotropic noise around it, for instance using a Gaussian density with isotropic variance:</p><formula xml:id="formula_41">p θ (x) = z N (x|f θ (z), σI n )p(z)dz = z N i=1 N (x i |f θ (z) i , σ)dz.</formula><p>(2.38)</p><p>Because of the isotropic nature of this volume, it cannot be used to model any structure, and all the structure has to be captured by f θ (z). In what follows, ρ θ denotes a density with degenerate support, and p θ denotes the density obtained by adding isotropic volume to ρ θ , which thus has a non-degenerate support. As f θ becomes more flexible and accurate, less volume is needed, and σ decreases. At the optimal limit and with infinite data ρ θ fits p * perfectly, and σ → 0 such that:</p><formula xml:id="formula_42">p θ (x|z) → δ(x -f θ (z)) = ρ θ (x|z) and ρ θ (x) = p * (x).</formula><p>(2.39)</p><p>In practice f θ (z) is not flexible enough, so volume will always be necessary to model what f θ cannot capture.</p><p>Limitations of isotropic noise. To take a look at the practical implications, let us first clarify what we mean by «[the volume] cannot be used to model any structure». Suppose the mean of the distribution f θ (z) is a good image, then sampling an image from p * θ (x|z) means adding 'salt and pepper' per-pixel noise around f θ (z), which leads to a poor noisy image. The best we can hope Figure <ref type="figure" target="#fig_0">2</ref>.8: Interpolating from point x 1 to point x 2 using the blue arrow requires going through a region of low density, which illustrates that the Euclidean distance is a bad notion of distance for points on a non-linear manifold. Instead, the geodesic distance (black arrow) should be used. Importantly, x 3 is in a region of low density and should thus be "far" from any point in the red area. That is not the case when using the Euclidean distance.</p><p>for is that σ is small enough that we can't notice the difference. In terms of density modelling performance, we are wasting the probability mass of p θ (x) on unlikely points around f θ (x), i.e we are over-generalizing and taking samples from x ∼ p θ (x) will yield unrealistic images. This is particularly problematic if f θ lacks "a lot" of flexibility, as a lot of mass has to be wasted to compensate. This illustrates that the conditional independence assumption is a poor assumption for p θ (x).</p><p>Considering these limitations, which concern p θ (x), it is reasonable to see ρ θ as the real quantity of interest, as is the case in GANs. In this view, the noise σ is just here to train ρ θ and can be considered as belonging to the training loss rather than to the model. This point of view is reasonable in the sense that it is consistent: the loss is zero for ρ θ (x) = p * , and ρ θ is a universal density approximator. However this view is also problematic: we care about one density, ρ θ , but optimize and evaluate another density. It is therefore reasonable to ask: what will the impact be on the training of ρ θ ?</p><p>Impact on the training of ρ θ (x). The point of view that ρ θ is the object of interest, and σ is just "here to help", nevertheless has implications on the training loss of ρ θ . We can see it in terms of "reconstruction losses": maximizing the log-likelihood of an isotropic Gaussian with fixed variance corresponds to minimizing an L 2 reconstruction loss,</p><formula xml:id="formula_43">-log(p θ (x|z)) ∝ x -f θ (z) 2 .</formula><p>(2.40)</p><p>Therefore, to measure the distance between an image and a prediction, the Euclidean norm in pixel space is used. It is sensible in that it is a distance, so it is minimised for x = f θ (z).</p><p>Figure <ref type="figure" target="#fig_0">2</ref>.9: If a model is assumed to have isotropic variance, two situations are possible. i) The model if flexible to fit all points, and will reduce the variance almost to a Dirac, which can lead to a model that is significantly more complex than it should be, and over-fitting. Indeed in the exemple on the left, the ground truth has one non-isotropic mode, while the model is significantly more complex. ii) The model is not flexible enough to fit all points closely, and has to substantially increase its variance, thus wasting probability mass and yielding unrealistic samples.</p><p>However, it is clear that the Euclidean norm is not a good measure of similarity between images. For instance, take x and translate it by two pixels to obtain x. Most humans will state, reasonably, that the two images are almost identical and yet, the Euclidean norm will be huge. The same goes for rotations and other low level transformations of x. Conversely, two images at a small L 2 distance of each other can seem visually very different.</p><p>What would a better distance look like? A reasonable assumption is that images lie (close to) a low-dimensional, non-linear, manifold of R N . A desirable distance could be the geodesic distance<ref type="foot" target="#foot_6">foot_6</ref> between the projection of x and x on that manifold, d g (proj(x), proj( x)). While following the geodesic to minimize d g , proj( x) would move on the manifold, so it's pixels would change in a coherent manner, and so d g would better satisfy human judgement. This is illustrated in Figure <ref type="figure" target="#fig_0">2</ref>.8. Unfortunately this manifold is precisely what we are trying to learn, so it is impossible to leverage the geodesic distance. However, it highlights the shortcomings of the Euclidean norm.</p><p>Pathological example. What happens when using the Euclidean distance anyway? Suppose a subset T ⊂ X in which p * is approximately unimodal, and take two samples</p><formula xml:id="formula_44">x 1 , x 2 ∼ p * (x|x ∈ T ). If p θ (x|z)</formula><p>is assumed isotropic, two cases may happen: either f θ is flexible enough to go close to both points, and the simple unimodal distribution is replaced by a more complicated distribution with two modes (and the model overfits), or f θ is not flexible enough and it will fit both points with a single, blurry mode between the two points and high variance around it, as illustrated in Figure <ref type="figure" target="#fig_0">2</ref>.9.</p><p>Going beyond the conditional independence assumption. Given the discussion above, it appears that building models that go beyond the conditional independence assumption is an interesting research direction. It can be seen as building better density models for p θ , or equivalently better training losses for ρ θ . In Chapter 3, we construct such a model by using autoregressive decoders, presented in Section 2.8, in a VAE model. The decoder is conditioned on some latent variable z produced by an inference network, and can be written:</p><formula xml:id="formula_45">p θ (x|z) = N i=1 p θ (x i |x j&lt;i , z).</formula><p>(2.41)</p><p>With this construction, the pixels are sampled sequentially, which is slower at sample time, but an arbitrarily complex dependencies can be learned around f θ (z). In Chapter 5, a different approach is taken. An invertible model f ψ is used to build an abstract feature space in which the Euclidean distance is a better measure of similarity. In other words we lift the targets of p θ in a feature space:</p><formula xml:id="formula_46">p θ (f ψ (x)|z) = N i=1 p θ (f i ψ (x)|z) (2.42)</formula><p>and use the invertibility of f ψ to go back to image space, both for likelihood evaluations and for sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9.2">Understanding mode-dropping in adversarial networks</head><p>Adversarial networks have the ability to generate very compelling samples, far beyond the sample quality of a VAE with an identical computational budget. On the other hand, a known issue is that they tend to model only a strict subset of the support of the true data distribution p * , which is called mode-dropping. Intuitively, this is a direct consequence of the way the model is trained as we show in the following paragraphs. This issue is made worse by the fact that GANs do not provide a clear way of measuring this phenomenon. Indeed there is no simple way of measuring the density of a given real data point x under the model, p θ (x), because the model has a degenerate support.</p><p>There are two types of failure cases for a GAN generator. The first is to produce poor looking images, i.e. put mass outside of the support of p * . The second is to produce good quality images with too little variability, i.e. miss some modes of the support and put too much masss on others.</p><p>We now discuss the way the discriminator penalises these two undesirable behaviours. To train p θ , a sample x is taken from the model, and it is fed to the discriminator which has to distinguish it from real images x ∼ p * (x), by computing D φ ( x). If a sample is of poor visual quality, the discriminator can reject it based on that. In other words it has an approximation of p * , denote it p * , and can estimate if "p * (x) is high". Thus it is clear that the discriminator can explicitly evaluate the quality of the image. What does not happen when training a GAN, is taking an image from the training set, x ∼ p * , and explicitly asking if it is well covered by the model p θ , i.e.</p><p>"is p θ (x) high ?". This is what MLE, for instance does. However if only p * (x) was evaluated, p θ would always collapse to the mode of p * , which does not usually happen. Therefore some mechanism also evaluates variability.</p><p>Implicit variability evaluation. Let us discuss the extreme case where the learned density collapses to a single image. Suppose that the generator learns a Dirac on a single real looking image x 0 from the training set:</p><formula xml:id="formula_47">p θ (x) = δ(x -x 0 ).</formula><p>(2.43)</p><p>The discriminator can not reject the image based on its quality, as it is a real image, i.e. evaluating p * (x) will yield a good score. Yet intuitively it can succeed by always predicting fake when seeing this image. Most of the time it will be correct, and very rarely the image will come from the dataset and it will be wrong. This is because the discriminator also holds an approximation of p θ , that we denote p θ . If an image is "seen too often", it's score is degraded, and the discriminator determines "too often" by comparing p θ (x) and p * (x). If p θ ( x) is "too high" and the discriminator pushes it down, the mass must go somewhere. Going outside of the support is heavily penalised, so it should go on the support. Thus, by explicitly evaluating the mass of samples x, the density mass is implicitly pushed towards the training set, due to the fact that the total mass is fixed. This shows an informal 'consistency' of adversarial training.</p><p>Practical consequences. We have seen that adversarial training explicitly rewards quality, and implicitly rewards coverage. In practice the consequence is that training is biased towards a mode-seeking behaviour. To see this, assume that p * is more flexible than p θ . To avoid going into low density regions of the space, p θ has to drop some modes of the distribution. Conversely, when training with MLE, p θ has to go through regions of low density to cover the full support of the distribution. This phenomenon is illustrated in Figure <ref type="figure" target="#fig_0">2</ref>.10. From this perspective, the fact that GANs tend to produce good-looking samples but drop part of the support is not surprising.</p><p>To build conditional models that produce compelling outputs, used for instance in photo editing products, this may be fine. But from a density modelling perspective, coverage and quality are two sides of the same coin: the goal is to learn p * . This motivates the goal of reducing mode collapse in GANs.</p><p>Reducing mode-collapse in GANs. The discussion above motivates training procedures that fight mode-collapse by more closely evaluating support coverage. In Chapter 4, a construction to explicitly evaluate variability is proposed. This idea is as follows: variability is hard to evaluate from i.i.d samples, because it is evaluated implicitly. Instead, one can consider batches of images, fake and real, as input to the discriminator. To fool the discriminator, it is necessary to produce a batch with as much variability as a batch of fake images. Thus, variability becomes a feature that can be explicitly computed as a batch statistic, rather than implicitly evaluated. In Chapter 5 a more direct approach is taken, based on the symmetrical properties of adversarial training and MLE: by combining the two, one is able to explicitly optimize both coverage and quality. With MLE, the reverse happens: the model is penalysed for not assigning mass to samples from the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.10">Details and refinements on Variational auto-encoders</head><p>In sections 2.5 and 2.7, the core of the VAE and GAN models have been presented. These two models are at the center of this thesis, thus we now present them more thoroughly. We first discuss additional details on the practical training of VAEs in Section 2.10.1. In sections 2.10.2 to 2.10.4, refinements of the VAE framework that aim at improving the accuracy of the approximate posterior are discussed. In Section 3.3.2, we present the idea of using flexible autoregressive decoders. Finally, a variant of the VAE using quantized latent vectors is presented in Section 2.10.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.10.1">Practical training algorithm</head><p>In Section 2.5 we presented the basics of the VAE model. Here we present technicalities and refinements that are necessary to make it work in practice. Recall the key VAE objective function:</p><formula xml:id="formula_48">L elbo θ,φ (x) = E q φ (z|x) [ln(p θ (x|z))] -D KL (q φ (z|x)||p(z)).</formula><p>(2.44)</p><p>In practice, parametric families remain to be specified for p θ (z), p θ (x|z) and q φ (z|x). One usual choice is to assume Gaussianity for p(z) and q φ (z|x), in which case the D KL term can be computed in closed form: with p(z) = N (z; 0, I) and q φ (z|x</p><formula xml:id="formula_49">) = N (z; g µ φ (x), g σ φ (x)), D KL (q φ (z|x)||p(z)) = 1 2 1 + ln g σ φ (x) -g µ φ (x) -g σ φ (x) .</formula><p>(2.45)</p><p>For the reconstruction term, empirical estimation is typically used: with</p><formula xml:id="formula_50">z s ∼ q φ (z|x), E q φ ln p θ (x|z) ≈ 1 S S s=1 ln p θ (x|z s ).</formula><p>This empirical estimation raises an issue: the estimator becomes nondifferentiable because of the sampling operator. A first solution can be to use the Reinforce algorithm: under assumptions compatible with the Leibniz Integral rule,</p><formula xml:id="formula_51">∇ φ E z∼q φ (z|x) [ln p θ (x|z)] = E z∼q φ (z|x) [ln p θ (x|z)∇ φ log q φ (z|x)]</formula><p>(2.46)</p><formula xml:id="formula_52">≈ 1 N N i=1 ln p θ (x|z i )∇ φ log q φ (z i |x), (2.47)</formula><p>where the first line comes from swapping E and ∇ φ , and from the log-derivative trick: ∇ φ q φ (z) = q φ (z)∇ φ log q φ (z). Unfortunately, this approach suffers from high variance. Intuitively, it is in fact too generic: it enables differentiation w.r.t. to any sampling operation. We are in a special case where we do need to sample, which is inherently non-differentiable, but from a parametric distribution that is differentiable w.r.t. it's parameters. This leads to the idea of treating the randomness as an additional input of the network, sampled from a standard Gaussian, and learning a transformation that reshapes this standard Gaussian into an other distribution. This approach is called the re-parametrization trick <ref type="bibr">Kingma and Welling [2014a]</ref> and is illustrated in Figure <ref type="figure" target="#fig_0">2</ref>.11. With : ε ∼ q(ε) and z = g φ (ε|x):</p><formula xml:id="formula_53">∇ φ E z∼q φ (z|x) [ln p θ (x|z)] = ∇ φ E ε∼q(ε) [ln(p θ (x|g φ (ε|x))] (2.48) = E ∼q(ε) [∇ φ log(p θ (x|g φ (ε|x))]</formula><p>(2.49)</p><formula xml:id="formula_54">≈ 1 N N i=1 ∇ φ ln p θ (x|g φ (ε i |x)).</formula><p>(2.50)</p><p>Both methods are unbiased, but this approach offers gradients with lower variance. In practice, g φ (z|x) is chosen Gaussian still to enable closed form computation of the KL:</p><formula xml:id="formula_55">z s ∼ q φ (z|x) = N (z; g µ φ (x), g σ φ (x)).</formula><p>With the re-parametrization trick, this is written as:</p><formula xml:id="formula_56">z s = g µ φ (x) + g σ φ (x) s , with s ∼ N ( s ; 0, I).</formula><p>In this set-up, the empirical estimator of the ELBO reduces to:</p><formula xml:id="formula_57">L θ,φ (x) ≈ 1 S S s=1 ln p θ x|g µ φ (x) + g σ φ (x) s - 1 2 1 + ln g σ φ (x) -g µ φ (x) -g σ φ (x) (2.51)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.10.2">Top down sampling</head><p>When optimizing</p><formula xml:id="formula_58">L elbo θ,φ (x) = E q φ (z|x) [ln(p θ (x|z))] -D KL (q φ (z|x)||p(z))</formula><p>, the flexibility of q φ (z|x) is crucial: approximating p θ (z|x) is a difficult task. One limitation of the vanilla VAE approximate posterior is the Gaussianity restriction with diagonal covariance, commonly referred to as the mean-field assumption. More precisely, equation Equation <ref type="formula" target="#formula_91">2</ref>.23 shows that the optimal variational distribution verifies q φ (z|x) = p θ (z|x). With restricted density families for q(.) this is not realizable. One possible solution is to use a hierarchy of latent vectors, rather than a single one. In the generative model p θ , latent variables z can be split into L groups, each one at a Figure <ref type="figure" target="#fig_0">2</ref>.11: The re-parametrization trick consists in treating the "randomness" as an input to the network, denoted . This allows differentiation with respect to σ and µ, and reduces the variance of the estimator compared to sampling z directly. Quantities inside a circle are stochastic, while those inside a diamond are deterministic functions of their possibly stochastic inputs. different layer, and the density over z can be written as:</p><formula xml:id="formula_59">q(z) = q(z L ) L-1 i=1 q(z i |z i+1 ).</formula><p>(2.52) At depth i, z i-1 can be treated as a standard input feature map, and assume z i |z &lt;i to be Gaussian, as in the standard construction. Yet non-linear functions of z i-1 can be used to implement the conditionning. Therefore when integrating over z the posterior is no longer Gaussian:</p><formula xml:id="formula_60">z p θ (x|z) = z 1 . . . zn p θ (x|z)p(z n |z j&lt;n ) . . . p(z 1 ) (2.53)</formula><p>with p(z n |z &lt;n ) implemented using a deep functions, for instance p(z n |z &lt;n-1 ) = N (.|µ θn (z &lt;n ), σ n I) with µ θn the output of a deep network. Additionally, to allow the chain of latent variables to be sampled in the same order when encoding and when sampling, top-down sampling is used <ref type="bibr" target="#b4">[Bachman, 2016</ref><ref type="bibr">, Kingma et al., 2016b</ref><ref type="bibr" target="#b137">, Sønderby et al., 2016]</ref>. With top-down sampling, the encoder (symmetric to the decoder) extracts deterministic features h i at different levels as the image is being encoded, constituting the bottom-up deterministic pass. While decoding the image, these previously extracted deterministic features h i are used for top-down sampling and help determining the posterior over latent variables at different depths in the decoder. These posteriors are also conditioned on the latent variables sampled at lower feature resolutions, using normal densities as follows:</p><formula xml:id="formula_61">q φ (z 1 |x) = N (z 1 |µ 1 (x, h 1 ), σ 2 1 (x), h 1 ), (2.54) q φ (z i |z i-1 ) = N (z i |µ i (x, z i-1 , h i-1 ), σ 2 i (x, z i-1 , h i-1 )).</formula><p>(2.55)</p><p>This constitutes the stochastic top-down pass, and is illustrated in Figure <ref type="figure" target="#fig_0">2</ref>.12. We refer the reader to <ref type="bibr" target="#b4">[Bachman, 2016</ref><ref type="bibr">, Kingma et al., 2016b</ref><ref type="bibr" target="#b137">, Sønderby et al., 2016]</ref> for more detail.</p><p>Figure <ref type="figure" target="#fig_0">2</ref>.12: With top-down sampling, deterministic information is hierarchically extracted during the encoding. During decoding, latent variables z i are sampled top-down, using the information extracted at corresponding layer i during encoding to determine the distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.10.3">Normalizing Flows for flexible posteriors</head><p>There are other solution to go beyond simplistic parametric families and mean-field approximations. The principle of normalizing flows <ref type="bibr">(Tabak &amp; Turner, 2013;</ref><ref type="bibr">Tabak &amp; VandenEijnden, 2010)</ref> can be used to obtain more flexible posteriors. The key idea is to apply a sequence of invertible transformations to an initially simple probability density. Applying this sequence yields a valid probability distribution, and its density can be evaluated exactly by repeatedly applying the change of variables formula. As the initial density 'flows' through the sequence it becomes increasingly flexible. Consider an invertible, smooth mapping f : R d → R d , with inverse f -1 = g. Given a random variable z with distribution q(z), we can use this mapping to transform z into f (z). The change of variable formula (or inverse function theorem) yields the density of z :</p><formula xml:id="formula_62">q(z ) = q(z) det ∂f -1 ∂z = q(z) det ∂f ∂z .</formula><p>(2.56)</p><p>Arbitrarily complex densities can be constructed by successively applying Equation <ref type="formula" target="#formula_91">2</ref>.56. After transforming a random variable z 0 with distribution q 0 through a chain of K invertible transformations f k , one obtains:</p><formula xml:id="formula_63">z K = f K • . . . • f 2 • f 1 (z 0 ) (2.57)</formula><p>and</p><formula xml:id="formula_64">ln q K (z K ) = ln q 0 (z 0 ) - K k=1 ln | det ∂f k ∂z k-1 |.</formula><p>(2.58) Expectations w.r.t. the transformed density q K can be computed without explicitly knowing q K as for any function h:</p><formula xml:id="formula_65">E q K [h(z)] = E q 0 [h(f K • f K-1 • . . . • f 1 (z 0 ))].</formula><p>(2.59)</p><p>Note that in the case where h(z) does not depend on q K (which is not the case in maximumlikelihood estimation), this does not even require computation of the log det-Jacobian terms. The invertible flows can be interpreted as applying a series of expansions or contractions onto the initial density function. The map z = f (z) can push points towards the interior of a region in R d , increasing the density inside the region and decreasing it outside (contraction). It can also pull the points z away from a region, thus reducing the density in that region (expansion). An initially simple prior, e.g. factorized Gaussian, can be turned into increasingly flexible and multi-modal distributions by applying normalizing flows with an increasing number of transformations.</p><p>In <ref type="bibr">Kingma et al. [2016b]</ref>, a type of normalizing flow called inverse autoregressive flow (IAF) is introduced. The main benefits of this normalizing flow are its scalability to high dimensions, and its ability to leverage autoregressive neural network, such as those introduced in van den <ref type="bibr">Oord et al. [2016b]</ref>. First, a latent variable vector is sampled using the re-parametrization trick <ref type="bibr">Kingma and Welling [2014b]</ref>, z 0 = µ 0 + σ 0 , with ∼ N (0, I). Then, new mean and variance parameters µ 1 and σ 1 are computed as functions of z 0 using autoregressive models, and a new latent variable z 1 is obtained:</p><formula xml:id="formula_66">z 1 = µ 1 (z 0 ) + σ 1 (z 0 )z 0 .</formula><p>(2.60)</p><p>Since σ 1 and µ 1 are implemented by autoregressive networks, the Jacobian dz 1 dz 0 is triangular with the values of σ 1 on the diagonal, and the density under the new latent variable remains efficient to compute. This transformation can be repeated an arbitrary number of times for increased flexibility in theory, but in practice a single step is used by <ref type="bibr">Kingma et al. [2016b]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.10.4">Importance weighted VAE</head><p>Overly simplistic assumptions about the posterior can lead to overly simple latent representations, that do not use the full modelling capacity of the network. Importance weighting can be used to improve inference by deriving strictly tighter lower bounds on the log-likelihood using several samples in latent space to approximate the posterior, without otherwise modifying the VAE framework. This yields increased flexibility to model posteriors that go beyond the standard VAE modelling assumptions, and can improve likelihood performance, at train or test time.</p><p>To present it, we write the ELBO differently:</p><formula xml:id="formula_67">log p θ (x) = log E q φ (z|x) p θ (x, z) q φ (z|x) ≥ E q φ (z|x) log p θ (x, z) q φ (z|x) = L elbo θ,φ (x).</formula><p>(2.61)</p><p>The gradients of L elbo θ,φ are obtained using the re-parametrization trick, using z = µ + σ . Because the distribution on does not depend on θ, the differentiation and expectation on can be swapped:</p><formula xml:id="formula_68">∇ φ L elbo θ,φ (x) = ∇ φ E q φ (z|x) log p θ (x, z) q φ (z|x) = E ∼N (0,I) ∇ φ log p θ (x, z( , φ)) q φ (z( , φ)|x) . (2.62)</formula><p>The gradient inside the expectation can be approximated via Monte Carlo estimation, by generating k samples of and using standard back-propagation. Let w θ,φ (x, z) = p θ (x, z)/q φ (z|x), then</p><formula xml:id="formula_69">∇ φ E q φ (z|x) log p θ (x, z) q φ (z|x) ≈ 1 k k i=1 ∇ φ log w θ,φ (x, z( k , φ)).</formula><p>(2.63)</p><p>This yields an unbiased estimate of ∇ φ L(x), with a variance that decreases as k increases. In the case where k &gt; 1, the additional samples can also be used to construct a tighter lower-bound using the Jensen inequality:</p><formula xml:id="formula_70">L elbo k (x, θ, φ) = E z 1 ,...,z k ∼q φ (z|x) ln 1 k k i=1 w θ,φ (x, z i ) (2.64) ≤ ln E z 1 ,...,z k ∼q φ (z|x) 1 k k i=1 w(x, z i ) (2.65) = ln E z∼q φ (z|x) w θ,φ (x, z) (2.66) = ln p θ (x).</formula><p>(2.67)</p><p>In the above expression, the standard VAE lower bound is recovered for k = 1. For all k, the lower bounds tighten as k increases, i.e. F k ≤ F k+1 ≤ ln p(x). <ref type="bibr" target="#b14">Burda et al. [2016]</ref> showed that if the weights are bounded, then L k → ln p(x) as k → ∞. These tighter bounds can be used in two ways: i) to improve the training objective, in which case k ≈ 10 is typically used as a trade-off between tightness and computational efficiency, and ii) only at test time, to define a more accurate bound on the log-likelihood of an already trained VAE, in which case k ≈ 10 3 can be used. The gradients of the importance weighted lower bound can be expressed as:</p><formula xml:id="formula_71">∇L elbo k (x, θ, φ) = E z 1:k ∼q φ (z|x) k i=1 w i θ,φ ∇ ln p(x, z i ) -ln q φ (z i |x) . (2.68)</formula><p>This update rule is similar to that of the VAE, but the samples weighted w.r.t. the true posterior. Normalized importance weights are computed as:</p><formula xml:id="formula_72">w i θ,φ = w θ,φ (x, z i )/ k j=1 w θ,φ (x, z j )</formula><p>. This allows for more accurate models with complex posteriors: poor choices of z will be penalised proportionally to the importance weights. This is to be contrasted with the mode-seeking behaviour of the standard ELBO, which will strongly penalyse the model for putting mass in low density regions of the true posterior rather than in high density ones. Richer approximate posteriors can be learned owing to this relaxation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.10.5">Quantized variational auto-encoders</head><p>The VAE model can be used with a discrete latent space rather than a continuous one, as pioneered by van den <ref type="bibr">Oord et al. [2017]</ref> and further extended by <ref type="bibr">Razavi et al. [2019b]</ref> and <ref type="bibr" target="#b38">Fauw et al. [2019]</ref>. To do so, a finite number K of latent vectors e i of dimension D are learned by gradient descent, constituting a codebook, e = {e 1 . . . e K }. Given an input x, the encoder f φ yields f φ (x). which is then discretized by a nearest neighbour look-up, using the codebook e:</p><formula xml:id="formula_73">q φ (z = k|x) = 1 if k = argmin j f φ (x) -e j 2 , 0 otherwise , (2.69)</formula><p>The discretized encoding is then the input to the decoder, let us denote it z q φ (x) . The embedding space e is trained together with the parameters of the encoder and decoder. Note that this can be extended to use more than one element of the codebook per image, for instance by using a latent vector field with a spatial resolution of H × H, and with D seen as a channel dimension. In that case the codebook can yield K (H×H) different combinations of shape (H, H, D).</p><p>This formulation does not break the variational interpretation of the auto-encoder, in the sense that log p(x) is bounded with an ELBO. Indeed q φ (z = k|x) is deterministic, and a uniform prior over z yields a constant KL divergence, equal to log K, per latent embedding. The likelihood of the model is</p><formula xml:id="formula_74">log p(x) = log k p(x|z k )p(z k ). (2.70)</formula><p>This can be bounded using Jensen's inequality: log p(x) ≥ log p(x|z q φ (x) )p(z q φ (x) ), which is the bound reported to evaluate the model. Equation <ref type="formula" target="#formula_91">2</ref>.69 is not differentiable, so training of the encoder is done by copying the gradients from the discretized input of the decoder z q φ (x) to the encoder output f φ (x). This does not provide signal to train the embedding vectors e i . Therefore an l 2 distance measuring the error introduced by the nearest-neighbour lookup is also minimized. The full loss function is summarized in Equation <ref type="formula" target="#formula_91">2</ref>.71 where "sg" stands for the stop-gradient operator and β is an hyper-parameter to be tuned.</p><formula xml:id="formula_75">L vq-vae = log p θ (x|z q φ (x) ) + sg[f φ (x)] -e 2 2 + β f φ (x) -sg[e] 2 2 , (2.71)</formula><p>One intersting property of this approach is that because the KL cost of the encoder is constant, it can be used together with very expressive autoregressive decoders without suffering from posterior collapse, as shown by <ref type="bibr" target="#b38">Fauw et al. [2019]</ref>.</p><p>Auto-regressive priors. As in standard VAEs, there is a miss-match between the prior and true posterior. Therefore samples from the prior do not perfectly match samples seen by the decoder at training time, which degrades sample quality. To improve on this, it is possible to train an additional prior with the posterior distribution as target, which will hopefully be closer to the posterior than the constant, uniform prior. Autoregressive models, which perform well on discrete data, can be used, and <ref type="bibr">Razavi et al. [2019b]</ref> has shown this to be beneficial to sample quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.11">Lipschitz continuity for generative adversarial networks</head><p>We now make a more in depth presentation of adversarial models, introduced in Section 2.7.</p><p>A lot of research has been devoted to developing more stable training procedures, through the design of better losses. In Section 2.11, we present a connection of adversarial training with optimal transport that has led to a now almost ubiquitous variant of GAN regularization, based on Lipschitz continuity. Generative adversarial networks are notoriously unstable to optimize and research has been devoted to improving adversarial training procedures. One variation that has emerged is the use of Lipschitz continuity as a regularizer, which can be motivated with an interesting connection to optimal transport and the Wasserstein distance. We chose to present this development in this chapter because it may be, at present, the most widely used GAN variant.</p><p>Limitations of the vanilla GAN. One source of difficulty in training GANs is that strong discriminator lead to vanishing gradients of <ref type="bibr" target="#b3">Arjovsky et al., 2017]</ref>. This phenomenon is especially prone to happen early in training, when the generator is essentially random and it is easy for the discriminator to be very accurate This makes it important to tune the capacity and training regime of the discriminator. To mitigate vanishing gradients early in training, <ref type="bibr" target="#b42">Goodfellow et al. [2014]</ref> proposed to train the generator by minimizing -E pz ln(D(G(z))) to boost the gradient signal in early training. This loss has the same stable points in the minimax optimization of V (θ, φ) and helps in early training, but the problem remains: if at some point D φ becomes too good, it's gradients vanish and no signal remains to train G θ . This is a motivation for designing better adversarial losses.</p><formula xml:id="formula_76">E pz ln(1 -D φ (G θ (z))) w.r.t. θ [</formula><p>Both maximum-likelihood estimation and standard GAN objective rely on Kullback-Liebler divergence minimization. Indeed recall that the optimal loss approximated by D φ is the Jensen-Shannon divergence, composed of two KL divergences. An important property of D KL (p||q) is that it is infinite if p has a zero in the support of q, which is easy to check from the definition D KL (p||q) = x p(x) ln q(x) -ln p(x) dx.. This means it can not be used to train a model that does not cover the full support of the training set. Because in GANs latent variable vectors z live in a lower dimensional space than natural images, G θ (z) is a low-dimensional manifold of R N , and has degenerate support almost surely <ref type="bibr" target="#b3">Arjovsky et al. [2017]</ref>. Therefore, the optimal loss approximated by D φ is degenerate. Note that in maximum-likelihood estimation this problem is handled by adding volume around the low dimensional manifold G θ (z), for instance with Gaussian noise, to obtain a density with non-degenerate support, see Section 2.9.1 for detail.</p><p>The earth-mover distance as an alternative adversarial loss. Alternative adversarial losses can be derived using optimal transport. Let us consider a joint distribution γ(x, y) with marginals p(x) = γ(x) and q(y) = γ(y). The conditional γ(y|x) can be seen as "moving mass" to transform p(•) into q(•). A notion of cost associated with a given transformation γ can be defined as:</p><formula xml:id="formula_77">T (γ) = x,y γ(x, y) ||x -y|| = x p(x) y γ(y|x) ||x -y|| (2.72)</formula><p>This definition is rather intuitive: if γ(y|x) is high, γ is likely to transform x into y, and thus to require moving by ||x -y||. T (γ) is thus the cost of transformation using γ. One can then seek to find a γ that minimizes the cost of transportation. This cost is called the Wasserstein distance D W S , and is the cost of optimal transportation: D W S (p||q) = inf γ∈Γ(p,q) T (γ). Intuitively, it is based on a notion of distance between p and q, rather than a notion of overlap as is the case for D KL , and so is defined between densities that have non-overlapping support.</p><p>Consider an example where R 2 is the ambient space, with x = (x 1 , x 2 ). Assume two densities p 0 and p θ with low dimensional support: Dual definition and approximation. In general, the Wasserstein distance is challenging to compute. For high dimensional data an arbitrary γ can not be integrated out in closed form, so it is impossible to find the infimum on γ. However, the Kantorovich-Rubinstein duality Theorem <ref type="bibr" target="#b151">[Villani, 2009]</ref> states that D W S can equivalently be defined as:</p><formula xml:id="formula_78">p 0 is uniform on x 2 ∈ [0, 1] for x 1 = 0, and p θ is uniform on x 2 ∈ [0, 1] for x 1 = θ. This is illustrated in</formula><formula xml:id="formula_79">D W S (p * ||q) = inf γ∈Γ(p,q) T (γ) = 1 k max ||D φ || L ≤k E p * D φ (x) -E pz D φ (G θ (z)), (2.73)</formula><p>In the previous equations, ||.|| L denotes the Lipschitz norm. Intuitively, this theorem says that finding the optimal transport between p and q is the same as finding a Lipschitz function D φ that maximally separates the two. This result is similar in spirit to the duality in linear programming.</p><p>The constant k can be viewed as a scaling factor and safely set to 1.</p><p>This formulation points to approximation schemes for D W S . Suppose taking the supremum over k-Lipschitz function is intractable. Then, perhaps the supremum can be taken on a strict subset</p><formula xml:id="formula_80">F k ⊂ {f | f L ≤ k}.</formula><p>If F k "covers the full set well" in some sense, then the approximation should be good. In the context of deep learning, the universal approximation theorem <ref type="bibr" target="#b21">[Cybenko, 1989]</ref> can be invoked: with enough flexibility (number of "neurons"), any k-Lipschitz function can in theory be approximated to arbitrary precision. So in practice, D φ is restricted to some deep network architecture, parametrized by φ, such that ||D φ || L ≤ k, and φ is optimized by gradient descent to find the supremum.</p><p>From theory to practice.</p><p>The dual formulation of the loss in Equation <ref type="formula" target="#formula_91">2</ref>.73 now looks very similar to the training loss of a GAN discriminator:</p><formula xml:id="formula_81">L wgan = 1 k max ||D φ || L ≤k {E p * [D(x)] -E pz [D φ (G θ (z))]} (2.74) L gan = 1 k max D φ {E p * [log(D φ (x))] + E pz [log(1 -D φ (G θ (z)))]} (2.75)</formula><p>There are two differences: i) log has disappeared, and ii) the Lipschitz norm of D φ is constrained. Theoretically i) should not matter: the universal approximation theorem states that exp can be approximated, so the log can be inverted by setting D φ = exp •D φ . Nevertheless, this difference can have a big impact in practice: the squashing effect of the log function can strongly aggravate the vanishing gradient problem when points are far from the decision boundary.</p><p>Constraining the Lipschitz norm is easy in practice. In the case where ReLu <ref type="bibr" target="#b41">[Glorot et al., 2011]</ref> non-linearities are used, they do not modify the Lipschitz norm. It is thus enough to control the norm of each layer l i , and use the bound:</p><formula xml:id="formula_82">D φ L ≤ L i=1 l i L .</formula><p>(2.76)</p><p>Without loss of generality, k can be set to 1, so if l i ≤ 1 for all i, D φ ≤ 1. Because each l i is a linear operation, ||l i || is the largest singular value of l i , σ(l i ). So ensuring D φ ≤ 1 can be done by bounding σ(l i ) by 1 for all i. This can be enforced by clipping the weights of the discriminator, as originally proposed in <ref type="bibr" target="#b3">Arjovsky et al. [2017]</ref>. An other option is to add a penalty on the magnitude of gradients <ref type="bibr">[Gulrajani et al., 2017a]</ref>:</p><formula xml:id="formula_83">G pen = λE x [(||∇ x D(x)|| 2 -1) 2 ].</formula><p>(2.77)</p><p>A third possibility is to estimate the largest singular value of each layer l i , and normalize: li ← l i / σ(l i ). This can be done using the power iteration method, see <ref type="bibr">Miyato et al. [2018b]</ref>.</p><p>Controlling the Lipschitz norm of the discriminator has been shown to be of great help in practice <ref type="bibr">[Miyato et al., 2018b]</ref>, and is now common practice.</p><p>The use of D W S instead of D KL , which seems crucially different at first, leads to a loss that is rather similar to the vanilla GAN loss. How relevant is the analysis in Section 2.11 in practice, and could we simply say that penalizing the Lipschitz norm is a good regularizer? First, it is important to note that the analysis in Section 2.11 regards the ideal losses, D KL vs. D W S . In practice, both are (losely) approximated by similar discriminators, and a property that is true of the limits of sequences of functions is not necessarily true of the functions themselves. In particular, note that in the standard GAN, having distributions with non-overlapping supports does not break D φ , in the sense that a finite value is computed by the discriminator. This ability to handle support that don't overlap is precisely the reason why it is possible to train without adding volume around the low-dimensional manifold learned by the model, unlike in VAEs where this breaks likelihood computations. Overall, this analysis guarantees that the optimal solution D * φ (θ) is well behaved in the Wasserstein framework, but may be less critical when considering how to approximate it with a discriminator. However, it does provide an insightful connection to optimal transport, and has led to the now standard use of Lipschitz regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.12">Evaluation metrics for generative models</head><p>Evaluating generative models applied to complex natural images is still an open research problem. Several metrics exist, with their complementarities and respective drawback. The classical evaluation is to report the log-likelihood performance of the model, normalised by the number of pixel values, which yields the Bits-per-dim metric (BPD). By reporting it on unseen data, one can detect over-fitting and know how well the model generalises to the full support of p * . In practice, image data is discretized, which can complicate log-likelihood evaluations. In sections 2.12.1 and 2.12.2, we present two ways of dealing with it, namely data dequantization and practical discrete parametric densities, which have an impact on both training and evaluation. One drawback of BPD is that it favours models with good coverage of the support over models that produce good looking images but do not cover the full support. Other metrics that better evaluate the visual quality of samples are therefore also desirable. Several of these have been develloped with GAN evaluation in mind, because i) GANs do not readily offer log-likelihood evaluations and ii) this type of metric is more aligned with the training objective of GANs. We present them in Section 2.12.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.12.1">Data dequantization</head><p>In practice, image datasets such as CIFAR10 or ImageNet, are discrete representations of continuous signals that has been quantized (e.g. 8-bits colors). Though it is natural to assume continuous density models for natural images, fitting such models to discrete data is degenerate. This is because the probability of a singleton, which is of measure 0, under a continuous model is always 0. Training on discrete data anyway produces a degenerate solution that collapses probability mass to discrete datapoints, and evaluating p θ (x) will yield very high values, that in theory tend to +∞. The first solution, called dequantization, is to convert the discrete data distribution into a continuous distribution. The second is to use discrete output distributions, despite the continuous nature of real images.</p><p>Dequantization can be performed by adding uniform noise to the discrete data. For instance if x has D dimensions, and each discrete component is encoded with 8-bits, (i.e. takes values in {0, 1, . . . , 255}) then the data can be dequantized by applying y = x + u, with u sampled uniformly from [0, 1) D . This is equivalent to using rectangular approximations of the continuous density function to integrate it. As shown by <ref type="bibr" target="#b141">Theis et al. [2016]</ref>, training a continuous p θ on y can be interpreted as training another, related and discrete, model P θ on the discrete data x by maximizing a lower bound on its log-likelihood. Indeed, let</p><formula xml:id="formula_84">P θ (x) := [0,1) D p θ (x + u) du (2.78)</formula><p>If P * is the true density of the discrete data (i.e. the density obtained by discretizing the true, continuous, data distribution), and p * is the distribution of uniformly dequantized data, Jensen's inequality yields</p><formula xml:id="formula_85">E y∼p * [log p θ (y)] = x P * (x) [0,1) D log p θ (x + u) du (2.79) ≤ x P * (x) log [0,1) D p θ (x + u) du (2.80) = E x∼P * [log P θ (x)] (2.81)</formula><p>Thus p θ can not collapse onto the discrete data, because its objective is bounded above by the log-likelihood of a discrete model and so bounded by a finite value. Uniform dequantization is correct but can be harmfull to performance. Intuitively, the data is corrupted with random noise from which no signal can be extracted, and the model needs to learn to be invariant to that noise. Indeed p θ needs to assign uniform density to unit hypercubes x + [0, 1) D , centered around the data x. Smooth function approximators are not well suited to that task that requires using high-frequencies, which is in turn usually discouraged by regularization. Variational inference can be used instead to perform dequantization, as proposed in <ref type="bibr" target="#b52">Ho et al. [2019]</ref>. To build a better dequantization noise distribution q(u), still with support over u ∈ [0, 1) D , q can be made a function of x, q(u|x). Then for all q:</p><formula xml:id="formula_86">E x∼P * [log P θ (x)] = E x∼P * log [0,1) D q(u|x) p θ (x + u) q(u|x) du (2.82) ≥ E x∼P * [0,1) D q(u|x) log p θ (x + u) q(u|x) du (2.83) = E x∼P * E u∼q(•|x) log p θ (x + u) q(u|x) (2.84)</formula><p>Using an expressive q allows p θ to place density in each hypercube x + [0, 1) D according to flexible distribution q(u|x) which is more natural for p θ . In particular, a uniform distribution on u is within the family of distributions that can be learned by q. Therefore by taking the optimum over that family we necessarily get a tighter bound than by using a uniform q. In practice, q is implemented using a small network and optimized jointly with p θ using gradient descent, estimating the gradients by monte-carlo approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.12.2">Discrete parametric densities</head><p>Rather than dequantize the discrete data, it is possible to use discrete parametric densities. One possiblity, proposed by <ref type="bibr" target="#b105">Oord et al. [2016]</ref> is to use a softmax operator: given scores s i for each color class,</p><formula xml:id="formula_87">p soft (k) = e s k i e s i (2.85)</formula><p>This has the advantage that no parametric assumptions about the distributions predicted need to be made: a softmax can be arbitrarily multimodal, skewed, peaked or long tailed. There is also no need to worry about parts of the distribution mass that may be outside the interval 0, 255. A drawback is that no prior information is embedded about the relations between the 256 color categories. The notion that values i and i + 1 are neighbors is lost and has to be learned by the model, and small mistakes are penalised as much as big ones.</p><p>Another option is model color intensity with a continuous distribution, but taken from a family of functions that can be discretized exactly. <ref type="bibr">Kingma et al. (2016)</ref> use the logistic density which is parametrised by a location parameter µ i , a scale parameter σ i , and resembles a Gaussian density. It integrates to the sigmoid function σ, so it is easy to discretize: <ref type="bibr">Salimans et al. [2017a]</ref>, extend this approach using mixtures of logistic distributions, to have multiple modes as with the softmax. In practice a relatively small number of mixture components (5 to 10), is enough for natural images datasets.</p><formula xml:id="formula_88">P (x i |π, µ, s) = σ((x i + 0.5 -µ i )/s i ) -σ((x i -0.5 -µ i )/s i ) (2.86)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.12.3">GAN evaluations</head><p>Quantitative evaluation of Generative Adversarial Networks is challenging, in part due to the absence of log-likelihood but also because GANs are primarily optimised for image quality rather than support coverage. To measure the quality of samples, the ideal would be to directly evaluate p * (x). That is of course impossible, since learning p * is precisely the problem we are trying to solve. A simple solution could be to use the dataset to approximate p * by fitting parzen windows on data points <ref type="bibr" target="#b42">[Goodfellow et al., 2014]</ref>. In high dimensions, however, this provides very poor evaluations <ref type="bibr" target="#b141">[Theis et al., 2016]</ref>.</p><p>It is also important to note that any metric evaluating quality only would be degenerate as an evaluation for p θ , as collapsing p θ to the mode of the distribution would maximize it. Such a metric would need to be complemented with an other measure.</p><p>The most popular solutions to automate the qualitative evaluation of samples rely on the idea of relying on a pretrained image classifier to provide the evaluation. Intuitively, we know how to train classifiers that are very accurate on unseen data, which means that they have some "understanding" of what real images should look like. For instance, one can consider that a classifier should assign a low entropy distribution over classes to a good sample, and be confused and assign a high-entropy distribution over classes for a poor sample. This is the idea behind the Inception Score (IS), proposed by <ref type="bibr">Salimans et al. [2016b]</ref>. Another possibility is to compare the behaviour of a classifier when applied to a set of samples vs. a set of real images, which is the intuitive idea behind the Fréchet Inception distance (FID), proposed in <ref type="bibr" target="#b51">Heusel et al. [2017]</ref>.</p><p>Inception score (IS). The inception score is a statistic of the generated images, based on an external inception network <ref type="bibr" target="#b140">[Szegedy et al., 2015]</ref>, trained for classification on ImageNet. The score is given by:</p><formula xml:id="formula_89">IS(p θ ) = exp(E x∼p θ D KL (p(y|x)||p(y))), (2.87)</formula><p>where p(y|x) is the conditional class distribution obtained by applying the pre-trained classification network to the generated images, and p(y) = x p(y|x)p θ (x) is the class marginal over the generated images. To understand this score intuitively, decompose the Kullback-Liebler divergence in two:</p><formula xml:id="formula_90">E x∼p θ [D KL (p(y|x)||p(y))] = E x∼p θ y p(y|x) log p(y|x)dy - y p(y|x) log(p(y))dy .</formula><p>(2.88)</p><p>The first term inside the expectation is minus the entropy of the classifier, for a fixed x. This captures the idea that an image is condidered "good" if it can be well identified as a known object by the classifier, in which case the distribution over classes has low entropy. When taking the expectation over x, the average entropy should be low. The second term is the cross-entropy between y|x and y, and captures the idea that y|x should have as much variability as y and thus the entropy of averages should be high.</p><p>Fréchet Inception distance (FID). The Fréchet Inception distance compares the distributions of Inception embeddings i.e., activations from the penultimate layer of the Inception network, of real (p r (x)) and generated (p g (x)) images. Both of these distributions are modelled as multidimensional Gaussians parametrized by their respective mean and covariance. Finally, the score is computed as the Fréchet distance between the two Gaussian densities, which can be reduced to:</p><formula xml:id="formula_91">d 2 ((m r , C r ), (m g , C g )) = m r -m g 2 + Tr(C r + C g -2(C r C g ) 1 2 ),<label>(2.89)</label></formula><p>where (m r , C r ), (m g , C g ) denote the mean and covariance of the real and generated image distributions respectively.</p><p>Unlike the Inception Score, the Fréchet Inception distance does compare distributions obtained with real and fake images, and in that sense is similar to a distance. This more directly captures a notion of coverage: to be similar, the distributions of both real and fake inception activations must have the same "coverage". In the case of IS, this is more indirect, and works similarly to the GAN training loss: images must have variability, so the mass must be spread out. Because images must also look good, implicitly the mass must spread to the training support. In practice, however, both metrics are heavily dominated by quality as we now demonstrate.</p><p>Practical use of IS and FID. IS and FID correlate predominantly with the quality of samples.</p><p>In GAN literature, for instance <ref type="bibr">[Miyato et al., 2018a]</ref> To obtain a quantitative estimation of how much entropy/coverage impacts the IS and FID measures, we evaluate the scores obtained by random subsamples of the dataset, such that the quality is unchanged but coverage progressively degrades (see details of the scores below). Table <ref type="table">2</ref>.1 shows that when using the full set of images (50k) the FID is 0 as the distributions are identical. Notice that as the number of images decreases, IS is very stable (it can even increase, but by very low increments that fall below statistical significance, with a typical standard deviation of 0.1). This is because the entropy of the distribution is not strongly impacted by sub-sampling, even though coverage is. FID is more sensitive, as it behaves more like a measure of coverage (it compares the two distributions). Nonetheless, the variations remain extremely low even when dropping most of the dataset. For instance, when removing 80 percent of the dataset (i.e., using 10k images), FID is at 2.10, to be compared with typical GAN values that are around 20.</p><p>These measurements demonstrate that IS and FID scores are heavily dominated by the quality of images. From this, we conclude that IS and FID can be used as reasonable proxies to asses sample quality, even though they are also slightly influenced by coverage. One should bear in mind, however, that a small increase in these scores may come from better coverage rather than improved sample quality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Introduction</head><p>Latent variable approaches can learn disentangled and concise representations of the data <ref type="bibr" target="#b10">[Bengio et al., 2013]</ref>, which are useful for compression <ref type="bibr" target="#b44">[Gregor et al., 2016]</ref> and semi-supervised learning <ref type="bibr">[Kingma et al., 2014</ref><ref type="bibr" target="#b113">, Rasmus et al., 2015]</ref>. Autoregressive models on the other hand, are very effective to model low-level image statistics, and obtain state-of-the-art likelihoods on test data. One interesting property of VAEs is that can learn latent variable representations that abstract away from low-level details, but model pixels as conditionally independent given the latent variables. This renders the generative model computationally efficient, but the lack of low-level structure modeling leads to overly smooth and blurry samples Section 2.9.1. Autoregressive models, such as pixelCNNs <ref type="bibr" target="#b105">[Oord et al., 2016]</ref>, on the other hand, estimate complex translation invariant conditional distributions among pixels. They are effective to model low-level image statistics, and yield state-of-the-art likelihoods on test data <ref type="bibr">[Salimans et al., 2017a]</ref>. This is in line with the observations of <ref type="bibr" target="#b71">Kolesnikov and Lampert [2017]</ref> that low-level image details account for a large part of the likelihood. These autoregressive models, however, do not learn a latent variable representations to support, e.g., semi-supervised learning.</p><p>The complementary strengths of VAEs and pixelCNNs, modeling global and local image statistics respectively, suggest hybrid approaches combining the strengths of both. Prior work on such hybrid models needed to limit the capacity of the autoregressive decoder to prevent degenerate models that completely ignore the latent variables and rely on autoregressive modeling only <ref type="bibr" target="#b16">[Chen et al., 2017</ref><ref type="bibr">, Gulrajani et al., 2017c]</ref>. In this chapter we describe Auxiliary Guided Autoregressive Variational autoEncoders (AGAVE), an approach to train such hybrid models using an auxiliary loss function that controls which information is captured by the latent variables and what is left to the AR decoder. See Figure <ref type="figure" target="#fig_8">3</ref>.1 for a schematic illustration of our approach. Using high-capacity VAE and autoregressive components allows our models to obtain quantitative results on held-out data that are on par with the state of the art. Our models generate samples with both global coherence and low-level details. In Section B.2, related work on generative modelling of natural images is presented. In Section 3.3.2, the general motivation for using autoregressive decoders is given, followed by a discussion on why these models, when implemented naively, lead to posterior collapse, and finally by a presentation of our approach that circumvents this problem. Section B.4 contains experimental evaluation of the proposed approach, and ablations study to demonstrate the importance of each components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Related work</head><p>A variety of approaches leverage deep neural networks to learn complex density models. These include the variational autoencoders and autoregressive models that form the basis of our work, but also generative adversarial networks (GANs) <ref type="bibr" target="#b3">[Arjovsky et al., 2017</ref><ref type="bibr" target="#b42">, Goodfellow et al., 2014]</ref> and variable transformation with invertible functions <ref type="bibr" target="#b27">[Dinh et al., 2017]</ref>. While GANs produce visually appealing samples, they suffer from mode dropping and their likelihood-free nature prevents measuring how well they model held-out test data, see Section 2.9.2. In particular, GANs can only generate samples on a non-linear manifold in the data space with dimension equal to the number of latent variables. In contrast, this chapter focuses on probabilistic models that generalize to the entire data space, and provide likelihoods of held-out data that can be used for compression, and to quantitatively compare different models. To improve the quality of samples, a structured noise model is proposed, to reduce over-generalization in a MLE setting, see Section 2.9.1. The non-volume preserving (NVP) transformation approach of <ref type="bibr" target="#b27">Dinh et al. [2017]</ref> chains together invertible transformations to map a basic (e.g. unit Gaussian) prior on the latent space to a complex distribution on the data space. This method offers tractable likelihood evaluation and exact inference, but obtains likelihoods on held-out data below the values reported using state-of-the-art VAE and autoregressive models. Moreover, it is restricted to use latent representations with the same dimensionality as the input data, and is thus difficult to scale to model high-resolution images.</p><p>Autoregressive density estimation models, such as pixelCNNs <ref type="bibr" target="#b105">[Oord et al., 2016]</ref>, admit tractable exact likelihood evaluation, while for variational autoencoders <ref type="bibr">[Kingma and</ref><ref type="bibr">Welling, 2014a, Rezende et al., 2014]</ref> accurate approximations can be obtained using importance sampling <ref type="bibr" target="#b14">[Burda et al., 2016]</ref>. Naively combining powerful pixelCNN decoders in a VAE framework results in a degenerate model which ignores the VAE latent variable structure, as explained through the lens of bits-back coding by <ref type="bibr" target="#b16">Chen et al. [2017]</ref>. To address this issue, the capacity of the the autoregressive component can be restricted. This can, for example, be achieved by reducing its depth and/or field of view, or by giving the pixelCNN only access to grayscale values, i.e. modeling p(x i |x &lt;i , z) = p(x i |gray(x &lt;i ), z) <ref type="bibr" target="#b16">[Chen et al., 2017</ref><ref type="bibr">, Gulrajani et al., 2017c]</ref>. This forces the model to leverage the latent variables z to model part of the dependencies among the pixels. This approach, however, has two drawbacks. (i) Curbing the capacity of the model is undesirable in unsupervised settings where training data is abundant and overfitting unlikely, and is only a partial solution to the problem. (ii) Balancing what is modeled by the VAE and the pixelCNN by means of architectural design choices requires careful hand-design and tuning of the architectures. This is a tedious process, and a more reliable principle is desirable. To overcome these drawbacks, we propose to instead control what is modeled by the VAE and pixelCNN with an auxiliary loss on the VAE decoder output before it is used to condition the autoregressive decoder. This allows us to "plug in" powerful high-capacity VAE and pixelCNN architectures, and balance what is modeled by each component by means of the auxiliary loss.</p><p>In a similar vein, <ref type="bibr" target="#b71">Kolesnikov and Lampert [2017]</ref> force pixelCNN models to capture more high-level image aspects using an auxiliary representation y of the original image x, e.g. a low-resolution version of the original. They learn a pixelCNN for y, and a conditional pixelCNN to predict x from y, possibly using several intermediate representations. This approach forces modeling of more high-level aspects in the intermediate representations, and yields visually more compelling samples. <ref type="bibr" target="#b116">Reed et al. [2017]</ref> similarly learn a series of conditional autoregressive models to upsample coarser intermediate latent images. By introducing partial conditional independencies in the model they scale the model to efficiently sample high-resolution images of up to 512×512 pixels. <ref type="bibr" target="#b44">Gregor et al. [2016]</ref> use a recurrent VAE model to produces a sequence of RGB images with increasing detail derived from latent variables associated with each iteration. In <ref type="bibr" target="#b25">[Denton et al., 2015]</ref>, adversarially generated images are progressively refined using a Laplacian pyramid framework. Like our work, all these models work with intermediate representations in RGB space to learn accurate generative image models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Auxiliary guided autoregressive variational autoencoders</head><p>For self-containedness, we give a brief overview of variational autoencoders and relevant limitations in Section 3.3.1, before we present our approach to learning variational autoencoders with autoregressive decoders in Section 3.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Variational autoencoders with autoregressive decoders</head><p>Variational autoencoders <ref type="bibr">[Kingma and</ref><ref type="bibr">Welling, 2014a, Rezende et al., 2014]</ref> learn deep generative latent variable models using two neural networks. The "decoder" network implements a conditional distribution p θ (x|z) over observations x given a latent variable z, with parameters θ. Together with a basic prior on the latent variable z, e.g. a unit Gaussian, the generative model on x is obtained by marginalizing out the latent variable:</p><formula xml:id="formula_92">p θ (x) = p(z)p θ (x|z) dz.</formula><p>(3.1)</p><p>The marginal likelihood can, however, not be optimized directly since the non-linear dependencies in p θ (x|z) render the integral intractable. To overcome this problem, an "encoder" network is used to compute an approximate posterior distribution q φ (z|x), with parameters φ. The approximate posterior is used to define a variational bound on the data log-likelihood, by subtracting the Kullback-Leibler divergence between the true and approximate posterior:</p><formula xml:id="formula_93">ln p θ (x) ≥ L(θ, φ; x) = ln(p θ (x)) -D KL (q φ (z|x)||p θ (z|x)) (3.2) = IE q φ [ln(p θ (x|z)] Reconstruction -D KL (q φ (z|x)||p(z)) Regularization .</formula><p>(3.</p><p>3)</p><p>The decomposition in Equation <ref type="formula" target="#formula_94">3</ref>.3 interprets the bound as the sum of a reconstruction term and a regularization term. The first aims to maximize the expected data log-likelihood p θ (x|z) given the posterior estimate q φ (z|x). The second term prevents q φ (z|x) from collapsing to a single point, which would be optimal for the first term. See Section 2.5 for a more detailed presentation.</p><p>Variational autoencoders typically model the dimensions of x as conditionally independent,</p><formula xml:id="formula_94">p θ (x|z) = D i=1 p θ (x i |z),<label>(3.4)</label></formula><p>for instance using a factored Gaussian or Bernoulli model, see e.g. <ref type="bibr">Kingma and Welling [2014a]</ref>, <ref type="bibr">Kingma et al. [2016a]</ref>, <ref type="bibr" target="#b156">Yan et al. [2016]</ref>. The conditional independence assumption makes sampling from the VAE efficient: since the decoder network is evaluated only once for a sample z ∼ p(z) to compute all the conditional distributions p θ (x i |z), the x i can then be sampled in parallel. This comes at the price of over-generalisation and blurrier samples. Indeed, a result of relying on the latent variables to account for all pixel dependencies, however, is that all low-level variability must be modeled by the latent variables. Consider, for instance, a picture of a dog, and variants of that image shifted by one or a few pixels, or in a slightly different pose, with a slightly lighter background, or with less saturated colors, etc. If these factors of variability are modeled using latent variables, then these low-level aspects are confounded with latent variables relating to the high-level image content. If the corresponding image variability is not modeled using latent variables, it will be modeled as independent pixel noise. In the latter case, using the mean of p θ (x|z) as the synthetic image for a given z results in blurry samples, since the mean is averaged over the low-level variants of the image. Sampling from p θ (x|z) to obtain synthetic images, on the other hand, results in images with unrealistic independent pixel noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Autoregressive decoders in variational autoencoders</head><p>Autoregressive density models, see e.g. <ref type="bibr" target="#b39">[Germain et al., 2015, Larochelle and</ref><ref type="bibr" target="#b74">Murray, 2011]</ref>, rely on the basic factorization of multi-variate distributions,</p><formula xml:id="formula_95">p θ (x) = D i=1 p θ (x i |x &lt;i ),<label>(3.5)</label></formula><p>with x &lt;i = x 1 , . . . , x i-1 , and model the conditional distributions using a (deep) neural network.</p><p>For image data, PixelCNNs <ref type="bibr" target="#b105">[Oord et al., 2016</ref><ref type="bibr">, van den Oord et al., 2016c</ref>] use a scanline pixel ordering, and model the conditional distributions using a convolution neural network. The convolutional filters are masked so as to ensure that the receptive fields only extend to pixels x &lt;i when computing the conditional distribution of x i . PixelCNNs can be used as a decoder in a VAE by conditioning on the latent variable z in addition to the preceding pixels, leading to a variational bound with a modified reconstruction term:</p><formula xml:id="formula_96">L(θ, φ; x) = IE q φ D i=1 ln p θ (x i |x &lt;i , z) -D KL (q φ (z|x)||p(z)). (3.6)</formula><p>The regularization term can be interpreted as a "cost" of using the latent variables. To effectively use the latent variables, the approximate posterior q φ (z|x) must differ from the prior p(z), which increases the KL divergence. In practice, combining a VAE with a flexible decoder (for instance an autoregressive one) leads to the latent code being ignored. This problem could be attributed to optimization challenges: at the start of training q(z|x) carries little information about x, the KL term pushes the model to set it to the prior to avoid any penalty, and training never recovers from falling into that local minimum. In fact in <ref type="bibr" target="#b16">[Chen et al., 2017</ref><ref type="bibr" target="#b164">, Zhao et al., 2017]</ref> extensive explanations have been proposed, showing that the problem is deeper: for the loss in Equation <ref type="formula" target="#formula_94">3</ref>.6 and a decoder with enough capacity, it is optimal to encode no information about x in z by setting q(z|x) = p(z). This is problematic: as x becomes independent from z, the encoder is not used at all and meaningful latent representations cannot be learned. This behaviour is often referred to as the information preference property.</p><p>The information preference problem. Intuitively, the information preference property arises because the miss-match between the approximate posterior and the true, inaccessible, posterior has a cost given by the KL regularization term in the ELBO. It is worth paying that cost only if the decoder benefits even more to compensate. In practice autoregressive decoders already predict x well without using any information from a latent variable z, and using an encoder q φ does not improve the reconstruction cost enough to compensate the regularization cost of the information in z. To show this more formally, let us take the expectation over x of the ELBO:</p><formula xml:id="formula_97">-E x∼p * [L elbo θ,φ (x)] = E x∼p * [-log(p θ (x)) + D KL (q φ (z|x)||p θ (z|x))] . (3.7)</formula><p>The cross-entropy of p θ is lower-bounded by the Shannon entropy,</p><formula xml:id="formula_98">H(p * ) = E x∼p * [-log(p * (x))] (3.8)</formula><p>so we have:</p><formula xml:id="formula_99">-E x∼p * [L elbo θ,φ (x)] ≥ H(p * ) + E x∼p * [D KL (q φ (z|x)||p θ (z|x))] . (3.9)</formula><p>Let F Θ a family of generative models, F Θ = {p θ (x), θ ∈ Θ}. Denote by θ |z any θ ∈ Θ such that p θ |z uses it's latent variables, and denote Θ |z the set of such θs, i.e.</p><formula xml:id="formula_100">θ |z ∈ {θ ∈ Θ|p θ (z|x) = p(z)} = Θ |z .</formula><p>Suppose a given set F Φ of inference networks, F Φ = {q φ (z|x), φ ∈ Φ}, does not have sufficient capacity to offer perfect inference, such that</p><formula xml:id="formula_101">∀φ ∈ Φ, D KL (q φ (z|x)||p θ |z (z|x)) &gt; 0.</formula><p>This yields for any θ ∈ Θ |z so:</p><formula xml:id="formula_102">H(p * ) A &lt; H(p * ) + E x∼p * [D KL (q φ (z|x)||p θ (z|x))] B ≤ -E x∼p * [L elbo θ,φ (x)] C . (3.10)</formula><p>This shows that the performance C for θ |z ∈ Θ |z can not be better than B. Now assume F θ contains models that do not use their latent variables and with enough capacity to "squeeze" between A -which is the best value any model can attain -and B. In our context, this model would typically be autoregressive so let us denote it with subscript θ &lt;j,⊥ ⊥z . This yields:</p><formula xml:id="formula_103">H(p * ) A ≤ E x∼p * [-log(p θ &lt;j,⊥ ⊥z (x|x j&lt;i ))] &lt;C &lt; H(p * ) + E x∼p * D KL (q φ (z|x)||p θ |z (z|x)) B (3.11) ≤ -E x∼p * [L elbo θ,φ (x)] C (3.12)</formula><p>Thus, any use of z that any model p θ might do will degrade the optimal performance that can be attained, and it is better not to use z at all and rely only on the autoregressive component of the model. If the family F Θ contains sufficiently expressive autoregressive models, these will always be preferred to the models in F Θ that do use latent variables. This is not just a theoretical problem, as in practice, using flexibe autoregressive decoders naively systematically leads to posterior collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Auxiliary Guided Autoregressive Variational Autoencoder</head><p>Several attempts have been made to circumvent the information preference property. One successful approach is to restrict the number of layers of the auto-regressive component of the decoder to a few layers. The receptive field of a given pixel x i is then a strict subset of those in the image. Any dependence between x i and a pixel outside of the receptive field has to go through z, or the two will be sampled independently. This is the approach taken by <ref type="bibr" target="#b16">Chen et al. [2017]</ref>, <ref type="bibr">Gulrajani et al. [2017c]</ref>. The autoregressive component is in charge of modelling low level detail, which it excels at, while the VAE component models global structure. One drawback is that this is only a partial solution as it does not enable the use of powerful autoregressive models, since these would again suffer from the information preference property. Other solutions to force information into z include <ref type="bibr" target="#b164">Zhao et al. [2017]</ref>, which compute an estimation of the mutual information between z and x, and more recently <ref type="bibr">Razavi et al. [2019a]</ref> which proposes to constrain the posterior distribution to be at a minimal distance from the prior. Finally, when using auto-regressive decoders together with quantized latent variables, the issue of ignored latent variables can be avoided as presented in Section 2.10.5.</p><p>In this section we present our approach relying on an auxiliary reconstruction loss to control the information that goes into z. To ensure meaningful latent representation learning Chen et al.</p><p>[2017] and <ref type="bibr">Gulrajani et al. [2017c]</ref> restrict the capacity of the pixelCNN decoder. In our approach, in contrast, it is always optimal for the autoregressive decoder, regardless of its capacity, to exploit the information on x carried by z. We rely on two decoders in parallel: the first one reconstructs an auxiliary image y from an intermediate representation f θ (z) in a non-autoregressive manner.</p><p>The auxiliary image can be either simply taken to be the original image (y = x), or a compressed version of it, e.g. with lower spatial resolution or with a coarser color quantization. The second decoder is a conditional autoregressive model that predicts x conditioned onf θ (z). Modeling y in a non-autoregressive manner ensures a meaningful representation z and renders x and z dependent, inducing a certain non-zero KL "cost" in (3.6). The uncertainty on x is thus reduced when conditioning on z, and there is no longer an advantage in ignoring the latent variable for the autoregressive decoder. We provide a more detailed explanation of why our auxiliary loss ensures a meaningful use of latent variables in powerful decoders in Section 3.3.2. To train the model we combine both decoders in a single objective function with a shared encoder network:</p><formula xml:id="formula_104">(a) (b) (c) (d)</formula><formula xml:id="formula_105">L(θ, φ; x, y) = IE q φ D i=1 ln p θ (x i |x &lt;i , z) Primary Reconstruction + IE q φ   E j=1 ln p θ (y j |z)   Auxiliary Reconstruction -λ D KL (q φ (z|x)||p(z))</formula><p>Regularization .</p><p>(3.13) Treating x and y as two variables that are conditionally independent given a shared underlying latent vairable z leads to λ = 1. Summing the lower bounds in Eq. (3.3) and Eq. (3.6) of the marginal log-likelihoods of y and x, and sharing the encoder network, leads to λ = 2. Larger values of λ result in valid but less tight lower bounds of the log-likelihoods. Encouraging the variational posterior to be closer to the prior, this leads to less informative latent variable representations.</p><p>Sharing the encoder across the two decoders is the key of our approach. The factored auxiliary VAE decoder can only model pixel dependencies by means of the latent variables, which ensures that a meaningful representation is learned. Now, given that the VAE encoder output is informative on the image content, there is no incentive for the autoregressive decoder to ignore the intermediate representation f θ (z) on which it is conditioned. The choice of the regularization parameter λ and auxiliary image y provide two levers to control how much and what type of information should be encoded in the latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">It is optimal for the autoregressive decoder to use the latent variable</head><p>Using the auxilliary reconstruction term presented above, it is always optimal for the autoregressive decoder, no matter how expressive, to use the information contained in the latent variable we now show. Intuitively, the auxiliary term requires some information to go into the latent variable, which is then available 'for free' to the autoregressive decoder. Recall the quantities involved in Equation <ref type="formula" target="#formula_94">3</ref>.3.2:</p><formula xml:id="formula_106">H(p * ) A &lt; H(p * ) + E x∼p * [D KL (q φ (z|x)||p θ (z|x))] B ≤ -E x∼p * [L elbo θ,φ (x)] C . (3.14)</formula><p>To obtain the AGAVE setting, an autoregressive decoder is added. Using again the fact that the Shannon entropy lower-bounds the cross-entropy between the autoregressive decoder and the true distribution, we obtain:</p><formula xml:id="formula_107">C + H(X|Z) ≤ C + E z∼q(z|x) [- i log(p(x i |z, x j&lt;i ))] = C AGAVE . (3.15)</formula><p>The entropy of a random variable decreases when it is conditioned on another, i.e. H(X|Z) ≤ H(X) Therefore, the theoretical lower-bound on the expected code length in our setup is always better when the autoregressive component takes Z into account, no matter its expressivity. In the limit case of an infinitely expressive autoregressive decoder, denoted by * , the lower bound is attained and so</p><formula xml:id="formula_108">C * AGAV E = C + H(X|Z) ≤ C + H(X).</formula><p>In non-degenerate cases, the VAE is optimized to encode information about X into a meaningful Z, with potentially near perfect reconstructions, and there exists &gt; 0 such that H(X|Z) &lt; H(X) -, making the lower bound stricly better by a possibly big margin:  </p><formula xml:id="formula_109">C * AGAVE = C + H(X|Z) &lt; C + H(X). (<label>3</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental evaluation</head><p>In this section we describe our experimental setup, and present results on the CIFAR10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Dataset and implementation</head><p>The CIFAR10 dataset <ref type="bibr" target="#b72">[Krizhevsky, 2009]</ref> contains 6,000 images of 32×32 pixels for each of the 10 object categories airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. The images are split into 50,000 training images and 10,000 test images. We train all our models in a completely unsupervised manner, ignoring the class information.</p><p>We implemented our model based on existing architectures, in particular we use the VAE architecture of <ref type="bibr">Kingma et al. [2016a]</ref>. To deal with the fact that the data is discrete we use a logistic distributions over the RGB color values as in <ref type="bibr">Kingma et al. [2016a]</ref>. We let the intermediate representation f (z) output by the VAE decoder be the per-pixel and per-channel mean values of the logistics, and learn per-channel scale parameters that are used across all pixels. The cumulative density function (CDF), given by the sigmoid function, is used to compute probabilities across the 256 discrete color levels, or fewer if a lower quantization level is chosen in y. Using RGB values y i ∈ [0, 255], we let b denote the number of discrete color levels and define c = 256/b. The probabilities over the b discrete color levels are computed from the logistic mean and variance µ i and s i as</p><formula xml:id="formula_110">p(y i |µ i , s i ) = σ (c + c y i /c |µ i , s i ) -σ (c y i /c |µ i , s i ) .</formula><p>(3.17 For the pixelCNN we use the architecture of <ref type="bibr">Salimans et al. [2017a]</ref>, and modify it to be conditioned on the VAE decoder output f (z), or possibly an upsampled version if y has a lower resolution than x. In particular, we apply standard non-masked convolutional layers to the VAE output, as many as there are pixelCNN layers. We allow each layer of the pixel-CNN to take additional input using non-masked convolutions from the feature stream based on the VAE output. This ensures that the conditional pixelCNN remains autoregressive. To speed up training, we independently pretrain the VAE and pixelCNN in parallel, and then continue training the full model with both decoders. We use the Adamax optimizer <ref type="bibr">[Kingma and Ba, 2015b]</ref> with a learning rate of 0.002 without learning rate decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Quantitative performance evaluation.</head><p>Following previous work, we evaluate models on the test images using the bits-per-dimension (BPD) metric: the negative log-likelihood divided by the number of pixels values (3×32×32). It can be interpreted as the average number of bits per RGB value in a lossless compression scheme derived from the model. The comparison in <ref type="bibr">Table B.4</ref> shows that our model performs on par with the state-of-the-art results of the pixelCNN++ model <ref type="bibr">[Salimans et al., 2017a</ref>]. Here we used the importance sampling-based bound of <ref type="bibr" target="#b14">Burda et al. [2016]</ref> with 150 samples to compute the BPD metric for our model. 1 We refer to Figure <ref type="figure" target="#fig_8">3</ref>.2 for qualitative comparison of samples from our model and pixelCNN++, the latter generated using the publicly available code. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Effect of KL regularization strength.</head><p>In Figure <ref type="figure" target="#fig_8">3</ref>.3 we show reconstructions of test images and samples generated by the VAE decoder, together with their corresponding conditional pixelCNN samples for different values of λ. As expected, the VAE reconstructions become less accurate for larger values of λ due to the heavier weighting of the KL term, mainly by lacking details while preserving the global shape of the input. At the same time, the samples become more appealing for larger λ, suppressing the unrealistic high-frequency detail in the VAE samples obtained at lower values of λ. Note that the VAE samples and reconstructions become more similar as λ increases, which makes the input to the pixelCNN during training and sampling more consistent.</p><p>For both reconstructions and samples, the pixelCNN clearly takes into account the output of the VAE decoder, demonstrating the effectiveness of our auxiliary loss to condition high-capacity pixelCNN decoders on latent variable representations. Samples from the pixelCNN faithfully reproduce the global structure of the VAE output, leading to more realistic samples, in particular for higher values of λ. For λ = 2 the VAE reconstructions are near perfect during training, and the pixelCNN decoder does not significantly modify the appearance of the VAE output. For larger values of λ, the pixelCNN clearly adds significant detail to the VAE outputs. of information it receives drops. However, in terms of BPD which sums KL divergence and pixelCNN reconstruction, a substantial gain of 0.2 is observed increasing λ from 1 to 2, after which smaller but consistent gains are observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Role of the auxilliary representation</head><p>The auxilliary variables are taken into account Section 3.3.2 shows that in theory it is always optimal for the autoregressive decoder to take the latent variables into account. Figure <ref type="figure" target="#fig_8">3</ref>.5 demonstrates this empirically by displaying auxiliary representations f (z) with z sampled from the prior f (z) as well as nine different samples from the autoregressive decoder conditioned on f (z). This qualitatively shows that the low level detail added by the pixelCNN, which is crucial for log-likelihood performance, always respects the global structure of the image being conditioned on. The VAE decoder used to generate that figure was trained with λ = 8, and in that case the KL divergence weighs very little. Yet it controls the global structure of the samples, which shows that our setup can be used to get the best of both worlds. In Figure <ref type="figure" target="#fig_8">3</ref>.6, samples are obtained by encoding ground truth images, then interpolating the latent variables obtained, decoding them with the decoder of the V AE and adding low level detail with the pixelCNN. This demonstrates that the model has learned to use the latent variables z produced by the encoder.</p><p>The auxilliary loss is necessary: The fact that the autoregressive decoder ignores the latent variables could be attributed to optimization challenges, as explained in Section 3.3.2. In that case, the auxilliary loss could be used as an initialization scheme only, to guide the model towards a good use of the latent variables. To evaluate this we perform a control experiment where during training we first optimize our objective function in Eq. (3.13), i.e. including the auxiliary reconstruction term, and then switch to optimize the standard objective function of Eq. (3.6) without the auxiliary term. We proceed by training the full model to convergence then removing the auxiliary loss and fine-tuning from there. Figure <ref type="figure" target="#fig_8">3</ref>.7 displays ground-truth images, with corresponding auxiliary reconstructions and conditional samples, as well as pure samples. The reconstructions have become meaningless and independent from the ground truth images. The samples display the same behavior: for each auxiliary representation four samples from the autoregressive component are displayed and they are independent from one another. Quantitatively, the KL cost immediately drops to zero when removing the auxiliary loss, in approximately two thousand steps of gradient descent. The approximate posterior immediately collapses to the prior and the pixel CNN samples become independent of the latent variables. This is the behavior predicted by the analysis of <ref type="bibr" target="#b16">Chen et al. [2017]</ref>: the autoregressive decoder is sufficiently expressive that it suffers from using the latent variables.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.5">Effect of different auxiliary images.</head><p>We assess the effect of using coarser RGB quantizations, lower spatial resolutions, and grayscale in the auxiliary image. All three make the VAE reconstruction task easier, and transfer the task of modelling color nuances and/or spatial detail to the pixelCNN. The VAE reconstructions in Figure <ref type="figure" target="#fig_8">3</ref>.8 (a) obtained using coarser colour quantization carry less detail than reconstructions based on the original images using 256 colour values, as expected. To understand the relatively small impact of the quantization level on the reconstruction, recall that the VAE decoder outputs the continuous means of the logistic distributions regardless of the quantization level. Only the reconstruction loss is impacted by the quantization level via the computation of the probabilities over the discrete color levels in Eq. (3.17). In Figure <ref type="figure" target="#fig_8">3</ref>.8 (b) we observe small but consistent gains in the BPD metric as the number of color bins is reduced, showing that it is more effective to model color nuances using the pixelCNN, rather than the latent variables. We trained models with auxiliary images down-sampled to 16×16 and 8×8 pixels, which yield 2.94 and 2.93 BPD, respectively. This is comparable to the 2.92 BPD obtained using our best model at scale 32×32. We also trained models with 4-bit per pixel grayscale auxiliary images, as in <ref type="bibr" target="#b71">Kolesnikov and Lampert [2017]</ref>. While the grayscale auxiliary images are subjectively the ones that have the best global structure, the results are still qualitatively inferior to those obtained by <ref type="bibr" target="#b71">Kolesnikov and Lampert [2017]</ref> with a pixelCNN modelling grayscale images. Our model does, however, achieve better quantitative performance at 2.93 BPD. In Figure <ref type="figure" target="#fig_8">3</ref>.9 (a) we show samples obtained using models trained with 4-bit per pixel grayscale auxiliary images, in Figure <ref type="figure" target="#fig_8">3</ref>.9 (b) with 32 color levels in the auxiliary image, and in Figure <ref type="figure" target="#fig_8">3</ref>.9 (c) and (d) with auxiliary images of size 16×16 and 8×8. The samples are qualitatively comparable, showing that in all cases the pixelCNN is able to compensate the less detailed outputs of the VAE decoder and that our framework can be used with a variety of intermediate reconstruction losses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Conclusion</head><p>We presented a new approach to training generative image models that combine a latent variable structure with an autoregressive model component. Unlike prior approaches, it does not require careful architecture design to trade-off how much is modelled by latent variables and the autoregressive decoder. Instead, this trade-off can be controlled using a regularization parameter and choice of auxiliary target images. We obtain quantitative performance on par with the state of the art on CIFAR10, and samples from our model exhibit globally coherent structure as well as fine details. The construction proposed goes beyond the conditional independence assumption, alleviating some of its limitations as presented in Section 2.9.1. It does so at the cost of slow, sequential sampling and so this construction cannot be used in an adversarial setting. In Chapter 5, another approach to go beyond conditional independence which is compatible with adversarial training is presented. Generative adversarial networks (GANs) are powerful generative models based on providing feedback to a generative network via a discriminator network. However, the discriminator usually assesses individual samples. This prevents the discriminator from accessing global distributional statistics of generated samples, and often leads the generator to model only part of the target distribution. This phenomenon is known as mode-dropping.In this chapter, we propose to feed the discriminator with batches that mix both true and fake samples, and train it to predict the ratio of true samples in the batch. The latter score does not depend on the order of samples in a batch. Rather than learning this invariance, we introduce a generic permutation-invariant discriminator architecture. This architecture is provably a universal approximator of all symmetric functions. Experimentally, our approach reduces mode collapse in GANs on two synthetic datasets, and obtains good results on the CIFAR10 and CelebA datasets, both qualitatively and quantitatively. The material presented in this chapter is based on the paper "Mixed batches and symmetric discriminators for GAN training", Thomas Lucas, Corentin Tallec, Jakob Verbeek and Yann Ollivier, International Conference on Machine Learning (ICML) 2018.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Outline of this chapter</head><p>Several approaches relying on latent variables have been proposed to learn flexible density estimators together with efficient sampling such as generative adversarial networks (GANs) <ref type="bibr" target="#b42">[Goodfellow et al., 2014]</ref>, variational auto-encoders <ref type="bibr">[Kingma and</ref><ref type="bibr">Welling, 2014a, Rezende et al., 2014]</ref>, iterative transformation of simple distributions <ref type="bibr" target="#b136">[Sohl-Dickstein et al., 2015]</ref>, or non-volume preserving transformations <ref type="bibr" target="#b27">[Dinh et al., 2017]</ref>. In this chapter we focus on GANs, currently the generative model that produces the most convincing natural image samples <ref type="bibr" target="#b60">[Karras et al., 2018]</ref>. GANs consist of a generator and a discriminator network. The generator maps samples from a latent random variable with a basic prior, such as a multivariate Gaussian, to the observation space. This defines a probability distribution over the observation space. A discriminator network is trained to distinguish between generated samples and true samples in the observation space. The generator, on the other hand, is trained to fool the discriminator. In an idealized setting with unbounded capacity of both networks and infinite training data, the generator should converge to the distribution from which the training data has been sampled.</p><p>In most adversarial setups, the discriminator classifies individual data samples. Consequently, it cannot directly detect discrepancies between the distribution of generated samples and global statistics of the training distribution, such as its moments or quantiles. For instance, if the generator models a restricted part of the support of the target distribution very well, this can fool the discriminator at the level of individual samples, a phenomenon known as mode dropping. In such a case there is little incentive for the generator to model other parts of the support of the target distribution. A more thorough explanation of this effect is described in <ref type="bibr">Salimans et al. [2016a]</ref> and in Section 2.9.2. In order to access global distributional statistics, imagine a discriminator that could somehow take full probability distributions as its input. This is impossible in practice. Still, it is possible to feed large batches of training or generated samples to the discriminator, as an approximation of the corresponding distributions. The discriminator can compute statistics on those batches and detect discrepancies between the two distributions. For instance, if a large batch exhibits only one mode from a multimodal distribution, the discriminator would notice the discrepancy. Even though a single batch may not encompass all modes of the distribution, it will still convey more information about missing modes than individual samples.</p><p>Training the discriminator to discriminate "pure" batches with only real or only synthetic samples makes its task too easy, as a single bad sample reveals the whole batch as synthetic. Instead, we introduce a "mixed" batch discrimination task in which the discriminator needs to predict the ratio of real samples in a batch. This use of batches differs from traditional minibatch learning. The batch is not used as a computational trick to increase parallelism, but as an approximate distribution, on which to compute global statistics. A naive way of doing so would be to concatenate the samples in the batch, feeding the discriminator a single tensor containing all the samples. However, this is parameter-hungry, and the computed statistics are not automatically invariant to the order of samples in the batch. To compute functions that depend on the samples only through their distribution, it is necessary to restrict the class of discriminator networks to permutation-invariant functions of the batch. For this, we adapt and extend the architecture  Each convolutional layer of an otherwise classical CNN architecture is modified to include permutation invariant batch statistics, denoted ρ(x). This is repeated at every layer so that the network gradually builds up more complex statistics.</p><p>of McGregor, also found in <ref type="bibr" target="#b161">Zaheer et al. [2017]</ref> in the context of deep learning, to compute symmetric functions of the input. We show this can be done with minimal modification to existing architectures, at a negligible computational overhead w.r.t. ordinary batch processing.</p><p>Discriminating between distributions at the batch level provides an equally principled alternative to approaches to GANs based on duality formulas <ref type="bibr" target="#b3">[Arjovsky et al., 2017</ref><ref type="bibr">, Gulrajani et al., 2017b</ref><ref type="bibr" target="#b104">, Nowozin et al., 2016]</ref>. Section B.2 covers related work, followed by a presentation of the proposed training procedure in Section 4.3. Section 4.4, describes how to build networks that have the required permutation invariance properties, and experimental results are presented in Section 4.5. The last two sections of this chapter, Section 4.7 and Section 4.8, contain proofs and technical details about results used in the rest of the chapter, and the reader can decide to skip them .</p><p>In summary, the contributions presented in this chapter are the following:</p><p>• Naively training the discriminator to discriminate "pure" batches with only real or only synthetic samples makes its task way too easy. A discrimination loss based on mixed batches of true and fake samples, that avoids this pitfall is presented. We also derive the associated optimal discriminator.</p><p>• A principled way of defining neural networks that are permutation-invariant over a batch of samples is provided. We formally prove that the resulting class of functions comprises all symmetric continuous functions, and only symmetric functions.</p><p>• We apply these insights to GANs, with good experimental results, both qualitatively and quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Related work</head><p>The training of generative models via distributional rather than pointwise information has been explored in several recent contributions. Batch discrimination <ref type="bibr">[Salimans et al., 2016a]</ref> uses a specially designed layer to compute batch statistics which are then combined with sample-specific features to enhance individual sample discrimination. <ref type="bibr" target="#b60">Karras et al. [2018]</ref> directly compute the standard deviation of features and feed it as an additional feature to the last layer of the network. Both methods use a single layer of handcrafted batch statistics, instead of letting the discriminator learn arbitrary batch statistics useful for discrimination as in our approach. Moreover, in both methods the discriminator still assesses single samples, rather than entire batches. Radford et al.</p><p>[2016] reported improved results with batch normalization in the discriminator, which may also be due to reliance on batch statistics.</p><p>Other works, such as <ref type="bibr" target="#b79">Li et al. [2015]</ref> and <ref type="bibr" target="#b35">Dziugaite et al. [2015]</ref>, replace the discriminator with a fixed distributional loss between true and generated samples, the maximum mean discrepancy, as the criterion to train the generative model. This has the advantage of relieving the inherent instability of GANs, but lacks the flexibility of an adaptive discriminator. Here we show that stacking such hidden layers and reducing the final layer with a permutation invariant reduction, covers the entire space of continuous permutation invariant functions. <ref type="bibr" target="#b161">Zaheer et al. [2017]</ref> first process each element of the set independently, then aggregate the resulting representation using a permutation invariant operation, and finally process the permutation invariant quantity. <ref type="bibr" target="#b109">Qi et al. [2016]</ref> process 3D point cloud data, and interleave layers that process points independently, and layers that apply equivariant transformations. The output of their networks are either permutation equivariant for pointcloud segmentation, or permutation invariant for shape recognition. In our approach we stack permutation equivariant layers that combine batch information and sample information at every level, and aggregate these in the final layer using a permutation invariant operation. More complex approaches to permutation invariance or equivariance appear in <ref type="bibr">[Guttenberg et al.]</ref>. We prove, however, that our simpler architecture already covers the full space of permutation invariant functions.</p><p>Improving </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adversarial learning with permutation-invariant batch features</head><p>Using a batch of samples rather than individual samples as input to the discriminator can provide global statistics about the distributions of interest. Such statistics could be useful to avoid mode dropping. Adversarial learning <ref type="bibr" target="#b42">[Goodfellow et al., 2014]</ref> can easily be extended to the batch discrimination case. For a fixed batch size B, the corresponding two-player optimization procedure becomes</p><formula xml:id="formula_111">min G max D E x 1 ,...,x B ∼D [log D(x 1 , . . . , x B )] + E z 1 ,...,z B ∼p(z) [log(1 -D(G(z 1 ), . . . , G(z B )))]</formula><p>(4.1) with D the empirical distribution over data, p(z) a distribution over the latent variable that is the input of the generator, G a pointwise generator and D a batch discriminator.<ref type="foot" target="#foot_8">foot_8</ref> This leads to a learning procedure similar to the usual GAN algorithm, except that the loss encourages the discriminator to output 1 when faced with an entire batch of real data, and 0 when faced with an entire batch of generated data.</p><p>Unfortunately, this basic procedure makes the work of the discriminator too easy. As the discriminator is only faced with batches that consist of either only training samples or only generated samples, it can base its prediction on any subset of these samples. For example, a single poorly generated sample would be enough to reject a batch. To cope with this deficiency, we propose to sample batches that mix both training and generated data. The discriminator's task is to predict the proportion of real images in the batch, which is clearly a permutation invariant quantity.</p><p>Bernoulli distribution with parameter p. The mixed batch with mixing vector β is denoted</p><formula xml:id="formula_112">m β (x, x) := x β + x (1 -β) (4.2)</formula><p>where denotes pointwise multiplication along the batch axis, with β being broadcasted across the other dimensions. This apparently wastes some samples, but we can reuse the discarded samples by using 1 -β in the next batch.</p><p>The discriminator has to predict the ratio of real images, #β B where #β is the sum of the components of β. As a loss on the predicted ratio, we use the Kullback-Leibler divergence between a Bernoulli distribution with the actual ratio of real images, and a Bernoulli distribution with the predicted ratio. The divergence between Bernoulli distributions with parameters u and v is</p><formula xml:id="formula_113">KL(B (u) || B (v)) = u log u v + (1 -u) log 1 -u 1 -v . (4.3)</formula><p>Formally, the discriminator D will minimize the objective</p><formula xml:id="formula_114">E p * ∼P, β∼B(p * ) B KL B #β B || B (D(m β (x, x))) , (4.4)</formula><p>where the expectation is over sampling p * from a distribution P, typically uniform on [0, 1], then sampling a mixed minibatch. For clarity, we have omitted the expectation over the sampling of training and generated samples The generator is trained with the loss</p><formula xml:id="formula_115">E p * ∼P, β∼B(p * ) B log(D(m β (x, x))). (4.5)</formula><p>This loss, which is not the generator loss associated to the min-max optimization problem, is known to saturate less <ref type="bibr" target="#b42">[Goodfellow et al., 2014]</ref>.</p><p>In some experimental cases, using the discriminator loss of Equation <ref type="formula" target="#formula_133">4</ref>.4 with P = U([0, 1]) made discriminator training too difficult. To alleviate some of the difficulty, we sampled the mixing variable p * from a reduced symmetric union of intervals [0, γ] ∪ [1 -γ, 1]. With low γ, all generated batches are nearly purely taken from either real or fake data. We refer to this training method as batch smoothing-γ. Batch smoothing-0 corresponds to no mixing, while batch smoothing-0.5 corresponds to Equation <ref type="formula" target="#formula_133">4</ref>.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">The optimal discriminator for batch smoothing</head><p>The optimal discriminator for batch smoothing can be computed explicitly, for p * ∼ U([0, 1]), and covers the usual GAN discriminator when B = 1.</p><p>Proposition 1. The optimal discriminator for the loss in Equation <ref type="formula" target="#formula_133">4</ref>.4, given a batch y ∈ R B×N , is</p><formula xml:id="formula_116">D * (y) = 1 2 p unbalanced (y) p balanced (y) (4.6)</formula><p>where the distribution p balanced and p unbalanced on batches are defined as</p><formula xml:id="formula_117">p balanced (y) = 1 B + 1 β∈{0,1} B p 1 (y) β p 2 (y) 1-β B #β p unbalanced (y) = 2 B + 1 β∈{0,1} B p 1 (y) β p 2 (y) 1-β B #β #β B . (4.7)</formula><p>in which p 1 is the data distribution and p 2 the distribution of generated samples, and where p 1 (y) β is shorthand for p 1 (y 1 )</p><formula xml:id="formula_118">β 1 . . . p 1 (y B ) β B .</formula><p>For non-uniform beta distributions on p, a similar result holds, with different coefficients depending on #β and B in the sum. The proof is technical and is deferred to Section 4.7. These expressions can be interpreted easily. First, in the case B = 1, the optimal discriminator reduces to the optimal discriminator for a standard GAN, D * = p 1 (y) p 1 (y)+p 2 (y) . Actually p balanced (y) is simply the distribution of batches y under our procedure of sampling p uniformly, then sampling β ∼ B (p) B . The binomial coefficients put on equal footing contributions with different true/fake ratios.</p><p>The generator loss (4.5), when faced with the optimal discriminator, is the Kullback-Leibler divergence between p balanced and p unbalanced (up to sign and a constant log(2)). Since p unbalanced puts more weight on batches with higher #β (more true samples), this brings fake samples closer to true ones. Since p balanced and p unbalanced differ by a factor 2#β/B, the ratio</p><formula xml:id="formula_119">D * = 1 2 p unbalanced (y) p balanced (y)</formula><p>is simply the expectation of #β/B under a probability distribution on β that is proportional to</p><formula xml:id="formula_120">p 1 (y) β p 2 (y) 1-β B #β</formula><p>. But this is the posterior distribution on β given the batch y and the uniform prior on the ratio p. Thus, the optimal discriminator is just the posterior mean of the ratio of true samples, D * (y) = IE β|y #β B . This is standard when minimizing the expected divergence between Bernoulli distributions and the approach can therefore be extended to non-uniform priors on p as shown in Section 4.7.</p><p>Note on available statistics. A reasonable handcrafted statistic to use for batch discrimination is the standard deviation. Observe that our discriminator can learn to compute it with with <ref type="figure">E[(X -E[X]</ref>) 2 ], which requires 2 layers of computation. Owing to the convolutional nature of the architecture, non-pixelwise statistics are also available, (e.g. covariances).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Permutation invariant networks</head><p>Computing statistics of probability distributions from batches of i.i.d. samples requires to compute quantities that are invariant to permuting the order of samples within the batch. In this section we propose a permutation equivariant layer that can be used together with a permutation invariant aggregation operation to build networks that are permutation invariant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Building a permutation invariant architecture</head><p>A naive way of achieving invariance to batch permutations is to consider the batch dimension as a regular feature dimension, and to randomly reorder the batches at each step. This multiplies the input dimension by the batch size, and thus greatly increases the number of trainable parameters. Moreover, this only provides approximate invariance to batch permutation, as the network has to infer the invariance based on the training data. Instead, we propose to directly build invariance into the architecture. This method drastically reduces the number of parameters compared to the naive approach, bringing it back in line with ordinary networks, and ensures strict invariance to batch permutation.</p><p>Let us first formalize the notion of batch permutation invariance and equivariance. A function f from R B×l to R B×L is batch permutation equivariant if permuting samples in the batch results in the same permutation of the outputs: for any permutation σ of the inputs,</p><formula xml:id="formula_121">f (x σ(1) , . . . , x σ(B) ) = f (x) σ(1) , . . . , f (x) σ(B) .</formula><p>(4.8)</p><p>For instance, any regular neural network or other function treating the inputs x 1 , . . . , x B independently in parallel, is batch permutation equivariant.</p><p>A function f from R B×l to R L is batch permutation invariant if permuting the inputs in the batch does not change the output: for any permutation on batch indices σ,</p><formula xml:id="formula_122">f (x σ(1) , . . . , x σ(B) ) = f (x 1 , . . . , x B ). (4.9)</formula><p>The mean, the maximum or the standard deviation along the batch axis are all batch permutation invariant. Permutation equivariant and permutation invariant functions can be obtained by combining ordinary, parallel treatment of batch samples with an additional batch-averaging operation that performs an average of the activations across the batch direction. In our architecture, this averaging is the only form of interaction between different elements of the batch. It is one of our Formally, on a batch of data x ∈ R B×N , our proposed batch permutation invariant network f θ is defined as</p><formula xml:id="formula_123">f θ (x) = 1 B B b=1 (φ θp • φ θ p-1 • . . . • φ θ 0 (x)) b (4.10)</formula><p>where each φ θ i is a batch permutation equivariant function from R B×l i-1 to R B×l i , where the l i 's are the layer sizes.</p><p>The equivariant layer operation φ θ with l input features and L output features comprises an ordinary weight matrix Λ ∈ R l×L that treats each data point of the batch independently ("nonbatch-mixing"), a batch-mixing weight matrix Γ ∈ R l×L , and a bias vector β ∈ R L . As in regular neural networks, Λ processes each data point in the batch independently. On the other hand, the weight matrix Γ operates after computing an average across the whole batch. Defining ρ as the batch average for each feature,</p><formula xml:id="formula_124">ρ(x 1 , . . . , x B ) := 1 B B b=1 x b (4.11)</formula><p>the permutation-equivariant layer φ is formally defined as</p><formula xml:id="formula_125">φ θ (x) b := µ β + x b Λ + ρ(x)Γ (4.12)</formula><p>where µ is a non-linearity, b is a batch index, and the parameter of the layer is θ = (β, Λ, Γ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">A universal approximation theorem for permutation invariant functions</head><p>The networks constructed above are permutation invariant by construction. However, it is unclear a priori that all permutation invariant functions can be represented this way: the functions that can be approximated to arbitrary precision by those networks could be a strict subset of the set of permutation invariant functions. The optimal solution for the discriminator could lie outside this subset, making our construction too restrictive. We now show this is not the case: our architecture satisfies a universal approximation theorem for permutation-invariant functions.</p><p>Theorem 1. The set of networks that can be constructed by stacking as in Eq. (4.10) the layers φ defined in Eq. (4.12), with sigmoid nonlinearities except on the output layer, is dense in the set of permutation-invariant functions (for the topology of uniform convergence on compact sets).</p><p>The standard universal approximation theorem for neural networks proves the following: for any continuous function f , we can find a network that given a batch x = (x 1 , . . . , x B ), computes (f (x 1 ), . . . , f (x B )). This is insufficient for our purpose as it provides no way of mixing information between samples in the batch, and we need extend that theorem. The proof is restricted to sigmoid nonlinearities here, for simplicity. It can easily be extended to other types of nonlinearities that yield universal function approximators.</p><p>To describe the set of functions that can be approximated by our construction, some structure on that set is needed. For instance, suppose two functions f 1 and f 2 can be approximated, then can we approximate f 1 +f 2 ? Intuitively, if D 1 and D 2 are networks that approximate f 1 and f 2 respectively, then D 1 + D 2 , summed using a linear operator, should approximate f 1 + f 2 . We will then need similar results for multiplication by a scalar , and for products. More precisely, we will prove that the set of functions that can be approximated to arbitrary precision by our networks is an algebra, i.e., a vector space stable under products. With this structure, it is possible to go from simple symmetric functions to complex ones. To show that any symmetric function can be approximated, we then need to prove that the set of functions we can approximate contains a family of functions that can be extended, using the algebra structure, to all continuous symmetric functions. Precisely, we must show that our algebra contains a generative family of the continuous symmetric functions.</p><p>While the case of one-dimensional features is relatively simple, the multidimensional case is more intricate, and the detailed proof is given in Section 4.8. Here we describe the key ideas underlying the proof, without technical details. To prove that we can compute the sum of two functions f 1 and f 2 , compute f 1 and f 2 on different channels (this is possible even if f 1 and f 2 require different numbers of layers, by filling in with the identity if necessary). Then sum across channels, which is possible in (4.12). To compute products, first compute f 1 and f 2 on different channels, then apply the universal approximation theorem to turn this into log f 1 and log f 2 , then add, then take the exponential thanks to the universal approximation theorem again.</p><p>Multiplication by a real scalar α is clearly given by linear layers. This gives us stability by scalar multiplication, sum and product, i.e. the algebra structure.</p><p>The key point is then the following: the algebra of all permutation-invariant polynomials over the components of (x 1 , . . . , x B ) is generated as an algebra by the averages 1 B (f (x 1 ) + . . . + f (x B )) when f ranges over all functions of single batch elements. This non-trivial algebraic statement is proved in Section 4.8. By construction, such functions 1 B (f (x 1 ) + . . . + f (x B )) are readily available in our architecture, by computing f as in an ordinary network and then applying the batch-averaging operation ρ in the next layer. Further layers provide sums and products of those thanks to the algebra property. Having obtained the desired result for polynomial symmetric functions, we can extend it and conclude with a symmetric version of the Stone-Weierstrass theorem (polynomials are dense in continuous functions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Practical architecture</head><p>In our experiments, we apply the constructions above to standard, deep convolutional neural networks. In practice, for the linear operations Λ and Γ in (4.12) we use convolutional kernels (of size 3 × 3) acting over x b and ρ(x) respectively. Weight tensors Λ and Γ are also reweighted like so that at the start of training ρ(x) does not contribute disproportionately compared with other features: Λ = |B| |B|+1 Λ and Γ = 1 |B|+1 Γ where |B| denotes the size of batch B. While these coefficients could be learned, we have found this explicit initialization to improve training. Figure <ref type="figure" target="#fig_16">4</ref>.1 shows how to modify standard CNN architectures to adapt each layer to our method.</p><p>In the first setup, which we refer to as BGAN, a permutation invariant reduction is done at the end of the discriminator, yielding a single prediction per batch, which is evaluated with the loss in (4.4). We also introduce a setup, M-BGAN, where we swap the order of averaging and applying the loss.<ref type="foot" target="#foot_9">foot_9</ref> Namely, letting y be the single target for the batch (in our case, the proportion of real samples), the BGAN case translates into</p><formula xml:id="formula_126">L((o 1 , . . . , o B ), y) = 1 B B i=1 o i , y (4.13)</formula><p>while M-BGAN translates to</p><formula xml:id="formula_127">L((o 1 , . . . , o B ), y) = 1 B B i=1 (o i , y) (4.14)</formula><p>where L is the final loss function, is the KL loss function used in (4.4), (o 1 , . . . , o b ) is the output of the last equivariant layer, and y is the target for the whole batch. Both these losses are permutation invariant. A more detailled explanation of M-BGAN is given in Section 4.5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiments</head><p>We now present experimental results obtained using our construction. First, results on a synthetic 2D dataset are provided on which mode-dropping can be assessed. Then results are presented on the CIFAR10 and STL10 natural images datasets. In terms of Inception score and Fréshet inception distance, our model was on par withe state of the art at the time of submission on these datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Synthetic 2D distributions</head><p>The synthetic dataset of <ref type="bibr" target="#b162">Zhang et al. [2017]</ref> is explicitly designed to test mode dropping. The data are sampled from a mixture of concentrated Gaussians in the 2D plane. We compare standard GAN training, "mixup" training <ref type="bibr" target="#b162">[Zhang et al., 2017]</ref>, and batch smoothing using the BGAN from Section 4.4.3. In all cases, the generators and discriminators are three-layer ReLU networks with 512 units per layer. The latent variables used as input to the generator are sampled from 2-dimensional standard Gaussians. The models are trained on their respective losses using the Adam <ref type="bibr">[Kingma and Ba, 2015b]</ref> optimizer, with default parameters. The discriminator is trained for five steps for each generator step.</p><p>Qualitative results are provided in Figure <ref type="figure" target="#fig_16">4</ref>.3. Batch smoothing and mixup have similar effects.</p><p>Results for BGAN and M-BGAN are qualitatively similar on this dataset and we only display results for BGAN. The standard GAN setting quickly diverges, due to its inability to fit several modes simultaneously, while both batch smoothing and mixup successfully fit the majority of modes of the distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Experimental results on CIFAR10</head><p>Next, we consider image generation on the CIFAR10 dataset. We use the simple architecture from <ref type="bibr">Miyato et al. [2018b]</ref>, minimally modified to obtain permutation invariance thanks to Equation <ref type="formula" target="#formula_133">4</ref>.12. All other architectural choices are unchanged. The same Adam hyper-parameters from <ref type="bibr">Miyato et al. [2018b]</ref> are used for all models: α = 2e -4 , β 1 = 0.5, β 2 = 0.999, and no learning rate decay. We performed hyper-parameter search for the number of discrimination steps between each generation step, n disc , over the range {1, . . . , 5}, and for the batch smoothing parameter γ over [0.2, 0.5]. All models are trained for 400, 000 iterations, counting both generation and discrimination steps. We compare smoothed BGAN and M-BGAN, and the same network trained with spectral normalization <ref type="bibr">Miyato et al. [2018b]</ref> (SN), and gradient penalty <ref type="bibr">[Gulrajani et al., 2017b]</ref> on both the Wasserstein <ref type="bibr" target="#b3">[Arjovsky et al., 2017]</ref> (WGP) and the standard loss (GP). We also compare to a model using the batch-discrimination layer of <ref type="bibr">Gulrajani et al. [2017b]</ref>, adding a final batch discrimination layer to the architecture of <ref type="bibr">Miyato et al. [2018b]</ref>. All models are evaluated by reporting the Inception Score and the Fréchet Inception Distance <ref type="bibr" target="#b51">Heusel et al. [2017]</ref> and results are summarized in Table <ref type="table">B</ref>.4.  Effect of batch smoothing on the generator and discriminator losses. To check the effect of the batch smoothing parameter γ on the loss, we plot the discriminator and generator losses of the network for different γ's. The smaller the γ, the purer the batches. We would expect discriminator training to be more difficult with larger γ. In Fig. <ref type="figure" target="#fig_16">4</ref>.2) the generator loss -which is the part of the discriminator loss that evaluates samples -is lower for larger γ, revealing the relative advantage of the generator on the discriminator. BGAN and M-BGAN behave similarly and we only report on BGAN in the figure. This suggests to increase γ if the discriminator dominates learning, and to decrease γ if the discriminator is stuck at a high value in spite of poor generated samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Results on celebA and STL10 datasets</head><p>Finally, on the celebA face dataset, we adapt the simple architecture of <ref type="bibr">Miyato et al. [2018b]</ref> to the increased resolution by adding a layer to both networks. For optimization we use Adam with β 1 = 0, β 2 = 0.9, α = 1e -4, and n disc = 1. Fig. <ref type="figure" target="#fig_16">4</ref>.5 dislays BGAN samples with pure batches, and BGAN and M-BGAN samples with γ = .5. The visual quality of the samples is reasonable; we believe that an improvement is visible from pure batches to M-BGAN. We provide results on the STL-10 dataset, in Figure <ref type="figure" target="#fig_16">4</ref>.7 and Table <ref type="table" target="#tab_10">4</ref>.2 where M-BGAN yields numerical results slightly worse than Spectral Normalization. Except for the adaptation of the network to 48 × 48 images, as done in <ref type="bibr">Miyato et al. [2018b]</ref>, the experimental set-up is left unchanged.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Interpretation of M-BGAN as an ensembling method</head><p>The experiments show that M-BGAN can quite significantly improve performance. Intuitively, the M-BGAN loss performs a simple ensembling of many strongly dependant permutation invariant discriminators, at no additional cost. In the general case, ensembling of N independent discriminators D 1 , . . . , D N amounts to training each discriminator independently, and using the averaged gradient signal to train the generator. Ensembling is expected to alleviate some of the difficulties of GAN training: as long as one of the discriminators still provides a significant gradient signal, training of the generator is possible.</p><p>With equation (4.14), M-BGAN is an ensemble of B permutation invariant discriminators, with respective outputs 1-th(o 1 , . . . , o B ), . . . , B-th(o 1 , . . . , o B ), where i-th is the function that returns the i-th greatest element of a B dimensional vector. Indeed,</p><formula xml:id="formula_128">1 N N i=1 l(i-th(o 1 , . . . , o B ), y) = 1 N N i=1 l(o i , y). (4.15)</formula><p>which is the M-BGAN loss. The ensembled discriminators of the M-BGAN all share the same weights. This ensembling effect at least partially explains the improved performance of M-BGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Conclusion</head><p>We have shown how to build neural network architectures that are, assuming sufficient capacity, dense in the set of all functions invariant to permutations on the batch axis. This means that our networks can approximate any statistics on the distributions given as input, where "approximate" comes from i) the finite size of the batch, which induces variance for the estimator of the statistic and ii) the fact that in practice the density result may require too much computation for a given statistic.</p><p>Because statistics on the distribution can be considered and used to reject a batch, the discriminator can explicitly evaluate variability of the images in the batch. With a perfect discriminator, a batch of samples must contain as much variability as a batch of real images, thus mode dropping is explicitly penalised together with image quality. With a perfect discriminator and an infinite batch size, the distributions p θ and p * must match exactly.</p><p>We also observe that in practice, feeding all-fake or all-genuine batches to a discriminator makes its task too easy, and therefore generalize the approach to mixed batches, without breaking the optimality results. Experimentally, this provides a new, alternative method to reduce mode dropping and reach good quantitative scores in GAN training. The vanilla GAN case is trivially recovered as a special case</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Optimal discriminator for general beta prior (*)</head><p>We now take a closer look at the optimal discriminator, for a given generator. First, we give a derivation of the optimal discriminator expression when the mixing parameter p is drawn from <ref type="bibr">Beta(a, b)</ref>. This extends the result for the uniform distribution given in Eq. (4.7), as Beta(1, 1) = U([0, 1]). Let us first build an intuition about the result, starting from the vanilla GAN case. Given p θ and a dataset sampled from p * , and mixing weights between equal to π 1 and 1 -π 1 , one can build a dataset containing samples from p * in proportion π 1 and from the model in proportion 1 -π 1 , and an image y i from this (mixed) dataset then has density</p><formula xml:id="formula_129">p mix (y) = π 1 p * (y) + (1 -π 1 )p θ (y).</formula><p>Denote B the random variable modelling the class of y i , taking value b = 0 for fake and b = 1 for real. The prior on class B is then p(B = 1) = π 1 , p(B = 0) = 1 -π 1 . Using Bayes' rule, the true posterior on C is</p><formula xml:id="formula_130">p mix (b|y i ) = p mix (y i |b)p(b) p mix (y i ) = p b (y i )π b p mix (y i ) .</formula><p>This gives us a probability for each discrete class c, so values between 0 and 1, that sum to 1, i.e. "soft" predictions. If we need to to pick a definite answer (classification), we pick the mode of p, i.e. the most likely class, by looking at which probability is greater than 0.5. This yields the optimal Bayes classifier. We already knew this result: it is exactly the optimal value of the discriminator in the standard GAN case. Indeed with π 1 = 1/2,</p><formula xml:id="formula_131">p mix (B = 0|y i ) = p θ (y i ) p θ (y i ) + p * (y i )</formula><p>.</p><p>Now, we have a probabilistic interpretation of this result: the optimal discriminator approximates the Bayes classifier, which is the true posterior. In the i.i.d case with mixed batches this is easy to apply over a batch rather than a single image: if β = (b 1 , . . . , b B ) is the class predictions for images (y 1 , . . . , y B ), then p(β|y)</p><formula xml:id="formula_132">= i p mix (b i |y).</formula><p>We are interested in extending this to the non-i.i.d vector case, which is the setting of our model. Looking at the vanilla GAN case, we have the intuition that we can link the optimal discriminator to p mix (β|y) using Bayes' rule. It will require p(y|β), p(β) and p mix (y). Knowing β and p θ we can easily specify p(y|β). Knowing p θ and the dataset provides p(y), so we need the prior on β, p(β), which we now derive.</p><p>Beta prior on batch mixing proportion. Consider mixed batches of samples of size B. The i-th sample of the batch is a real sample if β i = 1 and a generated sample if β i = 0. Given a certain mixing proportion p, assuming that samples are sampled independantly according to a Bernoulli of parameter p, the probability of a certain β is The marginal on the batch y is</p><formula xml:id="formula_133">P(β | p) = i p β i (1 -p) 1-β i . (<label>4</label></formula><formula xml:id="formula_134">P(y) = β P(y | β)P(β) (4.22) = β P(y|β) B(#β + a, B -#β + b) B(a, b) . (4.23)</formula><p>The numerator in Eq. ( <ref type="formula" target="#formula_133">4</ref>.21) can be written as a distribution on y, From the previous observation, it yields that</p><formula xml:id="formula_135">Q(y) = β P(y|β)Q(β) (4.24) Q(β) = a + b a P(β) #β B . (<label>4</label></formula><formula xml:id="formula_136">P(y | β) = B i=1 p 1 (y i ) β i p 2 (y i ) 1-β i . (4.28)</formula><p>From the latter and Eq. ( <ref type="formula" target="#formula_133">4</ref>.27) we obtain the optimal discriminator expression in Equation <ref type="formula" target="#formula_133">4</ref>.7.</p><formula xml:id="formula_137">p balanced (y) = 1 B + 1 β∈{0,1} B p 1 (y) β p 2 (y) 1-β B #β p unbalanced (y) = 2 B + 1 β∈{0,1} B p 1 (y) β p 2 (y) 1-β B #β #β B .</formula><p>(4.29)</p><p>The next section provides further analysis of these quantities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.1">p balanced and p unbalanced are well normalized:</head><p>We now show that p unbalanced is well defined, in the sense that it is normalized. The computation for p balanced is almost identical and left to the reader. First, recall that</p><formula xml:id="formula_138">y p unbalanced (y)dy = 2 B + 1 β∈{0,1} B #β BC #β B y p x (y) β p x(y) 1-β dy (4.30)</formula><p>Let us take a close look at the integral y p x (y) β p x(y) 1-β dy. Recall that p x (y) β is a shorthand for p x (y 1 ) β 1 . . . p x (y B ) β B , so</p><formula xml:id="formula_139">y p x (y) β p x(y) 1-β dy = i y i p x (y i ) β p x(y i ) 1-β i dy i .</formula><p>Each β i is either 0 or 1 so each term of the product is the integral of either p x or p x which are densities. Therefore the integral resolves to 1. We are now ready to solve Equation <ref type="formula" target="#formula_133">4</ref>.30:</p><formula xml:id="formula_140">y p unbalanced (y)dy = 2 B + 1 β∈{0,1} B #β BC #β B y p x (y) β p x(y) 1-β dy = 2 B + 1 B #β=1 C #β B #β BC #β B = 2 (B + 1)B B #β=1 #β = 2 (B + 1)B B(B + 1) 2 = 1</formula><p>Where line two is obtained by remarking that there are C #β B vectors β of cardinal #β. This shows that p unbalanced is indeed a valid density over y.</p><p>Let us now comment on how p unbalanced behaves. Suppose a fixed label vector β is given, and we want to evaluate how well it describes batch x. The term in the sum can be interpreted as a score for β, computed over the batch. Every time an image in the batch x i looks fake (p x (y i ) is low) but β i is a "real" label, the score decreases, or similarly if the image looks "real" but the label is 0. These per-image scores are multiplied together, so a single mistake, i.e. a single value close to 0, will strongly impact the product of scores of the fixed β. Therefore when summing over all βs, most of the mass will be concentrated on values of β that fit all points well. The extra term in p unbalanced , #β BC #β B , is very important: it reweights the scores to penalyse mistakes in the case where #β is high more, and thus brings fake samples closer to true ones. It is the only term left inside the posterior expectation in Equation <ref type="formula" target="#formula_133">4</ref>.20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Universal approximation theorem for symmetric functions (*)</head><p>In what follows, we aim at proving a universal approximation theorem for the class of permutation invariant neural networks we have defined. To ease readings, products, sums and real function applications are assumed to be broadcasted when need be. Throughout the paper the batch dimension n is constant and ommited from set indices. We begin with precise definitions, and there are then two main steps in the proof. The first is to show that the set of functions that can be approximated to arbitrary precision on a compact set K is an Algebra for some triplet (+, •, ×), and then to show that this algebra contains a generative family of the set of permutation invariant functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.1">Definitions</head><p>We begin by defining precisely what a symmetric function is, which you can read as "invariant to permutations on the batch axis". We then define permutation-equivariant functions. Finally, we define how to build the networks in our construction; we start with equivariant networks, then reduce them to obtain invariant networks.</p><p>Definition 1. (Symmetric functions) A function f : R n×k → R l is symmetric if for any permutation of indexes σ and for all</p><formula xml:id="formula_141">x ∈ R n×k , f (x σ(1) , . . . , x σ(n) ) = f (x 1 , . . . , x n ). The set of continuous symmetric functions from R n×k to R l is denoted by I l k Definition 2. (Permutation equivariance) A function f : R n×k → R n×l is permutation equivari- ant if for any permutation of indexes σ and for al x ∈ R n , f (x σ(1) , . . . , x σ(n) ) = f (x) σ(1) , . . . , f (x) σ(n) .</formula><p>The two previous definitions can be summarized by the following mnemonics: i) symmetric functions give the same result for any ordering of their inputs; ii) permutation equivariant functions give the same result, up to re-ordering of their outputs, for any ordering of their inputs. When symmetric functions and permutation equivariant functions are restricted to a compact K, we assume that the compact itself is symmetric, i.e.:</p><formula xml:id="formula_142">(x 1 , . . . , x n ) ∈ K =⇒ ∀σ, (x σ (1), . . . , x σ (n)) ∈ K.</formula><p>Note that a classical network, which treats a batch of inputs in an i.i.d manner, is a trivial case of permutation equivariant function: it's outputs are re-ordered when a permutation is applied to the batch. Typically, a loss function is then averaged over the batch, such that the final loss becomes invariant. So classical networks, with classical losses are already invariant. However, they are a trivial case that we wish to extend. In what follows, we use ρ as a reducing operator on vectors defined for x ∈ R n×k by</p><formula xml:id="formula_143">ρ(x) j = 1 n n i=1</formula><p>x i,j . (4.31)</p><p>Definition 3. (Recursive definition of equivariant networks) Let the sets E l k be sets that contain permutation equivariant neural networks from R n×k to R n×l , recursively defined thus:</p><p>• For all k ∈ N, the identity function on R n×k belongs to E k k .</p><p>• For all f ∈ E k r , Γ ∈ R l×k , Λ ∈ R l×k and β ∈ R l , and for act, a sigmoid activation function, g defined as</p><formula xml:id="formula_144">g(x) i,j = k p=1 Γ j,p act(f (x)) i,p + k p=1 Λ j,p ρ(act(f (x))) p + β j ) (4.32) is in E l r .</formula><p>The number of layers of the network is defined as the induction depth of the previous construction.</p><p>The set of thus constructed permutation equivariant neural networks with number of layers L is denoted by E(L) l k . Note that this class of function is trivially stable by composition, i.e. if</p><formula xml:id="formula_145">g 1 ∈ E l 2 l 1 and g 2 ∈ E l 3 l 2 , the g 2 • g 1 ∈ E l 3 l 1 .</formula><p>Definition 4. (Symmetric neural networks) Let I l k be a set containing symmetric neural networks from R n×k to R l defined as</p><formula xml:id="formula_146">I l k = ρ(E l k ). (4.33)</formula><p>Definition 3 can be thought of as a formal description of how our networks are built. If two subnetworks that are permutation equivariant are given, they can be combined using our permutation equivariant layer. The second definition simply says that taking a permutation equivariant network and reducing it using ρ yields a symmetric network. This is intuitive: an equivariant network produces the same result up to the order of outputs, and an invariant reduction doesn't care about the order of it's inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.2">Algebraic structure of approximable functions</head><p>We have constructed sets I l k , containing permutation invariant networks. We now show that the way they are constructed is not too restictive, i.e. that any analytical symmetric function can be approximated with arbitrary precision by a sufficiently expressive network of our construct.</p><p>Theorem 2. For all n, k, l and for all compact</p><formula xml:id="formula_147">K, I l k K is dense in I l k K .</formula><p>We begin by showing that the set of functions that can be approximated to arbitrary precision (i.e. the closure of I l k K</p><p>) can be equiped with an algebraic structure using + and × operators constructed pointwise with the + and × in R, i.e.</p><formula xml:id="formula_148">f + g = x → f (x) + R g(x) and f × g = x → f (x) × R g(x)</formula><p>. Intuitively, we have shown how to combine networks E l k while retaining permutation equivariance, and want to "lift" this to the set of functions that can be approximated as limits of our networks. The first step of the proof is to show that the closure of I l k K is a ring, i.e. that it is stable by sum, product and that each element has an inverse for +, as well as a vectorial space, making it an algebra.</p><p>The main 'technique' to prove this is to pick the right δs and s to obtain uniform convergence over K. The second step is to prove that this closure contains a generative familly of the set of all polynomials that operate symmetrically on the batch dimension. Finally, because symmetric polynomials are dense in the set of all symmetric functions this will conclude the proof. We wish to adapt the Stone-Weierstrass theorem, which states that on a compact, any continuous function of a single variable can be approximated with polynomials. We need an extension to symmetric functions, as the Stone-Weierstrass theorem works for univariate functions, or in the i.i.d case only.</p><p>Stability by composition "•". We begin by showing that if two equivariant functions can be approximated, then so can their composition (informally, one can think "lim(f • g) = (lim f ) • (lim g)"). Formally, we prove:</p><formula xml:id="formula_149">Lemma 1. If f 1 ∈ E l 2 l 1 K and f 2 ∈ E l 3 l 2 f 1 (K) then f 2 • f 1 ∈ E l 3 l 1 K . Proof. Let ε &gt; 0, f 2 is</formula><p>continuous on a compact set, thus uniformly continuous, and there exists an</p><formula xml:id="formula_150">η &gt; 0 such that x -x &lt; η implies f 2 (x) -f 2 (x ) &lt; ε 2 . Now let g 1 ∈ E l 2 l 1 K be such that g 1 -f 1 ∞ ≤ η and g 2 ∈ E l 3 l 2 K such that g 2 -f 2 ∞ ≤ ε 2 , then, for x in K f 2 • f 1 (x) -g 2 • g 1 (x) ≤ f 2 • f 1 (x) -g 2 • f 1 (x) + g 2 • f 1 (x) -g 2 • g 1 (x) ≤ ε</formula><p>Intuitively, this Lemma says: if your approximations of f 1 and of f 2 are good enough, then composing them yields a good enough approximation of</p><formula xml:id="formula_151">f 1 • f 2 .</formula><p>Stability by concatenation "concat". We now show that limits can be concatenated, i.e. "concat(lim f, lim g) = lim concat(f, g)". Formally:</p><p>Lemma 2. For any continuous functions g : R k → R l , the restriction of the function G : R n×k → R n×k , defined as G(x) = (g(x 1 ), . . . , g(x n )), to a compact K is in E l k K . More precisely, for all L ≥ 2, the restriction of</p><formula xml:id="formula_152">G to K is in E(L) l k K .</formula><p>Proof. This is a consequence of the neural network universal approximation theorem, as stated e.g. in <ref type="bibr" target="#b21">Cybenko [1989]</ref>.</p><p>The previous Lemma is simply the application of the universal approximation theorem to each component function independantly. Intuitively, this trivial statement will let us use "many approximations in parallel".</p><formula xml:id="formula_153">Lemma 3. If f 1 ∈ E l 1 k K , f 2 ∈ E l 2 k K</formula><p>and f 1 and f 2 have the same number of layers (i.e. they have the same induction depth), then concat</p><formula xml:id="formula_154">1 (f 1 , f 2 ) ∈ E l 1 ,l 2 k K , with concat 1 (x, y) i,j = x i,j if j ≤ l 1 y i,j-l 1 otherwise (4.34)</formula><p>Proof. By induction on the number of layers L,</p><p>• if L = 0, the result is clear.</p><p>• if L &gt; 0, let g 1 , Γ 1 , Λ 1 and β 1 as well as g 2 , Γ 2 , Λ 2 and β 2 be the parameters associated to f 1 and f 2 , then, by induction, concat 1 (g 1 , g 2 ) is a permutation equivariant network, and concat 1 (f 1 , f 2 ) is obtained by setting Γ to be the block diagonal matrix obtained with Γ 1 and Γ 2 , Λ, the block diagonal matrix obtained with Λ 1 and Λ 2 , and β the concatenation of both β's.</p><formula xml:id="formula_155">Lemma 4. If f 1 ∈ E l 1 k K , f 2 ∈ E l 2 k K , then concat 1 (f 1 , f 2 ) ∈ E l 1 +l 2 k K . Proof. Let ε &gt; 0, let g 1 ∈ E l 1 k K and g 2 ∈ E l 2 k K be such that g 1 -f 1 ∞ ≤ ε 4 and g 2 -f 2 ∞ ≤ ε 4 .</formula><p>Denote by L 1 and L 2 the numbers of layers of g 1 and g 2 . We assume L 1 ≥ L 2 without loss of generality. By lemma 2, there exist</p><formula xml:id="formula_156">h 1 ∈ E l 1 l 1 K and h 2 ∈ E l 2 l 2 K with h 1 of depth 2 and h 2 of depth L 1 -L 2 + 2 such that h 1 -Id ∞ ≤ ε 4 on g 1 (K) and h 2 -Id ∞ ≤ ε 4 on g 2 (K). The networks h 1 • g 1 and h 2 • g 2 have the same number of layers, consequently, concat 1 (h 1 • g 1 , h 2 • g 2 ) ∈ E l 1 ,l 2 k K . Besides, concat 1 (f 1 , f 2 ) -concat 1 (h 1 • g 1 , h 2 • g 2 ) ∞ (4.35) ≤ f 1 -g 1 ∞ + h 1 • g 1 -g 1 ∞ + f 2 -g 2 ∞ + h 2 • g 2 -g 2 ∞ (4.36) ≤ε (4.37)</formula><p>yielding the result.</p><p>Lemma 3 is simply a formal statement that you can concatenate equivariant networks, and that you get an equivariant network. It is trivial, but very usefull: if we can concatenate functions f 1 and f 2 we can then, for instance, sum them using an additional layer. Lemma 4 is more interesting: concatenating approximations that are "good enough" yields an approximation of the concatenation.</p><p>Stability by sum "+". We now show that we can sum limits, i.e. "lim f + lim g = lim(f + g)".</p><p>Formally:</p><formula xml:id="formula_157">Lemma 5. If f 1 and f 2 are in E l k K , then f 1 + f 2 is too. Proof. By lemma 3, concat 1 (f 1 , f 2 ) is in E 2l k K .</formula><p>Consider the layer g, with kernels</p><formula xml:id="formula_158">Γ i,j = 1 if j = i or j = k + i 0 otherwise for 1 ≤ i ≤ l, 1 ≤ j ≤ 2l, Λ = 0, β = 0.</formula><p>By lemma 1, as both concat 1 (f 1 , f 2 ) and g are in closures of permutation equivariant networks, their composition is too. This composition is act(f 1 + f 2 ). By the universal approximation theorem act -1 is also in the closure so f 1 + f 2 is in the closure.</p><p>Thus closures of permuation equivariant networks is stable by +, where + is defined pointwise using the + in R. More generally, following similar reasonings and noting that multiplication by a real scalar α ∈ R is possible with linear layers, closures of permutation equivariant networks are vectorial spaces on R. It follows that closures of permutation invariant networks are vectorial spaces too. Thus we have verified the vectorial space structure on R.</p><p>Broadcasted functions:. For the following proofs, we will need the ability to broadcast functions on the batch axis. Intuitively, if f can be approximated, then f copied b times also can.</p><formula xml:id="formula_159">Lemma 6. If f ∈ I l k K , then F defined by F (x) i,j = f (x) j (4.38) for all i, j, is in E l k K</formula><p>Proof. By definition, for any ε &gt; 0, there exists a G in E l k K such that f and ρ(G) are at distance at most ε 2 . Let α be a non zero real number such that act -1 (αG(x)) is well defined for any x ∈ K. Consider the equivariant layer</p><formula xml:id="formula_160">m(x) i,j = α -1 ρ(act(x)) j .</formula><p>(4.39)</p><p>Let η 1 be a positive real number, and L η 1 be a compact set that contains both act -1 (αG(K)) and any ball of radius η 1 contained in this set. m is uniformly continuous on L η 1 , and consequently there exists an η 2 such that if x and y are at distance at most η 2 , m(x) and m(y) are at distance at most ε 2 . Now, by composition and the universal approximation theorem, let h ∈ E l k be such that h and act -1 (αG) are at distance at most min(η 1 , η 2 ). Then m • act -1 (αG) and m • h are at distance at most ε 2 , and by triangular inequality, F and m • h are at distance at most ε.</p><p>Stability by product "×". We now show that we can multiply limits, i.e. "lim f × lim g = lim(f × g)". The ingredients are: (i) the broadcast operation we just defined, for technical reasons.</p><p>(ii) the ability to compose with a log, (iii) the ability to sum and to compose with an exp. This gives us the product, using exp(log(a) + log(b)) = ab.</p><formula xml:id="formula_161">Lemma 7. If f 1 and f 2 are in I l k K , then f 1 f 2 is too.</formula><p>Proof. Let F 1 and F 2 be the extensions of f 1 , f 2 as defined in lemma 6. There exists a C ∈ R such that for all i, j, x ∈ K, F 1 (x) i,j + C &gt; 0, and similarily for F 2 . Consequently, by lemma 1, lemma 2 and lemma 5, exp(log</p><formula xml:id="formula_162">(F 1 + C) + log(F 2 + C)) = F 1 F 2 + F 1 C + F 2 C + C 2 ∈ E l k K . As this closure is a vectorial space, F 1 F 2 ∈ E l k K . Consequently, f 1 f 2 = ρ(F 1 F 2 ) ∈ I l k K .</formula><p>We proved that I l k K is stable by +, and by multiplication by a real scalar α ∈ R, thus I l k K is a vector space over R (viewed as a field). We also proved that I l k K is stable by ×. Because + and × are defined using + R and × R , it is clear that × is bilinear. Thus, I l k K is an algebra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.8.3</head><p>The closure contains a generative family of all symmetric functions (**)</p><p>We have now shown that I l k K is an algebra. We are left to prove that it contains a generative familly of the continuous symmetric functions. Let us first exhibit a familly of continuous symmetric functions that is contained in the set of interest. We will then show that this family generates all continuous symmetric functions.</p><p>The key idea of the proof is that a symmetric function on variables x 1 , . . . , x n can be seen as an other function of one variable, evaluated at n different points. Technically speaking, the difficulty of the proof is to see how to jump from functions of a space X to functions of a space X n . One can think about the case of univariate functions (X = R for instance), that we wish to turn into multivariate functions (if X = R, X n = R n ), though our proof will be more generic: we care about batches of vectors, so in practice we use X = R d (the vectors) and X n = R d n R d (the batches of vectors). The idea is to use an algebra of functions on X, that we can denote F X (the algebra structure lets us scale, sum and multiply functions) and use it to construct an algebra of functions on X n , that we will denote F X n . We consider F X provided (we can use the universal approximation theorem to obtain it with neural networks), and we seek to construct F X n . We begin by establishing "bridges" between the two sets.</p><p>Lemma 8. For all f , restriction of a function from R l to R k to a compact set K, the symmetric function F , defined on K n×l by</p><formula xml:id="formula_163">F (x) = n i=1 f (x i ) (4.40) is in I l k .</formula><p>Proof. By the universal approximation theorem, f is in I l k K . By lemma 6, there exists a G in E l k K that replicates f along the batch axis of an equivariant network. Consequently,</p><formula xml:id="formula_164">ρ(G) = F is in I l k K .</formula><p>This first "bridge" is intuitive: if you have a function f on X, you can turn it into a function on X n by evaluating it at several points simultaneously. If you sum the results, you obtain an invariant function. We are going to prove that this familly of functions generates the set of all symmetric polynomials. Then, deriving a generalization of Stone Weierstrass theorem to symmetric functions, we obtain the final result.</p><p>We now present a tool to turn a function on X n into a symmetric function on X n : Symmetrization operator. To keep things general, in what follows, X denotes an arbitrary set, and S is the symmetrization operator on functions of X n , i.e. for all</p><formula xml:id="formula_165">(x 1 , . . . , x n ) ∈ X n , (Sf )(x 1 , . . . , x n ) = σ f (x σ(1) , . . . , x σ(n) ) (4.41)</formula><p>where the sum is over all permutations of <ref type="bibr">[1, n]</ref>. This is well defined: for any function f defined on X n , f • σ is also a function of X n . Since all possible results for all possible permutations of the inputs are summed, permuting the order of the inputs of Sf will simply reorder the sum.</p><p>Because the + operation in f (X n ) is commutative, the result is invariant to permutations. An important property of S is that it is linear. Let α ∈ R and g a functions of X n ,</p><formula xml:id="formula_166">(S(αf + g))(x 1 , . . . , x n ) = σ αf (x σ(1) , . . . , x σ(n) ) + g(x σ(1) , . . . , x σ(n) ) (4.42) = α σ (f (x σ(1) , . . . , x σ(n) )) + σ g(x σ(1) , . . . , x σ(n) ) (4.43) = (α(Sf ) + (Sg))(x 1 , . . . , x n ). (4.44)</formula><p>In what follows, we will build multivariate polynomials (in preparation of the use of a density result). We begin by recalling the definition of a monomial.</p><p>Definition 5. Monomials (informal) A monomial is a product of powers of variables (note the plural) with non-negative integer exponents. In the classical context of univariate polynomials, it is of the form x n . In the context of multivariate polynomials, it is of the form x a y b . . . z c . The degrees and the number of terms is arbitrary. The point is, there is no +.</p><p>Example: For instance, xy + yz is a multivariate polynomial, because there is a +, but x 128 y 3 z 29 is a monomial. The polynomial xy + yz is composed of monomials xy and yz.</p><p>Polynomials from monomials. Let F an algebra of functions on X (we can sum, multiply, and scale the functions of F ), and let P be the algebra of functions of X n generated by the functions f (x k ) : x → f (x k ) for f in F , with a slight abuse of notations.</p><p>We are allowed to define P like this: we give P an algebra structure, so it suffices to provide a family of functions, and the whole set P is generated using composition laws. The definition is valid: f (x k ) takes a value in X n , and maps it to a constant image. The functions we give are monomials with one variable. Because of the algebra structure, we have a × and so we can raise the degree, still obtaining monomials of the form f 1 (x 1 ) . . . f n (x n ). Then, because of the vector space structure, we can generate polynomials, of the form</p><formula xml:id="formula_167">i α i f i 1 (x i 1 ) . . . f in (x in ).</formula><p>Then, it remains to determine if the P that we generate this way is interesting. In fact, we will first symmetrize P (we care about symmetric functions), and then wonder if SP is interesting.</p><p>P is linearly generated by the monomials f 1 (x 1 ) . . . f n (x n ) for f k arbitrary functions of F , by definition.</p><p>We are interested in the symmetrization of P , SP . By linearity of S, SP is generated by the symmetrized monomials,</p><formula xml:id="formula_168">Sf 1 (x 1 ) . . . f n (x n ) = σ n k=1 f k (x σ(k) ). (4.45)</formula><p>Here, this result is based on an interesting transfer of structure between P and SP : because S is linear, the base of P yields a base of SP through S.</p><p>Now comes the crucial part of the proof: intuitively, because the functions f on X n are symmetric, you don't need to know how the polynomials evaluate on all x i , x 1 is enough. Note that this is true of polynomials, not all functions, but the closure remains the same.</p><p>Lemma 9. SP is generated as an algebra by Sf (x 1 ) for f ∈ F . Notably, Sf (x 1 ) takes the special form</p><formula xml:id="formula_169">Sf (x 1 ) = σ f (x σ(1) ) = (n -1)! n k=1 f (x k ). (4.46)</formula><p>Typically, for our case, X = R l for l the number of input features, F is an algebra of functions containing the multivariate polynomials on R l , and SP thus contains the set of all polynomials which are symmetric along the batch dimension.</p><p>Proof. Call rank of a monomial f 1 (x 1 ) . . . f n (x n ), the number of functions f k such that f k = 1.</p><p>Let k 1 , . . . , k r be these indices. Up to renaming f k 1 to f 1 , etc., the monomial can be written as</p><formula xml:id="formula_170">f 1 (x k 1 ) . . . f r (x kr ).</formula><p>We will work by induction on r. For r = 1 the claim is trivial. Since S does not care about permuting the variables, we have</p><formula xml:id="formula_171">Sf 1 (x k 1 ) . . . f r (x kr ) = Sf 1 (x 1 ) . . . f r (x r ) = σ∈S K r i=1 f i (x σ(i) )<label>(4.47)</label></formula><p>Intuitively, the magic has already hapenned: x 1 will take all positions in the "input batch". The values σ(r + 1), . . . , σ(n) have no influence so that</p><formula xml:id="formula_172">Sf 1 (x 1 ) . . . f n (x n ) = (n -r)! σ∈Inj n r r i=1 f i (x σ(i) ) (4.48)</formula><p>where Inj n r is the set of injective functions from r to n. This extra term comes from the fact that we have n variables but only r usefull ones, and the previous line gets 'rid' of that by focusing on usefull variables.</p><p>Assume we can generate all symmetric monomials up to rank r. We need to add a variable (replace one that was "not used" by one that is "used".) By definition we can generate Sf r+1 (x 1 ) for any f r+1 ∈ F (using our construction of P , then SP ). Then using the algebra structure, and the recurrense hypothesis, we can generate the product:</p><formula xml:id="formula_173">1 (n -r -1)! (Sf r+1 (x 1 ))   σ∈Inj n r r i=1 f i (x σ(i) )   = ( k∈n f r+1 (x k ))   σ∈Inj n r r i=1 f i (x σ(i) )   = σ∈Inj n r k∈n f r+1 (x k ) r i=1 f i (x σ(i) )</formula><p>Now, for each σ, we can decompose according to whether k ∈ Im σ or k ∈ n \ Im σ, where Im σ = {σ(1), . . . , σ(r)} is the image of σ. Intuitively, the variable that we added is permuted 4.8. UNIVERSAL APPROXIMATION THEOREM FOR SYMMETRIC FUNCTIONS (*) 93 around, and can either i) arrive on a variable slot that was not used before, or on a slot that was used. We obtain two terms:</p><p>. . . =</p><formula xml:id="formula_174">σ∈Inj n r k∈Im σ f r+1 (x k ) r i=1 f i (x σ(i) ) + σ∈Inj n r k∈n\Im σ f r+1 (x k ) r i=1 f i (x σ(i) )</formula><p>But if k is not in Im σ, then (σ(1), . . . , σ(r), k) is an injective function from r + 1 to n. So summing over σ then on k ∈ n \ Im σ is exactly equivalent to summing over σ ∈ Inj n r+1 . So the second term above is</p><formula xml:id="formula_175">σ∈Inj n r+1 r i=1 f i (x σ(i) ) f r+1 (σ(r + 1)) = σ∈Inj n r+1 r+1 i=1 f i (x σ(i) ) = Sf 1 (x k 1 ) . . . f r+1 (x k r+1 )</formula><p>which is the one we are interested in (it gives us the recurrence hypothesis we want for r + 1). So if we prove that we can generate the first term, we are done by substracting the first term on both sides using the algebra strucutre. Let us consider the first term, with k ∈ Im σ. Now, since k ∈ Im σ, we can decompose over the cases k = σ(1), . . . , k = σ(r). The notations are a bit heavy, but the idea of the trick is very simple: where there is a collision between the new variable and the one already used (say,</p><formula xml:id="formula_176">x r+1 = x i ) use a new function fi = f r+1 × f i . You can do this because F is an algebra. Let us now do it: σ∈Inj n r k∈Im σ f r+1 (x k ) r i=1 f i (x σ(i) ) = σ∈Inj n r r j=1 f r+1 (x σ(j) ) r i=1 f i (x σ(i) ) (4.49) = r j=1 σ∈Inj n r r i=1 fij (x σ(i) ) (4.50)</formula><p>where</p><formula xml:id="formula_177">fij := f i i = j f i f r+1 i = j (4.51) Now since F is a ring, f i f r+1 ∈ F . For each j the term σ∈Inj n r r i=1 fij (x σ(i) ) (4.52)</formula><p>is equal to S f1j . . . frj up to a factor (n -(r + 1))! that can be inverted (vector space structure). By our induction hypothesis, each term can be generated, and this ends the proof.</p><p>The hard part is done. Now, we just need density results.</p><p>Lemma 10. For any compact K, any l ∈ N, the intersection of I 1 l with the set of multivariate polynomials is dense in I 1 l for the infinity norm.</p><p>Proof. Let ε &gt; 0, and f be in I 1 l . There exists a multivariate polynomials P such that P -f ∞ ≤ ε. Let us consider the symmetrized polynomial</p><formula xml:id="formula_178">P (x 1 , . . . , x n ) = 1 n! σ P (x σ(1) , . . . , x σ(n) ). (4.53)</formula><p>Then P is in the intersection, and, for x ∈ K,</p><formula xml:id="formula_179">P (x) -f (x) = 1 n! σ (P (x σ(1) , . . . , x σ(n) ) -f (x σ(1) , . . . , x σ(n) )) (4.54) ≤ 1 n! σ P (x σ(1) , . . . , x σ(n) ) -f (x σ(1) , . . . , x σ(n) ) (4.55) ≤ ε. (4.56)</formula><p>Intuitively, the infinity norm does not change is we change the variables around for both P and f at the same time (think about a change of basis).</p><p>We now have all the ingredients to end the proof. We just have to plug in the universal approximation theorem to get our polynomials. For a given compact K of R l , for any multivariate polynomial</p><formula xml:id="formula_180">P of R l , any ε &gt; 0, there exists an element f of I 1 k at distance at most ε of x → n i=1 P (x i ).</formula><p>This means that the closure of the considered set contains all such functions. As this closure is an algebra (it is both a ring and a vectorial space), by lemma 8, it contains the intersection of I 2 l with the set of multivariate polynomials. By lemma 10, it contains I 1 l , which ends the proof.</p><p>Chapter 5</p><p>Adaptive density estimation for generative modelling Unsupervised learning of generative models has seen tremendous progress over recent years, in particular due to generative adversarial networks (GANs), variational auto-encoders, and flow-based models. GANs have dramatically improved sample quality, but suffer from two drawbacks: (i) they mode-drop, i.e., do not cover the full support of the train data, and (ii) they do not allow for likelihood evaluations on held-out data. In contrast likelihood-based training encourages models to cover the full support of the train data, but yields poorer samples. These mutual shortcomings can in principle be addressed by training generative latent variable models in a hybrid adversarial-likelihood manner. However, we show that commonly made parametric assumptions create a conflict between them, making successful hybrid models non trivial. As a solution, we propose to use deep invertible transformations in the latent variable decoder. This approach allows for likelihood computations in image space, is more efficient than fully invertible models, and can take full advantage of adversarial training. We show that our model significantly improves over existing hybrid models: offering GAN-like samples, IS and FID scores that are competitive with fully adversarial models and improved likelihood scores. The material presented in this chapter is based on the paper "Adaptive Density Estimation for Generative Models", Thomas Lucas, Konstantin Shmelkov, Karteek Alahari, Cordelia Shmid, and Jakob Verbeek, conference on Neural Information Processing Systems (NeurIPS) 2019.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Outline of this chapter</head><p>Successful recent generative models of natural images can be divided into two broad families, which are trained in fundamentally different ways. The first is trained using likelihood-based criteria, which ensure that all training data points are well covered by the model. This category includes variational auto-encoders (VAEs) <ref type="bibr">[Kingma and Welling, 2014b</ref><ref type="bibr">, Kingma et al., 2016b</ref><ref type="bibr" target="#b118">, Rezende and Mohamed, 2015</ref><ref type="bibr">, Rezende et al., 2014]</ref>, autoregressive models such as PixelCNNs <ref type="bibr">[Salimans et al., 2017b</ref><ref type="bibr">, van den Oord et al., 2016b]</ref>, and flow-based models such as real-NVP <ref type="bibr" target="#b27">[Dinh et al., 2017</ref><ref type="bibr" target="#b52">, Ho et al., 2019</ref><ref type="bibr" target="#b67">, Kingma and Dhariwal, 2018]</ref>. The second category is trained based on a signal that measures to what extent (statistics of) samples from the model can be distinguished from (statistics of) the training data, i.e., based on the quality of samples drawn from the model. This is the case for generative adversarial networks (GANs) <ref type="bibr" target="#b3">[Arjovsky et al., 2017</ref><ref type="bibr" target="#b42">, Goodfellow et al., 2014</ref><ref type="bibr" target="#b60">, Karras et al., 2018]</ref>, and moment matching methods <ref type="bibr" target="#b79">[Li et al., 2015]</ref>.</p><p>Despite tremendous recent progress, existing methods exhibit a number of drawbacks. Adversarially trained models such as GANs do not provide a density function, which poses a fundamental problem as it prevents assessment of how well the model fits held out and training data. Moreover, adversarial models typically do not allow to infer the latent variables that underlie observed images. Finally, adversarial models suffer from mode collapse <ref type="bibr" target="#b3">[Arjovsky et al., 2017]</ref>, i.e., they do not cover the full support of the training data. Likelihood-based model on the other hand are trained to put probability mass on all elements of the training set, but over-generalise and produce samples of substantially inferior quality as compared to adversarial models. The models with the best likelihood scores on held-out data are autoregressive models <ref type="bibr" target="#b99">[Menick and Kalchbrenner, 2019]</ref>, which suffer from the additional problem that they are extremely inefficient to sample from <ref type="bibr" target="#b111">[Ramachandran et al., 2017]</ref>, since images are generated pixel-by-pixel. The sampling inefficiency makes adversarial training of such models prohibitively expensive.</p><p>In order to overcome these shortcomings, we seek to design a model that (i) generates high-quality samples typical of adversarial models, (ii) provides a likelihood measure on the entire image space, and (iii) has a latent variable structure to allow for efficient sampling, that permits adversarial training. Additionally we show that, (iv) a successful hybrid adversarial-likelihood paradigm requires going beyond simplifying conditional independence assumptions commonly used with likelihood based latent variable models. These simplifying assumptions on the conditional distribution on data x given latent variables z, p(x|z), include full independence across the dimensions of x and/or simple parametric forms such as Gaussian <ref type="bibr">[Kingma and Welling, 2014b]</ref>, as detailed in Section 2.9.1, or use fully invertible networks <ref type="bibr">[Dinh et al., 2017, Kingma and</ref><ref type="bibr" target="#b67">Dhariwal, 2018]</ref>. These assumptions create a conflict between achieving high sample quality and high likelihood scores on held-out data. Autoregressive models, such as PixelCNNs <ref type="bibr">[Salimans et al., 2017b</ref><ref type="bibr">, van den Oord et al., 2016b]</ref>, do not make factorization assumptions, but are extremely inefficient to sample from. As a solution, we propose learning a non-linear invertible function f ψ between the image space and an abstract feature space as illustrated in Figures 5.1  make Gaussianity or independence assumptions in the conditional p(x|z). Trained by MLE, f ψ adapts to modelling assumptions made by p θ so we refer to this approach as "Adaptive density estimation".</p><p>We experimentally validate our approach on the CIFAR-10 dataset with an ablation study. Our model significantly improves over existing hybrid models, producing GAN-like samples as shown in Figure <ref type="figure" target="#fig_35">5</ref>.3, and IS and FID scores that are competitive with fully adversarial models. At the same time, we obtain likelihoods on held-out data comparable to state-of-the-art likelihoodbased methods which requires covering the full support of the dataset. We further confirm these observations with quantitative and qualitative experimental results on the STL-10, ImageNet and LSUN datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Related work</head><p>Mode-collapse in GANs has received considerable attention, and stabilizing the training process as well as improved and bigger architectures have been shown to alleviate this issue [Arjovsky  <ref type="bibr">et al., 2017</ref><ref type="bibr">, Gulrajani et al., 2017b</ref><ref type="bibr">, Miyato et al., 2018a]</ref>. Another line of work focuses on allowing the discriminator to access batch statistics of generated images, as pioneered by <ref type="bibr" target="#b60">Karras et al. [2018]</ref>, <ref type="bibr">Salimans et al. [2016b]</ref>, and further generalized by <ref type="bibr" target="#b82">Lin et al. [2018]</ref>, <ref type="bibr">Lucas et al. [2018a]</ref>. This enables comparison of distributional statistics by the discriminator rather than only individual samples. Other approaches to encourage diversity among GAN samples include the use of maximum mean discrepancy <ref type="bibr" target="#b2">[Arbel et al., 2018]</ref>, optimal transport <ref type="bibr" target="#b130">[Salimans et al., 2018]</ref>, and determinental point processes <ref type="bibr" target="#b36">[Elfeki et al., 2018]</ref>. In contrast to our work, these models lack an inference network, and do not define a explicit density over the full data support.</p><p>An other line of research has explored inference mechanisms for GANs. The discriminator of BiGAN <ref type="bibr" target="#b30">[Donahue et al., 2017]</ref> and ALI <ref type="bibr">[Dumoulin et al., 2017a]</ref>, given pairs (x, z) of images and latent variables, predict if z was encoded from a real image, or if x was decoded from a sampled z. In <ref type="bibr" target="#b144">Ulyanov et al. [2018]</ref> the encoder and the discriminator are collapsed into one network that encodes both real images and generated samples, and tries to spread their posteriors apart. In <ref type="bibr" target="#b15">Chen et al. [2018]</ref> a symmetrized KL divergence is approximated in an adversarial set-up, and uses reconstruction losses to improve the correspondence between reconstructed and target variables for x and z. Similarly, <ref type="bibr" target="#b121">Rosca et al. [2017]</ref> use a discriminator to replace the KL divergence term in the variational lower bound used to train VAEs with the density ratio trick.</p><p>In <ref type="bibr" target="#b94">Makhzani et al. [2016]</ref> the KL divergence term in a VAE is replaced with a discriminator that compares latent variables from the prior and the posterior in a more flexible manner. The VAE-GAN model <ref type="bibr" target="#b75">[Larsen et al., 2016]</ref> uses the intermediate feature maps of a GAN discriminator as target space for a VAE. This regularization is more flexible than the standard KL divergence. Unlike ours, these methods do not define a likelihood over the image space.</p><p>Likelihood-based models typically make modelling assumptions that conflict with adversarial training, these include strong factorization and/or Gaussianity. In our work we avoid these limitations by learning the shape of the conditional density on observed data given latent variables, p θ (x|z), beyond fully factorized Gaussian models. As in our work, Flow-GAN <ref type="bibr" target="#b45">[Grover et al., 2018]</ref> also builds on invertible transformations to construct a model that can be trained in a hybrid adversarial-MLE manner, see Figure <ref type="figure" target="#fig_35">5</ref>.2.However, Flow-GAN does not use efficient noninvertible layers we introduce, and instead relies entirely on invertible layers. Other approaches combine autoregressive decoders with latent variable models to go beyond typical parametric assumptions in pixel space <ref type="bibr" target="#b16">[Chen et al., 2017</ref><ref type="bibr">, Gulrajani et al., 2017c</ref><ref type="bibr">, Lucas and Verbeek, 2018a]</ref>. They, however, are not amenable to adversarial training due to the prohibitively slow sequential </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Preliminaries on MLE and adversarial training</head><p>Maximum-likelihood and over-generalization. The de-facto standard approach to train generative models is maximum-likelihood estimation. It maximizes the probability of data sampled from an unknown data distribution p * under the model p θ w.r.t. the model parameters θ. This is equivalent to minimizing the Kullback-Leibler (KL) divergence, D KL (p * || p θ ), between p * and p θ . This yields models that tend to cover all the modes of the data, but put mass in spurious regions of the target space; a phenomenon known as "over-generalization" or "zero-avoiding" <ref type="bibr" target="#b12">[Bishop, 2006]</ref>, and manifested by unrealistic samples in the context of generative image models, see Figure <ref type="figure" target="#fig_35">5</ref>.4. Over-generalization is inherent to the optimization of the KL divergence oriented in this manner.</p><p>Real images are sampled from p * , and p θ is explicitly optimized to cover all of them. The training procedure, however, does not sample from p θ to evaluate the quality of such samples (ideally using the inaccessible p * (x) as a score). Therefore p θ may put mass in spurious regions of the space without being heavily penalized. We refer to this kind of training procedure as "coveragedriven training" (CDT). This optimizes a loss of the form L C (p θ ) = x p * (x)s c (x, p θ ) dx, where s c (x, p θ ) = ln p θ (x) evaluates how well a sample x is covered by the model. Any score that verifies L C (p θ ) = 0 ⇐⇒ p θ = p * is equivalent to the log-score, in which case it is called strictly proper (see e.g. <ref type="bibr" target="#b22">Dawid and Musio [2014]</ref>), which forms a justification for MLE on which we focus.</p><p>Explicitly evaluating sample quality is redundant in the regime of unlimited model capacity and training data. Indeed, putting mass on spurious regions takes it away from the support of p * , and thus reduces the likelihood of the training data. In practice, however, datasets and model capacity are finite, and models must put mass outside the finite training set in order to generalize. The maximum likelihood criterion, by construction, only measures how much mass goes off the training data, not where it goes. In classic MLE, generalization is controlled in two ways: (i) inductive bias, in the form of model architecture, controls where the off-dataset mass goes, and (ii) regularization controls to which extent this happens. An adversarial loss, by considering samples of the model p θ , can provide a second handle to evaluate and control where the off-dataset mass goes. In this sense, and in contrast to model architecture design, an adversarial loss provides a "trainable" form of inductive bias.</p><p>Adversarial models and mode collapse. Adversarially trained models produce samples of excellent quality. As mentioned, their main drawbacks are their tendency to "mode-drop", and the lack of metric to assess mode-dropping, or their performance in general. The reasons for this are two-fold. First, defining a valid likelihood requires adding volume to the low-dimensional manifold learned by GANs to define a density under which training and test data have non-zero density. Second, computing the density of a data point under the defined probability distribution requires marginalizing out the latent variables, which is not trivial in the absence of an efficient inference mechanism.</p><p>When a human expert subjectively evaluates the quality of generated images, samples from the model are compared to the expert's implicit approximation of p * . This type of objective may be formalized as L Q (p θ ) = x p θ (x)s q (x, p * ) dx, and we refer to it as "quality-driven training" (QDT). To see that GANs <ref type="bibr" target="#b42">Goodfellow et al. [2014]</ref> use this type of training, recall that the discriminator is trained with the loss</p><formula xml:id="formula_181">L GAN = x p * (x) ln D(x) + p θ (x) ln(1 -D(x)) dx.</formula><p>It is easy to show that the optimal discriminator equals D * (x) = p * (x)/(p * (x) + p θ (x)), see <ref type="bibr" target="#b42">Goodfellow et al. [2014]</ref>. Substituting the optimal discriminator, L GAN equals the Jensen-Shannon divergence,</p><formula xml:id="formula_182">D JS (p * ||p θ ) = 1 2 D KL (p * || 1 2 (p θ + p * )) + 1 2 D KL (p θ || 1 2 (p θ + p * )),<label>(5.1)</label></formula><p>up to additive and multiplicative constants <ref type="bibr" target="#b42">[Goodfellow et al., 2014]</ref>. This loss, approximated by the discriminator, is symmetric and contains two KL divergence terms. Note that D KL (p * || 1 2 (p θ + p * )) is an integral on p * , so coverage driven. The term that approximates it in L GAN , i.e., x p * (x) ln D(x), is however independent from the generative model, and disappears when differentiating. Therefore, it cannot be used to perform coverage-driven training, and the generator is trained to minimize ln(1 -D(G(z))), where G(z) is the deterministic generator that maps latent variables z to the data space. Assuming D = D * , this yields</p><formula xml:id="formula_183">z p(z) ln(1 -D * (G(z))) dz = x p θ (x) ln p θ (x) p θ (x) + p * (x) dx = D KL (p θ || (p θ + p * )/2),</formula><p>(5.2) which is a quality-driven criterion, favoring sample quality over support coverage. An alternative training loss for the generator, also proposed in <ref type="bibr" target="#b42">Goodfellow et al. [2014]</ref>, is to maximize ln D(G(z)), which does not modify the optimum but improves early training. As remarked by <ref type="bibr" target="#b138">Sønderby et al. [2017]</ref>, it is possible to simultaneously minimize ln( <ref type="formula">1</ref> The complementarities that appear between maximum likelihood and adversarial training motivate hybrid training of generative models, which is presented in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Adaptive Density Estimation and hybrid adversarial-likelihood training</head><p>We present a hybrid training approach with MLE to cover the full support of the training data, and adversarial training as a trainable inductive bias mechanism to improve sample quality. Using both these criteria provides a richer training signal, but satisfying both criteria is more challenging than each in isolation for a given model complexity. In practice, model flexibility is limited by (i) the number of parameters, layers, and features in the model, and (ii) simplifying modelling assumptions, usually made for tractability. We show that these simplifying assumptions create a conflict between the two criteria, making successful joint training non trivial. We introduce Adaptive Density Estimation as a solution to reconcile them.</p><p>Latent variable generative models, defined as p θ (x) = z p θ (x|z)p(z) dz, typically make simplifying assumptions on p θ (x|z), such as full factorization and/or Gaussianity, see e.g. <ref type="bibr" target="#b31">Dorta et al. [2018]</ref>, <ref type="bibr">Kingma and Welling [2014b]</ref>, <ref type="bibr" target="#b83">Litany et al. [2018]</ref>. In particular, assuming full factorization of p θ (x|z) implies that any correlations not captured by z are treated as independent per-pixel noise. This is a poor model for natural images, unless z captures each and every aspect of the image structure. Crucially, this hypothesis is problematic in the context of hybrid MLE-adversarial training. If p * is too complex for p θ (x|z) to fit it accurately enough, MLE will lead to a high variance in a factored (Gaussian) p θ (x|z) as illustrated in Figure <ref type="figure" target="#fig_35">5</ref>.4 (right).</p><p>This leads to unrealistic blurry samples, easily detected by an adversarial discriminator, which then does not provide a useful training signal. Conversely, adversarial training will try to avoid these poor samples by dropping modes of the training data, and driving the "noise" level to zero. This in turn is heavily penalized by maximum likelihood training, and leads to poor likelihoods on held-out data.</p><p>Adaptive density estimation. The point of view of regression hints at a possible solution. For instance, with isotropic Gaussian densities, p(x|z) = N (x|µ(z), σI)) with fixed variance, solving the optimization problem θ * ∈ max θ ln(p θ (x)) is similar to solving min θ ||µ θ (z) -x|| 2 , i.e., 2 regression, where µ θ (z) is the mean of the decoder p θ (x|z). The Euclidean distance in RGB space is known to be a poor measure of similarity between images, non-robust to small translations or other basic transformations <ref type="bibr" target="#b96">Mathieu et al. [2016]</ref>. One can instead compute the Euclidean distance in a feature space, ||f ψ (x 1 ) -f ψ (x 2 )|| 2 , where f ψ is chosen so that the distance is a better measure of similarity. A popular way to obtain f ψ is to use a CNN that learns a non-linear image representation, that allows linear assessment of image similarity. This is the idea underlying GAN discriminators, the FID evaluation measure <ref type="bibr" target="#b51">[Heusel et al., 2017]</ref>, and the reconstruction loss of VAE- <ref type="bibr">GAN Larsen et al. [2016]</ref>.</p><p>Despite their flexibility, such similarity metrics are in general degenerate in the sense that they may discard information about the data point x. For instance, two different images x and y can collapse to the same points in feature space, i.e., f ψ (x) = f ψ (y). This limits the use of similarity metrics in the context of generative modeling for two reasons: (i) it does not yield a valid measure of likelihood over inputs, and (ii) points generated in the feature space f ψ cannot easily be mapped to images. To resolve this issue, we chose f ψ to be a bijection. Given a model p θ trained to model f ψ (x) in feature space, a density in image space is computed using the change of variable formula, which yields p θ,ψ (x) = p θ (f ψ (x)) det ∂f ψ (x)/∂x . Image samples are obtained by sampling from p θ in feature space, and mapping to the image space through f -1 ψ . We refer to this construction as Adaptive Denstiy Estimation. If p θ provides efficient loglikelihood computations, the change of variable formula can be used to train f ψ and p θ together by maximum-likelihood, and if p θ provides fast sampling adversarial training can be performed efficiently.</p><p>MLE with adaptive density estimation. To train a generative latent variable model p θ (x) which permits efficient sampling, we rely on amortized variational inference. We use an inference network q φ (z|x) to construct a variational evidence lower-bound (ELBO),</p><formula xml:id="formula_184">L ψ ELBO (x, θ, φ) = E q φ (z|x)</formula><p>[ln(p θ (f ψ (x)|z))] -D KL (q φ (z|x) || p θ (z)) ≤ ln p θ (f ψ (x)). (5.4)</p><p>Using this lower bound together with the change of variable formula, the mapping to the similarity space f ψ and the generative model p θ can be trained jointly with the loss</p><formula xml:id="formula_185">L C (θ, φ, ψ) = E x∼p * -L ψ ELBO (x, θ, φ) -ln det ∂f (x) ∂x T ≥ -E x∼p * [ln p θ,ψ (x)] .</formula><p>(5.5)</p><p>We use gradient descent to train f ψ by optimizing L C (θ, φ, ψ) w.r.t. ψ. The L ELBO term encourges the mapping f ψ to maximize the density of points in feature space under the model p θ , so that f ψ is trained to match modeling assumptions made in p θ . Simultaneously, the log-determinant term encourages f ψ to maximize the volume of data points in feature space. This guarantees that data points cannot be collapsed to a single point in the feature space. We use a factored Gaussian form of the conditional p θ (.|z) for tractability, but since f ψ can arbitrarily reshape the corresponding conditional image space, it still avoids simplifying assumptions in the image space. Therefore, the (invertible) transformation f ψ avoids the conflict between the MLE and adversarial training mechanisms, and can leverage both.</p><p>Adversarial training with adaptive density estimation. To sample the generative model, we sample latents from the prior, z ∼ p θ (z), which are then mapped to feature space through µ θ (z), and to image space through f -1 ψ . We train our generator using the modified objective proposed by <ref type="bibr" target="#b138">[Sønderby et al., 2017]</ref>, which combines both generator losses considered in <ref type="bibr" target="#b42">[Goodfellow et al., 2014]</ref> (see Equation <ref type="formula" target="#formula_182">5</ref>.3) and yields:</p><formula xml:id="formula_186">L Q (p θ,ψ ) = -E p θ (z) ln D(f -1 ψ (µ θ (z))) -ln(1 -D(f -1 ψ (µ θ (z)))) .</formula><p>(5.6)</p><p>Assuming the discriminator D is trained to optimality at every step, it is easy to demonstrate that the generator is trained to optimize D KL (p θ,ψ || p * ). The training procedure, written as an algorithm in Section A.3, alternates between (i) bringing L Q (p θ,ψ ) closer to it's optimal value</p><formula xml:id="formula_187">L * Q (p θ,ψ ) = D KL (p θ,ψ || p * ), and (ii) minimizing L C (p θ,ψ ) + L Q (p θ,ψ ).</formula><p>Assuming that the discriminator is trained to optimality at every step, the generative model is trained to minimize a bound on the sum of two symmetric KL divergences:</p><formula xml:id="formula_188">L C (p θ,ψ ) + L * Q (p θ,ψ ) ≥ D KL (p * || p θ,ψ ) + D KL (p θ,ψ || p * ) + H(p * ),<label>(5.7)</label></formula><p>where the entropy of the data generating distribution, H(p * ), is an additive constant independent of the generative model p θ,ψ . In contrast, MLE and GANs optimize one of these divergences each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Experimental evaluation</head><p>We present our evaluation protocol, followed by an ablation study to assess the importance of the components of our model in sectablation. We then show the quantitative and qualitative performance of our model, and compare it to the state of the art on the CIFAR-10 dataset in Section 5.5.3. We present additional results and comparisons on higher resolution datasets in Section 5.5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Evalutation metrics</head><p>We evaluate our models with three complementary metrics. To assess sample quality, we report the Fréchet inception distance (FID) <ref type="bibr" target="#b51">[Heusel et al., 2017]</ref> and the inception score (IS) <ref type="bibr">[Salimans et al., 2016b]</ref>, which are the de facto standard metrics to evaluate GANs <ref type="bibr" target="#b13">[Brock et al., 2019</ref><ref type="bibr">, Zhang et al., 2018]</ref>. Although these metrics focus on sample quality, they are also sensitive to coverage, see Section 2.12.3 for details. To specifically evaluate the coverage of held-out data, we use the standard bits per dimension (BPD) metric, defined as the negative log-likelihood on held-out data, averaged across pixels and color channels <ref type="bibr" target="#b27">[Dinh et al., 2017]</ref>.</p><p>Due to their degenerate low-dimensional support, GANs do not define a density in the image space, which prevents measuring BPD on them. To endow a GAN with a full support and a likelihood, we train an inference network "around it", while keeping the weights of the GAN generator fixed. We also train an isotropic noise parameter σ. For both GANs and VAEs, we use the inference network to compute a lower bound to approximate the likelihood, i.e., an upper bound on BPD.</p><p>We conduct an ablation study on the CIFAR-10 dataset with the standard split of 50k/10k train/test images of 32×32 pixels. We evaluate all metrics using held-out data not used during training, which improves over common practice in the GAN literature, where training data is often used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Ablation study and comparison to VAE and GAN baselines</head><p>Our GAN baseline uses the non-residual architecture of SNGAN <ref type="bibr">[Miyato et al., 2018a]</ref> Experimental results in Table <ref type="table" target="#tab_12">5</ref>.1 confirm that the GAN baseline yields better sample quality (IS and FID) than the VAE baseline, e.g., obtaining inception scores of 6.8 and 2.0, respectively. Conversely, VAE achieves better coverage, with a BPD of 4.4, compared to 7.0 for GAN, which is not  These experiments demonstrate that our proposed bijective feature space substantially improves the compatibility of coverage and quality driven training. We obtain improvements over both VAE and GAN in terms of held-out likelihood, and improve VAE sample quality to, or beyond, that of GAN. We further evaluate our model using the recent precision and recall approach of <ref type="bibr" target="#b124">[Sajjadi et al., 2018]</ref> an the classification framework of <ref type="bibr" target="#b133">[Shmelkov et al., 2018]</ref> in Section 5.6.1. Additional results showing the impact of the number of layers and scales in the bijective similarity mapping f ψ are presneted in Section 5.7, and reconstructions qualitatively demonstrating the inference abilities of our AV-ADE model are presented in Section A.2.1.</p><formula xml:id="formula_189">f ψ Adv. MLE BPD ↓ IS ↑ FID ↓ GAN × ×<label>[</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Refinements and comparison to the state of the art</head><p>We now consider further refinements to our model, inspired by recent generative modeling literature. Four refinements are used: (i) adding residual connections to the discriminator <ref type="bibr">[Gulrajani et al., 2017b]</ref> (rd), (ii) leveraging more accurate posterior approximations using inverse auto-regressive flow <ref type="bibr">[Kingma et al., 2016b]</ref> (iaf); see Section A.2, (iii) training wider generators with twice as many channels (wg), and (iv) using a hierarchy of two scales to build f ψ (s2); see Section 5.7. Table <ref type="table" target="#tab_12">5</ref>.2 shows consistent improvements with these additions, in terms of BPD, IS, FID.</p><p>Table <ref type="table" target="#tab_12">5</ref>.3 compares our model to existing hybrid approaches and state-of-the-art generative models on CIFAR-10. In the category of hybrid models that optimize likelihood, denoted by Glow @ 3.35 BPD FlowGan (H) @ 4.21 BPD AV-ADE (iaf, rd) @ 3.7 BPD 3.5 6.9 28.9 3.5 6.9 28.9 FlowGan(A) [Grover et al., 2018] 8.5 5.8 FlowGan(H) [Grover et al., 2018] 4.2 3.9</p><p>Hybrid (A) BPD ↓ IS ↑ FID ↓ AGE [Ulyanov et al., 2018]  5.9 ALI [Dumoulin et al., 2017a]  5.3 SVAE [Chen et al., 2018]  6.8 α-GAN [Rosca et al., 2017]  6.8 SVAE-r [Chen et al., 2018]  7.0</p><p>Adversarial BPD ↓ IS ↑ FID ↓ mmd-GAN [Arbel et al., 2018]  7.3 25.0 SNGan [Miyato et al., 2018a]  7.4 29.3 BatchGAN [Lucas et al., 2018a]  7.5 23.7 WGAN-GP [Gulrajani et al., 2017a]  7.9 SNGAN (R,H) 8.2 21.7</p><formula xml:id="formula_190">MLE BPD ↓ IS ↑ FID ↓</formula><p>NVP [Dinh et al., 2017]  3.5 4.5 † 56.8 † VAE-IAF [Kingma et al., 2016b] 3.1 3.8 † 73.5 † Pixcnn++ [Salimans et al., 2017b] 2.9 5.5 Flow++ [Ho et al., 2019]  3.1 Glow [Kingma and Dhariwal, 2018] 3.4 5.5 ‡ 46.8 ‡ Table <ref type="table" target="#tab_12">5</ref>.3: Performance on CIFAR10, without labels. MLE and Hybrid (L) models discard the test split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.4">Results on additional datasets</head><p>To further validate our model we evaluate it on STL10 (48 × 48), ImageNet and LSUN (both 64 × 64). We use a wide generator to account for the higher resolution, without iaf, a single scale in f ψ , and no residual blocks (see Section 5.5.3).</p><p>The architecture and training hyper-parameters are the same as before, besides adding one layer at resolution 64 × 64, which demonstrates the stability of our approach. Quantitative results on STL10, and ImageNet are reported in  inception score over SNGAN, from 9.1 up to 9.4, and is second best in FID, with corresponding samples displayed in Figure <ref type="figure" target="#fig_35">5</ref>.8. Our likelihood performance, between 3.8 and 4.4, and close to that of NVP at 3.7, demonstrates good coverage of the support of held-out data. On the ImageNet dataset, maintaining high sample quality, while covering the full support is challenging, due to its very diverse support. Our AV-ADE model obtains a sample quality behind that of MMD-GAN with IS/FID scores at 8.5/45.5 vs. 10.9/36.6. However, MMD-GAN is trained purely adversarially and does not provide support guarantees, unlike our approach.</p><p>Figure <ref type="figure" target="#fig_35">5</ref>.9 shows samples from our generator trained on a single GPU with 11 Gb of memory on LSUN classes. It yields compelling samples compared to those of the Glow model, despite having more flexibility (over 500 VS 7 layers) showing that Glow spills more mass outside of the training support (while our model mode drops more, hence the inferior BPD). Quantitative performance on the LSUN dataset is reported in Table <ref type="table" target="#tab_12">5</ref>.5. Additional samples and other LSUN categories are presented in Section 5.6.2.</p><p>Glow <ref type="bibr" target="#b67">[Kingma and Dhariwal, 2018]</ref> Ours: AV-ADE (wg, rd)</p><formula xml:id="formula_191">(C) (B)</formula><p>Figure <ref type="figure" target="#fig_35">5</ref>.9: Samples from models trained on LSUN Churches (C) and bedrooms (B). Our AV-ADE model over-generalises less and produces more compelling samples. See Section 5.6.2 for more classes and samples.  ).</p><p>Class conditioning with conditional weight-normalization. To perform this evaluation we develop a class conditional version of our AV-ADE model. The discriminator is conditioned using the class conditioning introduced by Miyato and Koyama <ref type="bibr">[2018]</ref>. GAN generators are typically made class-conditional using conditional batch normalization <ref type="bibr" target="#b23">[De Vries et al., 2017</ref><ref type="bibr">, Dumoulin et al., 2017b]</ref>, however batch normalization is known to be detrimental in VAEs <ref type="bibr">[Kingma et al., 2016b</ref>], as we verified in practice. To address this issue, we propose conditional weight normalization (CWN). As in weight normalization <ref type="bibr" target="#b125">[Salimans and Kingma, 2016]</ref>, we separate the training of the scale and the direction of the weight matrix. Additionally, the scaling factor g(y) of the weight matrix v is conditioned on the class label y:</p><formula xml:id="formula_192">w = g(y) v v,<label>(5.8)</label></formula><p>We also make the network biases conditional on the class label. Otherwise, the architecture is the same one used for the experiments in Table <ref type="table" target="#tab_12">5</ref>.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.2">Model evaluation using precision and recall</head><p>In this section, we evaluate our models using the precision and recall procedure of <ref type="bibr" target="#b124">[Sajjadi et al., 2018]</ref>. This evaluation is relevant as it seeks to evaluate coverage of the support and the quality of the samples separately, rather than aggregating them into a single score.</p><p>Figure <ref type="figure" target="#fig_35">5</ref>.11: Precision-recall curves using the evaluation procedure of <ref type="bibr" target="#b124">[Sajjadi et al., 2018]</ref>.</p><p>Figure <ref type="figure" target="#fig_35">5</ref>.11 presents the evaluation of our models in Section 5.5.2, as well as the Glow and NVP models, using the official code provided online by the authors at <ref type="url" target="https://github.com/msmsajjadi/precision-recall-distributions">https://github.com/ msmsajjadi/precision-recall-distributions</ref>. Our AV-ADE model obtains a better area under curve (AUC) than the GAN baseline, and model refinements improve AUC further. For comparison, Glow and NVP have lower AUC than both GAN and our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Qualitative influence of the feature space flexibility</head><p>We experiment with different architectures to implement the invertible mapping used to build the feature space as presented in Section 5.4. To assess the impact of the expressiveness of the invertible model on the behavior of our framework, we modify various standard parameters of the architecture. Popular invertible models such as NVP <ref type="bibr" target="#b27">[Dinh et al., 2017]</ref> readily offer the possibility of extracting latent representation at several scales, separating global factors of variations from low level detail. Thus, we experiment with varying number of scales. An other way of increasing the flexibility of the model is to change the number of residual blocks used in each invertible layer. Note that all the models evaluated so far in the main body of the paper are based on a single scale and two residual blocks, except the one denoted with (s2). In addition to our AV-ADE models, we also compare with similar models trained with maximum likelihood estimation (MLE). Models are first trained with maximum-likelihood estimation, then with both coverage and quality driven criteria.</p><p>The results in Table <ref type="table" target="#tab_12">5</ref>.12 (a) show that factoring out features at two scales rather than one is helpful in terms of BPD. For the AV-ADE models, however, IS and FID deteriorate with more scales, and so a tradeoff between must be struck. For the V-ADE models, the visual quality of samples also improves when using multiple scales, as reflected in better IS and FID scores. Their quality, however, remains far worse than those produced with the coverage and quality training used for the AV-ADE models. Samples in the maximum-likelihood setting are provided in Figure <ref type="figure" target="#fig_35">5</ref>.12 (c). With three or more scales, models exhibit symptoms of overfitting: train BPD keeps decreasing while test BPD starts increasing, and IS and FID also degrade.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Conclusion</head><p>We presented a generative model that leverages invertible network layers to relax the conditional pixel independence assumption commonly made in VAE decoders. It allows for efficient feedforward sampling, and can be trained using a maximum likelihood criterion that ensures coverage of the data generating distribution, as well as an adversarial criterion that ensures high sample quality. This is a step towards limitting mode-collapse in GANs, because the coverage of the support is explicitly optimized and can be evaluated. On the other hand adversarial training pushes the model to produce more compelling samples compared to purely maximum-likelihood based models, and this is achieved at little expense in terms of bits per dimension.</p><p>can thus be trained using an adversarial criterion that ensures high sample quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Distributional statistics in adversarial training</head><p>In Chapter 4 we have shown how to build adversarial networks architectures that can consider the samples in a batch together rather than independently. This allows the model to approximate statistics on the variability of the learned distribution and use them to reject or accept a batch.</p><p>The discriminator is thus able to explicitly evaluate the diversity of the samples produced by the generator. Assuming the discriminator has fit the distribution of batches of real images perfectly, fooling it requires two things: i) batches of samples must contain as much variability as batches of real images and ii) each image in a batch has to be of high quality. Our approach thus turns mode dropping into something that is explicitly penalised, together with image quality. After observing that the problem is invariant to permutations on the batch axis, we proposed architectures that are able to model any function invariant to permutations, assuming sufficient capacity. We also observed that in practice, using pure batches makes the discriminator's task too easy, and generalized the approach to mixed batches. Experimentally, we showed our training method to reduce mode dropping and reach good quantitative scores compared to the GAN literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Hybrid adversarial and maximum-likelihood based training</head><p>In Chapter 5 we presented a generative model that can be trained using a maximum likelihood criterion to optimize coverage of the data generating distribution, and an adversarial criterion to optimize sample quality. This is a solution to limit mode-collapse in GANs because the coverage of the support is explicitly optimized and can be evaluated. This approach is orthogonal to discriminating at the batch level, but pursues the same goal of explicitly evaluating support coverage.</p><p>The key observation to make our hybrid model work was that usual conditional independence assumptions induce a conflict between the two losses. Learning an abstract feature space in which to compute the distance between target and prediction allowed us to avoid conditional independence without incurring slow sampling, and thus to use a hybrid criterion successfully. Our model is able to generate samples of compelling visual quality, on par with purely adversarial methods. It also has inference capabilities -real images can be mapped to the latent spacewhich is useful for applications such as compression. It also achieves good bits per dimension performance on unseen data, significantly improved over existing hybrid models, which is a guarantee that the support of the dataset is well covered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Future work</head><p>It is reasonable to think that the recent progress in generative modelling makes it ripe to be used as a building block to solve other problems, and to tackle more challenging types of data. One of the next frontiers of generative modelling is that of videos -two topics of particular interest to the author are video compression and representation learning from videos. The author believes that in these domains, performance leaps are to be expected in the near future and possible research directions are presented in sections 6.2.1 and 6.2.2. Other research areas include video generation <ref type="bibr" target="#b20">[Clark et al., 2019]</ref> and world modelling for curiosity-driven reinforcement learning <ref type="bibr" target="#b106">[Pathak et al., 2017]</ref>. There is also still a lot of work to do to scale image models to larger datasets or to images of higher resolutions. Several applications of existing methods to photo and video editing, smartphone photo enhancement <ref type="bibr" target="#b78">[Ledig et al., 2017]</ref>, video games and virtual environments also have plenty of room for development and improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Video compression -Entropy coding and lossless compression</head><p>A natural application of generative models of images is video compression, which is one of the backbones of the internet. Video represents over 60 % of internet traffic<ref type="foot" target="#foot_14">foot_14</ref> , thus significant improvements to video compression can have a high impact. Deep models have been shown to be well suited to natural images, and could improve on traditional compression codecs, at least in terms of prediction performance. Two types of compression exist: lossy, and lossless. The cornerstone of lossless compression is entropy coding; the high-level idea is that frequent values should be associated to short codes and rare ones to long codes. The optimal transmission efficiency that can be reached when encoding a message x sampled from a distribution p is the Shannon Entropy, equal to E x∼p * [-log 2 p * (x)]. With a model p θ that does not perfectly match the data generating distribution, the efficiency becomes:</p><formula xml:id="formula_193">E x∼p * [-log 2 p θ (x)].</formula><p>(6.1)</p><p>The better the model p θ , the closer we get to the bound. The recent progress of deep generative models holds the promise of greatly improved compression performances, as they provide the tools to get models p θ that fit the real data distribution very well. Scaling and refining work such as <ref type="bibr" target="#b88">[Lu et al., 2019]</ref> in this context is a promising research direction. If x does not need to be perfectly recovered, one can extract some information about x into z, send z losslessly as above, and reconstruct x from z. To "extract information" and to "reconstruct", auto-encoders are natural candidates: x is encoded into z by a deep network g φ parametrised by weights φ, z is sent to the receiver and decoded into f θ (z). The metric used to evaluate the model is then a weighted sum between how good the reconstructions are, measured by L, and how costly it is to send z = g φ (x) (losslessly), using density model Q:</p><formula xml:id="formula_194">-log 2 [Q(g φ (x))] cost of sending z + βL[f θ (g φ (x)), x] distortion . (6.2)</formula><p>Variational auto-encoders provide a very natural replacement to traditional tools for lossy compression of natural images and videos. Different choices for Q and for L will lead to different models. Likelihood based losses such as the L 2 loss 2 L[f θ (g φ (x)), x] = f θ (g φ (x)) -x 2 are one form of evaluation. Another possibility is to evaluate the quality of the image using a GAN discriminator D ψ , then L adversarial = -D ψ (f θ (z)). can be added to the weighted sum. The discriminator can also be used to train the model to compensate the degradation in quality caused by the lossy compression <ref type="bibr" target="#b78">[Ledig et al., 2017]</ref>.</p><p>Such models can be directly applied to videos (in which case the encoder and decoder may be recurrent, or be 3D-convolutional networks). A strategic approach may be to decompose the problem: given a sequence of frames x 1 , . . . , x n , use an image model to independently compress all images into z 1 , . . . , z n . Then, noting that the redundancy between frames is high, a model that compresses (z 1 , . . . , z n ) into z can be built, for instance using an autoregressive model <ref type="bibr" target="#b58">[Kalchbrenner et al., 2016]</ref>. The compressed representation can be losslessly transmitted, and each z i decoded independently.</p><p>To summarize, this framework is suitable to lossy compression of videos, potentially at high compression rates. It can leverage many generative modelling concepts -VAEs for lossy compression, any model with a likelihood for lossless compression, and adversarial models to restore low level detail -and their interaction is a rich topic that has yet to be explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Representation learning</head><p>The spirit of self-supervised training is to use the structure naturally present inside raw data to extract a target signal used for optimisation. In the context of videos, one natural way to do that is the task of next-frame prediction <ref type="bibr" target="#b87">[Lotter et al., 2017]</ref>: given part of a video, the model has to predict the rest. This problem can be equivalently formulated as a conditional generative modelling problem. Training this type of models on videos in an unsupervised manner can be expected to yield a powerful representation learning framework <ref type="bibr" target="#b76">[LeCun, 2019]</ref>. Indeed, natural videos are complex and highly structured, and because there is a vast amount of data, very big models can be considered.</p><p>Given some video frames, multiple futures are usually possible which means that the output of the model needs to be a multi-modal distribution over possible futures. One approach to achieve that is to rely on latent variables, as in a VAE. These will be used to transmit information about which future to choose among possible ones, i.e., which mode of the distribution to select. Using the Euclidean norm as reconstruction loss and noting y the target, x the observed frames, z the latent variable and f θ the model, parametrised by θ, the optimisation problem being solved is then:</p><formula xml:id="formula_195">min θ min z ||y -f θ (x, z)|| 2 2 . (6.3)</formula><p>The latent variable z is here to help predict y. Only part of the signal y can be predicted from x and the other (unpredictable) part has to go through z. If no constraints are put on z, all the information contained in y can go through z making the problem trivial. So it is clear that some bottleneck on z needs to be added. It can be in the form of hard constraints for instance architectural constraints such as limiting its dimensionality, or a penalty term R(z) can be added, yielding:     For experiments with hierarchical latent variables, we use 32 of them per layer. In the generator we use ELU nonlinearity, in discriminator with residual blocks we use ReLU, while in simple convolutional discriminator we use leaky ReLU with slope 0.2.</p><formula xml:id="formula_196">min θ min z ||y -f θ (x, z)|| 2 2 + R(z). (<label>6</label></formula><p>Unless stated otherwise we use three NVP layers with a single scale and two residual blocks that we train only with the likelihood loss. Regardless of the number of scales, the VAE decoder always outputs a tensor of the same dimension as the target image, which is then fed to the NVP layers. As in the reference implementations, we use both batch normalization and weight normalization in NVP and only weight normalization in IAF. We use the reference implementations of IAF and NVP released by the authors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discriminator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Visualisations of reconstructions</head><p>We display reconstructions obtained by encoding and then decoding ground truth images with our models (AV-ADE from sectablation) in Figure A.8. As is typical for expressive variational autoencoders, real images and their reconstructions cannot be distinguished visually. regions and words but with an external representation that is learned off-line, e.g. pre-trained object detectors <ref type="bibr" target="#b37">Fang et al. [2015]</ref>, <ref type="bibr" target="#b154">Wu et al. [2016]</ref>, <ref type="bibr" target="#b160">You et al. [2016]</ref>. In contrast, our attention representation explicitly considers, in a single end-to-end trainable system, the direct interaction among caption words, image regions and RNN state. At each time-step, our model jointly predicts the next caption word and the associated image region. Similar to weakly-supervised object localization, the associations between image regions and words are inferred during training from image-level captions. Our experimental results show that our three pair-wise interactions clearly improve the attention focus and the quality of the generated sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real image AV-ADE reconstruction</head><p>Our second contribution is to integrate a localization sub-network in our model -similar to spatial transformer networks <ref type="bibr" target="#b55">Jaderberg et al. [2015]</ref>, but applied in a convolutional fashion-that regresses a set of attention areas from the image content. Earlier attention-based image captioning models used the positions in the activation grid of a CNN layer as attention areas, see e.g. <ref type="bibr" target="#b0">Xu et al. [2015]</ref>; such regions are not adaptive to the image content. Others have used object proposals as attention regions, see e.g. <ref type="bibr" target="#b56">Jin et al. [2015]</ref>, in which case the regions are obtained by an external mechanism, such as Edge boxes Zitnick and Dollár <ref type="bibr">[2014]</ref>, that is not trained jointly with the rest of the captioning system.</p><p>Our third contribution is a systematic experimental study of the effectiveness of these three different areas of attention using a common attention model, see <ref type="bibr">Figure B.1(bottom)</ref>. To the best of our knowledge we are the first to present such a comparison. Our experimental results show that the use of image-specific areas of attention is important for improved sentence generation. In particular, our spatial-transformer based approach is a good choice: it outperforms the other approaches, while using fewer regions and not requiring an external proposal mechanism. Using our proposed attention mechanism and the spatial transformer attention areas together we obtain state-of-the-art performance on the MSCOCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Related work</head><p>Image captioning with encoder-decoder models has recently been extensively studied, see e.g. <ref type="bibr" target="#b8">Bengio et al. [2015]</ref>, <ref type="bibr" target="#b29">Donahue et al. [2015]</ref>, <ref type="bibr" target="#b59">Karpathy and Fei-Fei [2015]</ref>, <ref type="bibr" target="#b70">Kiros et al. [2014]</ref>, <ref type="bibr" target="#b95">Mao et al. [2015]</ref>, <ref type="bibr" target="#b112">Ranzato et al. [2016]</ref>, <ref type="bibr" target="#b152">Vinyals et al. [2015]</ref>, <ref type="bibr" target="#b0">Xu et al. [2015]</ref>, <ref type="bibr" target="#b157">Yang et al. [2016]</ref>. In its basic form a CNN processes the input image to encode it into a vectorial representation, which is used as the initial input for an RNN. Given the previous word, the RNN sequentially predicts the next word in the caption without the need to restrict the temporal dependence to a fixed order, as in approaches based on n-grams. The CNN image representation can be entered into the RNN in different manners. While some authors <ref type="bibr" target="#b59">Karpathy and Fei-Fei [2015]</ref>, <ref type="bibr" target="#b152">Vinyals et al. [2015]</ref> use it only to compute the initial state of the RNN, others enter it in each RNN iteration <ref type="bibr" target="#b29">Donahue et al. [2015]</ref>, <ref type="bibr" target="#b95">Mao et al. [2015]</ref>. <ref type="bibr" target="#b0">Xu et al. Xu et al. [2015]</ref> were the first to propose an attention-based approach for image captioning, in which the RNN state update includes the visual representation of an image region. Which image region is attended to is determined based on the previous state of the RNN. They propose a "soft" variant in which a convex combination of different region descriptors is used, and a "hard" variant in which a single region is selected. The latter is found to perform slightly better, but is more complex to train due to a non-differentiable sampling operator in the state update. In thier approach the positions in the activation grid of a convolutional CNN layer is the loci of attention. Each position is described with the corresponding activation column across the layer's channels.</p><p>Several works build upon the approach of Xu et al. <ref type="bibr" target="#b0">Xu et al. [2015]</ref>. <ref type="bibr" target="#b160">You et al. You et al. [2016]</ref> learn a set of attribute detectors, similar to <ref type="bibr" target="#b37">Fang et al. Fang et al. [2015]</ref>, for each word of their vocabulary. These detectors are applied to an image, and the strongest object detections are used as regions for an attention mechanism similar to that of <ref type="bibr" target="#b0">Xu et al. Xu et al. [2015]</ref>.</p><p>In their work the detectors are learned prior and independently from the language model. Wu et al. <ref type="bibr" target="#b154">Wu et al. [2016]</ref> also learn attribute detectors but manually merge word tenses (walking, walks) and plural/singulars (dog, dogs) to reduce the set of attributes. <ref type="bibr" target="#b56">Jin et al. Jin et al. [2015]</ref> explore the use of selective search object proposals <ref type="bibr" target="#b143">Uijlings et al. [2013]</ref> as regions of attention. They resize the regions to a fixed size and use the <ref type="bibr">VGG16 Simonyan and Zisserman [2015]</ref> penultimate layer to characterize them. <ref type="bibr" target="#b157">Yang et al. Yang et al. [2016]</ref> improve the attention based encoder-decoder model by adding a reviewer module that improves the representation passed to the decoder. They show improved results for various tasks, including image captioning. <ref type="bibr" target="#b158">Yao et al. Yao et al. [2015]</ref>  Visual grounding of natural language expressions is a related problem <ref type="bibr" target="#b59">Karpathy and Fei-Fei [2015]</ref>, <ref type="bibr" target="#b120">Rohrbach et al. [2016]</ref>, which can be seen as an extension of weakly supervised object localization <ref type="bibr" target="#b11">Bilen and Vedaldi [2016]</ref>, <ref type="bibr" target="#b19">Cinbis et al. [2014]</ref>, <ref type="bibr" target="#b123">Russakovsky et al. [2012]</ref>. The goal is to localize objects referred to by natural language descriptions, while only using image-level supervision. Since the goal in visual grounding and weakly supervised localization is precise localization, methods typically rely on object proposal regions which are specifically designed to align well with object boundaries <ref type="bibr" target="#b143">Uijlings et al. [2013]</ref>, <ref type="bibr" target="#b80">Zitnick and Dollár [2014]</ref>. Instead of localizing a given textual description, our approach uses image-level supervision to infer a latent correspondence between the words in the caption and image regions.</p><p>Object proposal methods were designed to focus computation of object detectors on a selective set of image regions likely to contain objects. Recent state-of-the-art detectors, however, integrate the object proposal generation and recognition into a single network. This is computationally more efficient and leads to more accurate results <ref type="bibr" target="#b85">Liu et al. [2016]</ref>, <ref type="bibr" target="#b117">Ren et al. [2015]</ref>. <ref type="bibr" target="#b57">Johnson et al. Johnson et al. [2016]</ref> use similar ideas for the task of localized image captioning, which predicts semantically relevant image regions together with their descriptions. In each region, they generate descriptions with a basic non-attentive image captioning model similar to the one used by <ref type="bibr" target="#b152">Vinyals et al. Vinyals et al. [2015]</ref>. They train their model from a set of bounding-boxes with corresponding captions per image. In our work we do not exploit any bounding-box level supervision, we instead infer the latent associations between caption words and image regions. We propose a convolutional variant of the spatial transformer network of Jaderberg et al. <ref type="bibr" target="#b55">Jaderberg et al. [2015]</ref>, to place the attention areas in an image-adaptive manner. This module is trained in an integrated end-to-end manner with the rest of our captioning model. Compared to previous attention models <ref type="bibr" target="#b56">Jin et al. [2015]</ref>, <ref type="bibr" target="#b0">Xu et al. [2015]</ref>, <ref type="bibr" target="#b157">Yang et al. [2016]</ref>, <ref type="bibr" target="#b160">You et al. [2016]</ref>, our attention mechanism, consisting of a single interaction layer, is less complex yet improves performance. Our approach models a joint distribution over image regions and caption words, generalizing weakly supervised localization methods and RNN language models. It includes a region-word interaction found in weakly supervised localization, as well as a wordstate interaction found in RNN language models. In addition, our model includes a region-state interaction which forms a dynamic appearance-based salience mechanism. Our model naturally handles different types of attention regions (fixed grid, object proposals, and spatial transformers), and is applicable to all tasks where attention can model joint distributions between parts of the input data and output symbols. To the best of our knowledge, we propose the first trainable image-adaptive method to define attention regions, and present the first systematic comparison among different region types for attention-based image captioning in a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Attention in encoder-decoder captioning</head><p>In Section B.3.1 we describe our baseline encoder-decoder model. We extend this baseline in Section B.3.2 with our attention mechanism in a way that abstracts away from the underlying region types. In Section B.3.3 we show how we integrate regions based on CNN activation grids, object proposals, and spatial transformers networks in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.1 Baseline CNN-RNN encoder-decoder model</head><p>Our baseline encoder-decoder model uses a CNN to encode an image I into a vectorial representation φ(I) ∈ IR d I , which is extracted from a fully connected layer of the CNN. The image encoding φ(I) is used to initialize the state of an RNN language model. Let h t denote the RNN state vector at time t, then h 0 = θ hi φ(I), where θ hi ∈ IR d h ×d I linearly maps φ(I) to the RNN state space of dimension d h .</p><p>The distribution over w t , the word at time t, is given by a logistic regression model over the See Figure <ref type="figure">B</ref>.2 for a schematic illustration of our model.</p><p>We define a joint distribution, p(w t , r t |h t ), over words w t and image regions r t at time t given the RNN state h t . The marginal distribution over words, p(w t |h t ), is used to predict the next word at every time-step, while the marginal distribution over regions, p(r t |h t ), is used to provide visual feedback to the RNN state update. Let r t ∈ {0, 1} nr denote a 1-hot coding of the index of the region attended to among n r regions at time t. We write the state-conditional joint distribution on words and regions as where R contains the region descriptors in its rows. The score function s(w t , r t , h t ) is composed of three bi-linear pairwise interactions. The first scores state-word combinations, as in the baseline model. The second scores the compatibility between words and region appearances, as in weakly supervised object localization. The third scores region appearances given the current state, and acts as a dynamic salience term. The last two unary terms implement linear bias terms for words and regions respectively. Given the RNN state, the next word in the image caption is predicted using the marginal word distribution, p(w t |h t ) = rt p(w t , r t |h t ), which replaces Eq. (B.1) of the baseline model. The baseline model is recovered for R = 0.</p><p>In addition to using the image regions to extend the state-conditional word prediction model, we also use them to extend the feedback connections of the RNN state update. We use a mechanism related to the soft attention model of <ref type="bibr" target="#b0">Xu et al. Xu et al. [2015]</ref>. We compute a convex combination of region descriptors which will enter into the state-update. In contrast to Xu et al., we derive the region weights from the joint distribution defined above. In particular, we use the marginal distribution over regions, p(r t |h t ) = wt p(w t , r t |h t ), to pool the region descriptors as </p><formula xml:id="formula_197">v t = rt p(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.3 Areas of attention</head><p>Our attention mechanism presented above is agnostic to the definition of the attention regions. In this section we describe how to integrate three types of regions in our model.</p><p>Activation grid. For the most basic notion of image regions we follow the approach of Xu et al. <ref type="bibr" target="#b0">Xu et al. [2015]</ref>. In this case the regions of attention correspond to the z = x × y spatial positions in the activation grid of a CNN layer γ(I) with c channels. The region descriptors in the rows of R ∈ IR z×c are given by the activations corresponding to each one of the z locations of the activation grid. In this case, the receptive fields for the regions is the same as all regions have a fixed shape and size, independent of the image content.</p><p>Object proposals. To obtain attention regions that adapt to the image content, we consider the use of object detection proposals, similar to the approach of Jin et al. <ref type="bibr" target="#b56">Jin et al. [2015]</ref>. We expect such regions to be more effective since they tend to focus on scene elements such as (groups of) objects, and their parts. In particular we use edge-boxes Zitnick and Dollár <ref type="bibr">[2014]</ref>, and max-pool the activations in a CNN layer γ(I) over each object proposal to obtain a set of fixed-size region descriptors. To ensure a high-enough resolution of the CNN layer which allows to pool activations for small proposals, we use a separate CNN which processes the input image at a higher resolution than the one used for the global image representation φ(I). This is similar to <ref type="bibr" target="#b40">Girshick [2015]</ref>, <ref type="bibr" target="#b50">He et al. [2014]</ref>, but we pool to a single cell instead of using a spatial pyramid. This is more efficient and did not deteriorate performance, as compared to using a pyramid. In this case the number of proposals is not limited by the number of positions in the activation tensor of the CNN layer that is accessed for the region descriptors.</p><p>Spatial transformers. We propose a third type of attention region that has not been used in existing attention-based captioning models. It is inspired by recent object detectors and localized image captioning methods with integrated the region proposal networks <ref type="bibr" target="#b57">Johnson et al. [2016]</ref>, <ref type="bibr" target="#b85">Liu et al. [2016]</ref>, <ref type="bibr" target="#b117">Ren et al. [2015]</ref>. In contrast to the latter methods, which rely on boundingbox annotations to learn the region proposal network, we only use image captions for training. Therefore, we need a mechanism that allows back-propagation of the gradient of the captioning loss w.r.t. the region coordinates and the features extracted using them. To this end we use a bilinear sampling approach as in <ref type="bibr" target="#b55">Jaderberg et al. [2015]</ref>, <ref type="bibr" target="#b57">Johnson et al. [2016]</ref>. In contrast to the max-pooling we use for proposals, it enables differentiation w.r.t. the region coordinates.</p><p>Our approach is illustrated in Figure B.3. Given an activation map γ(I), we use a localization network that consists of two convolutional layers to locally regress an affine transformation A ∈ IR 2×3 for each location of the feature map. With each location of the activation map γ(I) we associate an "anchor box", which is centered at that position and covers 3 × 3 activations. The affine transformations, computed at each location in a convolutional fashion, are applied to the coordinates of the anchor boxes. Locally a 3 × 3 patch is bilinearly interpolated from γ(I) over the area of the transformed anchor box. A 3 × 3 filter is then applied to the locally extracted patches to compute the region descriptor, which has the same number of dimensions as the activation tensor γ(I) has channels. If the local transformations leave the anchor boxes unchanged, then this reduces to the activation grid approach.</p><p>As we have no bounding-box annotations, training the spatial transformer can get stuck at poor local minima. To alleviate this issue, we initialize the network with a model that was trained RNN uses only word-state interaction terms to predict the next word given the RNN state. Adding the word-region interaction term (second row) improves the CIDEr metric by 4.7 points to 83.6. This demonstrates the significance of localized visual input to the RNN. As in weakly-supervised object detection, the model learns to associate caption terms to local appearances. Adding the third pairwise interaction term between regions and the RNN state (third row) brings another improvement of 1.9 points to 85.5 CIDEr. This shows that the RNN is also able to implement a dynamic salience mechanism that favors certain regions over others at a given time-step by scoring the compatibility between the RNN state and the region appearance. Finally we add the visual feedback mechanism to our model (87.4, last row), which drives the CIDEr-D score further up by 1.9 points. We also experimented with a word-conditional version of the visual feedback mechanism (86.8, last but one row), which uses p(r t |w t , h t ) instead of p(r t |h t ) to compute the visual feedback. Although this also improves the CIDEr-D score, as compared to not using visual feedback, it is less effective than using the marginal distribution weights. The visualizations in Figure <ref type="figure">B</ref>.5 suggest that the reason for this is that the marginal distribution already tends to focus on a single semantically meaningful area.</p><p>Comparing areas of attention. In our next set of experiments we compare the effectiveness of different attention regions in our model. In Figure <ref type="figure">B</ref>.4 we consider the performance of the three region types as a function of the number of regions that are used when running the trained model on test images. For activation grids and spatial transformers the number of regions are regularly sampled from the original 14 × 14 resolution using increasing strides. For instance, using a stride of 2 generates 7 × 7 = 49 regions. For object proposals we test a larger range, from 1 up to 2,000 regions, sorted by their "objectness" score. For all three region types, performance quickly increases with the number of regions, and then plateaus off. Using four or less regions yields results below the baseline model, probably because strong sub-sampling at  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Conclusion</head><p>In this paper we made three contributions. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2</head><label>2</label><figDesc>Figure 2.5: At inference, f θ is used to smoothly and invertibly transform p X (x) into an isotropic Gaussian p Z (z). At sampling time, f -1 θ is used for the reverse, thus generating image samples from a simple distribution over latent variables. Notice how the coordinates are transformed in each cases, as visualized by the grids: to displace the points in the desired manner, the model distorts the space. Figure adapted from Dinh et al. [2017].</figDesc><graphic coords="33,121.69,185.59,264.55,82.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure</head><label></label><figDesc>Figure2.7: To predict the red pixel in the first map (top left), the model is allowed to look at the green pixels. When computing the second feature map (top centre) the yellow feature has been computed without looking at the red pixel, and therefore becomes observable. This explains why the first convolutional filter is different from the following ones. The second half of the figure shows how the receptive field of the central feature evolves through the successive layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.13: If p 0 is the uniform distribution on the orange line, and p θ is uniform on the blue line, the optimal transport from p 0 to p θ is γ * and has cost T (γ * ) = |θ|. The cost can be computed despite the fact that p 0 and p θ are disjoint. Figure adapted from Arjovsky et al. [2017].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 2.13. Let us compare D KL , D JS and D W S in this special case. If θ = 0, all measures are zero as p θ = p 0 . For θ = 0, we have D KL (p 0 ||p θ ) = ∞ and D JS (p 0 ||p θ ) = ln 2. Although D JS (p 0 ||p θ ) can be computed, it can not be used for training: it does not depend on θ, and so can not be used to bring p θ closer. But the Wasserstein distance, based on a measure of the proximity of supports can be used: in this case, D W S is easy to compute, and intuitive. Indeed the optimal transport is an horizontal translation by (0, θ), i.e. γ(x|y) = δ(x -yθ 0 )) with constant cost |θ| for every pair (x, y) so D W S (p 0 ||p θ ) = |θ|.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 3.1: Schematic illustration of our auxiliary guided autoregressive variational autoencoder (AGAVE). The objective function has three components: KL divergence regularization, per-pixel reconstruction with the VAE decoder, and autoregressive reconstruction with the pixelCNN decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 . 2 :</head><label>32</label><figDesc>Figure 3.2: Randomly selected samples from unsupervised models trained on 32×32 CIFAR10 images: (a) IAF-VAE Kingma et al. [2016a], (b) pixelCNN++ Salimans et al. [2017a], (c) our hybrid AGAVE model and (d) real CIFAR10 images for comparison. For our model, we show the intermediate high-level representation based on latent variables (left), that conditions the final sample based on the pixelCNN decoder (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 3.3: Effect of the regularization parameter λ. Reconstructions (a) and samples (b) of the VAE decoder (VR and VS, respectively) and corresponding conditional samples from the pixelCNN (PS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 3.4: Bits per dimension of the VAE and AGAVE model, as well as decomposition in KL regularization and reconstruction terms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 .</head><label>3</label><figDesc>Figure3.4 traces the BPD metrics of both the VAE and pixelCNN decoder as a function of λ. We also show the decomposition in regularization and reconstruction terms. By increasing λ, the KL divergence can be pushed closer to zero. As the KL divergence term drops, the reconstruction term for the VAE rapidly increases and the VAE model obtains worse BPD values, stemming from the inability of the VAE to model pixel dependencies other than via the latent variables. The reconstruction term of the pixelCNN decoder also increases with λ, as the amount</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 . 6 :</head><label>36</label><figDesc>Figure 3.6: The first and last columns contain auxilliary reconstructions, images in between are obtained from interpolation of the corresponding latent variables. Odd rows contain auxilliary reconstructions, and even rows contain outputs of the full model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure 3.8: Impact of the color quantization in the auxiliary image. (a) Reconstructions of the VAE decoder for different quantization levels (λ = 8). (b) BPD as a function of the quantization level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 3.9: Samples from models trained with grayscale auxiliary images with 16 color levels (a), 32×32 auxiliary images with 32 color levels (b), and at reduced resolutions of 16×16 (c) and 8×8 pixels (d) with 256 color levels. For each model the auxilliary representation f (z), with z sampled from the prior, is displayed above the corresponding conditional pixelCNN sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 4 . 1 :</head><label>41</label><figDesc>Figure 4.1: Graphical representation of our discriminator architecture. Each convolutional layer of an otherwise classical CNN architecture is modified to include permutation invariant batch statistics, denoted ρ(x). This is repeated at every layer so that the network gradually builds up more complex statistics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 4 . 4 :</head><label>44</label><figDesc>Figure 4.4: Sample images generated by our best model trained on CIFAR10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 4 . 5 :</head><label>45</label><figDesc>Figure 4.5: Samples obtained after 66000 iterations on the celebA dataset. From left to right: (a) Standard GAN (b) BGAN, no batch smoothing. (c) BGAN, batch smoothing γ = 0.5. (d) M-BGAN, batch smoothing γ = 0.5</figDesc><graphic coords="85,92.22,101.03,453.54,131.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 4</head><label>4</label><figDesc>Figure 4.6: Inception score for various versions of BGAN and for batch discrimination Salimans et al. [2016a].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Figure 4.4 displays sample images generated with our M-BGAN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 4 .</head><label>4</label><figDesc>Figure4.6 highlights the training dynamics of each model 3 . On this architecture, M-BGAN heavily outperforms both batch discrimination and our other variants, and yields results similar to, or slightly better thanMiyato et al. [2018b]. The model trained with batch smoothing display results on par with batch discrimination, and much better than without batch smoothing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>Figure 4.7: (a) Sample images generated by our best model trained on STL10. (b) real STL10 images for comparison</figDesc><graphic coords="90,65.66,181.61,340.17,204.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>. 16 )</head><label>16</label><figDesc>Posterior analysis. Through Bayes rule, the posterior expectation yields D * (y) = IE β|y #</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>.25) The distribution Q(β) sums to 1, as IE P(#β) [#β] = Ba a+b . uniform beta prior with a = b = 1 on p simplifies to y | β). Notice that m β (x, x) = y is equivalent to ∀i ∈ {1, ..., B}, x i = y i and β i = 1 or xi = y i and β i = 0. Denote by p 1 (resp. p 2 ) the distribution of real samples (resp. generated samples).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Contents 5 . 1</head><label>51</label><figDesc>Outline of this chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 5.2 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 5.3 Preliminaries on MLE and adversarial training . . . . . . . . . . . . . . 99 5.4 Adaptive Density Estimation and hybrid adversarial-likelihood training . 101 5.5 Experimental evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 5.6 Complementary evaluations . . . . . . . . . . . . . . . . . . . . . . . . . 111 5.7 Qualitative influence of the feature space flexibility . . . . . . . . . . . . . 112 5.8 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>Figure 5.1: An invertible non-linear mapping f ψ maps an image x to a vector f ψ (x) in feature space. f ψ is trained to adapt to modelling assumptions made by a trained density p θ in feature space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 5 . 2 :</head><label>52</label><figDesc>Figure 5.2: Variational inference is used to train a latent variable generative model in feature space. The invertible mapping f ψ maps back to image space, where adversarial training can be performed together with MLE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 5 . 3 :</head><label>53</label><figDesc>Figure 5.3: Our model yields compelling samples while the optimization of likelihood ensures coverage of all modes in the training support and thus sample diversity, here on LSUN churches (64 × 64).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 5 . 4 :</head><label>54</label><figDesc>Figure 5.4: Maximum likelihood training pulls probability mass towards high-density regions of the data distribution, while adversarial training pushes mass out of low-density regions. (Right) Independence assumptions become a source of conflict in a joint training setting, making hybrid training non-trivial.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head></head><label></label><figDesc>-D(G(z))) and maximize ln D(G(z)) by minimizing z assumption yields D KL (p θ,ψ ||p * ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head></head><label></label><figDesc>, stable and quick to train, without spectral normalization. The same convolutional architecture is kept to build a VAE baseline. 1 It produces the mean of a factorizing Gaussian distribution. To ensure a valid density model we add a trainable isotropic variance σ. We train the generator for coverage by optimizing L Q (p θ ), for quality by optimizing L C (p θ ), and for both by optimizing the sum L Q (p θ ) + L C (p θ ). The model using Variational inference with Adaptive Density Estimation (ADE) is refered to as V-ADE. The addition of adversarial training is denoted AV-ADE, and hybrid training with a Gaussian decoder as AV-GDE. The bijective function f ψ2 increases the number of weights by approximately 1.4%, which we compensate for with a slight decrease in the width of the generator for fair comparison. 3 Implementation details can be found in Section A.25: Samples from GAN and VAE baselines, our V-ADE, AV-GDE and AV-ADE models, all trained on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 5 . 6 :</head><label>56</label><figDesc>Figure 5.6: Samples from models trained on CIFAR-10. Our AV-ADE spills less mass on unrealistic samples, owing to adversarial training which controls where off-dataset mass goes.</figDesc><graphic coords="117,239.76,101.03,120.84,120.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 5 . 8 :</head><label>58</label><figDesc>Figure 5.8: Samples from our AV-ADE model trained on STL10, compared to real images. See Figure A.1 for more samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head></head><label></label><figDesc>Figure 5.10: Samples (at resolution 64 × 64) and quantitative performance obtained by our model AV-ADE, on LSUN classes (order of appearance given by the table).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head></head><label></label><figDesc>Figure A.2: Additional Cifar Samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure A. 4 :</head><label>4</label><figDesc>Figure A.4: Samples obtained from our AV-ADE (wg, rd) model trained on LSUN bridges (left), compared to training images (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure A. 5 :</head><label>5</label><figDesc>Figure A.5: Samples obtained from our AV-ADE (wg, rd) model trained on LSUN churches (left), compared to training images (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure A. 6 :</head><label>6</label><figDesc>Figure A.6: Samples obtained from our AV-ADE (wg, rd) model trained on LSUN dining rooms (left), compared to training images (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Figure A. 7 :</head><label>7</label><figDesc>Figure A.7: Samples obtained from our AV-ADE (wg, rd) model trained on LSUN restaurants (left), compared to training images (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Figure A. 8 :</head><label>8</label><figDesc>Figure A.8: Real images and their reconstructions with the AV-ADE models.</figDesc><graphic coords="138,155.17,546.54,260.48,142.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head></head><label></label><figDesc>use a temporal version of the same mechanism to adaptively aggregate visual representations across video frames per word for video captioning. Yeung et al. Yeung et al. use a similar temporal attention model for temporal action localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head></head><label></label><figDesc>Figure B.2: In our attention-based model the conditional joint distribution, p(w, r|h), over words and regions given the current state h is used to generate a word and to pool region descriptors in a convex combination. Both are then fed back to update the state at the next time-step.</figDesc><graphic coords="144,74.32,122.37,95.79,64.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head></head><label></label><figDesc>Figure B.3: For our spatial transformer network attention areas, the localization network regresses affine transformations for all feature map positions in a convolutional manner, which are applied to the anchor boxes that are used to re-sample the feature map.</figDesc><graphic coords="148,155.16,179.55,96.35,96.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>Figure B. 5 :</head><label>5</label><figDesc>Figure B.5: Visualization of the focus of our attention model during sequential word generation for the three different region types: activation grids, object proposals, and spatial transformers. The attention areas are drawn with line widths directly proportional to weights p(r t |h t ).</figDesc><graphic coords="151,78.26,470.14,57.40,57.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head></head><label></label><figDesc>(i)  We presented a novel attention-based model for image captioning. Our model builds upon the recent family of encoder-decoder models. It is based on a score function that consists of three pairwise interactions between the RNN state, image regions, and caption words. (ii) We presented a novel region proposal network to derive image-specific areas of attention for our captioning model. Our region proposal network is based on a convolutional variant of spatial transformer networks, and is trained without bounding-box supervision. (iii) We evaluated our model with three different region types based on CNN activation grids, object proposals, and our region proposal network. Our extensive experimental evaluation shows the importance of all our model components, as well as the importance of image-adaptive attention regions. This work is a first step towards weakly-supervised learning of objects and relations from captions, i.e. short sentences describing the content of an image. Future work will improve these associations for example by training object and relation detectors based on them. We release an open source Theano-Lasagne based implementation of our model: https://github.com/marcopede/AreasOfAttention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="17,148.82,101.03,333.35,214.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Figure 2.10: With adversarial training (left), a sample x is taken from the model, which is penalysed if x has low density under the data distribution. Thus, adversarial training pushes the model to avoid low density regions, and p θ has to drop modes of p * if it is not flexible enough.</figDesc><table><row><cell>Model</cell><cell>Model</cell></row><row><cell>Data</cell><cell>Data</cell></row><row><cell>Mode-dropping</cell><cell>Over-generalization</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, they are considered to correlate well with human judgement of quality. An empirical indicator of that is that state-of-the art likelihood-based models have very low IS/FID scores despite having good coverage, which shows that the low quality of their samples dominates. Conversely, state-of-the art adversarial models have high IS/FID scores, despite suffering from mode dropping (which strongly degrades BPD). So the score is determined mainly by the high quality of their samples.Table2.1: IS and FID scores obtained by the ground truth when progressively dropping parts of the CIFAR10 dataset. The metrics are largely insensitive to removing most of the dataset, unlike BPD. For reference, a reasonable GAN could get around 8 IS and 20 FID.</figDesc><table><row><cell>Split size</cell><cell>IS</cell><cell>FID</cell></row><row><cell cols="3">50k (full) 11.3411 0.00</cell></row><row><cell>40k</cell><cell cols="2">11.3388 0.13</cell></row><row><cell>30k</cell><cell cols="2">11.3515 0.35</cell></row><row><cell>20k</cell><cell cols="2">11.3458 0.79</cell></row><row><cell>10k</cell><cell cols="2">11.3219 2.10</cell></row><row><cell>5k</cell><cell cols="2">11.2108 4.82</cell></row><row><cell>2.5k</cell><cell cols="2">11.0446 10.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Previous approaches to such hybrid models<ref type="bibr" target="#b16">[Chen et al., 2017</ref>, Gulrajani et al., 2017c]  circumvent this problem by restricting the capacity of the autoregressive decoder to prevent degenerate models. In contrast we present a training procedure relying on an auxiliary loss function that controls which information is captured by the latent variables and what is left to the autoregressive decoder. Our approach can leverage arbitrarily powerful autoregressive decoders, achieves state-of-the art quantitative performance among models with latent variables, and generates qualitatively convincing samples.</figDesc><table><row><cell>Chapter 3</cell></row><row><cell>Auxiliary Guided Autoregressive</cell></row><row><cell>Variational Autoencoder</cell></row><row><cell>The material presented in this chapter is based on the paper "Auxiliary Guided Autoregressive</cell></row><row><cell>Variational Autoencoder", Thomas Lucas &amp; Jakob Verbeek, European Conference on Machine</cell></row><row><cell>Learning (ECML) 2018.</cell></row></table><note><p><p>Contents 3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 3.2 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 3.3 Auxiliary guided autoregressive variational autoencoders . . . . . . . . . 50 3.4 Experimental evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 3.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63</p>Successful approaches to generative modelling of high-dimensional data, in particular natural image collections, include latent variable models as discussed in Section 2.5, and autoregressive models as discussed in Section 2.8. The complementary strengths of these approaches, to model global and local image statistics respectively, suggest hybrid models that encode global image structure into latent variables while autoregressively modeling low level detail. The complementary strengths of these approaches, suggest hybrid models that encode global image structure into latent variables while auto-regressively modelling low level detail. However, a naive construction of this type yields models that ignore the latent variables and only rely on autoregressive modelling, a phenomenon known as the information preference property.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>.16) More generally, even if the optimum value C * AGAVE is not attained, taking the latent variables into account is the only way for the AGAVE model to squeeze between C + H(X|Z) and C + H(X). This analysis shows that in our setup it is theoretically always better for the autoregressive model</figDesc><table><row><cell>Model</cell><cell></cell><cell>BPD .|z .|xj&lt;i</cell></row><row><cell>NICE</cell><cell>[Dinh et al., 2015]</cell><cell>4.48</cell></row><row><cell>Conv. DRAW</cell><cell cols="2">[Gregor et al., 2016] ≤ 3.58</cell></row><row><cell>Real NVP</cell><cell>[Dinh et al., 2017]</cell><cell>3.49</cell></row><row><cell>MatNet</cell><cell cols="2">[Bachman, 2016] ≤ 3.24</cell></row><row><cell>PixelCNN</cell><cell>[Oord et al., 2016]</cell><cell>3.14</cell></row><row><cell>VAE-IAF</cell><cell cols="2">[Kingma et al., 2016a] ≤ 3.11</cell></row><row><cell>Gated pixelCNN</cell><cell>[van den Oord et al., 2016c]</cell><cell>3.03</cell></row><row><cell>Pixel-RNN</cell><cell>[Oord et al., 2016]</cell><cell>3.00</cell></row><row><cell>Aux. pixelCNN</cell><cell>[Kolesnikov and Lampert, 2017]</cell><cell>2.98</cell></row><row><cell>Lossy VAE</cell><cell cols="2">[Chen et al., 2017] ≤ 2.95</cell></row><row><cell>AGAVE, λ = 12</cell><cell cols="2">(this paper) ≤ 2.92</cell></row><row><cell>pixCNN++</cell><cell>[Salimans et al., 2017a]</cell><cell>2.92</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>1: Bits per dimension (lower is better) of models on the CIFAR10 test data.to make use of the latent and auxiliary representation it is conditioned on. That is true no matter how expressive the model is. It also shows that in theory our model should learn meaningful latent structure.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>the training of GANs has received a lot of recent attention. For instance, Arjovsky et al. [2017], Gulrajani et al. [2017b] and Miyato et al. [2018b] constrain the Lipschitz constant of the network and show that this stabilizes training and improves performance.</figDesc><table><row><cell cols="19">4.3. ADVERSARIAL LEARNING WITH PERMUTATION-INVARIANT BATCH FEATURES69</cell></row><row><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>= 0.2</cell></row><row><cell>Discriminator loss</cell><cell>0.03 0.06 0.09</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>= 0.2 = 0.5 = 0.3</cell><cell>Generator loss</cell><cell>2.1 2.4 2.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>= 0.5 = 0.3</cell></row><row><cell></cell><cell>0.00</cell><cell>0</cell><cell>700</cell><cell>1400</cell><cell>Iteration</cell><cell>2100</cell><cell>2800</cell><cell>3500</cell><cell>(×100)</cell><cell></cell><cell>1.8</cell><cell>0</cell><cell>700</cell><cell>1400</cell><cell>Iteration</cell><cell>2100</cell><cell>2800</cell><cell>3500</cell><cell>(×100)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Karras et al. [2018]</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="16">achieved impressive results by gradually increasing the resolution of the generated images as</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="16">training progresses. These research directions, orthogonal to our work, can be adapted to work</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="9">with batches of data rather than i.i.d. samples.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Figure 4.2: Effect of batch smoothing with different γ's on the generator and discriminator losses.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 .</head><label>4</label><figDesc>1: Comparison to the state of the art in terms of inception score (IS) and Fréchet inception distance (FID) on the CIFAR10 dataset.</figDesc><table><row><cell>Model</cell><cell>IS ↑</cell><cell>FID ↓</cell></row><row><cell cols="3">WGP Miyato et al. [2018b] 6.68 ± .06 40.2</cell></row><row><cell>GP Miyato et al. [2018b]</cell><cell cols="2">6.93 ± .08 37.7</cell></row><row><cell>SN Miyato et al. [2018b]</cell><cell cols="2">7.42 ± .08 29.3</cell></row><row><cell cols="2">Models with batch statistics</cell><cell></cell></row><row><cell>Salimans et al.</cell><cell cols="2">7.09 ± .08 35.0</cell></row><row><cell>BGAN</cell><cell cols="2">7.05 ± .06 36.5</cell></row><row><cell>M-BGAN</cell><cell cols="2">7.49 ± .06 23.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell>Model</cell><cell>IS FID</cell></row><row><cell>WGP Miyato et al. [2018b]</cell><cell>8.4 55</cell></row><row><cell>M-BGAN</cell><cell>8.7 51</cell></row><row><cell>SN Miyato et al. [2018b]</cell><cell>8.7 47.5</cell></row><row><cell cols="2">SN (Hinge loss) Miyato et al. [2018b] 8.8 43.2</cell></row></table><note><p>2: Comparison to the state of the art in terms of inception score (IS) and Fréchet inception distance (FID) on the STL-10 dataset.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 .</head><label>5</label><figDesc>1: Quantitative results. † : Parameter count decreased by 1.4% to compensate for f ψ .[Square brackets] denote that the value is approximated, see Section 5.5. far from the 8.0 BPD obtained if using a uniform distribution and suggests heavy mode-dropping. An identical generator trained for both quality and coverage, AV-GDE, obtains a sample quality that is in between that of the GAN and the VAE baselines, in line with the analysis in Section 5.4. Samples from the different models in Figure5.5 confirm these quantitative results. Using f ψ and training with L C (p θ ) only, denoted by V-ADE in the table, leads to improved sample quality with IS up from 2.0 to 3.0 and FID down from 171 to 112. Note that this quality is still below the GAN baseline and our AV-GDE model.When f ψ is used with coverage and quality driven training, AV-ADE, we obtain improved IS and FID scores over the GAN baseline, with IS up from 6.8 to 7.1, and FID down from 31.4 to 28.0. The examples shown in the figure confirm the high quality of the samples generated by our AV-ADE model. Our model also achieves a better BPD than the VAE baseline.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5</head><label>5</label><figDesc>Compared to models that train an inference network adversarially, denoted by Hybrid (A), our model shows a substantial improvement in IS from 7.0 to 8.2. Note that these models do not allow likelihood evaluation, thus BPD values are absent.Compared to adversarial models, which are not optimized for support coverage, AV-ADE obtains better FID (17.2 down from 21.7) and similar IS (8.2 for both) compared to SNGAN with residual connections and hinge-loss, despite training on 17% less data than GANs (test split removed). The improvement in FID is likely due to this measure being more sensitive to support coverage than IS. Compared to models optimized with MLE only, we obtain a BPD between 3.5 and 3.7, comparable to 3.5 for Real-NVP demonstrating a good coverage of the support of held-out data. We computed IS and FID scores for MLE based models using publicly released code, with provided parameters (denoted by † in the table) or trained ourselves (denoted by ‡ ). Despite being smaller (for reference Glow has 384 layers VS at most 10 for our deeper generator), our AV-ADE Samples from AV-ADE (wg, rd)Real images Figure 5.7: Samples from our AV-ADE model. Additional samples are given in Section 5.6.2. model generates better samples, e.g., IS up from 5.5 to 8.2 (samples displayed in Figure 5.6), owing to quality driven training controling where the off-dataset mass goes.</figDesc><table><row><cell>Hybrid (L)</cell><cell cols="2">BPD ↓ IS ↑ FID ↓</cell></row><row><cell>AV-ADE (wg, rd)</cell><cell>3.8</cell><cell>8.2 17.2</cell></row><row><cell>AV-ADE (iaf, rd)</cell><cell>3.7</cell><cell>8.1 18.6</cell></row><row><cell>AV-ADE (S2)</cell><cell></cell><cell></cell></row></table><note><p>.2: Model refinements. Brackets [] denote that the values have been estimated using an inference network. (rd) denotes the use of a residual discriminator, (wg) a wide generator, (iaf) the use of inverse-autoregressive flow, and (s2) the use of a hierarchy of two scales in the invertible function. These refinments and upgrades yield consistent improvements. Hybrid (L) in the table, FlowGAN(H) optimizes MLE and an adversarial loss, and FlowGAN(A) is trained adversarially. The AV-ADE model significantly outperforms these two variants both in terms of BPD, from 4.2 to between 3.5 and 3.8, and quality, e.g., IS improves from 5.8 to 8.2.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 5</head><label>5</label><figDesc>Table 5.4: Results on the STL-10 and ImageNet datasets. ‡ denotes models trained and evaluated by us using available source code</figDesc><table><row><cell>.4, which shows that our AV-ADE improves</cell></row></table><note><p><p>Samples from AV-ADE (wg, rd)</p>Real images</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Table A.1: Residual architectures for experiments from Table 5.2</figDesc><table><row><cell></cell><cell>Generator</cell></row><row><cell></cell><cell>conv 3 × 3, 16</cell></row><row><cell></cell><cell>IAF block 32</cell></row><row><cell>conv 3 × 3, 16</cell><cell>IAF block down 64</cell></row><row><cell>ResBlock 32</cell><cell>IAF block down 128</cell></row><row><cell>ResBlock down 64</cell><cell>IAF block down 256</cell></row><row><cell>ResBlock down 128</cell><cell>h ∼ N (0; 1)</cell></row><row><cell>ResBlock down 256</cell><cell>IAF block up 256</cell></row><row><cell>Average pooling</cell><cell>IAF block up 128</cell></row><row><cell>dense 1</cell><cell>IAF block up 64</cell></row><row><cell></cell><cell>IAF block 32</cell></row><row><cell></cell><cell>conv 3 × 3, 3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>p(w t , r t |h t ) ∝ exp s(w t , r t , h t ), (B.4) s(w t , r t , h t ) = w t W θ wh h t + w t W θ wr R r t +r t Rθ rh h t + w t W θ w + r t Rθ r , (B.5)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>r t |h t )r t R = p rh R,(B.6)    where p rh ∈ IR nr stacks all region probabilities at time t. This visual representation is concatenated to the generated word in the feedback signal of the state update, i.e. we replace the update of Eq. (B.2) of the baseline model withh t+1 = g(h t , [w t W v t ] ). (B.7)In Section B.4, we experimentally assess the importance of the different pairwise interactions, and the use of the attention mechanism in the state update.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>See Youtube press statistics at https://www.youtube.com/about/press/, accessed</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2020-07-08 1</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>High, but still probably lower than N which is "very" high.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>The density decreases exponentially with the squared euclidean distance to f θ (z), and most data points x are far from f θ (z)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>Manually specified basis expansion is prohibitively expensive in high-dimension. Kernel methods provide well understood and powerful basis expansions, but decouple learning from feature construction and still require kernel design.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>with respect to p θ , not to θ.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>Assuming a smooth manifold structure -a topology and a smooth atlas are given -and a metric, the geodesic between two points is the path that has the shortest length while staying on the manifold.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_7"><p>The graphs in Figure3.4 and Figure3.8 are based on the bound in Eq. (3.13) to reduce the computational effort.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_8"><p>The generator G could also be modified to produce batches of data, which can help to cover more modes per batch, but this deviates from the objective of learning a density estimator from which we can draw i.i.d. samples.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_9"><p>This was initially a bug that worked.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_10"><p>For readability, a slight smoothing is performed on the curves.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_11"><p>In the VAE model, some intermediate feature maps are treated as conditional latent variables, allowing for hierarchical top-down sampling (see Section A.2). Experimentally, we find that similar top-down sampling is not effective for the GAN model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_12"><p>implemented as a small Real-NVP with 1 scale,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_13"><p>residual blocks, 2 layers per block. 3 This is too small to make a significant difference in experiments.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_14"><p>see the Global Internet Phenomena Report at https://www.sandvine.com/phenomena</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_15"><p>Recall that minimizing the L2 loss is equivalent to maximizing the log-likelihood of a normal distribution</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_16"><p>C. Zitnick and P. Dollár. Edge boxes: locating object proposals from edges. In ECCV, 2014.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Acknowledgment. We thank <rs type="institution">NVIDIA</rs> for donating GPUs used in this research. This work was partially supported by the grants <rs type="funder">ERC Allegro</rs>, <rs type="grantNumber">ANR-16-CE23-0006</rs>, and <rs type="grantNumber">ANR-11-LABX-0025-01</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EF55G48">
					<idno type="grant-number">ANR-16-CE23-0006</idno>
				</org>
				<org type="funding" xml:id="_Xfz8BEW">
					<idno type="grant-number">ANR-11-LABX-0025-01</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Squares Circles</head><p>Gan mixup Gan BGan(γ = 0.3) Gan mixup Gan BGan(γ = 0.3) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Batch smoothing as a regularizer</head><p>A naive approach to sampling mixed batches would be, for each batch index, to pick a datapoint from either real or generated images with probability 1 2 . This is necessarily ill behaved: as the batch size increases, the ratio of training data to generated data in the batch tends to 1 2 by the law of large numbers. Consequently, a discriminator always predicting 1 2 would achieve very low error with large batch sizes, and provide no training signal to the generator. Instead, for each batch we sample a ratio p * from a distribution P on [0, 1], and construct a batch by picking real samples with probability p * and generated samples with probability 1 -p * . This forces the discriminator to predict across an entire range of possible values of p * .</p><p>Formally, suppose we are given a batch of training data x ∈ R B×N and a batch of generated data x ∈ R B×N . To mix x and x, a binary vector β is sampled from B (p) B , a B-dimensional Consider a beta prior distribution Beta(a, b) on the mixing parameter p ∈ [0, 1]. Because the βs are i.i.d, #β i β i is a sufficient stastic and the posterior distribution #β contains all the "interesting" information. It is given by the beta-binomial compound distribution: Now that we have an explicit expression for P(β), we need the value of the optimal discriminator. As in the vanilla case, we can obtain it by differentiating the B-GAN loss.</p><p>Optimal discriminator. Let y = m β (x, x) denote a mixed batch of samples. The discriminator minimizes the KL divergence between D(y) and β, averaged over batches and mixing vectors β, see Equation <ref type="formula">4</ref>.4. This reduces to minimizing the expected cross-entropy. For a given batch and mixing vector β,</p><p>Averaging over batches and mixing vectors,</p><p>From the latter it follows that for any y, the optimal discriminator value D * (y) is</p><p>i.e. the posterior expectation of the fraction of training samples in the batch. This expression of the optimal discriminator is very compact, and reasonable intuitively. However it is too generic: it is an expression over the vectors β and y and hides the fact that in our case the elements are generated in a i.i.d. manner. We now express the loss in terms of each element of the batch, to better understand how the discriminator acts on each y i . This gives more insight on how the generator is trained because the generator is i.i.d. The expressions become less compact, precisely because the mapping from elements y to answer β is not factorized, but it will be managable because of the invariance to permutations. We proceed by applying Bayes' rule.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Complementary evaluations</head><p>In this section we consider other, less standard evaluations. The first is based on the idea of training classifiers using generated data, the second using a precision-recall procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.1">Evaluation using samples as training data for a discriminator</head><p>We evaluate our approach using the two measures recently proposed by <ref type="bibr" target="#b133">[Shmelkov et al., 2018]</ref>. Figure <ref type="figure">5</ref>.12: Evaluation on CIFAR-10 of different architectures of the invertible layers of the model. In <ref type="bibr">Table (a)</ref>, adversarial training is used, while models in <ref type="bibr">Table (b)</ref> are purely maximumlikelihood based. In (c), samples displayed were obtained using VAE models trained with MLE (Table <ref type="table">5</ref>.12b), showing qualitative influence of multi-scale feature space. The models include one without invertible decoder layers, and with NVP layers using one, two and three scales. The samples illustrate the impact of using invertible NVP layers in these auto-encoders. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Summary of contributions</head><p>The work presented in this manuscript revolves around two main goals. The first is to develop models that avoid parametric assumptions, which are ill suited to natural images modelling, while remaining tractable to train. The second is to obtain generative models that can explicitly evaluate both sample quality and diversity of the learned distribution. The second goal is approached from two different angles: one purely adversarial, and one based on hybrid training. We now summarise our contributions and the extent to which these goals have been attained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Going beyond conditional independence</head><p>We have presented two approaches to training generative image models that go beyond the usual conditional independence assumption. The first, presented in Chapter 3, combines a latent variable structure with an autoregressive decoder. Unlike prior approaches to such models, we use a regularization parameter and auxiliary target images to control what is modelled by latent variables and what is left to model for the autoregressive decoder. This framework avoids the information preference property, without constraining the flexibility of the autoregressive component. We obtained quantitative performance on par with the state of the art on CIFAR10 at the time of publication, and compelling samples demonstrating globally coherent structure and fine details. While this construction goes beyond the conditional independence assumption, it does so at the cost of slow, sequential sampling which is incompatible with adversarial setting. We also presented another approach, in Chapter 5, that leverages invertible network layers to relax the conditional pixel independence assumption. It allows for efficient feed-forward sampling, and Appendix A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive density estimation: samples and other results</head><p>This appendix contains additional samples from our models as presented in Chapter 5 as well as architectural details and a detailed training algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Additional Samples</head><p>In this section we provide additional qualitative results on CIFAR10, STL10, LSUN (categories: Bedrooms, Towers, Bridges, Kitchen, Church, Living room, Dining room, Classroom, Conference <ref type="table">room</ref> and<ref type="table">Restaurant) at resolutions 64 × 64</ref> and<ref type="table">128 × 128, ImageNet</ref> and<ref type="table">CelebA</ref>. We report IS/FID scores together with BPD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Additional samples on CIFAR10 and STL10</head><p>Samples from AV-ADE (wg, rd)</p><p>Real images In this section we summarize the training procedure as an algorithm to give an overview of the differents quantities and steps involved. for number of training steps do • Sample m real images {x (1) , . . . , x (m) } from p * , approximated by the dataset.</p><p>• Map the real images to feature space {f (x) (1) , . . . , f (x) (m) } using the invertible transformation f .</p><p>• Encode the feature space vectors using the VAE encoder and get parameters for the posterior q φ (z|f (x)).</p><p>• Sample m latent variable vectors, { ẑ(1) , . . . , ẑ(m) } from the posterior q φ (z|x), and m latent variable vectors { z(1) , . . . , z(m) } from the VAE prior p θ (z)</p><p>• Decode both sets of latent variable vectors using the VAE decoder into the means of conditional Gaussian distributions, {µ( ẑ) (1) , . . . , µ( ẑ) (m) } and {µ( z</p><p>} i≤m and samples in feature space { f (x)</p><p>} i≤m • Map the samples and reconstructions back to image space using the inverse of the invertible transformation f -1 which yields reconstructions { x(i) } i≤m and samples { x(i) } i≤m • Compute L C (p θ ) using ground truth images {x (i) } i≤m and their reconstructions </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Areas of attention for image captioning</head><p>Note. The work presented in this appendix, conducted during my research internship and the first semester of my PhD, is separated from the main body of the manuscript because the subject differs substantially from the other contributions. It approaches the topic of image captioning and involves recurrent networks, attention mechanisms and natural language processing. Image captioning, not unlike image generation, requires high level representations of complex image content.</p><p>We propose "Areas of Attention", a novel attention-based model for automatic image captioning. Our approach models the dependencies between image regions, caption words, and the state of an RNN language model, using three pairwise interactions. In contrast to previous attention-based approaches that associate image regions only to the RNN state, our method allows a direct association between caption words and image regions. During training these associations are inferred from image-level captions, akin to weakly-supervised object detector training. These associations help to improve captioning by localizing the corresponding regions during testing. We also propose and compare different ways of generating attention areas: CNN activation grids, object proposals, and spatial transformers nets applied in a convolutional fashion. Spatial transformers give the best results. They allow for image specific attention areas, and can be trained jointly with the rest of the network. Our attention mechanism and spatial transformer attention areas together yield state-of-the-art results on the MSCOCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Introduction</head><p>Image captioning, i.e. automatically generating natural language image descriptions, is useful for the visually impaired, and for natural language based image search. It is significantly more challenging than classic vision tasks such as object recognition and image classification for two reasons. First, the structured output space of well formed natural language sentences is significantly more challenging to predict over than just a set of class labels. Second, this complex output space allows a finer interpretation of the visual scene, and therefore also requires a more detailed visual analysis of the scene to do well at this task. . Besides implementing our model using attention areas defined over CNN activation grids or object proposals, as used in previous work, we also present a end-to-end trainable convolutional spatial transformer approach to compute image specific attention areas (bottom).</p><p>location, object properties, and their interactions.</p><p>Neural encoder-decoder based approaches, similar to those used in machine translation <ref type="bibr" target="#b139">Sutskever et al. [2014]</ref>, have been found very effective for this task, see e.g. <ref type="bibr" target="#b70">Kiros et al. [2014]</ref>, <ref type="bibr" target="#b95">Mao et al. [2015]</ref>, <ref type="bibr" target="#b152">Vinyals et al. [2015]</ref>. These methods use a convolutional neural network (CNN) to encode the input image into a compact representation. A recurrent neural network (RNN) is used to decode this representation word-by-word into a natural language description of the image. While effective, these models are limited in that the image analysis is (i) static, i.e. does not change over time as the description is produced, and (ii) not spatially localized, i.e. describes the scene as a whole instead of focousing on local aspects relevant to parts of the description. Attention mechanisms can address these limitations by dynamically focusing on different parts of the input as the output sequence is generated. Such mechanisms are effective for a variety of sequential prediction tasks, including machine translation <ref type="bibr" target="#b5">Bahdanau et al. [2015]</ref>, speech recognition <ref type="bibr" target="#b17">Chorowski et al. [2015]</ref>, image synthesis <ref type="bibr" target="#b43">Gregor et al. [2015]</ref>, and image captioning <ref type="bibr" target="#b0">Xu et al. [2015]</ref>. For some tasks the definition of parts of the input to attend to are clear and limited in number: for example the individual words in the source sentence for machine translation. For other tasks with complex inputs, such as image captioning, the notion of parts is less clear. In this work we propose a novel attention model and three different ways to select parts of the image, or areas of attention, for the automatic generation of image captions.</p><p>The first contribution of our work is a new attention mechanism that models the interplay between the RNN state, image region descriptors, and word embedding vectors by means of three pairwise interactions. Previous attention approaches model either only the interaction between image regions and RNN state <ref type="bibr" target="#b56">Jin et al. [2015]</ref>, <ref type="bibr" target="#b0">Xu et al. [2015]</ref>, or the interaction between RNN state vector,</p><p>where w t ∈ {0, 1} nw is a 1-hot coding over the captioning vocabulary of n w words, W is a matrix which contains word embedding vectors as rows, and θ wh maps the word embedding space to the RNN state space. For sake of clarity, we omit the dependence on I in Eq. (B.1) and below.</p><p>We use an RNN based on gated recurrent units (GRU) <ref type="bibr" target="#b18">Chung et al. [2014]</ref>, which are simpler than LSTM units <ref type="bibr" target="#b53">Hochreiter and Schmidhuber [1997]</ref>, while we found them to be at least as effective in preliminary experiments. Abstracting away from the GRU internal gating mechanism (see supplementary material), the state update function is given by a non-linear deterministic function</p><p>2)</p><p>The feedback of w t in the state update makes that w t+1 recursively depends on both φ(I) and the entire sequence of words, w 1:t = (w 1 , . . . , w t ), generated so far.</p><p>During training we minimize the sum of losses induced by pairs of images I m with corresponding captions</p><p>where θ collectively denotes all parameters of the CNN and RNN component. This amounts to approximate maximum likelihood estimation, due to local minima in the loss.</p><p>Once the model is trained, captions for a new image can be generated by sequentially sampling w t ∼ p(w t |h t ), and updating the state h t+1 = g(h t , w t ). Since determining the maximum likelihood sequence is intractable, we resort to beam search if a single high-scoring caption is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.2 Attention for prediction and feedback</head><p>In the baseline model the image is used only to initialize the RNN, assuming that the memory of the recurrent net is sufficient to retain the relevant information of the visual scene. We now extend the baseline model with a mechanism to attend to different image regions as the caption is generated word-by-word. Inspired by weakly supervised object localization methods, we score region-word pairs and aggregate these scores by marginalization to obtain a predictive distribution over the next word in the caption. The advantage is that this model allows words to be associated with specific image region appearances instead of global image representations, which leads to better generalization to recognize familiar scene elements in novel compositions. Importantly, we maintain the word-state interaction in Eq. (B.1) of the baseline model, to ensure temporal coherence in the generated word sequence by recursive conditioning on all previous words. Finally, a region-state interaction term allows the model to highlight and suppress image regions based on their appearance and the state, implementing a dynamic salience mechanism. Table <ref type="table">B</ref>.1: Evaluation of the baseline and our attention model using activation grid regions, including variants with certain components omitted, and word-conditional instead of marginal feedback.</p><p>state. The "activation grid" regions are taken from the last convolutional layer. For the "spatial transformer" regions, we use the penultimate convolutional layer to regress the transformations, which are then applied to convolve a locally transformed version of the same layer. For the "object proposal" regions we max-pool features from the last convolutional layer. Similar to Ren et al.</p><p>[2015], we re-scale the image so that the smaller image dimension is 300 pixels while keeping the original aspect-ratio. When fine-tuning we do not share the parameters of the two CNNs. In all cases, the dimension of the region descriptors is given by the number of channels in the corresponding CNN layer, i.e. d r = 512.</p><p>Captioning vocabulary. We use all 6,325 unique words in the training captions that appear at least 10 times. Words that appear less frequently are replaced by a special OUT-OF-VOCABULARY token, and the end of the caption is marked with a special STOP token. The word embedding vectors of dimension d w = 512 collected in the matrix W are learned along with the RNN parameters.</p><p>Training. We use RNNs with a single layer of d h = 512 GRU units. We found it useful to train our models in two stages. In the first stage, we use pre-trained CNN weights obtained from the ImageNet 2010 dataset <ref type="bibr" target="#b24">Deng et al. [2009]</ref>. In the second stage, we also update the CNN parameters. We use the Adam stochastic gradient descend algorithm <ref type="bibr">Kingma and Ba [2015b]</ref>. To speed-up training, we sub-sample the 14 × 14 convolutional layers to 7 × 7 when using the activation grid and the spatial transformer regions. For proposal regions, each time we process an image we use 50 randomly selected regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.2 Experimental results</head><p>In this section we assess the relative importance of different components of our model, the effectiveness of the different types of attention regions, and the effect of jointly fine-tuning the CNN and RNN components. Finally, we compare our results to the state of the art.</p><p>Attention and visual feedback. In Table <ref type="table">B</ref>.1 we progressively add components of our model to the baseline system. Here we use activation grid regions for our attention model. Adding all components improves the CIDEr score of the baseline, 78.9, by 8.5 points to 87.4. The baseline test-time is sub-optimal for models trained using 7 × 7 or 50 regions. The spatial transformer regions consistently improve over the activation grid ones, demonstrating the effectiveness of the region transformation sub-network. As compared to object proposals, the spatial transformer regions yield better results, while also being computationally more efficient: taking only 18ms to process an image using 7 × 7 regions, as compared to 352ms for 50 proposals which is dominated by 320ms needed to compute the proposals. At 6ms per image, fixed 7 × 7 activation grids are even more efficient, but come with less accurate results. In the remaining experiments, we report performance with the optimal number of regions per method: 1,000 for proposals, and 196 for grids and transformers.</p><p>Joint CNN-RNN fine-tuning. We now consider the effect of jointly fine-tuning the CNN and RNN components. In Table <ref type="table">B</ref>.2 we report the performance with and without fine-tuning for each region type, as well as the baseline performance for reference. All models are significantly improved by the fine-tuning. The baseline improves the most in absolute terms, but its performance remains substantially behind that of our attention models. The two types of image-dependent attention regions improve over fixed activation grids, but the differences between them are reduced after fine-tuning. Spatial transformer regions lead to comparable results as edge-box object proposals, that were designed to align with object boundaries. Spatial transformer regions, however, are more appealing from a modeling perspective since the region module is trainable fully end-to-end and does not rely on an external image processing pipeline, while also being more efficient to compute.</p><p>Visualizing areas of attention. In Figure <ref type="figure">B</ref>.5 we provide a qualitative comparison of the attentive focus using different regions in our model. A larger selection, including failure cases, can be found in the supplementary material. We show the attention weights over the image regions at each point in the generated sentences. For the spatial transformers, we show the transformed anchor boxes. For the activation grid regions, we show the back-projection of a 3 × 3 activation block, which allows for direct comparison with the spatial transformers. Note that in all cases the underlying receptive fields are significantly larger than the depicted areas. For object proposals Method GT Gen. we directly show the edge-box proposals. The images displayed for the object proposals differ slightly from the others, since the high-resolution network used in that case applies a different cropping and scaling scheme. Proposals accurately capture objects, e.g. the elephants and the plane, but in other cases regions for background elements are missing, e.g. for the field and the sky. The spatial transformers tend to focus quite well on relational terms. For example, "standing" focuses on the area around the legs of the elephants in the first image, and "low" on the area between the airplane and the ground in the second image. For the spatial transformers in particular, the focus of attention tends to be stable across meaningful sub-sequences, such as noun phrases (e.g. "A couple of elephants") and verb phrases (e.g. "is flying."). Attention correctness. We follow the approach of Liu et al. <ref type="bibr" target="#b84">Liu et al. [2017]</ref> to quantitatively assess the alignment of attention with image regions corresponding to the generated caption words. Their approach uses the visual entity annotations on the Flickr30k dataset by <ref type="bibr" target="#b108">Plummer et al. Plummer et al. [2015]</ref>. For caption words that are associated with a ground-truth image region, they integrate the attention values over that region. See <ref type="bibr" target="#b84">Liu et al. Liu et al. [2017]</ref> for more details. Following the protocol of Liu et al., we measured the attention correctness of our model (based on spatial transformer regions) on MSCOCO for ground truth and generated sentences. As Liu et al. reported results with a model trained on Flickr30k, for a fairer comparison, we have also trained a model on Flickr30k using the same hyper-parameters and architecture as for MSCOCO. In terms of caption generation the model obtained a CIDEr of 41.3 and a BLEU4 of 22.2. As shown in Table <ref type="table">B</ref>.3, when considering the correctness computed on the ground truth sentences, both our models perform better than Liu et al. using the attention model of <ref type="bibr" target="#b0">Xu et al. Xu et al. [2015]</ref>, and come close to their model trained with additional spatial supervision. However, when evaluating the attention correctness on the generated sentences, our models perform significantly better than those in Liu et al., including those trained with spatial supervision.</p><p>Comparison to the state of the art. We compare our results obtained using the spatial transformer regions to the state of the art in Table <ref type="table">B</ref>.4; we refer to our method as "Areas of Attention". We obtain state-of-the-art results on par with Wu et al. <ref type="bibr" target="#b154">Wu et al. [2016]</ref>. They use a region-based high-level attribute representation instead of a global CNN image descriptor to condition the RNN language model. This approach is complementary to ours. For sake of comparability, we also ensemble our model and compare to ensemble results in the bottom part of Table <ref type="table">B</ref>.4. For our ensemble, we trained using 30K additional validation images on top of the 80K training images, and use a random horizontal flip of the images during training. We use the same 5K validation images and 5K images for reporting as in the other experiments. We obtain state-of-the-art results, on par with <ref type="bibr" target="#b8">Bengio et al. Bengio et al. [2015]</ref>. They used "scheduled</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
		<idno>30.0 24.2 89.6 Bengio et al. Bengio et al. [2015] 30.6 24.3 92.1</idno>
		<editor>Donahue et al. Donahue et al.</editor>
		<imprint>
			<date type="published" when="2015">2015. 2015. 2015. 2015. 2016</date>
		</imprint>
	</monogr>
	<note>soft 24.3 23.9 -. hard 25.0 23.0 -Yang et al. Yang et al. [2016] 29.0 23.7 88.6 Jin. 31 26 94 Areas of Attention 30.7 24.5 93.8 Ensemble methods Vinyals et al. Vinyals et al. [2015] 27.7 23.7 85.5 You et al. You et al. [2016] 30.4 24.3 -Bengio et al. Bengio et al. [2015</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Comparison of our results to the state of the art on the MSCOCO dataset. sampling&quot;, a modified RNN training algorithm that samples from the generated words during training</title>
		<author>
			<persName><forename type="first">B</forename><surname>Table</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>With standard training, as for our results, they report 95.7 CIDEr</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On gradient regularizers for mmd gans</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An architecture for deep, hierarchical generative models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural networks and principal component analysis: Learning from examples without local minima</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An essay towards solving a problem in the doctrine of chances</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society of London</title>
		<imprint>
			<date type="published" when="1763">1763</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning and deep learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Spinger-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Importance weighted autoencoders</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Symmetric variational autoencoder and connections to adversarial learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AISTATS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Variational lossy autoencoder. In ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-fold MIL training for weakly supervised object localization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<title level="m">Efficient video generation on complex datasets. arXiv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Control, Signals and Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989-12">Dec 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Theory and applications of proper scoring rules</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Musio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">NICE: Non-linear independent components estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structured uncertainty prediction networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dorta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simpson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarially learned inference</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.03906</idno>
		<title level="m">Training generative neural networks via maximum mean discrepancy optimization</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">GDPP: Learning diverse generations using determinantal point processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elfeki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno>arxiv.org/pdf/1812.00068</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hierarchical autoregressive image models with auxiliary decoders</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">MADE: Masked autoencoder for distribution estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">DRAW: A recurrent neural network for image generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Towards conceptual compression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Flow-gan: Combining maximum likelihood and adversarial learning in generative models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>AAAI Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>CoRR, abs/1704.00028</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">PixelVAE: A latent variable model for natural images</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Taiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Permutation-equivariant neural networks applied to dynamics prediction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Guttenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Virgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Witkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kanai</surname></persName>
		</author>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Flow++: Improving flow-based generative models with variational dequantization and architecture design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A method for the construction of minimum-redundancy codes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the I.R.E</title>
		<imprint>
			<biblScope unit="page" from="1098" to="1102" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Aligning where to see and what to tell: image caption with region-based attention and scene factorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06272</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">DenseCap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><surname>Glow</surname></persName>
		</author>
		<title level="m">Generative flow with invertible 1x1 convolutions</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Improving variational autoencoders with inverse autoregressive flow</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">PixelCNN models with auxiliary variables for natural image modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName><forename type="first">A</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sonderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Energy-based approaches to representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="https://www.math.ias.edu/files/special_year_workshops/lecun-20191016-ias.pdf" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Microsoft coco: common objects in context. CoRR, 2014b</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">PacGAN: The power of two samples in generative adversarial networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Deformable shape completion with graph convolutional autoencoders</title>
		<author>
			<persName><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Attention correctness in neural image captioning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Dvc: An end-to-end deep video compression framework</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Auxiliary guided autoregressive variational autoencoders</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Auxiliary guided autoregressive variational autoencoder</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECML</title>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Mixed batches and symmetric discriminators for GAN training</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Mixed batches and symmetric discriminators for gan training</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Adaptive density estimation for generative models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Adversarial autoencoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Deep captioning with multimodal recurrent neural networks (m-RNN)</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Neural network processing for multiset data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mcgregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Artificial Neural Networks</title>
		<meeting>the 17th International Conference on Artificial Neural Networks</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Further results in multiset processing with neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mcgregor</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2008.06.020</idno>
		<ptr target="https://doi.org/10.1016/j.neunet.2008.06.020" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="830" to="837" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Generating high fidelity images with subscale pixel networks and multidimensional upscaling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ICLR, 2018a</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1QRgziT" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, 2018b</title>
		<imprint/>
	</monogr>
	<note>accepted as oral presentation</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Curiosity-driven exploration by self-supervised prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Areas of attention for image captioning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>CoRR, abs/1612.00593</idno>
		<ptr target="http://arxiv.org/abs/1612.00593" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Fast generation for convolutional autoregressive models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Preventing posterior collapse with delta-vaes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Generating diverse high-fidelity images with VQ-VAE-2. NeurIPS</title>
		<author>
			<persName><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Freitas. Parallel multiscale autoregressive density estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Variational approaches for auto-encoding generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04987</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">EM Algorithms for PCA and SPCA</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Object-centric spatial pooling for image classification</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Improving gans using optimal transport</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948-07">Jul. 1948, 1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Prediction and entropy of printed english</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="35" to="39" />
			<date type="published" when="1951-01">Jan. 1951, 1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">How good is my GAN?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03585</idno>
		<title level="m">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Ladder variational autoencoders</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Amortised MAP inference for image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Mixtures of probabilistic principal component analysers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="482" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">It takes (only) two: Adversarial generator-encoder networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>AAAI Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>SSW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Conditional image generation with PixelCNN decoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Attention is all you need. 2017b</note>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Optimal transport, old and new</title>
		<author>
			<persName><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Grundlehren der mathematischen Wissenschaften</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kroiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Apps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Alphastar: Mastering the real-time strategy game starcraft ii</note>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">What value do explicit high level concepts have in vision to language problems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Encode, review, and decode: Reviewer module for caption generation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title level="m" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<title level="m">Deep sets. NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno>CoRR, abs/1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno>arXiv, abs/1706.02262</idno>
		<title level="m">InfoVAE: Information maximizing variational autoencoders</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
