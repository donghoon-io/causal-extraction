<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evolving voices based on temporal Poisson factorisation</title>
				<funder ref="#_Cft8UbD">
					<orgName type="full">Jubiläumsfonds of the Oesterreichische Nationalbank (OeNB</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-09-12">12 Sep 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jan</forename><surname>Vávra</surname></persName>
							<email>vavraj@karlin.mff.cuni.cz.</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Hofmarcher</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Probability and Mathematical Statistics</orgName>
								<orgName type="institution">Paris Lodron University of Salzburg</orgName>
								<address>
									<settlement>Salzburg</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">WU Vienna University of Economics and Business</orgName>
								<address>
									<settlement>Wien</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Charles University</orgName>
								<address>
									<addrLine>Sokolovská 83 +420) 731 066 832</addrLine>
									<postCode>186 75</postCode>
									<settlement>Prague Praha 8</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evolving voices based on temporal Poisson factorisation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-09-12">12 Sep 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2410.18486v2[stat.ME]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>auto-regressive process</term>
					<term>Poisson factorisation</term>
					<term>time-varying</term>
					<term>topic model</term>
					<term>variational inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The world is evolving and so is the vocabulary used to discuss topics in speech. Analysing political speech data from more than 30 years requires the use of flexible topic models to uncover the latent topics and their change in prevalence over time as well as the change in the vocabulary of the topics. We propose the temporal Poisson factorisation (TPF) model as an extension to the Poisson factorisation model to model sparse count data matrices obtained based on the bag-of-words assumption from text documents with time stamps. We discuss and empirically compare different model specifications for the time-varying latent variables consisting either of a flexible auto-regressive structure of order one or a random walk. Estimation is based on variational inference where we consider a combination of coordinate ascent updates with automatic differentiation using batching of documents. Suitable variational families are proposed to ease inference. We compare results obtained using independent univariate variational distributions for the time-varying latent variables to those obtained with a multivariate variant. We discuss in detail the results of the TPF model when analysing speeches from 18 sessions in the U.S. Senate (1981Senate ( -2016)).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Probabilistic topic models represent the state-of-the-art method for analysing a corpus of text documents based on the bag-of-words assumption to infer the content based on latent topics <ref type="bibr" target="#b1">(Blei, 2012)</ref>. The vanilla topic model provides a data generating process for the document-term matrix (DTM) which contains the frequency counts of the terms specified by a vocabulary for all documents in the corpus. The co-occurrence structure observed for terms within documents is explained by latent topics which differ in their term intensities and the document intensities which reflect how prevalent topics are for each document.</p><p>Two main approaches are considered in topic models: latent Dirichlet allocation (LDA; <ref type="bibr" target="#b3">Blei et al., 2003)</ref> and Poisson factorisation (PF; <ref type="bibr" target="#b11">Gopalan et al., 2014)</ref>. LDA assumes that the frequency counts for a given document are drawn from a multinomial distribution conditional on the length of the document. The success probabilities of the multinomial distribution correspond to a convex combination of term intensities weighted by the document intensities. Alternatively, PF directly models the counts based on independent Poisson distributions. The rate parameter for a document-term combination is obtained by the convex combination of the document intensities weighted by the term intensities.</p><p>If the documents in the corpus have time stamps as meta-information and the documents are collected for an extensive period of time, it is natural to include time in the modelling of the text documents and infer how the prevalence of topics as well as the content of topics evolve over time. Previous approaches including time information in topic models for text data focused on the LDA approach <ref type="bibr" target="#b2">(Blei and Lafferty, 2006;</ref><ref type="bibr" target="#b23">Wang et al., 2008;</ref><ref type="bibr" target="#b9">Glynn et al., 2019)</ref>. Extensions of the PF model to include a time-varying component were only proposed in the context of a different application where the number of interactions for user-item combinations were modelled with user-item matrices being available for several time points <ref type="bibr" target="#b6">(Charlin et al., 2015;</ref><ref type="bibr" target="#b13">Hosseini et al., 2017)</ref>.</p><p>In this paper, we propose the temporal Poisson factorisation (TPF) model which extends PF to modelling a DTM where the documents have associated time stamps. The TPF model thus represents a dynamic topic model for text data where both the topical content as well as topical prevalence may vary over time. Section 2 provides the model specification together with suitable choices for the prior distributions. To model the time dependence, we consider an auto-regressive specification where the coefficient induces a stationary behaviour of the AR (1) process in addition to the random walk parameterisation usually considered in the context of dynamic topic models. Section 3 describes the use of variational inference (VI) for estimation (with details given in Appendix A). We challenge the use of independent univariate variational families only, as usually pursued in mean-field VI <ref type="bibr" target="#b6">(Charlin et al., 2015)</ref>, and consider also a multivariate specification. In contrast to <ref type="bibr" target="#b2">Blei and Lafferty (2006)</ref> and <ref type="bibr" target="#b23">Wang et al. (2008)</ref> who account for the dependence on the previous time period in the variational distribution, we use a multivariate specification covering all time points. In Section 3 we also present different model selection criteria proposed for VI <ref type="bibr" target="#b17">(McGrory and Titterington, 2007;</ref><ref type="bibr" target="#b24">You et al., 2014)</ref> and suitable post-processing tools to summarise the estimated models and infer insights. Further details, including the results of a simulation study with synthetic data, are provided in the Supplementary Material (sections labelled with "S"). We apply the TPF model to 36 years of political discourse in the U.S. Senate in Section 4. We empirically investigate the performance of the auto-regressive model specification compared to using a random walk and assess the difference in the estimation when using independent univariate variational families compared to a multivariate specification. The suitability of the model to uncover topical variation over time as well as the change in prevalence is assessed. We conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Temporal Poisson factorisation (TPF)</head><p>In this section we define our TPF model. The model is in principle applicable to any timevarying modelling of count matrices where rows are associated with different time periods. For ease of understanding, we present the notation having the empirical application in mind where we apply the TPF model to a corpus of text documents collected over time transformed to a DTM using the bag-of-words assumption.</p><p>The fundamental unit of our model is the document, indexed by d = 1, . . . , D, where D is the total number of documents. Under the bag-of-words assumption, each document is represented by a sparse vector Y d of counts (non-negative integers) of the terms. The set of all considered terms, indexed by v = 1, . . . , V , is referred to as the vocabulary. We assume that the documents are collected over an extended period of time. Consequently, we split time into T time periods and assume in the model that temporal variations occur only between distinct time periods t = 1, . . . , T , but parameters remain constant within each individual time period. A suitable choice of splitting the time into these time periods is crucial to justify this assumption. To fix notation, let t d denote the time period in which document d is written and D t denote the set of all documents written in time period t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model specification</head><p>The TPF model is defined as a generalisation of the classical Poisson factorisation model with K underlying latent topics:</p><formula xml:id="formula_0">y dv ∼ Pois (λ dv ) with rate λ dv = K k=1 λ dkv = K k=1</formula><p>θ dk exp {h kv,t d } .</p><p>(2.1)</p><p>Each element y dv of the DTM Y is assumed to be drawn from a Poisson distribution, independent from the other counts, with a rate λ dv that sums up the contributions from different topics k = 1, . . . , K. The topic-specific Poisson rates λ dkv combine the document intensities θ dk for topic k and the term intensities h kv,t d of topic k at time point t d .</p><p>The document intensities θ dk , θ dk &gt; 0, capture the contribution of each topic k to a given document d. Since the document label d also includes the information about the time period when it was written (t d ), there is no need to explicitly specify θ dk as time-varying. Changes in intensity of a topic over time can be derived by aggregating over the document intensities of documents written in the specific time periods.</p><p>The term intensities h kv,t capture how the importance of a term for a topic varies over time on the logarithmic scale. To link the term intensities across adjacent time periods, we make use of a suitable prior setup mirroring auto-regressive processes. We use the raw, non-centred term intensities h kv,t without explicitly separating out the overall mean. Thus, adjustments related to the average rate are incorporated into the prior structure of h kv,t . This differs from the specification of the dynamic Poisson factorisation (DPF) model <ref type="bibr" target="#b6">(Charlin et al., 2015</ref>, see Section 2.3 for a detailed comparison).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prior specification</head><p>Priors need to be selected for the time-varying term intensities h kv,t and the document intensities θ kd . First, we discuss the prior choices for the time-varying term intensities h kv,t .</p><p>We use a shifted auto-regressive prior for h kv,t parameterised as follows:</p><formula xml:id="formula_1">h t -µ | h t-1 ∼ N δ (h t-1 -µ) , τ -1 and h 1 -µ ∼ N 0, τ -1 , (2.2)</formula><p>where we drop for clarity all indices except time. The sequence is centred around the mean µ.</p><p>The centred sequence then follows an AR (1) process where δ is the auto-regressive coefficient.</p><p>The precision τ controls the variability of the noise of the centred sequence including the first deviation from an artificial observation h 0 -µ = 0.</p><p>We write prior (2.2) in vectorised form using:</p><formula xml:id="formula_2">h ∼ AR T (µ, δ, τ ) ⇐⇒ h ∼ N T µ1, τ -1 ∆ T (δ) -1 , (2.3)</formula><p>where the precision matrix ∆ T (δ) is a tridiagonal matrix with 1 + δ 2 , . . . , 1 + δ 2 , 1 on the main diagonal and -δ on subdiagonal and superdiagonal. In the complete notation including all the indices, this implies for each combination of topic k and term v independently that</p><formula xml:id="formula_3">h kv ∼ AR T (µ kv , δ kv , τ kv ).</formula><p>This specification allows for a flexible auto-regressive coefficient δ and thus differs from the traditional, more rigid random-walk parameterisation (δ = 1), as for example used by Blei and <ref type="bibr" target="#b2">Lafferty (2006)</ref> or <ref type="bibr" target="#b6">Charlin et al. (2015)</ref>. We assign the prior δ kv ∼ N µ δ , σ δ 2 with fixed hyperparameters, enabling direct updates during VI. Alternatively, a truncated version with support restricted to [-1, 1] could be used to enforce stationarity while retaining the simplicity of the update steps. For values of δ close to zero, the sequence of term intensities varies around its mean µ without any time dependence. For values of δ close to 1, the intensity of the current term depends heavily on the value from the previous time period.</p><formula xml:id="formula_4">D V K Y λ θ h 1 → h 2 → • • • → h T ξ µ δ τ exp</formula><p>The means µ kv of the auto-regressive sequences are also given a normal prior to take advantage of conjugacy: µ kv ∼ N µ µ , (σ µ ) 2 . The use of a large variance together with a mean of zero, e.g., N 0, 100 2 , induces a flat prior and reduces the shrinkage of µ towards zero to yield a flexible fit of µ to the sequence h. Finally, the precisions τ are given a gamma prior to preserve conjugacy: τ kv ∼ Γ (a τ , b τ ), in applications we prefer Γ (0.3, 0.3).</p><p>For the document intensities, we follow <ref type="bibr" target="#b10">Gopalan et al. (2015)</ref> and assign a hierarchical gamma prior with additional gamma layer for the document-specific rates to account for differences in document length: θ dk ∼ Γ a θ , ξ d and ξ d ∼ Γ a ξ , b ξ . This hierarchical gamma structure provides us with closed-form updates contrary to the non-conjugate alternative of a log-normal distribution used, e.g., by <ref type="bibr" target="#b6">Charlin et al. (2015)</ref>. As Γ (0.3, 0.3) is widely used in the literature as a prior for θ dk , we use a θ = 0.3 and</p><formula xml:id="formula_5">a ξ = 0.3 with b ξ = 1 to fix E ξ d = 0.3 a priori.</formula><p>Figure <ref type="figure" target="#fig_1">1</ref> depicts the whole structure of the TPF model in plate notation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Comparing TPF to DPF</head><p>The most comparable approach to our model is dynamic Poisson factorisation (DPF) by <ref type="bibr" target="#b6">Charlin et al. (2015)</ref>, which also extends PF by incorporating a time-varying component.</p><p>However, its data structure requirements differ significantly from ours. Specifically, DPF assumes document-term matrices (DTMs) with fixed dimensions across all time periods: y av,t ∼ Pois (λ av,t ) where λ av,t = K k=1 exp g ak,t + g ak + h kv,t + h kv .</p><p>In this case, the first index does not indicate the document d but rather the author a ∈ {1, . . . , A}, in contrast to (2.1) for TPF. Eventually, the index does not have to be an author, but it needs to refer to a quantity that appears once per time period and repeatedly across all time periods. Hence, a single document is not a suitable index.</p><p>This specification allows the intensity θ to also be directly time-varying by using exp{g ak,t + g ak } where g ak captures the author-specific mean topic intensity and g ak,t is a centred random walk (δ = 1) sequence. Different from our specification of the mean through the prior, see (2.1) and (2.3), the mean values g ak and h kv ≈ µ kv are separated from the random walk sequence and directly included within the formula for the Poisson rates. This induces identifiability issues when both g ak,t and g ak are estimated with their own unrestricted location parameter.</p><p>In Section 4, we only apply TPF to the U.S. Senate speeches, because their data structure does not comply with the one required by DPF. Data with a suitable structure could eventually be obtained by aggregating documents from the same author within the same session into one very long document. However, then one could no longer determine the topic composition of individual speeches. Such an aggregation would reduce the dimension of the DTM to AT ≪ D, drastically reducing sparsity. Moreover, the presence of an author within a specific time period depends on that person having a seat in the Senate in this session. This implies that the vast majority of Senators would have only observations for a few sessions and missing observations for sessions where they were not members of the Senate.</p><p>We conduct an empirical comparison of DPF and TPF through a simulation study using synthetic data that aligns with the structural requirements of both modelling approaches.</p><p>The results demonstrate that TPF outperforms DPF in terms of both goodness-of-fit metrics and computational efficiency. The study design and detailed results are presented in Section S.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Inference, model evaluation and interpretation</head><p>We aim to infer the document intensities and the time-varying term intensities. In a Bayesian model, this requires estimating the posterior distribution of the model's parameters and latent variables. While exact analytical solutions for such posterior distributions are typically limited to simpler models, computational methods offer effective approximations. Markov chain Monte Carlo (MCMC) techniques, for instance, producing samples from the posterior are widely regarded as the gold standard due to their strong theoretical guarantees. However, variational methods offer a more computationally efficient and scalable alternative, albeit at the cost of some approximation error. Given the high dimensionality of the latent variables and parameter space, we opted for VI to leverage its efficiency and scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Variational inference</head><p>Variational inference (VI; for a review, see <ref type="bibr" target="#b4">Blei et al., 2017)</ref> frames the problem of posterior approximation as an optimisation problem. Instead of directly finding the posterior distribution, a suitable (variational) family of simply structured known distributions is constructed, and the member of that family that minimises the Kullback-Leibler (KL) divergence from the posterior is used for final inference. Minimising the KL divergence between the variational distribution and the true posterior is equivalent to maximising the so-called evidence lower bound (ELBO), which represents a lower bound on the log-likelihood of the observed data (see, e.g., <ref type="bibr" target="#b4">Blei et al., 2017)</ref>.</p><p>For PF, latent variables Y = ( y dkv ) D,K,V d,k,v=1 are included in the inference which correspond to the word-document counts y dkv assigned to each topic k with the sum across all topics being equal to y dv . Latent variables are kept separately from the vector ζ that combines the model parameters ζ = {θ, ξ, τ , δ, µ, h}. We denote by q ϕ ( Y, ζ) = q ϕ ( Y)q ϕ (ζ) the probability density function of the variational distribution given by a collection of variational parameters ϕ. We then maximise the ELBO function with respect to the variational parameters ϕ:</p><formula xml:id="formula_6">ELBO(ϕ) = E q ϕ log p(Y| Y) + log p( Y|ζ) -log q ϕ ( Y) + log p(ζ) -log q ϕ (ζ) , (3.1)</formula><p>where E q ϕ denotes expectation with respect to the variational family q ϕ .</p><p>A common approach is mean-field VI, which uses independent variational distributions for each variable and parameter. However, for auto-regressive sequences where strong dependencies among parameters are expected, this specification might be too restrictive. Therefore, we consider a variational family consisting of several independent blocks, each tailored to capture different aspects of the model:</p><p>• univariate gamma distributions Γ ϕ shp θdk , ϕ rte θdk , Γ ϕ shp ξd , ϕ rte ξd , Γ ϕ shp τ kv , ϕ rte τ kv for θ dk , ξ d , and τ kv to match the gamma-distributed variables;</p><p>• univariate normal distributions N ϕ loc δkv , ϕ var δkv , N ϕ loc µkv , ϕ var µkv for δ kv and µ kv to model the coefficient and mean of the AR sequence;</p><p>• multivariate normal distributions N T ϕ loc hkv , ϕ cov hkv for h kv of dimension T to handle the auto-regressive sequences with their inherent dependencies;</p><p>• multinomial distributions Mult K y dv , ϕ y dv for the latent variables y dkv if y dv &gt; 0.</p><p>This leads to the following set of variational parameters ϕ:</p><formula xml:id="formula_7">   ϕ shp θ ∈R DK &gt;0 , ϕ rte θ ∈R DK &gt;0 , ϕ shp ξ ∈R D &gt;0 , ϕ rte ξ ∈R D &gt;0 , ϕ shp τ ∈R KV &gt;0 , ϕ rte τ ∈R KV &gt;0 , ϕ loc δ ∈R KV , ϕ var δ ∈R KV &gt;0 , ϕ loc µ ∈R KV , ϕ var µ ∈R KV &gt;0 , ϕ loc h ∈R KV T , ϕ cov h ∈R KV T (T +1)/2 , ϕ y ∈R D(K-1)V &gt;0    .</formula><p>Using the coordinate ascent principle, we update the individual blocks of ϕ to maximise ELBO (ϕ) while keeping the other parameters fixed. Due to our careful choice of prior distributions and variational families, we can update the parameters of the variational families for θ, ξ, τ , µ, δ and Y in a closed-form manner using coordinate ascent variational inference (CAVI) updates. For a detailed discussion of these updates, we refer to Appendix A.</p><p>Only the variational parameters ϕ loc hkv and ϕ cov hkv require a gradient-based approach based on automatic differentiation to obtain updates (ADVI updates; <ref type="bibr" target="#b16">Kucukelbir et al. 2017)</ref>.</p><p>Due to the high memory demands when using objects of dimension D × K × V , we reduce the first dimension to a batch of documents (of size |B| = 512 in applications) and pursue a stochastic gradient approach. One epoch of the algorithm consists of one random split of the corpus of documents into batches and each batch corresponds to one iteration of updates. Given a batch of documents, we first update locally the document-specific parameters for the documents in the current batch. The remaining ones are updated during the same epoch but in a different iteration. The other parameters updated with CAVI are updated globally for each batch. We pursue a similar approach than in stochastic optimisation and move the previous value in direction of the CAVI update with step size ρ s . To satisfy the Robbins-Monro condition we set ρ s = (s + τ ) -κ and use κ = 0.51 and τ = 0, which increases the weight for the new direction for later iterations. Finally, the global ADVI updates for ϕ loc hkv and ϕ cov hkv employ the Adam algorithm <ref type="bibr" target="#b15">(Kingma and Ba, 2015)</ref> with a learning rate of α = 0.01 (cp. <ref type="bibr" target="#b22">Vávra et al., 2024)</ref> based on an (appropriately scaled) approximation of ELBO (ϕ) determined for the documents in the current batch. Unlike in other similar estimation approaches (e.g., <ref type="bibr" target="#b21">Vafa et al., 2020)</ref>, we avoid replacing expectations E q ϕ with Monte Carlo integrals when calculating the ELBO since all terms are tractable in closed form (see Section S.4). Therefore, the only source of randomness for the stochastic gradients stems from the selection of documents in each batch.</p><p>Algorithm 1 sketches how we combine batching, CAVI updates (denoted by •, always constructed based on the most recent parameter values) and ADVI updates. We initialise ϕ θ and ϕ loc µ with (transformed) estimates obtained for non-time-varying Poisson factorisation <ref type="bibr" target="#b10">(Gopalan et al., 2015)</ref> to anchor the meaning of topic labels across different TPF settings and attenuate identifiability problems; other parameters are initialised reasonably at random. Algorithm 1 is run for as many epochs E as needed for the sequence of the approximated ELBO values to converge. More details about the inference algorithm can be found in Appendix A and Sections S.2-S.4. Algorithm 1 also points out the optional steps which can be included to calculate the exact ELBO values (using all documents) and VAIC and VBIC Algorithm 1 Estimation combining CAVI and ADVI.</p><p>1: Input: Y, initial ϕ 0 , E, |B|, κ ∈ (0.5, 1], τ ≥ 0, α, hyperparameter values. 2: Initialise the step counter s := 0.  • Set s := s + 1 and the step size ρ s = (s + τ ) -κ . </p><formula xml:id="formula_8">for ϕ in {ϕ loc µ , ϕ var µ , ϕ loc δ , ϕ var δ , ϕ loc τ , ϕ var τ } do 9: • Update ϕ s := ρ s ϕ + (1 -ρ s )ϕ s-1 . ▷ CAVI global updates 10:</formula><p>end for 11:</p><p>• Approximate ELBO (ϕ) by replacing</p><formula xml:id="formula_9">D d=1 with D |B b | d∈B b in (S.4.2).</formula><p>12:</p><p>• Track gradients of ϕ loc h , ϕ cov h as by-product.</p><p>13:</p><p>• Update ϕ loc h , ϕ cov h using Adam with learning rate α. ▷ ADVI global updates 14:</p><p>end for 15:</p><p>• Evaluate ELBO (ϕ), VAIC, VBIC using (3.1), (3.2), (3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>▷ optional</head><p>16: end for 17: Return: The last values ϕ s .</p><p>for model selection (see Sections 3.2 and S.4). Our implementation using the Tensorflow environment is available in a Github repository (<ref type="url" target="https://github.com/vavrajan/TPF">https://github.com/vavrajan/TPF</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model evaluation</head><p>Model criteria like AIC and BIC are usually used to evaluate and compare statistical models, helping to balance model fit and complexity. Within a Bayesian context, also the deviance information criterion (DIC, <ref type="bibr" target="#b20">Spiegelhalter et al., 2002)</ref> is commonly used to compare different models. It resembles Akaike's AIC since it combines a goodness-of-fit measure with a penalisation for complexity p D which corresponds to the effective number of parameters.</p><p>The DIC is obtained by:</p><formula xml:id="formula_10">DIC = -2 log p(Y| ζ) + 2p D with p D = 2 log p(Y| ζ) -2 E ζ|Y [log p(Y|ζ)] ,</formula><p>where ζ is a Bayesian estimator of model parameters ζ, e.g., the posterior mean.</p><p>A version of DIC for VI has been proposed by <ref type="bibr" target="#b17">McGrory and Titterington (2007)</ref> where the posterior is replaced with its variational approximation:</p><formula xml:id="formula_11">VAIC = -2 log p(Y|ζ ⋆ ) + 2p ⋆ D with p ⋆ D = 2 log p(Y|ζ ⋆ ) -2 E q ϕ [log p(Y|ζ)] ,<label>(3.2)</label></formula><p>where ζ ⋆ = E q ϕ ζ is a vector of variational means (either ϕ loc or ϕ shp /ϕ rte ). Similar to <ref type="bibr" target="#b24">You et al. (2014)</ref>, we will refer to this criterion as the variational Akaike information criterion</p><formula xml:id="formula_12">(VAIC).</formula><p>Motivated by the Laplace approximation of the BIC, <ref type="bibr" target="#b24">You et al. (2014)</ref> propose in the context of Bayesian linear regression models the variational Bayesian information criterion (VBIC) which is of the form <ref type="table" target="#tab_2">Equations (3.2)</ref> and<ref type="table" target="#tab_2">(3.</ref>3) provide the generic expressions of the VAIC and VBIC criteria; the formulations specific to our model are given in Section S.4. In the following, we use the two criteria to compare and evaluate different model and VI specifications.</p><formula xml:id="formula_13">VBIC = -2ELBO (ϕ) + 2 E q ϕ log p(ζ). (3.3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Interpretation</head><p>VI results in estimates for the variational parameters which imply a variational distribution that approximates the posterior distribution of the latent variables and parameters. We obtain point estimates for document intensities and term intensities based on the means induced by the variational distributions of these parameters. In the following, we discuss how to infer the time-varying prevalence of topics, the topical content as well as the consistency of topics over time based on these estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Time-varying prevalence of topics</head><p>We determine the time-varying prevalence of topics to understand how the distribution of topics evolves over time. While our model does not include a parameter to directly capture the temporal evolution of topic prevalence, appropriate post-processing of the model parameters enables this analysis.</p><p>We determine a time period specific topic prevalence by averaging the estimated Poisson rates λ dkv over documents and terms. In particular, VI allows to approximate the posterior mean of λ dkv by</p><formula xml:id="formula_14">E q ϕ λ dkv = E q ϕ [θ dk • exp {h kv,t }] = ϕ shp θdk ϕ rte θdk • exp ϕ loc hkv,t + 1 2 ϕ var hkv,t .</formula><p>We sum over these approximate document-term specific rates for topic k to obtain topic prevalences ψ kt for time period t:</p><formula xml:id="formula_15">ψ kt ∝ d∈Dt V v=1 ϕ shp θdk ϕ rte θdk exp ϕ loc hkv,t + 1 2 ϕ var hkv,t ∝   d∈Dt ϕ shp θdk ϕ rte θdk   V v=1 exp ϕ loc hkv,t + 1 2 ϕ var hkv,t</formula><p>.</p><p>ψ kt are normalised so that they sum up to one over all topics for a given time period t and thus correspond to time period specific topic proportions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Topical content</head><p>The second key aspect in topic models is to assess the content of the topics. The content of a topic is usually characterised by the specific terms that are most representative of this topic. This allows to define the subject matter and themes that the topic covers.</p><p>The time period specific term effects for a topic are captured by h kv,t . Thus an obvious choice for selecting the terms that are most representative of a topic is based on their exponentiated values such that they correspond to the intensities used to obtain the Poisson rates. However, this results in the identification of frequent terms which are not characteristic of a specific topic but are rather prevalent in many topics. To address this common problem when characterising topics, <ref type="bibr" target="#b0">Bischof and Airoldi (2012)</ref> proposed the frequency-exclusivity (FREX) measure. FREX reflects both the popularity of the term as well as its uniqueness across topics and has been used to characterise topics in a range of applications (e.g., <ref type="bibr" target="#b19">Roberts et al., 2016)</ref>.</p><p>In our setting, the frequency and exclusivity measures are derived from the term intensities β kv,t obtained with</p><formula xml:id="formula_16">β kv,t = E q ϕ exp {h kv,t } = exp ϕ loc hkv,t + 1 2 ϕ var hkv,t .</formula><p>In particular, the frequency measure of term v for topic k during time period t is determined by evaluating the empirical distribution function constructed from the term intensities β k•,t (with the bullet representing all values associated with that index) at the corresponding value:</p><formula xml:id="formula_17">FR kt (v) = ECDF β k•,t (β kv,t ) = 1 V V u=1</formula><p>1 (βku,t≤βkv,t) .</p><p>Similarly, we define the exclusivity measure of term v for topic k during time period t:</p><formula xml:id="formula_18">EX kt (v) = ECDF β k•,t β kv,t = 1 V V u=1 1 (β ku,t ≤β kv,t ) ,</formula><p>where β kv,t = β kv,t /( K l=1 β lv,t ) measures how prevalent the term is for topic k compared to all other topics.</p><p>The FREX measure is defined as the reciprocal of the convex combination of the separate reciprocals of the frequency and exclusivity measure:</p><formula xml:id="formula_19">FREX kt (v) = 1 -w FR kt (v) + w EX kt (v) -1</formula><p>, where w ∈ [0, 1] declares the weight on exclusivity. In the following we set w = 0.5. Based on the FREX measure the top terms for a topic and time period can be easily extracted and inspected to characterise and manually label a topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Topic consistency</head><p>In addition to characterising a topic based on the top-10 terms according to some measure of topical content, e.g., FREX kt (v), a quantitative assessment of similarity of different topics at the same time period or the same topic across time periods helps to interpret the results obtained for the TPF model.</p><p>Given two vectors of length V characterising the topical content, a number of metrics were suggested to assess semantic congruence <ref type="bibr" target="#b5">(Bullinaria and Levy, 2007)</ref>. We propose to compare the vectors not as V -dimensional vectors but as V -dimensional probability distributions. Since term prevalence is solely based on the parameter h kv,t , we suggest to use the variational approximation of its posterior to assess congruence. This implies that we need to compare V pairs of independent univariate normal distributions. We opt for the symmetrised KL divergence as dissimilarity measure (which is not a metric as it does not fulfil the triangle inequality) for the following two reasons: (1) the independence of individual distribution pairs yields a sum of dissimilarities across the whole vocabulary and (2) KL divergence is invariant to transformations, i.e., the results do not depend on the choice of the scale. In particular the later aspect differentiates this choice from other popular dissimilarity measures used in this context, such as cosine dissimilarity.</p><p>In general, the symmetrised KL divergence between two (log-)normal distributions is</p><formula xml:id="formula_20">d KL N µ x , σ 2 x , N µ y , σ 2 y = 1 4 σ 2 x -σ 2 y 2 + σ 2 x + σ 2 y (µ x -µ y ) 2 σ 2 x σ 2 y .</formula><p>The dissimilarity of topical content (DTC) between two topic-time period combinations (k 1 , t 1 ), (k 2 , t 2 ) is given by:</p><formula xml:id="formula_21">DTC (k 1 , t 1 |k 2 , t 2 ) = V v=1 d KL N ϕ loc hk 1 v,t 1 , ϕ var hk 1 v,t 1 , N ϕ loc hk 2 v,t 2 , ϕ var hk 2 v,t 2 .</formula><p>When comparing the content between two topics across all time periods, we determine DTC based on two multivariate normal distributions:</p><formula xml:id="formula_22">DTC (k 1 |k 2 ) = V v=1 1 T d KL N T ϕ loc hk 1 v , ϕ cov hk 1 v , N ϕ loc hk 2 v , ϕ cov hk 2 v</formula><p>, where the symmetrised KL divergence</p><formula xml:id="formula_23">d KL (N T (µ x , Σ x ) , N T (µ y , Σ y )) = 1 4 (µ y -µ x ) ⊤ Σ -1 y (µ y -µ x ) + (µ x -µ y ) ⊤ Σ -1 x (µ x -µ y ) + Tr Σ -1 y Σ x + Tr Σ -1 x Σ y -2T</formula><p>reduces to the sum of d KL values determined based on univariate normal distributions only if the covariance matrices are diagonal, with Tr () denoting the trace of a matrix.</p><p>4 U.S. Senate speeches  We apply TPF to a dataset provided by <ref type="bibr" target="#b7">Gentzkow et al. (2018)</ref>. This dataset contains all speeches in the U.S. Senate during the Congress sessions <ref type="bibr">97-114 (1981-2016)</ref>. We aim to conduct an analysis at the speech level, where topic-specific term distributions evolve over time. Our TPF model is well-suited for this purpose, whereas the DPF model, which relies on repeated user-item matrices across time periods, is not applicable in this setting.</p><p>Following <ref type="bibr" target="#b12">Hofmarcher et al. (2025)</ref>, we conducted several pre-processing steps. We restricted our analysis to speeches given by Senators. We removed punctuation and numbers, changed the text to lower-case and eliminated stop words. For tokenisation, we used bigrams. The time periods correspond to the single sessions, yielding T = 18 time periods, each spanning two years. Each time period starts on January 1 of an odd year and ends on December 31 of the following even year. For these time periods the composition of the Senate is rather stable within but changes between due to election results leading to members leaving and entering the Senate. They are also short enough to reflect smooth transitions in vocabulary. We obtained a DTM Y consisting of D = 732 110 documents and a vocabulary with V = 12 791 unique bigrams. According to <ref type="bibr" target="#b8">Gentzkow et al. (2019)</ref> and <ref type="bibr" target="#b12">Hofmarcher et al. (2025)</ref>, assuming K = 25 latent topics is an appropriate choice for this dataset. Section S.6 provides more details on how the documents were transformed to obtain the DTM Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparing different model settings</head><p>We estimate TPF under four settings. We consider two different model specifications: the auto-regressive coefficient δ kv is either kept constant (δ kv = 1, representing a random walk) or allowed to vary (δ kv ∈ R, resulting in 2KV more variational parameters to estimate).</p><p>In addition, we use two versions for VI: the posterior distribution of h kv is approximated either with independent variational distributions (ϕ cov hkv diagonal, involving T KV variational parameters) or with a multivariate normal distribution (ϕ cov hkv general, involving 1 2 T (T + 1)KV variational parameters). This results in the following four settings: (A)</p><formula xml:id="formula_24">δ kv ∈ R, ϕ cov hkv general; (B) δ kv ∈ R, ϕ cov hkv diagonal; (C) δ kv = 1, ϕ cov hkv general; (D) δ kv = 1, ϕ cov hkv diagonal.</formula><p>We iterated Algorithm 1 for E = 101 epochs for each of the four settings. Figure <ref type="figure" target="#fig_6">2</ref> provides the change in ELBO as well as reconstruction values, i.e., the sum of the first three terms in (3.1), over epochs for all four settings. The ELBO curve shows the model's improvement in capturing the data distribution including log-prior and entropy, while the reconstruction curve reflects the model's performance in reconstructing the input data from its latent representation only. Both curves indicate that a reasonable level of convergence has been reached for all four settings after 101 epochs.</p><p>All four combinations are compared using the variational information criteria suggested in   Settings (B) and (D) with diagonal covariance matrix ϕ cov hkv reach lower ELBO values compared to the general specification. However, they result in a better fit with respect to the reconstruction term and also VAIC and VBIC suggest to prefer these settings. The diagonal specification also results in lower computational costs as the average run-times increase substantially when the independence assumption within the variational family is relaxed. Clearly, dealing with non-diagonal elements of ϕ cov hkv considerably slows down the algorithm. However, the inference results are rather comparable. Selected results for the four settings are shown in Section S.7 to indicate that similar insights are gained. Overall, these results suggest that the strongest differences are obtained when inspecting the results based on the DTC between pairs of topics across time points and when comparing models fitted using independent univariate instead of a multivariate distribution.</p><p>Overall, the simplest setting (D) with fixed δ = 1 and diagonal variance matrix ϕ cov hkv reaches the lowest VAIC and VBIC values. The average run-times for the VI algorithm as well as the full evaluation of ELBO and variational criteria are also the lowest for this setting. The next section thus presents the results of setting (D), the random walk model with a simplified diagonal structure for the covariance matrix ϕ cov hkv .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Inspecting results</head><p>In the following, we assess the change of prevalence of each topic and the similarity of topical content over time for one topic and between pairs of topics across time periods. For ease of interpretation we manually assign labels to the K = 25 topics. We inspect the most prevalent terms according to the FLEX measure for each topic-time period combination (e.g., such as shown in Table <ref type="table" target="#tab_2">3</ref> for topic 12) and infer the most prominent terms to characterise a topic. For the resulting labels see Table <ref type="table" target="#tab_1">2</ref>.</p><p>Figure <ref type="figure" target="#fig_7">3</ref> provides a stacked bar plot showing the prevalence of topics over time. For each time period the relative proportion of a topic prevalence ψ kt is determined. Clearly, topics vary in general prevalence over time. Topics 14 and 15 seem to be the most prevalent ones at the beginning. However, while topic 15 remains highly prevalent, the prevalence of topic 14 decreases over time. Inspecting the topical content of these topics indicates that topic 14 is dominated by terms like United States, trade, and foreign relations. Topic 15 is characterised at the beginning by common political rhetoric and formal expressions often used in the U.S.</p><p>Senate, e.g., phrases like distinguished Republican and on the other side of the aisle are typical of respectful address and bipartisan discourse within the legislative context. Over time, however, the focus of topic 15 shifts from these general political expressions to more specific discussions about immigration. Figure <ref type="figure" target="#fig_7">3</ref> also indicates that topic 12 (climate change) and 16 (affordable health care) show increasing prevalence over time. Notably, topic 1, which addresses taxes and business, is the most prevalent topic during the financial crisis. Overall these results indicate that the TPF model captures that the prevalence of topics is time- varying. In particular, the rather smooth changes in prevalence are remarkable given that the model does not explicitly impose any smoothness in this respect.</p><p>To inspect topic consistency, we quantify the dissimilarity of topics for consecutive sessions as well as the dissimilarity of pairs of topics across time periods based on DTC. Figure <ref type="figure" target="#fig_9">4</ref> summarises the results in two heatmaps. Figure <ref type="figure" target="#fig_9">4a</ref> provides the heatmap for the comparison across consecutive sessions with the time periods on the x-axis and the topics on the y-axis and darker colours indicate a stronger discrepancy between the topical content of the current and the previous time period. In particular, the topical content of topic 17 (education) changes considerably in session 110 and then changes back afterwards. Reasons might only be speculated, but this could be due to the implementation of the heavily discussed America COMPETES Act in 2007. For topic 20, a change in topical content is also evident following the appointment of two new judges -Samuel Alito and John Roberts -to the Supreme Court. Here we do not observe a change back which coincides with both judges keeping their mandate for the next sessions. In general, the strongest changes in topical content are observed for topic 15 which transforms from a topic that captures rather stylistic phrases in speech into a topic about immigration. We clearly see that the main change occurs in session 109. This observation is also supported by inspecting the evolution of the top-10 terms (see Table <ref type="table">5</ref> in Section S.7).</p><p>Figure <ref type="figure" target="#fig_9">4b</ref> provides the heatmap for the dissimilarity of pairs of topics across time periods with topics on both axes. Note that the range covered by the dissimilarities between topics, however, is two magnitudes larger than for the dissimilarity within a topic across consec-  utive sessions. This is indicative of the consistency of the topics over time as well as the distinctiveness of pairs of different topics for the same time period. Table <ref type="table" target="#tab_1">2</ref> suggests that topics 11 and 16 are of similar content as both are about health care, as well as topics 20 and 22 which are both about courts. This similarity between these pairs of topics is also reflected in Figure <ref type="figure" target="#fig_9">4b</ref> where the corresponding cells are light grey shaded.</p><p>To indicate how TPF captures differences over time in topic prevalence and topical content, we focus in the following on topic 12 which is about climate change. Obviously this topic increases in popularity over time. Its topical content evolves slowly without any dramatic changes as supported by the light grey shades indicating only slight dissimilarity in Figure <ref type="figure" target="#fig_9">4a</ref>.</p><p>Table <ref type="table" target="#tab_2">3</ref> provides the evolution of the top-10 terms over time determined based on the FREX measure with equal weight on frequency and exclusivity. In the 80's the topic was mainly driven by acid rain, then clean air act took over a more dominant position. Around 2000 foreign oil was heavily discussed and from 2011 onwards, the keystone xl pipeline project also began to receive significant attention. As one would expect, topic 5 about natural resources, national parks, water conservation funds, etc. is the closest topic in terms of DTC, see Figure <ref type="figure" target="#fig_9">4b</ref>.</p><p>To provide more details on how the model captures the change of the term intensities over time, we inspect the evolution of ϕ loc hkv and FREX over time for topic 12 (climate change) based on six terms which are important when discussing climate change and include for comparison four terms which have no obvious relation with climate change. Figure <ref type="figure" target="#fig_11">5a</ref> shows   <ref type="bibr">97: 1981-1982 98: 1983-1984 99: 1985-1986 100: 1987-1988 101: 1989-1990 102: 1991-1992</ref>  that the relevant terms acid rain, climate change, global warming and oil gas are at the top with FREX values close to 1. The terms carbon pollution and keystone xl which are only relevant during the last sessions have lower values at the beginning but also reach values close to 1 at the end of the observation period. Still these two terms have FREX values considerably higher than zero for the first sessions, presumably because of the fitted random walk process which enforces a smooth transition to the high values in later sessions. This smooth step-by-step transition for terms essentially absent in earlier sessions is also clearly visible in Figure <ref type="figure" target="#fig_11">5b</ref>. The term web site should not be present in the 80's and early 90's. However, its later partial importance for this topic forces h 12 web site to already have higher values in these early time periods than completely insignificant terms to avoid huge steps in the sequence (and thus reduce its variability). This is also reflected in the FREX measure values which are around 0.8 even though this term could not have been used in that time period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our TPF model extends the PF model for high-dimensional sparse matrices of count data taking the time information of the observations into account. The time-varying term intensities of the latent topics are modelled using a non-centred auto-regressive formulation with the auto-regressive parameter either being estimated or potentially fixed to one to induce a random walk. The careful choice of priors enables the estimation using VI based on mean-field inference where CAVI and ADVI update steps are combined with batching of the observations.</p><p>We applied the model to a dataset covering decades of intercourse in the U.S. Senate to investigate the change in topic prevalence as well as the consistency and variation of topical content over time. We compared results using different model specification settings where we contrast the use of the random walk with the more flexible auto-regressive specification and investigated the use of a multivariate variational family to relax the assumption of independence for the temporal sequence.</p><p>Results justify the general preference of the random walk specification and do not provide evidence for the superiority of the more complicated approaches involving also the multivariate VI approach, which was the only case we compared to the completely independent specification. To obtain additional insights into the benefits of taking dependence into account and using a block structure, one might employ a multivariate normal variational family with tridiagonal precision matrix (ϕ cov hkv ) -1 that should be flexible enough to capture the essence of the AR (1) sequence. This could represent a suitable compromise between faster evaluation and a better posterior approximation. This work offers several avenues for further development and extension. One potential direction is adopting a more flexible model specification. Our current framework enforces a degree of smoothness in temporal changes, which may limit its ability to capture rapidly emerging terms. To address this, model extensions-such as incorporating latent change point models-could be explored. Additionally, a time-varying prior structure on the document intensities θ dk could be introduced to explicitly model their temporal evolution.</p><p>In the model evaluations and comparisons, we focused on the use of variational in-sample criteria. Alternatively, within a Bayesian context, one could also consider out-of-sample criteria based on the predictive posterior. Such an approach would require inserting missing values to create an out-of-sample data. In the context of topic models where documents are available over time, the insertion of missing values is not straightforward, but different ways are possible. One could either omit a set of time points by omitting all documents from these time points, single documents from all time points or even single cells in the DTM. This ambiguity suggests that future work could focus on a detailed analysis of using out-of-sample criteria for model evaluation in the context of the TPF model.</p><p>In addition, future work could also rigorously investigate identifiability issues specific to the TPF model building on results provided by <ref type="bibr" target="#b14">Ke et al. (2024)</ref> for LDA models where they suggest a robust prior structure to ensure identifiability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A CAVI Updates</head><p>In this section, we provide details on all CAVI updates. The updated variational parameters are denoted by ϕ • , where • is a placeholder for the parameter name and necessary indices depending on context. The only variational parameters, where no CAVI updates are available, are the variational parameters ϕ loc hkv and ϕ cov hkv for h kv . This is due to the inclusion of the corresponding parameters in the Poisson rates after exponentiation. <ref type="bibr" target="#b4">Blei et al. (2017)</ref> provide a general approach to find CAVI updates, which we follow. In addition, some of our updates are inspired by <ref type="bibr" target="#b10">Gopalan et al. (2015)</ref> who introduce auxiliary counts y dkv corresponding to latent variables and summing up to y dv over topics. The multinomial distribution is used as variational distribution for these counts with the variational parameters (proportions) ϕ y dkv summing up to one over topics. More details can be found in Section S.2. We update them as follows:</p><formula xml:id="formula_25">ϕ y dkv ∝ exp ψ(ϕ shp θdk ) -log ϕ rte θdk + ϕ loc hkv,t d (A.1)</formula><p>where ψ() denotes the digamma function. Updates of the variational parameters for θ and related auxiliary variables resemble the ones by <ref type="bibr" target="#b10">Gopalan et al. (2015)</ref>:</p><formula xml:id="formula_26">ϕ shp θdk = a θ + V v=1 y dv ϕ y dkv and ϕ rte θdk = ϕ shp ξd ϕ rte ξd + V v=1 exp ϕ loc hkv,t d + 1 2 ϕ var hkv,t d , ϕ shp ξd = a ξ + Ka θ and ϕ rte ξd = b ξ + K k=1 ϕ shp θdk ϕ rte θdk .</formula><p>Under a batching mechanism, we update only the (local) parameters for documents d in the current batch.</p><p>We continue with the updates for precision parameters τ kv . The variational shape parameters have a constant update that can be set from the very beginning: ϕ shp τ kv = a τ + T 2 . However, the variational rate parameters have to be updated with every iteration of the algorithm:</p><formula xml:id="formula_27">ϕ rte τ kv = b τ + 1 2 1 + E q ϕ (δ kv ) 2 T -1 t=1 E q ϕ (h kv,t -µ kv ) 2 + E q ϕ (h kv,T -µ kv ) 2 - -2ϕ loc δkv T t=2 E q ϕ h kv,t -µ kv h kv,t-1 -µ kv ,</formula><p>where E q ϕ (δ kv ) 2 = ϕ loc δkv 2 + ϕ var δkv . The updates for the auto-regressive coefficients δ kv are obtainable as they appear only in combinable quadratic forms. In particular,</p><formula xml:id="formula_28">ϕ var δkv = 1 (σ δ ) 2 + ϕ shp τ kv ϕ rte τ kv T -1 t=1 E q ϕ (h kv,t -µ kv ) 2 -1 , ϕ loc δkv = ϕ var δkv • µ δ (σ δ ) 2 + ϕ shp τ kv ϕ rte τ kv T t=2 E q ϕ (h kv,t -µ kv ) (h kv,t-1 -µ kv ) ,</formula><p>where (stripped of indices) we abbreviate</p><formula xml:id="formula_29">E q ϕ (h t -µ) 2 = ϕ var h,t + (ϕ loc h,t -ϕ loc µ ) 2 + ϕ var µ , E q ϕ (h t -µ) (h t-1 -µ) = ϕ cov h,t,t-1 + (ϕ loc h,t -ϕ loc µ )(ϕ loc h,t-1 -ϕ loc µ ) + ϕ var µ .</formula><p>The updates for the auto-regressive means µ kv have the following form:</p><formula xml:id="formula_30">ϕ var µkv = 1 (σ µ ) 2 + ϕ shp τ kv ϕ rte τ kv 1 + (T -1)(1 -2ϕ loc δkv + E q ϕ (δ kv ) 2 ) -1 , ϕ loc µkv = ϕ var µkv • µ µ (σ µ ) 2 + ϕ shp τ kv ϕ rte τ kv µ , where µ = 1 + E q ϕ (δ kv ) 2 T -1 t=1 ϕ loc hkv,t + ϕ loc hkv,T -ϕ loc δkv T t=2 ϕ loc hkv,t-1 + ϕ loc hkv,t .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>This section provides more details complementing the derivations and results in the main paper "Evolving voices based on temporal Poisson factorisation". Each section within this section starts with the letter "S" for Supplement to distinguish the section numbering within this document and the original paper.</p><p>Section S.1 provides an overview on the notation used within our main paper and briefly summarises the hierarchy of the proposed temporal Poisson factorisation (TPF) model. Section S.2 introduces the latent auxiliary counts that decompose the observed counts into counts for each topic. These variables allow for straightforward updates within our algorithm. Some of the updates require computation of variational means of quadratic forms which are not trivial under auto-regressive prior (see Section S.3). The ELBO function consists of contributions from the model based on the auxiliary counts, the prior distributions and the entropies of the variational families. Therefore, Section S.4 writes down its full extent while rearranging the terms so that is clearer to see where the CAVI updates (listed in the Appendix of the main paper) come from. Section S.5 outlines the design and provides the results of a simulation study where the main focus is the identification of the true value of the auto-regressive coefficient δ under 7 different models, including the competing Dynamic Poisson Factorisation (DPF) model that requires a certain data structure. Models are compared in terms of variational information criteria and computational demands. Section S.6 provides more information on the pre-processing of the U.S. Senate speeches. The final Section S.7 provides additional plots and a table complementing the empirical results included in the main paper. In particular, we inspect the same plots included in the main paper for all four models to demonstrate that in fact similar conclusions are drawn with only marginal differences across the four choices.</p><p>S.1 TPF model specification </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.1.2 Hierarchical Structure</head><p>The TPF model assumes the following data generating process for the document-term matrix Y:</p><formula xml:id="formula_31">y dv ∼ Pois (λ dv ) with rate λ dv = K k=1 λ dkv = K k=1 θ dk exp {h kv,t d } . (S.1.1)</formula><p>We assume the following hierarchical prior for the document intensities θ dk :</p><formula xml:id="formula_32">θ dk ∼ Γ a θ , ξ d and ξ d ∼ Γ a ξ , b ξ (S.1.2)</formula><p>with document-specific rates ξ d <ref type="bibr" target="#b10">(Gopalan et al., 2015)</ref>.</p><p>We use a shifted auto-regressive prior for h kv,t , parameterised in the following way:</p><formula xml:id="formula_33">h t -µ | h t-1 ∼ N δ (h t-1 -µ) , τ -1 and h 1 -µ ∼ N 0, τ -1 , (S.1.3)</formula><p>where we drop for clarity all other indices except time. The sequence is centred around the mean µ. The centred sequence then follows an AR (1) process where δ is the auto-regressive coefficient. The precision τ controls the variability of the innovations added to the centred sequence including the first innovation from an artificially introduced observation h 0 -µ = 0.</p><p>We write this prior in vectorised form using:</p><formula xml:id="formula_34">h ∼ AR T (µ, δ, τ ) ⇐⇒ h ∼ N T µ1, τ -1 ∆ T (δ) -1 , (S.1.4)</formula><p>where the precision matrix ∆ T (δ) is a tridiagonal matrix:</p><formula xml:id="formula_35">∆ T (δ) =        1 + δ 2 -δ 0 . . . 0 0 -δ 1 + δ 2 -δ . . . 0 0 0 -δ 1 + δ 2 . . . 0 0 . . . . . . . . . . . . . . . . . . 0 0 0 . . . 1 + δ 2 -δ 0 0 0 . . . -δ 1        . (S.1.5)</formula><p>Including the indices, we assume h kv ∼ AR T (µ kv , δ kv , τ kv ).</p><p>For the parameters of the auto-regressive sequence, we assume</p><formula xml:id="formula_36">µ kv ∼ N µ µ , (σ µ ) 2 , δ kv ∼ N µ δ , σ δ 2 , τ kv ∼ Γ (a τ , b τ ) . (S.1.6)</formula><p>Moreover, in the simulation study and the empirical application, we also consider a deterministic distribution for δ kv (δ kv = 1, the random walk), and we include in the simulation study a truncated normal distribution where the support is restricted to the interval [-1, 1] (to ensure stationarity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.2 Latent variables</head><p>Under (S.1.1), the contribution E q ϕ log p(Y|ζ) to ELBO (ϕ) is given by</p><formula xml:id="formula_37">E q ϕ log p(Y|ζ) = D d=1 V v=1 y dv E q ϕ log K k=1 λ dkv - K k=1 E q ϕ λ dkv -log (y dv !) ,</formula><p>which is difficult to evaluate due to E q ϕ log K k=1</p><p>.</p><p>We follow <ref type="bibr" target="#b10">Gopalan et al. (2015)</ref> to overcome this issue and introduce latent variables by decomposing the word counts y dv into auxiliary individual contributions to each of the topics y dkv . This trick eliminates the sum over the topics in the Poisson rates λ dv = K k=1 λ dkv and leads to updates in closed form. In particular, this corresponds to assume the following data generating process for the latent variables: <ref type="bibr">d,k,v=1</ref> represents new latent variables, for which a variational family is set up. For y dv &gt; 0, the multinomial distribution Mult K y dv , ϕ y dv is a suitable option for y dv where the variational parameters 0 &lt; ϕ y dkv ≤ 1 satisfy ϕ y d1v + • • • + ϕ y dKv = 1. Under y dv = 0, we immediately obtain y dkv = 0.</p><formula xml:id="formula_38">y dkv |ζ ∼ Pois (λ dkv ) with y dv = K k=1 y dkv . (S.2.1) Y = ( y dkv ) D,K,V</formula><p>Auxiliary counts contribute to ELBO (ϕ) with</p><formula xml:id="formula_39">E q ϕ log p(Y| Y) + E q ϕ log p( Y|ζ) -E q ϕ log q ϕ ( Y).</formula><p>Notice that Y| Y is deterministic, hence, its contribution to ELBO(ϕ) is zero. The other two combine into</p><formula xml:id="formula_40">E q ϕ log p( Y|ζ) -E q ϕ log q ϕ ( Y) = D d=1 V v=1 y dv K k=1 ϕ y dkv E q ϕ log λ dkv -log ϕ y dkv - K k=1 E q ϕ λ dkv -log (y dv !) (S.2.2) (A.1) = D d=1 V v=1 y dv c dv + log K k=1 exp E q ϕ log λ dkv -c dv - K k=1 E q ϕ λ dkv -log (y dv !) ,</formula><p>where we inserted the CAVI updates ϕ y for ϕ y (A.1) to simplify the computation and a constant c dv to ensure numerical stability. In particular, we use c dv = max k=1,...,K E q ϕ log λ dkv .</p><p>Inserting the moments of the gamma and the log-normal distribution, we obtain</p><formula xml:id="formula_41">E q ϕ log λ dkv = ψ(ϕ shp θdk ) -log(ϕ rte θdk ) + ϕ loc hkv,t d , E q ϕ λ dkv = ϕ shp θdk ϕ rte θdk exp ϕ loc hkv,t d + 1 2 ϕ var hkv,t d .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.3 Variational mean of a quadratic form</head><p>Variational means of gamma and log-normal families are trivial to obtain. However, to evaluate the ELBO efficiently or to compute CAVI updates we also need the variational means of quadratic forms which may not be obvious to obtain at first sight. For scalar random variables, the result is trivial using Lemma S.3.1. For a quadratic form involving random vectors and a random matrix, the result is obtained using Lemma S.3.2.</p><p>Lemma S.3.1 Let the scalar random variables X and Y be independent and</p><formula xml:id="formula_42">E X 2 , E Y 2 &lt; ∞, then E (X -Y ) 2 = var X + (E X -E Y ) 2 + var Y .</formula><p>Lemma S.3.2 Let the random vectors X and Y have finite second moments and let the random matrix ∆ have mean E ∆. Then, under the independence of X, Y and ∆, the quadratic form E(X -Y ) ⊤ ∆(X -Y ) can be evaluated as</p><formula xml:id="formula_43">Tr (E ∆ cov X) + (E X -E Y ) ⊤ E ∆ (E X -E Y ) + Tr (E ∆ cov Y ) .</formula><p>The proofs of both lemmas are straightforward to derive using some basic statistics and probability results involving matrices.</p><p>Using Lemma S.3.2 we derive the contribution of h to the log-prior of ELBO (ϕ) under the multivariate normal variational family. Focusing on a single pair of k and v we obtain from (S.1.4):</p><formula xml:id="formula_44">E q ϕ (h -µ1) ⊤ ∆ T (δ) (h -µ1) = Tr E q ϕ ∆ T (δ) ϕ cov h + ϕ loc h -1ϕ loc µ ⊤ E q ϕ ∆ T (δ) ϕ loc h -1ϕ loc µ + ϕ var µ 1 ⊤ E q ϕ ∆ T (δ)1, (S.3.1)</formula><p>where we drop indices k, v for ease of notation.</p><p>Matrix ∆ T (δ) is tridiagonal (S.1.5) and allows for the following decomposition:</p><formula xml:id="formula_45">∆ T (δ) = I T + δ 2 I T,0 -δ (I T,-1 + I T,1 ) ,</formula><p>where I T is the identity matrix, I T,0 = diag 0 (1, . . . , 1) = diag(1, . . . , 1, 0) is the identity matrix where the last element is zero, I T,-1 = diag(1, . . . , 1) is the subdiagonal and</p><formula xml:id="formula_46">I T,1 = diag(1, . . . , 1</formula><p>) is the superdiagonal. Then, E q ϕ ∆ T (δ) is also a tridiagonal matrix with analogous decomposition:</p><formula xml:id="formula_47">E q ϕ ∆ T (δ) = I T + ϕ loc δ 2 + ϕ var δ I T,0 -ϕ loc δ (I T,-1 + I T,1 ) ,</formula><p>where by Lemma S.3.1, E q ϕ δ 2 = ϕ loc δ 2 + ϕ var δ . If we denote by Tr k (A) := Tr(I T,k A), k ∈ {-1, 0, 1}, i.e., either the sum of the subdiagonal elements, the diagonal elements except the last element or the superdiagonal elements of the T × T matrix A, then we can abbreviate this as:</p><formula xml:id="formula_48">Tr E q ϕ ∆ T (δ) A = Tr(A) + ϕ loc δ 2 + ϕ var δ Tr 0 (A) -ϕ loc δ Tr -1 (A) + Tr 1 (A) .</formula><p>For a symmetric matrix A, e.g. ϕ cov h , 11 ⊤ , . . ., this reduces to</p><formula xml:id="formula_49">Tr E q ϕ ∆ T (δ) A = Tr(A) + ϕ loc δ 2 + ϕ var δ Tr 0 (A) -2ϕ loc δ Tr 1 (A),</formula><p>which helps us evaluate (S.3.1) more efficiently.</p><p>The results hold even under the random walk setting (δ = 1), where the variational family is deterministic with ϕ loc δ = 1 and ϕ var δ = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.4 Evaluating the ELBO</head><p>Inference is based on ELBO (ϕ) where the components can be decomposed into:</p><formula xml:id="formula_50">ELBO (ϕ) = E q ϕ log p(Y| Y) + E q ϕ log p( Y|ζ) -E q ϕ log q ϕ ( Y) reconstruction + + E q ϕ log p(ζ) log-prior + (-E q ϕ log q ϕ (ζ)) entropy . (S.4.1)</formula><p>The sum of the first three terms is referred to as reconstruction (red). The remaining terms are called log-prior (blue) and entropy (olive).</p><p>For evaluating ELBO (ϕ) the entropies of the variational families are required. In the following we exemplify them for each parametric family for one set of parameters:</p><p>• Γ ϕ shp θdk , ϕ rte θdk :</p><p>-E q ϕ log q ϕ (θ dk ) = 1 -ϕ shp θdk ψ(ϕ shp θdk ) -log ϕ rte θdk + ϕ shp θdk + log Γ ϕ shp θdk ;</p><p>• N ϕ loc δkv , ϕ var δkv :</p><p>-E q ϕ log q ϕ (δ kv ) = log ϕ scl δkv + 1 2 log(2πe);</p><p>• N T ϕ loc hkv , ϕ cov hkv :</p><p>-</p><formula xml:id="formula_51">E q ϕ log q ϕ (h kv ) = 1 2 log |ϕ cov hkv | + T 2 log(2πe),</formula><p>where ψ(•) is the digamma function.</p><p>We now present the full evaluation of the ELBO (ϕ). To easily see where some of the CAVI updates come from, we arrange all contributions in the following way: </p><formula xml:id="formula_52">ELBO (ϕ) =</formula><formula xml:id="formula_53">a τ + T 2 -ϕ shp τ kv -b τ ϕ shp τ kv ϕ rte τ kv + + K k=1 V v=1 ϕ shp τ kv + log Γ ϕ shp τ kv -ϕ shp τ kv log ϕ rte τ kv - - 1 2 (σ µ ) 2 K k=1 V v=1 ϕ loc µkv -µ µ 2 + ϕ var µkv + K k=1 V v=1 log ϕ scl µkv - - 1 2 (σ δ ) 2 K k=1 V v=1 ϕ loc δkv -µ δ 2 + ϕ var δkv + K k=1 V v=1 log ϕ scl δkv + + D d=1 K k=1 V v=1 y dv ϕ y dkv ϕ loc hkv,t d - ϕ shp θdk ϕ rte θdk exp ϕ loc hkv,t d + 1 2 ϕ var hkv,t d - - 1 2 K k=1 V v=1 ϕ shp τ kv ϕ rte τ kv E q ϕ (h kv -1µ kv ) ⊤ ∆ T (δ kv ) (h kv -1µ kv ) + + 1 2 K k=1 V v=1 log |ϕ cov hkv | + c, (S.4.2)</formula><p>where c is an additive constant.</p><p>Note that only the last three rows (where (S.3.1) is applied) depend on variational parameters ϕ loc h and ϕ cov h . Since only gradients with respect to these parameters are desired, other terms could be omitted from evaluation to speed up computation. However, the rest (including constant c) is still needed for ELBO (ϕ) and variational criteria.</p><p>The variational criteria are, in general, defined as</p><formula xml:id="formula_54">VAIC = -2 log p(Y|ζ ⋆ ) + 2p ⋆ D with p ⋆ D = 2 log p(Y|ζ ⋆ ) -2 E q ϕ [log p(Y|ζ)] , VBIC = -2ELBO (ϕ) + 2 E q ϕ log p(ζ),</formula><p>where ζ ⋆ = E q ϕ ζ is a vector of variational means. Determining log p(Y|ζ ⋆ ) is straightforward for the Poisson distribution (S.1.1). Without the auxiliary latent counts Y, evaluating E q ϕ [log p(Y|ζ)] is complicated and we thus replace it with the reconstruction as suggested in Section S.2. Rearranging the terms, we obtain: VAIC = 2 log p(Y|ζ ⋆ ) -4reconstruction, VBIC = -2reconstruction -2entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.5 Simulation study</head><p>To compare different TPF settings as well as TPF to DPF, we designed the following simulation study. For each time period t ∈ {1, . . . , T }, where T = 10 or T = 20, we generated A = 1 000 documents (D = AT ) so that the data format is suitable for fitting both models, TPF and DPF. We create a synthetic vocabulary of size V = 500 and K = 6 latent topics. The counts are generated via TPF (S.1.1), where θ dk are specific to each document and are not tied to the document with the same index in the next time period (unlike DPF).</p><p>For simplicity, we generate θ dk ∼ Unif{0.8, 0.9, 1.0, 1.1, 1.2} to avoid extremely low or high values. By contrast, h kv,t are generated with wide variability. We fix τ kv = τ = 10 and δ kv = δ for all topic-word combinations kv. The study focuses on the performance regarding the identification of the true value of δ, where we consider three scenarios: δ = 0 (random noise), δ = 0.5 (stationary auto-regressive sequence) and δ = 1 (random walk). We define the auto-regressive sequence h kv,t = µ kv + h v,t where µ kv has a certain structure and the evolution in time h v,t ∼ AR T (0, δ, τ ) is the same across topics. In this way, we can create an interesting link between a word and the topic it affiliates with and how the word use evolves over time.</p><p>First, each word v is assigned T randomly generated vowels {a, e, i, o, u}. We sample them from a distribution where one random vowel has probability 5 9 and all others have the probability 1 9 . This ensures that the frequency of one of the vowels in most cases dominates the other vowels. We then align each topic with a certain vowel: k = 1 ↔ a, . . . , k = 5 ↔ u. The last topic, k = 6, is a redundant topic not aligned with any vowel. Then, we define</p><formula xml:id="formula_55">µ kv = -3 + 4 {# k-th vowel in word v} T .</formula><p>This implies that the mean of the sequence h kv,t is high for a word that contains the k-th vowel frequently and that the same word is less relevant for other topics. The last topic is the least prevalent one. Next, we generate h v,t ∼ AR T (0, δ, τ ) and use these values to determine T consonants for this word. We create an equidistant grid of 21 (= number of consonants) intervals. The middle interval around zero corresponds to consonant n, while the intervals to the left for negative values correspond to consonants earlier in the alphabet and the intervals to the right to later consonants in the alphabet. Depending on where the values of h v,t land in the grid we assign the corresponding consonant to this word resulting in a set of T consonants. A word is then created by pasting one generated vowel behind one generated consonant, thus obtaining a word of 2T letters. Its composition with respect to vowels and consonants indicates the topic it relates to and how it evolves over time.</p><p>For example, the word naratoranukahikenaro is highly relevant for topic 1 and follows a sinusoid curve, see Figure <ref type="figure" target="#fig_13">6</ref>.  Different from the empirical application, we flattened the prior distribution for δ to N (0.5, 1) when fitting the TPF model.</p><p>To estimate the DPF, we slightly adjusted the implementation of TPF and kept some of its specifications. For example, the prior specification of auto-regression is the same than for TPF (i.e., the mean is in the prior and not in the formula for the rates and priors given by (S.1.6). To resemble the original DPF formulation, we assume δ g ak , δ h kv = 1 and a diagonal structure for ϕ cov gak and ϕ cov hkv . Direct CAVI updates are also applied where possible. Hence, our implementation of DPF is not identical to the implementation proposed by <ref type="bibr" target="#b6">Charlin et al. (2015)</ref> but it closely corresponds it while being implemented within the Tensorflow environment. The code is also available at our Github repository (https:// github.com/vavrajan/TPF).  in all settings. However, these results might be taken rather cautiously given that VBIC was developed and proposed for Bayesian linear regression. We thus focus in the following on VAIC. In this case the DPF model performs the worst among the seven considered models in almost all simulation settings, see Figure <ref type="figure" target="#fig_15">7</ref>. The comparison of different TPF models shows that specifying a diagonal covariance matrix ϕ cov hkv (light colour) is in terms of VAIC only negligibly better than using a general covariance matrix (dark colour). Assuming different assumptions on δ induces much higher differences in VAIC values. Specifying δ = 1 if the true value δ = 0 yields higher VAIC values, i.e., using the random-walk parameterisation when no temporal dependence is present considerably reduces the model fit. Even if the true value δ = 1 coincides with the model specification in models C and D, these models still have slightly higher VAIC values but the difference to other models is smaller than if δ &lt; 1. The shorter time sequence (T = 10) leads to very similar VAIC values for the other models (A, B, E, F), with a negligible preference for models restricting δ to [-1, 1]. For the longer sequence (T = 20), the models where δ is restricted to [-1, 1] perform slightly worse than the models where δ ∈ R in case the true value δ = 1. For this setting the non-stationarity leads to wilder sequences h kv,t than for the shorter sequence.</p><p>Regarding the computational costs, the TPF models with diagonal ϕ cov hkv have the lowest run-times for one epoch as well as ELBO and variational criteria evaluation. In addition the models that restrict δ ∈ [-1, 1] are in general a bit slower than models without any restriction or fixing δ to 1. Despite also using a diagonal structure, DPF does not exhibit the same efficiency. According to the run-times, DPF is more comparable to the TPF models with general covariance ϕ cov hkv . We conjecture that this might be the case because of the additional time-varying component g ak,t (which is estimated with a stochastic gradient approach) despite the equivalent dimension (AKT = DK) of θ dk .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.6 Pre-processing the U.S. Senate speeches</head><p>The results can differ substantially depending on how the speeches are converted to the DTM Y. The main function to transform speech data to counts is the CountVectorizer function from the scikit-learn module <ref type="bibr" target="#b18">(Pedregosa et al., 2011)</ref>. We decided to limit its use to bigrams only. The combination of two words carries the ideological thought behind their use much better than just a single word, while the number of higher n-grams grows quickly but are only used very rarely. The list of stop words was taken from <ref type="bibr" target="#b21">Vafa et al. (2020)</ref>.</p><p>We processed the speeches of each session separately with CountVectorizer to create session specific vocabularies. These vocabularies were combined into the final vocabulary of size V = 12 791. Each included bigram satisfied the condition on minimum and maximum frequency set to 0.001 and 0.3 on session level for at least one session. Finally, we called CountVectorizer on the set of all documents with this combined vocabulary and eliminated the empty rows. The final Y is a sparse 732 110 × 12 791 matrix with 11 111 756 stored elements (0.119%).</p><p>This procedure was pursued because calling CountVectorizer on the complete set of speeches from all 18 Congress session resulted in a final vocabulary of size V = 6 468 which contained only the most general bigrams (united states, republican democrat) that appear consistently across all time-periods. Bigrams that were specific for only a brief era (acid rain, hurricane katrina) were not included because they did not meet the condition on appearing in at least 0.05% of documents across the whole corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.7 Additional empirical results</head><p>Figures <ref type="bibr">8,</ref><ref type="bibr">9,</ref><ref type="bibr">10,</ref><ref type="bibr">11,</ref><ref type="bibr">12</ref> (analogous to the figures in the main paper) show that the differences between the estimated models (A -D) are negligible. The only remarkable difference can be seen in Figure <ref type="figure" target="#fig_21">10</ref> where the dissimilarity in topical content DTC depends on non-diagonal elements of ϕ cov hkv . Models (B) and (D) with diagonal ϕ cov hkv seem to have a much lower amount of highly dissimilar topics than models (A) and (C) with general ϕ cov hkv . But even in this case the overall pattern still seems to rather be the same. In addition, Table <ref type="table">5</ref> shows the evolution of the top-10 words based on the FREX measure for topic 15, which shifts from general political expressions to more specific discussions about immigration, based on model (D).           <ref type="bibr">97: 1981-1982 98: 1983-1984 99: 1985-1986 100: 1987-1988 101: 1989-1990 102: 1991-1992</ref> distinguished majority </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Terms can consist of unigrams, bigrams, or other n-grams. Stacking the individual document vectors Y d row-wise results in the DTM Y = (Y d ) D d=1 = (y dv ) D,V d,v=1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The TPF model in plate notation. Observed data (rectangles), primary model parameters (rounded corners), additional parameters (circles). Coloured planes indicate dimension sizes with respect to documents (D), the vocabulary (V ) and topics (K).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3: for e in 1 : E do 4: • Divide D documents into B batches B b , b = 1, . . . , B, of size |B b | ≈ |B|.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>•</head><label></label><figDesc>Update ϕ s θd := ϕ θd , ϕ s ξd := ϕ ξd for d ∈ B b . ▷ CAVI local updates 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Evolution of ELBO and reconstruction values over epochs under different settings.</figDesc><graphic coords="14,82.69,79.74,209.53,157.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Evolution of topic prevalences ψ kt over time.</figDesc><graphic coords="16,176.68,79.74,241.92,241.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) Consecutive sessions -DTC(k, t -1|k, t). (b) Across topics -DTC(k 1 |k 2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The dissimilarity of topical content measured by DTC.</figDesc><graphic coords="17,305.23,79.74,207.36,227.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) FREX measure. (b) AR (1) sequence h kv estimated by ϕ loc hkv .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Evolution of selected 10 terms for topic 12 -Climate change.</figDesc><graphic coords="18,81.64,391.12,432.02,288.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>reconstruction + log-prior + entropy =shp τ kv ) -log ϕ rte τ kv</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Based on the rates λ dv = 6 k=1θ</head><label>6</label><figDesc>dk exp{h kv,t }, we generate 10 replicates of document-term matrices Y. Varying δ ∈ {0, 0.5, 1} and T ∈ {10, 20} results in 60 different DTMs. For each DTM, we estimate seven different models: (A) TPF with δ kv ∈ R (normal) and general ϕ cov hkv , (B) TPF with δ kv ∈ R (normal) and diagonal ϕ cov hkv , (C) TPF with δ kv = 1 (deterministic) and general ϕ cov hkv , (D) TPF with δ kv = 1 (deterministic) and diagonal ϕ cov hkv , (E) TPF with δ kv ∈ [-1, 1] (truncated normal) and general ϕ cov hkv , (F) TPF with δ kv ∈ [-1, 1] (truncated normal) and diagonal ϕ cov hkv , (G) DPF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Illustrative example of sampled h kv,t for v = naratoranukahikenaro.</figDesc><graphic coords="31,124.84,79.74,345.61,246.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Boxplots of VAIC values over 10 replications for estimated models under six simulation settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>(A) δ kv ∈ R, ϕ cov hkv general. (B) δ kv ∈ R, ϕ cov hkv diagonal. (C) δ kv = 1, ϕ cov hkv general. (D) δ kv = 1, ϕ cov hkv diagonal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Evolution of topic prevalence ψ kt over time.</figDesc><graphic coords="36,84.85,391.12,207.35,207.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>(</head><label></label><figDesc>A) δ kv ∈ R, ϕ cov hkv general. (B) δ kv ∈ R, ϕ cov hkv diagonal. (C) δ kv = 1, ϕ cov hkv general. (D) δ kv = 1, ϕ cov hkv diagonal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The dissimilarity of topical content across consecutive sessions measured by DTC(k, t -1|k, t).</figDesc><graphic coords="37,303.06,383.00,207.36,222.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>(</head><label></label><figDesc>A) δ kv ∈ R, ϕ cov hkv general. (B) δ kv ∈ R, ϕ cov hkv diagonal. (C) δ kv = 1, ϕ cov hkv general. (D) δ kv = 1, ϕ cov hkv diagonal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The dissimilarity of topical content across topics measured by DTC(k 1 |k 2 ).</figDesc><graphic coords="38,84.85,393.11,207.35,223.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>(</head><label></label><figDesc>A) δ kv ∈ R, ϕ cov hkv general. (B) δ kv ∈ R, ϕ cov hkv diagonal. (C) δ kv = 1, ϕ cov hkv general. (D) δ kv = 1, ϕ cov hkv diagonal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Evolution of h kv for selected 10 terms for topic 12 -Climate change.</figDesc><graphic coords="39,84.85,391.13,207.35,138.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>(</head><label></label><figDesc>A) δ kv ∈ R, ϕ cov hkv general. (B) δ kv ∈ R, ϕ cov hkv diagonal. (C) δ kv = 1, ϕ cov hkv general. (D) δ kv = 1, ϕ cov hkv diagonal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Evolution of FREX for selected 10 terms for topic 12 -Climate change.</figDesc><graphic coords="40,84.85,391.13,207.35,138.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>Evolution of top-10 terms selected based on FREX for topic 15 -Rhetoric → Immigration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Labels assigned to the K = 25 topics.</figDesc><table><row><cell>1 Taxes, business 2 World conflicts 3 Children, families 4 Law enforcement 5 Natural environment 13 Agriculture 9 Commemoration 10 Budget 11 Health, diseases 12 Climate change 6 Senate coordination 14 United States 7 Federal government 15 Rhetoric → Immigration 23 War 17 Education 18 Environment preservation 19 Finance 20 Supreme court 21 Civil rights 22 Circuit courts 8 Armed services 16 Affordable health care 24 Social security 25 Crime</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Evolution of the top-10 terms selected based on FREX for topic 12 -Climate change.</figDesc><table><row><cell>natural gas nuclear waste energy policy strategic petroleum air act acid rain foreign oil energy security energy sources alternative energy</cell><cell>natural gas acid rain energy security air act energy policy sulfur dioxide foreign oil gas industry energy sources price natural</cell><cell cols="2">natural gas acid rain energy policy gas industry energy security clean coal foreign oil domestic oil air act energy independence sulfur dioxide nuclear waste acid rain natural gas clean coal energy security air act energy policy greenhouse effect carbon dioxide</cell><cell>air act natural gas acid rain carbon dioxide energy policy sulfur dioxide clean coal air pollutants dioxide emissions foreign oil</cell><cell>natural gas energy policy air act energy efficiency energy strategy carbon dioxide energy security foreign oil greenhouse gases greenhouse gas</cell></row><row><cell>103: 1993-1994</cell><cell>104: 1995-1996</cell><cell>105: 1997-1998</cell><cell>106: 1999-2000</cell><cell>107: 2001-2002</cell><cell>108: 2003-2004</cell></row><row><cell>air act natural gas energy policy energy efficiency gas industry foreign oil energy security energy sources domestic oil wind energy</cell><cell>nuclear waste natural gas foreign oil energy policy energy security energy efficiency gas industry domestic oil clean air air act</cell><cell>foreign oil greenhouse gas gas emissions greenhouse gases carbon dioxide energy policy energy efficiency energy security oil percent reduce greenhouse</cell><cell>energy policy foreign oil energy security greenhouse gas clean coal energy efficiency domestic oil energy sources gas emissions domestic energy</cell><cell>natural gas energy policy foreign oil renewable portfolio energy sources energy efficiency portfolio standard energy supply greenhouse gas billion barrels</cell><cell>energy policy foreign oil fuel cell greenhouse gas comprehensive energy clean coal gas emissions energy efficiency energy sources wind energy</cell></row><row><cell>109: 2005-2006</cell><cell>110: 2007-2008</cell><cell>111: 2009-2010</cell><cell>112: 2011-2012</cell><cell>113: 2013-2014</cell><cell>114: 2015-2016</cell></row><row><cell cols="6">energy policy foreign oil energy independence greenhouse gas foreign oil energy policy greenhouse gas energy independence foreign oil greenhouse gas energy efficiency gas emissions energy efficiency energy efficiency greenhouse gases gas emissions gas emissions energy policy oil natural energy sources energy independence keystone xl energy policy foreign oil domestic energy energy production wind energy carbon dioxide energy sources billion barrels carbon dioxide greenhouse gas greenhouse gases carbon dioxide energy sources energy efficiency clean coal greenhouse gases hydraulic fracturing energy independence energy savings energy efficiency carbon pollution keystone xl xl pipeline energy policy greenhouse gas carbon dioxide fossil fuels energy independence greenhouse gas fossil fuel keystone xl energy efficiency carbon pollution energy policy xl pipeline energy security fossil fuels carbon dioxide</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>dv term v counts in document d Y DTM y dkv latent dv counts for topic k Y latent topic-specific DTMs N T (µ, Σ) T -variate normal distribution with mean µ and covariance matrix Σ Γ (a, b) gamma distribution with shape a and rate b Pois (λ) Poisson distribution with rate λ Mult K (T, π) K-dimensional multinomial distribution with number of trials T and success probabilities π</figDesc><table><row><cell cols="2">S.1.1 Notation</cell><cell></cell></row><row><cell>y</cell><cell>d index of a document v index of a term (word) k index of a topic t index of a time period t d time period of document d a index of an author</cell><cell>D number of documents V number of terms (vocabulary size) K number of topics T number of time periods D t document indices in time period t A number of authors</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Table4summarises the results based on variational criteria and the computational costs (averaged over 10 replications) of the seven estimated models under the six different model settings (combination of T and the true value of δ). According to VBIC, DPF performs best Comparison of seven model specifications under six simulation settings averaged over 10 replications in terms of variational information criteria (VAIC, VBIC; with values divided by 1000) and computation time ("sec/epoch" denotes the average time per epoch to update the variational parameters and the approximate ELBO values; "sec/ELBO" denotes the average time per epoch to determine exact ELBO values and the information criteria). The best value among seven models is highlighted by bold font.</figDesc><table><row><cell></cell><cell>A TPF B TPF C TPF D TPF E TPF [-1, 1] general R general R diagonal 1 general 1 diagonal</cell><cell>24 548.9 24 547.6 24 566.8 24 562.5 24 548.2</cell><cell>25 198.3 25 197.9 25 129.6 25 128.6 25 200.0</cell><cell>11.27 6.07 11.06 6.04 6.04 11.63</cell><cell>6.75 3.79 3.79 6.79 3.83 8.32</cell></row><row><cell></cell><cell>F TPF [-1, 1] diagonal G DPF 1 diagonal</cell><cell>24 547.4 24 547.4 24 556.5</cell><cell>25 200.6 24 407.8 24 407.8</cell><cell>6.48 11.69</cell><cell>5.34 7.37</cell></row><row><cell>20 0.5</cell><cell>A TPF B TPF C TPF D TPF E TPF [-1, 1] general R general R diagonal 1 general 1 diagonal</cell><cell>24 686.6 24 684.8 24 701.2 24 697.0 24 685.5</cell><cell>25 315.2 25 315.2 25 281.6 25 280.6 25 318.1</cell><cell>11.27 6.10 11.04 6.02 6.02 11.62</cell><cell>6.78 3.82 3.82 6.78 3.84 8.32</cell></row><row><cell></cell><cell>F TPF [-1, 1] diagonal G DPF 1 diagonal</cell><cell>24 684.4 24 684.4 24 692.1</cell><cell>25 318.7 24 545.0 24 545.0</cell><cell>6.52 11.69</cell><cell>5.43 7.37</cell></row><row><cell>20 1.0</cell><cell>A TPF B TPF C TPF D TPF E TPF [-1, 1] general R general R diagonal 1 general 1 diagonal</cell><cell>25 678.7 25 676.8 25 676.8 25 684.2 25 679.2 25 679.2</cell><cell>26 212.0 26 215.0 26 205.7 26 204.4 26 218.4</cell><cell>11.27 6.10 11.05 6.03 6.03 11.61</cell><cell>6.76 3.81 3.81 6.80 3.98 8.32</cell></row><row><cell></cell><cell>F TPF [-1, 1] diagonal G DPF 1 diagonal</cell><cell>25 677.3 25 714.4</cell><cell>26 221.0 25 603.0 25 603.0</cell><cell>6.44 11.80</cell><cell>5.35 7.40</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><head>Funding</head><p>This research was supported by the <rs type="funder">Jubiläumsfonds of the Oesterreichische Nationalbank (OeNB</rs>, grant no. <rs type="grantNumber">18718</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Cft8UbD">
					<idno type="grant-number">18718</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Conflicting Interests</head><p>The authors declared no potential conflicts of interest with respect to the research, authorship and/or publication of this article. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Summarizing topical content with word frequency and exclusivity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML &apos;12</title>
		<meeting>the 29th International Coference on International Conference on Machine Learning, ICML &apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="DOI">10.1145/2133806.2133826</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<idno type="DOI">10.1145/1143844.1143859</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning, ICML &apos;06</title>
		<meeting>the 23rd International Conference on Machine Learning, ICML &apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.2017.1285773</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting semantic representations from word co-occurrence statistics: A computational study</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03193020</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="510" to="526" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic Poisson factorization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcinerney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="DOI">10.1145/2792838.2800174</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM Conference on Recommender Systems, RecSys &apos;15</title>
		<meeting>the 9th ACM Conference on Recommender Systems, RecSys &apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Congressional record for the 43rd-114th Congresses: Parsed speeches and phrase counts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gentzkow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taddy</surname></persName>
		</author>
		<ptr target="https://data.stanford.edu/congress_text.StanfordLibraries" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2018" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Text as data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gentzkow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taddy</surname></persName>
		</author>
		<idno type="DOI">10.1257/jel.20181020</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Literature</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="535" to="574" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bayesian analysis of dynamic linear topic models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Glynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Tokdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howard</surname></persName>
		</author>
		<idno type="DOI">10.1214/18-BA1100</idno>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="80" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scalable recommendation with hierarchical Poisson factorization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hofman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 31st Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="326" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Content-based recommendations with Poisson factorization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Revisiting group differences in high-dimensional choices: Method and application to Congressional speech</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hofmarcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vávra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grün</surname></persName>
		</author>
		<idno type="DOI">10.1002/jae.3125</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Econometrics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="577" to="588" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent Poisson factorization for temporal recommendation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khodadadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arabzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Rabiee</surname></persName>
		</author>
		<idno type="DOI">10.1145/3097983.3098197</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;17</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="847" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust machine learning algorithms for text analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L M</forename><surname>Olea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nesbit</surname></persName>
		</author>
		<idno type="DOI">10.3982/QE1825</idno>
	</analytic>
	<monogr>
		<title level="j">Quantitative Economics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="939" to="970" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic differentiation variational inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Variational approximations in Bayesian model selection for finite mixture distributions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Mcgrory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Titterington</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csda.2006.07.020</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5352" to="5367" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A model of text for experimentation in the social sciences</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.2016.1141684</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">515</biblScope>
			<biblScope unit="page" from="988" to="1003" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian measures of model complexity and fit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Der Linde</surname></persName>
		</author>
		<idno type="DOI">10.1111/1467-9868.00353</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="583" to="639" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Text-based ideal points</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Naidu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5345" to="5357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A structural textbased scaling model for analyzing political discourse</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vávra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Prostmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-K</forename><surname>Grün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hofmarcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2410.11897" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Continuous time dynamic topic models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI&apos;08</title>
		<meeting>the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI&apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="579" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On variational Bayes estimation and variational information criteria for linear regression models</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Ormerod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1111/anzs.12063</idno>
	</analytic>
	<monogr>
		<title level="j">Australian &amp; New Zealand Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="87" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
