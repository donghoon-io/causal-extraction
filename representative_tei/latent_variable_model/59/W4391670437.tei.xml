<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Representation Learning from Multiple Distributions: A General Setting</title>
				<funder ref="#_CDC72xx">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_v6kH9VM">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">Apple Inc., KDDI Research Inc.</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-08-11">11 Aug 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shaoan</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yujia</forename><surname>Zheng</surname></persName>
						</author>
						<title level="a" type="main">Causal Representation Learning from Multiple Distributions: A General Setting</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-08-11">11 Aug 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2402.05052v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many problems, the measured variables (e.g., image pixels) are just mathematical functions of the latent causal variables (e.g., the underlying concepts or objects). For the purpose of making predictions in changing environments or making proper changes to the system, it is helpful to recover the latent causal variables Z i and their causal relations represented by graph G Z . This problem has recently been known as causal representation learning. This paper is concerned with a general, completely nonparametric setting of causal representation learning from multiple distributions (arising from heterogeneous data or nonstationary time series), without assuming hard interventions behind distribution changes. We aim to develop general solutions in this fundamental case; as a by product, this helps see the unique benefit offered by other assumptions such as parametric causal models or hard interventions. We show that under the sparsity constraint on the recovered graph over the latent variables and suitable sufficient change conditions on the causal influences, interestingly, one can recover the moralized graph of the underlying directed acyclic graph, and the recovered latent variables and their relations are related to the underlying causal model in a specific, nontrivial way. In some cases, most latent variables can even be recovered up to component-wise transformations. Experimental results verify our theoretical claims.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Causal representation learning holds paramount significance across numerous fields, offering insights into intricate relationships within datasets. Most traditional methodologies (e.g., causal discovery) assume the observation of causal variables. This assumption, however reasonable, falls short in complex scenarios involving indirect measurements, such as electronic signals, image pixels, and linguistic tokens. Moreover, there are usually changes on the causal mechanisms in real-world, such as the heterogeneous or nonstationary data. Identifying the latent causal variables and their structures together with the change of the causal mechanism is in pressing need to understand the complicated real-world causal process. This has been recently known as causal representation learning <ref type="bibr" target="#b32">(Schölkopf et al., 2021)</ref>.</p><p>It is worth noting that identifying only the latent causal variables but not the structure among them, is already a considerable challenge. In the i.i.d. case, different latent representations can explain the same observations equally well, while not all of them are consistent with the true causal process. For instance, nonlinear independent component analysis (ICA), where a set of observed variables X is represented as a mixture of independent latent variables Z, i.e, X = g(Z), is known to be unidentifiable without additional assumptions <ref type="bibr" target="#b7">(Comon, 1994)</ref>. While being a strictly easier task since there are no relations among latent variables, the identifiability of nonlinear ICA often relies on conditions on distributional assumptions (non-i.i.d. data) <ref type="bibr" target="#b14">(Hyvärinen &amp; Morioka, 2016;</ref><ref type="bibr" target="#b15">2017;</ref><ref type="bibr" target="#b18">Hyvärinen et al., 2019;</ref><ref type="bibr">Khemakhem et al., 2020a;</ref><ref type="bibr" target="#b34">Sorrenson et al., 2020;</ref><ref type="bibr" target="#b26">Lachapelle et al., 2022;</ref><ref type="bibr" target="#b11">Hälvä &amp; Hyvärinen, 2020;</ref><ref type="bibr" target="#b12">Hälvä et al., 2021;</ref><ref type="bibr" target="#b46">Yao et al., 2022)</ref> or specific functional constraints <ref type="bibr" target="#b7">(Comon, 1994;</ref><ref type="bibr" target="#b16">Hyvärinen &amp; Pajunen, 1999;</ref><ref type="bibr" target="#b39">Taleb &amp; Jutten, 1999;</ref><ref type="bibr" target="#b4">Buchholz et al., 2022;</ref><ref type="bibr" target="#b50">Zheng et al., 2022;</ref><ref type="bibr">Zheng &amp; Zhang, 2023)</ref>.</p><p>To generalize beyond the independent latent variables and achieve causal representation learning (recovering the latent variables and their causal structure), recent advances either introduce additional experiments in the forms of interventional or counterfactual data, or place more restrictive parametric or graphical assumptions on the latent causal model. For observational data, various graphical conditions have been proposed together with parametric assumptions such as linearity <ref type="bibr" target="#b33">(Silva et al., 2006;</ref><ref type="bibr" target="#b6">Cai et al., 2019;</ref><ref type="bibr" target="#b43">Xie et al., 2020;</ref><ref type="bibr">2022;</ref><ref type="bibr" target="#b0">Adams et al., 2021;</ref><ref type="bibr" target="#b13">Huang et al., 2022)</ref> and discreteness <ref type="bibr" target="#b24">(Kivva et al., 2021)</ref>. For interventional data, single-node interventions have been considered together with parametric assumptions (e.g., linearity) on the mixing function <ref type="bibr" target="#b41">(Varici et al., 2023;</ref><ref type="bibr" target="#b1">Ahuja et al., 2023;</ref><ref type="bibr" target="#b4">Buchholz et al., 2022)</ref> or also on the latent causal model <ref type="bibr" target="#b36">(Squires et al., 2023)</ref>. The nonparametric settings for both the mixing function and causal model have been explored by <ref type="bibr" target="#b3">(Brehmer et al., 2022;</ref><ref type="bibr" target="#b42">von Kügelgen et al., 2023;</ref><ref type="bibr" target="#b20">Jiang &amp; Aragam, 2023)</ref> together with additional assumptions on counterfactual views <ref type="bibr" target="#b3">(Brehmer et al., 2022)</ref>, distinct paired interventions <ref type="bibr" target="#b42">(von Kügelgen et al., 2023)</ref>, and graphical conditions <ref type="bibr" target="#b20">(Jiang &amp; Aragam, 2023)</ref>.</p><p>Despite the exciting developments in the field, one fundamental question pertinent to causal representation learning from multiple distributions remains unanswered-in the most general situation, without assuming parametric models on the data-generating process or the existence of hard interventions in the data, what information of the latent variables and the latent structure can be recovered? This paper attempts to provide an answer to it, which, surprisingly, shows that each latent variable can be recovered up to clearly defined indeterminacies. It suggests what we can achieve in the general case and furthermore, what unique contribution the typical assumptions that are currently made in causal representation learning from multiple distributions make towards complete identifiability of the latent variables (up to component-wise transformations). This may make it possible to figure out what minimal assumptions are needed to achieve complete identifiability, given partial knowledge of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>Concretely, as our contributions, we show that under the sparsity constraint on the recovered graph over the latent variables and suitable sufficient change conditions on the causal influences, interestingly, one can recover the moralized graph of the underlying directed acyclic graph (Theorem 2), and the recovered latent variables and their relations are related to the underlying causal model in a specific, nontrivial way (Theorem 3)-each latent variables is recovered as a function of itself and its so-called intimate neighbors in the Markov network implied by the true causal structure over the latent variables. Depending on the properties of the true causal structure over latent variables, the set of intimate neighbors might even be empty, in which case the corresponding latent variables can be recovered up to component-wise transformations (Remark 1). Lastly, we show how the recovered moralized graph relates to the underlying causal graph under new relaxations of faithfulness assumption (Proposition 2). Simulation studies verified our theoretical findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Setting</head><p>Let X = (X 1 , . . . , X d ) be a d-dimensional vector that represents the observed variables (e.g., image pixels). We assume that they are generated by n latent causal variables Z = (Z 1 , . . . , Z n ) via a nonlinear mixing function  <ref type="bibr">2000)</ref>. Putting them together, the underlying data generating process can be written as</p><formula xml:id="formula_0">g : R n → R d (d ≥ n), which is a C 2 -diffeomorphism onto its image X ⊆ R d .</formula><formula xml:id="formula_1">X = g(Z)</formula><p>Nonlinear mixing</p><formula xml:id="formula_2">, Z i = f i (PA(Z i ), ϵ i ; θ i ), i = 1, . . . , n Latent SEM</formula><p>.</p><p>(1) where PA(Z i ) denotes the parents of variable Z i , ϵ i 's are exogenous noise variables that are mutually independent, and θ i denotes the latent (changing) factor (or effective parameters) associated with each model. Here, the data generating process of each latent variable Z i may change, e.g., across domains or over time, governed by the corresponding latent factor θ i ; it is commonplace to encounter such changes in causal mechanisms in practice (arising from heterogeneous data or nonstationary time series). In addition, interventional data can be seen as a special type of change, which qualitatively restructure the causal relations. As their names suggest, we assume that the variables X are observed, while the latent causal variables Z and latent factors θ = (θ 1 , . . . , θ n ) are unobserved.</p><p>Let P X;θ and P Z;θ be the distributions of X and Z, respectively, and their probability density functions be p X (X; θ) and p Z (Z; θ), respectively.<ref type="foot" target="#foot_0">foot_0</ref> To lighten the notation, we drop the subscript in the density when the context is clear. The latent SEM in Eq. ( <ref type="formula">1</ref>) induces a causal graph G Z with vertices {Z i } n i=1 and edges Z j → Z i if and only if Z j ∈ PA(Z i ). We assume that G Z is acyclic, i.e., a directed acyclic graph (DAG). This implies that the distribution of variables Z satisfy the Markov property w.r.t. DAG G Z <ref type="bibr" target="#b30">(Pearl, 2000)</ref>, i.e., p(Z; θ) = n i=1 p(Z i | PA(Z i ); θ i ). We provide an example of the data generating process in Eq. ( <ref type="formula">1</ref>) and its corresponding latent DAG G Z in Figure <ref type="figure" target="#fig_0">1</ref>. Given samples of the observed variables X arising from multiple distributions (or domains), say Θ = {θ (1) , . . . , θ (m) }, our goal is to recover the latent causal variables Z and their causal relations up to minor indeterminacies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Causal Representations from Multiple Distributions</head><p>In this section, we provide theoretical results to show how one is able to recover the underlying latent causal variables and their causal relations up to certain indeterminacies from multiple distributions. Specifically, we show that under sparsity constraint on the recovered graph over the latent variables and suitable sufficient change conditions on the causal influences, the recovered latent variables are related to the true ones in a specific, nontrivial way. Such results serve as the foundation of our algorithm in Section 4.</p><p>To start with, we estimate a model (ĝ, f , p Ẑ , Θ) that assumes the same data generating process as in Eq. ( <ref type="formula">1</ref>) and matches the true distribution of X in different domains:</p><formula xml:id="formula_3">p X (X ′ ; θ (u) ) = p X (X ′ ; θ(u) ), ∀ θ (u) ∈ Θ, X ′ ∈ X (u) ,<label>(2)</label></formula><p>where θ (u) denotes the latent factor in the u-th domain, and (u) is the image of function g in the u-th domain. Here, X and X are generated by the true model (g, f, p Z , Θ) and the estimated model (ĝ, f , p Ẑ , Θ), respectively.</p><formula xml:id="formula_4">X</formula><p>A key ingredient of our results is the Markov network that represents conditional dependencies among random variables via an undirected graph. Let M Z be the Markov network over variables Z, i.e., with vertices {Z i } n i=1 and edges</p><formula xml:id="formula_5">{Z i , Z j } ∈ E(M Z ) if and only if Z i ⊥ ̸ ⊥ Z j | Z [n]\{i,j} . 2</formula><p>Also, we denote by |M Z | the number of undirected edges in the Markov network. In Section 3.1, apart from showing how to estimate the underlying latent causal variables up to certain indeterminacies, we also show that such latent Markov network M Z can be recovered up to isomorphism. To achieve so, we make use of the following property (assuming that p Z is twice differentiable):</p><formula xml:id="formula_6">Z i ⊥ ⊥ Z j | Z [n]\{i,j} ⇐⇒ ∂ 2 log p(Z; θ) ∂Z i ∂Z j = 0. (3)</formula><p>Such a connection between pairwise conditional independence and cross derivatives of the density function has been noted by <ref type="bibr" target="#b27">Lin (1997)</ref> and utilized in Markov network learning for observed variables <ref type="bibr">(Zheng et al., 2023)</ref>. With the recovered latent Markov network structure, we provide results in Section 3.2 to show how it relates to the moralized graph of true latent causal DAG G Z , by exploiting a specific type of faithfulness assumption that is considerably weaker than the standard faithfulness assumption used in the literature of causal discovery <ref type="bibr" target="#b35">(Spirtes et al., 2001)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Recovering Latent Causal Variables and Latent Markov Network</head><p>We consider a general, completely nonparametric setting of causal representation learning from multiple distributions.</p><p>Specifically, we show how one can recover the latent causal variables and the Markov network structure among them up to minor indeterminacies, by leveraging sparsity constraint and sufficient change conditions on the causal mechanisms. Notably, in some cases, most latent variables can even be recovered up to component-wise transformations.</p><p>We start with the following result that provides information about the derivative of true latent causal variables Z with respect to the estimated ones Ẑ, according to their corresponding Markov networks M Z and M Ẑ . Result of this form is often used in the proof of nonlinear ICA to obtain identifiability of component-wise nonlinear transformations <ref type="bibr" target="#b14">(Hyvärinen &amp; Morioka, 2016;</ref><ref type="bibr" target="#b18">Hyvärinen et al., 2019)</ref>. At the same time, our result here is different from that of nonlinear ICA as it allows for causal relations among latent variables. This result serves as the backbone of our further identifiability results in this section. Proposition 1. Let the observations be sampled from the data generating process in Eq. ( <ref type="formula">1</ref>), and M Z be the Markov network over Z. Suppose the following assumptions hold:</p><p>• A1 (Smooth and positive density): The probability density function of latent causal variables, i.e., p Z , is twice continuously differentiable and positive in R n .</p><p>• A2 (Sufficient changes): For each value of Z, there exist 2n + |M Z | + 1 values of θ, i.e., θ (u) with u = 0, . . . , 2n+|M Z |, such that the vectors w(Z, u)w(z, 0) with u = 1, . . . , 2n + |M Z | are linearly independent, where vector w(Z, u) is defined as follows:</p><formula xml:id="formula_7">3 w(Z, u) = ∂ log p(Z; θ (u) ) ∂Z i i∈[n] ⊕ ∂ 2 log p(Z; θ (u) ) ∂Z 2 i i∈[n] ⊕ ∂ 2 log p(Z; θ (u) ) ∂Z i ∂Z j {Zi,Zj }∈E(M Z ), i&lt;j</formula><p>.</p><p>Suppose that we learn (ĝ, f , p Ẑ , Θ) to achieve Eq. (2). Then, for every pair of estimated latent variables Ẑk and Ẑl that are not adjacent in the Markov network M Ẑ over Ẑ, we have the following statements:</p><p>(a) For each true latent causal variable Z i , we have</p><formula xml:id="formula_8">∂Z i ∂ Ẑk ∂Z i ∂ Ẑl = 0.<label>(4)</label></formula><p>(b) For each pair of true latent causal variables Z i and Z j that are adjacent in the Markov network M Z , we have</p><formula xml:id="formula_9">∂Z i ∂ Ẑk ∂Z j ∂ Ẑl = 0. (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>The proof is provided in Appendix B, which leverages the property of Markov network in Eq. (3). Assumption A2 can be viewed as suitable sufficient change conditions on the causal influences across different domains. It is worth noting that the requirement of a sufficient number of domains has been commonly adopted in the literature (e.g., see <ref type="bibr" target="#b19">Hyvärinen et al. (2023)</ref> for a recent survey), such as visual disentanglement <ref type="bibr">(Khemakhem et al., 2020b)</ref>, domain adaptation <ref type="bibr" target="#b25">(Kong et al., 2022)</ref>, video analysis <ref type="bibr" target="#b45">(Yao et al., 2021)</ref>, and image-to-image translation <ref type="bibr" target="#b44">(Xie et al., 2022)</ref>. Also, we do not specify exactly how to learn (ĝ, f , p Ẑ , Θ) to achieve Eq. ( <ref type="formula" target="#formula_3">2</ref>), and leave the door open for different approaches to be used, such as normalizing flow and variational approaches. For example, we adopt a variational approach in Section 4.</p><p>In Theorem 1, Eqs. ( <ref type="formula" target="#formula_8">4</ref>) and ( <ref type="formula" target="#formula_9">5</ref>) hold for every sample of Z. Intuitively, one may expect that Eq. ( <ref type="formula" target="#formula_8">4</ref>) implies either = 0 for all samples, i.e., the zero entries in the Jacobian matrix (of the function from Ẑ to Z) remain in the same positions across different samples. If this conclusion holds true, it indicates that the true latent variable Z i cannot be a function of both estimated latent variables Ẑk and Ẑl , which is helpful for disentanglement. The same reasoning applies to Eq. ( <ref type="formula" target="#formula_9">5</ref>). In fact, similar conclusion can often be obtained in the proof of identifiability for nonlinear ICA <ref type="bibr" target="#b18">(Hyvärinen et al., 2019)</ref>, by leveraging the continuity and invertibility of the Jacobian matrix.</p><p>However, this conclusion in general does not hold in our setting (that allows for causal relations among latent variables Z) without any constraint on the sparsity of recovered Markov network, for which counterexamples exist. The reason is that each of Eqs. (4) and (5) correspond to a pair of recovered latent variables Ẑ that are not adjacent in the Markov network M Ẑ , and can be viewed as a specific form of restriction on the Jacobian matrix (of the function from Ẑ to Z). When the recovered Markov network is relatively dense, less restrictions are imposed on the Jacobian matrix, and thus there are possibilities for the aforementioned zero entries to switch positions across different samples. Interestingly, incorporating sparsity constraint on the recovered Markov network during estimation can help eliminate these possibilities, formally described below.</p><p>Theorem 1 (Relations among true and recovered latent causal variables). Let the observations be sampled from the data generating process in Eq. ( <ref type="formula">1</ref>), and M Z be the Markov network over Z. Suppose that Assumptions A1 and A2 from Theorem 1 hold. Suppose also that we learn (ĝ, f , p Ẑ , Θ) to achieve Eq. ( <ref type="formula" target="#formula_3">2</ref>) with the minimal number of edges of the Markov network M Ẑ over Ẑ. Then, for every pair of estimated latent variables Ẑk and Ẑl that are not adjacent in the Markov network M Ẑ over Ẑ, we have the following statements:</p><p>(a) Each true latent causal variable Z i is a function of at most one of Ẑk and Ẑl .</p><p>(b) For each pair of true latent causal variables Z i and Z j that are adjacent in the Markov network M Z over Z, at most one of them is a function of Ẑk or Ẑl .</p><p>The proof can be found in Appendix D. The above result sheds light on how each pair of the estimated latent variables Ẑk and Ẑl that are not adjacent in Markov network M Ẑ relate to the true latent causal variables Z, thus providing information for further disentanglement. Furthermore, note that a trivial solution would be a complete graph over Ẑ without any constraint on the estimating process. Apart from providing information for disentanglement, we show below that incorporating sparsity constraint on the recovered Markov network also helps avoid this trivial solution and recover the underlying Markov network up to isomorphism. The proof is given in Appendix C.</p><p>Theorem 2 (Identifiability of latent Markov network). Let the observations be sampled from the data generating process in Eq. ( <ref type="formula">1</ref>), and M Z be the Markov network over Z. Suppose that Assumptions A1 and A2 from Theorem 1 hold. Suppose also that we learn (ĝ, f , p Ẑ , Θ) to achieve Eq. ( <ref type="formula" target="#formula_3">2</ref>) with the minimal number of edges of the Markov network M Ẑ over Ẑ. Then, the recovered latent Markov network M Ẑ is isomorphic to the true latent Markov network M Z .</p><p>In addition to recovering the underlying Markov network M Z , we show that the sparsity constraint on the recovered Markov network M Ẑ also allows us to recover the underlying latent causal variables Z up to specific, relatively minor indeterminacies. In the result, the following variable set, termed intimate neighbor set, plays an important role:</p><formula xml:id="formula_11">Ψ Zi := {Z j | Z j , j ̸ = i, is adjacent to Z i and all other neighbors of Z i in M Z }.</formula><p>For example, according to the Markov network implied by G Z in Figure <ref type="figure" target="#fig_0">1</ref>, </p><formula xml:id="formula_12">Ψ Z1 = {Z 2 }, Ψ Z2 = ∅, where ∅ denotes the empty set, Ψ Z3 = {Z 2 , Z 4 }, Ψ Z4 = ∅,</formula><formula xml:id="formula_13">Ψ Zi = ∅ for i = 1, 2, 3, 5, 6 and Ψ Z4 = {Z 3 , Z 6 }.</formula><p>Theorem 3 (Identifiability of latent causal variables). Let the observations be sampled from the data generating process in Eq. ( <ref type="formula">1</ref>). Suppose that Assumptions A1 and A2 from Theorem 1 hold. Suppose also that we learn (ĝ, f , p Ẑ , Θ) to achieve Eq. ( <ref type="formula" target="#formula_3">2</ref>) with the minimal number of edges of the Markov network M Ẑ over Ẑ. Then, there exists a permutation π of the estimated latent variables, denoted as Ẑπ , such that each Ẑπ(i) is solely a function of Z i and a subset of Ψ Zi .  Note that the term 'solely' indicates that Ẑπ(i) is not a function of other variables that are not specified in the theorem above. The proof is given in Appendix E. Roughly speaking, the proof leverages Theorems 1 and 2 to reason about the relationships among the true latent variables and the recovered ones, which imply that certain entries on the Jacobian matrix ∂Z ∂ Ẑ must be zero. We show that these entries remain zero in the powers of ∂Z It is worth noting that in many cases, Theorem 3 already enables us to recover some of the latent variables up to a component-wise transformation. Remark 1. No matter how many neighbors each latent causal variable Z i has, as long as each of its neighbors is not adjacent to at least one other neighbor in the Markov network M Z , then Z i can be recovered up to a componentwise transformation.</p><p>Even if the above case does not hold, Theorem 3 still shows how the estimated latent variables relate to the underlying causal variables in a specific, nontrivial way. Two examples are provided below. Example 1. Consider the Markov network M Z corresponding to the DAG G Z over Z in Figure <ref type="figure" target="#fig_0">1</ref>. By Theorem 3 and suitable permutation of estimated latent variables Ẑ, we have: (a) Ẑπ( <ref type="formula">1</ref>) is solely a function of a subset of <ref type="formula" target="#formula_8">4</ref>) is solely a function of Z 4 , and (e) Ẑπ( <ref type="formula" target="#formula_9">5</ref>) is solely a function of a subset of {Z 4 , Z 5 }. In this example, the latent causal variables Z 2 and Z 4 can be recovered up to component-wise transformation, while variables Z 1 , Z 3 , and Z 5 can be identified up to mixtures with certain neighbors in the Markov network. Example 2. One may think that generally speaking, the more complex G Z , the more indeterminacies we have in the estimated latent variables (in the sense that each estimated latent variable receives contributions from more latent variables). In fact, this may not be the case. For instance, consider the underlying latent causal graph G Z in Figure <ref type="figure" target="#fig_4">2</ref>(a), which involves more variables and more edges and whose Markov network is shown in Figure <ref type="figure" target="#fig_4">2</ref>(b). For every variable Z i that is not the sink node, it has Ψ Zi = ∅ and thus can be recovered up to a component-wise transformation.</p><formula xml:id="formula_14">{Z 1 , Z 2 }, (b) Ẑπ(2) is solely a function of Z 2 , (c) Ẑπ(3) is solely a function of a subset of {Z 2 , Z 3 , Z 4 }, (d) Ẑπ(</formula><p>Permutation of recovered latent variables. Theorems 2 and 3 involve certain permutation of the estimated latent variables Ẑ. Such an indeterminacy is common in the literature of causal discovery and representation learning tasks involving latent variables. In our case, since the function v := g -1 • ĝ where Z = v( Ẑ) is invertible, there exists a permutation of the latent variables such that the corresponding Jacobian matrix J v has nonzero diagonal entries (see Lemma 2 in Appendix A.1); such a permutation is what Theorems 2 and 3 refer to.</p><p>Connection with nonlinear ICA. It is worth noting that nonlinear ICA (with auxiliary variables) may be viewed as a special case of our result in this section. Specifically, if the true latent causal DAG G Z is an empty graph, then the latent causal variables are independent, which reduce to the nonlinear ICA setting <ref type="bibr" target="#b18">(Hyvärinen et al., 2019)</ref>. Furthermore, since traditional nonlinear ICA always has a valid solution (to produce nonlinear independent components) <ref type="bibr">(Hyvärinen et al., 1999)</ref>, one may wonder whether, in our setting, it is possible to always find nonlinear components as functions of X that are independent in each domain, as produced by recent methods for nonlinear ICA with auxiliary variables <ref type="bibr" target="#b18">(Hyvärinen et al., 2019)</ref>. As a corollary of Theorem 2, we show that the answer is no-there do not exist nonlinear components that are independent across domains if the true latent causal DAG G Z is not an empty graph. The proof is provided in Appendix F. Corollary 1 (Impossibility of finding independent components). Let the observations be sampled from the data generating process in Eq. (1). Suppose that Assumptions A1 and A2 from Theorem 1, as well as Assumptions 1 and 2, hold, and that the true latent causal DAG G Z is not an empty graph. Suppose also that we learn (ĝ, f , p Ẑ , Θ) with the components of Ẑ being independent in each domain. Then, (ĝ, f , p Ẑ , Θ) cannot achieve Eq. (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">From Latent Markov Network to Latent Causal DAG</head><p>Now we have identified the Markov network up to an isomorphism, which characterizes conditional independence relations in the distribution. To build the connection between Markov network or conditional independence relations and causal structures, prior theory relies on the Markov and faithfulness assumptions. However, in real-world scenarios, the faithfulness assumption could be violated due to various reasons including path cancellations <ref type="bibr" target="#b47">(Zhang &amp; Spirtes, 2008;</ref><ref type="bibr" target="#b40">Uhler et al., 2013)</ref>.</p><p>Since our goal is to generalize the identifiability theory as much as possible to fit practical applications, we introduce two relaxations of the faithfulness assumptions.</p><p>Assumption 1 (Single adjacency-faithfulness (SAF)).</p><p>Given a DAG G Z and distribution P Z;θ over the variable set Z, if two variables Z i and Z j are adjacent in G Z , then</p><formula xml:id="formula_15">Z i ⊥ ̸ ⊥ Z j | Z [n]\{i,k} .</formula><p>Assumption 2 (Single unshielded-collider-faithfulness (SUCF) <ref type="bibr" target="#b29">(Ng et al., 2021)</ref>). Given a latent causal graph G Z and distribution P Z;θ over the variable set Z, let</p><formula xml:id="formula_16">Z i → Z j ← Z k be any unshielded collider in G Z , then Z i ⊥ ̸ ⊥ Z k | Z [n]\{i,k} .</formula><p>We propose SAF as a relaxtion of the Adjacencyfaithfulness <ref type="bibr" target="#b31">(Ramsey et al., 2012)</ref>. The SUCF assumption is first introduced by Ng et al. ( <ref type="formula">2021</ref>), which is strictly weaker than Orientation-faithfulness <ref type="bibr" target="#b31">(Ramsey et al., 2012)</ref>. Thus, both of them are strictly weaker than the faithfulness assumption, since the combination of Adjacency-faithfulness and Orientation-faithfulness is weaker than the faithfulness assumption <ref type="bibr" target="#b47">(Zhang &amp; Spirtes, 2008)</ref>. It is worth noting that the connection between conditional independence relations and causal structures has been developed by <ref type="bibr" target="#b28">(Loh &amp; Bühlmann, 2014;</ref><ref type="bibr" target="#b29">Ng et al., 2021)</ref> in the linear case by leveraging the properties of the inverse covariance matrix; our results here focus on the nonparametric case and thus being able to serve the considered general settings for identifiability. Also note that the necessary and sufficient conditions may also be of independent interest for other causal discovery tasks exploring conditional independence relations in the nonparametric case.</p><formula xml:id="formula_17">Interestingly</formula><p>Discussion on additional assumptions. We investigated how the sparsity constraint on the recovered graph over latent variables and sufficient change conditions on causal influences can be used to recover the latent variables and causal graph up to certain indeterminacies. Our framework is connected with previous ones in a spectrum of related studies <ref type="bibr" target="#b41">(Varici et al., 2023;</ref><ref type="bibr" target="#b1">Ahuja et al., 2023;</ref><ref type="bibr" target="#b4">Buchholz et al., 2022;</ref><ref type="bibr" target="#b36">Squires et al., 2023;</ref><ref type="bibr" target="#b3">Brehmer et al., 2022;</ref><ref type="bibr" target="#b42">von Kügelgen et al., 2023;</ref><ref type="bibr" target="#b3">Brehmer et al., 2022;</ref><ref type="bibr" target="#b42">von Kügelgen et al., 2023;</ref><ref type="bibr">Zheng &amp; Zhang, 2023;</ref><ref type="bibr" target="#b48">Zhang et al., 2023)</ref>. For instance, the connection between conditional independence and cross-derivatives of the log density in both linear and nonlinear cases means our theorems directly apply to linear SEMs. Furthermore, our results do not require the mixing function to be sufficiently nonlinear, allowing them to encompass linear mixing processes as well.</p><p>At the same time, we may be able to leverage possible parametric constraints on the data generating process (or functions) or specific types of interventions. For instance, if we know that the changes happen to the linear causal mechanisms with Gaussian noises, this constraint can readily help reduce the search space and improve the identifiability. Moreover, since we only require the changing distribution, any type of interventions will be covered since any change to the conditional distribution is allowed. Given the additional information illustrated by experimental interventions (e.g., single-node interventions), alternative identifiability that might be particularly useful in certain tasks can be established. We hope this work can provide a helpful, bigger picture of causal representation learning in the general setting and further illustrates the necessity and connections of the different assumptions formulated in this line of works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Change Encoding Network for Representation Learning</head><p>Thanks to the identifiability result, we now present two different practical implementations to recover the latent variables and their causal relations from observations from multiple domains. We build our method on the variational autoencoder (VAE) framework and can be easily extended to other models, such as normalizing flows.</p><p>We learn a deep latent generative model (decoder) p(X|Z; θ(u) ) and a variational approximation (encoder) q(Z|X, u) of its true posterior p(Z|X; θ (u) ) since the true posterior is usually intractable. To learn the model, we minimize the lower bound of the log-likelihood as log p(X; θ(u) )</p><p>= log p(X|Z; θ(u) )p(Z; θ(u) )dZ = log q(Z|X, u) q(Z|X, u) p(X|Z; θ(u) )p(Z; θ(u) )dZ</p><formula xml:id="formula_18">≥ -KL(q(Z|X, u)||p(Z; θ(u) )) + E q [log p(X|Z; θ(u) )] = -L ELBO</formula><p>For the posterior q(Z|X, u), we assume that it is a multivariate Gaussian or a Laplacian distribution, where the mean and variance are generated by the neural network encoder.</p><p>As for q(X|Z), we assume that it is a multivariate Gaussian and the mean is the output of the decoder and the variance is a pre-defined value.</p><p>In practice, we can parameterize p(X|Z; θ(u) ) as the decoder which takes as input the latent representation Z and q(Z|X, u) as an encoder which outputs the mean and scale of the posterior distribution. An essential difference between VAE <ref type="bibr" target="#b23">(Kingma &amp; Welling, 2013)</ref> and iVAE <ref type="bibr">(Khemakhem et al., 2020a</ref>) is that our method allows the components of Z to be causally dependent and we are able to learn the components and causal relationships. And the key is the prior distribution P (Z; θ(u) ). Now we present two different implementations to capture the changes with a properly defined prior distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Nonparametric Implementation of the Prior Distribution</head><p>To recover the relationships and latent variables Z, we build the normalizing flow to mimic the inverse of the latent SEM <ref type="formula">1</ref>). We first assume a causal ordering as Ẑ1 , . . . , Ẑn . Then, for each component Ẑi , we consider the previous components { Ẑ1 , . . . , Ẑi-1 } as potential parents of Ẑi and we can select the true parents with the adjacency matrix Â, where Âi,j denotes that component Ẑj contributes in the generation of Ẑi . If Âi,j = 0, it means that Ẑj will not contribute to the generation of Ẑi . Since θ (u) governs the changes across domains, we use the observed domain index u to discover the changes. Then, we use the selected parents { Âi,1 Ẑ1 , . . . , Âi,i-1 Ẑi-1 } and the domain label u to generate parameters of normalizing flow and apply the flow transformation on Ẑi to turn it into εi . Specifically, we have</p><formula xml:id="formula_19">Z i = f i (PA(Z i ), ϵ i ) in Eq. (</formula><formula xml:id="formula_20">εi , log det i = Flow( Ẑi ; NN({ Âi,j Ẑj } i-1 j=1 , u)),</formula><p>where log det i is the log determinant of the conditional flow transformation on Ẑi and NN represents a neural network.</p><p>To compute the prior distribution, we make an assumption on the noise term ϵ that it follows an independent prior distribution p(ϵ), such as a standard isotropic Gaussian or a Laplacian. Then according to the change-of-variable formula, the prior distribution of the dependent latents can be written as</p><formula xml:id="formula_21">log p( Ẑ; θ (u) ) = n i=1 (log p(ε i ) + log det i ).</formula><p>Intuitively, to minimize the KL divergence loss between p(Z; θ(u) ) and q(Z|X, u), the network has to learn the correct structure and the underlying latent variables; otherwise, it can be difficult to transform the dependent latent variables Ẑ to a factorized prior distribution, e.g., N (0, I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Parametric Implementation of the Prior Distribution</head><p>We can make parametric assumption on the latent causal process and facilitate the learning of true causal structure and components. Here, we consider the linear SEM and more complex SEMs can be generalized. Specifically, we assume that the true generation process of the latent Z is linear and only consists of scaling and shifting mechanisms:</p><formula xml:id="formula_22">Z = A(C (u) Z) + S (u) ϵ + B (u) ,</formula><p>where A ∈ [0, 1] n×n is a causal adjacency matrix which can be permuted to be strictly lower-triangular, C (u) ∈ R n×n and S (u) ∈ R n×1 are underlying domain-specific scaling matrix and vector for domain u, respectively, B (u) ∈ R n×1 is the underlying domain-specific shift vector, and ϵ is the independent noise.</p><p>To estimate the latent variables Z, the causal structure A, and capture the changes across domains, we introduce the learnable scaling Ĉ ∈ R n×n , Ŝ ∈ R n×1 and bias parameters B ∈ R n×1 and pre-define a causal ordering as Ẑ1 , Ẑ2 , . . . , Ẑn . Then we have the matrix form as</p><formula xml:id="formula_23">ε = ( Ẑ -B(u) -Â Ĉ(u) Ẑ)/ Ŝ(u) .</formula><p>Note that the determinant of the strictly lower triangular matrix Ĉ is 0. Given a prior distribution of the noise term p(ε), and according to the change-of-variable rule, we then have the prior distribution for Ẑ in parametric case as</p><formula xml:id="formula_24">log p( Ẑ; θ(u) ) = n i=1 (log p(ε i ) -log | Ŝ(u) i |).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Full Objective</head><p>After we have properly defined the needed distributions p(X|Z; θ(u) ), q(Z|X, u), p(Z; θ(u) ), we can train our model to minimize the loss function L ELBO . However, without any further constraint, the powerful network may choose to use the fully connected causal graph during training. In other words, all lower-triangular elements of the estimated graph Â is non-zero, which implies that each component Ẑi is caused by all previous i -1 components. To exclude such unwanted solutions and encourage the model to learn the true causal structure among components of Z, we apply the ℓ 1 regularization on Â, i.e.,</p><formula xml:id="formula_25">L sparsity = ∥ Â∥ 1 .</formula><p>It is worth noting that the sparsity regularization term above is an approximation of the sparsity constraint on the edges of the estimated Markov network specified in Theorems 2 and 3, since it is not straightforward to impose the latter constraint in a differentiable end-to-end training process.</p><p>A more sophisticated alternative is to impose sparsity constraint on (I -Â) T Ω -1 (I -Â) where Ω is a randomly sampled positive diagonal matrix. Note that this corresponds to the formula of precision matrix whose nonzero entries represent the moral graph under certain conditions <ref type="bibr" target="#b28">(Loh &amp; Bühlmann, 2014)</ref> and we leave it for future investigation.</p><p>Finally, the full training objective is</p><formula xml:id="formula_26">L full = L ELBO + L sparsity .</formula><p>After the model converges, the output of the encoder Ẑ is our recovered latents from the observations in multiple domains and the revealed causal structure is in Â which encapsulates the causal relationships across the components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Simulations</head><p>To verify our theory and the proposed implementations, we run experiments on the simulated data because the ground truth causal adjacency matrix and the latent variables across domains are available for simulated data. Consequently, we consider following common causal structures (i) Y-structure with 4 variables,</p><formula xml:id="formula_27">Z 1 → Z 3 ← Z 2 , Z 3 → Z 4 and (ii) chain structure Z 1 → Z 2 → Z 3 → Z 4 .</formula><p>The noises are modulated with scaling random sampled from Unif[0.5, 2] and shifts are sampled from Unif <ref type="bibr">[-2, 2]</ref>. The scaling on the Z are also randomly sampled from Unif[0.5, 2]. In other words, the changes are modular. After generating Z, we feed the latent variables into multilayer perceptron (MLP) with orthogonal weights and LeakyReLU activations for invertibility. Specifically, we sample orthogonal matrix as the weights of the MLP layers. Since orthogonal matrix and LeakyReLU are invertible, the MLP function is also invertible.</p><p>We present the results in Figures <ref type="figure" target="#fig_7">3</ref> and<ref type="figure" target="#fig_8">4</ref>. Each sub-figure consist of 4×4 panels and penal on i-th row and j-th column denotes the relationship between the estimated component Ẑi with the true latent Z j . We can see that under most cases, our model learns a strong one-to-one correspondence from the estimated components and the true components.</p><p>For instance, the first column in Figure <ref type="figure" target="#fig_7">3</ref> show that Ẑ1 is strongly correlated with the true components Z 1 while it is nearly independent from the true Z 2 .</p><p>From the estimated Â, we find that our method is able to recover the true causal structure. For instance, on the Ystructure with Z 1 → Z 3 ← Z 2 and Z 3 → Z 4 , our estimated model only keep the components Â1,3 , Â2,3 , Â3,4 nonzero with the proposed sparsity regularization. The estimated causal graph is consistent with the true Y-structure causal graph. We can also see that the latent causal structure is also recovered from Figures <ref type="figure" target="#fig_8">4</ref> and<ref type="figure" target="#fig_7">3</ref>. We observe that the learned Ẑ1 is strongly correlated with the true Z 2 and is independent from the true Z 1 , but correlated with the Ẑ3 and Ẑ4 . These results align well with the true causal graph since Z 2 is independent from Z 1 while is the cause of Z 3 and Z 4 .</p><p>The experiments support our theoretical result that the components and structure are identifiable up to certain indeterminacies. As for the results in Figure <ref type="figure" target="#fig_7">3</ref>, we observe that our non-parametric method is still able to recover the true latent variables with Laplace noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Causal representation learning aims to unearth causal latent variables and their relations from observed data. Despite its significance, the identifiability of the hidden generating process is known to be impossible without additional constraints, especially with only observational data. In the linear, non-Gaussian case, <ref type="bibr" target="#b33">Silva et al. (2006)</ref>   <ref type="bibr" target="#b41">(Varici et al., 2023)</ref> incorporate for nonlinear causal model and linear mixing function; <ref type="bibr" target="#b41">(Varici et al., 2023;</ref><ref type="bibr" target="#b5">Buchholz et al., 2023;</ref><ref type="bibr" target="#b20">Jiang &amp; Aragam, 2023)</ref> provide the identifiability of the nonparametric causal model and linear mixing function; <ref type="bibr" target="#b1">(Ahuja et al., 2023)</ref> further generalize the result to nonparametric causal model and polynomial mixing functions with additional constraints on the latent support; and <ref type="bibr" target="#b3">(Brehmer et al., 2022;</ref><ref type="bibr" target="#b42">von Kügelgen et al., 2023;</ref><ref type="bibr" target="#b20">Jiang &amp; Aragam, 2023)</ref>    Our study lies in the line of leveraging only observational data, and provides identifiability results in the general nonparametric settings on both the latent causal model and mixing function. Unlike prior works with observational data, we do not have any parametric assumptions or graphical restrictions; Compared to those relying on interventional data, our results naturally benefit from the heterogeneity of observational data (e.g., multi-domain data, nonstationary time series) and avoid additional experiments for interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Discussions</head><p>We establish a set of new identifiability results to reveal latent causal variables and latent structures in the general nonparametric settings. Specifically, with sparsity regularization during estimation and sufficient changes in the causal influences, we demonstrate that the revealed latent variables and structures are related to the underlying causal model in a specific, nontrivial way. In contrast to recent works on the recovery of latent causal variables and structures, our results rely on purely observational data without graphical or parametric constraints. Our results offer insight into unveiling the latent causal process in one of the most universal settings. Experiments in various settings have been conducted to validate the theory. As future work, we will explore the scenario where only a subset of the causal relations change, which could be a challenge as well as a chance, and show up to what extent the underlying causal variables can be recovered with potentially weaker assumptions.</p><p>• Suppose</p><formula xml:id="formula_28">Z k = Z j . Since Z i ∈ Ψ Zj , we have Z j ∈ N Zi , which implies Z k = Z j ∈ N Zi ⊊ {Z i } ∪ N Zi .</formula><p>• Suppose Z k ∈ N Zj . Clearly, Eq. ( <ref type="formula">6</ref>) is true if</p><formula xml:id="formula_29">Z k = Z i . Now suppose Z k ∈ N Zj \ {Z i }. Since Z i ∈ Ψ Zj , we have Z k ∈ N Zi ,</formula><p>which implies Eq. ( <ref type="formula">6</ref>).</p><p>In each of the cases above, Eq. ( <ref type="formula">6</ref>) is true.</p><p>Lemma 5. Given Markov network M Z over variables Z, let N Zi and Ψ Zi be the neighbors and intimate neighbors of Z i in M Z , respectively. Suppose i ̸ = j and</p><formula xml:id="formula_30">{Z i } ∪ N Zi = {Z j } ∪ N Zj . Then, we have {Z i } ∪ Ψ Zi = {Z j } ∪ Ψ Zj .</formula><p>Proof. By the given condition and Lemma 4, we have</p><formula xml:id="formula_31">Z i ∈ Ψ Zj and Z j ∈ Ψ Zi . We first prove {Z j } ∪ Ψ Zj ⊆ {Z i } ∪ Ψ Zi . That is, for each Z k ∈ {Z j } ∪ Ψ Zj , we aim to show Z k ∈ {Z i } ∪ Ψ Zi .<label>(7)</label></formula><p>We consider the following two cases:</p><formula xml:id="formula_32">• Suppose Z k = Z j . Since Z j ∈ Ψ Zi , we have Z k = Z j ∈ Ψ Zi ⊊ {Z i } ∪ Ψ Zi . • Suppose Z k ∈ Ψ Zj . By Lemma 4, we have {Z j } ∪ N Zj ⊆ {Z k } ∪ N Z k , which implies {Z i } ∪ N Zi ⊆ {Z k } ∪ N Z k by the given condition. Applying Lemma 4 again indicates Z k ∈ Ψ Zi ⊊ {Z i } ∪ Ψ Zi .</formula><p>In each of the cases above, Eq. ( <ref type="formula" target="#formula_31">7</ref>) is true, which indicates</p><formula xml:id="formula_33">{Z j } ∪ Ψ Zj ⊆ {Z i } ∪ Ψ Zi . Similar reasoning can be straightforwardly applied to prove {Z i } ∪ Ψ Zi ⊆ {Z j } ∪ Ψ Zj . Therefore, we have {Z i } ∪ Ψ Zi = {Z j } ∪ Ψ Zj .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Proposition 1</head><p>Proposition 1. Let the observations be sampled from the data generating process in Eq. ( <ref type="formula">1</ref>), and M Z be the Markov network over Z. Suppose the following assumptions hold:</p><p>• A1 (Smooth and positive density): The probability density function of latent causal variables, i.e., p Z , is twice continuously differentiable and positive in R n .</p><p>• A2 (Sufficient changes): For each value of Z, there exist 2n+|M Z |+1 values of θ, i.e., θ (u) with u = 0, . . . , 2n+|M Z |, such that the vectors w(Z, u) -w(z, 0) with u = 1, . . . , 2n + |M Z | are linearly independent, where vector w(Z, u) is defined as follows:</p><formula xml:id="formula_34">w(Z, u) = ∂ log p(Z; θ (u) ) ∂Z i i∈[n] ⊕ ∂ 2 log p(Z; θ (u) ) ∂Z 2 i i∈[n] ⊕ ∂ 2 log p(Z; θ (u) ) ∂Z i ∂Z j {Zi,Zj }∈E(M Z ), i&lt;j</formula><p>.</p><p>Suppose that we learn (ĝ, f , p Ẑ , Θ) to achieve Eq. ( <ref type="formula" target="#formula_3">2</ref>). Then, for every pair of estimated latent variables Ẑk and Ẑl that are not adjacent in the Markov network M Ẑ over Ẑ, we have the following statements:</p><p>(a) For each true latent causal variable Z i , we have</p><formula xml:id="formula_35">∂Z i ∂ Ẑk ∂Z i ∂ Ẑl = 0.</formula><p>(b) For each pair of true latent causal variables Z i and Z j that are adjacent in the Markov network M Z , we have</p><formula xml:id="formula_36">∂Z i ∂ Ẑk ∂Z j ∂ Ẑl = 0.</formula><p>Proof. Denote by vol A the volume of matrix A, which is the product of its singular values. Note that vol A = √ det AA T when A is of full row rank. In the change-of-variable formula, when the Jacobian is a rectangular matrix, the absolute determinant of the Jacobian can be replaced with the matrix volume <ref type="bibr" target="#b2">(Ben-Israel, 1999;</ref><ref type="bibr" target="#b10">Gemici et al., 2016;</ref><ref type="bibr">Khemakhem et al., 2020a)</ref>.</p><p>Since X = g(Z) and X = ĝ( Ẑ), by Eq. ( <ref type="formula" target="#formula_3">2</ref>) and the change-of-variable formula, we have</p><formula xml:id="formula_37">p X = p X =⇒ p ĝ( Ẑ) = p g(Z) =⇒ p g -1 •ĝ( Ẑ) vol J g -1 = p Z vol J g -1 =⇒ p v( Ẑ) = p Z ,</formula><p>where J g -1 is the Jacobian matrix of g -1 and v := g -1 • ĝ is a composition of diffeomorphisms (and hence also a diffeomorphism). Let J v be the Jacobian matrix of v. The change-of-variable formula implies</p><formula xml:id="formula_38">p( Ẑ; θ)| det J v -1 | = p(Z; θ) log p( Ẑ; θ) = log p(Z; θ) + log | det J v |.<label>(8)</label></formula><p>Suppose Ẑk and Ẑl are conditionally independent given Ẑ[n]\{k,l} i.e., they are not adjacent in the Markov network over Ẑ. For each θ, by <ref type="bibr" target="#b27">Lin (1997)</ref>, we have</p><formula xml:id="formula_39">∂ 2 log p( Ẑ; θ) ∂ Ẑk ∂ Ẑl = 0.<label>(9)</label></formula><p>To see what it implies, we find the first-order derivative of Eq. ( <ref type="formula" target="#formula_38">8</ref>):</p><formula xml:id="formula_40">∂ log p( Ẑ; θ) ∂ Ẑk = n i=1 ∂ log p(Z; θ) ∂Z i ∂Z i ∂ Ẑk + ∂ log | det J v | ∂ Ẑk . Let η(θ) := log p(Z; θ), η ′ i (θ) := ∂ log p(Z; θ) ∂Z i , η ′′ ij (θ) := ∂ 2 log p(Z; θ) ∂Z i ∂Z j , h ′ i,l := ∂Z i ∂ Ẑl , and h ′′ i,kl := ∂ 2 Z i ∂ Ẑk ∂ Ẑl .</formula><p>We then derive the second-order derivative w.r.t. Ẑk and Ẑl and apply Eq. ( <ref type="formula" target="#formula_39">9</ref>):</p><formula xml:id="formula_41">0 = n j=1 n i=1 ∂ 2 log p(Z; θ) ∂Z i ∂Z j ∂Z j ∂ Ẑl ∂Z i ∂ Ẑk + n i=1 ∂ log p(Z; θ) ∂Z i ∂ 2 Z i ∂ Ẑk ∂ Ẑl + ∂ 2 log | det J v | ∂ Ẑk ∂ Ẑl = n i=1 ∂ 2 log p(Z; θ) ∂Z 2 i ∂Z i ∂ Ẑl ∂Z i ∂ Ẑk + n j=1 i:{Zj ,Zi}∈E(M Z ) ∂ 2 log p(Z; θ) ∂Z i ∂Z j ∂Z j ∂ Ẑl ∂Z i ∂ Ẑk + n i=1 ∂ log p(Z; θ) ∂Z i ∂ 2 Z i ∂ Ẑk ∂ Ẑl + ∂ 2 log | det J v | ∂ Ẑk ∂ Ẑl (10) = n i=1 η ′′ ii (θ)h ′ i,l h ′ i,k + n j=1 i:{Zj ,Zi}∈E(M Z ) η ′′ ij (θ)h ′ j,l h ′ i,k + n i=1 η ′ i (θ)h ′′ i,kl + ∂ 2 log | det J v | ∂ Ẑk ∂ Ẑl . (<label>11</label></formula><formula xml:id="formula_42">)</formula><p>Recall that E(M Z ) denotes the set of edges in the Markov network over Z. In the equation above, we made use of the fact that if Z i and Z j are not adjacent in the Markov network, then </p><formula xml:id="formula_43">0 = n i=1 (η ′′ ii (θ (u) )-η ′′ ii (θ (0) ))h ′ i,l h ′ i,k + n j=1 i:{Zj ,Zi}∈E(M Z ) (η ′′ ij (θ (u) )-η ′′ ij (θ (0) ))h ′ j,l h ′ i,k + n i=1 (η ′ i (θ (u) )-η ′ i (θ (0) ))h ′′ i,kl ,<label>(12)</label></formula><p>where u = 1, . . . , 2n + |M Z |. Since p Z is twice continuously differentiable, we have</p><formula xml:id="formula_44">η ′′ ij (θ (u) ) -η ′′ ij (θ (0) ) = η ′′ ji (θ (u) ) -η ′′ ji (θ (0) ),</formula><p>and therefore Eq. ( <ref type="formula" target="#formula_43">12</ref>) can be written as</p><formula xml:id="formula_45">0 = n i=1 (η ′′ ii (θ (u) ) -η ′′ ii (θ (0) ))h ′ i,l h ′ i,k + i,j: i&lt;j, {Zi,Zj }∈E(M Z ) (η ′′ ij (θ (u) ) -η ′′ ij (θ (0) ))(h ′ j,l h ′ i,k + h ′ i,l h ′ j,k ) + n i=1 (η ′ i (θ (u) ) -η ′ i (θ (0) ))h ′′ i,kl .</formula><p>Consider the vectors formed by collecting the corresponding coefficients in the equation above where u = 1, . . . , 2n + |M Z |.</p><p>By Assumption A2, these 2n + |M Z | vectors are linearly independent. Thus, for any i and j such that {Z i , Z j } ∈ E(M Z ), we have the following equations:</p><formula xml:id="formula_46">h ′ i,k h ′ i,l = 0,<label>(13)</label></formula><formula xml:id="formula_47">h ′ i,k h ′ j,l + h ′ j,k h ′ i,l = 0,<label>(14)</label></formula><formula xml:id="formula_48">h ′′ i,kl = 0. It remains to show h ′ i,k h ′ j,l = 0. Suppose by contradiction that h ′ i,k h ′ j,l ̸ = 0,<label>(15)</label></formula><p>which implies h ′ i,k ̸ = 0. By Eq. ( <ref type="formula" target="#formula_46">13</ref>), we have h ′ i,l = 0, which, by plugging into Eq. ( <ref type="formula" target="#formula_47">14</ref>), indicates h ′ i,k h ′ j,l = 0. This is a contradiction with Eq. ( <ref type="formula" target="#formula_48">15</ref>). Thus, we must have h ′ i,k h ′ j,l = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proof of Theorem 2</head><p>Theorem 2 (Identifiability of latent Markov network). Let the observations be sampled from the data generating process in Eq. ( <ref type="formula">1</ref>), and M Z be the Markov network over Z. Suppose that Assumptions A1 and A2 from Theorem 1 hold. Suppose also that we learn (ĝ, f , p Ẑ , Θ) to achieve Eq. ( <ref type="formula" target="#formula_3">2</ref>) with the minimal number of edges of the Markov network M Ẑ over Ẑ. Then, the recovered latent Markov network M Ẑ is isomorphic to the true latent Markov network M Z .</p><p>Proof. Let v := g -1 • ĝ, i.e., Z = v( Ẑ). Note that v is a composition of diffeomorphisms, and hence also a diffeomorphism. Consider a specific value of Ẑ, say ẑ. Since v is diffeomorphism, by Lemma 2, there exists a permutation π such that the diagonal entries of the corresponding Jacobian matrix (whose columns are permuted according to π) evaluated at Ẑ = ẑ are nonzero, i.e.,</p><formula xml:id="formula_49">∂Z i ∂ Ẑπ(i) Ẑ=ẑ ̸ = 0, i = 1, . . . , n.<label>(16)</label></formula><p>Suppose that Z i and Z j are adjacent in the Markov network M Z over Z, but Ẑπ(i) and Ẑπ(j) are not adjacent in the Markov network M Ẑ over Ẑ. By Proposition 1, we have</p><formula xml:id="formula_50">∂Z i ∂ Ẑπ(i) Ẑ=ẑ ∂Z j ∂ Ẑπ(j) Ẑ=ẑ = 0,</formula><p>which is clearly a contradiction with Eq. ( <ref type="formula" target="#formula_49">16</ref>).</p><p>Thus, we have shown by contradiction the following lemma. Lemma 6. If Z i and Z j are adjacent in the Markov network M Z over Z, then Ẑπ(i) and Ẑπ(j) are adjacent in the Markov network M Ẑ over Ẑ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The lemma above indicates |M</head><formula xml:id="formula_51">Ẑ | ≥ |M Z |.<label>(17)</label></formula><p>Also, note that the true model (g, f, p Z , Θ) is one of the solutions that achieves Eq. ( <ref type="formula" target="#formula_3">2</ref>). Since the recovered latent Markov network M Ẑ has the minimal number of edges among the solutions that achieve Eq. ( <ref type="formula" target="#formula_3">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proof of Theorem 1</head><p>Theorem 1 (Relations among true and recovered latent causal variables). Let the observations be sampled from the data generating process in Eq. ( <ref type="formula">1</ref>), and M Z be the Markov network over Z. Suppose that Assumptions A1 and A2 from Theorem 1 hold. Suppose also that we learn (ĝ, f , p Ẑ , Θ) to achieve Eq. ( <ref type="formula" target="#formula_3">2</ref>) with the minimal number of edges of the Markov network M Ẑ over Ẑ. Then, for every pair of estimated latent variables Ẑk and Ẑl that are not adjacent in the Markov network M Ẑ over Ẑ, we have the following statements:</p><p>(a) Each true latent causal variable Z i is a function of at most one of Ẑk and Ẑl .</p><p>(b) For each pair of true latent causal variables Z i and Z j that are adjacent in the Markov network M Z over Z, at most one of them is a function of Ẑk or Ẑl .</p><p>Proof. We first prove Statement (a). By Proposition 1, for every value of Z, we have</p><formula xml:id="formula_52">∂Z i ∂ Ẑk ∂Z i ∂ Ẑl = 0.</formula><p>Therefore, it suffices to prove that if ∂Zi ∂ Ẑk ̸ = 0 for some value of Ẑ, then ∂Zi ∂ Ẑl = 0 for all values of Ẑ. That is, these nonzero entries cannot switch positions.</p><p>By Theorem 2, there exists a permutation π of the estimated variables, denoted as Ẑπ , such that the Markov network M Ẑπ is identical to M Z . <ref type="foot" target="#foot_3">4</ref> Let Ẑπ(i) and Ẑπ(k) be two estimated latent variables that are not adjacent in the Markov network M Ẑπ . Now consider variable Z i . Suppose by contradiction that the nonzero entries switch positions, i.e., there exist two values of Ẑ, say ẑ(1) and ẑ(2) , such that</p><formula xml:id="formula_53">∂Z i ∂ Ẑπ(i) Ẑ=ẑ (1) ̸ = 0<label>(18)</label></formula><p>and</p><formula xml:id="formula_54">∂Z i ∂ Ẑπ(k) Ẑ=ẑ (2) ̸ = 0,<label>(19)</label></formula><p>Let N Zi be a set containing the indices of the neighbors of Z i in M Z , and N Ẑπ(i) be a set containing the indices of the neighbors of Z π(i) in M Ẑπ . Similarly, let S Zi be a set containing the indices of the variables that are not adjacent to Z i in M Z , and S Ẑπ(i) be a set containing the indices of the variables that are not adjacent to of Z π(i) in M Ẑπ . By definition, we have</p><formula xml:id="formula_55">N Zi ∪ S Zi ∪ {i} = [n],<label>(20)</label></formula><p>which are pairwise disjoint.</p><p>Since M Ẑπ and M Z are identical, we have N Zi = N Ẑπ(i) and S Zi = S Ẑπ(i) . Now define the following function</p><formula xml:id="formula_56">ϕ( Ẑ) = j∈N Z i ∪{i} ∂Z j ∂ Ẑπ(i) 2 - l∈S Z i j∈N Z i ∪{i} ∂Z j ∂ Ẑπ(l) 2 .</formula><p>Plugging in Ẑ = ẑ(1) , for l ∈ S Zi = S Ẑπ(i) and j ∈ N Zi ∪ {i}, Proposition 1 implies</p><formula xml:id="formula_57">∂Z i ∂ Ẑπ(i) Ẑ=ẑ (1) ∂Z j ∂ Ẑπ(l) Ẑ=ẑ (1) = 0,</formula><p>which, with Eq. ( <ref type="formula" target="#formula_53">18</ref>), indicates ∂Z j ∂ Ẑπ(l) Ẑ=ẑ (1) = 0.</p><p>Substituting the above equation and Eq. ( <ref type="formula" target="#formula_53">18</ref>) into function ϕ, we have</p><formula xml:id="formula_58">ϕ( Ẑ)| Ẑ=ẑ (1) = j∈N Z i ∪{i} ∂Z j ∂ Ẑπ(i) Ẑ=ẑ (1) 2 ≥ ∂Z i ∂ Ẑπ(i) Ẑ=ẑ (1) 2 &gt; 0.<label>(21)</label></formula><p>Now plug in Ẑ = ẑ(2) . For j ∈ N Zi ∪ {i}, Proposition 1 implies</p><formula xml:id="formula_59">∂Z j ∂ Ẑπ(i) Ẑ=ẑ (2) ∂Z i ∂ Ẑπ(k) Ẑ=ẑ (2) = 0,</formula><p>which, with Eq. ( <ref type="formula" target="#formula_54">19</ref>), indicates</p><formula xml:id="formula_60">∂Z j ∂ Ẑπ(i) Ẑ=ẑ (2) = 0.</formula><p>Substituting the above equation and Eq. ( <ref type="formula" target="#formula_54">19</ref>) into function ϕ, we have</p><formula xml:id="formula_61">ϕ( Ẑ)| Ẑ=ẑ (2) = - l∈S Z i j∈N Z i ∪{i} ∂Z j ∂ Ẑπ(l) Ẑ=ẑ (2) 2 ≤ - ∂Z i ∂ Ẑπ(k) Ẑ=ẑ (2) 2 &lt; 0.<label>(22)</label></formula><p>Since function ϕ is continuous (because all the partial derivatives involved are continuous) and its domain is a connected set, by applying Intermediate Value Theorem with Eqs. ( <ref type="formula" target="#formula_58">21</ref>) and ( <ref type="formula" target="#formula_61">22</ref>), there exists a value of Ẑ in the domain, say ẑ(3) , such that</p><formula xml:id="formula_62">ϕ( Ẑ)| Ẑ=ẑ (3) = 0,</formula><p>which, by plugging the definition of function ϕ, implies</p><formula xml:id="formula_63">j∈N Z i ∪{i} ∂Z j ∂ Ẑπ(i) Ẑ=ẑ (3) 2 = l∈S Z i j∈N Z i ∪{i} ∂Z j ∂ Ẑπ(l) Ẑ=ẑ (3) 2 .</formula><p>Note that if any of the terms in the summation on the left hand side (LHS) is nonzero, then, by Proposition 1, all terms in the summation on the right hand side (RHS) must be zero; in this case, LHS is nonzero but RHS equals zero, which is a contradiction. Similarly, if any of the terms in the summation on the RHS is nonzero, then, by Proposition 1, all terms in the summation on the LHS must be zero; in this case, RHS is nonzero but LHS equals zero, which is a contradiction. This implies that all terms in the summation on both LHS and RHS must be zero, i.e.,</p><formula xml:id="formula_64">∂Z j ∂ Ẑπ(l) Ẑ=ẑ (3) = 0 for j ∈ N Zi ∪ {i}, l ∈ S Zi ∪ {i}.</formula><p>Since |S Zi ∪ {i}| = n -|N Zi | by Eq. ( <ref type="formula" target="#formula_55">20</ref>), Lemma 3 indicates that the matrix ∂Z ∂ Ẑπ Ẑ=ẑ (3) is not invertible. Thus, the (Jacobian) matrix ∂Z ∂ Ẑ Ẑ=ẑ (3) is also not invertible, which is a contradiction because the mapping from Ẑ to Z is a diffeomorphism (specifically a a composition of diffeomorphisms).</p><p>Therefore, we have just proved Statement (a) by contradiction. Similar reasoning can be straightforwardly applied to prove Statement (b) and is omitted here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Proof of Theorem 3</head><p>We first state the following lemma that is used to prove Theorem 3. The proof is a straightforward consequence of Cayley-Hamilton theorem and is omitted here. Lemma 7. Let A be an n × n invertible matrix. Then, it can be expressed as a linear combination of the powers of A, i.e.,</p><formula xml:id="formula_65">A -1 = n-1 k=0 c k A k</formula><p>for some appropriate choice of coefficients c 0 , c 1 , . . . , c n-1 .</p><p>To refine further, now suppose that Z j is adjacent to Z i , but not adjacent to some Z k ∈ N Zi \ {Z j }. Since M Z and M Ẑπ are identical, Ẑπ(j) is also not adjacent to Ẑπ(k) in M Ẑπ . Since Z i and Z k are adjacent in M Z , by Theorem 1, at most one of them is a function of Ẑπ(j) or Ẑπ(k) . This implies that Z i cannot be a function of Ẑπ(j) , because we have shown that Z k is a function of Ẑπ(k) . Therefore, we have just shown that Z i is solely a function of Ẑπ(i) and a subset of { Ẑπ(r) | Z r ∈ Ψ Zi }. Now consider variable Z l ̸ ∈ {Z i } ∪ Ψ Zi . Since Z i is not a function of Ẑπ(l) , we have Since the above equation holds for all values of Z, we conclude that Ẑπ(i) cannot be a function of Z l .</p><p>We are now ready to prove our main identifiability result of the latent causal variables. The only difference with Proposition 4 is that the following result shows that Ẑπ(i) must be a function of Z i for permutation π.</p><p>Theorem 3 (Identifiability of latent causal variables). Let the observations be sampled from the data generating process in Eq. ( <ref type="formula">1</ref>). Suppose that Assumptions A1 and A2 from Theorem 1 hold. Suppose also that we learn (ĝ, f , p Ẑ , Θ) to achieve Eq. ( <ref type="formula" target="#formula_3">2</ref>) with the minimal number of edges of the Markov network M Ẑ over Ẑ. Then, there exists a permutation π of the estimated latent variables, denoted as Ẑπ , such that each Ẑπ(i) is solely a function of Z i and a subset of Ψ Zi .</p><p>Proof. By Proposition 4, there exists a permutation σ of the estimated latent variables, denoted as Ẑσ , such that each Ẑσ(i) is solely a function of a subset of {Z i } ∪ Ψ Zi .</p><p>By Lemma 2, there exists a permutation π such that the diagonal entries of the Jacobian matrix ∂ Ẑπ ∂Z are nonzero. Considering the transformation from Ẑσ to Ẑπ , the variables are transformed according to π • σ -1 , which is also a permutation. Note that any permutation (on finitely many elements) can be decomposed into disjoint cyclic subpermutations <ref type="bibr" target="#b9">(Ehrlich, 2013)</ref>. These subpermutations may include trivial ones, where a single element remains in its original position.</p><p>Consider variable Ẑπ(i) . Suppose that it was involved in a trivial cyclic subpermutation (from Ẑσ to Ẑπ ), i.e., π(i) = σ(i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Since the diagonal entries of ∂ Ẑπ</head><p>∂Z are nonzero by definition, clearly Ẑπ(i) is a function of Z i . Also, we have</p><formula xml:id="formula_66">∂ Ẑπ(i) ∂Z = ∂ Ẑσ(i)</formula><p>∂Z , which indicates that Ẑπ(i) is solely a function of Z i and a subset of Ψ Zi . Now suppose that Ẑπ(i) was involved in a nontrivial cyclic subpermutation (from Ẑσ to Ẑπ ). That is, there exists a sequence j 1 , . . . , j k such that π(j 1 ) = σ(i), π(j 2 ) = σ(j 1 ), π(j 3 ) = σ(j 2 ), . . . , π(j k ) = σ(j k-1 ), π(i) = σ(j k ).</p><p>(23)</p><p>Since the diagonal entries of ∂ Ẑπ ∂Z are nonzero by definition, we have</p><formula xml:id="formula_67">∂ Ẑσ(i) ∂Z j1 ̸ = 0, ∂ Ẑσ(j1) ∂Z j2 ̸ = 0, ∂ Ẑσ(j2) ∂Z j3 ̸ = 0, . . . , ∂ Ẑσ(j k-1 ) ∂Z j k ̸ = 0, ∂ Ẑσ(j k ) ∂Z i ̸ = 0,<label>(24)</label></formula><p>which indicate</p><formula xml:id="formula_68">Z j1 ∈ Ψ Zi , Z j2 ∈ Ψ Zj 1 , Z j3 ∈ Ψ Zj 2 , . . . , Z j k ∈ Ψ Zj k-1 , Z i ∈ Ψ Zj k .</formula><p>• If variables Z j and Z k correspond to a pair of non-adjacent spouses in G Z . Then they have an unshielded collider, indicating that the SUCF assumption is violated.</p><p>Necessary condition. We prove it by contradiction. Suppose SUCF or SAF is violated, we have the following two cases:</p><p>• Suppose SUCF is violated, i.e., there exists an unshielded collider Z j → Z i ← Z k in the DAG G Z such that Z j ⊥ ⊥ Z k | Z [n]\{j,k} . This conditional independence relation indicates that {Z j , Z k } / ∈ E(M Z ). Since Z j and Z k are spouses, there exists an edge between them in the moralized graph of G Z , but is not contained in the structure defined by M Z , showing that they are not the same.</p><p>• Suppose SAF is violated, i.e., there exists a pair of neighbors Z j and Z k in the DAG G Z such that Z j ⊥ ⊥ Z k | Z [n]\{j,k} .</p><p>This conditional independence relation indicates that {Z j , Z k } / ∈ E(M Z ). Because Z j and Z k are adjacent in G Z , clearly they are also adjacent in the moralized graph of G Z . However, the edge between them is not contained in the structure defined by M Z , showing that they are not the same. Thus, when or SAF is violated, the structure defined by M Z is the moralized graph of the true DAG G Z .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The generating process for each latent causal variable Z i changes, governed by a latent factor θ i . The observed variables X are generated by X = g(Z) with a nonlinear mixing function g.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>all samples of Z, or ∂Zi ∂ Ẑl</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and Ψ Z5 = {Z 4 }. As another example, according to the Markov network in Figure 2(b), which is implied by the DAG in Figure 2(a), we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustrative example 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>∂Ẑ</head><label></label><figDesc>, indicating that the same entries remain zero in ∂Z ∂ Ẑ -1 (because the inverse ∂Z ∂ Ẑ -1 can be written as a linear combination of the powers of ∂Z ∂ Ẑ ) and thus ∂ Ẑ ∂Z , from which the identifiability result can be derived.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>explore the nonparametric settings for both the causal model and mixing function. In addition to the single-node perfect interventions, Brehmer et al. (2022) introduced counterfactual pre-and post-intervention views; von Kügelgen et al. (2023) assume two distinct, paired interventions per node for multivariate causal models; Zhang et al. (2023) explore soft interventions on polynomial mixing functions; and Jiang &amp; Aragam (2023) places specific structural restrictions on the latent causal graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Recovered latent variables v.s. the true latent variables with Non-Parametric Approach. (a) Y-structure with Laplace noise. (b) Y-structure with Gaussian noise. (c) Chain structure with Laplace noise. (d) Chain structure with Gaussian noise. In each sub-figure, i-th row and j-th column depcits the relationship between the estimated Ẑi and the true components Z j .</figDesc><graphic coords="9,61.42,252.71,112.32,112.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Recovered latent variables v.s. the true latent variables with Linear Parameterization Approach. The X-axis denotes the components of true latent variables Z and the Y -axis represent the components of estimated latent variables Ẑ. (a) Y-structure with Laplace noise. (b) Y-structure with Gaussian noise. (c) Chain structure with Laplace noise. (d) Chain structure with Gaussian noise.</figDesc><graphic coords="9,185.70,252.71,112.32,112.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>, consider the 2n + |M Z | + 1 values of θ, i.e., θ (u) with u = 0, . . . , 2n + |M Z |, such that Eq. (11) hold. Then, we have 2n + |M Z | + 1 such equations. Subtracting each equation corresponding to θ (u) , u = 1, . . . , 2n + |M Z | with the equation corresponding to θ (0) results in 2n + |M Z | equations:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Equal contribution 1 Carnegie Mellon University 2 Mohamed bin Zayed University of Artificial Intelligence. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).</figDesc><table /><note><p>*</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, not only they are weaker variants of faithfulness, but we also prove that they are actually necessary and sufficient conditions, thus the weakest possible ones, to bridge conditional independence relations and causal structures. Specifically, we show that the recovered Markov network (e.g., in Theorem 2) is exactly the moralized graph of the true causal DAG if and only if the proposed variants of faithfulness hold. The proofs of Lemma 1 and Proposition 2 are shown in Appendix G.</figDesc><table /><note><p><p>Lemma 1. Given a latent causal graph G Z and distribution P Z;θ with its Markov Network M Z , under Markov assumption, the undirected graph defined by M Z is a subgraph of the moralized graph of the true causal DAG G.</p>Proposition 2 (Moralized graph and Markov network). Given a causal DAG G Z and distribution P Z;θ with its Markov Network M Z , under Markov assumption, the undirected graph defined by M Z is the moralized graph of the true causal DAG G Z if and only if the SAF and SUCF assumptions are satisfied.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>recover the Markov equivalence class, provided that each observed variable has a unique latent causal parent;<ref type="bibr" target="#b43">Xie et al. (2020)</ref>;<ref type="bibr" target="#b6">Cai et al. (2019)</ref> estimate the latent variables and their relations assuming at least twice measured variables as latent ones, which has been further extended to learn the latent hierarchical structure<ref type="bibr" target="#b44">(Xie et al., 2022)</ref>. Moreover,<ref type="bibr" target="#b0">Adams et al. (2021)</ref> provide theoretical results on the graphical conditions for identification. In the linear, Gaussian case,<ref type="bibr" target="#b13">Huang et al. (2022)</ref> leverage rank deficiency of the observed sub-covariance matrix to estimate the latent hierarchical structure, while<ref type="bibr" target="#b8">Dong et al. (2023)</ref> further extend the rank constraint to accommodate flexibly related latent and observed variables. In the discrete case,<ref type="bibr" target="#b24">Kivva et al. (2021)</ref> identify the latent causal graph up to Markov equivalence by assuming a mixture model where the observed children sets of any pair of latent variables are different.</figDesc><table><row><cell>Given the challenge of identifiability on purely observational</cell></row><row><cell>data, a different line of research leverage experiments by</cell></row><row><cell>assuming the accessibility of various types of interventional</cell></row><row><cell>data. Based on the single-node perfect intervention, Squires</cell></row><row><cell>et al. (2023) leverage single-node interventions for the iden-</cell></row><row><cell>tifiability of linear causal model and linear mixing function;</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>With a slight abuse of notation, we use the same capital letters X and Z to denote the variables and their values when the context is clear.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We use[n]  to denote {1, . . . , n} and Z [n]\{i,j} to denote {Zi} n i=1 \ {Zi, Zj}.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We denote by ⊕ the vector concatenation symbol. Also, the order in the mixed partial derivatives can be interchanged.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The Markov networks MZ and M Ẑπ are identical in the sense that Zi and Zj are adjacent in MZ if and only if Ẑπ(i) and Ẑπ(j) are adjacent in M Ẑπ .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>The Markov networks MZ and M Ẑπ are identical in the sense that Zi and Zj are adjacent in MZ if and only if Ẑπ(i) and Ẑπ(j) are adjacent in M Ẑπ .</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors would like to thank the anonymous reviewers for helpful comments and suggestions. The authors would also like to acknowledge the support from <rs type="funder">NSF</rs> Grant <rs type="grantNumber">2229881</rs>, the <rs type="funder">National Institutes of Health (NIH)</rs> under Contract <rs type="grantNumber">R01HL159805</rs>, and grants from <rs type="funder">Apple Inc., KDDI Research Inc.</rs>, <rs type="person">Quris AI</rs>, and <rs type="institution">Florin Court Capital</rs>.   </p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CDC72xx">
					<idno type="grant-number">2229881</idno>
				</org>
				<org type="funding" xml:id="_v6kH9VM">
					<idno type="grant-number">R01HL159805</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact Statement</head><p>This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material A. Proofs of Useful Lemmas</head><p>A.1. Proof of Lemma 2</p><p>The following lemma is a rather standard result in linear algebra <ref type="bibr" target="#b37">(Strang, 2006;</ref><ref type="bibr" target="#b38">2016)</ref>, which has also been used in existing works in causal representation learning, such as <ref type="bibr" target="#b26">Lachapelle et al. (2022)</ref>. We provide the proof here for completeness.</p><p>Lemma 2. For any invertible matrix A, there exists a permutation of its columns such that the diagonal entries of the resulting matrix are nonzero.</p><p>Proof. Suppose by contradiction that there exists at least a zero diagonal entry for every column permutation. By Leibniz formula, we have</p><p>where S n denotes the set of n-permutations. Since there exists a zero diagonal entry for every permutation, we have</p><p>which implies det(A) = 0 and that matrix A is not invertible. This is a contradiciton with the assumption that A is invertible.</p><p>Proof. By the given condition, there exist n -i columns in matrix A of which i + 1 rows are zero, i.e., at most n -i -1 rows are not zero. This implies that the n -i column vectors span a subspace of dimension less than n -i, which thus are linearly dependent. Therefore, matrix A cannot be invertible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Proofs of Lemmas 4 and 5</head><p>We provide the following lemmas that will be used to prove Theorem 3. Lemma 4. Given Markov network M Z over variables Z, let N Zi and Ψ Zi be the neighbors and intimate neighbors of Z i in M Z , respectively. Then, for each i ̸ = j, we have</p><p>Proof. We prove both directions as follows.</p><p>Sufficient condition. We proceed by contraposition. Suppose Z i ̸ ∈ Ψ Zj . We consider the following two cases:</p><p>• Suppose Z i ∈ N Zj and that there exists</p><p>Therefore, we have shown that if</p><p>Necessary condition. For each Z k ∈ {Z j } ∪ N Zj , we aim to show</p><p>We consider the following two cases:</p><p>Now consider the Markov network M Z over variables Z. With a slight abuse of notation, let N Zi be the set of neighbors of Z i in M Z . The following result relates a matrix to its inverse, given that the matrix satisfies certain property defined by M Z .</p><p>Proposition 3. Consider Markov network M Z over Z. Let N Zi be the set of neighbors of Z i in M Z , and A be an n × n invertible matrix. For each i ̸ = j where Z j is not adjacent to some nodes in</p><p>Proof. By Lemma 7, A -1 can be expressed as linear combination of the powers of A. Therefore, it suffices to prove that each matrix power A k satisfies the following property: A k ij = 0 for each i ̸ = j where Z j is not adjacent to some nodes in {Z i } ∪ (N Zi \ {Z j }). We proceed with mathematical induction on k. By definition, the property holds in the base case where k = 1. Now suppose that the property holds for A k . We prove by contradiction that the property holds for A k+1 . Suppose by contradiction that A k+1 ij ̸ = 0 for some i ̸ = j where Z j is not adjacent to some nodes in {Z i } ∪ (N Zi \ {Z j }). This implies that one of the following cases holds: <ref type="figure">A k</ref> im ̸ = 0 and A mj ̸ = 0. Since both A k and A satisfy the property, this indicates (i) Z m is adjacent to Z i and all nodes in N Zi \ {Z m }, and (ii) Z j is adjacent to Z m and all nodes in N Zm \ {Z j }. We consider the following cases:</p><p>• Case of m = l: By (ii), Z j is adjacent to Z l , which contradicts Case (b) above. Also, we know that Z l is adjacent to Z i by (i), which indicates that Z i is adjacent to Z j , contradicting Case (a) above.</p><p>• Case of m ̸ = l: By (i) and (ii), Z m is adjacent to Z i and Z j is adjacent to Z m , implying that Z i and Z j are adjacent, which is contradictory with Case (a) above. Furthermore, since Z l is a neighbor of Z i , we know that Z m and Z l are adjacent by (i). Also, by (ii), Z j is adjacent to Z l , which contradicts Case (b) above.</p><p>In each of the cases above, there is a contradiction.</p><p>Before proving Theorem 3, we first establish a weaker form of the identifiability result below. This result may be of interest on its own, as it provides useful information for disentanglement. Proposition 4. Let the observations be sampled from the data generating process in Eq. ( <ref type="formula">1</ref>). Suppose that Assumptions A1 and A2 from Theorem 1 hold. Suppose also that we learn (ĝ, f , p Ẑ , Θ) to achieve Eq. ( <ref type="formula">2</ref>) with the minimal number of edges of the Markov network M Ẑ over Ẑ. Then, there exists a permutation π of the estimated latent variables, denoted as Ẑπ , such that each Ẑπ(i) is solely a function of a subset of {Z i } ∪ Ψ Zi .</p><p>Proof. We first prove a simpler case: there exists a permutation π of the estimated latent variables, denoted as Ẑπ , such that Z i is solely a function of Ẑπ(i) and a subset of</p><p>By Theorem 2 and its proof, there exists a permutation π of the estimated variables, denoted as Ẑπ , such that the Markov network M Ẑπ over Ẑπ is identical to M Z , 5 and that</p><p>Clearly, each variable Z i is a function of Ẑπ(i) .</p><p>We first show that if Z j is not adjacent to Z i in M Z , then Z i cannot be a function of Ẑπ(j) . Since Z i and Z j are not adjacent in M Z , we know that Ẑπ(i) and Ẑπ(j) are not adjacent in M Ẑπ . By Theorem 1, Z i is a function of at most one of Ẑπ(i) and Ẑπ(j) , which implies that Z i cannot be a function of Ẑπ(j) , because we have shown that Z i is a function of Ẑπ(i) .</p><p>By Lemma 4 and the above equation, we have</p><p>and thus</p><p>Applying Lemma 5 with the above equation implies</p><p>Recall that, by definition, Ẑσ(j k ) is solely a function of a subset of {Z j k } ∪ Ψ Zj k . By the equation above, Ẑσ(j k ) is solely a function of a subset of {Z i } ∪ Ψ Zi . Since we have π(i) = σ(j k ) by Eq. ( <ref type="formula">23</ref>), Ẑπ(i) is solely a function of a subset of {Z i } ∪ Ψ Zi . Furthermore, we have</p><p≯ = 0 by Eq. ( <ref type="formula">24</ref>), which implies that Ẑπ(i) is a function of Z i .</p><p>Therefore, Ẑπ(i) is solely a function of Z i and a subset of Ψ Zi .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Proof of Corollary 1</head><p>Corollary 1 (Impossibility of finding independent components). Let the observations be sampled from the data generating process in Eq. ( <ref type="formula">1</ref>). Suppose that Assumptions A1 and A2 from Theorem 1, as well as Assumptions 1 and 2, hold, and that the true latent causal DAG G Z is not an empty graph. Suppose also that we learn (ĝ, f , p Ẑ , Θ) with the components of Ẑ being independent in each domain. Then, (ĝ, f , p Ẑ , Θ) cannot achieve Eq. ( <ref type="formula">2</ref>).</p><p>Proof. Suppose by contradiction that (ĝ, f , p Ẑ , Θ) achieves Eq. ( <ref type="formula">2</ref>). By assumption, the components of Ẑ are independent in each domain, indicating that the Markov network M Ẑ is an empty graph. By the same reasoning in the proof of Theorem 2 (specifically Lemma 6), there exists a permutation π such that: if Z i and Z j are adjacent in M Z , then Ẑπ(i) and Ẑπ(j) are adjacent in M Ẑ . Since M Ẑ is an empty graph, this implies that M Z is also an empty graph. Under Assumptions 1 and 2, Proposition 2 indicates that the undirected graph defined by M Z is the moralized graph of G Z . Therefore, the moralized graph of G Z , and thus G Z itself, are empty, contradicting the assumption that G Z is not an empty graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Proof of Lemma 1 and Proposition 2</head><p>Lemma 1. Given a latent causal graph G Z and distribution P Z;θ with its Markov Network M Z , under Markov assumption, the undirected graph defined by M Z is a subgraph of the moralized graph of the true causal DAG G.</p><p>Proof. Let Z j and Z k , j ̸ = k be two variables that are not adjacent in the moralized graph of G Z . Then it suffices to show that {Z j , Z k } / ∈ E(M Z ). Because they are not adjacent in the moralized graph of G Z , they must not be adjacent in G Z and must not share a common child in G Z . Thus, Z j and Z k are d-separated conditioning on Z [n]\{j,k} , which implies the conditional independence</p><p>Proposition 2 (Moralized graph and Markov network). Given a causal DAG G Z and distribution P Z;θ with its Markov Network M Z , under Markov assumption, the undirected graph defined by M Z is the moralized graph of the true causal DAG G Z if and only if the SAF and SUCF assumptions are satisfied.</p><p>Proof. We prove both directions as follows.</p><p>Sufficient condition. We prove it by contradiction. Suppose that the structure defined by M Z is not equivalent to the moralized graph of G Z . Then, according to Lemma 1, there exists a pair of variables Z j and Z k , j ̸ = k that are adjacent in the moralized graph but {Z j , Z k } / ∈ E(M Z ). Thus, we have Z j ⊥ ⊥ Z k | Z [n]\{j,k} . Then we consider the following two cases:</p><p>• If variables Z j and Z k correspond to a pair of neighbors in G Z , then they are adjacent. Together with the conditional independence relation Z j ⊥ ⊥ Z k | Z [n]\{j,k} , this implies that the SAF assumption is violated.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Identification of partially observed linear causal models: Graphical conditions for the non-gaussian and heterogeneous cases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="22822" to="22833" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interventional causal representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="372" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The change-of-variables formula using matrix volume</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Israel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Siam Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised causal representation learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="38319" to="38331" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Function classes for identifiable nonlinear independent component analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.06406</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning linear causal representations from interventions under general nonlinear mixing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.02235</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Triad constraints for learning causal structure of latent variables</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Independent component analysis -a new concept?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="287" to="314" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A versatile causal discovery framework to allow causally-related hidden variables</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Legaspi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fundamental Concepts of Abstract Algebra</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ehrlich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Dover Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Gemici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02304</idno>
		<title level="m">Normalizing flows on Riemannian manifolds</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hidden markov nonlinear ICA: Unsupervised learning from nonstationary time series</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hälvä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="939" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Disentangling identifiable features from noisy data with structured nonlinear ICA</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hälvä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Le Corff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lehéricy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gassiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Latent hierarchical causal structure discovery with rank constraints</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J H</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="5549" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised feature extraction by time-contrastive learning and nonlinear ICA</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Morioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3765" to="3773" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonlinear ICA of temporally dependent stationary sources</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Morioka</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="460" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonlinear independent component analysis: Existence and uniqueness results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pajunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="429" to="439" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A fast algorithm for estimating overcomplete ICA bases for image windows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cristescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. on Neural Networks</title>
		<meeting>Int. Joint Conf. on Neural Networks<address><addrLine>Washington, D.C.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="894" to="899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nonlinear ICA using auxiliary variables and generalized contrastive learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="859" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nonlinear independent component analysis for principled disentanglement in unsupervised deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Morioka</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patter.2023.100844</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S2666389923002234" />
	</analytic>
	<monogr>
		<title level="j">Patterns</title>
		<idno type="ISSN">2666-3899</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">100844</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning nonparametric latent causal graphs with unknown interventions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.02899</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Variational autoencoders and nonlinear ICA: A unifying framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2207" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ice-beem: Identifiable conditional energy-based deep models based on nonlinear ica</title>
		<author>
			<persName><forename type="first">I</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12768" to="12778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning latent causal graphs via mixture oracles. Advances in Neural Information Processing Systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kivva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18087" to="18101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Partial disentanglement for domain adaptation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Akinwande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11455" to="11472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Priol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Causal Learning and Reasoning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Factorizing multivariate function classes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High-dimensional learning of linear causal networks via inverse covariance estimation</title>
		<author>
			<persName><forename type="first">P.-L</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3065" to="3105" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reliable causal discovery with improved exact search and weaker assumptions</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning, and Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adjacencyfaithfulness and conservative causal inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Spirtes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6843</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards causal representation learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="612" to="634" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning the structure of linear latent variable models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="191" to="246" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Disentanglement by nonlinear ICA with general incompressible-flow networks (GIN)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sorrenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04872</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><surname>Causation</surname></persName>
		</author>
		<title level="m">Prediction, and Search</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Linear causal disentanglement via interventions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Bhate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Linear Algebra and Its Applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Strang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Thomson, Brooks/Cole, Belmont, CA</pubPlace>
		</imprint>
	</monogr>
	<note>4th edition</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Introduction to Linear Algebra</title>
		<author>
			<persName><forename type="first">G</forename><surname>Strang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Wellesley-Cambridge Press</publisher>
		</imprint>
	</monogr>
	<note>5th edition</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Source separation in post-nonlinear mixtures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Taleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jutten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on signal Processing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2807" to="2820" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Geometry of the faithfulness assumption in causal inference</title>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raskutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="436" to="463" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Score-based causal representation learning with interventions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Varici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Acarturk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tajer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.08230</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kekić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.00542</idno>
		<title level="m">Nonparametric identifiability of causal representations from unknown interventions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generalized independent noise condition for estimating latent variable causal graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Identification of linear non-gaussian latent hierarchical structure</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="24370" to="24387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning temporally causal latent processes from general temporal data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05428</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Temporally disentangled representation learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Detection of unfaithfulness and robust causal inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Minds and Machines</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="239" to="271" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Identifiability guarantees for causal disentanglement from soft interventions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generalizing nonlinear ica beyond structural sparsity</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="13326" to="13355" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On the identifiability of nonlinear ICA: Sparsity and beyond</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Generalized precision matrix for scalable estimation of nonparametric markov networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.11379</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
