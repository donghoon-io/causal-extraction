<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CAUSAL REPRESENTATION LEARNING IN TEMPORAL DATA VIA SINGLE-PARENT DECODING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-09">9 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Philippe</forename><surname>Brouillard</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Mila -Quebec AI Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Samsung -SAIT AI Lab</orgName>
								<address>
									<country>Montreal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julia</forename><surname>Kaltenborn</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Mila -Quebec AI Institute</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yaniv</forename><surname>Gurwicz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dhanya</forename><surname>Sridhar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Mila -Quebec AI Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Drouin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Mila -Quebec AI Institute</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">ServiceNow Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Technische Universität Dresden</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Rolnick</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Mila -Quebec AI Institute</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CAUSAL REPRESENTATION LEARNING IN TEMPORAL DATA VIA SINGLE-PARENT DECODING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-09">9 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2410.07013v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scientific research often seeks to understand the causal structure underlying highlevel variables in a system. For example, climate scientists study how phenomena, such as El Niño, affect other climate processes at remote locations across the globe. However, scientists typically collect low-level measurements, such as geographically distributed temperature readings. From these, one needs to learn both a mapping to causally-relevant latent variables, such as a high-level representation of the El Niño phenomenon and other processes, as well as the causal model over them. The challenge is that this task, called causal representation learning, is highly underdetermined from observational data alone, requiring other constraints during learning to resolve the indeterminacies. In this work, we consider a temporal model with a sparsity assumption, namely single-parent decoding: each observed low-level variable is only affected by a single latent variable. Such an assumption is reasonable in many scientific applications that require finding groups of low-level variables, such as extracting regions from geographically gridded measurement data in climate research or capturing brain regions from neural activity data. We demonstrate the identifiability of the resulting model and propose a differentiable method, Causal Discovery with Single-parent Decoding (CDSD), that simultaneously learns the underlying latents and a causal graph over them. We assess the validity of our theoretical results using simulated data and showcase the practical validity of our method in an application to real-world data from the climate science field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In scientific domains, we often seek to learn causal relationships between high-level variables. For example, climate scientists want to understand how major modes of climate variability, such as the El Niño Southern Oscillation (ENSO) affect weather patterns worldwide <ref type="bibr">(76; 77; 62)</ref>. Neuroscientists want to uncover how different brain regions may be defined and influence one another <ref type="bibr" target="#b68">(69)</ref>. Identifying true causal links in a network of correlations is challenging in itself, but to compound the difficulty, scientists typically collect low-level and noisy measurements in place of causally relevant high-level variables. For example, instead of recording the presence or absence of ENSO and its global impact, climate scientists measure sea-surface temperatures at many locations. Instead of measuring overall communication between brain regions, neuroscientists must work with proxy information such as blood flow or electrical activity in specific locations. Thus, scientific discovery requires causal representation learning: the coupled tasks of learning latent variables that represent semantically meaningful abstractions of the observed measurements and the quantification of causal relationships among these latents <ref type="bibr" target="#b80">(81)</ref>.</p><p>What makes causal representation learning particularly challenging from a theoretical perspective is the non-identifiability of the models: there are typically many solutions -mappings from observations to latents -that fit the observed measurements equally well. Of these many alternatives, only some disentangled solutions capture the semantics of the true latents while the other solutions entangle the latents, changing their semantics and making it impossible to then infer the causal relationships among the latents. As such, a key focus of causal representation learning is identifying the latents up to disentangled solutions using various inductive biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>In this paper, we introduce a causal representation learning method for temporal observations, Causal Discovery with Single-parent Decoding (CDSD), a fully differentiable method that not only recovers disentangled latents, but also the causal graph over these latents. The assumption underlying CDSD, that is crucial for identifiability, involves highly sparse mappings from latents to observations: each observed variable e.g., sea-level pressure at a given grid location on Earth, is a nonlinear function of a single latent variable. We call this single-parent decoding. While this condition is strong, such assumptions have given rise to interpretable latent variables models for gene expression <ref type="bibr" target="#b6">(7)</ref>, text <ref type="bibr" target="#b3">(4)</ref>, and brain imaging data <ref type="bibr" target="#b55">(56)</ref>. Although single-parent decoding may not fit the needs of some analyses (e.g., images), it leads to scientifically-meaningful groupings of observed variables for many scientific applications. For example, in climate science, the sparse mapping corresponds to latent spatial zones, each exhibiting similar weather patterns or trends in their climate.</p><p>A key innovation of this paper is that, with our sparse mapping assumption, we can identify the latents up to some benign indeterminacies (e.g., permutations) as well as the temporal causal graph over the latents. We prove these identifiability results theoretically, and verify empirically that they hold in simulated data. Furthermore, we demonstrate the practical relevance of our method and assumptions via an application to a real-world climate science task. Our results indicate that CDSD successfully partitions climate variables into geographical regions and proposes plausible teleconnections between them -remote interactions between distant climate or weather states <ref type="bibr" target="#b96">(97)</ref> that have long been a target for climate scientists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>1. We propose a differentiable causal discovery approach that simultaneously learns both latent variables and a causal graph over the latents, based on time-series data. (Section 3)</p><p>2. We prove that the single-parent decoding assumption leads to the identifiability of both the latent representation and its causal graph. (Section 3.4, Proposition 2)</p><p>3. We evaluate our method both on synthetic data and a real-world climate science dataset in which relevant latents must be uncovered from measurements of sea-level pressure. (Section 4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Causal discovery from time-series data. Many causal discovery methods have been proposed for time-series data <ref type="bibr">(77; 79)</ref>. Constraint-based approaches, such as tsFCI <ref type="bibr" target="#b15">(16)</ref>, PCMCI+ <ref type="bibr" target="#b74">(75)</ref> and TS-ICD <ref type="bibr" target="#b71">(72)</ref>, learn an equivalence class of directed acyclic graphs by iterative conditional independence testing. The proposed method is part of a line of work of score-based causal discovery methods that require a likelihood function to score each graph given the data. While standard score-based methods operate on a discrete search space of acyclic graphs (or Markov equivalence classes) that grows exponentially with the number of variables, continuous score-based methods enforce acyclicity only through a continuous acyclicity constraint, proposed by Zheng et al. <ref type="bibr" target="#b94">(95)</ref>. Some variants of these methods have been proposed specifically to handle time-series data with instantaneous connections <ref type="bibr">(63; 86; 17)</ref>. However, in contrast with CDSD, all the methods mentioned above do not address the problem of learning a latent representation.</p><p>Causal representation learning. Recently, the field of causal representation learning <ref type="bibr" target="#b80">(81)</ref> has emerged with the goal of learning, from low-level data, representations that correspond to actionable quantities in a causal structure. <ref type="foot" target="#foot_0">1</ref> Since disentangling latent variables is impossible from independent and identically distributed samples <ref type="bibr">(27; 51)</ref>, existing works learn causal representations with weak supervision from paired samples (2; 8; 52; 89; 19), auxiliary labels <ref type="bibr">(39; 40; 47; 26; 28; 25)</ref>, and temporal observations <ref type="bibr">(46; 50; 43; 92)</ref>, or by imposing constraints on the map from latents to observations <ref type="bibr">(57; 71; 96)</ref>.</p><p>This paper fits into the last category of work on sparse decoding, which constrains each observed variable to be related to a sparse set of latent parents, either linearly <ref type="bibr">(14; 56; 6; 44)</ref> or nonlinearly <ref type="bibr">(57; 96; 71)</ref>. In comparison, the single-parent decoding assumption that we use imposes a stronger form of sparsity, similar to some work on factor analysis <ref type="bibr">(84; 56; 91; 45)</ref>. In contrast, this paper develops an identifiable single-parent decoding model that is nonlinear and scales well with high-dimensional observations. The line of work on independent mechanism analysis <ref type="bibr">(20; 70; 11)</ref> is also related to our G k represents the connections between the latent variables, and F the connections between the latents and the observables (dashed lines). The colors represent the different groups. For clarity, we illustrate here connections only up to G 1 , but our method also leverages connections of higher order.</p><p>identifiability result. The class of single-parent decoders we propose in this work is a subset of the class of decoders with Jacobians consisting of orthogonal columns. This work contributes to identification results in this category of research, a task which has proven to be challenging.</p><p>Finally, this paper also relates to Varimax-PCMCI (87), a method that, unlike causal representation learning, learns the latent variables and their causal graph in two separate stages. This method first applies Principal Component Analysis (PCA) and a Varimax rotation <ref type="bibr" target="#b32">(33)</ref> to learn latent variables, as demonstrated in <ref type="bibr">(62; 76)</ref>, and then applies PCMCI <ref type="bibr" target="#b77">(78)</ref>, a temporal constraint-based causal discovery method, to recover the causal graph between the latents. In contrast, CDSD learns the latents and their temporal causal graph simultaneously via score-based structure learning, admitting nonlinearity in the relationships between latents as well as the mapping from latents to observations. Although Mapped-PCMCI supports nonlinear relationships between the latents, it does this via nonlinear conditional independence tests, which do not scale well <ref type="bibr">(93; 85; 83)</ref>. Nevertheless, we directly compare CDSD with Varimax-PCMCI in the experiments in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CAUSAL DISCOVERY WITH SINGLE-PARENT DECODING</head><p>We consider the time series model illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. We observe d x -dimensional variables {x t } T t=1 at T time steps. The observed variables x t are a function of d z -dimensional latent variables z t . For example, the observations x t might represent temperature measurements at d x grid locations on Earth while the latents z t might correspond to unknown region-level temperature measurements.</p><p>We consider a stationary time series of order τ (i.e., τ is the maximum number of past observations that can affect the present observation) over the latent variables z 1 , . . . , z T . Thus, we model the relationship between the latents at time t, z t , and those at each of the τ previous time steps using binary matrices G k τ k=0 that represent causal graphs between the latent variables and their past states. That is, each matrix G k ∈ {0, 1} dz×dz encodes the presence of lagged relations between the timestep t -k and the present timestep t, i.e.,</p><formula xml:id="formula_0">G k ij = 1 if and only if z t-k j is a causal parent of z t i .</formula><p>In what follows, we assume that there are no instantaneous causal relationships, i.e., the latents at time t have no edges between one another in G 0 (see Appendix H.2 for a relaxation).</p><p>Finally, F is the adjacency matrix of the bipartite causal graph with directed arrows from the latents z to the variables x. We assume that F has a specific structure: the single-parent decoding structure, where each variable x i has at most one latent parent. That is, the set of latent parents z pa F i of each x i , where pa F i is the set of indices of the parents in graph F , is such that |pa F i | ≤ 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GENERATIVE MODEL</head><p>We now describe the model in detail that can be used to generate synthetic data.</p><p>Transition model. The transition model defines the relations between the latent variables z. We suppose that, at any given time step t, the latents are independent given their past:</p><formula xml:id="formula_1">p(z t | z &lt;t ) := dz j=1 p(z t j | z &lt;t ),<label>(1)</label></formula><p>where the notation z &lt;t is equivalent to z t-1 , . . . , z t-τ . Each conditional is parameterized by a nonlinear function that depends on its parents:</p><formula xml:id="formula_2">p(z t j | z &lt;t ) := h(z t j ; g j ([G 1 j: ⊙ z t-1 , . . . , G τ j: ⊙ z t-τ ]) ),<label>(2)</label></formula><p>where the bracket notation denotes the concatenation of vectors, g j denotes transition functions, G j: is the j-th row of the graph G, ⊙ is the element-wise product, and h is a density function of a continuous variable with support R parameterized by the outputs of g j . In our experiments, h is a Gaussian density although our identifiability result (Proposition 1) requires only that h has full support.</p><p>Observation model. The observation model defines the relationship between the latent variables z and the observable variables x. We assume conditional independence of the x t j :</p><formula xml:id="formula_3">p(x t | z t ) := dx j=1 p(x t j | z t pa F j ); p(x t j | z t pa F j ) := N (x t j ; f j (z t pa F j ), σ 2 j ),<label>(3)</label></formula><p>where f j : R → R, and σ<ref type="foot" target="#foot_1">foot_1</ref> ∈ R dx &gt;0 are decoding functions. As previously mentioned, we assume a specific structure of F , namely that |pa F j | ≤ 1 for all nodes x j . In the next section, we will present a way to enforce this structure.</p><p>Joint distribution. The complete density of the model is thus given by:</p><formula xml:id="formula_4">p(x ≤T , z ≤T ) := T t=1 p(z t | z &lt;t )p(x t | z t ) .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EVIDENCE LOWER BOUND</head><p>The model can be fit by maximizing p(x ≤T ) = p(x ≤T , z ≤T ) dz ≤T , which unfortunately involves an intractable integral. Instead, we rely on variational inference and optimize an evidence lower bound (ELBO) for p(x ≤T ), as is common to many instantiations of temporal variational auto-encoders (VAEs) (see <ref type="bibr">Girin et al. (18)</ref> for a review).</p><p>We use q(z ≤T | x ≤T ) as the variational approximation of the posterior p(z ≤T | x ≤T ):</p><formula xml:id="formula_5">q(z ≤T | x ≤T ) := T t=1 q(z t | x t ); q(z t | x t ) := N (z t ; f (x t ), diag( σ2 )),<label>(5)</label></formula><p>where f : R dx → R dz and σ2 ∈ R dz &gt;0 are the encoding functions. Using the approximate posterior and the generative model from Section 3.1, we get the ELBO:</p><formula xml:id="formula_6">log p(x ≤T ) ≥ T t=1 E z t ∼q(z t |x t ) log p(x t | z t ) -E z &lt;t ∼q(z &lt;t |x &lt;t ) KL q(z t | x t ) || p(z t | z &lt;t ) ,<label>(6)</label></formula><p>where KL stands for the Kullback-Leibler divergence. We show explicitly the derivation of this ELBO in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">INFERENCE</head><p>We now present some implementation choices and our optimization problem of interest, namely maximizing the ELBO defined in Equation 6 with respect to the different parameters of our generative model.</p><p>Latent-to-observable graph. We parameterize F , the graph between the latent z and the observable x, using a weighted adjacency matrix W ∈ R dx×dz</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>≥0</head><p>. Put formally, W ij &gt; 0 if and only if x i is a child of z j . In order to enforce the single-parent decoding assumption for F , we follow Monti and Hyvärinen <ref type="bibr" target="#b55">(56)</ref> and constrain W to be non-negative and have columns that are orthonormal vectors. 2 From these constraints our single-parent decoding assumption follows: at most one entry per row of W can be nonzero, i.e., a given x i can have at most one parent. As stated earlier, these constraints on W are essential since they ensure that W is identifiable up to permutation (we elaborate on identifiability in Section 3.4).</p><p>Encoding/decoding functions. We parameterize the decoding functions f j in Equation 3 with a neural network r j whose input is filtered using W as a mask: f j (z t pa F j ) = r j (W j: z t ). In Appendix E we show an architecture for the functions r j that leverages parameter sharing using only one neural network. For all experiments in the linear setting, we take r j to be the identity function as in Monti and Hyvärinen <ref type="bibr" target="#b55">(56)</ref>. The encoding function f (Equation <ref type="formula" target="#formula_5">5</ref>) and the functions g j from the transition model (Equation <ref type="formula" target="#formula_2">2</ref>) are also parameterized using neural networks.</p><p>Continuous optimization. We use ϕ to denote the parameters of all neural networks (r j , g j , f ) and the learnable variance terms at Equations 3 and 5. To learn the graphs G k via continuous optimization, we use a similar approach to Ke et al. <ref type="bibr" target="#b35">(36)</ref>; Brouillard et al. <ref type="bibr" target="#b8">(9)</ref>; Ng et al. <ref type="bibr" target="#b58">(59)</ref>, where the graphs are sampled from distributions parameterized by Γ k ∈ R dz×dz that are learnable parameters. Specifically, we use</p><formula xml:id="formula_7">G k ij ∼ Bernoulli(σ(Γ k ij ))</formula><p>, where σ(•) is the sigmoid function. To simplify the notation, we use G and Γ as the sets {G 1 , . . . , G τ } and {Γ 1 , . . . , Γ τ } in the remainder of the presentation. This results in the following constrained optimization problem:</p><formula xml:id="formula_8">max W,Γ,ϕ E G∼σ(Γ) E x [L x (W, Γ, ϕ)] -λ s ||σ(Γ)|| 1 s.t.</formula><p>W is orthogonal and non-negative, <ref type="bibr" target="#b6">(7)</ref> where L x is the ELBO corresponding to the right-hand side term in Equation 6 and λ s &gt; 0 is a coefficient for the regularisation of the graph sparsity. To enforce the non-negativity of W , we use the projected gradient on R ≥0 (see Appendix C.2). As for the orthogonality of W , we enforce it using the following constraint:</p><formula xml:id="formula_9">h(W ) := W T W -I dz .</formula><p>We relax the constrained optimization problem by using the augmented Lagrangian method (ALM), which amounts to adding a penalty term to the objective and incrementally increasing its weight during training ((60); see Appendix C.1). Hence, the final optimization problem is:</p><formula xml:id="formula_10">max W,Γ,ϕ E G∼σ(Γ) E x [L x (W, Γ, ϕ)] -λ s ||σ(Γ)|| 1 -Tr λ T W h(W ) - µ W 2 ||h(W )|| 2 2 ,<label>(8)</label></formula><p>where λ W ∈ R dz×dz and µ W ∈ R &gt;0 are the coefficients of the ALM.</p><p>We use stochastic gradient descent to optimize this objective. To estimate the gradients w.r.t. the parameters Γ, we use the Straight-Through Gumbel estimator <ref type="bibr">(53; 30)</ref>. In the forward pass, we sample G from the Bernoulli distributions, while in the backward pass, we use the Gumbel-Softmax samples. This estimator was successfully used in several causal discovery methods <ref type="bibr">(34; 9; 59)</ref>. For the ELBO optimization, we follow the classical VAE models (41) by using the reparametrization trick and a closed-form expression for the KL divergence term since both q(z t | x t ) and p(z t | z &lt;t ) are multivariate Gaussians. Using these tricks we can learn the graphs G and the matrix W end-to-end. For a more detailed exposition of the implementation, such as the neural network's architecture, see Appendix E. For an extension of this approach to support instantaneous relations, see Appendix H.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">IDENTIFIABILITY ANALYSIS</head><p>In this section, we discuss the identifiability of the model specified in Section 3.1. Put informally, we show that any solution that fits the ground-truth exactly recovers the true latents z t up to permutation and coordinate-wise transformations, i.e., transformations that preserve the semantics of the latents and admit valid causal discovery.</p><p>To formalize identifiability, we first state an important result that allows to show that two models expressing the same distribution over observations must have i) the same decoder image and ii) the relationship between latent representations must be a diffeomorphism. Similar results have been show in previous literature <ref type="bibr">(38; 46; 42; 1)</ref>, and for completeness' sake we state it and its proof in Appendix B.</p><p>For conciseness, we use f , g as the concatenations of functions [f 1 , . . . , f dx ] and [g 1 , . . . , g dz ].</p><p>Preprint Proposition 1 (Identifiability of f and p(z ≤T ) up to diffeomorphism). Assume we have two models p(x ≤T , z ≤T ) and p(x ≤T , ẑ≤T ) as specified in Section 3.1 with parameters (g, f , G, σ 2 ) and (ĝ, f , Ĝ, σ2 ), respectively. Assume further that f and f are diffeomorphisms onto their respective images and that d z &lt; d x (we do not assume single-parent decoding). Therefore, whenever p(x ≤T , z ≤T )dz ≤T = p(x ≤T , ẑ≤T )d ẑ≤T for all x ≤T , we have f (R dz ) = f (R dz ) and v := f -1 • f is a diffeomorphism. Moreover, the density of the ground-truth latents p(z ≤T ) and the density of the learned latents p( ẑ≤T ) are related via</p><formula xml:id="formula_11">p(v( ẑ≤T )) T t=1 | det Dv( ẑt )| = p( ẑ≤T ), ∀ ẑ≤T ∈ R dz×T .</formula><p>The following paragraphs discuss how the structure in F can be leveraged to show that v must be a trivial indeterminacy like a permutation composed with element-wise transformations.</p><p>Identifiability via the single-parent structure of F . The following proposition can be combined with Proposition 1 to show that the model specified in Section 3.1 with the single-parent decoding structure has a representation that is identifiable up to permutation and element-wise invertible transformations. The proof of this result can be found in Appendix B.</p><p>Proposition 2 (Identifying latents of f ). Let f : R dz → R dx and f : R dz → R dx be two diffeomorphisms onto their image f (R dz ) = f (R dz ). Assume both f and f have a single-parent decoding structure, i.e. |pa F j | ≤ 1 and |pa F j | ≤ 1. Then, the map v := f -1 • f has the following property: there exists a permutation π such that, for all i, the function v i (z) depends only on z π(i) .</p><p>Can we identify the causal graph G over latent variables? The above result, combined with Proposition 1, shows that we can identify the distribution p(z ≤T ) up to permutation and trivial reparameterizations. The question then reduces to "can we identify the causal graph G from p(z ≤T )?", which is the central question of causal discovery. It is well-known that, in the absence of instantaneous causal connections, a temporal causal graph can be identified from the observational distribution (65, Theorem 10.1). It is thus possible, without instantaneous connections, to identify G (up to permutation) from p(x ≤T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We empirically study the performance of CDSD on a number of linear and nonlinear settings using synthetic datasets. First, we compare CDSD to Varimax-PCMCI <ref type="bibr" target="#b86">(87)</ref>, an alternate method that has a closely similar application. The results, reported at Section 4.1, emphasize the advantages of CDSD over Varimax-PCMCI, especially in nonlinear settings. Next, we show that CDSD also compares favorably to identifiable representation methods (iVAE <ref type="bibr" target="#b38">(39)</ref> and DMS <ref type="bibr" target="#b45">(46)</ref>) on synthetic data that respect the single-parent decoding assumption. Finally, we apply CDSD to a real-world climate science task and show that it recovers spatial aggregations related to known climate phenomena, such as the El Niño Southern Oscillation (Section 4.2).</p><p>An implementation of CDSD is available at <ref type="url" target="https://github.com/kurowasan/cdsd">https://github.com/kurowasan/cdsd</ref>. For Varimax-PCMCI, we follow the implementation of Tibau et al. <ref type="bibr" target="#b86">(87)</ref>, where dimensionality reduction is done by combining PCA with a Varimax rotation <ref type="bibr" target="#b32">(33)</ref>, the causal graph is learned with PCMCI+ (75), and conditional independence is tested using a partial correlation test when latent dynamics are linear or the CMI-knn test (74) otherwise. Note that while PCMCI+ supports instantaneous connections, we always restrict the minimum time lag considered to 1. For further implementation details on both methods, refer to Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SYNTHETIC DATA BENCHMARK</head><p>The first task is to compare CDSD and Varimax-PCMCI. The key modeling components to evaluate in the two compared methods are: linear versus nonlinear dynamics in the learned causal graphs over latents, and linear versus nonlinear decoding functions from latents to observations. CDSD can flexibly handle all these settings, whereas Varimax-PCMCI assumes linear maps from latents to observations. We evaluate the methods in the following cases: 1) linear dynamics and linear decoding, 2) nonlinear dynamics and linear decoding, and 3) linear dynamics and nonlinear decoding. We expect to find that  CDSD shows clear advantages when the mappings from latents to observations are nonlinear. We also compare to other causal representation methods to show the identifiability gain induced by using the constraints on W for data respecting the single-parent decoding assumption.</p><p>Datasets. We consider datasets randomly generated according to the model described at Section 3.1. The generative process is described in detail in Appendix D.1. Unless otherwise specified, we consider T = 5000 timesteps, a stationary process of order τ = 1, d x = 100 observed variables, d z = 10 latent variables, and random latent dynamic graphs, akin to Erdős-Rényi graphs, with a probability p = 0.15 of including an edge. The nature of the relationships among latents -and from latents to observables -is either linear or nonlinear, depending on the specific experiment.</p><p>Protocol. We assess variability in each experimental condition by repeating each experiment 100 times with different randomly generated datasets. The hyperparameters of both methods are chosen to maximize overall performance on 10 randomly generated datasets distinct from the evaluation (see Appendix F). Note that, for both methods, d z and τ are not part of the hyperparameter search and are set to the ground-truth values in the generative process.</p><p>Metrics. Performance is assessed using two metrics: i) mean correlation coefficient (MCC), which measures the quality of the learned latent representation, and ii) structural Hamming distance (SHD), which measures the number of incorrect edges in the learned causal graph. MCC corresponds to the highest correlation coefficient between the estimated latents ( ẑ) and the ground-truth latent (z) across all possible permutations (as described in <ref type="bibr" target="#b39">(40)</ref>). The use of permutations is necessary since identification can only be guaranteed up to a permutation (see Section 3.4).</p><p>1) Linear latent dynamics, Linear decoding. We start by evaluating the methods in a context where all causal relationships are linear. We consider a variety of conditions: d z = {5, 10, 20}, τ = {1, 2, 3}, T = {500, 1000, 5000}, and p = {0.15, 0.3} (which corresponds to sparse and dense graphs). We observed that both methods achieve a high MCC ≥ 0.95, in all conditions, which is not surprising since they are both capable of identifying the latents when the decoding function is linear (see Appendix G.2). The average SHD and its standard error are reported at Fig. <ref type="figure" target="#fig_2">2a</ref>. Varimax-PCMCI performs slightly better than CDSD in most conditions, except for more challenging cases such as stationary processes of greater order (τ = 3) and denser graphs (p = 0.3). The latter result is in line with previous studies, which observed that continuous optimization methods tend to outperform their constraint-based counterparts (95; 9) in dense graphs.</p><p>2) Nonlinear latent dynamics, Linear decoding. We now consider the case where causal relationships between the latents are nonlinear, while those from latents to observables remain linear. The results are reported at Fig. <ref type="figure" target="#fig_2">2b</ref>. In contrast with the linear case, we do not present the results under all the experimental conditions due to the prohibitive running time of Varimax-PCMCI, which was greater than 24 hours for a single experiment (for the complete results, see Appendix G.2). This can be explained by its reliance on nonlinear conditional independence tests whose running time scales unfavorably w.r.t. the number of samples and variables <ref type="bibr">(74; 94; 85)</ref>. Consequently, results for Varimax-PCMCI are only reported up to 1000 samples. In sharp contrast, CDSD completed all experiments in a timely manner. Hence, while its solutions tend to have slightly higher SHD, CDSD can be used in contexts where Varimax-PCMCI, at least with a non-parametric conditional independence test, cannot.  3) Linear latent dynamics, Nonlinear decoding. The purpose of this experiment is to showcase the inability of Varimax-PCMCI to identify the latent representation when the relationships between the latents and the observables are nonlinear. This is the case, since PCA with a Varimax rotation is a linear dimension-reduction method. In contrast, CDSD should have no problem identifying the latents in this setting. We consider a dataset generated with the previously stated default conditions, where we ensure that the identifiability conditions of Section 3.4 are satisfied. The results are reported at Fig. <ref type="figure" target="#fig_3">3</ref>. As expected, Varimax-PCMCI fails to recover the latent representation, achieving a poor MCC and, consequently, a poor SHD. In contrast, CDSD performs much better according to both metrics. These results clearly show the superiority of CDSD over Varimax-PCMCI when the relationships between latents and observables are nonlinear because of the linearity assumption in the Varimax step.</p><p>Comparison to causal representation methods. We compare CDSD to two causal representation methods, iVAE <ref type="bibr" target="#b39">(40)</ref> and DMS <ref type="bibr" target="#b45">(46)</ref>, on the synthetics data sets with linear decoding and nonlinear dynamics in Figure <ref type="figure" target="#fig_4">4</ref>. For a fair comparison, we implement iVAE and DMS by modifying the objective for method in each case. For iVAE, this corresponds to not applying the constraints on W , not using regularisation on the graph G (i.e., λ s = 0) and fitting the variance of z t |z t-1 . For DMS, it simply corresponds to not applying the constraints on W . Both methods have a worse MCC than CDSD. This is in line with our theoretical result since only CDSD leverages the single-parent decoding assumption. Note however that several assumptions required by iVAE and DMS may not hold in our datasets. For example, the identifiability result of iVAE assumes that the variance of z t |z t-1 varies sufficiently, which is not the case in our synthetic data. For DMS, while we use sparse transition graphs (G), we did not verify if the graphical criterion required by Lachapelle et al. <ref type="bibr" target="#b45">(46)</ref> is respected (Theorem 5, assumption 5), nor whether its assumptions of sufficient variability hold. In Appendix I.3, we conduct a similar ablation for Varimax-PCMCI, which shows the necessity of the Varimax rotation in order for PCA to recover a good latent representation for the case of linear latent dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">REAL-WORLD APPLICATION TO CLIMATE SCIENCE</head><p>To test the capabilities of CDSD in real-world settings, we apply it to the National Oceanic and Atmospheric Administration's (NOAA) Reanalysis 1 mean sea-level pressure (MSLP) dataset <ref type="bibr" target="#b34">(35)</ref>. Local variations in MSLP reflect changes in the dynamical state of the atmosphere for a certain region or, in other words, the occurrence and passing of weather systems (e.g. low-or high-pressure systems). Over time, MSLP data can thus be used to identify regions that share common weather properties, and to understand how regional weather systems are coupled to each other globally. Identifying such causal relationships would help climate scientists to better understand Earth's global dynamical weather system and could provide leverage for data-driven forecasting systems.</p><p>Here, we use MSLP data from 1948-2022 on a global regular grid with a resolution of 2.5 • longitude × 2.5 • latitude. We aggregated the daily time-series to weekly data and regridded it onto an icosahedral-hexagonal grid (see Appendix D.2) <ref type="bibr" target="#b54">(55)</ref>. The resulting dimensions are 3900 × 6250, covering 52 weeks of 75 years (T = 3900) and d x = 6250 grid cells. We apply CDSD in order to cluster regions of similar weather properties, and identify which regions are causally linked to weather phenomena in other regions. We use the method with linear dynamics and linear decoding, and, similarly to <ref type="bibr" target="#b75">(76)</ref>, we use d z = 50 and τ = 5.   <ref type="formula" target="#formula_31">29</ref>)). The learned clusters broadly reflect a superposition of the effects of transport timescales, ocean-to-land boundaries, and the zonal and meridional patterns of the tropospheric circulation (Hadley, Ferrel, and polar cells) in both hemispheres. Among the visually most prominent features is the identification of East Pacific, Central Pacific, and Western Pacific clusters (clusters 13, 39, 48) along the tropical Pacific. These zones are well-known to be coupled through ENSO, but the East Pacific typically sees the most pronounced temperature oscillations due to its shallow oceanic thermocline (e.g., 61). We also recover a relatively zonal structure of clusters in the Southern Hemisphere mid-latitudes (clusters 17, 2, 41, 14, 20, 40, 15, 18, and 43 from west to east) where the zonal tropospheric circulation moves relatively freely without significant disturbances from land boundaries. While not strictly enforced, all the learned regions are spatially connected/homogeneous, i.e. not divided into several de-localized parts (see Appendix G.3). Highly de-localized, globally distributed, components are for example a major issue in interpreting standard principal component analyses of MSLP data <ref type="bibr" target="#b20">(21)</ref>. In contrast, the regions learned by CDSD without constraints are not localized and are harder to associate to known regions such as those related to ENSO (see Appendix G.3).</p><p>While a detailed analysis of the learned causal graphs (Fig. <ref type="figure" target="#fig_5">5b</ref>) is beyond the scope of this study, it is intuitive that the strongest and most frequent connections are found within a timescale of one week (G 1 ), but notably longer -likely more distant connections -are found, too. These likely reflect the well-known presence of long-distance teleconnections between world regions <ref type="bibr" target="#b61">(62)</ref>. In Fig. <ref type="figure" target="#fig_5">5c</ref> we show one example of the causal coupling inferred for ENSO-related modes (clusters 13, 39, 47, 48) which is similar to the causal graph found in Runge et al. <ref type="bibr" target="#b77">(78)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>We present CDSD, a method that relies on the single-parent decoding assumption to learn a causal representation and its connectivity from time series data. The method is accompanied by theoretical results that guarantee the identifiability of the representation up to benign transformations. The key benefits of CDSD over Varimax-PCMCI are that i) it supports nonlinear decoding functions, and ii) as opposed to its constraint-based tests, it scales well in the number of samples and variables in the nonlinear dynamics case. Furthermore, as illustrated in the application to climate science, CDSD and its assumptions, appear to be applicable in practice and seem particularly well-suited for problems of scientific interest, such as the spatial clustering of weather measurements.</p><p>We highlight a few limitations that should be considered. Several assumptions, such as the stationarity of the dynamical system or the single-parent decoding assumption, can be partially or totally violated in real-world applications. We did not study the impact of these model misspecifications on the performance of CDSD. In all our experiments, we assumed that d z and τ were known. However, in practical applications, these values are unknown and might be difficult to infer, even for experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>Besides these limitations, CDSD, in its general form, can be used in several contexts or be readily extended. It can be used with multivariate data (e.g., in climate science applications, one could be interested in modeling sea-level pressure, but also temperature, precipitation, etc.). Furthermore, in other contexts, such as brain imaging studies, one could be interested in learning different graphs G for different subjects, while sharing a common spatial aggregation (as in Monti and Hyvärinen <ref type="bibr" target="#b55">(56)</ref>). We want to highlight that the method can be further extended to include instantaneous connections, and learn from observational and interventional data. In Appendix H.1, we show how our method can be adapted to support all these cases. Overall, we believe that CDSD is a significant step in towards the goal of bridging the gap between causal representation learning and scientific applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DERIVATION OF THE ELBO</head><p>We show explicitly the derivation of the ELBO by starting from the marginal likelihood and by using the model we have proposed in Section 3.1.</p><formula xml:id="formula_12">log p(x ≤T ) = E z ≤T ∼q(z ≤T |x ≤T ) log p(x ≤T ) p(x ≤T , z ≤T ) p(x ≤T , z ≤T ) q(z ≤T | x ≤T ) q(z ≤T | x ≤T ) (9) = E z ≤T ∼q(z ≤T |x ≤T ) log q(z ≤T | x ≤T ) p(z ≤T | x ≤T ) + log p(x ≤T , z ≤T ) q(z ≤T | x ≤T ) (10) = KL(q(z ≤T | x ≤T )||p(z ≤T | x ≤T )) + E z ≤T ∼q(z ≤T |x ≤T ) log p(x ≤T , z ≤T ) q(z ≤T | x ≤T ) .<label>(11)</label></formula><p>Since KL ≥ 0, we have:</p><formula xml:id="formula_13">log p(x ≤T ) ≥ E z ≤T ∼q(z ≤T |x ≤T ) log p(x ≤T , z ≤T ) q(z ≤T | x ≤T ) . (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>Now we will replace p(x ≤T , z ≤T ) and q(z ≤T | x ≤T ) by the factorisation we assumed in Equation <ref type="formula" target="#formula_4">4</ref>and 5:</p><formula xml:id="formula_15">log p(x ≤T ) ≥ E z ≤T ∼q(z ≤T |x ≤T ) log T t=1 p(z t | z &lt;t )p(x t | z t ) T t=1 q(z t | x t )<label>(13)</label></formula><formula xml:id="formula_16">≥ E z ≤T ∼q(z ≤T |x ≤T ) T t=1 log p(x t | z t ) + E z ≤T ∼q(z ≤T |x ≤T ) T t=1 log p(z t | z &lt;t ) q(z t | x t ) .<label>(14)</label></formula><p>Finally, thanks to the decomposition of our proposed posterior (Equation <ref type="formula" target="#formula_5">5</ref>), we have:</p><formula xml:id="formula_17">log p(x ≤T ) ≥ T t=1 E z t ∼q(z t |x t ) log p(x t | z t ) -<label>(15)</label></formula><formula xml:id="formula_18">E z &lt;t ∼q(z &lt;t |x &lt;t ) KL q(z t | x t ) || p(z t | z &lt;t ) .<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B IDENTIFIABILITY</head><p>In what follows, we overload the notation by defining f (z ≤T ) := [f (z 1 ) . . . f (z T )] and similarly for other functions. Lemma 3 (Denoising x). Assume we have two models p(x ≤T , z ≤T ) and p(x ≤T , ẑ≤T ) as specified in Section 3.1 with parameters (g, f , G, σ 2 ) and (ĝ, f , Ĝ, σ2 ), respectively. Assume d z &lt; d x . Therefore, whenever p(x ≤T , z ≤T )dz ≤T = p(x ≤T , ẑ≤T )d ẑ≤T for all x ≤T , we have that the distributions of y ≤T := f (z ≤T ) and ŷ≤T := f ( ẑ≤T ) are equal.</p><p>Proof. Let P x ≤T and P x≤T be the probability measures respectively induced by the densities p(x ≤T ) := p(x ≤T , z ≤T )dz ≤T and p( x≤T ) := p( x≤T , ẑ≤T )d ẑ≤T . We can thus write</p><formula xml:id="formula_19">P x ≤T = P x≤T .<label>(17)</label></formula><p>Let us define y t := f (z t ) and ŷt := f (ẑ t ) for all t where z ≤T ∼ p(z ≤T ) and ẑ≤T ∼ p( ẑ≤T ). Let P y ≤T and P ŷ≤T be the probability distributions of y ≤T and ŷ≤T , respectively. Notice how we can write x ≤T = y ≤T + n ≤T and x≤T = y ≤T + n≤T , where n t ∼ N (0, σ 2 I dx ) and nt ∼ N (0, σ2 I dx ) for all t ≤ T , where the noises are mutually independent across time. This means we can write</p><formula xml:id="formula_20">P y ≤T * P n ≤T = P ŷ≤T * P n≤T ,<label>(18)</label></formula><p>where P n ≤T stands for the probability measure of the Gaussian noise (similarly for P n≤T ) and * stands for the convolution operator on measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>The following step makes use of the Fourier transform F generalized to arbitrary probability measures. See Pollard <ref type="bibr" target="#b66">(67,</ref><ref type="bibr">Chapter 8)</ref>. Note that the Fourier transform of a probability distribution is exactly the characteristic function of the random variable it represents.</p><formula xml:id="formula_21">F(P y ≤T * P n ≤T ) = F(P ŷ≤T * P n≤T ) (19) F(P y ≤T )F(P n ≤T ) = F(P ŷ≤T )F(P n≤T )<label>(20)</label></formula><formula xml:id="formula_22">F(P y ≤T )(ω)e -σ 2 2 ω ⊤ ω = F(P ŷ≤T )(ω)e -σ2 2 ω ⊤ ω , ∀ω ∈ R T •dx ,<label>(21)</label></formula><p>where we used the fact that (i) the Fourier transform of a convolution is the product of the Fourier transforms and (ii) the fact that the Fourier transform of a Gaussian random vector with mean 0 and covariance</p><formula xml:id="formula_23">σ 2 I is e -σ 2 2 ω ⊤ ω .</formula><p>Now our goal is to show that σ 2 = σ2 . Assume this is false, i.e. σ 2 &lt; σ2 (w.l.o.g.). Because this Fourier transform is positive for all ω ∈ R T •dx , we can divide by its value on both sides and obtain</p><formula xml:id="formula_24">F(P y ≤T )(ω) = F(P ŷ≤T )(ω)e - σ2 -σ 2 2 ω ⊤ ω , ∀ω ∈ R T •dx ,<label>(22)</label></formula><p>where we recognize that e - σ2 -σ 2 2</p><p>ω ⊤ ω is the Fourier transform of a Gaussian distribution with mean zero and covariance (σ 2 -σ 2 )I T •dx . Now notice how the l.h.s. is the Fourier transform of a distribution with support contained in f (R T dz ), which is a d z -dimensional manifold embedded in R T dx . Since we assume d z &lt; d x , the set f (R T dz ) is a proper subset of R T dx (i.e. f (R T dz ) ̸ = R T dx ). In contrast, the r.h.s. is the Fourier transform of a distribution with full support, i.e. R T dx , since it is the convolution of P ŷ≤T and a Gaussian distribution. This is a contradiction since both supports should be equal. Hence we must have that σ 2 = σ2 . We can thus write</p><formula xml:id="formula_25">F(P y ≤T ) = F(P ŷ≤T )<label>(23)</label></formula><formula xml:id="formula_26">P y ≤T = P ŷ≤T ,<label>(24)</label></formula><p>which concludes the proof.</p><p>Proposition 1 (Identifiability of f and p(z ≤T ) up to diffeomorphism). Assume we have two models p(x ≤T , z ≤T ) and p(x ≤T , ẑ≤T ) as specified in Section 3.1 with parameters (g, f , G, σ 2 ) and (ĝ, f , Ĝ, σ2 ), respectively. Assume further that f and f are diffeomorphisms onto their respective images and that d z &lt; d x (we do not assume single-parent decoding). Therefore, whenever p(x ≤T , z ≤T )dz ≤T = p(x ≤T , ẑ≤T )d ẑ≤T for all x ≤T , we have f (R dz ) = f (R dz ) and v := f -1 • f is a diffeomorphism. Moreover, the density of the ground-truth latents p(z ≤T ) and the density of the learned latents p( ẑ≤T ) are related via</p><formula xml:id="formula_27">p(v( ẑ≤T )) T t=1 | det Dv( ẑt )| = p( ẑ≤T ), ∀ ẑ≤T ∈ R dz×T .</formula><p>Proof. By Lemma 3, we have that</p><formula xml:id="formula_28">P y ≤T = P ŷ≤T .<label>(25)</label></formula><p>By marginalizing out y 2:T on both sides, we get</p><formula xml:id="formula_29">P y 1 = P ŷ1 ,<label>(26)</label></formula><p>which of course implies that their supports are equal:</p><formula xml:id="formula_30">supp(P y 1 ) = supp(P ŷ1 ) (27) f (supp(p(z t ))) = f (supp(p( ẑt )))<label>(28)</label></formula><formula xml:id="formula_31">f (R dz ) = f (R dz ) ,<label>(29)</label></formula><p>where supp(•) stands for support of a distribution. Note that the last step is because p(z 1 ) is assumed to have full support (Section Section 3.1).</p><p>Equation ( <ref type="formula" target="#formula_31">29</ref>) and the fact that both f and f are diffeomorphisms onto their image implies that the map v := f -1 • f is both well-defined and a diffeomorphism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>From ( <ref type="formula" target="#formula_28">25</ref>), we get</p><formula xml:id="formula_32">P z ≤T • f -1 = P ẑ≤T • f (30) P z ≤T • f -1 • f = P ẑ≤T<label>(31)</label></formula><formula xml:id="formula_33">P z ≤T • v = P ẑ≤T . (<label>32</label></formula><formula xml:id="formula_34">)</formula><p>The above equation combined with the change-of-variable formula yields</p><formula xml:id="formula_35">p(v( ẑ≤T )) T t=1 | det Dv( ẑt )| = p( ẑ≤T ), ∀ ẑ≤T ∈ R dz×T ,<label>(33)</label></formula><p>which concludes the proof.</p><p>Proposition 2 (Identifying latents of f ). Let f : R dz → R dx and f : R dz → R dx be two diffeomorphisms onto their image f (R dz ) = f (R dz ). Assume both f and f have a single-parent decoding structure, i.e. |pa F j | ≤ 1 and |pa F j | ≤ 1. Then, the map v := f -1 • f has the following property: there exists a permutation π such that, for all i, the function v i (z) depends only on z π(i) .</p><p>Proof. We see that</p><formula xml:id="formula_36">v = f -1 • f implies f = f • v . (<label>34</label></formula><formula xml:id="formula_37">)</formula><p>Taking the derivative on both sides of Equation <ref type="formula" target="#formula_36">34</ref>, write</p><formula xml:id="formula_38">D f (z) = D(f • v)(z) = Df (v(z))Dv(z),<label>(35)</label></formula><p>where the second equality follows from applying the chain rule, each Jacobian D f (z) and Df (v(z)) is a d x × d z matrix and the Jacobian Dv(z) is a d z × d z matrix.</p><p>In words, Equation 34 tells us that the mapping f is "imitating" the mapping f in the following sense: Evaluating f is the same as first evaluating v and then evaluating f .</p><p>We need to show that v(z) is a permutation-scaling transformation in the sense defined in the statement of this proposition. To achieve this, we will show that the Jacobian Dv(z) is a permutation-scaling matrix for all z.</p><p>We show this in a number of steps:</p><p>Step 1. Since f is a diffeomorphism, its Jacobian, D f , has full column rank everywhere. Thus, its Moore-Penrose inverse (also known as its pseudo-inverse), D f (z) + , can be written as,</p><formula xml:id="formula_39">D f (z) + = (D f (z) ⊤ D f (z)) -1 Df (z) ⊤ . (<label>36</label></formula><formula xml:id="formula_40">) Further, D f (z) + is a left inverse; that is, D f (z) + D f (z) = I.</formula><p>We can left-multiply both sides of Equation 35 by D f (z) + , yielding,</p><formula xml:id="formula_41">I = (D f (z) ⊤ D f (z)) -1 D f (z) ⊤ Df (v(z))Dv(z). (<label>37</label></formula><formula xml:id="formula_42">)</formula><p>Step 2. We now show that the matrix D f (z) ⊤ D f (z) is diagonal. To see this, consider k ̸ = k ′ and write</p><formula xml:id="formula_43">(D f (z) ⊤ D f (z)) k,k ′ = dx d=1 D f (z) d,k D f (z) d,k ′ . (<label>38</label></formula><formula xml:id="formula_44">)</formula><p>This must be zero since, otherwise, it would imply that there exists a d such that both D f (z) d,k and D f (z) d,k ′ are different from zero, but this is impossible since y d has only one parent in the graph F (by the "single-parent property").</p><p>Define Λ(z) := D f (z) ⊤ D f (z), which we just showed is diagonal. Equation 37 implies that</p><formula xml:id="formula_45">Dv(z) -1 = Λ(z) -1 D f (z) ⊤ Df (v(z)),<label>(39)</label></formula><p>and since Dv(z) -1 is invertible, D f (z) ⊤ Df (v(z)) must also be invertible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>Key idea. To show that Dv(z) is a permutation-scaling matrix, we will show that its inverse is a permutation-scaling matrix. We have already shown that Λ(z) is a diagonal matrix. Thus, what remains is to show that D f (z) ⊤ Df (v(z)) is a permutation-scaling matrix.</p><p>Step 3. For any d z × d z invertible matrix L,</p><formula xml:id="formula_46">det(L) = π∈Π dz sign(π) dz k=1 L π(k),k ̸ = 0,</formula><p>where Π dz is the set of (d z )-permutations. This implies that there exists a permutation π ∈ Π dz so that for all k ≤ d z , L π(k),k ̸ = 0.</p><p>Applying this result to the invertible matrix D f (z</p><formula xml:id="formula_47">) ⊤ Df (v(z)), ∃π ∈ Π dz : ∀k ≤ d z , (D f (z) ⊤ Df (v(z))) π(k),k ̸ = 0 =⇒ ∀k ≤ d z , dx d=1 D f (z) d,π(k) Df (v(z)) d,k ̸ = 0 =⇒ ∀k ≤ d z , ∃d 1 : D f (z) d1.π(k) ̸ = 0 ̸ = Df (v(z)) d1,k .<label>(40)</label></formula><p>Key idea. To show that D f (z) ⊤ Df (v(z)) is a permutation-scaling matrix, we want to show that this matrix has nonzero values only at entries of the form (π(k), k). We will prove this by contradiction, using the existence of the observed feature y d1 from Equation <ref type="formula" target="#formula_47">40</ref>.</p><p>Step 4. By contradiction, suppose there exists a pair of indices (k, k ′ ) such that k ′ ̸ = π(k) and</p><formula xml:id="formula_48">(D f (z) ⊤ Df (v(z))) k ′ ,k = dx d=1 D f (z) ⊤ k ′ ,d Df (v(z)) d,k ̸ = 0 .<label>(41)</label></formula><p>This means that there exists a feature y d2 so that </p><formula xml:id="formula_49">D f (z) d2,k ′ ̸ = 0 ̸ = Df (v(z)) d2,k .<label>(42</label></formula><formula xml:id="formula_50">f (z) {d 1 ,d 2 } ,• = Df (v(z)) {d 1 ,d 2 } ,• Dv(z).<label>(43)</label></formula><p>By ( <ref type="formula" target="#formula_47">40</ref>) &amp; ( <ref type="formula" target="#formula_49">42</ref>) and the fact that both f and f satisfy the "single-parent property", we have</p><formula xml:id="formula_51">D f (z) {d 1 ,d 2 } ,• = π(k) k ′ d 1 0 • • • 0 * 0 0 • • • 0 d 2 0 • • • 0 0 * 0 • • • 0 (44) Df (v(z)) {d 1 ,d 2 } ,• = k d 1 0 • • • 0 * 0 • • • 0 d 2 0 • • • 0 * 0 • • • 0 , (<label>45</label></formula><formula xml:id="formula_52">)</formula><p>where * denotes nonzero entries. From the above, it is clear that D f (z) {d 1 ,d 2 } ,• has a rank of 2 and Df (v(z)) {d 1 ,d 2 } ,• has a rank of 1. Since Dv(z) is invertible we have that the r.h.s. of ( <ref type="formula" target="#formula_50">43</ref>) has rank 1. Equation ( <ref type="formula" target="#formula_50">43</ref>) is thus a contradiction since the l.h.s. has rank 2.</p><p>Thus,</p><formula xml:id="formula_53">(D f (z) ⊤ Df (v(z))) k ′ ,k ̸ = 0 if and only if k ′ = π(k), where π is a (d z )-permutation. In other words, D f (z) ⊤ Df (v(z)</formula><p>) is a permutation-scaling matrix.</p><p>Going back to Equation <ref type="formula" target="#formula_45">39</ref>, Dv(z) -1 is a permutation-scaling matrix since it is a product of a diagonal matrix and a permutation-scaling matrix. Thus, its inverse, Dv(z) is also a permutation-scaling matrix.</p><p>The argument above holds point-wise, i.e. for each z. However, a priori, it is possible that the permutation π changes for different values of z. It turns out this is impossible since that would violate the fact that Dv(z) is continuous. 2-Dynamic's functions. We first explain how we generate the mechanisms for the nonlinear case and, since it is a particular case, we then explain the linear case.</p><p>It is not obvious in general how to sample a nonlinear generative process that is stationary or, at least, that won't greatly diverge over time. In order to have non-divergent generative processes with nonlinear functions, we follow (49) (See Theorem 1). Their method relies on two tricks: 1) the process is additive and the functions used have a linear behavior for large values, and 2) the coefficients are chosen so that an equivalent linear process would be stationary.</p><p>We use the following structural equation:</p><formula xml:id="formula_54">z t i := τ k=0 dz j=1 G k ij A k ij s k ij (z t-k j ) + ϵ i ,</formula><p>where ϵ i ∼ N (0, 1) and the coefficients A ij are independently sampled from U([-1, -0.2] ∪ [0.2, 1]) and will be reweighted to ensure stationary. The range around 0 is removed to ensure that the data is faithful to the graph. Following <ref type="bibr" target="#b77">(78)</ref>, each nonlinear dynamic's function s k ij has a structure similar to:</p><formula xml:id="formula_55">f 1 (x) = x(1 + 4e -x<label>2</label></formula><p><ref type="foot" target="#foot_2">foot_2</ref> )</p><formula xml:id="formula_56">f 2 (x) = x(1 + 4x 3 e -x<label>2</label></formula><p>2 ).</p><p>See the code to see all the functions that were used. As it can be noticed in Fig. <ref type="figure" target="#fig_8">6</ref>, these functions are almost linear when x is large. Then, to make the process stationary, we have to make sure that the linear process that has the same coefficient matrix A would be stationary. One way to verify this is to make sure that all the eigenvalues of the following matrix H have a modulus of less than one:</p><formula xml:id="formula_57">H =     A τ A τ -1 . . . A 0 I τ 0 . . . 0 0 . . . 0 . . . 0 0 I τ 0    <label>(47)</label></formula><p>To do so, we compute the spectrum ρ(H) and we divide each matrix A k by ρ(H) k+1 . The resulting process will be stationary <ref type="bibr" target="#b48">(49)</ref>.</p><p>For the linear datasets, we follow the same sampling process except that s k ij is the identity.</p><p>3-Graph F. To generate the observable x from z, we first sample the matrix W ∈ R dx×dz ≥0 which is non-negative and orthogonal following these steps:</p><p>1. We first sample an assignment matrix M ∈ {0, 1} dx×dz where in each row, only one element is set to 1 and the rest is equal to 0. We also make sure that for each column, there is at least one element equal to 1. Preprint 3. Finally, we normalize the column of W to ensure that it is orthogonal. In other words:</p><formula xml:id="formula_58">W :j := W:j || W:j ||2 .</formula><p>4-Decoding functions. In the nonlinear case, each function r j (from Section 3.3 §3) was randomly sampled either as a linear function or as a nonlinear function. The nonlinear function takes the following general form:</p><formula xml:id="formula_59">f (x) = ax(2σ(10x) -1),</formula><p>where a ∼ U[0.2, 0.7]. This nonlinear function is approximately an absolute value function but is still differentiable since it is smooth at x = 0. In the linear case, each function r j is the identity. We sample the observable x following:</p><formula xml:id="formula_60">x t j | z t ∼ N (r j (W z t ), σ 2 j ),</formula><p>where σ 2 j = 0.1 and 0.5 in experiments with linear and nonlinear decoding, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 REAL-WORLD DATASETS</head><p>Here, we describe how the sea-level pressure data was regridded to an icosahedral-hexagonal grid to achieve an equal-area projection <ref type="bibr">(54; 55)</ref>. This form of regridding is motivated by the fact that the poles are singularities in the traditional longitude-latitude grid, i.e. that each grid cell contains a different amount of total surface area (for 1 • × 1 • resolution, the length of a grid cell decreases from ≈ 111 km at the equator to 0 km at the poles). This means that more grid cells represent a smaller area the further we move from the equator towards the south or north poles, leading to an overrepresentation of polar regions and an underrepresentation of equatorial regions. One way to address this issue is by projecting the data on a geodesic grid, such as the icosahedral-hexagonal grid (80), as nowadays used in climate models (e.g. ICON, see Pham et al. ( <ref type="formula">66</ref>)). By regridding the data we can ensure that northern/southern weather regions are not overrepresented in the clustering and during the causal discovery process.</p><p>The sea-level pressure data was projected to the GME icosahedral grid <ref type="bibr" target="#b54">(55)</ref>, with first-order conservative remapping <ref type="bibr" target="#b31">(32)</ref> and the number of intervals N I = 24 to match the original resolution of 2.5 • × 2.5 • at the equator as closely as possible. The original netCDF4 input files had to be converted to GRIB2 files for this purpose since netCDF4 files can only store quadrilateral data, whereas GRIB2 has no such restrictions <ref type="bibr" target="#b14">(15)</ref>. After making the original data GRIB2 compliant and converting it to this format, Climate Data Operators (CDO) <ref type="bibr" target="#b81">(82)</ref>, was used to remap the longitude-latitude gridded data to the GME icosahedral-hexagonal grid. Two parameters are relevant during the regridding process: 1) N I, the number of intervals, and 2) the remapping function. The number of intervals was chosen in a manner to resemble the resolution of the original data at the equator, and in a manner to allow the recursive generation of bisected equilateral triangles <ref type="bibr" target="#b89">(90)</ref>. The remapping function was chosen to ensure a monotonic remapping that can deal with a fine-to-coarse setting. First-order or second-order conservative remapping seem to be the best choices for that and we decided to use first-order conservative remapping since it is sufficiently accurate and easier to handle. The regridded sea-level pressure files in GRIB2 format can be easily loaded with the xarray package in Python <ref type="bibr" target="#b23">(24)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E METHODS AND IMPLEMENTATION DETAILS E.1 VARIMAX-PCMCI</head><p>Varimax-PCMCI is a two-step method where 1) a dimensionality reduction is conducted to obtain time series of latent variables and then 2) a causal discovery method is applied on the latent level time series. We follow Tibau et al. ( <ref type="formula">87</ref>) by using PCA with a Varimax-rotation as the dimensionality reduction method. This method was observed empirically in Tibau et al. <ref type="bibr" target="#b86">(87)</ref> to lead to better results. For more details on PCA-Varimax, see Appendix I. For the causal discovery algorithm we use PCMCI+ <ref type="bibr" target="#b74">(75)</ref> which is an update to PCMCI, that also supports instantaneous connections. For the conditional independence tests used by PCMCI+, we use the partial correlation test for linear dynamics and the CMI-knn test <ref type="bibr" target="#b73">(74)</ref>, which is a test based on a nearest-neighbor estimator of conditional mutual information, for nonlinear dynamics. We use the implementation from <ref type="url" target="https://github.com/jakobrunge/tigramite">https://github.com/jakobrunge/tigramite</ref> and <ref type="url" target="https://github.com/xtibau/savar/tree/master/savar">https://github.com/xtibau/savar/tree/master/savar</ref>. The significance level in PCMCI+ was set to α PC = 0.2 and the time lags of causal links were restricted to τ ≥ 1, hence, only lagged links are considered. PCMCI+ then still slightly differs from PCMCI. Both share the PC 1 phase that detects lagged (supersets) of parents, but they differ in the second phase because the MCI tests are only restricted to the superset of parents found in the first phase in PCMCI+ <ref type="bibr" target="#b74">(75)</ref>, whereas all lagged links are tested again in case of PCMCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 CDSD</head><p>For all neural networks, we use leaky-ReLU as activation functions. For the neural networks g j fitting the nonlinear dynamic, we used MLPs with 2 hidden layers and 8 hidden units. For the neural network r j fitting the nonlinear encoding, instead of having d x functions, we use some parameter in order to keep the number of parameters low. We use a single neural network that receives as input the masked W z t and an embedding (dimension 10) of the index s(j) is concatenated to the input. This neural network has 2 hidden layers and 32 hidden units. The log of the variance terms and the matrix W are free parameters initialized respectively to -4 and U[ 1 10dz , 1 dz ]. The parameters Γ are initialized to 5 which corresponds to an almost full graph (i.e. σ(Γ) ≈ 1). We use the optimizer RMSProp <ref type="bibr" target="#b21">(22)</ref> with a learning rate of 1e -3 and batch size of 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 MCC METRIC</head><p>As stated in the main text, the mean coefficient correlation (MCC) is a metric commonly used in causal representation learning to assess the quality of the learned representation. It is necessary since the identifiability result is up to permutation. We use an implementation from Khemakhem et al. <ref type="bibr" target="#b36">(37)</ref> (<ref type="url" target="https://github.com/ilkhem/icebeem/blob/master/metrics/mcc.py">https://github.com/ilkhem/icebeem/blob/master/metrics/mcc.py</ref>). This corresponds to calculating the Pearson correlation between the learned representation ẑ and the ground-truth z under all possible permutations, selecting the permutation π leading to the highest score, and taking its mean. To calculate the SHD between the learned graph Ĝ and G, we first apply the permutation π to the learned graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F HYPERPARAMETER SEARCH</head><p>For all experimental conditions, we use the default hyperparameters specified in Table <ref type="table">1</ref> and<ref type="table">2</ref>. These values were determined based on the SHD from a few experiments on distinct synthetic datasets than those used for evaluation. The regularisation coefficient (for the graph sparsity of CDSD) and the alpha term (the significance threshold for the conditional independence tests of Varimax-PCMCI) have a bigger impact in terms of performance and accordingly they vary per dataset. For both methods, they have been tested respectively on the log scale [-3, 1] and [-11, -1]. For these values, for each experimental condition, 10 datasets (distinct from evaluation) were used each with 10 different values of regularisation and alpha. The average of the parameter value leading to the best SHD of the 10 datasets was used for the synthetic experiments. As stated earlier, both d z and τ are given when using synthetic datasets. In the linear dynamics case, Varimax-PCMCI uses the partial correlation test and CDSD uses neural networks g j without hidden layers. In the nonlinear dynamics case, Varimax-PCMCI uses the CMI-knn test and CDSD uses neural networks g j without 2 hidden layers and 8 hidden units.</p><p>For the real-world dataset, we reused the same default hyperparameters. For the regularisation coefficient, we selected it based on the mean-square error between the prediction xt and x t on a held-out testing set. The split ratio is 0.8 for the training set and 0.2 for the testing set. We show the running time of CDSD and Varimax-PCMCI as a function of the number of samples and the number of latent variables. Note that the methods have not been highly optimized and that these running times are only given to give a general idea and assess the trends of growth. Varimax-PCMCI is much faster than CDSD when using partial correlation as the conditional independence test (see the left panels of Fig. <ref type="figure" target="#fig_12">8</ref> and<ref type="figure" target="#fig_14">9</ref>). However, for the nonlinear dynamics, CDSD is much faster, whereas Varimax-PCMCI often requires more than 24 hours to run. We only included cases where the time was under 24 hours (see the right panels of Fig. <ref type="figure" target="#fig_12">8</ref> and<ref type="figure" target="#fig_14">9</ref>).</p><p>In Figure <ref type="figure" target="#fig_12">8</ref>, it can be observed that for linear dynamics, the running time is almost constant in function of the number of samples. However, for the nonlinear dynamics, the time for Varimax-PCMCI    increases steeply and is &gt; 24h for 5000 samples. In Figure <ref type="figure" target="#fig_14">9</ref>, it can be observed that, as expected, the running time increases as the number of latent variables of the model increases. The same pattern can be observed for Varimax-PCMCI: while the running time is extremely low for linear functions, it is very high and only below 24 hours when 5 latent variables (with 5000 samples) are considered.</p><p>All experiments were run on AMD EPYC 7742 2.25GHz 64-Core Processor with 40G of RAM. We present in Table <ref type="table" target="#tab_0">3</ref> the total running time for each experiment.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 SYNTHETIC EXPERIMENTS</head><p>We show in Fig. <ref type="figure" target="#fig_15">10</ref> the results of CDSD on the different experimental conditions in the nonlinear dynamics case. These results were not included in the main text since, as explained in the previous section, Varimax-PCMCI had a running time too high in most conditions. It can be observed that overall the nonlinear dynamics seems to be a more challenging task than its linear counterpart. Otherwise, the general trends are similar (improvement with the number of samples, deterioration as the other parameters increase).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 REAL-WORLD EXPERIMENT</head><p>In this section, we show additional results on the real-world dataset, namely we show an alternate result using a lower regularisation, we show more clearly the learned clusters and we show additional results using a lower number of latents.</p><p>In Fig. <ref type="figure" target="#fig_16">11</ref> we show an alternate result that had a similar validation loss, but learned a denser graph. Overall, the conclusions remain the same. It can be observed that the learned spatial aggregation is nearly identical to the one in the main text. The graph is denser, but the same general pattern is observed: G 1 is denser, the diagonal has edges, etc. Furthermore, a similar pattern can be observed between the nodes related to ENSO with two additional edges.</p><p>In the main text, we claimed that the learned clusters are localized. Since in the main figure of the real-world application (Fig. <ref type="figure" target="#fig_5">5</ref>) some neighboring clusters have very similar colors, for clarity, we show in Fig. <ref type="figure" target="#fig_2">12</ref> all the learned clusters separately. It can be observed that all the clusters are well localized even if this is not directly enforced in CDSD. In contrast, if CDSD without any constraints on W is used, the learned clusters are not localized (Fig. <ref type="figure" target="#fig_5">15</ref> and Fig. <ref type="figure" target="#fig_20">16</ref>) and regions specific to ENSO are now all in a unique cluster.</p><p>We also investigated using a number of latent relatively smaller (d z = 20). Fifty latents were chosen in the main text since it is close to what has been used in similar studies (60 were used in <ref type="bibr" target="#b77">(78)</ref>). In Fig. <ref type="figure" target="#fig_17">13</ref> we present the learned clustering. It can be seen that the coarser aggregation has many regions that are the union of the one observed in Fig. <ref type="figure" target="#fig_5">5</ref>, however, interestingly, the regions related to ENSO (WPAC, CPAC, Preprint EPAC, but not ATL) are still separated. Contrary to using fifty latents, some regions are composed of disconnected regions potentially indicating that the number of latent variables is too low, see Fig. <ref type="figure" target="#fig_4">14</ref> For a feature k we assume the observables x k can only be related to the latent variables z k . This amounts to blacklisting some edges in W ∈ R dz×dx . Thus, W will be a block matrix of the form:</p><formula xml:id="formula_61">    W 1 W 2 . . . W k     ,<label>(48)</label></formula><p>where</p><formula xml:id="formula_62">W k ∈ R d k z ×d k</formula><p>x is the matrix defining the connections of the variable related to the feature k. Besides this change, the orthogonality constraint can be directly applied to each matrix W k .</p><p>Multiple subjects case. In many scientific applications such as neurosciences, many subjects are observed and it is often assumed that they share common properties. For example, in a brain imaging application, one could assume that the brain regions are invariant, whereas the connectivity between these regions is patient-specific (as in Monti and Hyvärinen <ref type="bibr" target="#b55">(56)</ref>). Similarly to the multivariate case, we can use graphs with an extra dimension (G ∈ {0, 1} dp×τ ×dz×dz where d p is the number of patients) and blacklist connections along the dimension related to the patient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 SUPPORTING INSTANTANEOUS CAUSAL RELATIONSHIPS</head><p>In order to support instantaneous relations, we add two components: 1) a graph G 0 and the parameters Γ 0 to the generative model, and 2) an acyclicity constraint to the objective. Note that contrary to G 1 , . . . , G τ , the graph G 0 has to be a directed acyclic graph (DAG) in order to have a valid factorization. We have the following modified transition model:</p><formula xml:id="formula_63">p(z t | z &lt;t ) := dz j=1 p(z t j | z pa G 0 j , z &lt;t ). (<label>49</label></formula><formula xml:id="formula_64">)</formula><p>Each conditional is:</p><p>p(z t j | z pa G 0 j , z &lt;t ) := h(z t j ; g j ([G 0 j: ⊙ z t , G 1 j: ⊙ z t-1 , . . . , G τ j: ⊙ z t-τ ]) ).</p><p>The observation and the density models remain the same. For the objective, we add the acyclicity constraint (95) on Γ 0 : Tr(e σ(Γ 0 ) ) -d z = 0.</p><p>This constraint formulation is continuous and thus differentiable. We can then use the quadratic penalty method to optimize this constrained problem <ref type="bibr" target="#b57">(58)</ref>. However, in this case, the identifiability of the graph G is not guaranteed: at best the Markov Equivalence Class can be recovered or additional assumptions are needed such as assuming an additive noise model. </p><formula xml:id="formula_67">p (k) (z t | z &lt;t ) := j∈I k p (k) (z t j | z &lt;t ) j / ∈I k p(z t j | z &lt;t ),<label>(52)</label></formula><p>where p (k) are conditionals different from the observational conditionals except for k = 0 where I 0 := ∅. Since there are no interventions on x, the conditional p(x t | z t ) remains the same as in the observational case. The joint interventional distribution is:</p><formula xml:id="formula_68">p (k) (x ≤T , z ≤T ) := T t=1 p (k) (z t | z &lt;t )p(x t | z t ) .<label>(53)</label></formula><p>In terms of ELBO, we now have:</p><formula xml:id="formula_69">log p (k) (x ≤T ) ≥ T t=1 E z t ∼q(z t |x t ) log p(x t | z t ) -<label>(54)</label></formula><p>E z &lt;t ∼q(z &lt;t |x &lt;t ) KL q(z t | x t ) || p (k) (z t | z &lt;t ) .</p><p>We denote this ELBO as L (k)</p><p>x . Finally, the objective changes to: </p><p>where ϕ is now augmented with different parameters for each conditional p (k) .</p><p>I PCA-VARIMAX I.1 PCA PCA is a commonly used dimensionality reduction method that finds a linear mapping W between the data and a latent projection having a smaller dimensionality. It can be framed as finding the projection that maximizes the variance or, alternatively, as the projection that minimizes the reconstruction loss <ref type="bibr">(64; 23)</ref>. If we consider the latter formulation, we can write the PCA method as the following optimization problem:</p><formula xml:id="formula_72">L(x; W ) = ||x -W W T x|| 2 2 Ŵ ∈ arg min W s.t.W T W =I dz L(x; W ),</formula><p>where x ∈ R dx and W ∈ R dx×dz .</p><p>In that case, the matrix W is not identifiable since any rotation would lead to the same fit of PCA (as already noted by (3)). To see this, suppose that a rotation matrix R ∈ R dz×dz is applied to W . The resulting matrix W = W R is still orthogonal (since orthogonal matrices are closed under multiplication) and leads to the same loss since RR T = I:   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: In the proposed generative model, the variables z are latent and x are observable variables.G k represents the connections between the latent variables, and F the connections between the latents and the observables (dashed lines). The colors represent the different groups. For clarity, we illustrate here connections only up to G 1 , but our method also leverages connections of higher order.</figDesc><graphic coords="3,157.50,81.86,297.00,79.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Preprint</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of Varimax-PCMCI and CDSD in terms SHD (lower is better) on simulated datasets with linear decoding and both a) linear and b) nonlinear latent dynamics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>PreprintFigure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of CDSD and Varimax-PCMCI in terms of MCC (higher is better) and SHD (lower is better) on simulated datasets with linear dynamics and nonlinear decoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of CDSD to DMS and iVAE in terms of MCC.</figDesc><graphic coords="8,333.72,90.08,160.37,73.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Overview of the climate science results for CDSD. a) Segmentation of the Earth's surface according to W . The groups are colored and numbered based on the latent variable to which they are related. b) Adjacency matrices for latent dynamic graphs G 1 , . . . , G 5 , shown as d z × d z heatmaps. c) Subgraph of G 1 showing the learned causal relationships between known ENSO-related regions.</figDesc><graphic coords="9,120.87,81.86,368.27,113.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig</head><label></label><figDesc>Fig. 5a shows the learned spatial aggregations and the causal graph G obtained with CDSD. The learned aggregations match well with the coarser climatological regions used in the latest climate change assessment reports of the Intergovernmental Panel on Climate Change (IPCC), which were manually defined (compare to Figure 1 in (29)). The learned clusters broadly reflect a superposition of the effects of transport timescales, ocean-to-land boundaries, and the zonal and meridional patterns of the tropospheric circulation (Hadley, Ferrel, and polar cells) in both hemispheres. Among the visually most prominent features is the identification of East Pacific, Central Pacific, and Western Pacific clusters (clusters 13, 39, 48) along the tropical Pacific. These zones are well-known to be coupled through ENSO, but the East Pacific typically sees the most pronounced temperature oscillations due to its shallow oceanic thermocline (e.g., 61). We also recover a relatively zonal structure of clusters in the Southern Hemisphere mid-latitudes (clusters 17, 2, 41, 14, 20, 40, 15, 18, and 43 from west to east) where the zonal tropospheric circulation moves relatively freely without significant disturbances from land boundaries. While not strictly enforced, all the learned regions are spatially connected/homogeneous, i.e. not divided into several de-localized parts (see Appendix G.3). Highly de-localized, globally distributed, components are for example a major issue in interpreting standard principal component analyses of MSLP data<ref type="bibr" target="#b20">(21)</ref>. In contrast, the regions learned by CDSD without constraints are not localized and are harder to associate to known regions such as those related to ENSO (see Appendix G.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>) We know that d 2 is not equal to d 1 from Equation 40, since if d 1 and d 2 were the same index, then y d1 would have two distinct parents if f , namely π(k) and k ′ . By selecting only columns d 1 and d 2 in Equation 35, we obtain the following equality D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Examples of nonlinear functions used for the dynamics. For large values, the functions behave as linear functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visualization of icosahedral-hexagonal grids with different N I's (number of intervals on triangle edge). the grid is generated recursively by halving the triangles, resulting in new, finer triangles. The figure stems from Majewski (54).</figDesc><graphic coords="23,187.20,81.86,237.60,228.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Table 1 : 32 Table 2 :</head><label>1322</label><figDesc>Default hyperparameters for CDSD CDSD hyperparameters ALM parameters: threshold: 10 -4 , µ 0 : 10 -3 , γ 0 : 0, η: 2, δ: 0.9 Optimizer: RMSProp, learning rate: 10 -3 , batch size: 64 Transition NN (g j ): Nonlinearity: Leaky-ReLU, # hidden layers: [linear = 0, nonlinear = 2], # hidden units: 8 Encoder/Decoder NN for nonlinear encoding: Nonlinearity: Leaky-ReLU, # hidden layers: 2, # hidden units: Default hyperparameters for PCMCI+ PCMCI+ hyperparameters α P C = 0.2 τ min = 1 CI test: linear = partical corr., nonlinear = CMI-knn G ADDITIONAL EXPERIMENTS G.1 RUNNING TIME</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>the number of samples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Comparison of the running time with respect to the number of samples in linear and nonlinear dynamics settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>the number of latent variables</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Comparison of the running time with respect to the number of latent variables in linear and nonlinear dynamics settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Comparison in terms of SHD (lower is better) on nonlinear dynamics datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Spatial partitioning learned by CDSD using a lower regularisation. The dashed lines in the causal graph represent edges that were not present in the main text.</figDesc><graphic coords="26,117.90,211.25,376.16,125.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Preprint H. 3</head><label>3</label><figDesc>SUPPORTING INTERVENTIONS In order to support interventions on z, the model can be adapted following Brouillard et al. (9); Gao et al. (17); Lei et al. (48). Assume that we have interventional data from K different interventions. The k-th interventional distribution of the transition model p (k) (z t | z &lt;t ) with the intervention target I k ⊆ [d z ] is given by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>E</head><label></label><figDesc>G∼σ(Γ) E x∼p (k) L (k) x (W, Γ, ϕ) -λ s ||σ(Γ)|| 1 s.t.W is orthogonal and non-negative,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>L 2 = 2 =</head><label>22</label><figDesc>(x; W ) = ||x -W W T x|| 2 ||x -W RR T W T x|| 2 ||x -W W T x|| 2 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Representation of each region learned by CDSD without constraint on W (d z = 50).</figDesc><graphic coords="32,117.90,97.35,376.19,596.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Comparison of 1) PCA without rotation (PCA), 2) PCA with the Varimax rotation (PCA-Var), and 3) PCA with the Varimax rotation and reflection (PCA-Var+). The Varimax rotation is crucial to recover the graph (left panel) and a good representation (middle panel). In order to recover the matrix W, the reflection operation is necessary (right panel).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="31,117.90,251.49,376.19,231.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 3 :</head><label>3</label><figDesc>Running time for each set of experiments</figDesc><table><row><cell>Synthetic experiments</cell></row><row><cell>Linear dynamics, linear decoder:</cell></row><row><cell>CDSD: 76 hours</cell></row><row><cell>Varimax-PCMCI: 0.3 hours</cell></row><row><cell>Nonlinear dynamics, linear decoder:</cell></row><row><cell>CDSD: 116 hours</cell></row><row><cell>Varimax-PCMCI: 1212 hours</cell></row><row><cell>Linear dynamics, nonlinear decoder:</cell></row><row><cell>CDSD: 383 hours</cell></row><row><cell>Varimax-PCMCI: 0.03 hours</cell></row><row><cell>Ablation study: 4.3 hours</cell></row><row><cell>Real-world experiments: 75 hours</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. H EXTENSIONS OF CDSD H.1 USING CDSD IN MULTIVARIATE AND MULTI-SUBJECT SETTINGS Multivariate case. Researchers are often interested in multivariate problems where each feature might require different clustering. For the climate example, researchers might have several features of interest: sea-level pressure, temperature, precipitation, etc. While we only presented the univariate version of CDSD, it can easily be used as a multivariate method by slightly modifying W . Assume we have d features of interest. Reusing the notation from Section 3, let x t ki and z t kj be the observable and latent variables at time t pertaining to the k-th feature. Note that now the cardinality of d k x and d k z can vary in function of k. As a matter of fact, since they are different features, it might be adequate to model them at a different coarsening. We denote the total dimensionality by d x :=</figDesc><table><row><cell>d k=1 d k x and d z :=</cell><cell>d k=1 d k z .</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>As a side note, the general idea of aggregating several low-level observations in order to only consider causal relationships at a high level is somewhat reminiscent of causal discovery with typed variables<ref type="bibr" target="#b9">(10)</ref>, causal abstractions (73; 5) and causal feature learning<ref type="bibr" target="#b12">(13)</ref> which was also applied to climate science<ref type="bibr" target="#b11">(12)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Note that, to simplify, we will sometimes say that W is orthogonal even if it is not a square matrix; by that, we specifically mean that its columns are orthonormal vectors.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>We sample independently A ij ∼ U([0.2, 1]) and mask it: W := M ⊙ A.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C OPTIMIZATION C.1 AUGMENTED LAGRANGIAN METHOD</head><p>The main idea of the Augmented Lagrangian Method (ALM) is to remove constraints in an optimization problem and instead add a penalty term to the objective (For more detailed explanations, see Nocedal and Wright <ref type="bibr" target="#b59">(60)</ref>, <ref type="bibr">Chapter 17)</ref>. For an optimization problem where we aim to minimize L(x) subject to a constraint h(x) = 0, the equivalent ALM objective is:</p><p>To approximately solve the constrained problem, a sequence of problems will be solved where λ k and µ k will be incrementally increased as k increases. A problem is considered solved when the loss on a held-out dataset does not decrease. Then, a new problem is initialized with the values λ k+1 and µ k+1 :</p><p>where µ k &gt; 0 and η &gt; 1 and W * (k) is the approximate solution to the k-th problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 PROJECTED GRADIENT</head><p>The projected gradient descent method can be used when some parameters are under constraints. In general, we proceed in two steps: 1) perform a step of gradient descent and 2) project the result on the feasible set. More formally, we do a normal gradient descent step from x k :</p><p>and then we project the result y k+1 on the feasible set Q:</p><p>In our case, the feasible set is R ≥0 , thus the projection is simply:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DATASETS D.1 SYNTHETIC DATASETS</head><p>Here, we detail the generative process of the synthetic datasets. The four main steps are:</p><p>2. Sample mechanisms to generate the latent variables z with relations following G.</p><p>3. Sample a matrix W .</p><p>4. Sample mechanisms of the decoding function to generate the observable x from z.</p><p>1-Graphs G. For the adjacency matrices representing the lagged relations, we sampled independently G k ij ∼ Bernoulli(p) where p ∈ [0, 1] is a parameter corresponding to the probability of adding an edge. As a reminder:</p><p>is a parent of z t i . Note that since these adjacency matrices represent links at different timesteps, we don't need to sample DAGs since it is impossible to create cycles.</p><p>For the graph G 1 , we systematically set the elements of its diagonal to 1. These edges correspond to the relations of z t i to themselves at the previous timestep: z t-1 i . This assumption is often observed in real-world phenomena and is commonly used to generate time-series datasets (78; 49).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint I.2 THE VARIMAX ROTATION</head><p>In practice, researchers from many different fields, such as Earth science (88; 87), use a criterion called Varimax <ref type="bibr" target="#b32">(33)</ref> in order to find a particular rotation of W. The optimal rotation according to this criterion is given by:</p><p>While it leads to the same loss, this rotation has been shown empirically to lead to more interpretable variables (31; 68) (i.e. the latent dimensions are recognized as important factors of variation by experts in the fields). The criterion corresponds to maximizing the sum of the variances of the squared loadings W 2 ij . Intuitively, this criterion will make the large loadings larger and the small loadings smaller making the rotated matrix sparse. The good representation learned by PCA-Varimax when W respects the single-parent assumption could be explained by the identifiability results of Zheng et al. <ref type="bibr" target="#b95">(96)</ref>. They show that, under a more general sparsity assumption than the single-parent assumption, W can be identified using an adequate sparsity regularisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.3 EMPIRICAL VALIDATION</head><p>In this section, we show empirically that PCA-Varimax can learn a disentangled representation when the decoding function is linear and W respects the single-parent assumption. The results stress the fact that PCA without the Varimax rotation generally leads to entangled representation.</p><p>The PCA-Varimax algorithm used in Varimax-PCMCI can be decomposed in three steps:</p><p>• Step 1. Apply PCA and recover a matrix Ŵ .</p><p>• Step 2. Find the rotation matrix R VAR that maximizes the Varimax criterion. Apply this matrix to Ŵ :</p><p>• Step 3. Apply a reflection matrix S:</p><p>Ŵ + VAR := ŴVAR S, where S jj = sgn(arg max i |W ij |) and S ij = 0 for all i ̸ = j.</p><p>For this experiment, we do an ablation study of these three steps, namely we consider the three conditions: 1) PCA without rotation (PCA), 2) PCA with the Varimax rotation (PCA-Var), and 3) PCA with the Varimax rotation and reflection (PCA-Var+). We reuse the default synthetic datasets with the same alpha values that were chosen in the main hyperparameter search. For each condition, we present the average on 10 different datasets.</p><p>In terms of representation, it can be seen in Figure <ref type="figure">17</ref> that the Varimax rotation is essential to learn a disentangled representation as observed by the MCC metric. Learning a good representation has a repercussion on the recovery of the graph G as shown by the SHD metric. The reflection operation might flip signs but does not have any impact in terms of disentanglement. However, if we look at the recovered matrices Ŵ in terms of absolute error with W , the reflection allows us to recover a matrix more similar to W .   </p><note type="other">Preprint</note></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Properties from mechanisms: an equivariance perspective on identifiable representation learning</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised representation learning with sparse perturbations</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Statistical inference in</title>
		<author>
			<persName><forename type="first">Herman</forename><surname>Tw Anderson</surname></persName>
		</author>
		<author>
			<persName><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability: Held at the Statistical Laboratory</title>
		<meeting>the Third Berkeley Symposium on Mathematical Statistics and Probability: Held at the Statistical Laboratory</meeting>
		<imprint>
			<publisher>Univ of California Press</publisher>
			<date type="published" when="1954-12">December, 1954, July and August, 1955. 1956</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">111</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A practical algorithm for topic modeling with provable guarantees</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Approximate causal abstractions</title>
		<author>
			<persName><forename type="first">Sander</forename><surname>Beckers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">Y</forename><surname>Halpern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparse bayesian infinite factor models</title>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="291" to="306" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive estimation in structured factor models with applications to overlapping clustering</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florentina</forename><surname>Bunea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marten</forename><surname>Wegkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly supervised causal representation learning</title>
		<author>
			<persName><forename type="first">Johann</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pim</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Differentiable causal discovery from interventional data</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Brouillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Drouin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21865" to="21877" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Typing assumptions improve identification in causal discovery</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Brouillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perouz</forename><surname>Taslakian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Drouin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Causal Learning and Reasoning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="162" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Function classes for identifiable nonlinear independent component analysis</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="16946" to="16961" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unsupervised discovery of el nino using causal feature learning on microlevel climate data</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Chalupka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Bischoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09370</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Causal feature learning: an overview</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Chalupka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviormetrika</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="137" to="164" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identification of linear latent variable model with arbitrary distribution</title>
		<author>
			<persName><forename type="first">Zhengming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="6350" to="6357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Guide to the wmo table driven code form used for the representation and exchange of regularly spaced data in binary form: Fm 92 grib</title>
		<author>
			<persName><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><surname>Clochard</surname></persName>
		</author>
		<author>
			<persName><surname>Hennessy</surname></persName>
		</author>
		<ptr target="http://www.wmo.int/pages/prog..." />
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">98</biblScope>
		</imprint>
		<respStmt>
			<orgName>WMO Tech</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On causal discovery from time series data using fci. Probabilistic graphical models</title>
		<author>
			<persName><forename type="first">Doris</forename><surname>Entner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Idyno: Learning nonparametric dags from interventional dynamic data</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debarun</forename><surname>Bhattacharjya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6988" to="7001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Diard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hueber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12595</idno>
		<title level="m">Dynamical variational autoencoders: A comprehensive review</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The incomplete rosetta stone problem identifiability results for multi-view nonlinear ICA</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrjou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Independent mechanism analysis, a new concept?</title>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Stimper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Besserve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="28233" to="28248" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Empirical orthogonal functions and related techniques in atmospheric science: A review</title>
		<author>
			<persName><forename type="first">Abdel</forename><surname>Hannachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Stephenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Climatology: A Journal of the Royal Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1119" to="1152" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural networks for machine learning lecture 6a overview of mini-batch gradient descent</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cited on</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Analysis of a complex of statistical variables into principal components</title>
		<author>
			<persName><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of educational psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">417</biblScope>
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">xarray: N-D labeled arrays and datasets in Python. In revision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hamman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Open Res. Software</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised feature extraction by time-contrastive learning and nonlinear ica</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Morioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonlinear ICA of Temporally Dependent Stationary Sources</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Morioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 20th International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nonlinear independent component analysis: Existence and uniqueness results</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petteri</forename><surname>Pajunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="429" to="439" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An update of ipcc climate reference regions for subcontinental analysis of climate model data: definition and aggregated datasets</title>
		<author>
			<persName><forename type="first">Maialen</forename><surname>Iturbide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lincoln</forename><forename type="middle">M</forename><surname>Gutiérrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquín</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruth</forename><surname>Bedia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ezequiel</forename><surname>Cerezo-Mota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><forename type="middle">S</forename><surname>Cimadevilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Cofiño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">Henrique</forename><surname>Di Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><forename type="middle">V</forename><surname>Faria</surname></persName>
		</author>
		<author>
			<persName><surname>Gorodetskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Earth System Science Data</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2959" to="2970" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Applied multivariate statistical analysis</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnson</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><forename type="middle">W</forename><surname>Wichern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">First-and second-order conservative remapping schemes for grids in spherical coordinates</title>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">W</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2204" to="2210" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The varimax criterion for analytic rotation in factor analysis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="187" to="200" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Structural agnostic model, causal discovery and penalized adversarial learning</title>
		<author>
			<persName><forename type="first">Diviyan</forename><surname>Kalainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Goudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michèle</forename><surname>Sebag</surname></persName>
		</author>
		<author>
			<persName><surname>Sam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The ncep/ncar 40-year reanalysis project</title>
		<author>
			<persName><forename type="first">Eugenia</forename><surname>Kalnay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masao</forename><surname>Kanamitsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kistler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Deaven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Gandin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Iredell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suranjana</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Woollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American meteorological Society</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="437" to="472" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Nan</forename><surname>Rosemary Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olexa</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Mozer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01075</idno>
		<title level="m">Chris Pal, and Yoshua Bengio. Learning neural causal models from unknown interventions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ice-beem identifiable conditional energy-based deep models based on nonlinear ica</title>
		<author>
			<persName><forename type="first">I</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Variational autoencoders and nonlinear ica: A unifying framework</title>
		<author>
			<persName><forename type="first">Ilyes</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Twenty Third International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Variational autoencoders and nonlinear ica: A unifying framework</title>
		<author>
			<persName><forename type="first">Ilyes</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2207" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ice-beem: Identifiable conditional energy-based deep models based on nonlinear ica</title>
		<author>
			<persName><forename type="first">Ilyes</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Monti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Diederik Kingma, and Aapo Hyvarinen</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12768" to="12778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Identifiability of deep generative models without auxiliary information</title>
		<author>
			<persName><forename type="first">Bohdan</forename><surname>Kivva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goutham</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Towards nonlinear disentanglement in natural data with temporal sparse coding</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Klindt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Ustyuzhaninov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Paiton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Nonparametric bayesian sparse factor models with application to gene expression modeling</title>
		<author>
			<persName><forename type="first">David</forename><surname>Knowles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Applied Statistics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Causal clustering for 1-factor measurement models</title>
		<author>
			<persName><forename type="first">Erich</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Ramsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1655" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA</title>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pau</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><forename type="middle">E</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Rémi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Priol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Conference on Causal Learning and Reasoning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Synergies between disentanglement and sparsity: Generalization and identifiability in multi-task learning</title>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divyat</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Bertrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Anson</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.11131</idno>
		<title level="m">Variational causal dynamics: Discovering modular world models from interventions</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Single-index additive vector autoregressive time series models</title>
		<author>
			<persName><forename type="first">Yehua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Genton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="369" to="388" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">CITRIS: Causal Identifiability from Temporal Intervened Sequences</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sindy</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Raetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4114" to="4124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Weakly-supervised disentanglement without compromises</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The new global icosahedral-hexagonal gridpoint model gme of the deutscher wetterdienst</title>
		<author>
			<persName><forename type="first">Detlev</forename><surname>Majewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECMWF on Recent Developments in Numberical Methods for Atmospheric Modelling</title>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="page" from="172" to="201" />
		</imprint>
	</monogr>
	<note>Procceding of a Seminar</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The operational global icosahedral-hexagonal gridpoint model gme: Description and high-resolution tests</title>
		<author>
			<persName><forename type="first">Detlev</forename><surname>Majewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dörte</forename><surname>Liermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Prohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bodo</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Buchhold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hanisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Werner</forename><surname>Wergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Baumgardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="319" to="338" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">A unified probabilistic model for learning latent factors and their connectivities from high-dimensional data</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Pio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monti</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09567</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Identifiable variational autoencoders via sparse decoding</title>
		<author>
			<persName><forename type="first">Gemma</forename><forename type="middle">E</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhanya</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On the convergence of continuous constrained optimization for structure learning</title>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8176" to="8198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Masked gradient-based causal structure learning</title>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuangyan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the 2022 SIAM International Conference on Data Mining (SDM)</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Numerical optimization</title>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">On the role of ozone feedback in the enso amplitude response under global warming</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Nowack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Braesicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Pyle</surname></persName>
		</author>
		<idno type="DOI">10.1002/2016GL072418</idno>
		<ptr target="https://agupubs.onlinelibrary.wiley.com/doi/10.1002/2016GL072418" />
	</analytic>
	<monogr>
		<title level="j">Geophysical Research Letters</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="3858" to="3866" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Causal networks for climate model evaluation and constrained projections</title>
		<author>
			<persName><forename type="first">Peer</forename><surname>Nowack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronika</forename><surname>Eyring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><forename type="middle">D</forename><surname>Haigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1415</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Dynotears: Structure learning from time-series data</title>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Roxana</forename><surname>Pamfil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nisara</forename><surname>Sriwattanaworachai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shaan</forename><surname>Desai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Philip</forename><surname>Pilgerstorfer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Konstantinos</forename><surname>Georgatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Beaumont</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1595" to="1605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">on lines and planes of closest fit to systems of points in space</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><surname>Liii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The London, Edinburgh, and Dublin philosophical magazine and journal of science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="559" to="572" />
			<date type="published" when="1901">1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Elements of Causal Inference -Foundations and Learning Algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Icon in climate limited-area mode (icon release version 2.6. 1): a new regional climate model</title>
		<author>
			<persName><forename type="first">Trang</forename><surname>Van Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Steger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burkhardt</forename><surname>Rockel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Keuler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariano</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Günther</forename><surname>Zängl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Früh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Geoscientific Model Development</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="985" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">A User&apos;s Guide to Measure Theoretic Probability</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pollard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Applied functional data analysis: methods and case studies</title>
		<author>
			<persName><forename type="first">James</forename><surname>Ramsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Advancing functional connectivity research from association to causation</title>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">B</forename><surname>Andrew T Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><forename type="middle">D</forename><surname>Headley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Mill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucina</forename><forename type="middle">Q</forename><surname>Sanchez-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Uddin</surname></persName>
		</author>
		<author>
			<persName><surname>Marinazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">A</forename><surname>Daniel J Lurie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">José</forename><surname>Valdés-Sosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharat</forename><forename type="middle">B</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><surname>Biswal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1751" to="1760" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Embrace the gap: Vaes perform independent mechanism analysis</title>
		<author>
			<persName><forename type="first">Patrik</forename><surname>Reizinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Zietlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Martius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Besserve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="12040" to="12057" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Local disentanglement in variational auto-encoders using jacobian l_1 regularization</title>
		<author>
			<persName><forename type="first">Travers</forename><surname>Rhodes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="22708" to="22719" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">From temporal to contemporaneous iterative causal discovery in the presence of latent confounders</title>
		<author>
			<persName><forename type="first">Shami</forename><surname>Raanan Y Rohekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Nisimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Gurwicz</surname></persName>
		</author>
		<author>
			<persName><surname>Novik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Paul K Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Weichwald</surname></persName>
		</author>
		<author>
			<persName><surname>Bongers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Grosse-Wentrup</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00819</idno>
		<title level="m">Causal consistency of structural equation models</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="938" to="947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1388" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Identifying causal gateways and mediators in complex spatio-temporal systems</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Petoukhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">F</forename><surname>Donges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaroslav</forename><surname>Hlinka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Jajcay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Vejmelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hartman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Marwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Paluš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Kurths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Inferring causation from time series in earth system sciences</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bathiany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Bollt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustau</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dim</forename><surname>Coumou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Deyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marlene</forename><surname>Kretschmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">D</forename><surname>Mahecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Muñoz-Marí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Detecting and quantifying causal associations in large nonlinear time series datasets</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peer</forename><surname>Nowack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marlene</forename><surname>Kretschmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Flaxman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Sejdinovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4996</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Gherardo Varando, Veronika Eyring, and Gustau Camps-Valls. Causal inference for time series</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Gerhardus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Earth &amp; Environment</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">2553</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Geodesic discrete global grid systems</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Sahr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kimerling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cartography and Geographic Information Science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="134" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Toward causal representation learning</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="612" to="634" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">Uwe</forename><surname>Schulzweida</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.7112925</idno>
		<ptr target="https://doi.org/10.5281/zenodo.7112925" />
		<title level="m">Cdo user guide</title>
		<imprint>
			<date type="published" when="2022-10">October 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">The hardness of conditional independence testing and the generalised covariance measure</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rajen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1514" to="1538" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning the structure of linear latent variable models</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chickering</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Approximate kernel-based conditional independence tests for fast non-parametric causal discovery</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Visweswaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Nts-notears: Learning nonparametric temporal dags with time-series data and prior knowledge</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Schulte</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04286</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A spatiotemporal stochastic climate model for benchmarking causal discovery methods for teleconnections</title>
		<author>
			<persName><forename type="first">Xavier-Andoni</forename><surname>Tibau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Gerhardus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronika</forename><surname>Eyring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmental Data Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Non-random correlation structures and dimensionality reduction in multivariate climate data</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Vejmelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucie</forename><surname>Pokorná</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaroslav</forename><surname>Hlinka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hartman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Jajcay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Paluš</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Climate Dynamics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="2663" to="2682" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Self-supervised learning with data augmentations provably isolates content from style</title>
		<author>
			<persName><forename type="first">J</forename><surname>Von Kugelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Geometric properties of the icosahedral-hexagonal grid on the two-sphere</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Luen</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2536" to="2559" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Causal discovery of 1-factor measurement models in linear latent variable models with arbitrary noise distributions</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangbo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Learning temporally causal latent processes from general temporal data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Kernel-based conditional independence test and application in causal discovery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Large-scale kernel methods for independence testing</title>
		<author>
			<persName><forename type="first">Qinyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Filippi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Sejdinovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="113" to="130" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Dags with no tears: Continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Pradeep K Ravikumar</surname></persName>
		</author>
		<author>
			<persName><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07751</idno>
		<title level="m">On the identifiability of nonlinear ica: Sparsity and beyond</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Teleconnection paths via climate network direct link detection</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Gozolchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yosef</forename><surname>Ashkenazy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shlomo</forename><surname>Havlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page">268501</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
