<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GROUNDING CONTINUOUS REPRESENTATIONS IN GEOMETRY: EQUIVARIANT NEURAL FIELDS</title>
				<funder ref="#_unS73H3">
					<orgName type="full">Dutch Cancer Society (KWF)</orgName>
				</funder>
				<funder ref="#_KK7nHSs #_eRVJswR">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_BC74dYQ">
					<orgName type="full">Elekta Oncology Systems AB</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-07">7 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Wessels</surname></persName>
							<email>d.r.wessels@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">VISLab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Knigge</surname></persName>
							<email>d.m.knigge@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">VISLab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Riccardo</forename><surname>Valperga</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">VISLab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samuele</forename><surname>Papa</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">VISLab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sharvaree</forename><surname>Vadgama</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">VISLab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">VISLab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">VISLab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Amlab</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">VISLab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GROUNDING CONTINUOUS REPRESENTATIONS IN GEOMETRY: EQUIVARIANT NEURAL FIELDS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-07">7 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2406.05753v5[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conditional Neural Fields (CNFs) are increasingly being leveraged as continuous signal representations, by associating each data-sample with a latent variable that conditions a shared backbone Neural Field (NeF) to reconstruct the sample. However, existing CNF architectures face limitations when using this latent downstream in tasks requiring fine-grained geometric reasoning, such as classification and segmentation. We posit that this results from lack of explicit modelling of geometric information (e.g. locality in the signal or the orientation of a feature) in the latent space of CNFs. As such, we propose Equivariant Neural Fields (ENFs), a novel CNF architecture which uses a geometry-informed cross-attention to condition the NeF on a geometric variable-a latent point cloud of features-that enables an equivariant decoding from latent to field. We show that this approach induces a steerability property by which both field and latent are grounded in geometry and amenable to transformation laws: if the field transforms, the latent representation transforms accordingly-and vice versa. Crucially, this equivariance relation ensures that the latent is capable of (1) representing geometric patterns faitfhully, allowing for geometric reasoning in latent space, (2) weight-sharing over similar local patterns, allowing for efficient learning of datasets of fields. We validate these main properties in a range of tasks including classification, segmentation, forecasting, reconstruction and generative modelling, showing clear improvement over baselines with a geometry-free latent space. Code attached to submission here. Code for a clean and minimal repo here. * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Neural Fields (NeFs) <ref type="bibr" target="#b54">(Xie et al., 2022)</ref> have recently gained prominence in the machine learning community as a novel representation method that models data as continuous functions. These fields, expressed as f θ : R d → R c , map spatial coordinates-such as pixel locations x ∈ R 2 -to a corresponding signal, like RGB values f θ (x) ∈ R 3 , with θ representing the model's parameters. The parameters are optimized to approximate a target signal f , ensuring ∀x : f (x) ≈ f θ (x). This capability makes NeFs effective for representing continuous spatial, spatio-temporal, and geometric data, particularly in cases where grid-based methods fall short <ref type="bibr" target="#b19">(Dupont et al., 2022)</ref>. Their promise lies in serving as resolution-free representation that may be used irrespective of data resolution or discretization <ref type="bibr" target="#b54">(Xie et al., 2022)</ref>. Moreover, NeF-representations unify downstream models over different data modalities, allowing for transfer of modelling principles between data modalities that otherwise require data-specific engineering efforts <ref type="bibr" target="#b19">(Dupont et al., 2022;</ref><ref type="bibr" target="#b36">Papa et al., 2023)</ref>.</p><p>Building on this concept, conditional neural fields (CNFs) introduce a conditioning variable z ∈ Z to the model. Given a dataset of N fields D = {f i : R d → R c } N i=1 , each specific field can now be represented by a specific conditioning variable z i via ∀x : f i (x) ≈ f θ (x; z i ), while model weights θ are shared over the entire dataset. This approach enables CNFs to efficiently model datasets of fields using a set of latent variables that condition a shared backbone NeF. This allows for representing and analysing fields f i by means of their latent representation z i , enabling novel approaches for solving tasks involving fields through a framework known as learning with functa <ref type="bibr">(Dupont et al.,</ref> Figure <ref type="figure">1</ref>: Equivariant Neural Fields (ENFs) ground Neural Fields (NeFs) in geometry using a latent point cloud. A latent set z consisting of tuples (p i , c i ) of pose information p i and context c i is optimized to reconstruct to the field f (•) as a function f θ (•; z) using gradient-descent. Due to their explicit positional grounding and locality, the latent retains important geometric features in the input field. The latent z can then be used in downstream tasks, e.g. classification, segmentation, and geometric reasoning, where transformations in the field are mirrored in the latent representation through group actions L g [f ] ∼ g • z.</p><p>2022). Applications of this include tasks such as classification, segmentation, and the generation of continuous fields <ref type="bibr" target="#b19">(Dupont et al., 2022;</ref><ref type="bibr" target="#b36">Papa et al., 2023)</ref>, as well as continuous PDE forecasting by solving dynamics in the latent space <ref type="bibr" target="#b57">(Yin et al., 2022;</ref><ref type="bibr" target="#b29">Knigge et al., 2024)</ref>.</p><p>Geometry in CNFs A notable limitation of conventional CNFs, however, is a lack of explicit geometric interpretability; each field f i is encoded by a "global" variable z i , meaning for instance that any notion of locality or other explicit spatial relationships-which have proven a strong inductive bias in computer vision-is lost. Althought this global representation has inherent benefits, e.g. enabling the use of simple MLPs as downstream models and allowing for intuitive interpolation between latent signal representations, empirically it has shown limited performance in settings where samples of the dataset are not consistently globally aligned <ref type="bibr" target="#b3">(Bauer et al., 2023)</ref>. For instance, in classification tasks, spatial organisation of an image's content is crucial for understanding shape and enabling geometric reasoning <ref type="bibr" target="#b51">(Van Quang et al., 2019)</ref>; current neural fields lack such geometric inductive biases, limiting their performance in e.g. classification and generative modelling <ref type="bibr" target="#b3">(Bauer et al., 2023;</ref><ref type="bibr" target="#b36">Papa et al., 2023)</ref>. To this end, we propose equivariant neural fields (ENFs), a new class of NeFs that allows for the identification of continuous fields with concrete geometric representations (Fig. <ref type="figure">1</ref>).</p><p>Geometry-grounded neural fields When the goal is to utilise field representations z i in downstream tasks, it is crucial that these representations capture both textural/appearance information and explicit geometric information. Our approach is inspired by the idea of neural ideograms-learnable geometric representations <ref type="bibr" target="#b49">(Vadgama et al., 2022;</ref><ref type="bibr">2023)</ref>, and addresses the pervasive issue of texture bias, which causes typical deep learning systems to overfit to textural patterns and ignore important geometric cues <ref type="bibr" target="#b23">(Geirhos et al., 2018;</ref><ref type="bibr" target="#b24">Hermann et al., 2020)</ref>. To address this challenge, we propose defining representations that (1) explicitly separate aspects of geometry-specifically the pose of features-from appearance and (2) are localized in the input signal such that geometric concepts like orientation and distance are maintained from input to latent space. This necessitates that the geometric components of the representations have a meaningful structure, adhering to the same group transformation laws applicable to the fields. Geometrically, this means that distortions in the field translate to corresponding distortions in the latent space, ensuring that geometric (shape) variations are preserved and representable in latent space.</p><p>To establish an explicit grounding in geometry, we propose modelling the conditioning variables as geometric point clouds z = {(p i , c i )} N i=1 , comprising N pose-appearance tuples, with p i ∈ G being a pose (element) in a group G, and c i ∈ R c an appearance vector. This representation space has a well-defined group action, namely gz = {(gp i , c i )} N i=1 , allowing us to formalise the notion of grounding a neural field through the following</p><formula xml:id="formula_0">Steerability property: ∀g ∈ G : f θ (g -1 x; z) = f θ (x; gz) .<label>(1)</label></formula><p>Figure <ref type="figure">2</ref>: ENFs preserve transformations through their steerability property; if the field transforms with a group action g, the latents transform accordingly via the following group action on the pointcloud; gz = {gp i , c i } N i=1 .</p><p>Figure <ref type="figure">3</ref>: ENFs are a local signal encoding; a latent z i is optimized to represent a local signal patch. We show that this inductive bias allows for leveraging weight-sharing, and improves downstream performance by retaining important geometric features.</p><p>This property ensures that if the field transforms, the latent transforms accordingly (Fig. <ref type="figure">2</ref>). This property has also been shown in <ref type="bibr" target="#b0">(Atzmon et al., 2022;</ref><ref type="bibr" target="#b9">Chatzipantazis et al., 2022)</ref> for learning implicit shape representations and is well known in equivariance literature more broadly <ref type="bibr" target="#b4">(Bekkers, 2019;</ref><ref type="bibr" target="#b11">Cohen et al., 2019;</ref><ref type="bibr" target="#b52">Weiler &amp; Cesa, 2019;</ref><ref type="bibr" target="#b14">Deng et al., 2021)</ref>.</p><p>Contributions With this work we present the following contributions: A new class of geometrygrounded equivariant neural fields that posses</p><p>• the steerability property and thus proveable generalization over group actions</p><p>• weight sharing which enables more efficient learning</p><p>• localized representations in a latent point set which enables unique editing properties</p><p>We verify these properties through a range of experiments that (1) support the claim that latents are geometrically meaningful, (2) show competitive reconstruction and representation capacity on segmentation, classification, forecasting and super-resolution tasks on image and shape data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Neural Fields and conditioning variables Neural Fields (NeFs) are learned functions f θ mapping signal coordinates x to signal values f (x), parameterized by a neural network with parameters θ. Due to their flexibility they have emerged as prominent continuous data representation, applied on datatypes varying from object or scene data <ref type="bibr" target="#b37">(Park et al., 2019;</ref><ref type="bibr" target="#b34">Mescheder et al., 2019;</ref><ref type="bibr">Sitzmann et al., 2020a;</ref><ref type="bibr" target="#b35">Mildenhall et al., 2021)</ref> to audio and images <ref type="bibr" target="#b46">Tancik et al. (2020);</ref><ref type="bibr">Sitzmann et al. (2020b)</ref>. In order to more efficiently represent whole datasets of signals, Conditional Neural Fields only optimise a latent conditioning variable z f per signal-instance f ∈ D instead of optimising a separate set of neural network parameters θ i . The two most common approaches for obtaining latents z f are autodecoding <ref type="bibr" target="#b37">(Park et al., 2019)</ref> -where latents z f are initialized and optimized alongside NeF parameters θ -and Meta-Learning based encoding -where optimization is split into an outer loop that optimizes the backbone θ and an inner loop that optimizes latents z f . We explain both in detail in Appx. A.1.1, and use both in experiments described in Sec. 4.</p><p>In seminal work by <ref type="bibr" target="#b19">Dupont et al. (2022)</ref>, a datatype agnostic approach for learning on these continuous signal representations was proposed -involving first the optimization of a set of conditioning variables z to reconstruct a dataset of signals D := {f j } n j=1 ∼{z fj } n j=1 , and afterward using these variables as a surrogate for the data in downstream tasks such as classification, generation and completion. Although this work highlighted the flexible data-agnostic nature of Conditional Neural Fields (CNFs) by representing signals through a single "global" condition variable z fj , later work by <ref type="bibr" target="#b3">Bauer et al. (2023)</ref> showed their limitations in performance for more complex tasks (i.e. involving higher-resolution more varied natural data).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group theoretical preliminaries</head><p>The notion of transformation-preservation of an operator -such as the relation between field f and latent z -is best expressed through group theory. A group is an algebraic construction (G, •) defined by a set G and a binary operator</p><formula xml:id="formula_1">• : G × G → G called the group product, satisfying: closure: ∀h, g ∈ G : h • g ∈ G, identity: ∃e ∈ G : ∀g ∈ G, g • e = g, inverse: ∀g∃g -1 ∈ G : g • g -1 = e and associativity: ∀g, h, i ∈ G : (g • h) • i = g • (h • i)</formula><p>Given such a group G with identity element e ∈ G, and a set X, we can define the group action as a map G × X → X, which we will denote with direct multiplication i.e. a group element g ∈ G action on a coordinate x ∈ X is denoted gx. Note that when X = G the group action equals the group product. For the group action on fields f : X → R we use a separate symbol, namely [L g f ](x) := f (g -1 x). In this work we are interested in the Special Euclidean group SE(n) = T n ⋊ SO(n). SE(n) is the roto-translation group consisting of elements g = (t, R) with group operation g g ′ = (t, R) (t ′ , R ′ ) = (t + Rt ′ , RR ′ ); the left-regular action on function spaces is defined by</p><formula xml:id="formula_2">L g f (x) = f (g -1 x) = f (R -1 (x -t)).</formula><p>Equivariant graph neural networks for downstream tasks. A key property of our framework is its ability to associate geometric representations with fields. This capability unlocks a rich toolset for field analysis through the lens of geometric deep learning (GDL) <ref type="bibr" target="#b7">(Bronstein et al., 2021)</ref>. The GDL field has made significant advancements in the analysis and processing of geometric data, including the study of molecular properties <ref type="bibr" target="#b2">(Batzner et al., 2022;</ref><ref type="bibr" target="#b1">Batatia et al., 2022;</ref><ref type="bibr" target="#b22">Gasteiger et al., 2021;</ref><ref type="bibr" target="#b6">Brandstetter et al., 2021)</ref> and the generation of molecules <ref type="bibr" target="#b27">(Hoogeboom et al., 2022;</ref><ref type="bibr" target="#b5">Bekkers et al., 2023)</ref> and protein backbones <ref type="bibr" target="#b13">(Corso et al., 2022;</ref><ref type="bibr" target="#b56">Yim et al., 2023)</ref>. In essence, these approaches characterise shape. Our encoding scheme makes these tools now applicable to analysing the geometric components of neural fields, shown in Sec. 4.</p><p>Equivariant Graph Neural Networks (EGNNs) are a class of Graph Neural Networks (GNNs) that imposes roto-translational equivariance constraints on their message passing operators to ensure that the learned representations adhere to specific transformation symmetries of the data. Among the various forms of equivariant graph NNs <ref type="bibr" target="#b48">(Thomas et al., 2018;</ref><ref type="bibr" target="#b6">Brandstetter et al., 2021;</ref><ref type="bibr" target="#b42">Satorras et al., 2021;</ref><ref type="bibr" target="#b22">Gasteiger et al., 2021;</ref><ref type="bibr" target="#b5">Bekkers et al., 2023;</ref><ref type="bibr" target="#b20">Eijkelboom et al., 2023;</ref><ref type="bibr" target="#b30">Kofinas et al., 2024)</ref> we will utilise PΘNITA <ref type="bibr" target="#b5">(Bekkers et al., 2023)</ref> as an equivariant operator to analyse and process our latent point-set representations of fields. For the neural field, we leverage the same optimal bi-invariant attributes as introduced in <ref type="bibr" target="#b5">Bekkers et al. (2023)</ref>-which are based on the theory of homogeneous spaces-to parameterise our neural fields, allowing for seamless integration. They formalise the notion of weight sharing in convolutional networks as the sharing of message functions (kernels) over point-pairs -e.g. relative pixel positions -that should be treated equally. By defining equivalence classes of point-pairs that are identical up to a transformation in the group, we too derive attributes that uniquely identify these classes and enable weight sharing in our proposed ENFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section we introduce the Equivariant Neural Field (ENF) architecture. We start Sec. 3.1 by showing how we impose the proposed steerability property (Eq. 1), through the definition of bi-invariant attributes. We construct a cross-attention operator conditioned by these attributes. We subsequently constrain the operator to represent local sub-regions of the input domain by applying a Gaussian window. Finally, we propose a k-nearest neighbouring approach to cope with the computational complexity of the cross-attention operation between pixels and latents. In Sec. 3.2 we finally discuss how we obtain a latent point cloud z f for a signal f . Bi-invariance constraint Before presenting our equivariant neural field design, we need to understand the constraints imposed by the steerability property (Eq. 1). The key result-shown also by <ref type="bibr" target="#b11">Cohen et al. (2019)</ref>; <ref type="bibr" target="#b4">Bekkers (2019)</ref> in the context of equivariant CNNs-is that for steerability, the field f θ must be bi-invariant with respect to both coordinates and latents. </p><formula xml:id="formula_3">= {(p i , c i )} N i=1 . (a)</formula><p>Bi-invariant a m,i is calculated between coordinate x m and pose p i as p -1 i x m . (b) The query and key functions q transforms a m,i into a query q m,i , and key function k maps context vector c i to key k i . Attention coefficients are calculated through a softmax over q m,i k i . The softmax is taken over the N latents, yielding N attention coefficients att m,i , one for each latent z i . (c) A value v m,i for each latent-coordinate pair is calculated as a function v of c i and a i -and the resulting values are aggregated, weighted by their corresponding attention coefficients att m,i .</p><p>Lemma 1. A conditional neural field satisfies the steerability property iff it is bi-invariant, i.e., ∀g ∈ G :</p><formula xml:id="formula_4">f θ (gx; gz) = f θ (x; z). Proof. If f θ satisfies the steerability property, then f θ (gx; gz) = f θ (g -1 gx; z) = f θ (x; z), so it is bi-invariant. Conversely, if f θ is bi-invariant, then f θ (g -1 x; z) = f θ (gg -1 x; gz) = f θ (x; gz),</formula><p>satisfying the steerability property equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">EQUIVARIANT NEURAL FIELDS CONDITIONED ON GEOMETRIC ATTRIBUTES</head><p>The cross-attention operation enables the use of latent-sets as conditioning variables for CNFs by applying cross-attention between embedded coordinates x m and a latent set of context vectors z={c i } N i=1 <ref type="bibr" target="#b58">(Zhang et al., 2023)</ref>. Such cross-attention fields assign to each x m a corresponding value f θ (x m ; z), by matching a coordinate (query) embedding q(x m ) against latent (key) vectors k(c i ) to obtain attention coefficients att m,i , and aggregating associated values v(c i ) via</p><formula xml:id="formula_5">f θ (x m ; z) = W o N i=1 att m,i v(c i ) with att m,: = softmax i=1,...,N q(x m ) T k(c i ) √ d k ,</formula><p>where W o maps cross-attention outputs to NeF output/signal dimension R c . Our desired latent representation contains a geometric component, namely the poses p i associated with the context vectors c i . In order to see how this geometric information could be leveraged, we highlight how each of the three components (q, k, v) could depend on the geometric attributes:</p><formula xml:id="formula_6">f θ (x m ; z) = W o N i=1 att m,i v(x m , p i , c i ) with att m,: = softmax i=1,...,N q(x m , p i ) T k(x m , p i , c i ) √ d k .</formula><p>The steerability condition demands that the field has to be bi-invariant with respect to transformations on both x m and p i , and the easiest way to achieve this is to replace any instance of x m , p i by an invariant pair-wise attribute a(x m , p i ) that is both invariant and maximally informative. With maximally informative we mean that coordinate-pose pairs that are not the same up to a group action receive a different vector descriptor, i.e., a(x m , p</p><formula xml:id="formula_7">i ) = a(x ′ m , p ′ i ) if and only if there exists a g ∈ G such that x ′ m = gx m and p ′ i = gp i .</formula><p>Equivariant Neural Fields Based on recent results in the context of equivariant graph neural networks <ref type="bibr" target="#b5">Bekkers et al. (2023)</ref>, we let a(x m , p i ) := p -1 i x be the the unique and complete bijective identifier for the equivalence class of all coordinate-pose pairs that are the same up to a group action. Bijectivity here implies that the descriptor p -1 i x m contains all information possible to identify the equivalence classes, and thus the use of those attributes leads to maximal expressivity.</p><p>In this work we use bi-invariants for translation (a R n ) roto-translation (a SE(2) ) -as well as a "biinvariant" that breaks any equivariance a ∅ (see Appx. B for details). We define the ENF as follows:</p><formula xml:id="formula_8">f θ (x; z) = W o N i=1 att m,i v(a(x, p i ), c i ) with att m,: = softmax i=1,...,N q(a(x, p i )) T k(c i ) √ d k .</formula><p>As specific parameterizations for a, q, k, v, we choose:</p><formula xml:id="formula_9">a(x, p i ) := ϕ(p -1 i x) (2) q(a(p -1 i x)) := W q a(x, p i ) k(c i ) := W k c i , (3) v(a(x, p i ), c i ) := (W v c i ) ⊙ (W aγ a(x, p i )) + (W a β a(x, p i ))<label>(4)</label></formula><p>with ⊙ denoting element-wise multiplication and ϕ a relative coordinate embedding function which we set to be a Gaussian RFF embedding <ref type="bibr" target="#b46">(Tancik et al., 2020)</ref>. In neural field literature it is known that neural networks suffer from high spectral biases <ref type="bibr" target="#b40">(Rahaman et al., 2019)</ref>. Due to smooth inputoutput mappings it becomes difficult to learn high-frequency information in low-dimensions such as the coordinate inputs for a NeF. Gaussian RFF embeddings introduce high-frequency signals in the embedding to alleviate the spectral bias.</p><p>Since the value transform has as goal to fill in spatially varying signal patches during reconstruction, the value-function is also conditioned on the geometric attributes p -1 x. To add extra expressivity we chose to apply the conditioning via FiLM modulation <ref type="bibr" target="#b39">(Perez et al., 2018)</ref> which applies a featurewise linear modulation with a learnable shift β and scale γ modulation.</p><p>A crucial difference with standard transformer-type methods on point clouds is that cross-attention is between relative position embeddings-relative to the latent pose p i -and that the value transform is of depth-wise separable convolutional form <ref type="bibr">(Chollet, 2017;</ref><ref type="bibr" target="#b5">Bekkers et al., 2023)</ref>, which is a stronger form of conditioning <ref type="bibr" target="#b31">(Koishekenov &amp; Bekkers, 2023)</ref> than additive modulation as is typically done in biased self-attention networks such as Point Transformer <ref type="bibr" target="#b59">(Zhao et al., 2021)</ref>. attenuating the dot-product q m,i k i as a function of the distance between x m and p i .</p><p>To address this issue, we modify the attention mechanism by incorporating a Gaussian spatial windowing function with parameter σ att into the computation of attention coefficients. This approach follows the strategy proposed by <ref type="bibr" target="#b12">Cordonnier et al. (2019)</ref>. Specifically, the attention scores derived from the dot product between the query and key values are modulated by a Gaussian window µ, which is dependent on the Euclidean distance between latent positions p pos and the input coordinates, expressed as</p><formula xml:id="formula_10">µ σ (x m , p i ) = -σ att ||p pos i -x m || 2 .</formula><p>Here, σ att is a hyperparameter that regulates the size for each latent (Fig. <ref type="figure">5</ref>). We have:</p><formula xml:id="formula_11">att(x, z) = softmax i=1,...,N q(a(x, p i )) T k(c i ) √ d k + µ σ (x, p i ) ,<label>(5)</label></formula><p>where µ(x m , p i ) represents the Gaussian window computed for each latent position. To enhance the expressiveness of the model even further, σ att can be made latent-specific, encoding for the spatial extent of a latent z i . This extension allows the latents to be expressed as point clouds:</p><formula xml:id="formula_12">z f = {(p i , c i , σ i )} N i=1</formula><p>, effectively coupling position, appearance, and locality attributes within the latent space. However, we keep this for further research -fixing σ att in our experiments.</p><p>A note on efficiency A limitation of the proposed method is the considerable computational complexity required to calculate output values for large numbers of input coordinates; larger more com-plex signals require a larger number of latents to be represented accurately leading to an exponential number of attention coefficients needing to be calculated each forward pass (complexity scales as O(N latents × N coordinates )). Since we localize latents, larger input domains present a trade-off. Either we maintain a small number of latent points-requiring a larger σ att to cover the entire domain, diminishing locality-or we increase the number of latent points, aggravating computational cost.</p><p>To mitigate the computational overhead associated with the cross-attention between the latent points and the sampled coordinates, we propose employing a k-nearest neighbours (k-NN) approach for the cross-attention operation. Specifically, for each pixel, we first identify its k-nearest latent points and then compute cross-attention for this coordinate only over these k nearest latents. This approach reduces computational cost while preserving the advantages of local representations (Appx. D.5). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Properties of Equivariant Neural Fields</head><p>Weight sharing -Conditioning ENF's attention operator on attributes a which uniquely identify equivalence classes of (latent-coordinate)pairs, ensures that the cross-attention operators-be it the attention logits or the value transform-respond similarly regardless of the pose under which a signal pattern presents itself. This form of weight-sharing has shown improved data and representation efficiency in GNN-based architectures <ref type="bibr" target="#b5">(Bekkers et al., 2023)</ref> and-as such-we hypothesize that our proposed ENF architecture similarly benefits from these properties compared to other types of NeFs. We confirm this property in figure <ref type="figure" target="#fig_6">10</ref>, which shows that ENFs share weights over group actions g ∈ G for the geometry in which the point clouds are grounded. In Sec. 4 we verify these benefits on downstream tasks.</p><p>Locality and (geometric) interpretability-The use of latent point clouds allows for localization of the cross-attention mechanisms around the latent pose, akin to how a convolution operator works with localised kernels. Not only does this improve interpretability and downstream performance (field patterns can be attributed to specific latent points), it enables unique field editing possibilities. Since our method is based on sets of latents and each element is responsible for a local neighbourhood of the input domain, we can take arbitrary unions(e.g. stitching) or intersections(e.g. latent-merging) of point-sets of different samples (Fig. <ref type="figure" target="#fig_2">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">OBTAINING LATENT POINT CLOUDS z</head><p>Following <ref type="bibr" target="#b37">Park et al. (2019)</ref>; <ref type="bibr" target="#b19">Dupont et al. (2022)</ref> we obtain a latent point cloud z f for a specific sample f using gradient descent, optimizing z f for reconstruction of the original signal f . For instance, in images a latent point cloud z may be optimized for an L 2 -loss between f θ (•, z) and f (•). Of course, this also requires optimizing θ to meaningfully map latents to fields. The two most common approaches to this end are Autodecoding <ref type="bibr" target="#b37">(Park et al., 2019)</ref> -where z f and θ are optimised simultaneously over a dataset, or MAML <ref type="bibr" target="#b21">(Finn et al., 2017;</ref><ref type="bibr" target="#b47">Tancik et al., 2021</ref>) -where optimization is split into an outer and inner loop, with θ being optimized in the outer loop and z being re-initialized every outer step to reconstruct the current signal batch in a limited number of SGD steps in the inner loop. We detail these approaches in Appx. A.1.1, using both in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>First, we show the ability of Equivariant Neural Fields (ENFs) to reconstruct datasets of input fields f -i.e. to associate a latent point cloud z fj to a given dataset of samples f j ∈ D that accurately reconstructs them. Then, we validate ENFs as an improved NeF-based downstream representation for various tasks requiring geometric reasoning; classification, segmentation and forecasting. To show the flexibility of NeF-based representations, we perform these tasks on a range of modalities. Each downstream experiment consists of two stages: (1) fitting a ENF backbone f θ for reconstruction of the input fields f j ∈ D to obtain latents pointclouds z fj , and ( <ref type="formula">2</ref>) training a downstream model-that takes z fj as input-for each specific task. The bi-invariant a m,i we choose to condition our ENF, as well as the downstream model, varies depending one the type of task we're performing-described per experiment below. Since our goal is to assess the impact of grounding continuous representations in geometry, besides dataset-specific baselines, we also compare against Functa <ref type="bibr" target="#b19">(Dupont et al., 2022)</ref>, the framework that originally proposed functional representations z fj as data surrogates.</p><p>For experimental details and hyperparameters we refer to Appx. C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RECONSTRUCTION CAPACITY</head><p>First we evaluate our proposed ENFs on their reconstruction capabilities. • Image data We show results for reconstruction trained with Meta-Learning on CIFAR10 <ref type="bibr" target="#b32">Krizhevsky et al. (2009)</ref>, CelebA64×64 <ref type="bibr" target="#b33">(Liu et al., 2015)</ref> and ImageNet1K <ref type="bibr" target="#b15">(Deng et al., 2009)</ref> using different bi-invariant attributes a m,i -resulting in equivariance to different corresponding transformation groups-in Tab. 1. We provide results for a Functa baseline, following the setup described in <ref type="bibr" target="#b19">(Dupont et al., 2022)</ref>. Notably, translational weight sharing (a R 2 ) outperforms settings with no-transformation (a ∅ ) and roto-translational weight sharing (a SE(2) ). Moreover, it seems that locality alone is itself a useful inductive bias when moving to higher resolution, more varied images; Functa outperforms ENF a ∅ on CIFAR10 reconstruction, but on CelebA and ImageNet all ENF parameterizations outperform Functa. These results reinforce locality and equivariance as inductive biases in image-based continuous reconstruction tasks. • Shape data We show shape reconstruction results (Tab. 2) on two common shape representations; voxels (3D occupancy grids) and meshes. For voxel data we take train and test splits from the 16-class ShapeNet-Part segmentation dataset <ref type="bibr" target="#b55">(Yi et al., 2016</ref>) (which we denote ShapeNet16) and fit their corresponding voxel-based representation as occupancy function R 3 →{0, 1} -also using the obtained representations in the ShapeNet-Part segmentation experiment detailed below. For mesh data we opt instead to fit the full 55-class ShapeNetCore (v2) object dataset <ref type="bibr" target="#b8">(Chang et al., 2015)</ref>, fitting these with 150k points sampled from the signed distance function of the mesh (more details in Appx. C.3). Unlike <ref type="bibr" target="#b19">Dupont et al. (2022)</ref>, we were unable to get sufficient quality reconstructions with meta-learning and instead obtain latents z using autodecoding on shape data (finding discussed in Appx. A.1.1). Results show that ours as well as the baseline models struggle with accurately reconstructing the underlying shape from the SDF point clouds, we think due to the more complex optimization objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DOWNSTREAM TASKS</head><p>Image classification One major limitation of Functa-noted by Bauer et al. ( <ref type="formula">2023</ref>)-was lacking performance on complex image tasks such as classification. To show performance of our model in this setting, we reproduce the CIFAR10 classification experiment listed in <ref type="bibr" target="#b3">Bauer et al. (2023)</ref>-augmenting CIFAR10 with 50 random crops and flips per image, and training an ENF to reconstruct these using meta-learning, obtaining latent point clouds z. We do this for different bi-invariants a corresponding to no equivariance (a ∅ ), translational (a R 2</p><p>) and roto-translational (a SE(2) ) equivariance. We then train a PΘNITA classifier <ref type="bibr" target="#b5">(Bekkers et al., 2023)</ref> to classify these latent point cloudsconditioning the message passing function on the same bi-invariants, now calculated between poses p i . Results (Tab. 1) show a test-accuracy improvement of 13.8 percentage points (68.7%→82.1%) over Functa <ref type="bibr" target="#b19">(Dupont et al., 2022)</ref>, and also indicate that in this setting (roto-)translational equivariance is a strong inductive bias-with a SE(2) , a R 2 -ENFs outperforming a ∅ -ENFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shape classification</head><p>Highlighting the flexibility of NeF-representations we apply the same setup to shape classification, training PΘNITA classifiers on the aforementioned ShapeNet16 dataset. Results in Tab. 2 show that relevant geometric features are better preserved in a localized latent space.</p><p>Here, the performance difference between equivariant (a R 3 ) and non-equivariant (a ∅ ) ENFs are negligible. This is to be expected due to the global alignment of the ShapeNet dataset, and shows ENF is able to perform under equivariance constraints even in non-equivariant tasks. ShapeNet-Part segmentation Where classification primarily evaluates how well the latent point clouds captures global information, we also want to evaluate ENFs performance on fine-grained tasks. As such, we evaluate on the ShapeNet part segmentation task <ref type="bibr" target="#b55">(Yi et al., 2016)</ref>. The ShapeNet-Part dataset consists of point clouds for 16 ShapeNet object-classes, each with a varying number of annotated parts for a total of 50 segmentation classes. We use the ENF a R 3 backbone defined in the voxel reconstruction task above (f θrecon ) to obtain a latent z recon for a shape. Then, a second ENF (f θseg ) is trained to map points on this shape to a one-hot encoding of their corresponding segmentation classes, i.e. f θseg (x m ; z recon ) maps x m to its class label y m . Results (Tab. 3) somewhat surprisingly show ENF and Functa perform comparably in this task (detailed results and visualizations in Appx. D.6). We again think this attributable to the fact that all shapes are aligned and centered -we further investigate these results in Appx. D.6.1. We additionally include results for point cloud-specific architectures, and NF2Vec -a framework for self-supervised representation learning on 3D data from (non-conditional) NeFs. These results show that ENF only slightly underperforms modality-specific baselines.   Flood Map Segmentation For a more challenging segmentation task we apply ENFs on multi-modal flood mapping dataset <ref type="bibr" target="#b16">(Drakonakis et al., 2022)</ref>. This small dataset (759/85 train/test split) provides dual-modal temporal data; aligned Synthetic Aperture Radar (SAR) and optical satellite images at 256×256 resolution obtained by satellites Sentinel 1 and 2 (S1,S2), of disaster sites before and after their flooding, along with corresponding masks that segment the flooded area. The goal is to predict binary segmentation mask given these 4 different input fields. We first train a reconstruction a SE(2) -ENF f θrecon with MAML to obtain a latent z recon that decodes into the four observations. Next, a segmentation ENF f θseg is trained to predict, given a latent z recon , the binary mask at each location.</p><p>We evaluate and compare our model against the multi-modal U-Net proposed by <ref type="bibr" target="#b16">Drakonakis et al. (2022)</ref>. As suggested by <ref type="bibr" target="#b19">Dupont et al. (2022)</ref> we trained Functa-unable to fit the training set with MAML-using autodecoding instead. However, we found Functa unable to generalize to test images in this complex low-data setting, collapsing to remembering the training dataset (achieving 31.5 recon PSNR and 93.7 IoU on the 256×256 train set). Results (Tab. 4, Fig. <ref type="figure" target="#fig_3">7</ref>) show the importance of inductive biases in complex limited-data regimes. We provide results for subsampled observations to simulate missing data, showing the robustness of NeF-based methods to sparsity-ENF performs  ERA5 Climate forecasting Following <ref type="bibr" target="#b57">(Yin et al., 2022;</ref><ref type="bibr" target="#b29">Knigge et al., 2024)</ref> we evaluate our NeFbased representation on dynamics forecasting. ERA5 <ref type="bibr" target="#b25">(Hersbach et al., 2019)</ref> is a dataset of hourly global temperature observations. We use the dataset as described in <ref type="bibr" target="#b18">Dupont et al. (2021)</ref>, which contains data defined over 46×90 latitude-longitude grids. From the training and test sets, we extract 5693 and 443 pairs of subsequent observations T t , T t+1 for train and test sets respectively. Using MAML, we train an ENF f θ with bi-invariant a ∅ (no symmetries exist in this data) to reconstruct the global temperature state T t using z t , and optimize a PΘNITA MPNN to predict an update ∆z t that maps z t to a latent ẑt+1 = z t + ∆z t which decodes into the state at t + 1, i.e. T t+1 ≈f θ (•; z t+1 ) ≈ f θ (•; z t + ∆z t ). Training is done sequentially, i.e. first the backbone f θ is optimized and afterward the MPNN is trained, keeping f θ fixed. Results (Tab. 6, Fig. <ref type="figure" target="#fig_4">8</ref>) show that the latent space of ENF lends itself well for modelling such complex dynamics -where a global latent representation such as Functa seems unable to model the relevant fine-grained details needed for forecasting.  As downstream diffusion model, we utilize DiT-B <ref type="bibr" target="#b38">(Peebles &amp; Xie, 2023)</ref>, a natural choice for our set-latent (training detailed in Appx. C.4). We provide results in FID <ref type="bibr" target="#b26">(Heusel et al., 2017)</ref> for unconditional generation in Tab. 7 and samples in Fig. <ref type="figure" target="#fig_5">9</ref>. We provide comparison to Functa <ref type="bibr" target="#b19">(Dupont et al., 2022)</ref>, as well as other frameworks for generative modelling over fields <ref type="bibr" target="#b17">(Du et al., 2021;</ref><ref type="bibr" target="#b18">Dupont et al., 2021;</ref><ref type="bibr" target="#b60">Zhuang et al., 2023)</ref>-notably each of these methods is trained on a generative objective and does not support self-supervised pre-training like Functa or ENF. On globally aligned CelebA64×64, both Functa and ENF produce perceptually qualitative samples, but unlike ENF, Functa is unable to generalize to CIFAR-10, where data is less homogeneous and not aligned. These results again show clear benefit of a geometrically interpretable latent space for downstream tasks, though previously proposed frameworks specific to generative modelling over fields achieve better performance than ENF. The latter points to a possible area of improvement, and future work could look into incorporating insights from these works into a generative adaptation of the ENF framework, e.g. through latent-space regularization of perceptual consistency as per <ref type="bibr" target="#b17">Du et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Building upon fascinating work using Neural Fields (NeFs) as continuous data surrogates, this paper introduces Equivariant Neural Fields (ENFs); a novel NeF parameterization that re-introduces inductive biases (locality, equivariance) into NeF-based representations. ENF uses a geometry-grounded conditioning variable-a latent attributed point cloud-to achieve an equivariant decoding process, ensuring that transformations in an input field are preserved in the latent space and enabling steering of the latent to transform the output signal. This steerability property allows for the accurate representation of geometric information, and for efficient weight-sharing over spatially similar patterns, significantly improving learning efficiency and generalization, as validated on a range of experiments with varying data modalities and objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">REPRODUCIBILITY</head><p>All datasets can be downloaded via their cited references without any effort except for the ShapeNet dataset -which requires registration and approval. The pre-processing steps are described in appendix C.3. For all model parameter settings for the ENFs, downstream models or Functa we refer to the appendix C. As supplementary material we added a codebase containing code to reproduce results for the CIFAR10 and OMBRIA experiments. Code for all other experiments will be released during the rebuttal phase of the review process, containing all settings to reproduce the experiments in config files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMAGE RECONSTRUCTION, CLASSIFICATION, SEGMENTATION, FORECASTING</head><p>CIFAR10 reconstruction and classification For CIFAR10 <ref type="bibr" target="#b32">(Krizhevsky et al., 2009)</ref> reconstruction and classification we use a hidden dim of 128 with 3 heads, 25 latents of size 64, a batch size of 32 and restrict the cross-attention operator to k=4 nearest latents for each input coordinate x. For σ q , σ v we choose 1.0 and 3.0 respectively. We train the ENF model and the classifier for 100 epochs.</p><p>CelebA64×64 For CelebA <ref type="bibr" target="#b33">Liu et al. (2015)</ref> we use a hidden dim of 256, 36 latents of size 64, a batch size of 2 and restrict the cross-attention operator to k=4 nearest latents for each input coordinate x. For σ q , σ v we choose 2.0 and 10.0. We train the model for 30 epochs.</p><p>ImageNet1K reconstruction For ImageNet1K <ref type="bibr" target="#b15">(Deng et al., 2009)</ref> reconstruction we use a hidden dim of 128 with 3 heads, 169 latents of size 64, a batch size of 2, restricting the cross-attention operator to k=4 nearest latent for each input coordinate x. For σ q , σ v we choose 2.0 and 10.0 respectively. We train the model for 2 epochs.</p><p>Ombria For OMBRIA <ref type="bibr" target="#b16">Drakonakis et al. (2022)</ref> we trained a reconstruction model f θrecon with, 256 hidden dim, 4 heads, 169 latents of size 128, a batch size 8. Restricting the cross-attention operator to k=1 nearest latent for each input coordinate x. For σ q , σ v we choose 2.0 and 10.0 respectively. The segmentation model f θseg has hidden size of 128, 8 heads, with cross-attention restricted to k=4 nearest latents, trained with batch size 16. For σ q , σ v we choose 2.0 and 3.0 respectively. We train both models, sequentially, for 500 epochs.</p><p>ERA5 forecasting For ERA5 forecasting <ref type="bibr" target="#b25">(Hersbach et al., 2019)</ref> we train a reconstruction model f θrecon with 128 hidden dim, 3 heads, 36 latents of size 64, a batch size of 32. Restricting the crossattention operator to k=4 nearest latent for each input coordinate x. For σ q , σ v we choose 2.0 and 8.0 respectively. Inputs are defined over a latitude longitude θ, ϕ grid, which we map to 3D euclidean coordinates per x = [cos θ cos ϕ, cos θ sin ϕ, sin θ]. We first train the ENF for 800 epochs. As forecasting model, we train a PΘNITA MPNN of 3 layers with 256 hidden dim for 1000 epochs. Both models are trained with a batch size of 32.</p><p>As objective, since we don't want to overfit the reconstruction error incurred by fitting a latent z t to the initial state, we supervise the forecasting model with L 2 loss between the decoded output for predicted latent ẑt+1 per:</p><formula xml:id="formula_13">L forecast = ||(f θrecon (•; z t ) + ∆T ) -(f θrecon (•; ẑt+1 )|| 2 2</formula><p>∆T being the ground truth change in temperature, and ẑt+1 = F ψforecast (z t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 FUNCTA BASELINE MODELS</head><p>For the Functa baselines <ref type="bibr" target="#b19">(Dupont et al., 2022)</ref>, we try to keep as close as possible to the setup defined by the original authors. However, we found that training deeper models in the shape experiments (≥ 8 layers) lead to very unstable training. Instead, for these experiments, we opted to go for shallower models, up to 6 layers. For all experiments we use a hidden dim of 512 and latent modulation size of 512 as used in <ref type="bibr" target="#b3">(Bauer et al., 2023)</ref>, except for ImageNet1K reconstruction, where we use a 1024 latent modulation. We would like to note here that in all experiments, the Functa baseline has larger parameter count than the ENF models applied to each task (e.g. for CIFAR10, ENF has 522K params where Functa has 2.6M params). Although we did not explore this in-depth, it seems that the proposed ENF representation is more parameter efficient compared to the deep SIREN model defined in <ref type="bibr" target="#b19">(Dupont et al., 2022)</ref>.</p><p>For downstream models we follow <ref type="bibr" target="#b3">(Bauer et al., 2023)</ref> and use a 1024 hidden dim 3 layer residual MLP (∼2.1M params). Like in our ENF experiments, we use the same architecture across tasks, only changing the output head to accommodate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 SHAPE RECONSTRUCTION AND CLASSIFICATION</head><p>Voxel Dataset and Segmentation The voxels are given with the ShapeNet dataset where the segmentation labels are given as point clouds. However, the coordinate frames of the voxels are different, so to align them we mapped both between -1 and 1.</p><p>We trained a model f with a hidden dim of 128, 3 heads, 27 latents of size 32. We set σ q , σ v for the RFF embedding functions ϕ q , ϕ v to 2 and 10 respectively. For Functa we used, a latent dim of 864 to have the same latent parameters as the conditioning variable used for ENF. As NeF we used a 5-layer Siren with a hidden-dim of 512, w 0 is set to 10. The modulation network is a two-layer MLP of hidden sizes 256 and 512.</p><p>As is customary for ShapeNet-part segmentation, we condition on the object class and supervise over all segmentation classes with a cross-entropy loss, but only calculate test IoU based on segmentation classes that correspond to the object class. We chose the class-emb dim to be 32 for all settings.</p><p>The segmentation NeFs are all trained for 500 epochs with the same parameters as the reconstruction NeF. However, for ENF, we chose σ q , σ v to be 1, 1 for extra stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SDF Dataset</head><p>To create the signed distance functions from ShapeNetCore V2 Chang et al. ( <ref type="formula">2015</ref>) objects, we took their meshes and made them water-tight using Point Cloud Utils <ref type="bibr" target="#b53">(Williams, 2022)</ref>. Afterwards, we sampled a point cloud of 150,000 points from the surface. To create the actual SDF, we perturbed the points with Gaussian noise along the mesh normals, and recalculate the signed distances to the surface. Finally, the dataset consisted of 55 classes with a total number of 42.472 and 5.000 samples for the train and test set respectively.</p><p>For ENF we used a latent point cloud of 27 points with context vectors of dimension 32. The std parameters σ q , σ v for the RFF embedding functions ϕ q , ϕ v are 2 and 10 respectively. The hidden dim of the ENF was set to 128 and we used 3 attention-heads. The For Functa <ref type="bibr" target="#b19">(Dupont et al., 2022)</ref> we used a latent modulation of 864 which corresponds to ENFs chosen 27*32 parameters for the conditioning variable. As a NeF we used a 5 layer Siren with an hidden-dim of 512 and a w 0 parameter of 15. As a modulation network we used a two-layer MLP with 256,512 hidden-dim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 GENERATIVE MODELLING ON ENF LATENT SPACE</head><p>For generative modelling experiments CelebA64×64 and CIFAR-10 we train a Diffusion Transformer (DiT-B) <ref type="bibr" target="#b38">(Peebles &amp; Xie, 2023)</ref> on ENF latents, utilizing the context vectors as input tokens and their positions as input for an RFF position embedding added to the tokens. We use the same approach for CelebA as for CIFAR10; we train an a R 2 ENF with MAML on the image dataset, and use this model to obtain sets of "ground truth" latents z 0 := {p i , c i,0 } N i=1 for each image. We then train a DiT-B on a diffusion objective on this latent space, where the forward diffusion kernel is given by:</p><formula xml:id="formula_14">z t = {(p i , √ ᾱt c i,0 + √ 1 -ᾱt ϵ c )} N i=1 ,<label>(9)</label></formula><p>with ϵ c ∈ N (0, 1), i.e. we only add noise to the latent vectors, as we find adding noise to the poses leads to unstable training (something to be investigated in future work). To generate a sample, we take a random set of "ground truth" poses from the training set, and attach a context vector c i,t ∈ N (0, 1) to each pose to denoise. We supervise the DiT-B with the v objective <ref type="bibr" target="#b41">(Salimans &amp; Ho, 2022)</ref>. Like <ref type="bibr" target="#b38">(Peebles &amp; Xie, 2023)</ref>, we use a t max =1000 linear noise schedule ranging from 1e-4 to 1e-2, and generate samples using DDIM <ref type="bibr" target="#b45">(Song et al., 2020)</ref> in 512 steps.</p><p>In both settings, we train the diffusion model for 200 epochs using Adam, a constant learning rate of 1e-4, no weight-decay or dropout, and generate 50k samples to calculate FID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ADDITIONAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 SIZE OF LATENT POINT-CLOUD</head><p>In this section, we delve deeper into the hyper-parameters of the latent point clouds used as conditioning variables. Equivariant Neural Fields can increase the number of parameters used to represent a signal in two ways: by increasing the number of latent points or by expanding the dimensionality of the context vectors. Intuitively, we can either enhance the representational capability of a single region in the input domain or create more, smaller regions with lower representational dimensions.</p><p>To further insights into how these conditioning variables behave, we train multiple CNFs using different latent point-cloud configurations. In these experiments, the chosen latent dimension or number of latent points is adjusted to keep the total number of parameters as close as possible across configurations. We trained each model for 400 epochs, as only small improvements occurred beyond this point and the overall trend was already clear. After fitting the ENF, we used the meta-learned representation to perform classification tasks. We employed a simple message-passing GNN, which we trained for 20 epochs, after which performance improvements began to degrade. Below, we present the ablation results for these different approaches to increasing representational capabilities. To evaluate the impact of Gaussian spatial windowing (GSW) and the k-Nearest Neighbors (kNN) approximation in the proposed method, we trained four models on the CIFAR10 dataset: one with both features disabled, one with only kNN enabled, one with only GSW enabled, and one with both enabled. Besides evaluating the difference in reconstruction capabilities, we are mainly interested in the downstream performance. We argue that the introduced locality enhances latent-space structure by improving weight-sharing across local-patches.</p><p>After training the models, we used the different ENF models to generate latent representations for CIFAR10 classification. The results are shown in Table <ref type="table" target="#tab_7">9</ref>. While reconstruction performance remains almost consistent across the different setups, GWS significantly improves downstream classification accuracy. Moreover, the kNN approximation does not negatively affect either reconstruction nor classification performance. Interestingly, kNN even provides a slight improvement even without GWS. We hypothesize that this improvement comes from kNN introducing an implicit form of windowing-not by modifying attention values directly but by limiting the set of attention values considered. To conclude, there can be observed that the introduced locality in CNF latents does improve the downstream performance. We provide visualizations for transformations applied to the latent pointclouds for different biinvariants a in Fig. <ref type="figure" target="#fig_6">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 ENF WITH GEOMETRY-FREE LATENT SETS</head><p>To further investigate what design choices the performance of ENF results from, we provide an ablation on CIFAR10 over a geometry-free implementation of ENF. We do this by removing the pose information from the latent set, i.e. we set z := {c i } N i=1 , and use a "bi-invariant" that is only a function of x j , a i,j = x j . Since now latents do not have a position, we remove the Gaussian windowing and KNN approximation, but keep the rest of the ENF architecture as well as the hyperparameters used identical to the settings reported in Appx. C under 'CIFAR10 reconstruction and m,i is bi-invariant to roto-translations; the output f θ (x; z) equivaries with both rotations and translations applied to z. classification'. We observe highly unstable training during the reconstruction phase, and reconstruction performance on the test set converges to 22.3. We think this attributable to the fact that now any update to one of the latent codes affects the output of the NEF globally, leading to a much more complex optimization landscape. This highlights another advantage of either having a single global latent, or using locality as inductive bias; optimization of single or locally responsible latents seems to lead to a simpler optimization landscape compared to optimizing a set of global latents.</p><p>We subsequently train a simple 4 layer transformer with hidden dim 256 and 4 heads as classifier. Note that this transformer uses no positional encoding, since the latent z in this setting has no associated geometry/positional information. We train for 500 epochs on the augmented dataset, after which training accuracy has converged to 95%. We observed overfitting early into training. Utilizing early stopping, best performance was achieved after just 5 epochs, yielding a test set accuracy of 0.47. These observations (Tab. 10) are in line with the outcome of our other experiments; geometrygrounded latents are more informative for downstream tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 DETAILS ON COMPUTATIONAL EFFICIENCY</head><p>To allow for more fine-grained comparison of our method with previous work, we provide details on time and memory efficiency of our approach on the CIFAR10 classification experiments listed in Tab. 1 with a R 2 , when both Functa and ENF are fit using MAML with 3 inner loop steps. Moreover, we compare efficiency also when ablating over the KNN approximation of the attention operation. We report estimated FLOPs (obtained through JAX's AOT api), GPU memory usage per sample and training time per epoch for a batch size of 32. We see (Tab. 11) that the naive implementation that does not truncate the attention operator is significantly more FLOP-intensive and memory intensive compared to Functa <ref type="bibr" target="#b19">(Dupont et al., 2022)</ref> and the KNN approximate implementation. Functa in all settings does have considerably higher runtime, attributable to its relatively deep sequential architecture compared to the shallow single layer architecture of ENF. These results show that, besides</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A visual intuition for the proposed cross-attention between coordinate x m and latent z = {(p i , c i )} N i=1 . (a) Bi-invariant a m,i is calculated between coordinate x m and pose p i as p -1 i x m . (b)The query and key functions q transforms a m,i into a query q m,i , and key function k maps context vector c i to key k i . Attention coefficients are calculated through a softmax over q m,i k i . The softmax is taken over the N latents, yielding N attention coefficients att m,i , one for each latent z i . (c) A value v m,i for each latent-coordinate pair is calculated as a function v of c i and a i -and the resulting values are aggregated, weighted by their corresponding attention coefficients att m,i .</figDesc><graphic coords="5,108.00,81.86,130.28,105.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 5: (a) Global attention between coordinates x m and latents z i can result in high attention values for non-local latents. (b) Locality is enforced through a Gaussian window µ σ (x m , p i ), attenuating the dot-product q m,i k i as a function of the distance between x m and p i .</figDesc><graphic coords="6,306.00,459.44,97.02,58.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Latent point cloud editing. Subfigures (a) and (b) show two reconstructed CIFAR-10 images with corresponding latents z car ,z duck . Subfigure (c) shows a reconstruction of the latent set z car-duck when selecting latents from either z car or z duck based on their position.</figDesc><graphic coords="7,306.00,228.95,63.36,70.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Top-OMBRIA test sample of SAR (S1), optical (S2) before and after flooding with ground truth flood map. Bottom-ENF reconstructions f θrecon (•; z recon ) and predicted mask f θseg (•; z recon ).Table 4: Test IOU (↑) for flood map segmentation on OMBRIA, for different observation rates.</figDesc><graphic coords="9,306.00,402.21,197.99,72.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Visualization of ground truth ERA5 test sample and ENF prediction of the change in temperature between observations T t and T t+1 . We show the true (∆T) and predicted (∆ T ) difference between the temperature maps at t and t + 1.</figDesc><graphic coords="10,246.60,83.16,126.13,76.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Qualitative samples for generative modelling on CIFAR-10 and Celeba64×64.</figDesc><graphic coords="10,246.60,474.98,128.69,84.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Transformations applied to z for different bi-invariants a. (a) a ∅ m,i is not biinvariant to any transformations, (b) a R 2 m,i is bi-invariant to translations, producing distorted patterns on rotation and (c) a SE(2)</figDesc><graphic coords="20,199.60,81.86,212.80,164.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,108.00,81.86,395.99,140.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test-set reconstruction PSNR (db↑) on CIFAR10, CelebA64x64, ImageNet1k, test accuracy (%↑) on CIFAR10.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Table 2:</cell><cell cols="3">Test reconstruction (IoU↑) on</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">ShapeNet16 and ShapeNet55 and test clas-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">sification accuracy (%↑) on ShapeNet16.</cell></row><row><cell></cell><cell cols="2">CIFAR10</cell><cell cols="2">CELEBA IMAGENET</cell><cell>MODALITY</cell><cell cols="2">SHAPENET16 VOXEL (OCC)</cell><cell>SHAPENET55 P. CLOUD (SDF)</cell></row><row><cell>TASK</cell><cell cols="2">RECON. CLASS.</cell><cell>RECON.</cell><cell>RECON.</cell><cell>TASK</cell><cell cols="2">RECON. CLASS.</cell><cell>RECON.</cell></row><row><cell>Functa ENF a ∅ ENF a R 2 ENF a SE(2)</cell><cell>38.1 36.5 42.2 41.6</cell><cell>68.3 68.7 82.1 81.5</cell><cell>28.0 30.6 34.6 32.9</cell><cell>7.2 24.7 27.5 26.8</cell><cell>NF2vec Functa ENF a ∅ ENF a R 3</cell><cell>-92.1 90.7 92.9</cell><cell>93.3 90.3 96.4 96.6</cell><cell>-25.7 72.3 73.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">: Mean class</cell></row><row><cell cols="3">and instance IoU</cell></row><row><cell cols="3">(↑) on ShapeNet.</cell></row><row><cell>MODEL</cell><cell>IN S T M IO U</cell><cell>C L S M IO U</cell></row><row><cell>PointNet</cell><cell>83.1</cell><cell>79.0</cell></row><row><cell>PointNet++</cell><cell>84.9</cell><cell>82.7</cell></row><row><cell>DGCNN</cell><cell>83.6</cell><cell>80.9</cell></row><row><cell>NF2vec</cell><cell>81.3</cell><cell>76.9</cell></row><row><cell>Functa</cell><cell>82.8</cell><cell>74.8</cell></row><row><cell>ENF</cell><cell>82.2</cell><cell>75.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Test</figDesc><table><row><cell></cell><cell cols="2">IOU (↑)</cell></row><row><cell cols="3">for flood map segmen-</cell></row><row><cell cols="3">tation on OMBRIA, for</cell></row><row><cell cols="3">different observation</cell></row><row><cell>rates.</cell><cell></cell><cell></cell></row><row><cell>MODEL</cell><cell>PSNR (↑)</cell><cell>IoU (↑)</cell></row><row><cell></cell><cell cols="2">100% OF fIN OBSERVED</cell></row><row><cell>OmbriaNet</cell><cell>N.A.</cell><cell>72.36</cell></row><row><cell>Functa</cell><cell>16.77</cell><cell>42.75</cell></row><row><cell>ENF</cell><cell>31.65</cell><cell>74.00</cell></row><row><cell></cell><cell cols="2">50% OF fIN OBSERVED</cell></row><row><cell>OmbriaNet</cell><cell>N.A.</cell><cell>27.02</cell></row><row><cell>Functa</cell><cell>16.71</cell><cell>42.74</cell></row><row><cell>ENF</cell><cell>31.37</cell><cell>73.65</cell></row><row><cell></cell><cell cols="2">10% OF fIN OBSERVED</cell></row><row><cell>OmbriaNet</cell><cell>N.A.</cell><cell>0.0</cell></row><row><cell>Functa</cell><cell>16.77</cell><cell>42.92</cell></row><row><cell>ENF</cell><cell>24.87</cell><cell>71.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Test IOU (↑) zero-shot resolution transfer on OMBRIA. f θrecon ,f θseg were trained on 128×128 resolution.</figDesc><table><row><cell cols="2">MODEL PSNR (↑)</cell><cell>IoU (↑)</cell></row><row><cell></cell><cell cols="2">256×256 TEST RESOLUTION</cell></row><row><cell>Functa</cell><cell>16.72</cell><cell>37.14</cell></row><row><cell>ENF</cell><cell>28.61</cell><cell>72.92</cell></row><row><cell></cell><cell cols="2">128×128 TEST RESOLUTION</cell></row><row><cell>Functa</cell><cell>16.71</cell><cell>35.48</cell></row><row><cell>ENF</cell><cell>29.31</cell><cell>73.21</cell></row><row><cell></cell><cell cols="2">64×64 TEST RESOLUTION</cell></row><row><cell>Functa</cell><cell>16.58</cell><cell>36.90</cell></row><row><cell>ENF</cell><cell>33.31</cell><cell>72.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>ERA5 reconstruction T t -MSE↓ and 1-hour forecasting T t+1 -MSE↓. *MSE between ground truth observations at T t and T t+1 .</figDesc><table><row><cell></cell><cell cols="2">Tt-MSE↓ Tt+1-MSE↓</cell></row><row><cell>Identity*</cell><cell>-</cell><cell>2.42E-05</cell></row><row><cell>Functa</cell><cell>5.75E-05</cell><cell>3.45E-03</cell></row><row><cell>ENF</cell><cell>8.04E-06</cell><cell>9.44E-06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>FID for generative modelling on CIFAR-10 and Celeba64×64.</figDesc><table><row><cell></cell><cell cols="2">CelebA64×64 CIFAR-10</cell></row><row><cell>MODEL</cell><cell>FID ↓</cell><cell>FID ↓</cell></row><row><cell>GEM</cell><cell>-</cell><cell>23.8</cell></row><row><cell>GASP</cell><cell>13.5</cell><cell>-</cell></row><row><cell>DPF</cell><cell>13.2</cell><cell>15.1</cell></row><row><cell>Functa</cell><cell>40.4</cell><cell>78.2</cell></row><row><cell>ENF</cell><cell>33.8</cell><cell>23.5</cell></row></table><note><p>Image generation Following Dupont et al. (2022); Bauer et al. (2023), we provide results for diffusion applied to a dataset of latents obtained from pretrained ENFs on CIFAR10 and Celeba64 × 64.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Reconstruction PSNR (db↑) and ACC (%↑) on CIFAR10 for different parametrisations of the latent point-clouds, i.e. varying N, d in z := {c i ∈ R d } N i=1 .</figDesc><table><row><cell cols="5"># LATENTS(N) LATENT DIM(D) # PARAMS PSNR ACC (%)</cell></row><row><cell>1</cell><cell>1600</cell><cell>1600</cell><cell>22.69</cell><cell>53.21</cell></row><row><cell>4</cell><cell>400</cell><cell>1600</cell><cell>29.14</cell><cell>64.98</cell></row><row><cell>9</cell><cell>178</cell><cell>1602</cell><cell>35.49</cell><cell>73.54</cell></row><row><cell>16</cell><cell>100</cell><cell>1600</cell><cell>39.93</cell><cell>77.09</cell></row><row><cell cols="5">D.2 ABLATION ON GAUSSIAN SPATIAL WINDOWING AND KNN APPROXIMATION</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Reconstruction PSNR (db↑) and ACC (%↑) on CIFAR10 to ablate the Gaussian spatial windowing and kNN approximation.</figDesc><table><row><cell>ENF SETUPS</cell><cell cols="2">PSNR (DB ↑) ACC (% ↑)</cell></row><row><cell>ENF</cell><cell>39.1</cell><cell>70.1</cell></row><row><cell>ENF + kNN</cell><cell>40.8</cell><cell>72.8</cell></row><row><cell>ENF + GWS</cell><cell>TBD</cell><cell>TBD</cell></row><row><cell>ENF + GWS + kNN</cell><cell>42.2</cell><cell>82.1</cell></row><row><cell cols="2">D.3 TRANSFORMING THE LATENT POINT-CLOUD</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 :</head><label>10</label><figDesc>Reconstruction PSNR (db↑) and classification test accuracy (%↑) on CIFAR10 when ablating over latent geometry.</figDesc><table><row><cell></cell><cell cols="2">PSNR ACC (%)</cell></row><row><cell>Functa</cell><cell>38.1</cell><cell>68.3</cell></row><row><cell>ENF w/ pose-free latents</cell><cell>22.3</cell><cell>47.9</cell></row><row><cell>ENF w/ R 2 latents</cell><cell>42.2</cell><cell>82.1</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">ACKNOWLEDGEMENTS</head><p>David Knigge is partially funded by <rs type="funder">Elekta Oncology Systems AB</rs> and a <rs type="grantName">RVO public-private partnership grant</rs> (<rs type="grantNumber">PPS2102</rs>). David Wessels is partially funded Ellogon.AI and a public grant of the <rs type="funder">Dutch Cancer Society (KWF)</rs> under subsidy (<rs type="grantNumber">15059/2022-PPS2</rs>). This work used the Dutch national e-infrastructure with the support of the SURF Cooperative using grant no. <rs type="grantNumber">EINF-9549</rs> and <rs type="grantNumber">EINF-10544</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BC74dYQ">
					<idno type="grant-number">PPS2102</idno>
					<orgName type="grant-name">RVO public-private partnership grant</orgName>
				</org>
				<org type="funding" xml:id="_unS73H3">
					<idno type="grant-number">15059/2022-PPS2</idno>
				</org>
				<org type="funding" xml:id="_KK7nHSs">
					<idno type="grant-number">EINF-9549</idno>
				</org>
				<org type="funding" xml:id="_eRVJswR">
					<idno type="grant-number">EINF-10544</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 AUTODECODING AND META-LEARNING A.1.1 META-LEARNING When fitting samples with (Conditional) NeFs using autodecoding (gradient-descent based optimisation of the latent at test-time) <ref type="bibr" target="#b37">(Park et al., 2019)</ref>, two key challenges emerge: (1) optimising the sample-specific parameters/latent z for a novel sample can be time-consuming -taking e.g. up to 500 gradient updates <ref type="bibr" target="#b57">(Yin et al., 2022)</ref>-and (2) more gradient updates to NeF weights may impede downstream performance through a phenomenon known as overtraining <ref type="bibr" target="#b36">(Papa et al., 2023)</ref> -where the relationship between field f and z is obscured by oversensitivity to high-frequency details. To address the first point, Sitzmann et al. (2020a); <ref type="bibr" target="#b47">Tancik et al. (2021)</ref>; <ref type="bibr" target="#b19">Dupont et al. (2022)</ref> propose a Model-Agnostic Meta-Learning (MAML) <ref type="bibr" target="#b21">(Finn et al., 2017)</ref> based optimisation method, enabling the network or latent initialisation to be learned such that each sample can be fitted with just a few gradient steps. More specifically, <ref type="bibr" target="#b19">Dupont et al. (2022)</ref> proposes an inner-outer loop structure where the modulations are updated in the inner loop, while the base network weights are updated in the outer loop. This method corresponds to an instance of learning a subset of weights with MAML, also known as Contextual Variable Interaction Analysis (CAVIA) <ref type="bibr" target="#b61">Zintgraf et al. (2019)</ref>. More recently, <ref type="bibr" target="#b29">Knigge et al. (2024)</ref> note that this meta-learning approach also improves downstream performance by imposing structure on the NeF's latent-space. We provide pseudocode for this approach in Alg. 1. </p><p>During our experiments, we found that not all types of signals lend themselves easily to this encoding approach when using ENFs (specifically SDFs and occupancy functions). Although it saves time in inference and adds structure to the latent space, <ref type="bibr" target="#b19">(Dupont et al., 2022</ref>) also remark on the limited expressivity of Meta-Learning due to the small number of gradient descent steps used to optimize a latent z. As such, for all shape experiments we instead opt for autodecoding <ref type="bibr" target="#b37">(Park et al., 2019)</ref> </p><p>We noted during our experiments that initialization of the latent poses-i.e. their initial position/orientation in the inner loop-has a significant impact on the reconstruction capacity and stability of the ENF. We found that a good way to initialize the latents is to space them as equidistantly as possible and then adding small Gaussian noise ( N (0, 1e -3)), e.g. for 2D images on a perturbed 2D grid. Any orientations are initialized canonically, i.e. all latents are initialized with the same orientation. When defining an equidistantly spaced grid is hard, for example on point clouds or data defined on a sphere, we propose using Farthest Point Sampling on a training grid to initialize positions for the latents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B BI-INVARIANT FUNCTION PARAMETERIZATIONS a m,i</head><p>The bi-invariants attributes that are used in the experiments section are listed here.</p><p>Translational symmetries R n In this setting, poses correspond to translations</p><p>Roto-translational symmetries SE(2). In this setting, poses p i correspond to group elements g = (θ i , t i ) ∈ SE(2). We adopt the invariant attribute introduced by <ref type="bibr" target="#b5">(Bekkers et al., 2023)</ref>:</p><p>No transformation symmetries. A simple "bi-invariant" for this setting that preserves all geometric information is given by simply concatenating coordinates p with coordinates x:</p><p>8) Parameterizing the cross-attention operation in Eq. 3.1 as function of this bi-invariant results in a framework without any equivariance constraints. We use this in experiments to ablate over equivariance constraints and its impact on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENTAL DETAILS</head><p>We provide hyperparameters per experiment. We optimize the weights of the neural field f θ in all experiments with Adam (Kingma &amp; Ba, 2014) with a learning rate of 1e-4, and an inner step size of 30.0 for c i and 1.0 for p i (increasing inner step size in general speeds up convergence and improves reconstruction -but may also lead to instabilities). For downstream classification we train an equivariant MPNN F ψ , using 3 message passing layers in the architecture specified in <ref type="bibr" target="#b5">Bekkers et al. (2023)</ref> conditioned on the same bi-invariant which was used to fit the ENF, with a hidden dimensionality of 256, always trained with learning rate 1e-4. The std parameters σ q , σ v of the RFF embedding functions φ q , φ v are chosen per experiment based on an ablation. In general, increasing both values leads to increased frequency response of the ENF, though generally the model is more susceptible to small change in σ q . as well as hidden dim size and the number of attention heads are chosen per experiment, detailed below. We run all experiments on a single H100. being performant on fine-grained downstream tasks, ENF also scales favourably compared to Functa.  Table <ref type="table">12</ref>: Segmentation class and instance averaged IOU (↑) on ShapeNet, and mIoUs per class. Further investigating the results obtained in the ShapeNet Part classification task, we train an ENF f θseg without conditioning on z recon -i.e. without any shape-specific conditioning but instead only conditioning on the object class. This model obtains class and instance mIoU of 64.3 and 69.2 respectively, indicating that a lot of points in this dataset can be correctly segmented purely based on their absolute position, and as such the backbone NeF model does not need to capture to perform decently on this dataset -though we would expect additional geometric to help with performance.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frame averaging for equivariant shape space learning</title>
		<author>
			<persName><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koki</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameh</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="631" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Higher order equivariant message passing neural networks for fast and accurate force fields</title>
		<author>
			<persName><forename type="first">Ilyes</forename><surname>Batatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Ortner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Csányi</surname></persName>
		</author>
		<author>
			<persName><surname>Mace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11423" to="11436" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Batzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Musaelian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">P</forename><surname>Mailoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mordechai</forename><surname>Kornbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Molinari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tess</forename><forename type="middle">E</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Kozinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2453</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">Richard</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.03130</idno>
		<title level="m">Spatial functa: Scaling functa to imagenet classification and generation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">B-spline cnns on lie groups</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12057</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharvaree</forename><surname>Vadgama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><forename type="middle">D</forename><surname>Hesselink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Putri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Van Der Linden</surname></persName>
		</author>
		<author>
			<persName><surname>Romero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.02970</idno>
		<title level="m">Fast, expressive se (n) equivariant networks through weight-sharing in positionorientation space</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Geometric and physical quantities improve e (3) equivariant message passing</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Brandstetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Hesselink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elise</forename><surname>Van Der Pol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Veličković</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13478</idno>
		<title level="m">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Angel X Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Se (3)-equivariant attention networks for shape reconstruction in function space</title>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Chatzipantazis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Pertigkiozoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Dobriban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02394</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
	<note>Franc ¸ois Chollet</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A general theory of equivariant cnns on homogeneous spaces</title>
		<author>
			<persName><forename type="first">Mario</forename><surname>Taco S Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><surname>Weiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">On the relationship between selfattention and convolutional layers</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03584</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diffdock: Diffusion steps, twists, and turns for molecular docking</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannes</forename><surname>Stärk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vector neurons: A general framework for so (3)-equivariant networks</title>
		<author>
			<persName><forename type="first">Congyue</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueqi</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Poulenard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12200" to="12209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ombrianet-supervised flood mapping via convolutional neural networks using multitemporal sentinel-1 and sentinel-2 data fusion</title>
		<author>
			<persName><forename type="first">Grigorios</forename><surname>Georgios I Drakonakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantina</forename><surname>Tsagkatakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Fotiadou</surname></persName>
		</author>
		<author>
			<persName><surname>Tsakalides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2341" to="2356" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning signal-agnostic manifolds of neural fields</title>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8320" to="8331" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04776</idno>
		<title level="m">Generative models as distributions of functions</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">From data to functa: Your data point is a function and you can treat it like one</title>
		<author>
			<persName><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12204</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">E (n) equivariant message passing simplicial networks</title>
		<author>
			<persName><forename type="first">Floor</forename><surname>Eijkelboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Hesselink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9071" to="9081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gemnet: Universal directional graph neural networks for molecules</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gasteiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6790" to="6802" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12231</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The origins and prevalence of texture bias in convolutional neural networks</title>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="19000" to="19015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Era5 monthly averaged data on single levels from 1979 to present</title>
		<author>
			<persName><forename type="first">Hans</forename><surname>Hersbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Berrisford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Biavati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">András</forename><surname>Horányi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquín</forename><surname>Muñoz Sabater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName><surname>Rozum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Copernicus Climate Change Service (C3S) Climate Data Store (CDS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="252" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Equivariant diffusion for molecule generation in 3d</title>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garcia</forename><surname>Vıctor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clément</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8867" to="8887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>David M Knigge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Wessels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuele</forename><surname>Valperga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Jakob</forename><surname>Papa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstratios</forename><surname>Sonke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><surname>Bekkers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.06660</idno>
		<title level="m">Space-time continuous pde forecasting using equivariant neural fields</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph neural networks for learning equivariant representations of neural networks</title>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Kofinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gertjan</forename><forename type="middle">J</forename><surname>Burghouts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=oO6FsMyDBt" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An exploration of conditioning methods in graph neural networks</title>
		<author>
			<persName><forename type="first">Yeskendir</forename><surname>Koishekenov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.01933</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3d reconstruction in function space</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4460" to="4470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Nerf: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Samuele</forename><surname>Papa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Valperga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Knigge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Kofinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Jakob</forename><surname>Sonke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.10531</idno>
		<title level="m">How to train neural field representations: A comprehensive study and benchmark</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepsdf: Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4195" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harm</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the spectral bias of neural networks</title>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Nasim Rahaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Draxler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5301" to="5310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Progressive distillation for fast sampling of diffusion models</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.00512</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">E (n) equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">Garcia</forename><surname>Vıctor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Welling</forename><surname>Hoogeboom</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9323" to="9332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Metasdf: Meta-learning signed distance functions</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Vincent Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="10136" to="10147" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Implicit neural representations with periodic activation functions</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Vincent Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7462" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<title level="m">Denoising diffusion implicit models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratul</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nithin</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utkarsh</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7537" to="7547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learned initializations for optimizing coordinate-based neural representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divi</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2846" to="2855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lusann</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08219</idno>
		<title level="m">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Kendall shape-vae: Learning shapes in a generative framework</title>
		<author>
			<persName><forename type="first">Sharvaree</forename><surname>Vadgama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Mikolaj Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Continuous kendall shape variational autoencoders</title>
		<author>
			<persName><forename type="first">Sharvaree</forename><surname>Vadgama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Bekkers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Geometric Science of Information</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="73" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Capsulenet for micro-expression recognition</title>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Van Quang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhee</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Tokuyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 14th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2019)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">General e (2)-equivariant steerable cnns</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Cesa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Point cloud utils</title>
		<author>
			<persName><forename type="first">Francis</forename><surname>Williams</surname></persName>
		</author>
		<ptr target="https://www.github.com/fwilliams/point-cloud-utils" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neural fields in visual computing and beyond</title>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Towaki</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Numair</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Tompkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="641" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Chao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengyan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alla</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Fast protein backbone generation with se (3) flow matching</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew Yk</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gastegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Jiménez-Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Garcia Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bastiaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.05297</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Yuan Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Yves</forename><surname>Kirchmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName><surname>Gallinari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14855</idno>
		<title level="m">Continuous pde dynamics forecasting with implicit neural representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">3dshape2vecset: A 3d shape representation for neural fields and generative diffusion models</title>
		<author>
			<persName><forename type="first">Jiapeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Philip Hs Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16259" to="16268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Diffusion probabilistic fields</title>
		<author>
			<persName><forename type="first">Peiye</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Bautista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fast context adaptation via meta-learning</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyriacos</forename><surname>Shiarli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Vitaly Kurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><surname>Whiteson</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7693" to="7702" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
