<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-modality fusion using canonical correlation analysis methods: Application in breast cancer survival prediction from histology and genomics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-11-27">27 Nov 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vaishnavi</forename><surname>Subramanian</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tanveer</forename><surname>Syeda-Mahmood</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Do</surname></persName>
							<email>minhdo@illinois.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Health Care Engineering Systems Center</orgName>
								<orgName type="department" key="dep2">Computer Engi- neering Department</orgName>
								<orgName type="institution" key="instit1">University of Illinois at Urbana- Champaign</orgName>
								<orgName type="institution" key="instit2">IBM-Illinois Center for Cognitive Computing Systems Research</orgName>
								<orgName type="institution" key="instit3">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IBM Almaden Research Center</orgName>
								<address>
									<postCode>95120</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-modality fusion using canonical correlation analysis methods: Application in breast cancer survival prediction from histology and genomics</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-11-27">27 Nov 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2111.13987v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multi-modality</term>
					<term>learning</term>
					<term>fusion</term>
					<term>prediction</term>
					<term>correlation</term>
					<term>canonical correlation analysis</term>
					<term>survival</term>
					<term>cancer</term>
					<term>imaging</term>
					<term>genomics</term>
					<term>imaging-genomics</term>
					<term>histopathology</term>
					<term>histology</term>
					<term>RNA-sequencing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The availability of multi-modality datasets provides a unique opportunity to characterize the same object of interest using multiple viewpoints more comprehensively. In this work, we investigate the use of canonical correlation analysis (CCA) and penalized variants of CCA (pCCA) for the fusion of two modalities. We study a simple graphical model for the generation of two-modality data. We analytically show that, with known model parameters, posterior mean estimators that jointly use both modalities outperform arbitrary linear mixing of single modality posterior estimators in latent variable prediction. Penalized extensions of CCA (pCCA) that incorporate domain knowledge can discover correlations with high-dimensional, low-sample data, whereas traditional CCA is inapplicable. To facilitate the generation of multi-dimensional embeddings with pCCA, we propose two matrix deflation schemes that enforce desirable properties exhibited by CCA. We propose a two-stage prediction pipeline using pCCA embeddings generated with deflation for latent variable prediction by combining all the above. On simulated data, our proposed model drastically reduces the mean-squared error in latent variable prediction. When applied to publicly available histopathology data and RNA-sequencing data from The Cancer Genome Atlas (TCGA) breast cancer patients, our model can outperform principal components analysis (PCA) embeddings of the same dimension in survival prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Breast cancer in females is the most commonly diagnosed cancer worldwide. It is estimated that the United States alone will witness a striking 284,200 new breast cancer cases and 44,130 deaths from breast cancer in 2021 <ref type="bibr" target="#b0">[1]</ref>. Given this increasing incidence rate, it is crucial to develop improved techniques for patient survival prediction. More accurate survival prediction will aid in clinical decisionmaking, enabling the right treatment and care to be provided with better outcomes at reduced costs to patients.</p><p>Breast cancer is a highly heterogeneous disease, with heterogeneity manifesting across patients, across tumors of a patient, and also within a single tumor of patients. This makes the accurate survival prediction of breast cancer patients highly challenging <ref type="bibr" target="#b1">[2]</ref>. A study of the morphological, clinical, and molecular features enables treatment planning in practice. These features are captured to varying extents in the different modalities of histology imaging, radiology imaging, genomics and clinical variables (See Figure <ref type="figure" target="#fig_4">1</ref>). Intelligent aggregation of information from these modalities is essential to yield a complete characterization of the tumor in breast cancer patients. For this, multidisciplinary cancer care teams comprising specialists with focus on different modalities (oncologists, radiologists, pathologists, and nurses) provide expert opinions on the same patient to arrive at the best possible care. Such teams have shown potential in improving patient outcome <ref type="bibr" target="#b2">[3]</ref>.</p><p>Algorithms to automatically grade the cancer patients have been developing in the past few decades. One of the first and most widely used methods in breast cancer for survival and prognosis prediction is based on the gene expression of a subset of 50 genes, known as the PAM50 set <ref type="bibr" target="#b3">[4]</ref>. The role of broader genomics markers have been studied for survival prediction and therapeutic implications <ref type="bibr" target="#b4">[5]</ref>. With the rise of automated image analysis and deep learning methods, several works have addressed the task of survival prediction and response to therapy using histology imaging <ref type="bibr" target="#b5">[6]</ref>.</p><p>With the increasing availability of multi-modality datasets, there is great potential in developing algorithms to grade diseases and further improve patient care by combining information from multiple viewpoints using automated methods. In cancer settings, the modalities originate from the same cancer and correspond to histology tissue imaging or radiology imaging, different levels of genomics (such as gene expression and transcriptomics), and overall status of the cancer patient reflected in clinical data (See Figure <ref type="figure" target="#fig_4">1</ref>). These modalities jointly describe the cancer in a more comprehensive way, accounting for the cancer properties at different physical scales.</p><p>To take advantage of the diverse information available from the multiple modalities using automated machinery, fusion methods have been proposed including methods that combine genomics and clinical data <ref type="bibr" target="#b6">[7]</ref>, different imaging modalities <ref type="bibr" target="#b7">[8]</ref>, and imaging data with data from other Figure <ref type="figure" target="#fig_4">1</ref>: Multi-scale cancer data: Information about the same cancer captured at the organism level (clinical data), the organ level (radiology images), the tissue level (histopathology images) and gene expression levels (RNAsequencing etc.) should be aggregated together effectively to characterize the underlying cancer.</p><p>modalities <ref type="bibr" target="#b8">[9]</ref>. Several works have recently focused on fusing imaging and genomics data using deep learning in the form of convolutional neural networks <ref type="bibr" target="#b9">[10]</ref>, novel fusion modules <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, multi-modal autoencoders <ref type="bibr" target="#b12">[13]</ref>, by modelling uncertainty <ref type="bibr" target="#b13">[14]</ref> and machine learning tools <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Other methods also integrate information from more than two modalities to capture the most general setting of multi-modality data <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>Cross-correlations across modalities can capture the joint variation of the modalities by identifying which features are positively correlated, which are negatively correlated, and which are uncorrelated. These correlations can be utilized to overcome missing modalities <ref type="bibr" target="#b18">[19]</ref>, and in learning good representations <ref type="bibr" target="#b19">[20]</ref>. Statistical methods such as canonical correlation analysis (CCA) and independent components analysis (ICA) can provide an understanding of the cross-interactions and cross-correlations from the same object of interest. Canonical correlation analysis (CCA) <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> identifies correlated linear combinations of two given modalities, or multiple given modalities <ref type="bibr" target="#b22">[23]</ref>. Independent components analysis (ICA) <ref type="bibr" target="#b23">[24]</ref> finds statistically independent components, or factors, that compose the multimodality data assuming non-gaussian data. CCA, ICA, and other methods including group ICA, clustering and multifactor dimensionality reduction have been widely applied to imaging-genetics and imaging-genomics problems <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Cross-modality correlations and similarities yield joint embeddings with desirable semantic properties <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> and have also been exploited in recent contrastive learning methods for aligning modalities <ref type="bibr" target="#b28">[29]</ref>.</p><p>In this article, we focus on canonical correlation analysis (CCA) and its penalized variants (pCCA) since these methods make direct use of correlations. CCA can highlight the common information shared across two given modalities by identifying the cross-modality correlations. Penalties based on prior domain knowledge can be added to the CCA formulation in order to effectively work on highdimensional, low-sample-size datasets, such as those of cancer imaging-genomics. These penalized CCA (pCCA) variants add penalties/constraints in the form of sparsity <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, groups <ref type="bibr" target="#b31">[32]</ref> or graphs <ref type="bibr" target="#b32">[33]</ref>.</p><p>The CCA method has been widely used to understand diseases like cancer with multiple modalities, including imaging data <ref type="bibr" target="#b7">[8]</ref> and spatial transcriptomics data <ref type="bibr" target="#b33">[34]</ref>. In our previous works, we studied the effectiveness of CCA and sparsity-based pCCA in discovering correlations between histology imaging and genomics data <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Despite the utility of CCA in diverse analytical medical settings, the effectiveness of CCA-based embeddings for downstream prediction tasks in the presence of additional labels has not yet been sufficiently investigated. CCA has been used for prediction in computer vision tasks <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, though without rigorous justification. Additionally, although CCA embeddings have been used for prediction tasks <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, the use of penalized CCA (pCCA) versions for predictions has remained largely unexplored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>The main contributions of this work are summarized below.</p><p>i) We analytically show that, under a probabilistic model of two-modality data, the posterior mean estimator of the latent variable that make use of both the modalities together perform better than any arbitrary linear combinations of single modality posterior mean estimators. ii) We demonstrate how CCA can be used for two-stage prediction based on the above result by recognizing that CCA outputs serves as maximum likelihood estimators of the model parameters in the probabilistic model. Equivalently, we can by-pass the model parameter estimation and directly use the CCA embeddings for prediction as shown in the two-stage pipeline in Figure <ref type="figure" target="#fig_3">2a</ref>. Stage 1 uses CCA variants to generate multidimensional joint embeddings of the two modalities without label supervision. Stage 2 utilizes the joint embeddings to predict latent variables. iii) We introduce two novel matrix update (deflation) schemes to generate diverse multi-dimensional embeddings with pCCA. These matrix deflation schemes, extended from deflation schemes for sparse PCA <ref type="bibr" target="#b38">[39]</ref>, allow us to capture multi-dimensional correlations by enforcing orthogonality between canonical weights across iterations and, thus, to generate embeddings that capture diverse correlations (Figure <ref type="figure" target="#fig_3">2b</ref>). iv) We demonstrate how our fusion module achieves superior performance on simulated data, and can outperform embeddings obtained from principal components analysis (PCA) in TCGA-BRCA survival prediction.</p><p>The rest of our paper is structured as follows. In section 2, we cover the background on CCA and pCCA. Our main contributions are presented in Section 3, including a mathematical analysis of CCA-based latent variable prediction and our novel deflation schemes. Experiments and results are presented in Section 4. Section 5 concludes our article with key takeaways, limitations of our method, and potential directions of future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>In this section we set up the mathematical notation, present a simple probabilistic model for the generation of twomodality data, and review CCA and its penalized versions.  To generate embeddings with pCCA, we make use of an iterative deflation scheme where each iteration j identifies canonical weights u j , v j . The final embeddings ( Xi , Ỹi ) are generated by taking the product U T X i and</p><formula xml:id="formula_0">V T Y i where U = U 1:K = [u 1 . . . u K ] and V = V 1:K = [v 1 . . . v K ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Mathematical Notations and Problem Setup</head><p>A straightforward probabilistic graphical model for twomodality data is shown graphically in Figure <ref type="figure">3</ref> and is expressed mathematically as</p><formula xml:id="formula_1">z ∼ N (0, I d ), min{p, q} &gt; d &gt; 1, x ∼ N (W x z, Ψ x ), W x ∈ R p×d , Ψ x 0, Ψ x ∈ R p×p , y ∼ N (W y z, Ψ y ), W y ∈ R q×d , Ψ y 0, Ψ y ∈ R q×q ,</formula><p>where z ∈ R d , x ∈ R p , y ∈ R q and I d denotes the identity matrix of d dimensions. The variables x and y represent two different modalities derived from the underlying latent variable z under linear transformations. The probabilistic model can be extended to include structural constraints on the combination matrices W x and W y . The above model is a direct extension of the probabilistic PCA model <ref type="bibr" target="#b39">[40]</ref> and was used in the probabilistic interpretation of CCA <ref type="bibr" target="#b40">[41]</ref>.</p><p>Figure <ref type="figure">3</ref>: A graphical model for two-modalities <ref type="bibr" target="#b40">[41]</ref>. The latent variable of interest (z) influences the two observed modality variables x and y (Theorem 1). For the breast cancer survival prediction setting, z is the survival status, x the genomics feature and y are the imaging features.</p><p>Consider n samples from the above model. The i th sample corresponds to an underlying latent variable z i which is observed as a label i , and a single observation of the two modalities as (x i , y i ), of dimensions p and q respectively. (For each sample, this corresponds to sampling z once, and sampling x, y once given the sampled z.) The observed data from the two modalities can be represented as data matrices X ∈ R p×n and Y ∈ R q×n , with different samples captured across columns and different features represented across rows. That is, the i th column of X, X i , and the i th column of Y, Y i , are both from the same sample, sample i, ∀i ∈ [1 . . . n]. The corresponding labels are { i } n i=1 . In this work, we will apply our method on breast cancer multimodality data, with matrix X as RNA-sequencing gene expressions, Y as the cellular and nuclear features from histopathology images, and unseen labels i corresponding to survival data. Detailed description of these features and labels are in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Canonical Correlation Analysis (CCA)</head><p>Canonical correlation analysis (CCA) <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, aims to discover correlations between two sets of variables. The CCA formulation identifies linear combinations of features in each modality such that the resulting vectors are wellcorrelated, as shown in Figure <ref type="figure" target="#fig_2">4a</ref>. This is in contrast to traditional correlation analyses which focus on pairwise correlations of individual features across modalities. CCA enables the discovery of underlying concepts that can be represented as linear transformations of the two modalities without requiring any label information.</p><p>Consider zero-mean data (Σ n i=1 X i = 0, Σ n i=1 Y i = 0, where X i , Y i are the i th columns of X and Y respectively). The empirical cross-correlation matrix is C xy = XY T and the auto-correlation matrices are C xx = XX T , C yy = YY T . The first iteration of the CCA problem finds</p><formula xml:id="formula_2">ρ * = max u,v u T C xy v s.t. u T C xx u = 1, v T C yy v = 1, (1)</formula><p>with optimal canonical weights (u * , v * ). The corresponding canonical variates are (u * ) T X and (v * ) T Y. This problem is usually simplified to a generalized eigenvalue decomposition <ref type="bibr" target="#b22">[23]</ref> using the Lagrangian stationarity of the optimal canonical weights. Another equivalent formulation is</p><formula xml:id="formula_3">ρ * = max ũ,ṽ ũT Aṽ s.t. ũT ũ = 1, ṽT ṽ = 1, with A = C -1/2 xx C xy C -1/2</formula><p>yy , assuming invertibility of the auto-correlation matrices C xx , C yy . The solution to this optimization problem is the same as finding the first singular vectors (ũ * , ṽ * ) using the singular value decomposition (SVD) of A <ref type="bibr" target="#b30">[31]</ref>. The optimal CCA canonical weights u * and v * can then be obtained as</p><formula xml:id="formula_4">u * = C -1/2 xx ũ * , v * = C -1/2 yy ṽ * .</formula><p>The CCA formulation can be further extended <ref type="bibr" target="#b22">[23]</ref> to identify k canonical weights</p><formula xml:id="formula_5">{u i } k i=1 , {v i } k i=1 such that the resulting matrices U = [u 1 . . . u k ] and V = [v 1 . . . v k ] are the optimizers for the trace of U T C xy V, tr U T C xy V , as below max U,V tr U T C xy V s.t. U T C xx U = I p , V T C yy V = I q , u T i C xy v j = 0, ∀ j = i,</formula><p>and are constrained to be orthogonal across iterations with respect to the empirical correlation matrices. This optimization problem identifies multiple directions of correlations between the two modalities' features, capturing more connections between the two modalities. In this formulation, the canonical weights {u i } k i=1 , {v i } k i=1 are computed simultaneously.</p><p>An iterative formulation of SVD performs</p><formula xml:id="formula_6">ρ * j = max uj ,vj u T j C xy v j s.t. u T j C xx u j = v T j C yy v j = 1, u T j C xx u i = v T j C yy v i = u T j C xy v i = 0, ∀i &lt; j. at the j th iteration, for j = 1 . . . k.</formula><p>Since the CCA solution can be derived from the SVD solution for</p><formula xml:id="formula_7">A = C -1/2 xx C xy C -1/2</formula><p>yy , we take a look at the equivalent formulations of SVD for any matrix A. The simultaneous formulation of SVD of A is given by</p><formula xml:id="formula_8">(U * , V * ) = arg max Ũ, Ṽ tr ŨT A Ṽ s.t. ŨT Ũ = I p , ṼT Ṽ = I q .</formula><p>The iterative formulation of SVD of A with Hotelling deflation (See Figure <ref type="figure">5a</ref>) is</p><formula xml:id="formula_9">(ũ j , ṽj ) = arg max u,v u T A j v s.t. u T u = v T v = 1, A j+1 = A j -σ j ũj ṽT j , (Hotelling Deflation) Ũ = [ũ 1 . . . ũk ], Ṽ = [ṽ 1 . . . ṽk ],</formula><p>, where j = 1, 2, . . . k -1 and A 1 = A. Both the above formulations are equivalent, up to permutation of columns, as can be proven by the orthonormality conditions of the weight vectors. Therefore, carrying forward the result to CCA, the iterative and simultaneous formulations of CCA result in equivalent solutions.</p><p>The iterative formulation of CCA with Hotelling deflation scheme <ref type="bibr" target="#b41">[42]</ref> can be used to obtain the the canonical weights by performing the following updates at the end of each SVD iteration:</p><formula xml:id="formula_10">A j+1 = A j -ρ j ũj ṽT j , j = 1, 2, . . . k -1,<label>(2)</label></formula><p>with</p><formula xml:id="formula_11">A 1 = C -1/2 xx C xy C -1/2</formula><p>yy , with singular vectors ũj , ṽj and singular value ρ j . From these, the canonical weights u j and v j can be computed as</p><formula xml:id="formula_12">u j = C -1/2 xx ũj , v j = C -1/2 yy ṽj ∀ j ∈ [1 . . . k].</formula><p>Since both the iterative and simultaneous formulations of CCA are equivalent, the iterative formulation is preferred because it is computationally cheaper, solving a simpler problem at each iteration. Therefore, we focus on the iterative formulation of CCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Latent variable prediction using CCA</head><p>CCA, proposed as an analysis tool, also has probabilistic implications as noted by Bach and Jordan <ref type="bibr" target="#b40">[41]</ref>. The authors proved that for the two-modality data model in Section 2.1, CCA lends the maximum likelihood estimator of the model parameters.</p><p>Theorem 1 (CCA as ML estimators, Theorem 2 <ref type="bibr" target="#b40">[41]</ref>). Consider the probabilistic model for two-modality data in Section 2.1. Let C xx , C yy and C xy denote the sample correlation matrices. Then, any maximum likelihood (ML) estimator of the model parameters W x , W y , Ψ x , Ψ y are of the form</p><formula xml:id="formula_13">W x = C xx U M x , W y = C yy V M y , Ψ x = C xx -W x W T x , Ψ y = C yy -W y W T y</formula><p>, where U ∈ R p×d and V ∈ R q×d are the canonical weights from CCA stacked column-wise, matrices M x , M y ∈ R d×d are such that M x M T y = P, where P ∈ R d×d is the diagonal matrix of the first d canonical correlations.</p><p>The above theorem proves that the model parameters W x , W y with the maximum likelihood can be estimated using the canonical weight matrices U and V from CCA. These estimated model parameters can then be used to compute posterior estimates of the latent variable z via E[z|x], E[z|y] and E[z|(x, y)], with</p><formula xml:id="formula_14">E[z|x] = M T x U T x = W T x C -1 xx x,<label>(3)</label></formula><formula xml:id="formula_15">E[z|y] = M T y V T y = W T y C -1 yy y,<label>(4)</label></formula><formula xml:id="formula_16">E[z|(x, y)] = M x M y T I P P I -1 U T x V T y (5) = W C xx C xy C yx C yy -1 x y .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Penalized variants of CCA</head><p>The original CCA formulation requires large sample sizes for high-dimensional data (n ≥ max(p, q)) to successfully evaluate the solution via decompositions like SVD. This is restrictive for imaging-genomic studies since most imaging and genomic data are high dimensional with max(p, q) n. In such settings, solving the corresponding generalized eigenvalue problem <ref type="bibr" target="#b22">[23]</ref> is not computationally feasible, and the auto-correlation matrices C xx and C yy fail to be invertible such that the SVD-based algorithm fails.</p><p>To resolve this, convex penalty constraints P x (u) and P y (u) on canonical weights u and v respectively can be added <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. These penalized variants of CCA, henceforth penalized CCA or pCCA, result in biconvex/bilinear problems which can be solved to a local optimum. Adding the convex penalty constraints reduces the search space of solutions and enables CCA to work with high-dimensional, low-sample-size data. For example, sparsity constraints on canonical weight pair (u, v) result in the Sparse CCA (SCCA) formulation <ref type="bibr" target="#b42">[43]</ref> illustrated in Figure <ref type="figure" target="#fig_2">4b</ref>. Setting both penalties to be the 1 norm, SCCA solves</p><formula xml:id="formula_17">ρ * = max u,v u T C xy v s.t. u 2 ≤ 1, v 2 ≤ 1, u 1 ≤ c 1 , v 1 ≤ c 2 ,</formula><p>where C xy = XY T . This biconvex problem is solved in an alternating manner using soft-thresholding based update rules <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Note that the unit-norm constraints here are on the canonical weights u and v, instead of the canonical variates u T X and v T Y in (1), to ensure strict convexity of constraints <ref type="bibr" target="#b43">[44]</ref>. Relaxing the unit norm constraints from u T X, v T Y to u, v in the original CCA problem in (1) (or equivalently assuming C xx = I p , C yy = I q ) results in the SVD problem of the cross-covariance matrix C xy .</p><p>Other penalties capturing structure in the canonical weights u and v can be imposed in the pCCA formulation for incorporating dependencies and prior domain knowledge. For example, the fused-lasso penalty to encourage smoothness in the linear combination <ref type="bibr" target="#b29">[30]</ref> and the grouplasso penalty to to encourage non-overlapping groups of features to be in or out of the correlation together <ref type="bibr" target="#b31">[32]</ref> .</p><p>Graph-based penalties as a generalization of groupbased penalties have also been proposed <ref type="bibr" target="#b32">[33]</ref>. Here, each modality has an underlying graph structure and the graphconstrained elastic net penalties encourage pCCA to select graph-local regions or communities to arrive at Graph-Net SCCA, GN-SCCA (Figure <ref type="figure" target="#fig_2">4c</ref>). For nodes connected by highly weighted edges, the canonical coefficients should be similar. The graphs considered in genomic settings could be from pathways, gene regulatory networks, or proteinprotein interaction networks. The resulting graph-based optimization problem is bi-convex and can be solved using alternating optimization in u and v to a local optimum, similar to SCCA.</p><p>The pCCA variants are well-suited for our application of high-dimensional, low-sample cancer data and additionally provide the opportunity to integrate prior knowledge and structural expectations. In order to generate mul-tiple canonical weights u 1 . . . u k , v 1 . . . v k , the Hotelling deflation scheme is frequently used to deflate the crosscorrelation matrix, and repeat the same set of steps. The direct use of Hotelling deflation might fail to learn novel weights across iterations since orthogonality constraints on canonical weights across iterations are no longer enforced with pCCA. Thus, there is a need to develop new methods of generating multiple canonical weights with pCCA.</p><p>To recapitulate, in this section we presented a simple probabilistic graphical model for generation of twomodality data, the background of CCA and penalized CCA variants, and how CCA can determine the model parameters for the graphical model of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>In this section, we first mathematically show the value of working with both the available modalities jointly in latent variable prediction. We then present two equivalent twostage approaches to utilize CCA for latent variable prediction -one based on estimation of the model parameters using CCA prior to prediction, and the other based on the direct use of the canonical weights from CCA to generate two-modality embeddings for input to the predictors. Lastly, we extend the second two-stage approach to work with pCCA embeddings for latent variable prediction in the most general case. To enforce desirable properties in the generation of pCCA embeddings, we introduce two novel matrix deflation schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Latent variable prediction</head><p>Consider the two-modality data model, with the covariance matrices as Ψ x = σ 2</p><p>x I p and Ψ y = σ 2 y I q . Assume that the model parameters W x , W y , σ x and σ y are all known. Let ẑc = E[z|(x, y)] be posterior mean estimator which estimates z using the two modalities jointly and ẑβ = βE[z|x] + βE[z|y] for β ∈ [0, 1], β = 1 -β be any β linear combination of the estimates based on single modalities x and y. The above theorem states that the posterior mean estimator of z which uses both modalities is better than any arbitrary linear combination of the posterior mean estimators constructed using single modalities. Below, we highlight the key steps and results in the proof. For the complete proof, see Appendix A.</p><p>If only a single modality x or y is used for the estimation, we have the posterior mean estimators as</p><formula xml:id="formula_18">ẑx E[z|x] = (W T x Ψ -1 x W x + I d ) -1 W T x Ψ -1 x Gx x = G x x, ẑy E[z|y] = (W T y Ψ -1 y W y + I d ) -1 W T y Ψ -1 y Gy y = G y y,</formula><p>where Ψ x = σ 2 x I p and Ψ y = σ 2 y I q . A β-weighted linear mixing of these estimators is</p><formula xml:id="formula_19">ẑβ βẑ x + βẑ y = βG x x + βG y y = G x G y βI p 0 0 βI q G β x y = G β x y ,</formula><p>where β ∈ [0, 1] and β = 1 -β. Lastly, the modalities can be used jointly in the estimation of z which leads to the estimator that combines x and y as ẑc E[z|(x, y)]</p><formula xml:id="formula_20">= (W T Ψ -1 W + I d ) -1 W T Ψ -1 Gc x y = G c x y , where W = W x W y and Ψ = Ψ x 0 0 Ψ y = σ 2 x I p 0 0 σ 2 y I q .</formula><p>Lemma 1. For any estimator of the form ẑ = G x y with G ∈ R d×(p+q) , the error in the estimation of z is</p><formula xml:id="formula_21">e(ẑ) = z T (GW -I d ) T (GW -I d )z + tr GΨG T . Lemma 2. Let K be given by K = GW -I d where G = (W T Ψ -1 W + I d ) -1 W T Ψ -1 and Ψ 0. Then, K = -(W T Ψ -1 W + I d ) -1 and is negative definite.</formula><p>From Lemma 1 and 2, we obtain</p><formula xml:id="formula_22">e(ẑ β ) = z T K T β K β z + tr G β ΨG T β ,<label>and</label></formula><formula xml:id="formula_23">e(ẑ c ) = z T K T c K c z + tr G c ΨG T c ,</formula><p>where</p><formula xml:id="formula_24">K β = G β W -I d and K c = G c W -I d . It can be shown that K β -K c 0 and K β + K c 0. These together also imply that tr G T β G β -G T c G c ≥ 0. Together, we have e(ẑ β ) -e(ẑ c ) = z T (K T β K β -K T c K c )z + tr Ψ(G T β G β -G T c G c ) ≥ 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Latent variable prediction with CCA and pCCA</head><p>When the model parameters are known, one can compute the posterior mean estimates using the two-modality observations. CCA provides the maximum likelihood estimators for the model parameters in the general case when the model parameters are known, as shown in Section 2.3. The outputs of CCA can then be used for the posterior mean estimation of z as shown in (3)-( <ref type="formula" target="#formula_54">5</ref>).</p><p>The equations ( <ref type="formula" target="#formula_14">3</ref>)-( <ref type="formula" target="#formula_54">5</ref>) for the latent variable prediction can be cast as two different two-stage prediction models: The second two-stage prediction model can be adapted to more general settings, when the exact prediction matrices are hard to compute, for example with pCCA. Since we are motivated by cancer imaging-genomics settings which have high-dimensionality low-sample data, the use of pCCA is necessary. This two-stage model can also incorporate a broader range of complex prediction modules, for example supervised predictors like multi-layer perceptrons and random forests, with the CCA embeddings as inputs. Hence, we work with this two-stage model for latent variable prediction, as demonstrated in Figure <ref type="figure" target="#fig_3">2a</ref>. Adapting this twostage pipeline to pCCA requires the generation of informative multi-dimensional pCCA embeddings with a wide range of penalties, which we investigate next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unsupervised embedding generation with pCCA</head><p>To generate k-dimensional embeddings, SCCA <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b42">[43]</ref> adopts the same Hotelling deflation (HD) scheme <ref type="bibr" target="#b41">[42]</ref> (Figure <ref type="figure">5a</ref>) as that used in CCA and SVD <ref type="bibr" target="#b1">(2)</ref>. Specifically, the j th canonical weights of SCCA are identified using</p><formula xml:id="formula_25">(u j , v j ) = arg max u,v u T C j-1 v s.t. u 2 ≤ 1, v 2 ≤ 1, u 1 ≤ c 1 , v 1 ≤ c 2 . C j =C j-1 -u j u T j C j-1 v j v T j<label>(7)</label></formula><p>for j ≥ 1, where C 0 = XY T . The applicability of HD to CCA relies on the close relation between CCA and SVD, and the properties of SVD as a decomposition with orthonormal singular vectors. It is desired to discover novel correlations at each iteration. To support this, CCA + HD guarantees that CCA will not pick canonical variates already identified in previous iterations, by enforcing that</p><formula xml:id="formula_26">C T j u i = 0, ∀i ≤ j, C j v i = 0, ∀i ≤ j.</formula><p>Since the objective u T C j v = 0 for all pairs of vectors {(u i , v)} k i=1 , and {(u, v i )} k i=1 , this condition enforces matrix-based orthogonality on the canonical weights across iterations and disallows the canonical weights to be repeated. This encourages a different correlation (and thus different information) to be identified in later iterations.</p><p>(a) Hotelling Deflation: At iteration j a component ρj of the matrix Mj = ujv T j is subtracted from the current crosscovariance matrix Cj to obtain Cj+1. In the case of CCA, this results in an orthogonal projection onto the space R(M ⊥ j ).</p><p>(b) Projected Deflation: At iteration j, the data matrices Xj and Yj are projected onto the orthogonal space of the just-found canonical weights uj and vj respectively.</p><p>(c) Orthogonalized Projected Deflation: At iteration j, the data matrices Xj and Yj are projected onto the orthogonal space of all the previous found canonical weights. Since vectors rj and sj are the new components of the just-found canonical weights uj and vj respectively, it suffices to project the data matrices orthogonal to the vectors rj and sj respectively.</p><p>Figure <ref type="figure">5</ref>: Illustrations of the three deflation schemes used However, the above result relies entirely on the relation of the CCA canonical weights with the singular vectors and does not directly carry forward to a generic pCCA+HD. In particular, whenever the canonical weights are different from the singular vectors, the above result does not hold. The presence of non-zero C T j u i and C j v i is undesirable because pCCA is not being forced to identify new canonical weights, and therefore new correlations, across iterations. We experimentally observed a highly collinear behaviour when using the Hotelling deflation scheme in our previous work on employing SCCA to BRCA <ref type="bibr" target="#b34">[35]</ref>.</p><p>This highlights the need for a better mechanism to generate multiple directions for SCCA and other pCCA variants. In our recent work, we presented a matrix deflation scheme for generating higher dimensional embeddings with pCCA variants <ref type="bibr" target="#b15">[16]</ref>. This update scheme can be shown to reduce to a normalized variant of the Hotelling update scheme as shown in Appendix B, and runs into the same issue. In this work, we only compare our novel deflation schemes with the original Hotelling deflation scheme presented in <ref type="bibr" target="#b6">(7)</ref>.</p><p>The key modification we make in the Hotelling update scheme (Figure <ref type="figure">5a</ref>) is to update the data matrices X j , Y j instead of updating the cross-correlation matrix C j = X j Y T j at iteration j. Directly updating the data matrices removes all contributions of the found canonical weights, enforcing orthogonality of canonical weights across iterations and eliminating the repetition of correlations across iterations. We propose two approaches for this -the projected deflation and orthogonalized projected deflation (Figures <ref type="figure">5b-5c</ref>). These deflation schemes are adapted from the matrix deflation schemes for sparse PCA <ref type="bibr" target="#b38">[39]</ref>.</p><p>In the j th iteration of pCCA with projected deflation (PD) (Figure <ref type="figure">5b</ref>), the steps are</p><formula xml:id="formula_27">(u j , v j ) = pCCA(X j-1 , Y j-1 ), X j = (I p -u j u T j )X j-1 , Y j = (I q -v j v T j )Y j-1</formula><p>, where the initial values of X 0 and Y 0 are set to the data matrix X and Y respectively, and u j , v j are assumed to be normalized to unit norm. At each deflation iteration, the projected deflation scheme projects the data matrices X j-1 and Y j-1 on the space orthogonal to the newly found vector u j and v j respectively. Intuitively, this implies that the next iteration of pCCA will find no correlation by selecting the vectors u j and v j again at iteration j + 1.</p><formula xml:id="formula_28">Mathematically, if u T j u j = 1, C T j u j = Y j X T j u j = Y j X T j-1 (I p -u j u T j )u j = Y j X T</formula><p>j-1 (u j -u j u T j u j ) = 0, and similarly C j v j = 0. Thus, PD ensures that C T j u j = 0, C j v j = 0. Therefore, the value u T C j v obtained by picking u as u j at the next iteration, or picking v as v j in the next iteration, will be 0. However, there is no constraint enforced on how future vectors u i , i &gt; j relate to u j (and similarly for v j ). To further improve the deflation scheme, we can look at the components of newly found vectors u j and v j which are in the space orthogonal to all previous vectors. That is, we can perform orthogonalized projected deflation (OPD) as</p><formula xml:id="formula_29">(u j , v j ) = pCCA(X j-1 , Y j-1 ), r j = rj / rj , rj = (I p -R j-1 R T j-1 )u j , (8) s j = sj / sj , sj = (I q -S j-1 S T j-1 )v j ,<label>(9)</label></formula><formula xml:id="formula_30">X j = (I p -r j r T j )X j-1 , Y j = (I q -s j s T j )Y j-1 , for j ≥ 1, where r 1 = u 1 , s 1 = v 1 , R j-1 = [r 1 . . . r j-1 ],</formula><p>and S j-1 = [s 1 . . . s j-1 ]. Note that (i) r 1 . . . r j-1 are orthonormal vectors (ii) r 1 . . . r j-1 form the basis of the space R j-1 spanned by vectors u 1 . . . u j-1 , (iii) R j-1 R T j-1 is the projection matrix onto the space R j-1 . Similarly, S j-1 S T j-1 is the projection matrix onto the space spanned by vectors v 1 . . . v j-1 . By adding a memory element through matrices R j-1 and S j-1 , OPD enforces additional orthogonality, as demonstrated in Figure <ref type="figure">5c</ref>. By construction, X j lies in the space orthogonal to R j and u j ∈ R j , by <ref type="bibr" target="#b7">(8)</ref>. Therefore, u T j X j = 0 ∀j. Similarly, ∀i j, X i lies in the space orthogonal to R i , while u j ∈ R j ⊆ R i . Extending similar statements to Table <ref type="table">1</ref>: Comparison of different deflation schemes for CCA and pCCA: Hotelling's deflation (HD), projected deflation (PD), orthogonalized projected deflation (OPD) for three properties -P1 (no repetition of canonical weight pairs in consecutive iterations): u T j X j Y T j v j = 0, P2 (no repetition of canonical weights in consecutive iterations): u T j X j = v T j Y j = 0, and P3 (no repetition of canonical weights across all iterations): u</p><formula xml:id="formula_31">T j X i = v T j Y i = 0 ∀i &gt; j. Method P1 P2 P3 CCA pCCA CCA pCCA CCA pCCA HD - - PD - OPD</formula><p>v j with respect to Y i , i ≥ j, the OPD scheme exhibits u T j X i = 0, ∀i ≥ j, v T j Y i = 0, ∀i ≥ j. This guarantees that the new canonical weights identified in later iterations of pCCA lie in a space orthogonal to already found canonical weights. This ensures that the new correlations are identified in newer subspaces, enabling more and diverse correlations to be captured.</p><p>The proposed scheme of deflating the data matrices can be used for CCA, and pCCA with different structural penalties. Table <ref type="table">1</ref> summarizes the different properties: no repetition of canonical weight pairs in consecutive iterations (P1), no repetition of canonical weights in consecutive iterations (P2), and no repetition of canonical weights across all iterations (P3). While all deflation schemes display the desired properties for CCA, the schemes differ for pCCA.</p><p>Our proposed model for latent variable prediction makes use of pCCA and the proposed deflation schemes PD/OPD followed by a supervised prediction module (Figure <ref type="figure" target="#fig_3">2a</ref>). For each setting, the embeddings are generated in an iterative manner using deflation, as illustrated in (Figure <ref type="figure" target="#fig_3">2b</ref>). Among pCCA variants, we focus on SCCA and GN-SCCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND RESULTS</head><p>In this section, we systematically evaluate our proposed prediction pipeline. We begin by working with simulated data to demonstrate the potential of the pCCA-based embeddings in latent variable prediction, and the deflation schemes. We then demonstrate the use of our proposed methods on the TCGA-BRCA data for survival prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Simulated data</head><p>We simulate data according to the model in Theorem 1. For i ∈ {1 . . . N }, we first sample z i ∼ N (0, I d ), the hidden latent variable. We then sample the observation samples x i , y i according to x i |z i ∼ N (W x z, σ 2 p I p ), y i |z i ∼ N (W y z, σ 2 q I q ). Here, the weight matrices W x ∈ R p×d and W y ∈ R q×d are chosen to represent two different structural settings:</p><p>(i) The first setting, Sparse, takes into account sparsity when generating the weight matrices W x and W y . In particular, each row of the weight matrices W x and W y have a fraction s non-zero entries, sampled from the normal distribution N (0, 1). (ii) The second setting, Graph, assumes a graph structure of the weight matrices W x and W y . Here, a connected graph is generated randomly for each modality. Each row of the corresponding weight matrix is generated iteratively as follows. The eigenvectors corresponding to the lowest k non-zero eigenvalues of the graph are linearly combined using uniformly random weights in [0, 1], and then projected onto the space orthogonal to the previous rows. In our simulation experiments, we generate 10 different folds with n = 100, p = q = 200, d = 5, k = 5, σ x = σ y = 0.1, s = 0.25. Each fold is split into 60-10-30% trainingvalidation-testing sets. We evaluate the different pCCA methods and deflation schemes on this simulated data. For the graph-based CCA methods, the underlying graphs are constructed using the empirical covariance matrices as done by <ref type="bibr" target="#b32">[33]</ref>. For all pCCA methods, the hyper-parameter tuning is done on the validation set based on the sum of additional correlations across dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Results</head><p>For this simulated data, we evaluate the different methods on three metrics:</p><p>1) the mean of additional correlations across iterations, 2) the extent of orthogonality of new variates given by ( C T k u j + C k v j )/2, and 3) the mean-squared error (MSE) in the estimation of the latent variable z First, we compute the amount of additional (or new) correlation discovered across iterations which also provides an estimate of the new information contribution across covariate pairs. Specifically, let R k-1 and S k-1 denote the basis of the space covered by already found covariate vector pairs. Then, the additional correlation ρk at each step k is given by the correlation coefficient between r T k X and s T k Y where X and Y are the input data matrices and the vectors r k and s k capture the component of the covariate pair (u k , v k ) orthogonal to the space spanned by R k-1 and S k-1 respectively using  <ref type="table" target="#tab_0">2</ref>. The variation of sum of the new correlations across iterations is shown in Figure <ref type="figure" target="#fig_5">6</ref>. From these, we can observe that both our proposed deflation schemes can identify novel correlations better across simulation structures, especially with GN-SCCA.</p><formula xml:id="formula_32">r k = (I -R k-1 R T k-1 )u k , s k = (I -S k-1 S T k-1 )v k . The vectors</formula><p>Next, we measure the average extent of orthogonality across the two modalities for the pCCA problem with deflation schemes using the term C T k u j 2 + C k v j 2 /2 for k ≥ j. A lower value indicates that the deflation scheme is encouraging diversity in the canonical variates, by removing existing components efficiently. Heatmap visualizations of this is provided in Figure <ref type="figure" target="#fig_6">7</ref> for one of the folds for each type of simulation structure. It can be observed, as expected, that the Hotelling deflation (HD) scheme does not enforce any of the variates to be orthogonal, the projected deflation (PD) scheme enforces orthogonality along the diagonal (low values across the diagonal), while the orthogonalized projected deflation (OPD) scheme enforces orthogonality across variates (low values throughout).</p><p>Lastly, we evaluate the predictive potential of the resulting pCCA-based embeddings from the different pCCA methods using the different deflation schemes. To be consistent across the different variants of pCCA, we learn the latent variable prediction using the machine learning model of multi-layer perceptron (MLP). Specifically, we Table <ref type="table">3</ref>: MSE in the prediction of the latent variable z using the pCCA embeddings with an MLP compared to directly feeding observed data to an MLP (first row) on simulated data (n = 100, p = 200, q = 200, σ = 0. feed in the concatenated embedding vector</p><formula xml:id="formula_33">U T X V T Y to an</formula><p>MLP regressor from scikit-learn. As baselines, we also feed the original data (Modality 1 and Modality 2), the concatenated data (Concatenated) and the single-modality embeddings from the different methods as inputs to MLPs.</p><p>When dealing with single-modality, we use an MLP with 50 hidden neurons, and 100 hidden neurons for the concatenated inputs. The mean-squared errors between the ground truth z and the predicted ẑ is summarized across folds in Table <ref type="table">3</ref> for the two structural settings of simulations.</p><p>It is observed that although concatenation of the original data inputs confuses the MLP predictor, the use of pCCA benefits from the concatenated inputs across simulation settings. Further the proposed deflation schemes PD and OPD improve performance over HD, especially with SCCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Breast cancer data</head><p>Having demonstrated the potential of our proposed method on simulated data, we now proceed to work with realworld cancer data. In the context of breast cancer multimodality data, the underlying survival state (or risk of death) is evident in both genomics and imaging features.</p><p>The particular genomic signature of the cancer determines the aggressiveness of the cancer while the imaging features provides information about the local intensity and impact of the cancer. Hence, it is reasonable to expect, the probabilistic CCA model also applies to the breast cancer survival prediction. Therefore, using the two modalities jointly with the pCCA framework will provide a better estimate of survival, than working with single modalities independently.</p><p>In this subsection, we first introduce the TCGA breast adenocarcinoma (BRCA) data and discuss the feature extraction pipeline before presenting the experimental results. The histology imaging data was acquired from the National Cancer Institute's Genomic Data Commons portal. Corresponding to each patient, we have the histology slides of over 20,000 x 20,000 pixels in size. We downloaded the nuclei segmentations corresponding to the histology images from a recently published adversarial learning framework <ref type="bibr" target="#b44">[45]</ref>. These were converted in 2000 x 2000 pixel patches to generate nuclei segmentation masks across all patches for all patients. Example images and nuclei segmentation are shown in Figure <ref type="figure" target="#fig_8">8</ref>.</p><p>To extract imaging features in a computationally feasible manner, we randomly selected 25 patches of size 2000 x 2000 pixels for each patient. We fed these histology patches and the corresponding segmentation masks to the CellProfiler tool <ref type="bibr" target="#b45">[46]</ref> to extract area, shape and texture properties for each nuclei and cell in the patch -examples of which include area, compactness, eccentricity, Euler number and Zernike moments. These features were summarized across different patches of the same patient, using a 5-bin histogram for each of the 215 extracted features, yielding 1075dimensional imaging feature vectors for each patient. The imaging feature can be expanded to higher dimensions capturing more and diverse features. In particular, techniques using deep learning and convolutional neural networks can be employed for feature extraction or for an end-to-end feature learning, for example if deep learning based CCA is employed instead of pCCA.</p><p>The RNA-seq expression data was downloaded from the FireBrowse platform for all the TCGA-BRCA patients. The gene expression is available for over 20,000 genes. For computational feasibility with the pCCA-based framework, a subset of genes need to be selected. To do so, we evaluated the most variant genes using the coefficient of variation (the ratio σ/µ of standard deviation σ and the mean µ) of the log2-transformed expression values. We selected the top 1000 genes based on the coefficient of variation, and the corresponding z-scores of the genes served as the genomic feature vector for each patient.</p><p>The resulting genomics and imaging features, of dimensions 1000 and 1075 respectively, are input to our proposed prediction pipeline to generate joint embeddings using pCCA and the deflation schemes. The computed embedding vectors from samples can be used for survival prediction using models like Cox proportional hazards model and random survival forests. Survival prediction trains models to correctly assign a risk-value to each sample. With survival data, it is not necessary that all samples have experienced the event of interest. We work with the Cox proportional hazards model which is a commonly used semi-parametric model in survival analysis. The elastic net regularization is frequently imposed on the weights of the model <ref type="bibr" target="#b46">[47]</ref>. We use the algorithm's implementation from the lifelines Python package.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results</head><p>The proposed method is run on the TCGA-BRCA data with the imaging and genomic features as described above. As with simulated data, we evaluate the amount of average additional correlations, the extent of orthogonality and the predictive performance of the different methods with different deflation schemes. The average of the additional correlations across deflation iterations is reported in Table <ref type="table" target="#tab_3">4</ref>. The variation of the sume of additional correlations is plotted in Figure <ref type="figure">9</ref>. When using real data, the graph-based pCCA does not converge with the HD scheme, while PD and OPD are both applicable. The PD and OPD schemes greatly improve the correlations returned by SCCA.</p><p>The extent of orthogonality is visualized as before in Figure <ref type="figure" target="#fig_10">10</ref> for one of the 5 folds. From the plot, it can be seen that our proposed PD and OPD schemes do better at encouraging orthogonality of the later variates for this fold. The same behaviour is also observed across folds. which quantifies the effectiveness of a given risk-prediction algorithm in correctly ordering events. Consider survival data { i = (e i , t i )} N i=1 indicating whether the patient died (e i = 1) or not (e i = 0) during the observation period, and the time t i at which the patient was last alive/observed. Let {o i } N i=1 denote the predicted risk-score for all N patients in the data. It is desired to predict higher risk scores o i for patients with a lower t i if the event was observed, e i = 1. This is captured in the C-index, defined as</p><formula xml:id="formula_34">C-index = 1 n i|ei=1 tj &gt;ti 1[o i &gt; o j ],</formula><p>where n is the number of ordered pairs in the ground-truth data. High C-index values are desired. A C-index of 0.5 is equivalent to a random guess.</p><p>For each fold, we fit the CoxPH model on the training set and report performance on the testing set. We impose the elastic net penalty with a penalizer factor of 0.1. The Cindices for the different methods are in Table <ref type="table" target="#tab_4">5</ref>. We run the CoxPH model with different inputs for each setting. The first row reports the C-indices on a 100-dimensional PCA embedding generated from all genes, a 100-dimensional PCA embedding generated from the imaging feature, and their concatenation. Similarly, later rows report the C-indices of using embeddings from different pCCA and deflation schemes with genomics embeddings U T X only, imaging embedding V T Y only, and the concatenation of both as inputs U T X V T Y to the CoxPH. In our experiments, GN-SCCA was unable to train successfully when using HD. However, GN-SCCA worked with PD and OPD schemes.</p><p>From the C-index performance, it can be observed that while the use of the existing HD deflation scheme with pCCA results in a poorer performance than PCA embeddings, the use of PD and OPD schemes has the potential to improve performance. This is evidenced in the concatenated GN-SCCA embeddings with PD and OPD. The concatenated GN-SCCA embeddings perform better than single </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work we investigated the use of CCA-based embeddings for latent variable prediction. We first proved mathematically why the joint posterior mean estimators are better than combining individual modality posterior mean estimators for latent variable prediction using a probabilistic model of two-modality data. We recalled that CCA provides the maximum likelihood estimates of the model parameters for the studied graphical model. Using this result, we proposed a two-stage prediction model with CCA. Next, we proposed two deflation schemes that can help generate informative, multi-dimensional embeddings when working with the penalized versions of CCA, pCCA. We demonstrated the efficacy of our proposed two-stage prediction model on simulated data and real data from TCGA-BRCA histology and RNA-seq data. Our method improves latent variable prediction across different settings of simulated data, with desirable properties of embeddings. On the TCGA-BRCA data, we discovered better embeddings with desirable properties that displayed potential to improve survival prediction. Overall, our results highlight the importance of intelligently combining multi-modality data. In this work, by focusing on the shared information captured by pCCA variants, we are able to improve latent variable prediction performance.</p><p>Our work faces a few limitations. Although the use of penalized variants is motivated by the possibility to use prior knowledge, we did not make use of any such information from protein-protein interaction networks, gene regulatory networks or molecular pathways in biology. Incorporating these prior knowledge could greatly benefit survival prediction. In addition, our method does not tackle non-linear or more complex embeddings, for example those generated from neural networks. It would be interesting to explore how multi-modality fusion can be performed using deep learning methods in a systematic way with theoretical backing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A ERROR IN ESTIMATION OF LATENT VARIABLES</head><p>We work with a two-modality data model as motivated in <ref type="bibr" target="#b40">[41]</ref>. Consider z ∼ N (0, I d ),</p><formula xml:id="formula_35">x = W x z + x , x ∼ N (0, σ 2</formula><p>x I p ), y = W y z + y , y ∼ N (0, σ 2 y I q ), where z ∈ R d , x ∈ R p , y ∈ R q , the combination matrices are W x ∈ R p×d and W y ∈ R q×d and random vectors z, x , y are independent. Assume that the model parameters W x , W y , σ x and σ y are all known. Let ẑc = E[z|(x, y)] be posterior mean estimator which estimates z using the two modalities jointly and ẑβ = βE If only a single modality x is used for the estimation, we have the posterior mean estimator as</p><formula xml:id="formula_36">ẑx E[z|x] = (W x W T x + σ 2 x I) -1 W T x x = (W T x Ψ -1 x W x + I d ) -1 W T x Ψ -1 x Gx x = G x x,</formula><p>where Ψ x = σ 2 x I p . Similarly, if only modality y is used, the posterior mean estimator is</p><formula xml:id="formula_37">ẑy E[z|y] = (W y W T y + σ 2 y I) -1 W T y y = (W T y Ψ -1 y W y + I d ) -1 W T y Ψ -1 y Gy y = G y y,</formula><p>where Ψ y = σ 2 y I q . A β-weighted linear mixing of these estimators is</p><formula xml:id="formula_38">ẑβ βẑ x + βẑ y = βG x x + βG y y = G x G y βI p 0 0 βI q G β x y = G β x y ,</formula><p>where β ∈ [0, 1] and β = 1 -β. Note that this estimator ẑβ includes the estimators ẑx and ẑy as special cases for β = 1 and β = 0 respectively. Lastly, the modalities can be used jointly in the estimation of z which leads to the estimator that combines x and y as ẑc E[z|(x, y)]</p><formula xml:id="formula_39">= (W T Ψ -1 W + I d ) -1 W T Ψ -1 Gc x y = G c x y , where W = W x W y and Ψ = Ψ x 0 0 Ψ y = σ 2 x I p 0 0 σ 2 y I q such that Ψ -1 = Ψ -1 x 0 0 Ψ -1 y = 1 σ 2 x I p 0 0 1 σ 2 y I q .</formula><p>Lemma 3. For any estimator of the form ẑ = G x y with G ∈ R d×(p+q) , the error in the estimation of z is</p><formula xml:id="formula_40">e(ẑ) = z T (GW -I d ) T (GW -I d )z + tr GΨG T .</formula><p>Proof. Expanding the error expression, we obtain</p><formula xml:id="formula_41">e(ẑ) = E[(ẑ -z) T (ẑ -z)] = E[(G x y -z) T (G x y -z)] = E[(G W x z + x W y z + y -z) T (G W x z + x W y z + y -z) = E[(GWz + G -z) T (GWz + G -z)] = E[z T (GW -I d ) T (GW -I d )z] + E[ T G T G ] = z T (GW -I d ) T (GW -I d )z + E[ T G T G ],</formula><p>where =</p><p>x y , with ∼ N (0, Ψ). Thus, G ∼ N (0, GΨG T ), such that the second term of the error e(ẑ) is</p><formula xml:id="formula_42">E[ T G T G ] = E[(G ) T G ] = E[tr (G ) T G ] = E[tr (G )(G ) T ] = E[tr GΨG T ] = tr GΨG T ,</formula><p>where tr [M] denotes the trace of matrix M. Therefore,</p><formula xml:id="formula_43">e(ẑ) = z T (GW -I d ) T (GW -I d )z + tr GΨG T .</formula><p>From Lemma 3, we obtain</p><formula xml:id="formula_44">e(ẑ β ) = z T (G β W -I d ) T (G β W -I d )z + tr G β ΨG T β = z T K T β K β z + tr G β ΨG T β , and<label>(A1)</label></formula><formula xml:id="formula_45">e(ẑ c ) = z T (G c W -I d ) T (G c W -I d )z + tr G c ΨG T c = z T K T c K c z + tr G c ΨG T c ,<label>(A2)</label></formula><p>where</p><formula xml:id="formula_46">K β = G β W -I d and K c = G c W -I d . Lemma 4. Let K be given by K = GW -I d where G = (W T Ψ -1 W + I d ) -1 W T Ψ -1 and Ψ 0. Then, K = -(W T Ψ -1 W + I d ) -1</formula><p>and is negative definite.</p><p>Proof. Simplifying the expression for K, we obtain The matrix products L x L T x and L y L T y are positive definite, and thus invertible. For the joint estimator, the matrix K c is</p><formula xml:id="formula_47">K = GW -I d = (W T Ψ -1 W + I) -1 W T Ψ -1 W -I d = (W T Ψ -1 W + I) -1 (W T Ψ -1 W + I d -I d ) -I d = I d -(W T Ψ -1 W + I) -1 -I d = -(W T Ψ -1 W + I) -1 . Further Ψ 0, =⇒ Ψ -1 0 =⇒ W T Ψ -1 W 0 =⇒ W T Ψ -1 W + I d 0 =⇒ (W T Ψ -1 W + I d ) -1 0. Corollary 1. K x = G x W x -I d , K y = G y W y -I d ,</formula><formula xml:id="formula_48">K c = G c W -I d = (W T Ψ -1 W + I d ) -1 W T Ψ -1 W -I d = -(W T Ψ -1 W + I d ) -1 = -(W T x Ψ -1 x W x + W T y Ψ -1 y W y + I d ) -1 = -((L x L T x ) -1 + (L y L T y ) -1 -I d ) -1 = -(L y L T y ) L x L T x + L y L T y -(L x L T x )(L y L T y ) L -1 (L x L T x ) = -L x L y 0 L T y L-1 L x 0 L T x L T y = -L x L y 0 0 L T y L-1 L x 0 L T x L T y .</formula><p>(A4) Lemma 5. Let K β and K c be as defined in (A3) and (A4). Then,</p><formula xml:id="formula_49">K β -K c 0.</formula><p>Proof. Since L is lower triangular with non-positive diagonal entries, L is negative semi-definite. Thus, K β -K c is negative semi-definite.</p><p>Lastly, note that</p><formula xml:id="formula_50">G β = - βI d 0 0 βI d G x G y = - βI d 0 0 βI d K x W T x Ψ -1 x K y W T y Ψ -1 y = - βI d 0 0 βI d K x K y K β W T Ψ -1 = -K β W T Ψ -1</formula><p>, and</p><formula xml:id="formula_51">G c = (W T Ψ -1 W + I d ) -1 W T Ψ -1 = -K c W T Ψ -1 .</formula><p>We now have all the results to prove Theorem A.1.</p><p>Proof of Theorem A.1. Comparing the errors corresponding to ẑβ (A1) and ẑc (A2), we have e(ẑ β ) -e(ẑ c )</p><formula xml:id="formula_52">= (z T K T β K β z + tr G β ΨG T β ) -(z T K T c K c z + tr G c ΨG T c ) = z T (K T β K β -K T c K c )z + tr G β ΨG T β -tr G c ΨG T c = z T (K 2 β -K 2 c )z + tr ΨG T β G β -tr ΨG T c G c = z T (K β -K c )(K β + K c )z ≥0 +tr Ψ(G T β G β -G T c G c ) ≥ tr Ψ(G T β G β -G T c G c ) = tr Ψ Ψ -1 W(K T β K β -K T c K c )W T Ψ -1 = tr (K T β K β -K T c K c )W T Ψ -1 W = tr (K β -K c )(K β + K c )W T Ψ -1 W ≥ 0.</formula><p>In the derivation above, we used the fact that</p><formula xml:id="formula_53">(K β - K c )(K β + K c ) 0 since z T (K β -K c )(K β + K c )z ≥ 0 ∀ z and (K β -K c )(K β +K c</formula><p>) is symmetric. Similarly, the matrix (K β -K c )(K β + K c )W T Ψ -1 W 0 and has non-negative eigenvalues, leading to a non-negative trace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B NORMALIZED HOTELLING DEFLATION</head><p>The deflation scheme to generate higher-dimensional embeddings in our previous work <ref type="bibr" target="#b15">[16]</ref> is a normalized version of the Hotelling deflation scheme. The first step of this update scheme is</p><formula xml:id="formula_54">C j = C j-1 - C j-1 , u j v T j v u j v T j u j v T j ,<label>(A5)</label></formula><p>where ., . v denotes the inner product of the vectorized version of the matrix entries. Under unit-norm constraints on the canonical variates of the CCA problem, (A5) can be shown to reduce exactly to the Hotelling update, as following. Assume that the SVD decomposition for the matrix C 1 is given by C 1 = r =1 σ α β T , where r is the rank of the cross correlation matrix C 1 . Further, it can be shown that the vectorized inner product α i β T i , α j β T j v is equal to α i , α j β i , β j .</p><p>For j = 2, we have</p><formula xml:id="formula_55">C 2 = C 1 - C 1 , u 1 v T 1 v u 1 v T 1 u 1 v T 1 = C 1 - r =1 ρ α β T , u 1 v T 1 v u 1 v T 1 u 1 v T 1 ,</formula><p>which can be simplified to</p><formula xml:id="formula_56">C 2 = C 1 - r =1 ρ α β T , α 1 β T 1 v α 1 β T 1 α 1 β T 1 = C 1 - r =1 ρ α β T , α 1 β T 1 v α 1 β T 1 , α 1 β T 1 1/2 v α 1 β T 1 = C 1 -ρ 1 α 1 β T 1 , α 1 β T 1 v α 1 β T 1 , α 1 β T 1 1/2 v α 1 β T 1 = C 1 -ρ 1 α 1 β T 1 , α 1 β T 1 1/2 v α 1 β T 1 = C 1 -ρ 1 α 1 , α 1 1/2 β 1 , β 1 1/2 α 1 β T 1 = C 1 -ρ 1 α 1 β T 1 = r =2 ρ α β T ,</formula><p>which is equivalent to the Hotelling update for this setting. This can be extended to further iterations. Let's assume that at any iteration i, we have C i = r =i ρ α β T . Then, at iteration i + 1,</p><formula xml:id="formula_57">C i+1 = C i - C i , u i v T i v u i v T i u i v T i = C i - r =i ρ α β T , α i β T i v α i β T i α i β T i = C i -ρ i α i β T i , α i β T i 1/2 v α i β T i = C i -ρ i α i , α i 1/2 β i , β i 1/2 α i β T i = r =i+1 ρ α β T ,</formula><p>such that the first step of the update scheme in <ref type="bibr" target="#b15">[16]</ref> is the same as the Hotelling deflation in this setting. The second step of the update scheme involving the scaling normalization of the cross-covariance matrix is primarily for numerical reasons. Therefore, the overall update scheme in <ref type="bibr" target="#b15">[16]</ref> is a normalized version of the Hotelling deflation.</p><p>The scale factor will propagate across the iterations, such that the properties of the Hotelling deflation would continue to hold here.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Overview of our proposed two-stage model for prediction.(b) Overview of embedding generation using deflation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Overview: (a) Our proposed two-stage model for prediction takes in two modalities (X i , Y i ) and utilizes pCCA with deflation to generate embeddings ( Xi , Ỹi ) in an unsupervised manner. The embeddings are concatenated and fed to a (potentially) supervised prediction module for label prediction. (b) To generate embeddings with pCCA, we make use of an iterative deflation scheme where each iteration j identifies canonical weights u j , v j . The final embeddings ( Xi , Ỹi ) are generated by taking the product U T X i and V T Y i where U = U 1:K = [u 1 . . . u K ] andV = V 1:K = [v 1 . . . v K ].</figDesc><graphic coords="3,336.96,86.12,227.04,127.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: CCA, SCCA and GN-SCCA aim to identify the first set of canonical weights u 1 and v 1 such that the correlation between u T X and v T Y is maximized, subject to no constraints (CCA), sparsity constraints on u, v (SCCA), and graphbased smoothness constraints on u, v (GN-SCCA). This can be repeated to identify later sets (u k , v k ), k &gt; 1 using deflation schemes. Here, the first two sets of canonical weights (u 1 , v 1 ) and (u 2 , v 2 ) are shown.</figDesc><graphic coords="5,259.55,44.73,118.70,147.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 2 .</head><label>2</label><figDesc>Under the assumed data model with known model parameters W x , W y , σ x and σ y , ẑc is the better estimator compared to ẑβ in terms of e(ẑ) = E (x,y)|z [ ẑz 2 2 ], the mean squared error in the estimation of z, with e(ẑ β ) ≥ e(ẑ c ) ∀β ∈ [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 )</head><label>1</label><figDesc>Stage 1: Estimation of the probabilistic model parameters W x , W y , Ψ x , Ψ y ; Stage 2: Posterior mean estimation of z based on analytical formula using estimates of the model parameters W, and 2) Stage 1: Estimation of the CCA-based embeddings U T x, V T y; Stage 2: Estimation of z using the concatenated vector U T x V T y . The equations (3)-(5) lead to a prediction with the matrix M x M y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: A plot of the sum of additional correlations identified across iterations k j=1 ρj summarized across 10 folds of simulations, with k along the x-axis and the sum along the y-axis for sparse data (top) and graph-structured data (bottom) with SCCA and GN-SCCA. Higher values desired.</figDesc><graphic coords="8,312.00,538.93,252.84,105.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Heatmap visualization of the orthogonality term ( C T k u j + C k v j )/2 for k ≥ j with k along the x-axis, and j along the y-axis for different deflations across rows for SCCA and GN-SCCA schemes. Results for graph-structured data (left) and sparse data (right). Lower values desired.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>r k and s k are appended as columns to the matrices R k-1 , S k-1 to yield R k , S k . Under the definition above, the mean of additional correlations returned across the d embedding dimensions 1 d d k=1 ρk of the different pCCA methods and deflation schemes are shown in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: (a-b) Examples of histology images from TCGA-BRCA , and (c) an example of segmentations output by [45]. Best viewed in color.</figDesc><graphic coords="10,135.30,43.70,77.39,78.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: A plot of the sum of additional correlations identified across iterations k j=1 ρj summarized across 5 folds, with k along the x-axis and the sum along the y-axis for the TCGA-BRCA data with SCCA and GN-SCCA. Higher values desired.</figDesc><graphic coords="10,312.00,520.64,255.41,88.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Theorem A. 1 .</head><label>1</label><figDesc>[z|x] + βE[z|y] for β ∈ [0, 1], β = 1 -β be any β linear combination of the estimates based on single modalities x and y. Under the assumed data model with known model parameters W x , W y , σ x and σ y , ẑc is the better estimator compared to ẑβ in terms of e(ẑ) = E (x,y)|z [ ẑz 2 2 ], the mean squared error in the estimation of z, with e(ẑ β ) ≥ e(ẑ c ) ∀β ∈ [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>and K c = G c W -I d are all negative definite.Proof. For K x , K y , substitute G, W with matrices G x , W x and G y , W y respectively. Setting G = G c yields the result for K c .Corollary 2. Kβ = G β W -I d is negative definite.Proof. We observe thatK β = G β W -I d = G x G y βI p 0 0 βI q W -I d = β(G x W x -I d ) + β(G y W y -I d ), = βK x + βK y ,where β ∈ [0, 1], β = 1 -β. Thus, K β is negative definite.The matrices K x and K y can be decomposed using the Cholesky decomposition asK x = -L x L T x K y = -L y L T yusing lower triangular matrices L x , L y ∈ R d×d with positive diagonal entries, such thatK β = -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>K=</head><label></label><figDesc>β -K c = L x L y L x L y -βI d 0 L T y L-1 L x -βI d L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Mean of additional correlations identified across d = 5 deflation iterations on simulated data (n = 100, p = 200, q = 200, σ = 0.1) with sparse structure (top) and graph structure (bottom). Results summarized across 10 folds and shown in percentages %. Higher values desired.</figDesc><table><row><cell></cell><cell>pCCA</cell><cell>HD</cell><cell>PD</cell><cell>OPD</cell></row><row><cell>Sparse</cell><cell cols="4">SCCA GN-SCCA 92.69 ± 3.34 99.19 ± 0.31 99.21 ± 0.34 89.48 ± 8.64 99.57 ± 0.28 99.61 ± 0.36</cell></row><row><cell>Graph</cell><cell cols="4">SCCA GN-SCCA 81.13 ± 5.70 93.89 ± 0.93 94.32 ± 1.08 93.41 ± 3.22 94.83 ± 1.91 94.87 ± 1.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>25 ± 18.18 36</head><label></label><figDesc>1) for sparse and graph-structured simulation data. Lower values desired. .49 ± 16.68 36.40 ± 25.12 GN-SCCA 22.93 ± 18.44 24.72 ± 19.00 16.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">(a) Sparse data</cell></row><row><cell></cell><cell>Method</cell><cell>Modality 1</cell><cell>Modality 2</cell><cell>Concatenated</cell></row><row><cell></cell><cell>Original</cell><cell>40.72 ± 6.45</cell><cell>39.74 ± 6.66</cell><cell>42.76 ± 8.73</cell></row><row><cell>HD</cell><cell>SCCA</cell><cell>35.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>05 ± 11.03</head><label></label><figDesc></figDesc><table><row><cell>PD</cell><cell>SCCA GN-SCCA</cell><cell cols="3">29.04 ± 18.35 29.32 ± 19.88 24.17 ± 19.21 20.57 ± 9.14 20.38 ± 7.71 16.63 ± 6.21</cell></row><row><cell>OPD</cell><cell>SCCA GN-SCCA</cell><cell cols="3">19.74 ± 13.66 21.28 ± 15.05 20.26 ± 15.69 23.40 ± 9.58 22.58 ± 8.65 17.42 ± 7.69</cell></row><row><cell></cell><cell></cell><cell cols="2">(b) Graph-structured data</cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell>Modality 1</cell><cell>Modality 2</cell><cell>Concatenated</cell></row><row><cell></cell><cell>Original</cell><cell cols="2">74.28 ± 13.03 75.30 ± 6.51</cell><cell>75.50 ± 8.37</cell></row><row><cell>HD</cell><cell cols="3">SCCA GN-SCCA 34.30 ± 12.27 27.95 ± 10.59 21.24 ± 13.82 21.98 ± 12.53</cell><cell>18.66 ± 9.42 20.74 ± 8.04</cell></row><row><cell>PD</cell><cell>SCCA GN-SCCA</cell><cell>15.34 ± 8.69 31.23 ± 6.61</cell><cell>14.51 ± 7.68 29.64 ± 8.13</cell><cell>12.90 ± 7.03 26.06 ± 9.57</cell></row><row><cell>OPD</cell><cell>SCCA GN-SCCA</cell><cell>14.50 ± 8.10 27.64 ± 7.12</cell><cell>14.61 ± 6.36 27.63 ± 9.84</cell><cell>12.16 ± 6.63 24.04 ± 10.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The average additional correlations across the 50 iterations of deflation used to generate the pCCA-based embeddings with TCGA-BRCA data. Higher values desired.</figDesc><table><row><cell>Method</cell><cell>HD</cell><cell>PD</cell><cell>OPD</cell></row><row><cell>SCCA</cell><cell>6.76 ± 0.87</cell><cell cols="2">9.47 ± 1.10 12.05 ± 1.56</cell></row><row><cell>GN-SCCA</cell><cell>-</cell><cell cols="2">10.29 ± 0.43 10.07 ± 0.42</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The C-indices returned for survival prediction of TCGA-BRCA using CoxPH with different inputs. Results summarized across the 5-folds. Higher values desired. ± 8.40 55.66 ± 11.14 67.37 ± 6.58 GN-SCCA 64.94 ± 8.98 56.81 ± 13.31 66.42 ± 12.08 SCCA embeddings. The poorer performance of imaging features compared to genomics features across the table highlight the need to improve these features by adding more complexity in the histology feature extraction.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Genomics</cell><cell>Imaging</cell><cell>Concatenated</cell></row><row><cell></cell><cell>PCA</cell><cell>65.76 ± 9.02</cell><cell>61.24 ± 6.39</cell><cell>66.86 ± 10.40</cell></row><row><cell>HD</cell><cell>SCCA GN-SCCA</cell><cell cols="2">65.95 ± 10.70 54.00 ± 11.23 --</cell><cell>65.71 ± 7.96 -</cell></row><row><cell cols="4">PD 68.34 OPD SCCA SCCA 67.63 ± 9.43 54.19 ± 11.23 GN-SCCA 65.52 ± 7.67 57.87 ± 10.28</cell><cell>65.25 ± 5.61 68.08 ± 9.88</cell></row><row><cell cols="2">modality GN-</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cancer statistics, 2021</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Journal for Clinicians</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="33" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>CA: a</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Molecular and cellular heterogeneity in breast cancer: challenges for personalized medicine</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Rivenbark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American journal of pathology</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1113" to="1124" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multidisciplinary team approach in breast cancer care: benefits and challenges</title>
		<author>
			<persName><forename type="first">O</forename><surname>Blackwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Indian Journal of Pathology and Microbiology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">105</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised risk predictor of breast cancer based on intrinsic subtypes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mullins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Cheang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Voduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vickery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fauron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of clinical oncology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1160</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toward the precision breast cancer survival prediction utilizing combined whole genome-wide expression and somatic mutation analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC medical genomics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="99" to="107" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Classification of breast cancer histology images using multi-size and discriminative patches based on deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">408</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting the survival of cancer patients with multimodal graph neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An automatic computer-aided diagnosis system based on the multimodal fusion of breast cancer (MF-CAD)</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mokni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gargouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Damak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sellami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mnif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Supervised regularized canonical correlation analysis: integrating histologic and proteomic measurements for predicting biochemical recurrence following prostate surgery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Golugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Master</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Tomaszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Speicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting cancer outcomes from histology and genomics using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mobadersany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yousefi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amgad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Barnholtz-Sloan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E V</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Brat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">13</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>pp. E2970-E2979</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pathomic fusion: an integrated framework for fusing histopathology and genomic features for cancer diagnosis and prognosis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Rodig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Lindeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal fusion of imaging and genomics for lung cancer recurrence prediction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Syeda-Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="804" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">G-MIND: an end-to-end multimodal imaging-genetics framework for biomarker identification and disease classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pergola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Blasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rampino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bertolino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging 2021: Image Processing</title>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">11596</biblScope>
			<biblScope unit="page">115960</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling uncertainty in multi-modal fusion for lung cancer survival analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Syeda-Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">1169</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Integrating genomic data and pathological images to effectively predict breast cancer clinical outcome</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer methods and programs in biomedicine</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="45" to="53" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal fusion using sparse CCA for breast cancer survival prediction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Syeda-Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">1429</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning with multimodal representation for pancancer prognosis prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cheerla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gevaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="446" to="454" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Braman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Goossens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Willis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stumpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Venkataraman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00648</idno>
		<title level="m">Deep orthogonal fusion: Multimodal prognostic biomarker discovery integrating radiology, pathology, genomic, and clinical data</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation with missing modalities via latent multi-source correlation representation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="533" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modality-correlation-aware sparse representation for rgb-infrared object tracking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="12" to="20" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relations between two sets of variates</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="321" to="377" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Canonical analysis of several sets of variables</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kettenring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="433" to="451" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Independent component analysis: algorithms and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="411" to="430" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A review of multivariate analyses in imaging genetics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Calhoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imaging genomics for accurate diagnosis and treatment of tumors: A cutting edge overview</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedicine &amp; Pharmacotherapy</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Crossclr: Crossmodal contrastive learning for multi-modal video representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1450" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="214" to="229" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV 16</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Extensions of sparse canonical correlation analysis with applications to genomic data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical applications in genetics and molecular biology</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sparse canonical correlation analysis with application to genomic data integration</title>
		<author>
			<persName><forename type="first">E</forename><surname>Parkhomenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tritchler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beyene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical applications in genetics and molecular biology</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An efficient optimization algorithm for structured sparse CCA, with applications to EQTL mapping</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in Biosciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="26" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">GN-SCCA: Graphnet based sparse canonical correlation analysis for brain imaging genetics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Risacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Saykin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Brain Informatics and Health</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="275" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Comprehensive integration of single-cell data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hafemeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Papalexi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Mauck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stoeckius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Satija</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1888" to="1902" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Correlating cellular features with gene expression using CCA</title>
		<author>
			<persName><forename type="first">V</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chidester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="805" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Integration of spatial distribution in imaging-genetics</title>
		<author>
			<persName><forename type="first">V</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chidester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="245" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A new method of feature fusion and its application in image recognition</title>
		<author>
			<persName><forename type="first">Q.-S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2437" to="2448" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fully automatic face normalization and single sample face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haghighat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdel-Mottaleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Alhalabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="23" to="34" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deflation methods for sparse PCA</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Mackey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Probabilistic principal component analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="611" to="622" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A probabilistic interpretation of canonical correlation analysis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Analysis of a complex of statistical variables into principal components</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of educational psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">417</biblScope>
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="515" to="534" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Collaborative regression</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="326" to="338" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust histopathology image analysis: to label or to synthesize</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Kurc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Saltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference CVPR</title>
		<meeting>the IEEE Conference CVPR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8533" to="8542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cellprofiler: image analysis software for identifying and quantifying cell phenotypes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lamprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Friman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Guertin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Lindquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Regularization paths for generalized linear models via coordinate descent</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical software</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
