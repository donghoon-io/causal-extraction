<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Bayesian Inference for Audio-Visual Tracking of Multiple Speakers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yutong</forename><surname>Ban</surname></persName>
						</author>
						<author>
							<persName><roleName>IEEE Senior Member</roleName><forename type="first">Xavier</forename><surname>Alameda-Pineda</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Girin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Radu</forename><surname>Horaud</surname></persName>
						</author>
						<title level="a" type="main">Variational Bayesian Inference for Audio-Visual Tracking of Multiple Speakers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Audio-visual tracking</term>
					<term>multiple object tracking</term>
					<term>dynamic Bayesian networks</term>
					<term>variational inference</term>
					<term>expectationmaximization</term>
					<term>speaker diarization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we address the problem of tracking multiple speakers via the fusion of visual and auditory information. We propose to exploit the complementary nature and roles of these two modalities in order to accurately estimate smooth trajectories of the tracked persons, to deal with the partial or total absence of one of the modalities over short periods of time, and to estimate the acoustic status -either speaking or silent -of each tracked person over time. We propose to cast the problem at hand into a generative audio-visual fusion (or association) model formulated as a latent-variable temporal graphical model. This may well be viewed as the problem of maximizing the posterior joint distribution of a set of continuous and discrete latent variables given the past and current observations, which is intractable. We propose a variational inference model which amounts approximating the joint distribution with a factorized distribution. The solution takes the form of a closed-form expectation maximization procedure. We describe in detail the inference algorithm, we evaluate its performance and we compare it with several baseline methods. These experiments show that the proposed audio-visual tracker performs well in informal meetings involving a time-varying number of people.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In this paper we address the problem of tracking multiple speakers via the fusion of visual and auditory information <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b6">[7]</ref>. We propose to exploit the complementary nature of these two modalities in order to accurately estimate the position of each person at each time step, to deal with the partial or total absence of one of the modalities over short periods of time, and to estimate the acoustic status, either speaking or silent, of each tracked person. We propose to cast the problem at hand into a generative audio-visual fusion (or association) model formulated as a latent-variable temporal graphical model. We propose a tractable solver via a variational approximation.</p><p>We are particularly interested in tracking people involved in informal meetings and social gatherings, e.g. Fig. <ref type="figure">1</ref>. In this type of scenarios, participants wander around, cross each other, move in and out the camera field of view, take speech turns, etc. Acoustic room conditions, e.g. reverberation, and overlapping audio sources of various kinds drastically deteriorate or modify the microphone signals. Likewise, occluded persons, Y. Ban, X. Alameda-Pineda and R. Horaud are with Inria Grenoble Rhône-Alpes, Montbonnot Saint-Martin, France. L. Girin is with GIPSA Lab, Univ. Grenoble Alpes, France.</p><p>Funding from the European Union via the FP7 ERC Advanced Grant VHIA #340113 is greatly acknowledged. Fig. <ref type="figure">1</ref>: Multiple speaker tracking is cast into the framework of Bayesian inference. Visual observations (person detections) and audio observations (inter-channel spectral features) are assigned to continuous latent variables (i.e. speaker positions) via discrete latent variables (one for each observation). As shown, the algorithm is causal (it uses only past and present observations) and incorporates a birth process to account for not yet seen/heard persons. lighting conditions and mid-range camera distance complicate the task of visual processing. It is therefore impossible to gather reliable and continuous flows of visual and audio observations. Hence one must design a fusion and tracking method that is able to deal with intermittent visual and audio data.</p><p>We propose a multi-speaker tracking method based on a dynamic Bayesian model that fuses audio and visual information over time from their respective observation spaces. This may well be viewed as a generalization of single-observation and single-target Kalman filtering -which yields an exact recursive solution -to multiple observations and multiple targets, which makes the exact recursive solution computationally intractable. We propose a variational approximation of the joint posterior distribution over the continuous variables (positions and velocities of tracked persons) and discrete variables (observation-toperson associations) at each time step, given all the past and present audio and visual observations. The proposed approximation consists on factorizing the joint distribution. We obtain a variational expectation maximisation (VEM) algorithm that is not only computationally tractable, but also very efficient.</p><p>In general, multiple object tracking consists of the temporal estimation of the kinematic state of each object, i.e. position and velocity. In computer vision, local descriptors are used to better discriminate between objects, e.g. person detectors/descriptors based on hand-crafted features <ref type="bibr" target="#b7">[8]</ref> or on deep neural networks <ref type="bibr" target="#b8">[9]</ref>. If the tracked objects emit sounds, their states can be inferred as well using soundsource localization techniques combined with tracking, e.g. <ref type="bibr" target="#b9">[10]</ref>. These techniques are often based on the estimation of the sound's direction of arrival (DOA) using a microphone array, e.g. <ref type="bibr" target="#b10">[11]</ref>, or on a steered beamformer <ref type="bibr" target="#b9">[10]</ref>. DOA estimation can be carried out either in the temporal domain <ref type="bibr" target="#b11">[12]</ref>, or in the spectral (Fourier) domain <ref type="bibr" target="#b12">[13]</ref>. However, spectral-domain DOA estimation methods are more robust than temporaldomain methods, in particular in the presence of background noise and reverberation <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The multiple sound-source localization and tracking method of <ref type="bibr" target="#b9">[10]</ref> combines a steered beamformer with a particle filter. The loudest sound source is detected first, the second loudest one is next detected, etc., and up to four sources. This leads to many false detections. Particle filtering is combined with source-to-track assignment probabilities in order to determine whether a newly detected source is a false detection, a source that is currently being tracked, or a new source. In practice, this method requires several empirically defined thresholds.</p><p>Via proper camera-microphone calibration, audio and visual observations can be aligned such that a DOA corresponds to a 2D location in the image plane. In this paper we adopt the audio-visual alignment method of <ref type="bibr" target="#b15">[16]</ref>, which learns a mapping from the space spanned by inter-channel spectral features (audio features) to the space of source locations, which in our case corresponds to the image plane. Interestingly, the method of <ref type="bibr" target="#b15">[16]</ref> estimates both this mapping and its inverse via a closed-form EM algorithm. Moreover, this allows us to exploit the richness of representing acoustic signals in the short-time Fourier domain <ref type="bibr" target="#b16">[17]</ref> and to extract noise-and reverberationfree audio features <ref type="bibr" target="#b13">[14]</ref>.</p><p>We propose to represent the audio-visual fusion problem via two sets of independent variables, i.e. visual-feature-to-person and audio-feature-to-person sets of assignment variables. An interesting characteristic of this way of doing is that the proposed tracking algorithm can choose to use visual features, audio features, or a combination of both, and this choice can be made independently for every person and for every time step. Indeed, audio and visual information are rarely available simultaneously and continuously. Visual information suffers from limited camera field-of-view, occlusions, false positives, missed detections, etc. Audio information is often corrupted by room acoustics, environmental noise and overlapping acoustic signals. In particular speech signals are sparse, non-stationary and are emitted intermittently, with silence intervals between speech utterances. Hence a robust audio-visual tracking must explicitly take into account the temporal sparsity of the two modalities and this is exactly what is proposed in this paper.</p><p>We use the AV16.3 <ref type="bibr" target="#b17">[18]</ref> and the AVDIAR <ref type="bibr" target="#b18">[19]</ref> datasets to evaluate the performance of the proposed audio-visual tracker. We use the Multiple Object Tracking (MOT) metrics and the Optimal Sub Pattern Assignment fo Tracks (OSPA-T) metrics to quantitatively assess method performance. MOT and in particular MOTA (tracking accuracy), which combines false positives, false negatives, identity switches, by comparing the estimated tracks with the ground-truth trajectories, is a commonly used score to assess the quality of a multiple person tracker. <ref type="foot" target="#foot_0">1</ref> OSPA-T measures the distance between two point sets and hence it is also useful to compare groundtruth tracks with estimated tracks in the context of multitarget tracking <ref type="bibr" target="#b19">[20]</ref>. We use MOT and OSPA-T metrics to compare our method with two recently proposed audio-visual tracking methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref> and with a visual tracker <ref type="bibr" target="#b7">[8]</ref>. An interesting outcome of the proposed method is that speaker diarization, i.e. who speaks when, can be coarsely inferred from the tracking output, thanks to the audio-feature-to-person assignment variables. The speaker diarization results obtained with our method are compared with two other methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref> based on the Diarization Error Rate (DER) score.</p><p>The remainder of the paper is organized as follows. Section II describes the related work. Section III describes in detail the proposed formulation. Section IV describes the proposed variational approximation and Section V details the variational expectation-maximization procedure. The algorithm implementation is described in Section VI. Tracking results and comparisons with other methods are reported in Section VII. Finally, Section VIII draws a few conclusions. <ref type="foot" target="#foot_1">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In computer vision, there is a long history of multiple object tracking methods. While these methods provide interesting insights concerning the problem at hand, a detailed account of existing visual trackers is beyond the scope of this paper. Several audio-visual tracking methods were proposed in the recent past, e.g. <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b21">[22]</ref>. These papers proposed to use approximate inference of the filtering distribution using Markov chain Monte Carlo particle filter sampling (MCMC-PF). These methods cannot provide estimates of the accuracy and merit of each modality with respect to each tracked person.</p><p>More recently, audio-visual trackers based on particle filtering and probability hypothesis density (PHD) filters were proposed, e.g. <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref>. <ref type="bibr" target="#b5">[6]</ref> used DOAs of audio sources to guide the propagation of particles, and combined the filter with a mean-shift algorithm to reduce the computational complexity. Some PHD filter variants were proposed to improve the tracking performance <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. The method of <ref type="bibr" target="#b3">[4]</ref> also used DOAs of active audio sources to give more importance to particles located around DOAs. Along the same line of thought, <ref type="bibr" target="#b6">[7]</ref> proposed a mean-shift sequential Monte Carlo PHD (SMC-PHD) algorithm that used audio information to improve the performance of a visual tracker. This implies that the persons being tracked must emit acoustic signals continuously and that multiple-source audio localization is reliable enough for proper audio-visual alignment.</p><p>PHD-based tracking methods are computationally efficient but their inherent limitation is that they are unable to associate observations to tracks. Hence they require an external postprocessing mechanism that provides associations. Also, in the case of PF-based audio-visual filtering, the number of tracked persons must be set in advance and sampling can be a computational burden. In contrast, the proposed variational formulation embeds association variables within the model, uses a birth process to estimate the initial number of persons and to add new ones along time, and an explicit dynamic model yields smooth trajectories.</p><p>Another limitation of the methods proposed in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref> is that they need as input a continuous flow of audio and visual observations. To some extent, this is also the case with <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, where only the audio observations are supposed to be continuous. All these methods showed good performance in the case of the AV16.3 dataset <ref type="bibr" target="#b17">[18]</ref> in which the participants spoke simultaneously and continuously -which is somehow artificial. The AV16.3 dataset was recorded in a specially equipped meeting room using three cameras that generally guarantee that frontal views of the participants were always available. This contrasts with the AVDIAR dataset which was recorded with one sensor unit composed of two cameras and six microphones. The AVDIAR scenarios are composed of participants that take speech turns while they look at each other, hence they speak intermittently and they do not always face the cameras.</p><p>Recently, we proposed an audio-visual clustering method <ref type="bibr" target="#b25">[26]</ref> and an audio-visual speaker diarization method <ref type="bibr" target="#b18">[19]</ref>. The weighted-data clustering method of <ref type="bibr" target="#b25">[26]</ref> analyzed a short time window composed of several audio and visual frames and hence it was assumed that the speakers were static within such temporal windows. Binaural audio features were mapped onto the image plane and were clustered with nearby visual features. There was no dynamic model that allowed to track speakers. The audio-visual diarization method <ref type="bibr" target="#b18">[19]</ref> used an external multi-object visual tracker that provided trajectories for each tracked person. The audio-feature-space to imageplane mapping <ref type="bibr" target="#b15">[16]</ref> was used to assign audio information to each tracked person at each time step. Diarization itself was modeled with a binary state variable (speaking or silent) associated with each person. The diarization transition probabilities (state dynamics) were hand crafted, with the assumption that the speaking status of a person was independent of all the other persons. Because of the small number of state configurations, i.e. {0, 1} N (where N is the maximum number of tracked persons), the MAP solution could be found by exhaustively searching the state space. In Section VII-I we use the AVDIAR recordings to compare our diarization results with the results obtained with <ref type="bibr" target="#b18">[19]</ref>.</p><p>The variational Bayesian inference method proposed in this paper may well be viewed as a multimodal generalization of variational expectation maximization algorithms for multiple object tracking using either visual-only information <ref type="bibr" target="#b7">[8]</ref> or audio-only information <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. We show that these models can be extended to deal with observations living in completely different mathematical spaces. Indeed, we show that two (or several) different data-processing pipelines can be embedded and treated on an equal footing in the proposed formulation. Special attention is given to audio-visual alignment and to audio-to-person assignments: (i) we learn a mapping from the space of audio features to the image plane, as well as the inverse of this mapping, which are integrated in the proposed generative approach, and (ii) we show that the an increase in the number of assignment variables, due to the use of two modalities, do not affect the complexity of the algorithm. Absence of observed data of any kind or erroneous data are carefully modeled: this enables the algorithm to deal with intermittent observations, whether audio, visual, or both. This is probably one of the most prominent features of the method, in contrast with most existing audio-visual tracking methods which require continuous and simultaneous flows of visual and audio data.</p><p>This paper is an extended version of <ref type="bibr" target="#b28">[29]</ref> and of <ref type="bibr" target="#b29">[30]</ref>. The probabilistic model and its variational approximation were briefly presented in <ref type="bibr" target="#b28">[29]</ref> together with preliminary results obtained with three AVDIAR sequences. Reverberation-free audio features were used in <ref type="bibr" target="#b29">[30]</ref> where it was shown that good performance could be obtained with these features when the audio mapping was trained in one room and tested in another room. With respect to these two papers. we provide detailed descriptions of the proposed formulation, of the variational expectation maximization solver and of the implemented algorithm. We explain in detail the birth process, which is crucial for track initialization and for detecting potentially new tracks at each time step. We experiment with the entire AVDIAR dataset and we several sequences from the AV16.3 dataset; we benchmark our method with the state-of-the-art multiplespeaker audio-visual tracking methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref> and with <ref type="bibr" target="#b7">[8]</ref>. Moreover, we show that our tracker can be used for audiovisual speaker diarization <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Mathematical Definitions and Notations</head><p>Unless otherwise specified, uppercase letters denote random variables while lowercase letters denote their realizations, e.g. p(X = x), where p(•) denotes either a probability density function (pdf) or a probability mass function (pmf). For the sake of conciseness we generally write p(x). Vectors are written in slanted bold, e.g. X, x, whereas matrices are written in bold, e.g. Y, y. Video and audio data are assumed to be synchronized, and let t denote the common frame index. Let N be the upper bound of the number of persons that can simultaneously be tracked at any time t, and let n ∈ {1 . . . N } be the person index. Let n = 0 denote nobody. A t subscript denotes variable concatenation at time t, e.g. X t = (X t1 , . . . , X tn , . . . , X tN ), and the subscript 1 : t denotes concatenation from 1 to t, e.g. X 1:t = (X 1 , . . . , X t ).</p><p>Let X tn ∈ X ⊂ R 2 , Y tn ∈ Y ⊂ R 2 and W tn ∈ W ⊂ R 2 be three latent variables that correspond to the 2D position, 2D velocity and 2D size (width and height) of person n at t, respectively. Typically, X tn and W tn are the center and size of a bounding box of a person while Y tn is the velocity of X tn . Let S t = {(X tn , W tn , Y tn ) } N n=1 ⊂ R 6 be the complete set of continuous latent variables at t, where denotes the transpose operator. Without loss of generality, in this paper a person is characterized with the bounding box of her/his head and the center of this bounding box is assumed to be the location of the corresponding speech source.</p><p>We now define the observations. At each time t there are M t visual observations and K t audio observations. Let f t = {f tm } Mt m=1 and g t = {g tk } Kt k=1 be realizations of the visual and audio observed random variables {F tm } Mt m=1 and {G tk } Kt k=1 , respectively. Visual observations, f tm = (v tm , u tm ) , correspond to the bounding boxes of detected faces, namely the concatenation of the bounding-box center, width and height, v tm ∈ V ⊂ R 4 , and of a feature vector u tm ∈ H ⊂ R d that describes the photometric content of that bounding box, i.e. a d-dimensional face descriptor (Section VII-D). Audio observations, g tk , correspond to interchannel spectral features, where k is a frequency sub-band index. Let's assume that there are K sub-bands, that K t ≤ K sub-bands are active at t, i.e. sub-bands with sufficient signal energy, and that there are J frequencies per sub-band. Hence, g tk ∈ R 2J corresponds to the real and imaginary parts of J complex-valued Fourier coefficients. It is well established that inter-channel spectral features {g tk } Kt k=1 contain audio-source localization information, which is what is needed for tracking. These audio features are obtained by applying the multichannel audio processing method described in Section VII-C below. Note that both the number of visual and of audio observations at t, M t and K t , vary over time. Let o 1:t = (o 1 , . . . , o t ) denote the set of observations from 1 to t, where o t = (f t , g t ).</p><p>Finally, we define the assignment variables of the proposed latent variable model. There is an assignment variable (a discrete random variable) associated with each observed variable. Namely, let A tm and B tk be associated with f tm and with g tk , respectively, e.g. p(A tm = n) denotes the probability of assigning visual observation m at t to person n. Note that p(A tm = 0) and p(B tk = 0) are the probabilities of assigning visual observation m and audio observation k to none of the persons, or to nobody. In the visual domain, this may correspond to a false detection while in the audio domain this may correspond to an audio signal that is not uttered by a person. There is an additional assignment variable, C tk that is associated with the audio generative model described in Section III-D. The assignment variables are jointly denoted with Z t = (A t , B t , C t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Filtering Distribution</head><p>We remind that the objective is to estimate the positions and velocities of participants (multiple person tracking) and, possibly, to estimate their speaking status (speaker diarization). The audio-visual multiple-person tracking problem is cast into the problems of estimating the filtering distribution p(s t , z t |o 1:t ) and of inferring the state variable S t . Subsequently, speaker diarization can be obtained from audio-feature-to-person information via the estimation of the assignment variables B tk (Section VI-C).</p><p>We reasonably assume that the state variable S t follows a first-order Markov model, and that the visual and audio observations only depend on S t and Z t . By applying Bayes rule, one can then write the filtering distribution of (s t , z t ) as:</p><formula xml:id="formula_0">p(s t , z t |o 1:t ) ∝ p(o t |s t , z t )p(z t |s t )p(s t |o 1:t-1 ),<label>(1)</label></formula><p>with:</p><formula xml:id="formula_1">p(o t |s t , z t ) = p(f t |s t , a t )p(g t |s t , b t , c t ),<label>(2)</label></formula><formula xml:id="formula_2">p(z t |s t ) = p(a t )p(b t )p(c t |s t , b t ),<label>(3)</label></formula><formula xml:id="formula_3">p(s t |o 1:t-1 ) = p(s t |s t-1 )p(s t-1 |o 1:t-1 )ds t-1 .<label>(4)</label></formula><p>Eq. ( <ref type="formula" target="#formula_1">2</ref>) is the joint (audio-visual) observed-data likelihood.</p><p>Visual and audio observations are assumed independent conditionally to S t , and their distributions will be detailed in Sections III-C and III-D, respectively. <ref type="foot" target="#foot_2">3</ref> Eq. ( <ref type="formula" target="#formula_2">3</ref>) is the prior distribution of the assignment variable. The observation-toperson assignments are assumed to be a priori independent so that the probabilities in (3) factorize as:</p><formula xml:id="formula_4">p(a t ) = Mt m=1 p(a tm ),<label>(5)</label></formula><formula xml:id="formula_5">p(b t ) = Kt k=1 p(b tk ),<label>(6)</label></formula><formula xml:id="formula_6">p(c t |s t , b t ) = Kt k=1 p(c tk |s tn , B tk = n).<label>(7)</label></formula><p>It makes sense to assume that these distributions do not depend on t and that they are uniform. The following notations are introduced:</p><formula xml:id="formula_7">η mn = p(A tm = n) = 1/(N + 1) and ρ kn = p(B tk = n) = 1/(N +1). The probability p(c tk |s tn , B tk = n) is discussed below (Section III-D).</formula><p>Eq. ( <ref type="formula" target="#formula_3">4</ref>) is the predictive distribution of s t given the past observations, i.e. from 1 to t -1. The state dynamics in (4) are modeled with a linear-Gaussian first-order Markov process. Moreover, it is assumed that the dynamics are independent over speakers:</p><formula xml:id="formula_8">p(s t |s t-1 ) = N n=1 N (s tn ; Ds t-1 n , Λ tn ),<label>(8)</label></formula><p>where Λ tn is the dynamics' covariance matrix and D is the state transition matrix, given by:</p><formula xml:id="formula_9">D =   I 4×4 I 2×2 0 2×2 0 2×4 I 2×2   .</formula><p>As described in Section IV below, an important feature of the proposed model is that the predictive distribution (4) at frame t is computed from the state dynamics model ( <ref type="formula" target="#formula_8">8</ref>) and an approximation of the filtering distribution p(s t-1 |o 1:t-1 ) at frame t -1, which also factorizes across speaker. As a result, the computation of (4) factorizes across speakers as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Visual Observation Model</head><p>As already mentioned above (Section III-A), a visual observation f tm consists of the center, width and height of a bounding box, namely v tm ∈ V ⊂ R 4 , as well as of a feature vector u tm ∈ H ⊂ R d describing the region inside the bounding box. Since the velocity is not observed, a 4 × 6 projection matrix P f = (I 4×4 0 4×2 ) is used to project s tn onto V. Assuming that the M t visual observations {f tm } Mt m=1 available at t are independent, and that the appearance representation of a person is independent of his/her position in the image, e.g. CNN-based embedding, the visual likelihood in ( <ref type="formula" target="#formula_1">2</ref>) is defined as:</p><formula xml:id="formula_10">p(f t |s t , a t ) = Mt m=1 p(v tm |s t , a tm )p(u tm |h, a tm ),<label>(9)</label></formula><p>where the observed bounding-box centers, widths, heights, and feature vectors are drawn from the following distributions:</p><formula xml:id="formula_11">p(v tm |s t , A tm = n) = N (v tm ; P f s tn , Φ tm ) if 1 ≤ n ≤ N U(v tm ; vol(V)) if n = 0,<label>(10)</label></formula><formula xml:id="formula_12">p(u tm |h, A tm = n) = B(u tm ; h n ) if 1 ≤ n ≤ N U(u tm ; vol(H)) if n = 0,<label>(11)</label></formula><p>where Φ tm ∈ R 4×4 is a covariance matrix quantifying the measurement error in the bounding-box center and size, U(•; vol(•)) is the uniform distribution with vol(•) being the volume of the support of the variable, B(•; h) is the Bhattacharya distribution <ref type="bibr" target="#b30">[31]</ref>, and h = (h 1 , . . . , h N ) ∈ R d×N is a set of prototype feature vectors that model the appearances of the N persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The Audio Observation Model</head><p>It is well established in the recent audio signal processing literature that inter-channel spectral features encode soundsource localization information <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Therefore, observed audio features, g t = {g tk } Kt k=1 are obtained by considering all the pairs of a microphone array. Audio observations depend neither on the size of the bounding box w t , nor on the velocity y t . Indeed, we note that the velocity of a sound source (a moving person) is of about 1 meter/second, which is negligible compared to the speed of sound. Moreover, the inter-microphone distance is small compared to the sourceto-microphone distance, hence the Doppler effect, if any, is similar across microphones. Hence one can replace s with x = P g s in the equations below, with P g = (I 2×2 0 2×4 ). By assuming independence across frequency sub-bands (indexed by k), the audio likelihood in (2) can be factorized as:</p><formula xml:id="formula_13">p(g t |s t , b t , c t ) = Kt k=1 p(g tk |x tb tk , b tk , c tk ). (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>While the inter-channel spectral features g tk contain localization information, in complex acoustic environments there is no explicit transformation that maps a source location onto an inter-channel spectral feature. We therefore make recourse to modeling this mapping via learning a non-linear regression.</p><p>We use the method of <ref type="bibr" target="#b13">[14]</ref> to extract audio features and the piecewise-linear regression model of <ref type="bibr" target="#b31">[32]</ref> to learn a mapping between the space of audio-source locations and the space of audio features. The method of <ref type="bibr" target="#b31">[32]</ref> belongs to the mixture of experts (MOE) class of models and hence it embeds well in our latent-variable mixture model. Let {h kr } r=R r=1 be a set of linear regressions, such that the r-th linear transformation h kr maps x ∈ R 2 onto g k ∈ R 2J for the frequency sub-band k. It follows that <ref type="bibr" target="#b11">(12)</ref> writes:</p><formula xml:id="formula_15">p(g tk |x tn , B tk = n, C tk = r) =<label>(13)</label></formula><formula xml:id="formula_16">N (g tk ; h kr (x tn ), Σ kr ) if 1 ≤ n ≤ N U(g tk ; vol(G)) if n = 0,</formula><p>where Σ kr ∈ R 2J×2J is a covariance matrix that captures the linear-mapping error and C tk is a discrete random variable, such that C tk = r means that the audio feature g tk is generated through the r-th linear transformation. Please consult Appendix A for details on how the parameters of the linear transformations h kr are learned from a training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. VARIATIONAL APPROXIMATION</head><p>Direct estimation of the filtering distribution p(s t , z t |o 1:t ) is computationally intractable. Consequently, evaluating expectations over this distribution is intractable as well. We overcome this problem via variational inference and associated EM closed-form solver <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. More precisely p(s t , z t |o 1:t ) is approximated with the following factorized form:</p><formula xml:id="formula_17">p(s t , z t |o 1:t ) ≈ q(s t , z t ) = q(s t )q(z t ),<label>(14)</label></formula><p>which implies</p><formula xml:id="formula_18">q(s t ) = N n=1 q(s tn ), q(z t ) = Mt m=1 q(a tm ) K k=1 q(b tk , c tk ),<label>(15)</label></formula><p>where q(A tm = n) and q(B tk = n, C tk = r) are the variational posterior probabilities of assigning visual observation m to person n and audio observation k to person n, respectively. The proposed variational approximation <ref type="bibr" target="#b13">(14)</ref> amounts to break the conditional dependence of S and Z with respect to o 1:t which causes the computational intractability. Note that the visual, A t , and audio, B t , C t , assignment variables are independent, that the assignment variables for each observation are also independent, and that B tk and C tk are conditionally dependent on the audio observation. This factorized approximation makes the calculation of p(s t , z t |o 1:t ) tractable. The optimal solution is given by an instance of the variational expectation maximization (VEM) algorithm <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, which alternates between two steps:</p><p>• Variational E-step: the approximate log-posterior distribution of each one of the latent variables is estimated by taking the expectation of the complete-data log-likelihood over the remaining latent variables, i.e. ( <ref type="formula" target="#formula_19">16</ref>), <ref type="bibr" target="#b16">(17)</ref>, and (18) below, and • M-step: model parameters are estimated by maximizing the variational expected complete-data log-likelihood. <ref type="foot" target="#foot_3">4</ref>In the case of the proposed model the latent variable logposteriors write:</p><formula xml:id="formula_19">log q(s tn ) = E q(zt) =n q(s t ) [log p(s t , z t |o 1:t )] + const, (<label>16</label></formula><formula xml:id="formula_20">) log q(a tm ) =<label>(17)</label></formula><formula xml:id="formula_21">E q(st) =m q(a t ) k q(b tk ,c tk ) [log p(s t , z t |o 1:t )] + const, log q(b tk , c tk ) =<label>(18)</label></formula><p>E q(st) m q(atm) =k q(b t ,c t ) [log p(s t , z t |o 1:t )] + const.</p><p>A remarkable consequence of the factorization ( <ref type="formula" target="#formula_17">14</ref>) is that p(s t-1 |o 1:t-1 ) is replaced with q(s t-1 ) = N n=1 q(s t-1 n ), consequently (4) becomes:</p><formula xml:id="formula_22">p(s t |o 1:t-1 ) ≈ p(s t |s t-1 ) N n=1 q(s t-1 n )ds t-1 .<label>(19)</label></formula><p>It is now assumed that the variational posterior distribution q(s t-1 n ) is Gaussian with mean µ t-1 n and covariance Γ t-1 n :</p><formula xml:id="formula_23">q(s t-1 n ) = N (s t-1 n ; µ t-1 n , Γ t-1 n ).<label>(20)</label></formula><p>By substituting ( <ref type="formula" target="#formula_23">20</ref>) into <ref type="bibr" target="#b18">(19)</ref> and combining it with (8), the predictive distribution <ref type="bibr" target="#b18">(19)</ref> becomes:</p><formula xml:id="formula_24">p(s t |o 1:t-1 ) ≈ N n=1 N (s tn ; Dµ t-1 n , DΓ t-1 n D + Λ tn ).<label>(21)</label></formula><p>Note that the above distribution factorizes across persons. Now that all the factors in (1) have tractable expressions, a VEM algorithm can be derived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. VARIATIONAL EXPECTATION MAXIMIZATION</head><p>The proposed VEM algorithm iterates between an E-S-step, an E-Z-step, and an M-step on the following grounds.</p><p>1) E-S-step: the per-person variational posterior distribution of the state vector q(s tn ) is evaluated by developing <ref type="bibr" target="#b15">(16)</ref>. The joint posterior p(s t , z t |o 1:t ) in ( <ref type="formula" target="#formula_19">16</ref>) is the product of ( <ref type="formula" target="#formula_1">2</ref>), ( <ref type="formula" target="#formula_2">3</ref>) and <ref type="bibr" target="#b20">(21)</ref>. We thus first sum the logarithms of (2), of (3) and of <ref type="bibr" target="#b20">(21)</ref>. Then we ignore the terms that do not involve s tn . Evaluation of the expectation over all the latent variables except s tn yields the following Gaussian distribution:</p><formula xml:id="formula_25">q(s tn ) = N (s tn ; µ tn , Γ tn ),<label>(22)</label></formula><p>with:</p><formula xml:id="formula_26">Γ tn = K k=1 R r=1 β tknr P g L kr Σ -1 kr L kr P g #1<label>(23)</label></formula><formula xml:id="formula_27">+ Mt m=1 α tmn P f Φ -1 tm P f #2 + Λ tn + DΓ t-1 n D -1 #3 -1</formula><p>, and with:</p><formula xml:id="formula_28">µ tn = Γ tn K k=1 R r=1 β tknr P g L kr Σ -1 kr (g kr -l kr ) #1<label>(24)</label></formula><formula xml:id="formula_29">+ Mt m=1 α tmn P f Φ -1 tm v tm #2 + Λ tn + DΓ t-1 n D -1 Dµ t-1 n #3</formula><p>, where α tmn = q(A tm = n) and β tknr = q(B tk = n, C tk = r) are computed in the E-Z-step below. A key point is that, because of the recursive nature of the formulas above, it is sufficient to make the Gaussian assumption at t = 1, i.e. q(s 1n ) = N (s 1n ; µ 1n , Γ 1n ), whose parameters may be easily initialized. It follows that q(s tn ) is Gaussian at every frame.</p><p>We note that both ( <ref type="formula" target="#formula_26">23</ref>) and ( <ref type="formula" target="#formula_28">24</ref>) are composed of three terms: the first (#1), second (#2) and third terms (#3) of ( <ref type="formula" target="#formula_26">23</ref>) and of (24) correspond to the audio, visual, and past cumulated information contributions to the precision matrix and the mean vector, respectively. Remind that the covariance Φ tm is associated with the visual observed variable in <ref type="bibr" target="#b9">(10)</ref>. Matrices L kr and vectors l kr characterize the piecewise affine mappings from the space of person locations to the space of audio features, i.e. Appendix A, and covariances Σ kr capture the errors that are associated with both audio measurements and the piecewise affine approximation in <ref type="bibr" target="#b12">(13)</ref>. A similar interpretation holds for the three terms of <ref type="bibr" target="#b23">(24)</ref>.</p><p>2) E-Z-step: by developing <ref type="bibr" target="#b16">(17)</ref>, along the same reasoning as above, we obtain the following closed-form expression for the variational posterior distribution of the visual assignment variable:</p><formula xml:id="formula_30">α tmn = q(A tm = n) = τ tmn η mn N i=0 τ tmi η mi ,<label>(25)</label></formula><p>where τ tmn is given by:</p><formula xml:id="formula_31">τ tmn =        N (v tm ; P f µ tn , Φ tm )e -1 2 tr P f Φ -1 tm P f Γtn × B(u tm ; h n ) if 1 ≤ n ≤ N U(v tm ; vol(V))U(u tm ; vol(H)) if n = 0.</formula><p>Similarly, for the variational posterior distribution of the audio assignment variables, developing (18) leads to:</p><formula xml:id="formula_32">β tknr = q(B tk = n, C tk = r) = κ tknr ρ kn π r N i=0 R j=1 κ tkij ρ ki π j ,<label>(26)</label></formula><p>where κ tknr is given by:</p><formula xml:id="formula_33">κ tknr = (27)        N (g tk ; L kr P g µ tn + l kr , Σ kr )e -1 2 tr P g L kr Σ -1 kr L kr PgΓtn × N (x tn ; ν r , Ω r )) if 1 ≤ n ≤ N U(g tk ; vol(G)) if n = 0.</formula><p>To obtain <ref type="bibr" target="#b26">(27)</ref>, an additional approximation is made. Indeed, the logarithm of (39) in Appendix A is part of the completedata log-likelihood and the denominator of this formula contains a weighted sum of Gaussian distributions. Taking the expectation of this term is not tractable because of the denominator. Based on the dynamical model ( <ref type="formula" target="#formula_8">8</ref>), we replace the state variable x tn in (39) with a "naive" estimate xtn predicted from the position and velocity inferred at t -1: xtn = x t-1 n + y t-1 n .</p><p>3) M-step: The entries of the covariance matrix of the state dynamics, Λ tn , are the only parameters that need be estimated. To this aim, we develop E q(st)q(zt) [log p(s t , z t |o 1:t )] and ignore the terms that do not depend on Λ tn . We obtain:</p><formula xml:id="formula_34">J(Λ tn ) = E q(stn) log N (s tn ; Dµ t-1 n , DΓ t-1 n D + Λ tn ) ,</formula><p>which can be further developed as:</p><formula xml:id="formula_35">J(Λ tn ) = log |DΓ t-1 n D + Λ tn | + Tr (DΓ t-1 n D + Λ tn ) -1 × (µ tn -Dµ t-1 n )(µ tn -Dµ t-1 n ) + Γ tn .<label>(28)</label></formula><p>Hence, by differentiating <ref type="bibr" target="#b27">(28)</ref> with respect to Λ tn and equating to zero, we obtain:</p><formula xml:id="formula_36">Λ tn = Γ tn -DΓ t-1 n D +(µ tn -Dµ t-1 n )(µ tn -Dµ t-1 n ) .<label>(29)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ALGORITHM IMPLEMENTATION</head><p>The VEM procedure above will be referred to as VAVIT which stands for variational audio-visual tracking, and pseudo-code is shown in Algorithm 1. In theory, the order in which the two expectation steps are executed is not important. In practice, the issue of initialization is crucial. In our case, it Algorithm 1:</p><p>Variational audio-visual tracking (VAVIT).</p><p>Input: visual observations f 1:t = {v 1:t , ξ 1:t }; audio observations g 1:t ; Output: Parameters of q(s 1:t ): {µ 1:t,n , Γ 1:t,n } N n=0 (the estimated position of each person n is given by the two first entries of µ is more convenient to start with the E-Z step rather than with the E-S step because the former is easier to initialize than the latter (see below). We start by explaining how the algorithm is initialized at t = 1 and then how the E-Z-step is initialized at each iteration. Next, we explain in detail the birth process. An interesting feature of the proposed method is that it allows to estimate who speaks when (i.e. perform speaker diarization) which is explained in detail at the end of the section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Initialization</head><p>At t = 1 one must provide initial values for the parameters of the distributions <ref type="bibr" target="#b21">(22)</ref>, namely µ 1n and Γ 1n for all n ∈ {1 . . . N }. These parameters are initialized as follows. The means are initialized at the image center and the covariances are given very large values, such that the variational distributions q(s 1n ) are non-informative. Once these parameters are initialized, they remain constant for a few frames, i.e. until the birth process is activated (see Section VI-B below).</p><p>As already mentioned, it is preferable to start with the E-Z-step than with the E-S-step because the initialization of the former is straightforward. Indeed, the E-S-step (Section V) requires current values for the posterior probabilities <ref type="bibr" target="#b24">(25)</ref> and <ref type="bibr" target="#b26">(27)</ref> which are estimated during the E-Z-step and which are both difficult to initialize. Conversely, the E-Z-step only requires current mean values, µ tn , which can be easily initialized by using the model dynamics <ref type="bibr" target="#b7">(8)</ref>, namely µ tn = Dµ t-1n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Birth Process</head><p>We now explain in detail the birth process, which is executed at the start of the tracking to initialize a latent variable for each detected person, as well as at any time t to detect new persons. The birth process considers B consecutive visual frames. At t, with t &gt; B, we consider the set of visual observations assigned to n = 0 from t -B to t, namely observations whose posteriors <ref type="bibr" target="#b24">(25)</ref> are maximized for n = 0 (at initialization all the observations are in this case). We then build observation sequences from this set, namely sequences of the form (ṽ m t-B , . . . , ṽmt ) ñ ∈ B, where m t indexes the set of observations at t assigned to n = 0 and ñ indexes the set B of all such sequences. Notice that the birth process only uses the bounding-box center, width and size, v, and that the descriptor u is not used. Hence the birth process is only based on the smoothness of an observed sequence of bounding boxes. Let's consider the marginal likelihood of a sequence ñ, namely:</p><formula xml:id="formula_37">L ñ = p((ṽ m t-B , . . . , ṽmt ) ñ)<label>(30)</label></formula><p>= • • • p(ṽ m t-B |s t-B ñ) . . . p(ṽ mt |s t ñ)</p><p>×p(s t ñ|s t-1 ñ) . . . p(s t-B+1 ñ|s t-B ñ)p(s t-B ñ)ds t-B:t ñ, where s t,ñ is the latent variable already defined and ñ indexes the set B. All the probability distributions in <ref type="bibr" target="#b29">(30)</ref> were already defined, namely ( <ref type="formula" target="#formula_8">8</ref>) and <ref type="bibr" target="#b9">(10)</ref>, with the exception of p(s t-B,ñ ). Without loss of generality, we can assume that the latter is a normal distribution centered at ṽmt and with a large covariance. Therefore, the evaluation of (30) yields a closedform expression for L ñ. A sequence ñ generated by a person is likely to be smooth and hence L ñ is high, while for a nonsmooth sequence the marginal likelihood is low. A newborn person is therefore created from a sequence of observations ñ if L ñ &gt; τ , where τ is a user-defined parameter. As just mentioned, the birth process is executed to initialize persons as well as along time to add new persons. In practice, in <ref type="bibr" target="#b29">(30)</ref> we set B = 3 and hence, from t = 1 to t = 4 all the observations are initially assigned to n = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Speaker Diarization</head><p>Speaker diarization consists of assigning temporal segment of speech to persons <ref type="bibr" target="#b34">[35]</ref>. We introduce a binary variable χ tn such that χ tn = 1 if person n speaks at time t and χ tn = 0 otherwise. Traditionally, speaker diarization is based on the following assumptions. First, it is assumed that speech signals are sparse in the time-frequency domain. Second, it is assumed that each time-frequency point in such a spectrogram corresponds to a single speech source. Therefore, the proposed speaker diarization method is based on assigning time-frequency points to persons.</p><p>In the case of the proposed model, speaker diarization can be coarsely inferred from frequency sub-bands in the following way. The posterior probability that the speech signal available in the frequency sub-band k at frame t was uttered by person n, given the audio observation g tk , is:</p><formula xml:id="formula_38">p(B tk = n|g tk ) = R r=1 p(B tk = n, C tk = r|g tk ),<label>(31)</label></formula><p>where B tk is the audio assignment variable and C tk is the affine-mapping assignment variable defined in Section III-D and in Appendix A. Using the variational approximation <ref type="bibr" target="#b25">(26)</ref>, this probability becomes:</p><formula xml:id="formula_39">p(B tk = n|g tk ) ≈ R r=1 q(B tk = n, C tk = r) = R r=1 β tknr ,<label>(32)</label></formula><p>and by accumulating probabilities over all the frequency subbands, we obtain the following formula:</p><formula xml:id="formula_40">χ tn = 1 if 1 Kt Kt k=1 R r=1 β tknr ≥ γ 0 otherwise, (<label>33</label></formula><formula xml:id="formula_41">)</formula><p>where γ is a user-defined threshold. Note that there is no dynamic model associated with diarization: χ tn is estimated independently at each frame and for each person. More sophisticated diarization models can be found in <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The AVDIAR Dataset</head><p>We used the AVDIAR<ref type="foot" target="#foot_4">foot_4</ref> dataset <ref type="bibr" target="#b18">[19]</ref> to evaluate the performance of the proposed audio-visual tracking method. This dataset is challenging in terms of audio-visual analysis. There are several participants involved in informal conversations while wandering around. They are in between two and four meters away from the audio-visual recording device. They take speech turns and often there are speech overlaps. They turn their faces away from the camera. The dataset is annotated as follows: The visual annotations comprise the centers, widths and heights of two bounding boxes for each person and in each video frame, a face bounding box and an upper-body bounding box. An identity (a number) is associated with each person through the entire dataset. The audio annotations comprise the speech status of each person over time (speaking or silent), with a minimum speech duration of 0.2 s. The audio source locations correspond to the centers of the face bounding boxes.</p><p>The dataset was recorded with a sensor composed of two cameras and six microphones, but only one camera is used in the experiments described below. The videos were recorded at 25 FPS. The frame resolution is of 1920 × 1200 pixels corresponding to a field of view of 97 • ×80 • . The microphone signals are sampled at 16 kHz. The dataset was recorded into two different rooms, living-room and meeting-room, e.g. Fig. <ref type="figure">3</ref> and Fig. <ref type="figure">4</ref>. These two rooms have quite different lighting conditions and acoustic properties (size, presence of furniture, background noise, etc.). Altogether there are 18 sequences associated with living-room (26927 video frames) and 6 sequences with meeting-room (6031 video frames). Additionally, there are two training datasets, T 1 and T 2 (one for each room) that contain input-output pairs of multichannel audio features and audio-source locations that allow to estimate the parameters (37) using the method of <ref type="bibr" target="#b15">[16]</ref>. This yields a mapping between source locations in the image plane, x, and audio features, g. Audio feature extraction is described in detail below.</p><p>One interesting characteristic of the proposed tracking is its flexibility in dealing only with visual data, only with audio data, or with visual and audio data. Moreover, the algorithm is able to automatically switch from unimodal (audio or visual) to multimodal (audio and visual). In order to quantitatively assess the performance and merits of each one of these variants we used two configurations:</p><p>• Full camera field of view (FFOV): The entire horizontal field of view of the camera, i.e. 1920 pixels, or 97 The PFOV configuration allows us to test scenarios in which a participant may leave the camera field of view and still be heard. Notice that since ground-truth annotations are available for the full field of view, it is possible to assess the performance of the tracker using audio observations only, as well as to analyse the behavior of the tracker when it switches from audio-only tracking to audio-visual tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The AV16.3 Dataset</head><p>We also used the twelve recordings of the AV16.3 dataset <ref type="bibr" target="#b17">[18]</ref> to evaluate the proposed method and to compare it with <ref type="bibr" target="#b3">[4]</ref> and with <ref type="bibr" target="#b6">[7]</ref>. The dataset was recorded in a meeting room. The videos were recorded at 25 FPS with three cameras fixed on the room ceiling. The image resolution is of 288 × 360 pixels. The audio signals were recorded with two eight-microphone circular arrays, both placed onto a table top, and sampled at 16 kHz. In addition, the dataset comes with internal camera calibration parameters, as well as with external calibration parameters, namely camera-tocamera and microphone-array-to-camera calibration parameters. We note that the scenarios associated with AV16.3 are somehow artificial in the sense that the participants speak simultaneously and continuously. This stays in contrast with the AVDIAR recordings where people take speech turns in informal conversations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Audio Features</head><p>In the case of AVDIAR, the STFT (short-time Fourier transform) <ref type="bibr" target="#b16">[17]</ref> is applied to each microphone signal using a 16 ms Hann window (256 audio samples per window) and with an 8 ms shift between successive windows (50% overlap), leading to 128 frequency bins and to 125 audio FPS. Inter-microphone spectral features are then computed using <ref type="bibr" target="#b14">[15]</ref>. These features -referred to in <ref type="bibr" target="#b14">[15]</ref> as directpath relative transfer function (DP-RTF) features -are robust against background noise and against reverberations, hence they do not depend on the acoustic properties of the recording room, as they encode the direct path from the audio source to the microphones. Nevertheless, they may depend on the orientation of the speaker's face. If the microphones are positioned behind a speaker, the direct-path sound wave (from the speaker to the microphones) propagates through the speaker's head, hence it is attenuated. This may have a negative impact on the direct-to-reverberation ratio. Here we assume that, altogether, this has a limited effect.</p><p>The audio features are averaged over five audio frames in order to be properly aligned with the video frames. The feature vector is then split into K = 16 sub-bands, each sub-band being composed of J = 8 frequencies; sub-bands with low energy are disregarded. This yields the set of audio observations at t, {g tk } Kt k=1 , K t ≤ K (see Section III-D and Appendix A). Interestingly, the computed inter-microphone DP-RTF features can be mapped onto the image plane and hence they can be used to estimate directions of arrival (DOAs). Please consult <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b31">[32]</ref> for more details.</p><p>Alternatively, one can compute DOAs explicitly from time differences of arrival (TDOAs) between the microphones of a microphone array, provided that the inter-microphone geometry is known. The disadvantage is that DOAs based on TDOAs assume free-field acoustic-wave propagation and hence they don't have a built-in reverberation model. Moreover, if the camera parameters are known and if the camera location (extrinsic parameters) is known in the coordinate frame of the microphone array, as is the case with the AV16.3 dataset, it is possible to project DOAs onto the image plane. We use the multiple-speaker DOA estimator of <ref type="bibr" target="#b36">[37]</ref> as it provides accurate results for the AV16.3 sensor setup <ref type="bibr" target="#b17">[18]</ref>. Let d tk be the line corresponding to the projection of a DOA onto the image plane and let x tn be the location of person n at time t. It is straightforward to determine the point xtk ∈ d tk the closest to x tn , e.g. Fig. <ref type="figure" target="#fig_0">2</ref>. Hence the inter-channel spectral features {g tk } Kt k=1 are replaced with {x tk } Kt k=1 and ( <ref type="formula" target="#formula_15">13</ref>) is replaced with:</p><formula xml:id="formula_42">p(x tk |x tn , B tk = n) =<label>(34)</label></formula><formula xml:id="formula_43">N (x tk ; x tn , σI) if 1 ≤ n ≤ N U(x tk ; vol(X )) if n = 0,</formula><p>where σI is an isotropic covariance that models the uncertainty of the DOA, e.g. Fig. <ref type="figure">5</ref>, third row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visual Features</head><p>In both AVDIAR and AV16.3 datasets participants do not always face the cameras and hence face detection is not robust. Instead we use the person detector of <ref type="bibr" target="#b37">[38]</ref> from which we infer a body bounding-box and a head bounding-box. We use the person re-identification CNN-based method <ref type="bibr" target="#b38">[39]</ref> to extract an embedding (i.e. a person descriptor) from the body boundingbox. This yields the feature vectors {u tm } Mt m=1 ⊂ R 2048 (Section III-C). Similarly, the center, width and height of the head bounding-box yield the observations {v tm } Mt m=1 ⊂ R 4 at each frame t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Evaluation Metrics</head><p>We used standard multi-object tracking (MOT) metrics <ref type="bibr" target="#b39">[40]</ref> to quantitatively evaluate the performance of the proposed tracking algorithm. The multi-object tracking accuracy (MOTA) is the most commonly used metric for MOT. It is a combination of false positives (FP), false negatives (FN; i.e. missed persons), and identity switches (IDs), and is defined as:</p><formula xml:id="formula_44">MOTA = 100 1 -t (FP t + FN t + IDs t ) t GT t ,<label>(35)</label></formula><p>where GT stands for the ground-truth person trajectories. After comparison with GT trajectories, each estimated trajectory can be classified as mostly tracked (MT) and mostly lost (ML) depending on whether a trajectory is covered by correct estimates more than 80% of the time (MT) or less than 20% of the time (ML). In the tables below, MT and ML indicated the percentage of ground-truth tracks under each situation.    In addition to MOT, we also used the OSPA-T metric <ref type="bibr" target="#b19">[20]</ref>. OSPA-T is based on a distance between two point sets and combines various aspects of tracking performance, such as timeliness, track accuracy, continuity, data associations and false tracks. It should be noted that OSPA-T involves a number of parameters whose values must be provided in advance. We used the publicly available code provided by one of the authors of <ref type="bibr" target="#b19">[20]</ref> for computing the OSPA-T scores in all the experimental evaluations reported below. <ref type="foot" target="#foot_5">6</ref>In our experiments, the threshold of overlap to consider that a ground truth is covered by an estimation is set to 0.1 intersection over union (IoU). In the PFOV configuration, we need to evaluate the audio-only tracking, i.e. the speakers are in the blind areas. As mentioned before, audio localization is less accurate than visual localization. Therefore, for evaluating the audio-only tracker we relax by a factor of two the expected localization accuracy with respect to the audio-visual localization accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Benchmarking with Baseline Methods</head><p>To quantitatively evaluate its performance, we benchmarked the proposed method with two state-of-the-art audio-visual tracking methods. The first one is the audio-assisted video adaptive particle filtering (AS-VA-PF) method of <ref type="bibr" target="#b3">[4]</ref>, and the second one is the sparse audio-visual mean-shift sequential Monte-Carlo probability hypothesis density (AV-MSSMC-PHD) method of <ref type="bibr" target="#b6">[7]</ref>. Notice that both these methods do not make recourse to a person detector as they use a trackingby-detection paradigm. This stays in contrast with our method which uses a person detector and probabilistically assigns each detection to each person. In principle, the baseline methods can be modified to accept person detection as visual information. However, we did not modify the baseline methods and used the software provided by the authors of <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b6">[7]</ref>. Sound locations are used to reshape the typical Gaussian noise distribution of particles in a propagation step, then <ref type="bibr" target="#b3">[4]</ref> uses the particles to weight the observation model. <ref type="bibr" target="#b6">[7]</ref> uses audio information to improve the performance and robustness of a visual SMC-PHD filter. Both <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b6">[7]</ref> require input from a multiple sound-source localization (SSL) algorithm. In the case of AVDIAR recordings, the multi-speaker localization method proposed in <ref type="bibr" target="#b14">[15]</ref> is used to provide input to <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b6">[7]</ref>. 7 In the case of AV16.3 recordings the method of <ref type="bibr" target="#b17">[18]</ref> is used to provide DOAs to <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref> and to our method, as explained in Section VII-C above.</p><p>We also compare the proposed method with a visual multiple-person tracker, more specifically the online Bayesian variational tracker (OBVT) of <ref type="bibr" target="#b7">[8]</ref>, which is based on a similar variational inference as the one presented in this paper. In <ref type="bibr" target="#b7">[8]</ref> visual observations were provided by color histograms. In our benchmark, for the sake of fairness, the proposed tracker and <ref type="bibr" target="#b7">[8]</ref> share the same visual observations, as described in Section VII-D.</p><p>The OSPA-T and MOT scores obtained with these methods as well as the proposed method are reported in Table <ref type="table" target="#tab_2">I</ref>, Table <ref type="table" target="#tab_3">II</ref>, Table <ref type="table" target="#tab_4">III</ref>, Table <ref type="table" target="#tab_5">IV</ref>, and Table <ref type="table" target="#tab_6">V</ref>. The symbols ↑ and ↓ indicate higher the better and lower the better, respectively. In the case of AVDIAR, we report results with both meetingroom and living-room in the two configurations: FFOV, Table <ref type="table" target="#tab_2">I</ref> and Table <ref type="table" target="#tab_3">II</ref> and PFOV, Table <ref type="table" target="#tab_4">III</ref> and Table IV. In the case of AV16.3 we report results with the twelve recordings commonly used by audio-visual tracking algorithms, Table <ref type="table" target="#tab_6">V</ref>.</p><p>The most informative metrics are OSPA-T and MOTA (MOT accuracy) and one can easily see that both <ref type="bibr" target="#b7">[8]</ref> and the proposed method outperform the other two methods. The poorer performance of both <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b6">[7]</ref> for all the configurations is generally explained by the fact that these two methods expect audio and visual observations to be simultaneously available. In particular, <ref type="bibr" target="#b3">[4]</ref> is not robust against visual occlusions, which leads to poor IDs (identity switches) scores.</p><p>The AV-MSSMC-PHD method <ref type="bibr" target="#b6">[7]</ref> uses audio information 7 The authors of <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b6">[7]</ref> kindly provided their software packages.</p><p>in order to count the number of speakers. In practice, we noticed that the algorithm behaves differently with the two datasets. In the case of AVDIAR, we noticed that the algorithm assigns several visible participants to the same audio source, since in most of the cases there is only one active audio source at a time. In the case of AV16.3 the algorithm performs much better, since participants speak simultaneously and continuously. This explains why both FN (false negatives) and IDs (identity switches) scores are high in the case of AVDIAR, i.e. Tables I, II, and III.</p><p>One can notice that in the case of FFOV, <ref type="bibr" target="#b7">[8]</ref> and the proposed method yield similar results in terms of OSPA-T and MOT scores: both methods exhibit low OSPA-T, FP, FN and IDs scores and, consequently, high MOTA scores. Moreover, they have very good MT and ML scores (out of 40 sequences 37 are mostly tracked, 3 are partially tracked, and none is mostly lost). As expected, the inferred trajectories are more accurate for visual tracking (whenever visual observations are available) than for audio-visual tracking: indeed, the latter fuses visual and audio observations which slightly degrades the accuracy because audio localization is less accurate than visual localization.</p><p>As for the PFOV configuration (Table <ref type="table" target="#tab_4">III</ref> and Table <ref type="table" target="#tab_5">IV</ref>), the proposed algorithm yields the best MOTA scores both for meeting-room and for living-room. Both <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b6">[7]</ref> have difficulties when visual information is not available: both these algorithms fail to track speakers when they walk outside the visual field of view. While <ref type="bibr" target="#b6">[7]</ref> can detect a speaker when it re-enters the visual field of view, <ref type="bibr" target="#b3">[4]</ref> cannot. Obviously, the visual-only tracker <ref type="bibr" target="#b7">[8]</ref> fails outside the camera field of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Audio-Visual Tracking Examples</head><p>We now provide and discuss results obtained with three AVDIAR recordings and one AV16.3 recording, namely the FFOV recording Seq13-4P-S2-M1 (Fig. <ref type="figure">3</ref>), the PFOV recordings Seq19-2P-S1M1 (Fig. <ref type="figure">4</ref>) and Seq22-1P-S0M1 (Fig. <ref type="figure">6</ref>), and the seq45-3p-1111 recording of AV16.3 (Fig. <ref type="figure">5</ref>). <ref type="foot" target="#foot_6">8</ref> All these recordings are challenging in terms of audio-visual tracking: participants are seated, then they stand up or they wander around. In the case of AVDIAR, some participants take speech turns and interrupt each other, while others remain silent.</p><p>The first rows of Fig. <ref type="figure">3</ref>, Fig. <ref type="figure">4</ref> and Fig. <ref type="figure">5</ref> show four frames sampled from two AVDIAR recordings and one AV16.3 recording, respectively. The second rows show ellipses of constant density that correspond to visual uncertainty (covariances). The third rows show the audio uncertainty. The audio uncertainties (covariances) are much larger than the visual ones since audio localization is less accurate than visual localization. The fourth rows shows the contribution of the dynamic model to the uncertainty, i.e. the inverse of the precision (#3) in eq. <ref type="bibr" target="#b22">(23)</ref>. Notice that these "dynamic" covariances are small, in comparison with the "observation" covariances. This ensures tracking continuity (smooth tracjectories) when audio or visual observations are either weak or  Both <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b7">[8]</ref> fail to track outside the camera field of view. In the case of the OBVT trajectory (c), there is an identity switch, from "red" (before the person leaves the visual field of view) to "blue" (after the person re-enters in the visual field of view).</p><p>totally absent. Fig. <ref type="figure">4</ref> shows a tracking example with a partial camera field of view (PFOV) configuration. In this case, audio and visual observations are barely available simultaneously. The independence of the visual and audio observation models and their fusion within the same dynamic model guarantees robust tracking in this case.</p><p>Fig. <ref type="figure">6</ref> shows the ground-truth trajectory of a person and the trajectories estimated with the audio-visual tracker <ref type="bibr" target="#b3">[4]</ref>, with the visual tracker <ref type="bibr" target="#b7">[8]</ref>, and with the proposed method. The ground-truth trajectory corresponds to a sequence of boundingbox centers. Both <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b7">[8]</ref> failed to estimate a correct trajectory. Indeed, <ref type="bibr" target="#b3">[4]</ref> requires simultaneous availability of audio-visual data while <ref type="bibr" target="#b7">[8]</ref> cannot track outside the visual field of view. Notice the non-smooth trajectory obtained with <ref type="bibr" target="#b3">[4]</ref> in comparison with the smooth trajectories obtained with variational inference, i.e. <ref type="bibr" target="#b7">[8]</ref> and proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Computation Times</head><p>Matlab implementations of algorithms <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> and VAVIT were run on an Intel(R) 8-core 2.40 GHz CPU E5-2609 equipped with 32 GB of RAM and with a GeForce GTX 1070 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Speaker Diarization Results</head><p>As already mentioned in Section VI-C, speaker diarization information can be extracted from the output of the proposed VAVIT algorithm. Notice that, while audio diarization is an extremely well investigated topic, audio-visual diarization has received much less attention. In <ref type="bibr" target="#b35">[36]</ref> it is proposed an audio-visual diarization method based on a dynamic Bayesian network that is applied to video conferencing. Their method assumes that participants take speech turns with a small silent interval between turns, which is an unrealistic hypothesis in the general case. The diarization method of <ref type="bibr" target="#b40">[41]</ref> requires audio, depth and RGB data. More recently, <ref type="bibr" target="#b18">[19]</ref> proposed a Bayesian dynamic model for audio-visual diarization that takes as input fused audio-visual information. Since diarization is not the main objective of this paper, we only compared our diarization results with <ref type="bibr" target="#b18">[19]</ref>, which achieves state of the art results, and with the diarization toolkit of <ref type="bibr" target="#b20">[21]</ref> which only considers audio information.</p><p>The diarization error rate (DER) is generally used as a quantitative measure. As is the case with MOT, DER combines false positives (FP), false negatives (FN) and identity swithches (IDs) scores within a single metric. The NIST-RT evaluation toolbox <ref type="foot" target="#foot_7">9</ref> is used. The results obtained with <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref> and with the proposed method are reported in Table <ref type="table" target="#tab_7">VII</ref>, for both the full field-of-view and partial field-of-view configurations (FFOV and PFOV). The proposed method performs better than the audio-only baseline method <ref type="bibr" target="#b20">[21]</ref>. In comparison with <ref type="bibr" target="#b18">[19]</ref>, the proposed method performs slightly less well despite the lack of a special-purpose diarization model. Indeed, <ref type="bibr" target="#b18">[19]</ref> implements diarization within a hidden Markov model (HMM) that takes into account both diarization dynamics and the audio activity observed at each time step, whereas our method is only based on observing the audio activity over time.</p><p>The ability of the proposed audio-visual tracker to perform diarization is illustrated in Fig. <ref type="figure">7</ref> and in Fig. <ref type="figure">8</ref> with a FFOV sequence (Seq13-4P-S2-M1, Fig. <ref type="figure">3</ref>) and with a PFOV sequence (Seq19-2P-S1M1, Fig. <ref type="figure">4</ref>), respectively. generative model introduced in Section III-D, i.e. equation <ref type="bibr" target="#b12">(13)</ref>. For that purpose we consider a training set of audio features, or inter-channel spectral features (which in practice correspond to the real and imaginary parts of complex-valued Fourier coefficients) and their associated source locations, T = {(g i , x i )} I i=1 and let (g, x) ∈ T . The vector g is the concatenation of K vectors g = [g 1 | . . . g k | . . . g K ] where [•|•] denotes vertical vector concatenation. We recall that for all sub-bands k; 1 ≤ k ≤ K, g k ∈ R 2J where J is the number of frequencies in each sub-band. Without loss of generality we consider the sub-band k. The joint probability of (g k , x) can be marginalized as:  </p><p>These parameters can be estimated via a closed-form EM procedure from a training dataset, e.g. T (please consult <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b28">[29]</ref> and Section VII-C).</p><p>One should notice that there is a parameter set for each sub-band k, 1 ≤ k ≤ K, hence there are K models that need be trained in our case. It follows that <ref type="bibr" target="#b11">(12)</ref>  The right-hand side of ( <ref type="formula" target="#formula_6">7</ref>) can now be written as:</p><formula xml:id="formula_46">p(C tk = r|x tn , B tk = n) = π r N (x tn ; ν r , Ω r ) R i=1 π i N (x tn ; ν i , Ω i )</formula><p>. <ref type="bibr" target="#b38">(39)</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: This figure displays two DOAs, associated with one microphone array (bottom left), projected onto the image plane, and illustrates the geometric relationship between a DOA and the current location of a speaker.</figDesc><graphic coords="10,97.38,56.72,154.23,123.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Fig.3: Four frames sampled from Seq13-4P-S2M1 (living room). First row: green digits denote speakers while red digits denote silent participants. Second, third and fourth rows: the ellipses visualize the visual, audio, and dynamic covariances, respectively, of each tracked person. The tracked persons are color-coded: green, yellow, blue, and red.</figDesc><graphic coords="12,57.07,413.79,118.23,73.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Fig. 5: Four frames sampled from seq45-3p-1111 of AV16.3. In this dataset, the participants speak simultaneously and continuously.</figDesc><graphic coords="13,56.72,425.77,115.67,102.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>Fig. 7: Diarization results obtained with Seq13-4P-S2M1 (FFOV). The first row shows the audio signal recorded with one of the microphones. The red boxes show the result of the voice activity detector which is applied to all the microphone signals prior to tracking. For each speaker, correct detections are shown in blue, missed detections are shown in green, and false positives are shown in magenta</figDesc><graphic coords="15,100.37,273.50,411.26,156.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>p(g k , x) = R r=1 p(g k |x, C k = r)p(x k |C k = r)p(C k = r).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 36 )</head><label>36</label><figDesc>Assuming Gaussian variables, we have p(g k |x, C k = r) = N (g k |h kr (x), Σ kr ), p(x|C k = r) = N (x|ν kr , Ω kr ), and p(C k = r) = π kr , where h kr (x) = L kr x + l kr with L kr ∈ R 2J×2 and l kr ∈ R 2J , Σ r ∈ R 2J×2J is the associated covariance matrix, and x is drawn from a Gaussian mixture model with R components, each component r being charac-terized by a prior π kr , a mean ν kr ∈ R 2 and a covariance Ω kr ∈ R 2×2 . The parameter set of this model for sub-band k is:Θ k = {L kr , l kr , Σ kr , ν kr , Ω kr , π kr } r=R r=1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>writes:p(g tk |x tn , B tk = n, C tk = r) = (38) N (g tk ; L kr x tn + l kr , Σ kr ) if 1 ≤ n ≤ N U(g tk ; vol(G)) if n = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="1,321.84,176.27,231.33,264.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>OSPA-T and MOT scores for the living-room sequences (full camera field of view)</figDesc><table><row><cell>Method</cell><cell>OSPA-T(↓)</cell><cell>MOTA(↑)</cell><cell>FP(↓)</cell><cell>FN(↓)</cell><cell>IDs(↓)</cell><cell>MT(↑)</cell><cell>ML(↓)</cell></row><row><cell>[4]</cell><cell>28.12</cell><cell>10.37</cell><cell>44.64 %</cell><cell>43.95%</cell><cell>732</cell><cell>20%</cell><cell>7.5 %</cell></row><row><cell>[7]</cell><cell>30.03</cell><cell>18.96</cell><cell>8.13 %</cell><cell>72.09%</cell><cell>581</cell><cell>17.5%</cell><cell>52.5%</cell></row><row><cell>[8]</cell><cell>14.79</cell><cell>96.32</cell><cell>1.77%</cell><cell>1.79%</cell><cell>80</cell><cell>92.5%</cell><cell>0%</cell></row><row><cell>VAVIT</cell><cell>17.05</cell><cell>96.03</cell><cell>1.85%</cell><cell>2.0%</cell><cell>86</cell><cell>92.5%</cell><cell>0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>OSPA-T and MOT scores for the meeting-room sequences (full camera field of view).</figDesc><table><row><cell>Method</cell><cell>OSPA-T (↓)</cell><cell>MOTA(↑)</cell><cell>FP(↓)</cell><cell>FN(↓)</cell><cell>IDs(↓)</cell><cell>MT(↑)</cell><cell>ML(↓)</cell></row><row><cell>[4]</cell><cell>5.76</cell><cell>62.43</cell><cell cols="2">18.63% 17.19%</cell><cell>297</cell><cell>70.59 %</cell><cell>0%</cell></row><row><cell>[7]</cell><cell>7.83</cell><cell>28.48</cell><cell>0.93%</cell><cell>69.68%</cell><cell>155</cell><cell>0 %</cell><cell>52.94%</cell></row><row><cell>[8]</cell><cell>3.02</cell><cell>98.50</cell><cell>0.25%</cell><cell>1.11%</cell><cell>25</cell><cell>100.00%</cell><cell>0%</cell></row><row><cell>VAVIT</cell><cell>3.57</cell><cell>98.16</cell><cell>0.38%</cell><cell>1.27%</cell><cell>32</cell><cell>100.00%</cell><cell>0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>OSPA-T and MOT scores for the living-room sequences (partial camera field of view).</figDesc><table><row><cell>Method</cell><cell>OSPA-T(↓)</cell><cell>MOTA(↑)</cell><cell>FP(↓)</cell><cell>FN(↓)</cell><cell>IDs(↓)</cell><cell>MT(↑)</cell><cell>ML(↓)</cell></row><row><cell>[4]</cell><cell>28.14</cell><cell>17.82</cell><cell>36.86%</cell><cell>42.88%</cell><cell>1722</cell><cell>32.50%</cell><cell>7.5%</cell></row><row><cell>[7]</cell><cell>29.73</cell><cell>20.61</cell><cell>5.54%</cell><cell>72.45%</cell><cell>989</cell><cell>12.5%</cell><cell>40%</cell></row><row><cell>[8]</cell><cell>22.25</cell><cell>66.39</cell><cell>0.48%</cell><cell>32.95%</cell><cell>129</cell><cell>45%</cell><cell>7.5%</cell></row><row><cell>VAVIT</cell><cell>21.77</cell><cell>69.62</cell><cell>8.97%</cell><cell>21.18%</cell><cell>152</cell><cell>70%</cell><cell>5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>OSPA-T and MOT scores for the meeting-room sequences (partial camera field of view).</figDesc><table><row><cell>Method</cell><cell>OSPA-T(↓)</cell><cell>MOTA(↑)</cell><cell>FP(↓)</cell><cell>FN(↓)</cell><cell>IDs(↓)</cell><cell>MT(↑)</cell><cell>ML(↓)</cell></row><row><cell>[4]</cell><cell>7.23</cell><cell>29.04</cell><cell cols="2">23.05% 45.19 %</cell><cell>461</cell><cell>29.41%</cell><cell>17.65%</cell></row><row><cell>[7]</cell><cell>8.17</cell><cell>26.95</cell><cell>1.05%</cell><cell>70.62%</cell><cell>234</cell><cell>5.88%</cell><cell>52.94%</cell></row><row><cell>[8]</cell><cell>5.80</cell><cell>64.24</cell><cell>0.43%</cell><cell>35.18%</cell><cell>24</cell><cell>36.84%</cell><cell>15.79%</cell></row><row><cell>VAVIT</cell><cell>5.81</cell><cell>65.27</cell><cell>5.07%</cell><cell>29.5%</cell><cell>26</cell><cell cols="2">47.37% 10.53%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>OSPA-T and MOT scores obtained with the AV16.3 dataset.</figDesc><table><row><cell>Method</cell><cell>OSPA-T (↓)</cell><cell>MOTA(↑)</cell><cell>FP(↓)</cell><cell>FN(↓)</cell><cell>IDs(↓)</cell><cell>MT(↑)</cell><cell>ML(↓)</cell></row><row><cell>[7]</cell><cell>17.28</cell><cell>36.4</cell><cell cols="2">16.72% 42.22%</cell><cell>765</cell><cell>11.11%</cell><cell>0%</cell></row><row><cell>[8]</cell><cell>13.32</cell><cell>82.9</cell><cell>5.29%</cell><cell>11.5 %</cell><cell>51</cell><cell>85.2%</cell><cell>0%</cell></row><row><cell>VAVIT</cell><cell>10.88</cell><cell>84.1</cell><cell>6.51%</cell><cell>9.18%</cell><cell>29</cell><cell>92.6%</cell><cell>0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI :</head><label>VI</label><figDesc>Computation times (in seconds). All four algorithms are implemented in Matlab and run on the same computer. The computation times provided in Table VI correspond to the total number of frames associated with all the sequences available in the two datasets. Both<ref type="bibr" target="#b7">[8]</ref> and VAVIT necessitate a person detector. The CNN-based person detector runs on the same computer at 2 FPS. Person detection is run offline.</figDesc><table><row><cell>Methods</cell><cell cols="3">AVDIAR: Living room AVDIAR: Meeting room AV16.3</cell></row><row><cell>Number of frames</cell><cell>26927</cell><cell>6031</cell><cell>11135</cell></row><row><cell>[4]</cell><cell>20821</cell><cell>2424</cell><cell>-</cell></row><row><cell>[7]</cell><cell>10510</cell><cell>2267</cell><cell>611</cell></row><row><cell>[8]</cell><cell>542</cell><cell>130</cell><cell>236</cell></row><row><cell>VAVIT</cell><cell>3456</cell><cell>759</cell><cell>260</cell></row><row><cell>GPU.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://motchallenge.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Supplemental materials are available at https://team.inria.fr/perception/ research/var-av-track/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We will see that Gt depends on Xt but depends neither on Wt nor on Yt, and Ft depends on Xt and Wt but not on Yt.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Even if the M-step is in closed-form, the inference is based on the variational posterior distributions. Therefore, the M-step could also be regarded as variational.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://team.inria.fr/perception/avdiar/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>http://ba-tuong.vo-au.com/codes.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>https://team.inria.fr/perception/research/variational av tracking/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>https://www.nist.gov/itl/iad/mig/rich-transcription-evaluation</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>We addressed the problem of tracking multiple speakers using audio and visual data. It is well known that the generalization of single-person tracking to multiple-person tracking is computationally intractable and a number of methods were proposed in the past. Among these methods, sampling methods based on particle filtering (PF) or on PHD filters have recently achieved the best tracking results. However, these methods have several drawbacks: (i) the quality of the approximation of the filtering distribution increases with the number of particles, which also increases the computational burden, (ii) the observation-to-person association problem is not explicitly modeled and a post-processing association mechanism must be invoked, and (iii) audio and visual observations must be available simultaneously and continuously. Some of these limitations were recently addressed both in <ref type="bibr" target="#b3">[4]</ref> and in <ref type="bibr" target="#b6">[7]</ref>, where audio observations were used to compensate the temporal absence of visual observations. Nevertheless, people speak with pauses and hence audio observations are rarely continuously available.</p><p>In contrast, we proposed a variational approximation of the filtering distribution and we derived a closed-form variational expectation-maximization algorithm. The observationto-person association problem is fully integrated in our model, rather than as a post-processing stage. The proposed VAVIT algorithm is able to deal with intermittent audio or visual observations, such that one modality can compensate the other modality, whenever one of them is noisy, too weak or totally missing. Using the OSPA-T and MOT scores we showed that the proposed method outperforms the PF-based method <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A AN AUDIO GENERATIVE MODEL</head><p>In this appendix we describe the audio observation model used in this paper. More precisely, we make explicit the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B DERIVATION OF THE E-S VARIATIONAL STEP</head><p>The E-S step for the per-person variational posterior distribution of the state vector q(s tn ) is evaluated by expanding <ref type="bibr" target="#b15">(16)</ref>, namely:</p><p>where constant terms are omitted. Using <ref type="bibr" target="#b19">(20)</ref> and after some algebraic derivations one obtains that q(s tn ) follows a Gaussian distribution, i.e. <ref type="bibr" target="#b21">(22)</ref>, where the covariance matrix and mean vector are given by ( <ref type="formula">23</ref>) and ( <ref type="formula">24</ref>), respectively. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Audiovisual probabilistic tracking of multiple speakers in meetings</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lathoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mccowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="601" to="616" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Structure inference for Bayesian multisensory scene understanding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2140" to="2157" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A multimodal approach to blind source separation of moving sources</title>
		<author>
			<persName><forename type="first">S</forename><surname>Naqvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="895" to="910" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Audio assisted robust visual tracking with adaptive particle filtering</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kılıc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">¸</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="186" to="200" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Information-driven active audio-visual source localization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Reineking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kluss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zetzsche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mean-shift and sparse sampling-based SMC-PHD filtering for audio informed visual speaker tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2417" to="2431" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mean-shift and sparse sampling-based SMC-PHD filtering for audio informed visual speaker tracking</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kılıc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">¸</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2417" to="2431" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An on-line variational Bayesian model for multi-person tracking from cluttered scenes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xompero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page" from="64" to="76" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Confidence-based data association and discriminative deep appearance learning for robust online multi-object tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="595" to="610" />
			<date type="published" when="2018-03">March 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust localization and tracking of simultaneous moving sound sources using beamforming and particle filtering</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rouat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="216" to="228" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TDOA estimation for multiple sound sources in noisy and reverberant environments using broadband independent component analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lombard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Buchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kellermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1490" to="1503" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A geometric approach to sound source localization from time-delay estimates</title>
		<author>
			<persName><forename type="first">X</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Alameda-Pineda</forename></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1082" to="1095" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tree-based recursive expectationmaximization algorithm for localization of acoustic sources</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dorfan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1692" to="1703" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimation of the directpath relative transfer function for supervised sound-source localization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2171" to="2186" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiple-speaker localization based on direct-path features and likelihood maximization with spatial sparsity regularization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gannot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1997" to="2012" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Co-localization of audio sources in images using binaural features and locally-linear regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="718" to="731" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Speech and audio signal processing: processing and perception of speech and music</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An audiovisual corpus for speaker localization and tracking</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lathoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Multimodal Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="182" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Audio-visual speaker diarization based on spatiotemporal Bayesian fusion</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1086" to="1099" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A metric for performance evaluation of multi-target tracking algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ristic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-T</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3452" to="3457" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">DiarTk: an open source toolkit for research in multistream speaker diarization and its application to meeting recordings</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vijayasenan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Valente</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="2170" to="2173" />
			<pubPlace>Portland, OR, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiple person and speaker activity tracking with a particle filter</title>
		<author>
			<persName><forename type="first">N</forename><surname>Checka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siracusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="881" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Particle flow SMC-PHD filter for audio-visual multi-speaker tracking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kilic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Latent Variable Analysis and Signal Separation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-zero diffusion particle flow SMC-PHD filter for audio-visual multi-speaker tracking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2018-04">April 2018</date>
			<biblScope unit="page" from="4304" to="4308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3D audio-visual speaker tracking with an adaptive particle filter</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>New-Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2896" to="2900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">EM algorithms for weighted-data clustering with application to audio-visual scene analysis</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2402" to="2415" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Online Localization and Tracking of Multiple Moving Speakers in Reverberant Environments</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="103" />
			<date type="published" when="2019-03">Mar. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tracking Multiple Audio Sources with the Von Mises Distribution and Variational EM</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Evers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="798" to="802" />
			<date type="published" when="2019-06">Jun. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploiting the complementarity of audio and visual data in multi-speaker tracking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV Workshop on Computer Vision for Audio-Visual Media</title>
		<meeting><address><addrLine>Venezia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="page" from="446" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Accounting for room acoustics in audio-visual multi-speaker tracking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Calgary, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04">Apr. 2018</date>
			<biblScope unit="page" from="6553" to="6557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On a measure of divergence between two statistical populations defined by their probability distributions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Calcutta Math. Soc</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="99" to="109" />
			<date type="published" when="1943">1943</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">High-dimensional regression with Gaussian mixtures and partially-latent response variables</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="893" to="911" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The Variational Bayes Method in Signal Processing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Smidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Quinn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Speaker diarization: A review of recent research</title>
		<author>
			<persName><forename type="first">X</forename><surname>Miro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bozonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fredouille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="356" to="370" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multimodal speaker diarization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Noulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Englebienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J A</forename><surname>Krose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="93" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A sector-based, frequency-domain approach to detection and localization of multiple speakers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lathoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Magimai-Doss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="265" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Person re-identification in the wild</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1367" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Mot16: A benchmark for multi-object tracking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multimodal multi-channel on-line speaker diarization using sensor fusion through SVM</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Minotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1694" to="1705" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
