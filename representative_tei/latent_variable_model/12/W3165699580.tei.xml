<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">City Research Online</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sergio</forename><surname>Naval Marimont</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cit-AI</orgName>
								<address>
									<settlement>City</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Giacomo</forename><surname>Tarroni</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cit-AI</orgName>
								<address>
									<settlement>City</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of London Institutional Repository</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">University of London</orgName>
								<address>
									<addrLine>2 BioMedIA</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">City Research Online</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1945-7928 doi: 10.1109/ISBI48211.2021.9433778</idno>
					</monogr>
					<idno type="DOI">10.1109/ISBI48211.2021.9433778</idno>
					<note type="submission">This version of the publication may differ from the final published version.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised anomaly detection</term>
					<term>outof-distribution</term>
					<term>VAE</term>
					<term>Vector Quantized-VAE</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an out-of-distribution detection method that combines density and restoration-based approaches using Vector-Quantized Variational Auto-Encoders (VQ-VAEs).</p><p>The VQ-VAE model learns to encode images in a categorical latent space. The prior distribution of latent codes is then modelled using an Auto-Regressive (AR) model. We found that the prior probability estimated by the AR model can be useful for unsupervised anomaly detection and enables the estimation of both sample and pixel-wise anomaly scores. The sample-wise score is defined as the negative loglikelihood of the latent variables above a threshold selecting highly unlikely codes. Additionally, out-of-distribution images are restored into in-distribution images by replacing unlikely latent codes with samples from the prior model and decoding to pixel space. The average L1 distance between generated restorations and original image is used as pixelwise anomaly score. We tested our approach on the MOOD challenge datasets, and report higher accuracies compared to a standard reconstruction-based approach with VAEs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>A wide range of methods using deep learning has been recently proposed to automatically identify anomalies in medical images <ref type="bibr" target="#b1">[1]</ref>. Most of them are based on supervised learning, and consequently have two important constraints. First, they require large and diverse annotated datasets for training. Second, they are specific to the abnormalities annotated in the datasets, and therefore are unable to generalize to other pathologies. Unsupervised anomaly detection methods, on the other hand, aim to overcome these constraints by not relying on annotated datasets <ref type="bibr" target="#b2">[2]</ref>. Instead, they focus on learning the underlying distribution of normal images and then identifying as anomalies the images that do not conform to the learnt distribution.</p><p>Recently, methods based on Variational Auto-Encoders (VAEs) have been proposed to identify and localize anomalies in medical images <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr">4]</ref>. VAEs are generative models trained by minimizing a loss function composed of a reconstruction term (measuring the distance between original images and reconstructions) and a Kullback-Leibler (KL) divergence term (measuring the distance between the latent distribution and a prior, generally assumed to be Gaussian). The default approach consists in using the reconstruction loss to identify samples with anomalies, based on the assumption that the VAE will reconstruct their anomaly-free versions. However, recent results suggest that the KL divergence is actually a better anomaly score <ref type="bibr" target="#b3">[3]</ref>. This can be caused by the high representational power of VAEs, which can reconstruct even (previously unseen) anomalies. In addition, in <ref type="bibr">[4]</ref> anomalies are modelled as spatially localized deviations from a prior distribution of normal images. Gradient descent in pixel space is used to "restore" images, effectively removing anomalies. Anomalies are then localized by comparing original images to restorations. Restoration-based approaches seem to overall outperform reconstruction-based ones <ref type="bibr" target="#b2">[2]</ref>.</p><p>Under the hypothesis that abnormal images are encoded in different, lower density regions in the latent space, we propose to use an estimated latent density as anomaly score. Vector-Quantized VAEs (VQ-VAEs) <ref type="bibr" target="#b5">[5]</ref> are well suited for this strategy because their discrete latent distribution can be modelled with expressive Auto-Regressive (AR) models, which provide state of the art performance in density estimation in images. Additionally, we enable anomaly localization relying on the generative capabilities of the AR model, with a method that we refer to as Latent Space Restoration. Results obtained in the MOOD challenge datasets (consisting of brain MR and abdominal CT images) suggest that our approach outperforms a standard reconstruction-based anomaly detection method using VAEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Vector Quantized Variational Auto-Encoders</head><p>Vector Quantized Variational Auto-Encoders (VQ-VAEs) <ref type="bibr" target="#b5">[5]</ref> encode observed variables in a discrete latent space instead of a continuous one. The discrete latent space can be very expressive, allowing the generation of high quality and detailed reconstructions. The discrete latent space also enables the pairing with AR models, which can independently learn the prior distribution (Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>VQ-VAEs are built around a dictionary that maps K discrete keys to a D-dimensional embedding space. In other words, the encoder network maps observed variables to the embedding space, ğ‘§ ğ‘’ (ğ‘¥) âˆˆ R ğ· . Differently from standard VAEs, for which the posterior ğ‘(ğ‘§|ğ‘¥) follows a Gaussian distribution, the posterior in VQ-VAEs is categorical and deterministic, and is defined as the index of the nearest embedding vector. ej, to the encoder output:</p><formula xml:id="formula_0">ğ‘(ğ‘§ = ğ‘˜|ğ‘¥) = { 1, for ğ‘˜ = argmin ğ‘— â€–ğ‘§ ğ‘’ (ğ‘¥) -ğ‘’ ğ‘— â€– 2 0, otherwise</formula><p>Finally, the decoder network takes as input ğ‘§ ğ‘ (ğ‘¥)~ ğ‘(ğ‘§|ğ‘¥) (i.e. the embedding in the dictionary nearest to the encoder output) and learns to reconstruct the observed variable distribution ğ‘(ğ‘¥|z ğ‘ (ğ‘¥)).</p><p>Network parameters for both encoder and decoder networks and embeddings are learnt using back-propagation. Given that the argmin operator is non-differentiable, the gradient in the encoder is usually approximated using straight-through estimator <ref type="bibr" target="#b5">[5]</ref>. Additional terms in the VQ-VAE loss function are introduced to provide gradients to the embeddings and to incentivize the encoder to commit to embeddings. The complete VQ-VAE loss function is therefore defined as</p><formula xml:id="formula_1">ğ¿ = log (ğ‘¥|ğ‘§ ğ‘ (ğ‘¥)) + â€–sg[z ğ‘’ (x)] -eâ€– 2 2 + â€–sg[ğ‘’] -z ğ‘’ (x)â€– 2 2</formula><p>where sg[.] represents the stop gradient operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Auto-Regressive prior modelling</head><p>In our method, the prior distribution of VQ-VAE is learnt using an Auto-Regressive model (AR). This will allow the estimation of the probability of samples and consequently the identification of anomalies (defined as samples associated to low probability). In addition, since AR models are generative, they enable the iterative sampling of one variable at a time, a property that we will leverage to generate multiple restorations. In an AR model, the joint probability is modelled using factorization, meaning that each variable is modelled as dependent from previous variables: ğ‘(ğ‘¥) = âˆ ğ‘(ğ‘¥ ğ‘– |ğ‘¥ 1 , â€¦ , ğ‘¥ ğ‘–-1 )</p><p>ğ‘ ğ‘–</p><p>. In our implementation, we used the PixelSNAIL <ref type="bibr" target="#b6">[6]</ref> architecture for the AR model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Sample-wise anomaly score estimation</head><p>A sample-wise anomaly score is a numerical indicator of how likely it is for a given sample to contain an anomaly. Scores are generally either density-based (based on an estimated probability of a sample) or reconstruction-based (based on the assumption that models trained on normal data will not be able to reconstruct anomalies).</p><p>Consistent with our previous findings using VAEs, VQ-VAE with a large enough latent space are able to reconstruct abnormal regions of images and this makes the full VQ-VAE loss a poor anomaly score. However, we found that abnormal regions translate into unusual latent variables, for which the AR model assigns low probability. Therefore, we derived a sample wise anomaly score from the prior probability estimated by the AR model.</p><p>A negative log-likelihood (NLL) threshold ğœ† ğ‘  defines highly unlikely latent variables. The proposed sample-wise anomaly score (ğ´ğ‘†sample) is the sum of NLL of the latent variables above threshold (over a total of ğ‘ variables):</p><formula xml:id="formula_2">ğ´ğ‘† ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ = âˆ‘ ğœ‰(ğ‘(ğ‘¥ ğ‘– )) ğ‘ ğ‘– ğœ‰(ğ‘§) = {</formula><p>-log(ğ‘§), if -log(ğ‘§) &gt; ğœ† ğ‘  0, otherwise</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Pixel-wise anomaly score estimation</head><p>Pixel-wise anomaly scores quantify, for each pixel in an image, its likelihood of containing an anomaly, consequently providing anomaly localization. The proposed pixel-wise score follows the restoration paradigm presented in <ref type="bibr">[4]</ref>. Our restoration method consists in replacing high loss latent variables with samples from the learnt prior AR model and keeping low loss latent variable unaltered. New samples are drawn only when their latent NLL is above a threshold Î»p. Fig. <ref type="figure" target="#fig_1">2</ref> illustrates this process (that we refer to as "Latent Space Restoration"). The restoration image is then generated with the decoder network, and the residual image is computed as |ğ‘‹ -Restoration|.</p><p>Multiple restorations (j âˆˆ 1,2,...,S) are generated for each test image to reduce variance in the anomaly estimation. The multiple residual images are consolidated using a weighting factor wj defined as:</p><formula xml:id="formula_3">ğ‘¤ ğ‘— = softmax(ğ‘˜ / âˆ‘ |Y ğ‘– -ğ‘‹ ğ‘— ğ‘– | ğ‘ƒ ğ‘–</formula><p>), where k is a softmax temperature parameter and the sum is over all the image pixels P. w reduces the weight of restorations which have lost consistency. The final consolidated pixelwise anomaly score (ğ´ğ‘†pixel) is estimated as the weighted mean of all residuals: Finally, ğ´ğ‘†pixel scores are smoothed using a 3x3 MinPooling filter followed by a 7x7 AveragePooling filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>MOOD challenge <ref type="bibr" target="#b7">[7]</ref> datasets have been used to train and evaluate the proposed method. They consist of</p><p>â€¢ Brain MR: 800 scans obtained from the Human Connectome Project (HCP) dataset <ref type="bibr">[8]</ref>. HCP incorporates only young healthy participants; â€¢ Abdominal CT: 550 normal scans from <ref type="bibr" target="#b9">[9]</ref>. The challenge test set is kept confidential. However, images from 4 subjects for each dataset with added synthetic anomalies were provided as validation set and used for hyperparameter tunning. Results listed in the following section correspond to this validation set.</p><p>Both datasets are pre-processed according to guidelines from challenge organizers. Images were resized to obtain axials slices of 160x160 pixels. Brain images were normalized to have zero mean and unit standard deviation subject-wise. Image augmentation used in the training set included elastic transforms, gaussian blur, random scale and rotations, random brightness, contrast adjustment and gaussian noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation details</head><p>Our VQ-VAE includes 5 blocks, each composed of 4 residual blocks and a downsample/upsample operator. The latent space in the brain dataset was set to 20x20 with an embedding space with 128 keys and 256 dimensions. The latent space in the abdominal dataset was set to 10x10. L1 distance was used as reconstruction loss. Additionally, dropout with 0.1 probability is used during training.</p><p>PixelSNAIL network consists of 4 blocks, each with 4 residual blocks and a self-attention module. Latent probability distribution was conditioned on the axial slice position. Consequently, in order to estimate a sample probability, the AR model receives as input not only the encoded sample but also the position of the slice within the volume, encoded in a variable in the range [-0.5,0.5]. Dropout with 0.1 probability is used during training.</p><p>Network architectures and training procedures were implemented in PyTorch and made openly available in <ref type="url" target="https://github.com/snavalm/lsr_mood_challenge_2020/">https://github.com/snavalm/lsr_mood_challenge_2020/</ref>.</p><p>Adam optimizer (with parameters Î²1=0.9, Î²2=0.999, Îµ=10 -8 ) and learning rate of 10 -4 were used to train both VQ-VAE and PixelSNAIL networks. A batch size of 64 was used in both networks. Batches were created by combining 8 random slices from 8 volumes. VQ-VAE was first trained. The trained encoder was then used to generate the latent variables fed to PixelSNAIL, which was consequently trained. Networks were trained on a single Nvidia GTX1070.</p><p>Finally, Î» ğ‘  and Î» ğ‘ thresholds were adjusted using the validation set provided (slice-wise performance was used for Î»s). We used Î» ğ‘  = 7 and Î» ğ‘ = 5 , corresponding to percentiles 98 and 90 respectively in the validation set. In the pixel-wise score, we found that a lower threshold incentivizes more variance in reconstructions which improved results. ğ‘† = 15 restorations was also heuristically determined.</p><p>We compare our method to a standard VAE with the same architecture as the VQ-VAE (5 downsample/upsample blocks, each with 4 residual blocks). A dense layer is incorporated as the final layer of the encoder to define a 128 latent space. VAE loss is used as sample-wise AS and reconstruction as pixel-wise AS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS AND DISCUSSION</head><p>Since only 4 volumes are provided for each dataset, we approximated the sample-wise performance using slice-wise performance metrics. Slice and pixel-wise results for our method are summarized in Table <ref type="table" target="#tab_0">1</ref>. Area under receiver operating characteristics curve (AUROC) and average precision (AP) are reported for sample and pixel wise scores. In pixel-wise score we additionally evaluated the Dice similarity coefficient (DSC) by identifying abnormal pixels with an AS threshold. We include examples of the samplewise scores assigned by our method in Fig. <ref type="figure">3</ref>. For pixel-wise scores, Fig. <ref type="figure">4</ref> shows one validation image, 2 of the 15 restorations generated, residuals and final anomaly score.  The obtained results suggest that our approach outperforms a standard VAE method. Pixel-wise results are superior in brain images compared to abdominal (probably due to the higher variance in the abdominal dataset). We also observed that the method is sensitive to the pixel intensity of the anomaly. Anomalies with intensities near the expected intensities are often missed. This can be due to the anomaly scored being calculated as the residual of pixel intensities. Alternative scores will be evaluated in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We presented a novel unsupervised anomaly detection and localization method based on VQ-VAEs that improves results upon an existing standard VAE approach. In the MOOD challenge, our approach achieved 2 nd and 3 rd position in sample and pixel-wise respectively, only surpassed by non-VAE-based methods. In the future, we intend to evaluate our approach in a broader range of datasets and medical anomalies to better assess its robustness and usefulness in a realistic scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">COMPLIANCE WITH ETHICAL STANDARDS</head><p>This research study was conducted retrospectively using human subject data made available in open access by <ref type="bibr" target="#b9">[9]</ref>. Ethical approval was not required as confirmed by the license attached to the open access data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Diagram of the proposed approach.</figDesc><graphic coords="4,52.60,74.50,244.40,145.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of Latent Space Restoration process.</figDesc><graphic coords="5,59.30,74.55,491.15,111.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparative of slice and pixel-wise performance.</figDesc><table><row><cell></cell><cell cols="2">ğ´ğ‘†sample</cell><cell></cell><cell>ğ´ğ‘†pixel</cell><cell></cell></row><row><cell></cell><cell cols="2">AUROC AP</cell><cell cols="3">DSC AUROC AP</cell></row><row><cell>Brain dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VQ-VAE (ours)</cell><cell>0.97</cell><cell>0.92</cell><cell>0.79</cell><cell>0.99</cell><cell>0.81</cell></row><row><cell>VAE</cell><cell>0.90</cell><cell>0.82</cell><cell>0.70</cell><cell>0.98</cell><cell>0.72</cell></row><row><cell>Abdominal dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VQ-VAE (ours)</cell><cell>0.83</cell><cell>0.73</cell><cell>0.57</cell><cell>0.98</cell><cell>0.57</cell></row><row><cell>VAE</cell><cell>0.65</cell><cell>0.48</cell><cell>0.29</cell><cell>0.93</cell><cell>0.23</cell></row></table></figure>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGEMENTS</head><p>The authors declare no conflicts of interest.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Survey on Deep Learning in Medical Image Analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Autoencoders for Unsupervised Anomaly Segmentation in Brain MR Images: A Comparative Study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Denner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno>CoRR, abs/2004.03271</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised Anomaly Localization using Variational Auto-Encoders</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zimmerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention -MICCAI</title>
		<imprint>
			<biblScope unit="volume">11767</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised Lesion Detection via Image Restoration with a Normative Prior</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tezcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<idno>PMLR 102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 2 nd International Conference on Medical Imaging with Deep Learning</title>
		<meeting>The 2 nd International Conference on Medical Imaging with Deep Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="540" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural Discrete Representation Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;17: Proceedings of the 31 st International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6309" to="6318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PixelSNAIL: An Improved Autoregressive Generative Model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>PMLR 80</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35 th International Conference on Machine Learning</title>
		<meeting>the 35 th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="864" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Medical Out-of-Distribution Analysis Challenge</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zimmerer</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3715870</idno>
	</analytic>
	<monogr>
		<title level="j">Zenodo</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Human Connectome Project: a data acquisition perspective</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data From CT COLONOGRAPHY</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Cancer Imaging Arch.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><surname>Fig</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Test images and corresponding sample-wise anomaly scores. Abnormal images are highlighted in red. Fig. 3. Visualization of restorations and pixel-wise score</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
