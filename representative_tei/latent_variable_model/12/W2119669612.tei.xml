<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Multisensory Integration and Coordinate Transformation via Density Estimation</title>
				<funder ref="#_pc2Y8HS">
					<orgName type="full">NIH NEI</orgName>
				</funder>
				<funder ref="#_84Egg45">
					<orgName type="full">Reorganization and Plasticity to Accelerate Injury Recovery</orgName>
				</funder>
				<funder>
					<orgName type="full">Salukhutdinov and Hinton</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2013-04-18">April 18, 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Joseph</forename><forename type="middle">G</forename><surname>Makin</surname></persName>
							<email>makin@phy.ucsf.edu</email>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Fellows</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><forename type="middle">N</forename><surname>Sabes</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Physiology and the Center for Integrative Neuroscience</orgName>
								<orgName type="institution">University of California San Francisco</orgName>
								<address>
									<addrLine>San Francisco</addrLine>
									<region>California</region>
									<country key="US">United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Queen&apos;s University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Multisensory Integration and Coordinate Transformation via Density Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-04-18">April 18, 2013</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1371/journal.pcbi.1003035</idno>
					<note type="submission">Received November 5, 2012; Accepted March 3, 2013;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sensory processing in the brain includes three key operations: multisensory integration-the task of combining cues into a single estimate of a common underlying stimulus; coordinate transformations-the change of reference frame for a stimulus (e.g., retinotopic to body-centered) effected through knowledge about an intervening variable (e.g., gaze position); and the incorporation of prior information. Statistically optimal sensory processing requires that each of these operations maintains the correct posterior distribution over the stimulus. Elements of this optimality have been demonstrated in many behavioral contexts in humans and other animals, suggesting that the neural computations are indeed optimal. That the relationships between sensory modalities are complex and plastic further suggests that these computations are learnedbut how? We provide a principled answer, by treating the acquisition of these mappings as a case of density estimation, a well-studied problem in machine learning and statistics, in which the distribution of observed data is modeled in terms of a set of fixed parameters and a set of latent variables. In our case, the observed data are unisensory-population activities, the fixed parameters are synaptic connections, and the latent variables are multisensory-population activities. In particular, we train a restricted Boltzmann machine with the biologically plausible contrastive-divergence rule to learn a range of neural computations not previously demonstrated under a single approach: optimal integration; encoding of priors; hierarchical integration of cues; learning when not to integrate; and coordinate transformation. The model makes testable predictions about the nature of multisensory representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The brain often receives information about the same feature of the same object from multiple sources; e.g., in a visually guided reach, both vision and proprioception provide information about hand location. Were both signals infinitely precise, one could simply be ignored; but fidelity is limited by irrelevant inputs, intrinsic neural noise, and the spatial precisions of the transducers, so there are better and worse ways to use them. The best will not throw away any information-in Bayesian terms, the posterior probability over the stimulus given the activities of the integrating neurons will match the corresponding posterior given the input signals. Encoding in the integrating neurons the entire posterior for each stimulus, and not merely the best point estimate, is crucial because this distribution contains information about the confidence of the estimate, which is required for optimal computation with the stimulus estimate <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. A sensible code will also ''compress'' the information-for example, by representing it in fewer neurons-otherwise the brain could simply propagate forward independent copies of each sensory signal.</p><p>Psychophysical evidence suggests that animals-and therefore their brains-are indeed integrating multisensory inputs in such an ''optimal'' manner. Human subjects appear to choose actions based on the peak of the optimal posterior over the stimulus, given a variety of multisensory inputs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. Prism and virtualfeedback adapation experiments <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> have demonstrated the plasticity of these multisensory mappings, and it is not likely limited to recalibration: Deprivation studies <ref type="bibr" target="#b12">[13]</ref>; afferent rerouting experiments <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>; the ability to learn novel, cross-modal mappings; and genetic-information constraints together suggest that integration is learned, with the organization of association cortices driven by sensory data.</p><p>A plausible neural model of multisensory integration, then, must learn without supervision how to combine optimally signals from two or more input populations as well as a priori information, encoding both the most likely estimate and certainty about iteven when the relationship between the signal spaces is nonlinear (like retinotopic and proprioceptive-encoded hand location), and when their relationship is mediated by another variable (like gaze angle). Existing computational models of multisensory integration or cross-modal transformation neglect one or more of these desiderata (see Discussion).</p><p>Here we show that the task of integration can be reformulated as latent-variable density estimation, a problem from statistics that can be implemented by a neural network, and the foregoing requirements thereby satisfied. The goal is to learn a data distribution (here, the activities of populations of visual and somatosensory neurons while they report hand location in their respective spaces) in terms of a set of parameters (synaptic strengths) and a set of unobserved variables (downstream, integrating neurons). In particular, we model the cortical association area with a restricted Boltzmann machine (RBM), an undirected generative model trained with a fast, effective Hebbian learning rule, contrastive divergence <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. By making the machine a good model of the distribution of the training data, learning obliges the downstream units to represent their common underlying causes-here, hand location. The same formulation turns out to be equally suited to coordinate transformation as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>A network that has learned to perform the integration task will transmit to downstream neurons (v), on each trial, all the information in its inputs (r) about the stimulus (s). In our case, that network is the RBM, the stimulus is the location of the hand, and the inputs are two neural populations (visual and proprioceptive) encoding hand location in different spaces (Fig. <ref type="figure" target="#fig_0">1A</ref>; see also Methods). Equivalently, integration requires that the posterior distribution over the stimulus given the activities of the downstream (''hidden'' or ''multisensory'') units, q(sDv), match the posterior over the stimulus given the two inputs, p(sDr). Henceforth, we call the latter of these distributions the optimal posterior, since it serves as the benchmark for performance. Having arranged, by our choice of input-population encoding, for the optimal posterior to be Gaussian (see Methods), its statistics consist only of a mean and a covariance. Thus to show that the network successfully integrates its inputs, we need show only that these two cumulants can be recovered from the multisensory neurons-intuitively, that they have learned to encode the optimal stimulus location and confidence in that location, respectively. We emphasize that throwing away covariance (or other statistical information) would render subsequent computations suboptimal: for example, if the integrated estimate is itself to be integrated downstream with another modality, it must be weighted by its own precision, i.e. inverse covariance (see Text S1 and Hierarchical networks below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multisensory integration in the RBM</head><p>We begin by examining the ability of our model to perform optimal multisensory integration, in the sense just described. We use our ''standard'' network, with a visible layer of 1,800 Poisson units, comprising two 30630 input populations, and a hidden layer of half that number of Bernoulli units. We trained and tested this network on separate datasets, with stimuli chosen uniformly in the 2D space of joint angles (see Methods and Fig. <ref type="figure" target="#fig_0">1B</ref>).</p><p>Decoding the posterior mean. We first show that the hidden layer successfully encodes the optimal-posterior mean. For a fixed stimulus location, s, we compare the distribution of the stimulus decoded from 15 samples of the hidden units, ŝ s RBM (v) (''RBM-based estimate'', see Methods), with the distribution of the optimal-posterior mean, ŝ s MAP (r). (The latter estimate also has a distribution across trials, even for a fixed stimulus, because the input encodings are noisy.) We compare the distributions of these two estimates, rather than simply examining the distribution of their difference, because the resulting figures (Fig. <ref type="figure">2A</ref>) then resemble those typically presented in psychophysical studies, where behavior plays the role of the estimate-and indeed, has been found to correspond to the optimal-posterior mean <ref type="bibr" target="#b0">[1]</ref>.</p><p>Fig. <ref type="figure">2A</ref> shows the mean and covariance of the conditional estimator distributions, p(ŝ sDs), for various stimulus locations s, and for four separate estimates of the posterior mean: the MAP estimate using the visual-population activities (magenta), the MAP estimate using the proprioceptive-population activities (orange), the MAP estimate using both input populations (the ''optimal'' posterior mean, black), and the estimate using the hidden-layer activities (''RBMbased integrated estimate,'' green). Each ellipse depicts the 95% confidence interval of the distribution's covariance, centered at its mean, as in all subsequent figures. Clearly, the RBM-based estimate matches the MAP estimate over nearly all of the workspace. Visible errors occur only at the edges of joint space, probably a result of the ''edge effects,'' i.e., the proximity of extreme joint angles to regions of space not covered by the (perforce finite) grid of neurons.</p><p>We can quantify the contribution of these imperfections to the total optimality of the model. Since the MAP estimate is the unique minimizer of the average (over all stimuli) mean square error, the marginal error distribution, p(ŝ s{s)~Ð s p(s)p(ŝ s{sDs), summarizes all the conditional estimator distributions. These marginal error statistics (Fig. <ref type="figure">2B</ref> stdmargstats) show that the overall performance of the network is very nearly optimal.</p><p>Decoding the posterior covariance. We next show that the hidden layer also encodes the optimal-posterior covariance. The posterior covariance represents the uncertainty on a single trial about the true stimulus location, given the specific spike counts on this trial. Since on a single trial, only one point from the posterior distribution (presumably the mean) manifests itself in a behaviore.g., a reach-, that trial's posterior covariance cannot be read off the behavior as the posterior mean can. Nevertheless, the posterior covariance has important behavioral consequences across trials: it determines the relative weighting of each input during optimal integration (see Eq. 3b in Methods). This is clearly a requirement for the input populations, vis and prop; but if, for example, the multisensory (hidden-unit) estimate, ŝ s(v), is itself to be integrated with yet another sensory population at a further stage of processing, optimality of that integration requires knowledge of the posterior covariance, in order to weight ŝ s(v) properly. We show in Hierarchical networks below that the model can learn just such an architecture, demonstrating that posterior covariance information is indeed encoded in the hidden units; but here we exhibit the result more directly.</p><p>The posterior precision (inverse covariance) on each trial is a 2|2 symmetric matrix and therefore ostensibly has three degrees</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Summary</head><p>Over the first few years of their lives, humans (and other animals) appear to learn how to combine signals from multiple sense modalities: when to ''integrate'' them into a single percept, as with visual and proprioceptive information about one's body; when not to integrate them (e.g., when looking somewhere else); how they vary over longer time scales (e.g., where in physical space my hand tends to be); as well as more complicated manipulations, like subtracting gaze angle from the visually-perceived position of an object to compute the position of that object with respect to the head-i.e., ''coordinate transformation.'' Learning which sensory signals to integrate, or which to manipulate in other ways, does not appear to require an additional supervisory signal; we learn to do so, rather, based on structure in the sensory signals themselves. We present a biologically plausible artificial neural network that learns all of the above in just this way, but by training it for a much more general statistical task: ''density estimation''-essentially, learning to be able to reproduce the data on which it was trained. This also links coordinate transformation and multisensory integration to other cortical operations, especially in early sensory areas, that have have been modeled as density estimators. World-driven data are generated according to the directed graphical model boxed in the lower right: On each trial, a hand location s and the population gains g x and g h for the two sensory modalities are drawn from their respective prior distributions. Given these, a spike count is drawn for each neuron (magenta and orange colored circles) from a Poisson distribution (see Eq. 2), yielding (e.g.) the set of firing rates shown by the heat maps at left. The center of mass of each population is marked with an x. The visual (magenta) and proprioceptive (orange) neural populations each encode the location of the hand, but in different spaces: 2D Cartesian space and joint space, respectively, drawn in outline in the heat maps. Since the neurons' preferred stimuli uniformly tile their respective spaces (indicated by the grids), but the forward kinematics relating these variables is nonlinear (inset; joint limits are indicated with red shading, joint origins with black lines), hand position is encoded differently by the two populations. These population codes also constitute the input layer, R, of the of freedom. However, as shown below in Eq. 3a (Methods), the encoding scheme constrains it to a lower-dimensional manifold: the only quantities that change from trial to trial are the ''total spike counts,'' P i r x i ~: g x , P i r h i ~: g h , and the location where the Jacobian of the forward kinematics is evaluated. The latter is given by the posterior mean, which we have just shown can be reconstructed nearly optimally. Therefore, reconstruction of the posterior precision requires the additional recovery only of the total spike counts of the respective input populations.</p><p>Fig. <ref type="figure">3A</ref> shows the coefficients of determination (R 2 ) for two different estimators of the total spike counts, one using 15 samples from the hidden-layer units (as for the posterior mean above), and the other using hidden-layer means (i.e., infinite samples; see Methods). In all cases, R 2 values are greater than 0.82, with the infinite-sample decoder approaching 0.9.</p><p>How do these values translate into the quantity we really care about, the posterior covariance, and by implication the posterior distribution itself? To quantify this, we employ the standard measure of similarity for distributions, the KL divergence. Since the true posterior is Gaussian, and since the RBM encodes the (nearly) correct mean and variance of q(sDv), it too must be (nearly) Gaussian. (Given a specified mean and finite variance, the maximum-entropy distribution is normal. Thus if q(sDv) and p(sDr) have identical mean and variance, but the latter is Gaussian while the former is not, then the former has lower entropy-which is impossible, since information about S cannot be gained in the transition from R to V.) The KL divergence between two Gaussian distributions has a very simple form, and in fact we make it simpler still by examining only that portion contributed by the covariances-i.e., ignoring mean differences, since we have just examined them in the previous section: KLfN (m,S 0 ),N (m,S 1 )g</p><formula xml:id="formula_0">~(trace(S {1 1 S 0 ){log(DS {1 1 S 0 D){m)=2</formula><p>, where m is the number of dimensions. The first bar of Fig. <ref type="figure">3B</ref> show this divergence from the optimal posterior to the RBM-based posterior (again based on 15 samples).</p><p>What constitutes a proper point of comparison? Consider a fixed computation of the covariance which uses Eq. 3a but using the average (across all trials) total spike counts, g g x and g g h , rather than their trial-by-trial counterparts. If the model had learned the prior distribution over the total spike counts, but was not actually encoding any trial-by-trial information, it could do no better than this fixed computation. The KL divergence of the optimal posterior from this fixed computation is shown in the second bar of Fig. <ref type="figure">3B</ref>. The model is clearly far superior, demonstrating that it is indeed transmitting trial-by-trial covariance information.</p><p>RBM (lower right). Its hidden units, V, are Bernoulli conditioned on their inputs, corresponding to the presence or absence of a spike. The green heat map in the upper right depicts the mean of 15 samples from the hidden layer of a trained network for the example inputs shown at left. (B) Testing and training. In the first step of training (first panel), the external world elicits a vector of Poisson spikes from the input layer, driving recurrent activity in the neural network-up, down, and back up (second through fourth panels). The weights are then adapted according to the one-step contrastivedivergence rule. Testing also begins with a world-driven vector of Poisson spikes from the input populations, which drives 15 samples of hidden-layer activity (second panel). We then decode the input and hidden layers, yielding their respective posterior distributions. doi:10.1371/journal.pcbi.1003035.g001 Figure <ref type="figure">2</ref>. Recovery of the posterior mean. The four ellipses in each plot correspond to the covariances of four different estimates of the stimulus: the MAP estimate of the stimulus using only the visual input population (magenta), the MAP estimate using the proprioceptive input population (orange), the MAP estimate using both populations (i.e., the true posterior mean, which is the optimal estimate; black), and the estimate based on decoding the hidden layer (''RBM-based estimate''; green). (The color conventions are the same throughout the paper.) Each ellipse bounds the 95% confidence interval and is centered at its mean. All results are shown in the space of joint angles in units of radians. (A) Conditional errors. The middle plot shows the conditional errors for a grid of stimulus locations (each centered at the true stimulus); four examples are enlarged for clarity. Note that nontrivial biases arise only at the edges of the workspace. (B) Marginal error statistics. The RBM-based error (green) is unbiased and its covariance closely matches the optimal covariance (black). doi:10.1371/journal.pcbi.1003035.g002</p><p>Finally, we directly demonstrate the fidelity of the entire model posterior, q(sDv), to the entire optimal posterior, p(sDr), as a function of the population gains, by calculating the fractional information lost in terms of the normalized KL divergence: fractional information lost for fixed g 1 , g 2 ~SKLfp(sjr)jjq(sjv)gT q(vjr)p(rjg 1 ,g 2 )</p><p>SKLfp(sjr)jjp(s)gT p(rjg 1 ,g 2 ) :</p><p>Figure <ref type="figure">3</ref>. Recovery of the posterior distribution. (A) Reconstruction of the input total spike counts, g h and g x , for VIS and PROP, resp., from 15 samples of the hidden units (''samples''), and from infinite samples of the hidden units (''means''). Decoding these, along with the posterior mean (demonstrated in Fig. <ref type="figure">2</ref>), is sufficient to recover to posterior covariance. (B) Average (across all trials) KL divergences for two distributions from the optimal posterior, p(sDr h ,r x ): (black) the posterior over s given the mean (across trials) total spike counts ( g g x and g g h ) and the optimal posterior mean, ŝ s(r) :~E½SDr; and (green) the sample-based model-posterior, given also the optimal posterior mean. The mean-based model posterior, not shown, is visually indistinguishable. This measures purely the divergence resulting from failure to pass covariance information on to the hidden units. That the RBM-based posterior is so much smaller demonstrates that the model is not merely passing on mean spike-count information, but their trial-by-trial fluctuations. (C) Percent of total information lost from input to hidden units (measured by normalized KL divergence between the optimal and RBMbased posteriors; see Text S1), as a function of gains. Information loss is less than about 1.2% for all gains. (D) Posterior distributions (means and covariances) from three randomly selected trials. Color scheme is as throughout; dashed green shows the posterior computed from hidden-unit means ( v v), as opposed to samples (v, solid green). doi:10.1371/journal.pcbi.1003035.g003</p><p>This quantity is 0 in the best case, when q(sDv)~p(sDr), and 1 in the worst, when q(sDv)~p(s). (See also Text S1 for a more extended discussion.) Fig. <ref type="figure">3C</ref> shows that slightly more information is lost at low visual gains, but that in fact the slope is very shallow, since all information losses are between the small amounts of 0:9% and 1:2%. To visualize this small discrepancy, Fig. <ref type="figure">3D</ref> provides a qualitative comparison of the single-population, dual-population (optimal), and RBM-based posterior distributions, for three random trials. (These are not to be confused with the distribution of the posterior mean, as in Fig. <ref type="figure">2A</ref>, which is measured across trials.) The match between model and optimal posterior is evident for both covariance (size and shape of the ellipse) and mean (its location).</p><p>Effects of hidden-layer size and hidden-layer noise. Figs. 2 and 3 have shown model performance to be ''nearly'' optimal, in that both the posterior mean and the posterior covariance are encoded in the hidden layer. The small deviations from optimality can result from two distinct causes: (1) the network having failed to learn the ideal information-preserving transformation, and (2) the noise in the hidden layer having corrupted that transformation. In order to gauge the relative contribution of the two, we re-tested the model under a range of different capacities and noise levels by varying the number of hidden units and the number of samples taken at the hidden layer, respectively. Note that since the hidden units are Bernoulli, increasing the number of samples is akin to increasing the time window over which mean rates of activity are computed. Our assay is the error in the RBM-based estimate of the posterior mean; and since we observe that only the size, rather than the shape or position, of the error-covariance ellipse is greatly distorted as a function of decreasing samples, for simplicity we plot only the determinant of the error-covariance matrix.</p><p>Fig. <ref type="figure">4</ref> shows that, as expected, the error measure decreases both with more hidden units and more samples. However, a comparison of the different curves shows that the error asymptotes at N hidden units (cyan line), which is the number of units in one input population-increasing the hidden layer beyond that has no effect on performance. Performance also asymptotes at around ten samples per unit. At asymptote, the errors are close to optimal (solid black line), and much better than the single-input (PROP) error (dashed black line). (The VIS determinant is much larger and therefore omitted.).</p><p>Fig. <ref type="figure">4</ref> also shows the error for a network with 5N hidden units and the use of means (equivalent to taking infinite samples) in the hidden layer (dotted black line). This error lies about halfway between the optimal and asymptotic RBM-based errors, showing that about half that network's suboptimality is due to noise, and half due to network architecture and the learning algorithm; but in any case the network performance is quite close to optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulating psychophysical studies of optimal integration</head><p>We now relate our model to some familiar results from psychophysical investigations of multisensory integration. In the foregoing simulations, the input populations were driven by the same stimulus. The most common experimental probe of integration, however, is to examine the effects of a small, fixed discrepancy between two modalities-with, e.g., prism goggles or virtual feedback <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. Integrated estimates tend to fall between the means of the discrepant inputs, revealing the relative weighting of the two modalities. The mean location of the integrated estimate therefore allows experimenters to assess integration without having to obtain reliable estimates of the error covariance. Notice this point will not necessarily lie along the straight line connecting the input means, since the sensory covariances need not be aligned <ref type="bibr" target="#b0">[1]</ref>.</p><p>To replicate these experiments, the trained network from Fig. <ref type="figure">2</ref> was tested on sets of ''shifted'' data in which joint angles had been displaced from their corresponding visual locations by a fixed quantity, the ''input discrepancy,'' before being encoded in the prop population. To determine how large to make this discrepancy, we returned to the Figure <ref type="figure">4</ref>. Dependence of error covariance on numbers of samples and hidden units. Networks with different numbers of hidden units (see legend; N~number of units in a single input population) were trained on the input data, and then decoded for the posterior mean in the usual way but using different numbers of samples from the hidden layer (abscissa) before averaging. The determinants of the resulting error covariances are plotted here with colored lines. Dashed line, MAP error covariance using only proprioceptive input; solid line, optimal error covariance; dotted line, error covariance from the 5N network when using means in the hidden layer-i.e., infinite samples-the asymptote of the colored lines. doi:10.1371/journal.pcbi.1003035.g004 original, unshifted data. Although the average discrepancy between the two inputs in this data set is zero (as seen in the locations of the magenta and orange ellipses in Fig. <ref type="figure">2</ref>), the noisy encoding renders the discrepancy on single trials non-zero, with the probability of finding such a discrepancy determined by the sum of the input covariances, (S x zS h )~: S IN . This quantity providing, then, a natural measure of discrepancy, each set of shifted data was created with an input discrepancy of K standard deviations of S IN , with K[f2:5, 5, 7:5, 10, 12:5, 15g. Note that large K enables a further investigation-into the generalization of the trained network: The extent to which the RBM's optimality is maintained as the input discrepancy grows indicates, qualitatively, the generalization powers of the machine on these data.</p><p>Fig. <ref type="figure" target="#fig_1">5A</ref> shows the error statistics for these testing datasets for several discrepancy magnitudes along a single direction (discrepancies along other directions, not shown, were qualitatively similar). Psychophysicists examine conditional errors, but again for generality we have averaged across stimulus locations to produce marginal errors. The RBM-based estimator (green) becomes noticeably suboptimal by 7.5 standard deviations. Furthermore, the distribution of errors becomes distinctly nonnormal for large input discrepancies, spreading instead over the arc connecting the centers of the input error distributions. This arc corresponds to the location of the optimal estimate for varying relative sizes of the input error covariances <ref type="bibr" target="#b0">[1]</ref>. Whether such a pattern of errors is exhibited by human or animal subjects is an interesting open question.</p><p>Another way of measuring machine generalization is to test its performance under gain regimes outside its testing data. Since no discrepancy is enforced between the modalities, biases should be zero. Performance should be approximately optimal in the training regime, where gains ranged from 12 to 18 spikes. And indeed, Fig. <ref type="figure" target="#fig_1">5B</ref> shows that neither the error covariance (the relative shapes of the green and black ellipses) nor the bias (the relative positions of the green and black ellipses) are noticeably worse than in the training regime until the gain ratios (PROP/ VIS) reach the extreme values on the plot.</p><p>Finally, we examine machine performance under both input discrepancy and gain modulation, with a constant input discrepancy of 2.5 standard deviations and various gain ratios Fig. <ref type="figure" target="#fig_1">5C</ref>. The black and green dotted lines, nearly identical, track the movement of the error means of the optimal and RBM-based estimators, respectively. This reproduces the familiar psychophysical finding that varying the relative reliability of two discrepant inputs will bias downstream activity (sc., behavior) toward the more reliable modality's estimate <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different training data</head><p>Learning non-flat priors. So far we trained on stimuli that were chosen uniformly in joint space, so that the posterior mean is simply the peak of the likelihood given the inputs, p(r h ,r x Dh). In general, of course, these quantities are distinct. Since the learning algorithm we employ is a density estimation algorithm, it is expected to reproduce the marginal density p(r h ,r x )~Ð h p(h)p (r h ,r x Dh)dh, and thus should learn the prior over the stimulus as well as the likelihood. Therefore, the distribution of hidden-layer activities in the trained model will reflect both of these ''input distributions,'' and we should be able to decode the maximum a posteriori (MAP) estimate from the RBM. Importantly, we use the same decoding scheme employed as throughout (see Methods), ensuring that the prior is instantiated in the RBM rather than the decoder.</p><p>For simplicity, we chose the prior p(h) to be a tight Gaussianwith covariance on the order of the input covariances-centered in the middle of joint space (see The optimal posterior distribution over the stimulus in Methods). Thus, for a fixed stimulus, the (conditional) optimal estimator will be biased toward the center of joint space relative to that stimulus. Averaged over all stimuli, the (marginal) optimal estimator will be centrally located, but have smaller error covariance than its flat-prior counterpart-intuitively, the prior information increases the precision of the estimator.</p><p>This is precisely what we see for the RBM-based estimate in Fig. <ref type="figure" target="#fig_2">6A,</ref><ref type="figure">B</ref>. Its conditional statistics are shown for six different fixed stimuli in Fig. <ref type="figure" target="#fig_2">6A</ref>, along with those of the two unisensory MAP estimates and the optimal estimate (the MAP estimate given both input populations). The corresponding marginal error statistics, averaged over all stimuli under their prior distribution, are shown in Fig. <ref type="figure" target="#fig_2">6B</ref> The RBM-based error covariance, like its optimal counterpart, is tighter than that achieved with a flat prior (cf. Fig. <ref type="figure">2B</ref>).</p><p>Sometimes-decoupled inputs. We have been supposing the model to correspond to a multisensory area that combines proprioception of the (say) right hand with vision. When not looking at the right hand, then, the populations ought to be independent; and an appropriate model should be able to learn this even more complicated dataset, in which the two populations have a common source on only some subset of the total set of examples. This is another well known problem in psychophysics and computational neuroscience (see e.g. <ref type="bibr" target="#b20">[21]</ref>). When the integrating area receives no explicit signal as to whether or not the populations are coupled, the posterior distribution over the right hand is a mixture of Gaussians, which therefore requires the encoding of numerous parameters-two means, two covariance matrices, and a mixing proportion-and is therefore rather complicated to decode. Simulations, omitted here, show that the RBM does indeed learn to encode at least the peaks of the two Gaussians.</p><p>A slightly simpler model includes among the input data an explicit signal as to whether the input populations are coupled, in our case by dedicating one neuron to reporting it. This model is shown in Fig. <ref type="figure" target="#fig_2">6C</ref>: populations were coupled in only 70% of trials; in the others, the vis (magenta) population reports the ''left hand,'' and the unit labelled T indicates this by firing at its maximum mean spike count (otherwise it is off). Derivation of the optimal error covariance for the MAP estimate is given in Text S1; intuitively, the model must learn to encode different distributions in its hidden units depending on whether or not T is on. When T is off, these units should integrate the stimulus estimates encoded by the two populations and encode this integrated estimate (and its variance). When T is on, it should encode the proprioceptive stimulus and the visual stimulus separately. The optimal error variance is calculated by a weighted average of the error variances in the two conditions, smaller and larger respectively, the weights being the percentage of the time each conditions occurs (0:7 and 0:3, resp.). (The optimal error mean is still zero.) Fig. <ref type="figure" target="#fig_2">6D</ref> shows that a network trained on these data-with the same architecture as throughout-again achieves this optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other architectures</head><p>Hierarchical networks. A plausible neural model of multisensory integration will be composable in the sense that the integrating neurons can themselves serve as an input population for further integration with, e.g., another modality. Fig. <ref type="figure" target="#fig_4">7A</ref> illustrates the architecture of one such network. As above, input layers are Poisson, hidden layers are Bernoulli. The first RBM is the same as in the foregoing results; the second was trained on an input layer comprising the hidden-layer population of the first RBM (''integrated representation 1'') and a new input population (''PROP 2''), which for simplicity encodes joint angles, just as the first-layer proprioceptive population (''PROP 1'') does-though of course the population activities are different, since these are noisy. The second population also has a different gain on each trial (see the bottom panel of Fig. <ref type="figure" target="#fig_4">7A</ref>).</p><p>Again we focus on the error statistics of the posterior mean (Fig. <ref type="figure" target="#fig_4">7B</ref>). Both integrated representation 1 (using two inputs) and integrated representation 2 (using all three inputs) approach their optimal values. Although these error statistics are direct measures of posterior-mean encoding only, that the posterior variance is being encoded is demonstrated indirectly, as well: Proper After training, the model was tested on data that differ from its training distribution. (A) Discrepant-input data: PROP input (orange) is shifted by progressively greater increments of the input covariance (see text), leading to suboptimal integration, as expected, and structured error distributions. The hidden-layer error mean, like the optimal error mean, shifts rightward with the PROP ''bias.'' (B) Gain-modulated data: The training data had gains between 12 and 18. Testing on gains (ratios listed between panels (B) and (C)) outside this regime yields suboptimal error covariances but essentially zero biases. (C) Gain-modulated, input-discrepant data: As the relative reliability of PROP is increased, the optimal estimate shifts toward PROP and away from VIS. The green and black dotted lines, nearly identical, trace this movement for the machine-based and optimal estimates, resp. For larger discrepancies (not shown), this optimal behavior breaks down, the green and black lines diverging. doi:10.1371/journal.pcbi.1003035.g005</p><p>integration at the second level requires variance information to be encoded in the first hidden layer. The (nearly) optimal error statistics for the second layer show that indeed the posterior variance information is encoded on a per-trial basis in the (first) hidden layer.</p><p>Coordinate transformation. We consider now another, seemingly different, computational problem studied in the sensorimotor literature, coordinate transformation (sometimes called ''sensory combination'' <ref type="bibr" target="#b21">[22]</ref>). In general, the relationship between proprioception and visually-encoded stimuli is mediated by other quantities-gaze angle, head-to-body orientation, body-to-arm orientation, etc. -which are themselves random variables. In the simplest version, the relationship of vision to proprioception depends only upon gaze position, X ~F (H){E, and the ''stimuli'' consist of two independent random variables H and E <ref type="bibr" target="#b22">[23]</ref>. Fig. <ref type="figure" target="#fig_4">7C</ref> depicts a probabilistic graphical model for this scenario, along with the RBM that is to learn these data (cf. Fig. <ref type="figure" target="#fig_0">1A</ref>). The optimality equations are slightly more complicated for this problem (see Coordinate Transformations in Text S1), but conceptually similar to that of simple multisensory integration (Eq. 3).</p><p>In this model, the proprioceptive population is responsible for a larger space than either of the other two variables, a consequence of our choice to sample in the space of the latter (see Fig. <ref type="figure">S2A</ref> and related discussion in Tuning of the coordinate-transforming neurons in Text S1). Allocating to each population the same number of neurons, while also demanding that the H variance be  <ref type="figure">(A,</ref><ref type="figure">B</ref>) : Learning a prior. The network was trained on population codes of an underlying stimulus that was drawn from a Gaussian (rather than uniform, as in the previous figures) prior. This makes the MAP estimate tighter (cf. the black ellipses here and in Fig. <ref type="figure">2B</ref>) -and indeed the RBM-based estimate's error covariance is correspondingly tighter. (A) Conditional estimate statistics (color scheme as throughout): The output estimates (green) have smaller covariances, but they, like the optimal estimates (black) are also biased toward the mean of the prior, located at the center of the workspace. The match between them is evidently very good. Note that the stimulus location for each of these conditional statistics is eight standard deviations from the mean of the prior-so the model has generalized well to points that constituted a trivial fraction of the training data. small enough for its contribution to affect appreciably the integrated estimate, requires that we increase its relative gain; hence we let g x ~5, g h ~15, g e ~5. In keeping with the simple relationship just given, all variables are one-dimensional; the network allocates 60 units to each, yielding 180 total input units. The hidden layer has only 160, respecting our requirement that it be smaller than the input layer. (The ratio of hidden/input was chosen with the following rationale: The input layer encodes six random variables-the three ''stimuli,'' X, H, and E, plus their three associated gainswhereas the hidden layer needs to encode five, one of the stimuli being redundant with the other two. And indeed, using fewer than 160 hidden units yields suboptimal results. Cf. the ''standard'' network, for which the input encodes six variables-the two gains and the two 2D stimuli-, and the hidden layer encodes four-two gains and a single 2D stimulus. A longer discussion of these approximate calculations can be found in Text S1.) Fig. <ref type="figure" target="#fig_4">7D</ref> shows  <ref type="figure">(A,</ref><ref type="figure">B</ref>) A ''hierarchical'' network, in which a third modality must be integrated with the integrated estimate from the first stage-which is just the original model. (A) Data generation (bottom), population coding (middle), and network architecture (cf. Fig. <ref type="figure" target="#fig_0">1A</ref>). Input units are Poisson and hidden units (green) are Bernoulli. The population codes, depicted in one dimension for simplicity, are actually 2D. Each hidden layer has half (~900) the number of units in its input layer (~1800). (B) Marginal error statistics. The error ellipses for PROP 1 (orange), for VIS (magenta), for both PROP 1 and VIS (dashed black), and for ''integrated representation 1'' (dashed green) replicate the results from Fig. <ref type="figure">2B</ref>. PROP 2 is encoded in the same way as PROP 1 (though their activities on a given trial are never equal because of the Poisson noise), and so has identical error statistics (orange). Conditioning on this third population in addition to the other two shrinks the optimal error covariance (solid black), and the estimate decoded from ''integrated representation 2'' (solid green) is correspondingly smaller as well, and again nearly optimal. (C,D) Coordinate transformation. (C) Data generation (bottom), population coding (middle), and network architecture (top). Each input population (bottom panel, color coded) depends on its own gain; whereas, both PROP (h, orange) and VIS (x, magenta) depend on the stimulus (hand position), and both VIS and EYE (e, blue) depend on gaze angle. (D) Mean square errors. The RBM-based estimates have nearly minimal MSEs, demonstrating that these estimates are nearly equal to the mean of the true posterior distribution. Inset: the physical setup corresponding to coordinate transformation. Red shading denotes joint limits; the black line denotes the origin of joint space. doi:10.1371/journal.pcbi.1003035.g007 that mean square errors (MSEs) of the RBM-based estimate of the stimulus are, once again, nearly optimal given the three inputs. (We can show mean and variance together as MSE without loss of generality because the posterior mean is the unique minimizer of the MSE, so showing that the RBM-based estimator achieves minimum MSE shows that it is the posterior mean.) This demonstrates the generality of our approach, as the same network and algorithm will learn to perform multisensory integration or coordinate transformations, depending simply on its inputs (cf. the networks of <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>, which are built specifically to perform coordinate transformations). Nor is there reason to believe that learnable transformations are limited to simple combinations of the form X ~F (H){E, which was chosen here merely to simplify our own computations of the optimality conditions (see Coordinate Transformations in Text S1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Properties of the hidden units and their biological implications</head><p>We now examine some of the properties of the hidden units, especially those that electrophysiologists have focused on in multisensory neurons in rhesus macaques.</p><p>Integrating neurons. Fig. <ref type="figure" target="#fig_5">8A</ref> shows tuning curves for a random subset of 16 tuned hidden units in our ''standard'' multisensory-integration network (Multisensory integration in the RBM). (By ''tuned'' we mean neurons whose mean firing rate-i.e., probability of firing-varied by more than 0.1 over the stimulus range.) To render tuning more clearly, curves were computed noiselessly-using means in both the input and hidden layers-and with a fixed gain of 15 for both populations.</p><p>Interestingly, the two-dimensional tuning for joint angles (left column) is multimodal for many cells-although also highly structured, as apparent from comparison of tuning for the trained (upper row) and untrained (lower row) networks. Although multimodal tuning has been found in multisensory areas, for example, area VIP (see Fig. <ref type="figure">3</ref> of <ref type="bibr" target="#b27">[28]</ref>), a comparison of these plots with empirical data is complicated by the fact that neurophysiologists typically do not collect tuning data over a complete planar workspace.</p><p>We therefore restrict attention to the 1D submanifold of joint space indicated by the black slash through the 2D tuning curves, corresponding to an arc in the visual space, since tuning over this range was reported in <ref type="bibr" target="#b28">[29]</ref> (see especially the supplement) for multisensory neurons in Area 5 and MIP; we show the corresponding model tuning in the right column for the same sixteen neurons as the left column. The determination of whether or not model neurons are tuned was made along this arc (rather than the entire planar workspace); in this limited range, 137 of the 900 hidden units were tuned. Results are qualitatively similar between data and model: Along the arc, units in the trained network are unimodal and occasionally monotonic (unlike in the untrained model, bottom right). Furthermore, although none of these 16 cells exhibited bimodal tuning for this arc, from the distribution of planar tuning we expect that some cells would; and indeed a subset of cells in <ref type="bibr" target="#b28">[29]</ref> exhibit bimodal tuning (see Fig. <ref type="figure" target="#fig_1">5</ref> and Supplemental Fig. <ref type="figure" target="#fig_2">6C</ref> in the cited work).</p><p>Fig. <ref type="figure" target="#fig_5">8A</ref> also shows how the tuning along the 1D arc depends on the input gains. Although broadly similar across gains, increasing gain does result in a subtle sharpening of the tuning curves. This can be quantified more directly by simply counting the number of active neurons for a given stimulus under different gains: sharper tuning curves will result in fewer neurons firing (though possibly more total spikes). And indeed, after sampling 15 spikes from the hidden layer, the percent of neurons firing is 22:5, 21:2, and 20:3, for g x ~gh ~12, 15, 18, respectively. This is in contrast to the input layer, where increase in gain increases the number of spiking units. Sharpening is also in contrast to the theoretical predictions of <ref type="bibr" target="#b1">[2]</ref>, where the hidden layer is a probabilistic population code of the same form as the inputs, with both having the property that higher gains imply a greater number of active neurons. This feature has not been investigated directly in multisensory areas of the cortex, and presents a useful test for the model. Although the absence of sharpening would not rule out a broader class of density estimation models, it would indeed rule out this particular implementation.</p><p>Coordinate-transforming neurons. Investigation of multisensory tuning properties has a longer history for coordinate transformations. Here, especially in Area 5d, MIP, and VIP, neurons have been reported to encode objects in references frames intermediate between eye and body (''partially shifting receptive fields'') -i.e., the receptive field moves with the eye, but not completely; and eye position modulates the amplitude of the tuning curve (''gain fields'') <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>. As in those studies, we find examples of retinotopic, body-centered, and partially shifting receptive fields-even fields that shift opposite to the change in gaze-angle. Fig. <ref type="figure" target="#fig_5">8B</ref> shows examples of all four types (see legend). (We conflate head-centered and body-centered tuning in what follows, since we assume a head-fixed model.).</p><p>More recently, Andersen and colleagues <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> have proposed to analyze these qualitative descriptions in terms of (1) the ''separability'' of the tuning curve-whether it can be written f (s,e)~f s (s)f e (e); and (2) the reference frame in which the neuron is tuned-body, eye, or something intermediate-as measured by the gradient of the tuning in the (s,e) space, since the direction of steepest change indicates the strongest tuning. All and only the neurons with pure gain fields (no shift) will be separable. The extent of receptive-field shift for inseparable neurons is indicated by the gradient analysis.</p><p>We reproduce that analysis on our model here. In <ref type="bibr" target="#b30">[31]</ref>, there is a third variable in addition to hand location and gaze position, namely target. However, direct comparison between model and data can be made simply by identifying the hand and target. Finally, since all tuning curves were measured in visual space, we do the same; thus we define: T ret :~X , the retinotopic hand/ target location in visual space; and T body :~L cos(H), the bodycentered hand/target in visual space; giving the familiar equation T ret ~Tbody {E. Fig. <ref type="figure" target="#fig_5">8C</ref> shows the resulting histogram of gradient directions, which is qualitatively quite similar to its counterpart, the top panel of Figure <ref type="figure">4</ref> of <ref type="bibr" target="#b30">[31]</ref>: a peak at T body , minor peaks at the other ''unmixed'' stimuli, with representative neurons at all stimuli combinations-except those intermediate between T body zE and E, where there is a gap in the histogram.</p><p>Nevertheless, we emphasize that correspondence between model and data in Fig. <ref type="figure" target="#fig_5">8C</ref> should be interpreted with extreme caution: it is possible to obtain different distributions of receptivefield properties with our model; see Text S1 : Tuning of the coordinate-transforming neurons for further discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We have demonstrated a neural-network model of multisensory integration that achieves a number of desirable objectives that have not been captured before in a single model: learning de novo to integrate the mean and covariance of representations of nonlinearly related inputs; learning prior distributions over the encoded stimuli; staged, hierarchical integration; and ''coordinate transformations.'' Our approach is based on two central ideas. The first, following <ref type="bibr" target="#b1">[2]</ref>, is that the goal of multisensory integration is not (merely) to encode in the multisensory neurons (v) an optimal point estimate of the stimulus (s) given the activities of the input populations (r~½r h ,r x ); but to encode an entire (optimal) distribution, so that q(sDv)~p(sDr). This criterion is equivalent to demanding that all the information in the input populations about the stimulus-the mean, variance, and higher cumulants, if applicable-be transferred to the multisensory neurons v. Behavior itself corresponds to a single point from this distribution, but the higher cumulants will be necessary for intervening computations: for example, the variance of the integrated estimate determines how to integrate it optimally with other estimates downstream (see Fig. <ref type="figure" target="#fig_4">7</ref>).  <ref type="figure"></ref>and<ref type="figure">2</ref>). The left column shows tuning curves in the space of joint angles for sixteen randomly chosen hidden units; the right column shows those same units for the arc of reach endpoints from <ref type="bibr" target="#b28">[29]</ref>. The top row shows tuning curves for the trained model; the second row shows the same curves for the untrained model. The location of the arc in joint space is shown by the black slash through the tuning curves in the left column. Whereas the left-column tuning curves were collected for a single gain (g~15), the right-column curves were collected for g~12 (blue), g~15 (green), and g~18 (red) (the same gain was used for both populations, VIS and PROP). (B) Example hidden-unit tuning curves from the coordinate transformation model for body-centered hand position (T body :~L cos(H); see text for details), for two different gaze positions (red and green curves). The dashed blue curves show where the red tuning curves would lie for the second gaze position if they shifted completely with the eyes, as illustrated by the red arrows, i.e. if they were retinotopic. Some cells (second column) are; some are body-centered (first column); some partially shift (third column); and some even shift in the opposite direction of the gaze angle. (C) Coordinate-transforming cells can be tuned for any of the variables on the continuum from gaze angle (E), to retinotopic hand position (T ret :~X ~Tbody {E), to body-centered hand position (T body ), to body-centered hand position plus gaze angle (T body zE). The histogram shows the distributions of such tunings in the hidden layer, using the analysis of <ref type="bibr" target="#b30">[31]</ref>. doi:10.1371/journal.pcbi.1003035.g008</p><p>The second central idea is that this information-retention criterion will be satisfied by the hidden or ''latent'' variables, V, of a generative model that has learned how to produce samples from the distribution of its input data, R, a process called latent-variable density estimation. The intuition connecting this learning problem with the seemingly very different task of multisensory integration is that being able to reproduce the input data (up to the noise) requires encoding their ''hidden causes''-the features, like hand location, that vary across trials, and thus should be transmitted downstream-in the latent-variable activities. The density estimator will likewise learn to represent the statistical features that do not vary across trials, like prior information, in its weights. Since a network that has learned to reproduce its inputs efficiently will have implicitly learned the underlying relationship between their hidden causes, density estimation also naturally solves other computational problems that arise in multisensory processing: the need to perform coordinate transformations (Fig. <ref type="figure" target="#fig_4">7C</ref>), for example, arises because a signal is available that correlates with a transformed version of other variables-like retinotopic object location with the combination of body-centered object location and gaze angle. Efficiently encoding the distribution of the larger set of variables requires learning the coordinate transformation.</p><p>With the network implementation of latent-variable density estimation, we have demonstrated how all three of these learning problems-optimal integration, the integration of prior information, and coordinate transformations-can be solved by multisensory neural circuits. We have previously argued that these three operations are exactly those required for planning multisensoryguided reaching movements <ref type="bibr" target="#b22">[23]</ref>. There is considerable evidence for multimodal, reaching-related signals across several brain areas in the posterior parietal cortex, including Area 5d, MIP, VIP, V6, and Area 7 <ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>. We propose that density estimation, driven by latent-variable learning, is the principle underlying computation performed by these areas. The fact that our network can be hierarchically composed is central to this hypothesis: these brain areas receive overlapping but distinct sets of inputs and with a rough hierarchical organization within them <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref>. Density estimation on these inputs, then, is expected to yield activity patterns that are also highly overlapping but distinct, as observed, for example, in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b43">44]</ref>. We have previously argued that having a collection of such representations allows for the flexible and (nearly) optimal use of a wide range of sensory inputs <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implications of the model</head><p>One example of a statistical feature that is constant across trials is the prior distribution of the stimulus, which the network therefore learns to encode in its weights. Whether prior distributions in the brain are encoded in synaptic weights <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>, as a separate neural population <ref type="bibr" target="#b1">[2]</ref>, or something else again, remains an area of active research (see also Text S1).</p><p>An interesting consequence of the present formulation is that it renders the gains random variables (see e.g. Fig. <ref type="figure" target="#fig_0">1A</ref>), no less than the stimulus location; that is, they represent information that is not constant across trials. This has testable implications for multisensory populations. For an M-dimensional stimulus, the posterior precision (inverse covariance) of the multisensory neurons is an M|M symmetric matrix and therefore has M(Mz1)=2 independent entries. But if the precisions of the two input populations are each functions only of a single parameter (their respective gains, reflecting the confidence in each modality), then the multisensory activities need only encode two, rather than M(Mz1)=2, numbers on each trial. Conversely, in the case of a one-dimensional stimulus, a population of multisensory neurons ostensibly need only encode the single value of the posterior variance, Var½SDr 1 ,r 2 , but the density-estimation approach predicts that the hidden-unit activities on a given trial will nevertheless encode both of that trial's input-population gainsand indeed they do in our model, albeit imperfectly (Fig. <ref type="figure">3A</ref>). Testing these predictions experimentally would be straightforward-try to decode unisensory covariances from a multisensory population-but it has never been done.</p><p>The question of whether cortical circuits learn to encode any posterior covariance information at all, as opposed to merely the point estimate that psychophysical experiments elicit, is itself a crucial, open one. Of course, in theory one can always compute a posterior over the stimulus given some population activities <ref type="bibr" target="#b47">[48]</ref>; but whether the posterior conditioned on activities deep in the hierarchy matches that conditioned on the activity in early sensory cortices, as in our model, is unknown. Our model also predicts that such constancy would emerge during learning-which could be tested, for instance, by training an animal on a novel multisensory pairing (e.g., audition and touch).</p><p>That fewer units are used to represent the same information (half as many in our simple integration model; see Multisensory integration in the RBM), and that the maximum spike count of each hidden neuron is bounded by the maximum mean spike count of the inputs, constrains the amount of information that can be transmitted. This forces the hidden units to represent the information more efficiently-i.e., to ''integrate'' it. In fact, without that constraint, no learning would be required to satisfy the information-retention criterion: A random N|N weight matrix has rank N almost surely, and the neuron nonlinearities are likewise invertible, so any random set of synaptic connections would suffice (since any invertible transformation is informationpreserving). We chose to constrain the multisensory representational capacity, so that the synaptic connections form an N=2|N matrix, which will not in general preserve stimulus information. One promising theoretical strategy would be to take ''passing on all the information'' as a given, and then to seek the set of constraints-fewest spikes <ref type="bibr" target="#b48">[49]</ref>, topography <ref type="bibr" target="#b49">[50]</ref>, fewest neurons, least processing time, computational efficiency <ref type="bibr" target="#b50">[51]</ref>, etc. -that yields the most biologically realistic activity patterns in the multisensory units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship to other work</head><p>Multisensory integration was first considered from the standpoint of information theory and unsupervised learning in <ref type="bibr" target="#b51">[52]</ref>, and in a related work <ref type="bibr" target="#b49">[50]</ref>, and our approach is similar in spirit, but with important differences. Crucially, a different objective function was minimized: integration was achieved by maximizing mutual information between the hidden/output units of two neural networks, each representing a modality, forcing these units to represent common information, the latter additionally constraining topography. In our model, contrariwise, integration is enforced indirectly, by requiring a reduced number of (hidden) units to represent the information in two populations. This allows for greater generality since it does not require foreknowledge of which populations should be forced to share information: if the information in the input populations is redundant, it will be ''integrated'' in the hidden units, and conversely. More recently, the idea of treating multisensory integration as a density estimation problem has been proposed independently by <ref type="bibr" target="#b52">[53]</ref>, a complementary report that explores both cognitive and neural implications of this view, without proposing an explicit neural implementation. As in <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b51">52]</ref>, then, no attempt is made to employ biological learning rules. Most significantly, none of these models invokes the criterion for optimal integration that we have argued to be central-the correct posterior distribution over the stimulus given hidden-unit activities (q(sDv)~p(sDr), in the notation of this paper). This renders the combination of three signals of two independent causes-coordinate transformation-a matter simply of allowing another population to feed the hidden units; whereas the other models would require something more sophisticated.</p><p>More recent models of multisensory integration or cross-modal transformation neglect some combination of the desiderata listed in the introduction. Basis-function networks with attractor dynamics <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b53">54]</ref> ignore prior distributions but more significantly require hand-wiring (no learning). The models of <ref type="bibr" target="#b45">[46]</ref> and <ref type="bibr" target="#b46">[47]</ref> extend these attractor networks to include the learning of priors, but even these must be hand wired and so are practical only for simple representations. Other models of learning <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b54">55]</ref> disregard variance information, so that what is learned is essentially a mapping of means; nor, correspondingly, do they account for the learning of priors. The probabilistic population coding model <ref type="bibr" target="#b1">[2]</ref> makes explicit the notion of encoding a posterior, but includes no model of learning.</p><p>Finally, many authors have either anticipated <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref> or explicitly proposed <ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref> that learning to process early sensory information might be viewed as forms of density estimation. Our work shows that the range of computations that can be assimilated to this statistical problem extends to the acquisition of two key operations for motor planning and control: multisensory integration, even when the underlying stimulus is distributed nonuniformly, and coordinate transformations; and further that these computations can be combined hierarchically, as is observed in the the neural circuits underlying these operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Notation is standard: capital letters for random variables, lowercase for their realizations; boldfaced font for vectors, italic for scalars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input-data generation</head><p>Throughout, we work with the example case of integrating twodimensional (2D) proprioceptive and visual signals of hand location, but the model maps straightforwardly onto any pair of co-varying sensory signals. These two signals report elbow and shoulder joint angles (PROP, H), and fingertip position in Cartesian space (VIS, X), respectively. Choosing the forward kinematics, X~F (H), to be invertible renders the variables isomorphic, so that we can refer generically to them as a ''stimulus'' (S), independent of space. The kinematics model for most of the results has joint ranges of ½{p=2,p=4 (shoulder) and ½p=4,3p=4 (elbow) and limb lengths of 12 (upper arm) and 20 (forearm) cm; see inset of Fig. <ref type="figure" target="#fig_0">1A</ref>. The exception is Fig. <ref type="figure" target="#fig_4">7C,</ref><ref type="figure">D</ref>, in which a one-degree-offreedom (1D) arm was used for simplicity: X ~L cos(H){E, with link length L~12 cm and joint range ½p=6,5p=6, and E the position of the eye (EYE, gaze angle). Below, we describe data generation from the 2D kinematics; the modifications for 1D are straightforward.</p><p>Each training vector consists of a set spike counts, ½r h ,r x , generated by choosing a random stimulus (s, i.e. h and x) and a random global gain for each modality (g h ,g x ), and encoding them in a populations of neurons with Gaussian tuning curves (f i ) and independent Poisson spike counts-a ''probabilistic population code'' <ref type="bibr" target="#b1">[2]</ref>: p(r h ,r x js,g h ,g x )~p(r h jh,g h )p(r x jx,g x )</p><formula xml:id="formula_2">~P i Pois½r i jg h f i (h) P i Pois½r i jg x f i (x),<label>ð2Þ</label></formula><p>as illustrated in Fig. <ref type="figure" target="#fig_0">1A</ref>. Each gain, g s , can be thought of as the confidence in its respective modality, since the posterior covariance of a single, sufficiently large population, Cov½SDr s , is inversely proportional to its gain <ref type="bibr" target="#b1">[2]</ref>. The tuning curves f i of each population are two-dimensional, isotropic, unnormalized Gaussians, whose width (variance) is S t , and whose centers form a regular grid over their respective spaces.</p><p>To avoid clipping effects at the edges, the space spanned by this grid of N|N neurons is larger than the joint space (or, for VIS, than the reachable workspace). Thus the grid consists of a central ''response area'' whose neurons can be maximally stimulated, and a ''margin'' surrounding it whose neurons cannot. The margin width is four tuning-curve standard deviations (4S 1=2 t ), making spiking of putative neurons outside the grid extremely unlikely even for stimuli at the edge of the response area. In accordance with the broad tuning curves found in higher sensory areas and with previous models of population coding in multisensory areas <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27]</ref>, tuning-curve widths were themselves chosen so that their full width at half maximum embraced one-sixth of the response area.</p><p>The prior over the stimulus is either uniform or Gaussian in the space of joint angles. (Implementation of the Gaussian prior is detailed in Learning non-flat priors.) Since both dimensions of prop space are allotted the same number of neurons (N) and the tuning curves are isotropic and evenly spaced, but the physical ranges of these dimensions differ (3p=4 and p=2 for the shoulder and elbow, resp.), the induced covariance Cov½HDr h in the population code is anisotropic, being more precise in elbow than shoulder angle. The nonlinearity of the forward kinematics likewise ensures anisotropy of Cov½HDr x ; see Fig. <ref type="figure" target="#fig_0">1A</ref>. This makes the problem more interesting, anisotropic covariances entailing, for example, optimal estimates that are not on the straight-line path between cue means (see e.g. Fig. <ref type="figure" target="#fig_0">1</ref> of <ref type="bibr" target="#b0">[1]</ref>).</p><p>The priors over the gains, G h and G x , which set the maximum mean spike counts, are independent and uniform between 12 and 18 spikes. Unless otherwise noted, gains in the testing data were drawn from the same distribution as the training-data gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The optimal posterior distribution over the stimulus</head><p>To show that the model works, we must compare two posterior distributions over the stimulus: the posterior conditioned on the input data, p(sDr h ,r x )-i.e. the ''true'' or ''optimal'' posterior-and the posterior conditioned on the downstream/integrating units, q(sDv) (see The RBM, below). That comparison is easiest to make, and to exhibit, when the optimal posterior is as simple as possible-ideally, a Gaussian, which has only two nonzero cumulants, mean and covariance. With a flat or Gaussian prior over the stimulus, the probabilistic population code that we are using does indeed have an approximately normal posterior for a unimodal population <ref type="bibr" target="#b1">[2]</ref>; but to guarantee this for two populations that are encoding the stimulus in different (i.e., nonlinearly related) spaces, the unimodal posterior covariances (Cov½XDr x and Cov½HDr h ) also must be small enough that typical errors lie within the linear regime of the arm kinematics (see Text S1). Given the gain (G) regime and the tuning-curve widths (S t ), choosing N~30 neurons in the N|N grid yields variances between 2 and 9 mm 2 for the two populations, satisfying the requirement. These values are also comparable to empirical values for visual and proprioceptive localization variances from human psychophysics, 5 mm 2 and 50 mm 2 , resp. <ref type="bibr" target="#b0">[1]</ref>. These latter are in fact an upper bound, since they are with respect to behavior, the furthest downstream assay of certainty. In any case, we stress that this and other compromises of the population code with biological realism (uniform tiling of the stimulus space, identical tuning curves, etc.) serve to simplify the analyses interpretation rather than reflecting any limitation of the neural-network model. Now, whereas a Gaussian posterior requires a flat or Gaussian prior, such a prior in prop space will induce an irregular prior in VIS space (and vice versa; see again Fig. <ref type="figure" target="#fig_0">1A</ref>) -so there can be a Gaussian posterior only in one space. Results are therefore computed in the space of the flat or Gaussian prior. Observing these constraints, the posterior cumulants can be written:</p><formula xml:id="formula_3">Cov½HDr {1 &amp;S {1 0 zg h S {1 t zg x (J T S {1 t J)<label>ð3aÞ</label></formula><formula xml:id="formula_4">E½Hjr&amp;Cov½Hjr½S {1 0 m 0 zg h S {1 t y(r h ) zg x (J T S {1 t J)F {1 ½y(r x ):<label>ð3bÞ</label></formula><p>(See Text S1 for a derivation.) Intuitively, the posterior precision (inverse covariance, Eq. 3) is a sum of three precisions: the prior precision, S {1 0 ; the weighted PROP (h) tuning-curve precision, S t {1 ; and the weighted VIS (x) tuning-curve precision, J T S {1 t J. (Since the posterior is expressed over H rather than X, the latter's precision must be warped into h-space by the Jacobian, J~LF =Lx, of the forward kinematics, which is evaluated at the center of mass of the proprioceptive population.) The weights are the total spike counts for each population, g s :~P j r s j , s~h,x. The posterior mean (Eq. 3b) is a normalized, weighted sum of three estimates: the prior mean, m 0 ; the center of mass of the h population, y(r h ); and the (transformed) center of mass of the x population, F {1 ½y(r x ). The weights are the three precisions. The center of mass y(r s ) :~P j s Ã j r s j = P j r s j , with s Ã j the j th preferred stimulus, is likewise intuitive, being the maximum-likelihood estimate of the stimulus for a single population <ref type="bibr" target="#b60">[61]</ref>.</p><p>The nonlinearity (cosine) in the 1D ''coordinate-transformation model'' (Fig. <ref type="figure" target="#fig_4">7C,</ref><ref type="figure">D</ref>), X ~L cos(H){E, likewise allows the posterior to be normal in only one space. Since two of the variables live in Cartesian space-X (VIS) and E (EYE) -and only H (PROP) lives in joint-angle coordinates, we chose uniform priors over the former, sampling them between L cos(5p=6)=2 and L cos(p=6)=2, so that their sum never exceeded the bounds of the joint range (see above, Input-data generation). Zero in this space corresponds to hand position at the center of fixation for x, and to central fixation for E.</p><p>The addition of a non-flat prior (Fig. <ref type="figure" target="#fig_2">6</ref>) will only have an appreciable effect on the posterior if the width of the prior distribution is comparable to that of the likelihoods, i.e. the single-modality localization covariances. The covariance of the prior was therefore constructed so that, along both dimensions, the extreme angles were 150 standard deviations apart-a reasonable prior distribution, perhaps, after extensive training on a reaching task to a single target location <ref type="bibr" target="#b46">[47]</ref>. Using more realistic, broader priors would require relaxing the constraint that the optimal posterior distribution over the stimulus be Gaussian-which again we insist upon only for ease of analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The RBM</head><p>The neural circuit for sensory integration was modeled as a restricted Boltzmann machine, a two-layer, undirected, generative model with no intralayer connections and full interlayer connections (Fig. <ref type="figure" target="#fig_0">1A</ref>, bottom right) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b61">62]</ref>. The input layer (R) consists of Poisson random variables, whose observed values are the population codes just described. The hidden-layer units (V) are binary, indicating whether or not a unit spiked on a given trial, making them Bernoulli random variables. Unless otherwise noted in the results, the number of hidden units in the model is equal to half the number of input units, i.e. the number of units in a single input population-thus forcing the model to represent the same information in half the number of neurons.</p><p>During RBM training <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b61">62]</ref>, input and hidden units reciprocally drive each other through the same weight matrix:</p><p>V*q(vDr)~P </p><p>which corresponds to Gibbs sampling from the joint distribution represented by the machine. Here fzg i is the i th entry of the vector z; b v and b r are, respectively, the vectors of biases for the hidden and observed units; W is the matrix of synaptic strengths; and s(x) :~1=(1ze {x ) is the logistic (sigmoid) function. (The lack of intralayer connections is what allows the entire joint to be sampled in just two steps.) As in a standard stochastic neural network, each unit's mean activity is a nonlinear transformation of a weighted sum of its inputs. To ensure that this mean is in the support of its associated exponential-family distribution, the nonlinearities are chosen to be the inverse ''canonical links'' <ref type="bibr" target="#b62">[63]</ref>: the logistic function for the Bernoulli hidden units, and the exponential function for the Poisson input units.</p><p>(Technically, the use of Poisson input units makes the model an ''exponential family harmonium'' <ref type="bibr" target="#b61">[62]</ref> rather than a restricted Boltzmann machine, which would have all Bernoulli units.) The unit's activity (presence of a spike, or spike count) is sampled from this mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>Weights and biases were initialized randomly, after which the networks were trained on batches of 40,000 vectors, with weight changes made after computing statistics on mini-batches of 40 vectors apiece. One cycle through all 1000 mini-batches constitutes an ''epoch,'' and learning was repeated on a batch for 15 epochs, after which the learning rates were lowered by a factor of ffiffiffiffiffi 10 p . This process was repeated a total of seven times, i.e. 90 epochs, after which learning was terminated. (The number of epochs and the learning-rate annealing schedule were determined empirically.) Weight and bias changes were made according to one-step contrastive divergence <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>: DW !Srv T {Sr rv v T T q(r rDv)q(v vDr r) T p(r)q(vDr) Db r !Sr{Sr rT q(r rDv) T p(r)q(vDr) Db v !Sv{Sv vT q(r rDv)q(v vDr r) T p(r)q(vDr) , ð5Þ where the circumflexes differentiate the zeroth (no hat) and first (hat) steps of Gibbs sampling. That is, the input data (r) are propagated up into the multisensory (hidden) layer (v), back down into the input units (r r), then back up into the multisensory neurons (v v); see Fig. <ref type="figure" target="#fig_0">1B</ref>. This is repeated for all the data (that is, for each r h ,r x drawn from Eq. 2, for each stimulus and set of gains drawn from p(s) and p(g)). The change in the weight connecting neuron i to neuron j is thus proportional to the difference between the first and second pair of correlations between them-a Hebbian and an anti-Hebbian term. This rule approximates gradient descent on an objective function for density estimation (Hinton's ''contrastive divergence'' <ref type="bibr" target="#b16">[17]</ref>, or alternatively ''probability flow'' <ref type="bibr" target="#b63">[64]</ref>). Although this specific learning rule has not been documented in vivo, it is constructed entirely of components that have been: change firing rate based on (local) correlations between pre-and postsynaptic spike counts. Anti-Hebbian learning has been observed in a neural circuit <ref type="bibr" target="#b64">[65]</ref>, albeit not in mammalian cortex, and plausible cellular mechanisms for it have been described <ref type="bibr" target="#b65">[66]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Testing</head><p>After training, learning was turned off, and the network was tested on a fresh batch of 40,000 data vectors (Fig. <ref type="figure" target="#fig_0">1B</ref>): stimuli were again drawn uniformly from the grid of joint angles, and the corresponding spike counts simulated by drawing from the two populations of Gaussian-tuned, Poisson neurons. For each input vector, hidden-layer activities were computed by drawing 15 sample vectors (from p(vDr)) and averaging them. Since the input gains are between 12 and 18, and assuming that hidden and input units integrate information over the same-sized time window from the past, this implies that hidden neurons fire no faster than input neurons-which would otherwise constitute a violation of the information bottleneck. This is essential for our task, since we require an efficient coding, not merely a different one.</p><p>For each trial, decoding the hidden vector consists of estimating from it the mean and covariance of the optimal posterior p(sDr)that is, all the information in the network about the stimulus. Generally, finding a good decoder can be hard; but because the network is a generative model, we can use its generative (hiddento-input) weights to turn the hidden vector back into expected input spike counts (E½R h ,R x Dv)-which we know how to decode: Eq. 3. In practice, it often turns out that the weighted sum in Eq. 3b is unnecessary: the center of mass from a single (updated) population suffices. When showing results in joint angles, we take the center of mass of the prop population; likewise for Cartesian space and vis. Also, reconstruction of the total spike counts was mildly improved by first mapping them to the true (input) total spike counts via a standard neural network; in cases where this final step was applied (Fig. <ref type="figure">3A</ref>), training and testing used different data. The posterior covariances used in Fig. <ref type="figure">3B-D</ref>, however, did not use any such trained decoder; they were reconstructed just as the posterior means were, i.e. by using the generative weights and then applying equation Eq. 3a.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Multisensory integration: data and model. (A) The model and example data. World-driven data are generated according to the directed graphical model boxed in the lower right: On each trial, a hand location s and the population gains g x and g h for the two sensory modalities are drawn from their respective prior distributions. Given these, a spike count is drawn for each neuron (magenta and orange colored circles) from a Poisson distribution (see Eq. 2), yielding (e.g.) the set of firing rates shown by the heat maps at left. The center of mass of each population is marked with an x. The visual (magenta) and proprioceptive (orange) neural populations each encode the location of the hand, but in different spaces: 2D Cartesian space and joint space, respectively, drawn in outline in the heat maps. Since the neurons' preferred stimuli uniformly tile their respective spaces (indicated by the grids), but the forward kinematics relating these variables is nonlinear (inset; joint limits are indicated with red shading, joint origins with black lines), hand position is encoded differently by the two populations. These population codes also constitute the input layer, R, of the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Model generalization across input discrepancies and input gains. After training, the model was tested on data that differ from its training distribution. (A) Discrepant-input data: PROP input (orange) is shifted by progressively greater increments of the input covariance (see text), leading to suboptimal integration, as expected, and structured error distributions. The hidden-layer error mean, like the optimal error mean, shifts rightward with the PROP ''bias.'' (B) Gain-modulated data: The training data had gains between 12 and 18. Testing on gains (ratios listed between panels (B) and (C)) outside this regime yields suboptimal error covariances but essentially zero biases. (C) Gain-modulated, input-discrepant data: As the relative reliability of PROP is increased, the optimal estimate shifts toward PROP and away from VIS. The green and black dotted lines, nearly identical, trace this movement for the machine-based and optimal estimates, resp. For larger discrepancies (not shown), this optimal behavior breaks down, the green and black lines diverging. doi:10.1371/journal.pcbi.1003035.g005</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Other data distributions. (A,B) : Learning a prior. The network was trained on population codes of an underlying stimulus that was drawn from a Gaussian (rather than uniform, as in the previous figures) prior. This makes the MAP estimate tighter (cf. the black ellipses here and in Fig.2B) -and indeed the RBM-based estimate's error covariance is correspondingly tighter. (A) Conditional estimate statistics (color scheme as throughout): The output estimates (green) have smaller covariances, but they, like the optimal estimates (black) are also biased toward the mean of the prior, located at the center of the workspace. The match between them is evidently very good. Note that the stimulus location for each of these conditional statistics is eight standard deviations from the mean of the prior-so the model has generalized well to points that constituted a trivial fraction of the training data. (B) Marginal error statistics. (C,D): Learning that the inputs need not report the same stimulus. (C) A graphical model showing the independence relationships holding among the variables of this model. The (observed) toggle T determines whether the visual population is reporting the left (S L ) or right (S R ) hand. (D) Marginal error statistics (colors as throughout) for the mean of the posterior distribution over the right hand. Since the visual population provides information about the right hand only 70% of the time, the optimal error covariance is broader than its counterpart in Fig. 2B. The RBM-based estimate again nearly matches it. doi:10.1371/journal.pcbi.1003035.g006</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 6. Other data distributions. (A,B) : Learning a prior. The network was trained on population codes of an underlying stimulus that was drawn from a Gaussian (rather than uniform, as in the previous figures) prior. This makes the MAP estimate tighter (cf. the black ellipses here and in Fig.2B) -and indeed the RBM-based estimate's error covariance is correspondingly tighter. (A) Conditional estimate statistics (color scheme as throughout): The output estimates (green) have smaller covariances, but they, like the optimal estimates (black) are also biased toward the mean of the prior, located at the center of the workspace. The match between them is evidently very good. Note that the stimulus location for each of these conditional statistics is eight standard deviations from the mean of the prior-so the model has generalized well to points that constituted a trivial fraction of the training data. (B) Marginal error statistics. (C,D): Learning that the inputs need not report the same stimulus. (C) A graphical model showing the independence relationships holding among the variables of this model. The (observed) toggle T determines whether the visual population is reporting the left (S L ) or right (S R ) hand. (D) Marginal error statistics (colors as throughout) for the mean of the posterior distribution over the right hand. Since the visual population provides information about the right hand only 70% of the time, the optimal error covariance is broader than its counterpart in Fig. 2B. The RBM-based estimate again nearly matches it. doi:10.1371/journal.pcbi.1003035.g006</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Other architectures. (A,B) A ''hierarchical'' network, in which a third modality must be integrated with the integrated estimate from the first stage-which is just the original model. (A) Data generation (bottom), population coding (middle), and network architecture (cf. Fig.1A). Input units are Poisson and hidden units (green) are Bernoulli. The population codes, depicted in one dimension for simplicity, are actually 2D. Each hidden layer has half (~900) the number of units in its input layer (~1800). (B) Marginal error statistics. The error ellipses for PROP 1 (orange), for VIS (magenta), for both PROP 1 and VIS (dashed black), and for ''integrated representation 1'' (dashed green) replicate the results from Fig.2B. PROP 2 is encoded in the same way as PROP 1 (though their activities on a given trial are never equal because of the Poisson noise), and so has identical error statistics (orange). Conditioning on this third population in addition to the other two shrinks the optimal error covariance (solid black), and the estimate decoded from ''integrated representation 2'' (solid green) is correspondingly smaller as well, and again nearly optimal. (C,D) Coordinate transformation. (C) Data generation (bottom), population coding (middle), and network architecture (top). Each input population (bottom panel, color coded) depends on its own gain; whereas, both PROP (h, orange) and VIS (x, magenta) depend on the stimulus (hand position), and both VIS and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Tuning curves in the hidden layer. (A) Tuning for the multisensory-integration model/data (Figs.1 and 2). The left column shows tuning curves in the space of joint angles for sixteen randomly chosen hidden units; the right column shows those same units for the arc of reach endpoints from<ref type="bibr" target="#b28">[29]</ref>. The top row shows tuning curves for the trained model; the second row shows the same curves for the untrained model. The location of the arc in joint space is shown by the black slash through the tuning curves in the left column. Whereas the left-column tuning curves were collected for a single gain (g~15), the right-column curves were collected for g~12 (blue), g~15 (green), and g~18 (red) (the same gain was used for both populations, VIS and PROP). (B) Example hidden-unit tuning curves from the coordinate transformation model for body-centered hand position (T body :~L cos(H); see text for details), for two different gaze positions (red and green curves). The dashed blue curves show where the red tuning curves would lie for the second gaze position if they shifted completely with the eyes, as illustrated by the red arrows, i.e. if they were retinotopic. Some cells (second column) are; some are body-centered (first column); some partially shift (third column); and some even shift in the opposite direction of the gaze angle. (C) Coordinate-transforming cells can be tuned for any of the variables on the continuum from gaze angle (E), to retinotopic hand position (T ret :~X ~Tbody {E), to body-centered hand position (T body ), to body-centered hand position plus gaze angle (T body zE). The histogram shows the distributions of such tunings in the hidden layer, using the analysis of<ref type="bibr" target="#b30">[31]</ref>. doi:10.1371/journal.pcbi.1003035.g008</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>i</head><label></label><figDesc>Bern½v i Ds(fW rzb v g i ) ð4aÞR*q(rDv)~P j Pois½r j Dexp(fW T vzb r g j ),</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>PLOS Computational Biology | www.ploscompbiol.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>April 2013 | Volume 9 | Issue 4 | e1003035</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Base code for training a deep belief network with contrastive divergence was taken from <rs type="funder">Salukhutdinov and Hinton</rs> <ref type="bibr" target="#b66">[67]</ref>. <rs type="person">Jeff Beck</rs> helpfully suggested the fractional information loss measure.</p></div>
			</div>
			<div type="funding">
<div><p>This work was supported by <rs type="funder">Reorganization and Plasticity to Accelerate Injury Recovery</rs> (REPAIR; <rs type="grantNumber">N66001-10-C-2010</rs>, <ref type="url" target="http://www.darpa.mil/">http://www.darpa.mil/</ref>) and <rs type="funder">NIH NEI</rs> (<rs type="grantNumber">EY015679</rs>, <ref type="url" target="http://www.nih.gov/">http://www.nih.gov/</ref>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_84Egg45">
					<idno type="grant-number">N66001-10-C-2010</idno>
				</org>
				<org type="funding" xml:id="_pc2Y8HS">
					<idno type="grant-number">EY015679</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supporting Information</head><p>Text S1 Derivation of the optimal posterior for multisensory integration, coordinate transformation, and sometimes-decoupled inputs; notes on the fractional information loss; a rationale for the number of hidden units; and a note on the tuning of coordinate-transforming neurons. (PDF)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head><p>Conceived and designed the experiments: JGM MRF. Performed the experiments: JGM. Analyzed the data: JGM. Wrote the paper: JGM PNS. Supplied the intuitions: MRF. Supplied the concepts: JGM.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Integration of proprioceptive and visual positioninformation: An experimentally supported model</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Van Beers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sittig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jjd</forename><surname>Van Der Gon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="1355" to="1364" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian inference with probabilistic population codes</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Latham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1423" to="1438" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Humans integrate visual and haptic information in a statistically optimal fashion</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Banks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="page" from="429" to="433" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The ventriloquist effect results from near-optimal bimodal integration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Alais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current biology : CB</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="257" to="262" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian integration in sensorimotor learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">427</biblScope>
			<biblScope unit="page" from="244" to="247" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Noise characteristics and prior expectations in human visual speed perception</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Stocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="578" to="585" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Bayesian brain: the role of uncertainty in neural coding and computation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Neurosciences</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="712" to="719" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Plasticity in Human Sensorimotor Control</title>
		<author>
			<persName><forename type="first">R</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="page" from="455" to="462" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vision calibrates sound localization in developing barn owls</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>Knudsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Knudsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="3306" to="3313" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generalization to local remappings of the visuomotor coordinate transformation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="7085" to="7096" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual-shift adaptation is composed of separable sensory and task-dependent effects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lmm</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Sabes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neurophysiology</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">2827</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Applications of prism adaptation: a tutorial in theory and method</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Redding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rossetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience and biobehavioral reviews</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="431" to="444" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Movement-produced stimulation in the development of visually guided behavior</title>
		<author>
			<persName><forename type="first">R</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiological Psychology</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="872" to="876" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-modal plasticity in cortical development: differentiation and specification of sensory neocortex</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Pallas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Roe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TINS</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="341" to="345" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Role of Afferent Activity in the Development of Cortical Specification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Lyckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Results and Problems in Cell Differentiation</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="139" to="156" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training Products of Experts by Minimizing Contrastive Divergence</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multisensory integration during motor planning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Sabes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="6982" to="6992" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural correlates of reliability-based cue weighting during multisensory integration</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Fetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Deangelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Angelaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="146" to="154" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flexible strategies for sensory integration during motor planning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Sabes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="490" to="497" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Causal inference in multisensory perception</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Ko ¨rding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Beierholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Quartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">943</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Merging the senses into a robust percept</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><surname>Hh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="162" to="169" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sensory transformations and the use of multiple reference frames for reach planning</title>
		<author>
			<persName><forename type="first">Lmm</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Sabes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1056" to="1061" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Cross-Modal Spatial Transformations through Spike Timing-Dependent Plasticity</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fregnac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="5604" to="5615" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Models of the posterior parietal cortex which perform multimodal integration and represent space in several coordinate frames</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cognitive neuroscience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="601" to="614" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transfer of Coded Information from to Motor Networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Abbott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="6461" to="6474" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient computation and cue integration with noisy population codes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dene `ve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Latham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="826" to="831" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatial invariance of visual receptive fields in parietal cortex neurons</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Duhamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bremmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Grafw</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">389</biblScope>
			<biblScope unit="page" from="845" to="848" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Heterogeneous representations in the superior parietal lobule are common across reaches to visual and proprioceptive targets</title>
		<author>
			<persName><forename type="first">Lmm</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Sabes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="6661" to="6673" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reference frames for representing visual and tactile locations in parietal cortex</title>
		<author>
			<persName><forename type="first">M</forename><surname>Avillac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dene `ve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Duhamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="941" to="949" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coding of the Reach Vector in Parietal Area 5d</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Bremner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="342" to="351" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dorsal premotor neurons encode the relative position of the hand, eye, and goal during reach planning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pesaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="125" to="134" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The posterior parietal cortex: sensorimotor interface for the planning and online control of visually guided movements</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Buneo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="2594" to="2606" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ventral Intraparietal Area of the Macaque : Congruent Visual and Somatic Response Properties Ventral Intraparietal Area of the Macaque : Congruent Visual and Somatic Response Properties</title>
		<author>
			<persName><surname>Duhamel</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Colby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Colby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="126" to="136" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combination of Hand and Gaze Signals During Reaching: Activity in Parietal Area 7m of the Monkey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ferraina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Garasto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ercolani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bianchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="1034" to="1038" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The cortical connections of area V6: an occipito-parietal network processing visual information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Galletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gamberini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Kutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fattori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luppino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European journal of neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1572" to="1588" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Where is my arm? The relative role of vision and proprioception in the neuronal representation of limb position</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Graziano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="10418" to="10421" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A visuo-somatomotor pathway through superior parietal cortex in the macaque monkey: cortical connections of areas V6 and V6A</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zeki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European journal of neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="3171" to="3193" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multiple Levels of Representation of Reaching in the Parieto-frontal Network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Battaglia-Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caminiti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lacquaniti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Sapienza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral cortexCerebral cortex</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1009" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spatial maps for the control of movement and Charles G Gross</title>
		<author>
			<persName><forename type="first">Msa</forename><surname>Graziano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Neurobiology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="195" to="201" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cortical networks for visual reaching: physiological and anatomical organization of frontal and parietal lobe arm regions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ferraina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caminiti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral cortex</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="102" to="119" />
			<date type="published" when="1991">1996. 1991</date>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Corticocortical connections of visual, sensorimotor, and multimodal processing areas in the parietal lobe of the macaque monkey</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of comparative neurology</title>
		<imprint>
			<biblScope unit="volume">428</biblScope>
			<biblScope unit="page" from="112" to="137" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Premotor and parietal cortex: corticocortical connectivity and combinatorial computations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Wise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boussaoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caminiti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of neuroscience</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="25" to="42" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Idiosyncratic and systematic aspects of spatial representations in the macaque parietal cortex</title>
		<author>
			<persName><forename type="first">Swc</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="7951" to="7956" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sensory integration for reaching: Models of optimality in the context of behavior and the underlying neural circuits</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Sabes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in brain research</title>
		<imprint>
			<biblScope unit="volume">191</biblScope>
			<biblScope unit="page" from="195" to="209" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Computing with continuous attractors: stability and online aspects</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amari</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2215" to="2239" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">How each movement changes the next: an experimental and theoretical study of fast adaptive priors in reaching</title>
		<author>
			<persName><forename type="first">T</forename><surname>Verstynen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Sabes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience : the official journal of the Society for Neuroscience</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="10050" to="10059" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fo ¨ldia ´k P (1993) The &apos;ideal homunculus&apos;: Statistical inference from neural population responses</title>
	</analytic>
	<monogr>
		<title level="m">Computation and neural systems</title>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Eeckman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bower</surname></persName>
		</editor>
		<meeting><address><addrLine>Norwell, MA: Norwell, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sparse coding with an overcomplete basis set: A strategy employed by V1?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Factorial learning and the EM algorithm</title>
		<author>
			<persName><forename type="first">Z ;</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Leen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<meeting><address><addrLine>Cambridge (MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Possible principles underlying the transformation of sensory messages</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Barlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensory communication</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="217" to="234" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Self-organizing neural network that discovers surfaces in random-dot stereograms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="page" from="161" to="163" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A rational analysis of the acquisition of multisensory representations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacobs</forename><surname>Ra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="305" to="332" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A computational perspective on the neural basis of multisensory spatial representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dene `ve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Duhamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Visuomotor transformations underlying arm movements toward visual targets: a neural network model of cerebral cortical operations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Burnod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grandguillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ferraina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1435" to="1453" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Some informational aspects of visual perception</title>
		<author>
			<persName><forename type="first">F</forename><surname>Attneave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="183" to="193" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Redundancy reduction revisited</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Barlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network (Bristol, England)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="241" to="253" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Probabilistic framework for the adaptation and comparison of image codes</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lewicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Opt Soc Am</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1587" to="1601" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Efficient coding of natural sounds</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lewicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="356" to="363" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Natural image coding in V1: how much use is orientation selectivity?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Eichhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Abbott</surname></persName>
		</author>
		<title level="m">Theoretical Neuroscience</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="101" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Exponential Family Harmoniums with an Application to Information Retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1481" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Mccullagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Nelder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Chapman and Hall/CRC</publisher>
			<biblScope unit="page" from="26" to="32" />
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
	<note>Generalized Linear Models. 2 nd edition</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Minimum Probability Flow Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Deweese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="905" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Storage of a sensory pattern by anti-Hebbian synaptic plasticity in an electric fish</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caputit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Serrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="4650" to="4654" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A mechanism for the Hebb and the anti-Hebb processes underlying learning and memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="9574" to="9578" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Reducing the Dimensionality of Data with Neural Networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
