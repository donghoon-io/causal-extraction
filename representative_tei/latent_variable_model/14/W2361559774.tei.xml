<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Streaming-LDA: A Copula-based Approach to Modeling Topic Dependencies in Document Streams</title>
				<funder ref="#_kBx89MB">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hesam</forename><surname>Amoualian</surname></persName>
							<email>hesam.amoualian@imag.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Grenoble Alps</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<country>LIG</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
							<email>eric.gaussier@imag.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University of Grenoble Alps</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<country>LIG</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marianne</forename><surname>Clausel</surname></persName>
							<email>marianne.clausel@imag.fr</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">University of Grenoble Alps</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<country>LJK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Massih-Reza</forename><surname>Amini</surname></persName>
							<email>massih-reza.amini@imag.fr</email>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">University of Grenoble Alps</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>KDD &apos;16 August 13-17</addrLine>
									<postCode>2016</postCode>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country>LIG USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Streaming-LDA: A Copula-based Approach to Modeling Topic Dependencies in Document Streams</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2939672.2939781</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T18:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Latent Dirichlet allocation</term>
					<term>Copulas</term>
					<term>Document Streams</term>
					<term>Topic Dependencies</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose in this paper two new models for modeling topic and word-topic dependencies between consecutive documents in document streams. The first model is a direct extension of Latent Dirichlet Allocation model (LDA) and makes use of a Dirichlet distribution to balance the influence of the LDA prior parameters wrt to topic and word-topic distribution of the previous document. The second extension makes use of copulas, which constitute a generic tools to model dependencies between random variables. We rely here on Archimedean copulas, and more precisely on Franck copulas, as they are symmetric and associative and are thus appropriate for exchangeable random variables. Our experiments, conducted on three standard collections that have been used in several studies on topic modeling, show that our proposals outperform previous ones (as dynamic topic models and temporal LDA), both in terms of perplexity and for tracking similar topics in a document stream.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The recent proliferation of temporal textual data on the Internet such as Tweets or comments on Youtube has brought new challenges for learning with interdependent data. Though important progress has been made in some directions <ref type="bibr">[8]</ref>, popular approaches for most of these tasks are designed to deal with static collections of documents. This is specially the case for latent topic modeling, albeit analyzes of social content have gained much attention in recent years for different aspects of daily life, such as latent health-related topic analysis <ref type="bibr" target="#b19">[19]</ref> or buzz detection <ref type="bibr" target="#b20">[20]</ref>.</p><p>Although the main goal of probabilistic modeling is to find word topics, an equally interesting objective is to examine topic evolutions and transitions. The seminal work of <ref type="bibr" target="#b4">[4]</ref> proposed to model the dynamic evolution of topics by first grouping documents into time slices and then to chain the evolution of both the word-topic and topic mixture distributions via a Gaussian process. In some cases, the Gaussian distribution was not found to be the appropriate distribution in modeling the topic shifts and some studies considered other probability distributions for capturing the evolution of topics over time <ref type="bibr" target="#b22">[22]</ref>. However, the idea of grouping documents into epochs for modeling topic evolution was echoed in a number of studies. For example, <ref type="bibr" target="#b24">[24]</ref> estimated a transition matrix over topic vectors between two predefined epochs and they showed that the LDA model <ref type="bibr" target="#b5">[5]</ref> can be enhanced by considering directly the evolution of the topics over time.</p><p>In this paper we propose two extensions of LDA for modeling the dependency between two consecutive documents in a stream. In our first model, we suppose that the dependency between topic distributions of two consecutive documents follows a Dirichlet distribution controlled by an hyperparameter. This model is similar to the one of <ref type="bibr" target="#b4">[4]</ref> with time slices equal to 1, but it offers a more precise mechanism for controlling the dependencies and is based on a framework encompassing all the situations (from complete independence to plain equality). This first study paves the way for a more general topic model in which the dependencies between the topics of two consecutive documents are captured by copulas which constitute generic tools to model dependencies between random variables <ref type="bibr" target="#b6">[6]</ref>. Among the several families of copulas that have been defined in the literature, our choice fell on Archimedean copulas <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref> as they are symmetric and associative, necessary conditions when dealing with exchangeable random variables <ref type="bibr" target="#b18">[18]</ref>. More particularly, we use Franck copulas, a special case of Archimedean copulas that rely on a single parameter, easier to estimate and more robust to sparse data. Using three collections with different characteristics, we show that our approaches are faster and improve over state-of-the-art topic models. We also analyze the precision of our models to track the topics on a labeled dataset.</p><p>The outline of this paper is as follows. In the next section, we present our models. In Section 3, we introduce an efficient procedure to estimate the most important, in terms of size, parameters. We then describe in Section 4 the experimental results obtained with our approaches on three distinct datasets. In Section 5, we position our work with respect to the state of the art. Finally, Section 6 concludes our study by summarizing its main results and by giving some pointers to future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">STREAMING LDA</head><p>Latent Dirichlet Allocation (LDA, <ref type="bibr" target="#b5">[5]</ref>) is a probabilistic Bayesian model used to describe a corpus of D documents, associated with a vocabulary of size V . In this model, latent variables, indexed in {1, • • • , K}, are used to represent the hidden (in the sense non-observed) topics underlying each document. LDA is associated to the following generative model 1 :</p><p>• Generate, for each topic k, 1 ≤ k ≤ K, a distribution over the words: φ k ∼ Dir(β), where φ k and β are V dimensional vectors;</p><p>• For each document d:</p><p>-Choose a distribution over the topics:</p><formula xml:id="formula_0">θ d ∼ Dir(α),</formula><p>where θ d and α are K dimensional vectors; -For each position (indexed by n, 1 ≤ n ≤ N ) in d:</p><p>(a) Choose a topic assignment:</p><formula xml:id="formula_1">z d n ∼ mult(1, θ d ); (b) Choose the word w d n from the topic z d n with probability P (w d n = v|z d n = k) = φ k,v ;</formula><p>where N is the length of each document and φ k,v is the v th coordinate of φ k . α and β correspond to the priors of the model. They are usually fixed, following <ref type="bibr" target="#b5">[5]</ref>. Furthermore, in almost all previous studies on LDA, the priors are considered to be symmetric, each coordinate of the vector being equal: α1 = • • • = αK . If one assumes a broad Gamma prior for both α and β, then their value can be easily learned from data by maximum a posteriori <ref type="bibr" target="#b1">[1]</ref> or Markov Chain Monte Carlo <ref type="bibr" target="#b15">[15]</ref> methods. One can also envisage learning asymmetric Dirichlet priors <ref type="bibr" target="#b21">[21]</ref>, which raises no particular difficulties for the models we are considering. For clarity sake, we however assume here fixed, symmetric priors; the extension to their learning through Gamma priors or through asymmetric priors is purely technical. In the remainder, we will denote by α and β the priors for the Dirichlet distributions as well the constant value taken by each coordinate of these priors, the context being sufficient to determine which element is referred to.</p><p>An important characteristic of LDA is that each document is generated independently from the previous ones. This is not a realistic assumption in different settings, as document streams, and we introduce below two extensions of LDA that model such dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dirichlet-based dependencies</head><p>We introduce here a first extension of LDA, that we refer to as ST-LDA-D. 1 For simplification and following standard practice, we do not model here the length of each document, assumed to be fixed and equal to N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Presentation of the model</head><p>In this first model, we rely on a direct extension of the LDA model to take into account dependencies between the document-specific topic distributions of two sequential documents, denoted (d -1) and d (2 ≤ d ≤ D). This extension uses, as the standard LDA model, Dirichlet distributions for the document-specific topic distributions, the parameters of which are linear combination of the standard prior α and the topic distribution estimated in the previous document:</p><formula xml:id="formula_2">θ d |θ d-1 ∼ Dir(α + λ d θ d-1 )<label>(1)</label></formula><p>where λ d is a uniformly distributed parameter that controls the influence of the topics of document (d -1) on the topics of document d (see Figure <ref type="figure" target="#fig_0">1</ref>). The expectation of each component of θ d is given by:</p><formula xml:id="formula_3">E[θ d i |θ d-1 i ] = α + λ d θ d-1 i Kα + λ d<label>(2)</label></formula><p>Hence, if λ d is high, i.e. if document d covers the same topics as document (d -1), then</p><formula xml:id="formula_4">E[θ d i |θ d-1 i ] ≈ θ d-1 i</formula><p>. We furthermore assume that the previous document, (d -1), can influence the word-topic distributions of the current document d. This assumption, also made in dynamic topic models <ref type="bibr" target="#b4">[4]</ref> and topic tracking models <ref type="bibr" target="#b11">[11]</ref>, is motivated by the fact that, within a given topic, if word distributions evolve over time, they tend to do so in a smooth way. As before, one can use a direct extension of the LDA model to account for dependencies between word-topic distributions in sequential documents:</p><formula xml:id="formula_5">∀k, 1 ≤ k ≤ K, φ d k |φ d-1 k ∼ Dir(β + µ d φ d-1 k )<label>(3)</label></formula><p>Here µ d is again a uniformly distributed parameter that controls the tradeoff between the prior β and the learned topicword distributions φ d-1 . As usual φ d-1 k is the word distribution of topic k. The conditional mean of each component of φ d k is given by:</p><formula xml:id="formula_6">E[φ d k |φ d-1 ] = β + µ d φ d-1 k V β + µ d<label>(4)</label></formula><p>and is approximately the value of the same component of document (d -1) when the two documents are strongly dependent.</p><p>Lastly, as one can note, by setting λ d = µ d = 0, ∀d, 2 ≤ d ≤ D, one "forgets" the dependencies between consecutive documents. The streaming model is in this case identical to the standard LDA model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Inference with Gibbs sampling</head><p>As mentioned before, the parameters α and β are considered fixed. The other parameters can be estimated through Gibbs sampling, with Metropolis-Hasting updates for the parameters λ d and β d . We give here the update formulas of each parameter.</p><p>For θ, one has:</p><formula xml:id="formula_7">θ d ∼ P (θ|θ d-1 , z d , w d , α, β, λ d , φ d-1 , φ d , µ d ) = B(α)B(α + λ d θ d-1 + Ω d ) B(α + Ω d )B(α + λ d θ d-1 ) × Dir(Ω d + α + λ d θ d-1 ) (5) w w z z θ d-1 θ d φ β N N (a) DMM w w z z θ d-1 θ d φ d-1 φ d α α β β N N (b) TTM w w z z θ d-1 θ d φ d-1 φ d α d-1 α d N N (c) DTM w w z z θ d-1 θ d θ d-h φ d-1 φ d α β T-Matrix N N . . . . (d) TM-LDA w w z z θ d-1 θ d φ d-1 φ d λd-1 λd µd-1 µd α β N N (e) ST-LDA-[D|C]</formula><p>Figure <ref type="figure" target="#fig_0">1</ref>: Graphical models for Dynamic Mixture Models (DMM, <ref type="bibr" target="#b25">[25]</ref>), Topic Tracking Models (TTM, <ref type="bibr" target="#b11">[11]</ref>), Dynamic Topic Models (DTM, <ref type="bibr" target="#b4">[4]</ref>), Temporal LDA (TM-LDA, <ref type="bibr" target="#b24">[24]</ref>) and Streaming-LDA (ST-LDA-[D|C])</p><p>where Ω d is defined as in <ref type="bibr" target="#b23">[23]</ref> and represents the d th row of the D × K count matrix Ω, with Ω d,k being the number of times that topic k is assigned to words in document d.</p><p>The update for</p><formula xml:id="formula_8">φ d k , 1 ≤ k ≤ K is similar: φ d k ∼ P (φ k |θ d-1 , θ d , z d , w d , α, β, λ d , φ d-1 , µ d ) = B(β)B(β + µ d φ d-1 k + Ψ k ) B(β + Ψ k )B(β + µ d φ d-1 k ) × Dir(Ψ k + β + µ d φ d-1 k )<label>(6)</label></formula><p>where Ψ k is again defined as in <ref type="bibr" target="#b23">[23]</ref> and represents the k th row of a K × V count matrix, Ψ k,v being the number of times that topic k is assigned to word v in the documents seen so far.</p><p>The Gibbs update for z is the same as the one for the standard LDA model:</p><formula xml:id="formula_9">∀k, 1 ≤ k ≤ K, P (z d v = k|θ d , φ d ) = θ d k × φ d k,v j θ d j × φ d j,v<label>(7)</label></formula><p>Finally, for λ d and µ d , one can not directly compute Gibbs updates as the normalizing factor for the distribution of λ given all the other parameters can not be computed exactly. One can nevertheless rely on a Metropolis-Hasting procedure, detailed in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Copula-based dependencies</head><p>Model ST-LDA-D captures topic and word-topic dependencies through Dirichlet distributions, which allow one to balance the influence of the priors (α and β) and of the topic and topic-word distributions of the previous document. We introduce now another extension of LDA in which the dependencies between the topics of consecutive documents are modeled through copulas, which constitute a generic tool to model dependencies and do not rely on a specific distribution. We first provide a brief overview of copulas, prior to describe our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Basics on copulas</head><p>For every p ≥ 2, a p-dimensional copula is a p-variate density function on [0, 1] p , whose univariate marginals are uniformly distributed on [0, 1]. Copulas are particularly useful when modeling dependencies between random variables. Indeed, the joint cumulative distribution function (CDF)</p><formula xml:id="formula_10">FX 1 ,••• ,Xp of any random vector X = (X1, • • • , Xp</formula><p>) can be written as a function of its marginals, as follows:</p><formula xml:id="formula_11">Theorem 1 (Sklar's theorem Theorem 2.3.3 of [16]) Let FX 1 ,••• ,Xp be a p-dimensional distribution function with marginals FX 1 , • • • , FX p .</formula><p>Then there exists a copula C with uniform marginals such that:</p><formula xml:id="formula_12">FX 1 ,••• ,Xp (x1, • • • , xp) = C(FX 1 (x1), • • • , FX p (xp)) Furthermore, when the CDF FX 1 ,••• ,Xp is continuous, the copula is unique.</formula><p>Copulas represent a general way of modeling the dependencies between random variables, from complete independence to equality. If the random variables X1, • • • , Xp are pairwise independent, their copula is the so-called independency copula:</p><formula xml:id="formula_13">FX 1 ,••• ,Xp (x1, • • • , xp) = FX 1 (x1) • • • FX p (xp)</formula><p>whereas in the case X1 = • • • = X d , one gets the comonotonicity copula:</p><formula xml:id="formula_14">FX 1 ,••• ,Xp (x1, • • • , xp) = min i∈{1,••• ,p} FX i (xi)</formula><p>Several copula families have been defined in the literature, among which the Archimedean copulas <ref type="bibr" target="#b16">([16,</ref><ref type="bibr">Ch. 4]</ref>), particularly interesting in our case. A p-dimensional Archimedean copula C with generator ψ is defined as:</p><formula xml:id="formula_15">Cp(u; ψ) := ψ(ψ -1 (u1) + • • • + ψ -1 (up)), u ∈ [0, 1] p</formula><p>where ψ is a continuous, decreasing function, from [0, ∞] to (0, 1), strictly decreasing on [0, inf{t : ψ(t) = 0}], and satisfying:</p><formula xml:id="formula_16">ψ(0) = 1, ψ(∞) = lim t→∞ ψ(t) = 0</formula><p>Archimedean copulas have the following interesting properties:</p><p>• They are symmetric, that is invariant by any permutation of their coordinates, which is important when dealing with exchangeable random variables, as is the case here<ref type="foot" target="#foot_0">foot_0</ref> ;</p><p>• They are associative: for any (u1,</p><formula xml:id="formula_17">• • • , up) ∈ [0, 1] p , one has: Cp-1(C2(u1, u2; ψ), u3, • • • , up; ψ) = Cp-1(u, • • • , up-2, C2(up-1, up; ψ); ψ)</formula><p>This means that the dependency properties are the same whatever the way we group the random variables.</p><p>In this study, we further consider a particular case of the Archimedean copulas, namely the one-parameter family of Franck copula, defined, for any λ ∈ R \ {0}, as:</p><formula xml:id="formula_18">C λ (u, v) = -(1/λ) ln(1 + (e -λu -1)(e -λv -1) e -λ -1 )<label>(8)</label></formula><p>When λ → 0, one approaches the independency copula, whereas λ = ∞ yields the comonotonicity copula. Lastly, for any λ ∈ R \ {0}, C λ is twice differentiable on [0, 1] 2 so that the copula function admits a density, denoted in the sequel c λ . By varying λ from 0 to ∞, Franck copula allows one to model all the possible dependencies between two random variables, from complete independency to equality. Dependency/independency is furthermore controlled by a single parameter, λ, which makes parameter estimation both easier and more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Generative process</head><p>Instead of generating the topic distribution of each document θ d independently, as is done in standard LDA we bind, as for our first model, ST-LDA-D, the topic distributions θ d-1 and θ d of consecutive documents, this time by using copulas, and more precisely Franck copula.</p><p>One can not however directly use Sklar's theorem as it does not extend to joint distributions over random vectors. This means that if we are given two random vectors X1, X2, one can not claim that there exists a copula C such that, for any (x1, x2)</p><formula xml:id="formula_19">∈ [0, 1] p 1 × [0, 1] p 2 : F X 1 ,X 2 (x1, x2) = C(F X 1 (x1), F X 2 (x2))</formula><p>except in very specific situation as when X1 and X2 are independent for example. One can nevertheless relate latent topics θ d-1 and θ d through their components. Indeed, the topic Dirichlet distribution can be decomposed into univariate Gamma distributions with parameters (α, 1), denoted Ga(α):</p><p>Theorem 2 (from Theorem 2.1 of <ref type="bibr" target="#b17">[17]</ref>) A random vector θ follows a Dirichlet distribution Dir(α) iff there exists a random vector</p><formula xml:id="formula_20">T ∼ Ga(α) ⊗ • • • ⊗ Ga(α) such that: θ (L) = T T 1<label>(9)</label></formula><p>where</p><formula xml:id="formula_21">(L)</formula><p>= means "equality in distribution". In addition, if we are given θ ∼ Dir(α) and R ∼ Ga(Kα) independent, then</p><formula xml:id="formula_22">T = Rθ ∼ Ga(α) ⊗ • • • ⊗ Ga(α).</formula><p>To bind the topic distributions θ d-1 and θ d of two consecutive documents, we thus consider the associated vectors T d-1 and T d , and bind them coordinate per coordinate using Franck copula. For the word-topic distributions, we use the same coupling between consecutive documents as the one used in model ST-LDA-D, as a tighter coupling through copulas would be too costly. We will come back to this issue in Section 3.</p><p>In the sequel for any γ &gt; 0, fγ (resp. Fγ) denotes the pdf (resp. cdf) of the Gamma distribution with parameters (γ, 1). The global generative model is thus as follows:</p><p>1. Generate the first document according to the standard LDA model 2. For each document d, 2 ≤ d ≤ D:</p><formula xml:id="formula_23">(a) Generate λ d ∼ U [0, τ λ ] (b) Generate µ d ∼ U [0, τµ] (c) For each topic k, 1 ≤ k ≤ K: • Generate T d k whose conditional density w.r.t. T d-1 k is: P (T d k |T d-1 k ) = fα(T d k ) c λ d (Fα(T d-1 k ), Fα(T d k )) • Generate φ d k |φ d-1 k ∼ Dir(β + µ d φ d-1 k ) (d) Set θ d = T d / T d 1 (e) For each word n, 1 ≤ n ≤ N in d:</formula><p>• Choose a topic assignment:</p><formula xml:id="formula_24">z d n ∼ mult(1, θ d ) • Choose the word w d n from the topic z d n with probability P (w d n |z d n ) = φ d z d n ,w d n</formula><p>where T d k represents the k th coordinate of the vector T d , and follows a distribution Ga(α) according to Theorem 2. We refer to the corresponding model as ST-LDA-C. Figure <ref type="figure" target="#fig_0">1</ref> provides a graphical representation of this model, together with the ones of previous models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Inference with Gibbs sampling</head><p>The updates for z d , φ d and µ d are identical to the ones for model ST-LDA-D. For λ d , one gets:</p><formula xml:id="formula_25">P (λ d |T d-1 , T d , z d , w d , α, β, φ d-1 , φ d , µ d ) ∝ P (λ d ) K k=1 fα(T d-1 k )fα(T d k )c λ (Fα(T d-1 k ), Fα(T d k ))</formula><p>The same Metropolis-Hasting procedure as the one used for model ST-LDA-D and detailed in Appendix A can then be used.</p><p>For θ d , one needs first to estimate the conditional probability of the random vector T d with respect to the other parameters. This expression can be factored as follows:</p><formula xml:id="formula_26">P (T d |T d-1 , z d , w d , α, β, λ d , φ d-1 , φ d , µ d ) = P (T d |T d-1 , α, λ d )P (z d |T d ) P (z d |α)</formula><p>As in the classical context of LDA, one has</p><formula xml:id="formula_27">P (z d |α) = B(Ω d + α)/B(Ω d )</formula><p>where Ω d is defined as before. By assumption on the distribution of the random vectors (T d-1 , T d ):</p><formula xml:id="formula_28">P (T d |T d-1 , α, λ d ) = K k=1 fα(T d k )c λ (Fα(T d-1 k ), Fα(T d k ))</formula><p>Developing P (z d |T d ) as detailed in Appendix B, finally leads to:</p><formula xml:id="formula_29">P (T d |T d-1 , z d , w d , α, β, λ d , φ d-1 , φ d , µ d ) ∝ ( K k=1 T d k ) -N × K k=1 f (Ω d,k +α-1) (T d k ) × c λ (Fα(T d-1 k ), Fα(T d k )) (<label>10</label></formula><formula xml:id="formula_30">)</formula><p>Each T d k can then be estimated through the Metropolis-Hasting procedure presented in Appendix A; θ d is finally obtained from T d through Eq. 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">COMPUTATIONAL CONSIDERATIONS</head><p>For model ST-LDA-C, the word-topic distributions φ d k (1 ≤ k ≤ K) could be estimated in the same way as θ d is estimated, as mentioned in Section 2.2. However, this would entail running K × V Metropolis-Hasting procedures, which is problematic as soon as the collections considered are relatively large. We thus proposed in Section 2.2 to estimate it through Eq. 6, as done for ST-LDA-D. This time, K × V Gibbs sampling updates are required. If this estimation procedure is faster, it may still be too slow for really large collections. Theorem 2 nevertheless suggests a way to approximate</p><formula xml:id="formula_31">φ d k (1 ≤ k ≤ K, 2 ≤ d ≤ D)</formula><p>through Gamma updates, as follows:</p><formula xml:id="formula_32">1. For each word v in d, generate t k,v ∼ Ga(β + φ d-1 k,v ) 2. For each word v in the vocabulary V, φ d k,v ← t k,v v∈V t k,v</formula><p>where β corresponds to the real parameter (i.e., the constant value that makes up the V dimensional vector of priors). The quantities t k,v are first initialized through t k,v ∼ Ga(β), and updated each time a new document is encountered.</p><p>As one can note, this update primarily concerns the words present in the current document (step 1), the components for the other words being just renormalized (step 2). This contrasts with Eq. 6 in which the contribution of all words is resampled for each document via a multivariate Dirichlet distribution. The above procedure simplifies this by relying on the univariate equivalent of the Dirichlet distribution, namely the Gamma distribution, and by binding the variables through the renormalization step. It is faster as it involves only K × N samplings from a Gamma distribution instead of K samplings from a multivariate, V (V &gt;&gt; N ) dimensional Dirichlet distribution (the K × V renormalizations in step 2 do not really harm the procedure and are negligible compared to the Dirichlet samplings). We have  observed in practice no difference, in terms of performance measures we consider (see <ref type="bibr">Section 4)</ref>, between this procedure and the more complex ones mentioned before, and make use of it in the remainder of the paper. In terms of speed, this procedure performed 1.5 times faster on the NIPS collection, which contains long documents and a relatively small vocabulary (ca. 12,000 words), and 2 times faster for the TDT4 and Tweets collections, which contain shorter documents with a larger vocabulary (up to 42,000 words). Algorithm 1 summarizes the inference process we rely on. It makes use of the above procedure to estimate φ, referred to as φ-procedure.</p><formula xml:id="formula_33">d k (1 ≤ k ≤ K); for each word v in d, topic assignment z d v // Initialization 1 for k = 1 to K, v ∈ V do 2 t k,v ∼ Ga(β)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL STUDY</head><p>We conducted a number of experiments aimed at evaluating how the proposed models behave on different collections by analyzing their stability, convergence time and performance.</p><p>Datasets. We performed experiments on three datasets with different characteristics. The NIPS dataset contains 1,500 scientific papers with no time dependency between them. The size of the vocabulary is 12,375 and documents contain 500 unique words in average. The collection was collected from the NIPS proceedings and is relatively homogeneous in terms of the topics covered. It allows us to assess whether topic dependencies are still useful in a "loose" context in which there is no more temporal dependency. It is available at the UCI ML Repository <ref type="bibr" target="#b12">[12]</ref>.</p><p>The Multilingual Text and Annotations data set (TDT4)<ref type="foot" target="#foot_1">foot_1</ref> proposed for topic detection and tracking, has 3,190 original documents in English and a vocabulary size of 22,965. Documents here are newswires extracted from different broadcasts and the number of unique words per document is 100 in average. Even though newswires are not extracted from the same source, they are ranked by the time.</p><p>The Tweets dataset is collected using Twitter's streaming API during 20 days from 8/10/2014 to 27/10/2014. The collection contains 72,592 tweets and a vocabulary of size 42,336. Tweets have been sequenced by time and are filtered over health issues using an SVM classifier trained over MeSH categories <ref type="foot" target="#foot_2">4</ref> .</p><p>Each dataset was separated into training and test sets. The NIPS collection was randomly splitted into training (90% of the collection) and test (10% of the collection) sets. For TDT4, we used the first 2800 newswires released in time for training, and the last 390 ones for testing. For the Tweets dataset, we used the tweets issued in the first 17 days for training (60,000 documents) and those of the last 3 days (12,000 documents) for testing. Table <ref type="table" target="#tab_0">1</ref> summarizes the characteristics of these collections.</p><p>Evaluation.</p><p>Results are evaluated over the test set using the widely used perplexity measure that can be approximated by <ref type="bibr" target="#b5">[5]</ref>.  Furthermore, for the TDT4 collection we use the available semantic labels of newswires in the test set in order to evaluate the ability of the models to find documents of the same semantic labels using only their predicted topic distributions (Section 4.2). To this aim, we measure ROC curves and AUC of different topic models on TDT4.</p><formula xml:id="formula_34">perplexity(C test ) = exp     - d n log k θ d k × φ d k,v d n D test × N    <label>(</label></formula><p>Settings and comparisons. For all models, both hyperparameters α and β were fixed to 0.5. Documents of the NIPS dataset are initially stoplisted, we did not perform further preprocessing of the data nor removed stop words from the TDT4 and Tweets documents as for all methods best results are obtained when collections are not filtered.</p><p>To validate the streaming LDA models described in the previous section, we test the following six methods. The first two are LDA models <ref type="bibr" target="#b5">[5]</ref>: (a) LDA1, which consists in training an LDA model on the whole training data, then fixing φ and updating θ for each document in the test set, (b) LDA all , which consists in training an LDA model on the whole on training data and updating both φ and θ for each document in the test set. In addition, we consider two stateof-the-art latent models that take into account dependencies between topics: Dynamic Topic Model (DTM) <ref type="bibr" target="#b4">[4]</ref> and Temporal LDA (TM-LDA) <ref type="bibr" target="#b24">[24]</ref>. DTM is certainly the most popular model to take into account topic dependencies. It is furthermore complete in the sense that it integrates both topic and word-topic distributions. TM-LDA is a very recent proposal with nice features. Lastly, we also consider the two streaming LDA models we have introduced (ST-LDA-D and ST-LDA-C). For these last two models, τ λ (see Appendix A) is set to 30,000 <ref type="foot" target="#foot_3">5</ref> . All the algorithms were implemented in Python with Numpy and Scipy<ref type="foot" target="#foot_4">foot_4</ref> except DTM that is a C++ implementation tool from <ref type="bibr" target="#b3">[3]</ref>. For both training and test, DTM is used considering that each document corresponds to a time slice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The effect of streams of documents</head><p>We start our evaluation by analyzing the gains provided by modeling dependencies between topics by streaming (as with ST-LDA-D and ST-LDA-C) compared to other approaches on the different datasets. Figure <ref type="figure" target="#fig_2">2</ref> shows the evolution of perplexities of different models over the test set with respect to the training time of each model on NIPS and TDT4 datasets. The code program of DTM (in C++) generally executes faster than the other code programs (written in pyhton), nevertheless we ignore this detail and consider all the curves identically. To measure the perplexity for each model, we estimate θ and φ over respectively all documents and all words of the training set. These estimates are then used to evaluate iteratively new φ and θ distributions for each document in the test set. This iterative update of φ and θ is done for all of the methods except LDA1 which updates the distributions θ and φ over the whole documents in the test set with the last parameters that were obtained from the training set.</p><p>As expected, all perplexity curves decrease monotonically with respect to time. On both datasets, perplexity curves ST-LDA-D and ST-LDA-C lower-bound the other curves on all iterations. On the NIPS dataset, DTM becomes competitive with the two others, at the end of the iterations, while on TDT4, where test documents come in a stream, ST-LDA-C stands clearly as the best model. These results show the ability of ST-LDA-C to capture dependencies between topics in document streams. Further, we note that at the beginning of iterations where dependencies are not yet apparent, the perplexity curves of both models are very similar to the one of LDA all . This is in line with our assertion of the previous section supporting that both models reduce to LDA in the case where topics are independent. TM-LDA is not competitive in this setting as it does really not make advantage of the fact that the words in the new, arriving documents are known. Its ability to predict future topics is not exploited in this setting. The evolution of perplexity on Tweets from the three last consecutive days considered in our experiments is shown in Figure <ref type="figure" target="#fig_3">3</ref>. The behavior of perplexity curves here are accentuated with the total stream characteristics of Tweets; the curve of LDA all gets away from those of ST-LDA-C and ST-LDA-D, while DTM comes close. In order to see if the number of topics, that we fixed for all models to 80, have an impact on these results or not, we repeated the experiments by varying the number of topics in the set {20, 40, 60}.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ability to detect semantic correlations</head><p>We further investigate on the ability of models to find topics that can detect documents of the same semantic class. For doing so, we used the TDT4 collection for which some documents are assigned semantic classes by experts. We hence use the cosine measure or the λ d parameter of ST-LDA-C, to detect consecutive documents in the test set of this collection that are found similar on the basis of their topic distributions; two consecutive documents are considered as similar if the cosine measure of their topic distributions (resp. estimated λ d -line 13 Algorithm 1) is higher than a given threshold. If two consecutive and similar documents share the same semantic label, we count them as a true positive; if they do not share the same semantic label, we count them as false positive. By changing the threshold, we can plot the ROC curves for the corresponding method. In order to compare between the different ROC curves, we estimated the area under them, shown in Table <ref type="table" target="#tab_3">3</ref>. From these results it comes clear, that topic distributions found by ST-LDA-C are more able to detect these semantic classes than topic distributions of DTM and TM-LDA.   see, the distributions of topics in the three pairs of consecutive documents with high λ d are similar. In addition, the two most probable topics of the document pairs retained in Figure <ref type="figure">6</ref>, also taken from TDT4, do not share any word when λ d is small and are almost identical when λ d is high. These examples illustrate the fact that λ d is a good indicator of the topic dependencies between documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>Some studies have considered the possibility to model different streams of documents, as in <ref type="bibr" target="#b10">[10]</ref>, trying to leverage standard models (as LDA) by considering topics common to the different streams. In such studies the evolution of topics over time is not considered. The study presented in <ref type="bibr" target="#b22">[22]</ref> aims at modeling, through an extension of LDA, the timestamp associated with each token in a document. If dependencies between topics are not explicitly modeled, topics tend to specialize over different time periods through the joint dependence of each word and timestamp on the topic variable (z in LDA). Other studies have addressed the problem of topic evolution and dependencies within a single document, as the recent sequential LDA model described in <ref type="bibr" target="#b7">[7]</ref>. We rather focus in this study on explicitly modeling topic dependencies across documents, for both topic and wordtopic distributions. Several studies have addressed a similar problem. One of the first proposals corresponds to the Dynamic Topic Model (DTM), introduced in <ref type="bibr" target="#b4">[4]</ref> and illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. An interesting feature of DTM is its use of time slices; we have not considered time slices in this study, but our models (as most dynamic models) can be extended to deal with them. DTM captures dependencies for both topic and word-topic distributions. These dependencies are however captured through Gaussian distributions, the expectation of which corresponds to the previous parameters. This entails that new parameter values are constrained to be distributed around the values observed previously. In contrast, even in model ST-LDA-D, the expectations of the new topic and word-topic distributions (Eqs. 2 and 4) can be uncor-related to the previous distributions in the absence of dependencies. Our models thus offer additional flexibility over the presence or absence of dependencies between consecutive documents in a stream. The Dynamic Mixture Model (DMM, see Fig. <ref type="figure" target="#fig_0">1</ref>) introduced in <ref type="bibr" target="#b25">[25]</ref> is similar to DTM except that topic dependencies are directly considered at the topic level (as is the case for ST-LDA-D and ST-LDA-C but not for DTM which operates at the prior level) and that word-topic dependencies are dropped. As for DTM, the expectation of a new topic distribution is given by the values obtained in the previous document. This again contrasts with our proposal that introduces additional flexibility, as mentioned before. The Topic Tracking Model (TTM, see Fig. <ref type="figure" target="#fig_0">1</ref>) introduced in <ref type="bibr" target="#b11">[11]</ref> is similar to our models in the sense that both topic and word-topic (more precisely interest-topic) dependencies are considered. However, as for DTM and DMM, the mean of the current topics and interests are the same as the ones of the previous topics and interests. The model is thus again limited in its ability to model the presence or absence of dependencies between consecutive documents. A more recent proposal, called Temporal LDA (TM-LDA, see Fig. <ref type="figure" target="#fig_0">1</ref>), was introduced in <ref type="bibr" target="#b24">[24]</ref>. TM-LDA differs from the previous models as it also aims at predicting future topics even in the situation where future documents are not seen. It thus assumes a strong dependency between consecutive documents, which is not always realistic, even on such collections as Tweets. Furthermore, TM-LDA does not consider dependencies for the word-topic distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>We have proposed in this paper two new models for modeling topic and word-topic dependencies between consecutive documents in document streams. The first model is a direct extension of Latent Dirichlet Allocation model (LDA) and makes use of a Dirichlet distribution to balance the influence of the LDA prior parameters wrt to topic and wordtopic distribution of the previous document. The second extension makes use of copulas, which constitute a generic tool to model dependencies between random variables. Our experiments, conducted on three standard collections that have been used in several studies on topic modeling, show that our proposals outperform previous ones (as dynamic topic models and temporal LDA), both in terms of perplexity and for tracking similar topics in a document streams. Compared to previous proposals, our models have extra flexibility and can adapt to situations where there is in fact no dependencies between the documents.</p><p>In the future, we plan to develop non-parametric extensions as well as versions of these models that scale well, following the improvements on the inference methods for LDA, proposed in streams <ref type="bibr" target="#b26">[26]</ref> or in online settings <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b2">2]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Inference process for ST-LDA-[D|C] Input: Stream of D documents of length N ; number of topics K Output: For each document d, topic distribution θ d , word-topic distributions φ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 for d = 1 to D do 4 8 For 9 For 9 12 2 14</head><label>48992</label><figDesc>Random initialization of λ d , µ d and z d n , 1 ≤ n ≤ N 5 λ1 = µ1 = 0 // Document processing 6 for d = 1 to D do 7 repeat ST-LDA-D: update θ d acc. to Eq. 5 ST-LDA-C: 10 (a) update T d (Metropolis-Hasting) 11 (b) obtain θ d from T d through Eq. Update φ d k acc. φ-procedure 13 Update λ d and µ d (Metropolis-Hasting), d &gt; Update z d n acc. to Eq. 7, 1 ≤ k ≤ K, 1 ≤ n ≤ N 15 until estimates are stable</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Perplexity curves with respect to time for all methods on NIPS and TDT4 collections (80 topics).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Perplexity of each method by number of tweets that are added to the test set (80 topics).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 depicts</head><label>4</label><figDesc>ROC curves of DTM, TM-LDA and ST-LDA-C defined over 8 different thresholds taken in the set [0.2 0.5 0.7 0.86 0.89 0.92 0.95 0.98] for the cosine measure and [0.5 1 2 5 10 15 20 50] for λ d when the number of topics is fixed to 20 and to 80.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Methods 20 (FalseFalse</head><label>20</label><figDesc>Fig. 4, left) 80 (Fig. 4, right) ST-LDA-C with λ d further illustrate the role of λ d , we pictorially illustrate the correlation between the estimated λ d and the topic distributions of three consecutive documents (Figure 5) with identical labels in the TDT4 collection. As one can 0 Positive Rate (Fall Out) True Positive Rate (Recall) TDT4 ST-LDA-C with λ d ST-LDA-C with cosine TM-LDA with cosine DTM with Positive Rate (Fall Out) True Positive Rate (Recall) TDT4 ST-LDA-C with λ d ST-LDA-C with cosine TM-LDA with cosine DTM with cosine</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: ROC curves of "semantic class matching" methods working over the topic distributions found by DTM, TM-LDA and ST-LDA-C, for the number of topics fixed to 20 (left) and 80 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Topic distribution of three pairs consecutive documents that have the same topic (Olympic -left, Election -middle, Sport -right) and subject labels in TDT4 dataset (20 topics).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Datasets used in our experiments along with their properties.</figDesc><table><row><cell></cell><cell>NIPS</cell><cell>TDT4</cell><cell>Tweets</cell></row><row><cell>Documents in Train set</cell><cell>1,350</cell><cell>2,800</cell><cell>60,000</cell></row><row><cell>Documents in Test set</cell><cell>150</cell><cell>390</cell><cell>12,000</cell></row><row><cell>Vocabulary size</cell><cell>12,375</cell><cell>22,965</cell><cell>42,336</cell></row><row><cell cols="2"># of unique words per doc. 500</cell><cell>100</cell><cell>15</cell></row><row><cell>Words in total</cell><cell cols="3">1,900,000 779,000 904,262</cell></row></table><note><p>where C test denotes the test collection, D test is its size and v d n represents the word at position n in document d. The parameters θ d k and φ d k are estimated on the training set.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Perplexity with respect to different number of topics in {20, 40, 60}.</figDesc><table><row><cell>Models</cell><cell>20</cell><cell>NIPS 40</cell><cell>60</cell><cell>20</cell><cell>TDT4 40</cell><cell>60</cell><cell>20</cell><cell>Tweets 40</cell><cell>60</cell></row><row><cell>LDA1</cell><cell>2068.4</cell><cell>2034.5</cell><cell>1986.4</cell><cell>900.8</cell><cell>930.2</cell><cell>960.4</cell><cell>470.8</cell><cell>580.3</cell><cell>615.5</cell></row><row><cell>LDA all</cell><cell>1625.4</cell><cell>1534.7</cell><cell>1458.1</cell><cell>723.1</cell><cell>768.4</cell><cell>792.7</cell><cell>431.8</cell><cell>508.6</cell><cell>577.1</cell></row><row><cell>TM-LDA</cell><cell>2038.7</cell><cell>2025.4</cell><cell>1985.3</cell><cell>876.7</cell><cell>900.3</cell><cell>916.3</cell><cell>455.1</cell><cell>520.1</cell><cell>585.2</cell></row><row><cell>DTM</cell><cell>1737.5</cell><cell>1551.2</cell><cell>1450.7</cell><cell>869.1</cell><cell>836.7</cell><cell>820.9</cell><cell cols="3">559.45 578.25 607.41</cell></row><row><cell>ST-LDA-D</cell><cell>1620.4</cell><cell>1520.9</cell><cell>1450.2</cell><cell>724.4</cell><cell>758.1</cell><cell>784.4</cell><cell>393.9</cell><cell>480.1</cell><cell>552.7</cell></row><row><cell>ST-LDA-C</cell><cell cols="3">1612.8 1497.6 1434.5</cell><cell cols="3">720.6 752.5 780.8</cell><cell cols="3">388.2 474.1 546.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>depicts the perplexities of all models on the three collections at the end when the parameters φ and θ have been estimated over all the test documents. In all experiments, best results are obtained with ST-LDA-C and ST-LDA-D, followed by DTM on NIPS and TDT4 and by LDA all on Tweets. These results are consistent with those of the figures 2 and 3. Again, TM-LDA does not perform well (as explained before); LDA all which is a standard LDA model, performs relatively well; however, both DTM and the ST-LDA-[D|C] models outperform it by taking into account dependencies between topics. We see here that the extra flexibility of the ST-LDA-[D|C] models allow them to outperform DTM.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Areas under the ROC curves of figure 4.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The LDA model is based on the assumption that topics are infinitely exchangeable within a document.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Linguistic Data Consortium, The Trustees of the University of Pennsylvania https://catalog.ldc.upenn.edu/ LDC2005T16.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://www.nlm.nih.gov/mesh/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>This value, upper bounding λ d , corresponds to a regime of the Franck copula close to comonotonicity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>We are working to release all the programs developed in this study publicly available for research purpose.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">ACKNOWLEDGMENTS</head><p>We thank the anonymous reviewers for their useful comments. This work was partly supported by the <rs type="funder">LabEx PERSYVAL-Lab</rs> <rs type="grantNumber">ANR-11-LABX-0025</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kBx89MB">
					<idno type="grant-number">ANR-11-LABX-0025</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. METROPOLIS-HASTING PROCEDURE</head><p>The Metropolis-Hasting procedure is based on the following steps:</p><p>1. Generate an initial value of x: draw x 1 ∼ P prior (x) 2. Initialize j = 1 3. Repeat till sequence is stable (a) Draw x ∼ q, where q represents the "jump" function</p><p>For x = λ d , one has:</p><p>where P prior (λ d ) ∼ U [0, τ λ ]. As λ d should be higher when θ d-1 and θ d are more similar (as in such a case the influence of θ d-1 on θ d is more important), we make use of the following jump function, based on the exponential distribution:</p><p>For x = µ d , the same distribution is used for the jump function, the cosine being taken between the vectors that correspond to the column-wise concatenation of the columns of each matrix φ d-1 and φ d . The prior this time is</p><p>, the jump function corresponds to Franck copula, and Π(T d k ) corresponds to the k th contribution in Eq. 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. GIBBS SAMPLING UPDATES (ST-LDA-C )</head><p>We provide here the complete derivation of Eq. 10. For any d ≥ 2, one has:</p><p>Let Fα (resp fα) denote the cdf (resp pdf) of the Gamma distribution with parameters (α, 1). By assumption:</p><p>and, since</p><p>Further, as usual <ref type="bibr" target="#b23">[23]</ref>: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On smoothing and inference for topic models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 25th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>UAI</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Topic models over text streams: A study of batch and online unsupervised learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th SIAM conference on Data Mining, SDM</title>
		<meeting>the 7th SIAM conference on Data Mining, SDM</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Free C++ implementation for dtm</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<ptr target="https://www.cs.princeton.edu/˜blei/topicmodeling.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference Proceeding Series, ICML</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised data classification using pairwise markov chains with automatic copulas selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Derrode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pieczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequential latent dirichlet allocation: Discover underlying topic structures within a document</title>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society, ICDM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mining data streams: A review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Gaber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zaslavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD Record</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online learning for latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A time-dependent topic model for multiple text streams</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gurumurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsioutsiouliklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Topic tracking model for analyzing consumer purchase behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Jont Conference on Artifical Intelligence</title>
		<meeting>the 21st International Jont Conference on Artifical Intelligence</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sampling nested Archimedean copulas</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Mcneil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Computation and Simulation</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multivariate Archimedean copulas, D-monotone functions and 1-norm symmetric distributions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Mcneil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nešlehovà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Slice sampling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An introduction to copulas</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Nelsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dirichlet and related distributions: Theory, methods and applications</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Properties of hierarchical Archimedean copulas</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ostap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yarema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wolfgang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics &amp; Risk Modeling</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">You are what you tweet: Analyzing twitter for public health</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Weblogs and Social Media</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Earthquake shakes twitter users: Real-time event detection by social sensors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sakaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on World Wide Web</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking LDA: why priors matter</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Conference</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Topics over time: A non-markov continuous-time model of topical trends</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distributed gibbs sampling of latent topic models: The gritty details</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">TM-LDA: Efficient online modeling of latent topic transitions in social media</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Benzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic mixture models for multiple time series</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Joint Conference on Artificial Intelligence, IJCAI</title>
		<meeting>the 20th International Joint Conference on Artificial Intelligence, IJCAI</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient methods for topic model inference on streaming document collections</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
