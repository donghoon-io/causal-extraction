<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Latent Forests for Medical Relation Extraction</title>
				<funder ref="#_KxhRt2Z">
					<orgName type="full">Ministry of Education, Singapore</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
							<email>guo@mymail.sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">StatNLP Research Group</orgName>
								<orgName type="institution">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
							<email>guoshunnan@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">StatNLP Research Group</orgName>
								<orgName type="institution">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
							<email>luwei@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">StatNLP Research Group</orgName>
								<orgName type="institution">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
							<email>scohen@inf.ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">ILCC</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="institution" key="instit1">University of Edinburgh zhijiang</orgName>
								<orgName type="institution" key="instit2">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Latent Forests for Medical Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of medical relation extraction is to detect relations among entities, such as genes, mutations and drugs in medical texts. Dependency tree structures have been proven useful for this task. Existing approaches to such relation extraction leverage off-the-shelf dependency parsers to obtain a syntactic tree or forest for the text. However, for the medical domain, low parsing accuracy may lead to error propagation downstream the relation extraction pipeline. In this work, we propose a novel model which treats the dependency structure as a latent variable and induces it from the unstructured text in an end-to-end fashion. Our model can be understood as composing task-specific dependency forests that capture non-local interactions for better relation extraction. Extensive results on four datasets show that our model is able to significantly outperform state-of-the-art systems without relying on any direct tree supervision or pre-training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With a significant growth in the medical literature, researchers in the area are still required to track it mostly manually. This is an opportunity to automate some of this tracking process, and indeed Natural Language Processing (NLP) techniques have been used to automatically extract knowledge from medical articles. Among these techniques, relation extraction plays a significant role as it facilitates the detection of relations among entities in the medical literature <ref type="bibr" target="#b11">[Peng et al., 2017;</ref><ref type="bibr" target="#b12">Song et al., 2019]</ref>. For example in Figure <ref type="figure" target="#fig_0">1</ref>, the sub-clause "human phenylalanine hydroxylase catalytic domain with bound catechol inhibitors" drawn from the CPR dataset <ref type="bibr" target="#b7">[Krallinger et al., 2017]</ref> contains two entities, namely phenylalanine hydroxylase and catechol. There is a "down regulator" relation between these two entities, denoted as "CPR:4".</p><p>Dependency structures are often used for relation extraction as they are able to capture non-local syntactic relations that are only implicit in the surface form alone <ref type="bibr">[Zhang et al., analysis ... phenylalanine</ref>   We omit some edges for simplicity. Phrase phenylalanine hydroxylase and catecho are gene and drug entity, respectively.</p><p>2018]. In the medical domain, early efforts leverage graph LSTM <ref type="bibr" target="#b11">[Peng et al., 2017]</ref> or graph neural networks <ref type="bibr">[Song et al., 2018;</ref><ref type="bibr">Guo et al., 2019a]</ref> to encode the 1-best dependency tree. However, dependency parsing accuracy is relatively low in the medical domain. Figure <ref type="figure" target="#fig_0">1</ref> shows the 1-best dependency tree obtained by the Stanford CoreNLP <ref type="bibr" target="#b9">[Manning et al., 2014]</ref>, where the dependency tree contains an error. In particular, the entity phrase phenylalanine hydroxylase is broken since the word hydroxylase is mistakenly considered as the main verb. In order to mitigate the error propagation when incorporating the dependency structure, <ref type="bibr" target="#b12">Song et al. [2019]</ref> construct a dependency forest by adding additional edges with high confidence scores given by a dependency parser trained on the news domain or merging the K-bests trees <ref type="bibr" target="#b3">[Eisner, 1996]</ref> by combining identical dependency edges. <ref type="bibr" target="#b9">Lifeng et al. [2020]</ref> jointly train a pre-trained dependency parser [Dozat and Manning, 2017] and a relation extraction model. The dependency forest generated by the parser is a 3-dimensional tensor, with each position representing the conditional probability of one word modifying another word with a relation, which encodes all possible dependency relations with the confidence scores. Unlike previous research efforts that rely on dependency parsers trained on newswire text, our proposed model treats the dependency parse as a latent variable and induces it in an end-to-end fashion. We build our model based on the mechanism of structured attention <ref type="bibr" target="#b5">[Kim et al., 2017</ref>   Then, the forest encoder uses graph neural networks to encode the induced forests. <ref type="bibr" target="#b9">Liu and Lapata, 2018]</ref>. Using a variant of the Matrix-Tree Theorem <ref type="bibr">[Tutte, 1984;</ref><ref type="bibr" target="#b7">Koo et al., 2007]</ref>, our model is able to generate task-specific non-projective dependency structures for capturing non-local interactions between entities without recourse to any pre-trained parsers or tree supervision. We further construct multiple forests by projecting the representations of nodes to different representation subspaces, allowing an induction of a more informative latent structure for better relation prediction. We name our proposed model as LF-GCN, where LF is the abbreviation of latent forests.</p><p>Experiments show that our LF-GCN model is able to achieve better performance on various relation extraction tasks. For the sentence-level tasks, our model surpasses the current stat-of-the-art models on the CPR dataset <ref type="bibr" target="#b7">[Krallinger et al., 2017]</ref> and the PGR dataset <ref type="bibr" target="#b13">[Sousa et al., 2019]</ref>) by 3.2% and 2.6% in terms of F 1 score, respectively. For the cross-sentence tasks <ref type="bibr" target="#b11">[Peng et al., 2017]</ref>, our model is also consistently better than others, showing its effectiveness on long medical text. We release our code at <ref type="url" target="https://github.com/Cartus/Latent-Forests">https://github.com/ Cartus/Latent-Forests</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Here we present the proposed model as shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Forest Inducer</head><p>Existing approaches leverage a dependency parser trained on newswire text <ref type="bibr" target="#b12">[Song et al., 2019]</ref> or a fine-tuned parser for the medical domain <ref type="bibr" target="#b9">[Lifeng et al., 2020]</ref> to generate a dependency forest, which is a fully-connected weighted graph. Unlike previous efforts, we treat the forest as a latent variable, which can be learned from a targeting dataset in an end-to-end manner. Inspired by <ref type="bibr" target="#b5">Kim et al. [2017]</ref> and <ref type="bibr" target="#b9">Liu and Lapata [2018]</ref>, we use a variant of Kirchhoff's Matrix-Tree Theorem (MTT) <ref type="bibr">[Tutte, 1984;</ref><ref type="bibr" target="#b7">Koo et al., 2007;</ref><ref type="bibr" target="#b11">Smith and Smith, 2007]</ref> to induce the latent structure of an input sentence. Such latent structure can be viewed as multiple full dependency forests, which efficiently represent all possible dependency trees within a compact and dense structure.</p><p>Given a sentence s = w 1 , ..., w n , where w i represents the i-th word, we define a graph G on n nodes, where each node refers to the word in the s, and the edge (i, j) refers to the dependency between the i-th word (head) to the j-th node (modifier). We denote the contextual output of the sentence h ∈ R n×d as h = h 1 , ..., h n , where h i ∈ R d represents the hidden state of the i-th word with a d dimension. We use the bidirectional LSTM to obtain contextual representations of the sentence.</p><p>For the graph G, MTT takes the edge scores and root scores as inputs then generates a latent forest by computing the marginal probabilities for each edge. Given the input h and a weight vector θ θ θ k ∈ R m of dependencies, where m ∈ R represents the number of dependencies for the k-th (k ∈ [1, N ]) latent structure, inducing the k-th latent forest for the input h amounts to finding the latent variables z k ij (h, θ θ θ k ) for all edges that satisfy i =j and root node whose index equals to 0.</p><p>The k-th latent forest induced by MTT contains many nonprojective dependency trees, which are denoted by T k . Let P (y|h; θ θ θ k ) denote the conditional probability of a tree y over T k . Following the formulation by <ref type="bibr" target="#b7">Koo et al. [2007]</ref>, the marginal probability of a dependency edge from i-th word to j-th word for the k-th forest can be expressed as :</p><formula xml:id="formula_0">P (z k ij = 1) = y∈T k :(i,j)∈y P (y|h; θ θ θ k )<label>(1)</label></formula><p>We derive two steps to obtain the marginal probabilities expressed in Equation (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computing Attention Scores</head><p>Inspired by <ref type="bibr" target="#b13">Vaswani et al. [2017]</ref>, we calculate the edge scores by the multi-head attention mechanism, which allows the model to jointly attend to information from different representation subspaces. N attention matrices will be fed into the MTT to obtain N latent forests in order to capture different dependencies in different representation subspaces. The attention matrix for the k-th head is calculated by a function of the query Q with the corresponding key K. Here Q and K are both equal to the contextual representation h. We project Q and K to different representation subspaces in order to generate N attention matrices for calculating N latent forests. Formally, the k-th forest S k is given by:</p><formula xml:id="formula_1">S k = softmax( QW Q × (KW K ) T √ d )<label>(2)</label></formula><p>where W Q ∈ R d×d and W K ∈ R d×d are parameters for projections. S k ij denotes the normalized attention score between the i-th token and the j-th token with h i and h j . Then, we compute the root score r k i , which represents the normalized probability of the i-th node to be selected as the root node of the k-th forest:</p><formula xml:id="formula_2">r k i = W k r h i (3)</formula><p>where W k r ∈ R 1×d is the weight for the projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Imposing Structural Constraint</head><p>Following <ref type="bibr" target="#b7">Koo et al. [2007] and</ref><ref type="bibr" target="#b11">Smith and</ref><ref type="bibr" target="#b11">Smith [2007]</ref>, we calculate the marginal probability of each dependency edge of the k-th latent forest, by injecting a structural bias on S k . We assign non-negative weights P k ∈ R n×n to the edges as:</p><formula xml:id="formula_3">P k ij = 0 if i = j exp (S k ij ) otherwise (4)</formula><p>where P k ij is the weight of the edge between the i-th and the j-th node. We define a Laplacian matrix L k ∈ R n×n of G in Equation ( <ref type="formula">5</ref>), and its variant Lk ∈ R n×n in Equation ( <ref type="formula">6</ref>).</p><formula xml:id="formula_4">L k ij = n i =1 P k i j if i = j -P k ij otherwise</formula><p>(5)</p><formula xml:id="formula_5">Lk ij = exp(r k i ) if i = 1 L k ij if i &gt; 1 (6)</formula><p>We use A k ij to denote the marginal probability of the dependency edge between the i-th node and the j-th node. Then, A k ij can be derived based on:</p><formula xml:id="formula_6">A k (z ij = 1) = (1 -δ 1,j )P k ij [( Lk ) -1 ] ij -(1 -δ i,1 )P k ij [( Lk ) -1 ] ji (7)</formula><p>where δ is Kronecker delta and A k ∈ R n×n can be interpreted as a weighted adjacency matrix for the k-th forest. Now we can feed A ∈ R N ×n×n into the forest encoder to update the representations of nodes in the latent structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive Pruning Strategy</head><p>Prior dependency-based model <ref type="bibr" target="#b16">[Zhang et al., 2018</ref>] also proposes rule-based method to prune a dependency tree to further improve relation classification performance. However, the weighted adjacency matrix A is derived based on a continuous relaxation, and such induced structures are not discrete, so the existing rule-based pruning methods are not applicable. Instead, we use α-entmax <ref type="bibr" target="#b0">[Blondel et al., 2018;</ref><ref type="bibr" target="#b2">Correia et al., 2019]</ref> to remove irrelevant information by imposing the sparsity constraints on the adjacency matrix. αentmax is able to assign exactly zero weights. Therefore, an unnecessary path in the induced latent forests will not be considered by the latent forest encoder. The expression of our soft pruning strategy is described as:</p><formula xml:id="formula_7">A k = α-entmax(A k ) (8)</formula><p>where α is a parameter to control the sparsity of each adjacency matrix. When α=2, the entmax recovers the sparsemax mapping <ref type="bibr">[Martins and Astudillo, 2016]</ref>. When α=1, it recovers the softmax mapping. <ref type="bibr" target="#b2">Correia et al. [2019]</ref> propose an exact algorithm to learn α automatically. Here we apply k α-entmax to k latent forests, which enables the model to develop different pruning strategies for different latent forest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Forest Encoder</head><p>Given N latent forests generated by the forest inducer, we encode them by using densely-connected graph convolutional networks <ref type="bibr" target="#b6">[Kipf and Welling, 2017;</ref><ref type="bibr">Guo et al., 2019b]</ref>. Formally, given the k-th latent forest, which is represented by the adjacency matrix A k , the convolution computation for the ith node at the l-th layer, which takes the representation h l-1 i from previous layer as input and outputs the updated representations h l i , can be defined as:</p><formula xml:id="formula_8">h l ki = σ( n j=1 A k ij W l k h l-1 i + b l k ) (9)</formula><p>where W l k and b l k are the weight matrix and bias vector for the k-th latent forest in the l-th layer, respectively. σ is an activation function. h 0 i ∈ R d is the initial contextual representation of the i-th node. Then a linear combination layer is used to integrate representations of the N latent forests:</p><formula xml:id="formula_9">h comb = W comb h out + b comb (10)</formula><p>where h out is the output by concatenating outputs from N separated convolutional layers, i.e., h out = [h (1) ; ...; )×d is a weight matrix and b comb is a bias vector for the linear transformation.</p><formula xml:id="formula_10">h (N ) ] ∈ R d×N . W comb ∈ R (d×N</formula><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>We For sentence-level relation extraction, we follow the experimental settings by <ref type="bibr" target="#b9">Lifeng et al. [2020]</ref> on BioCreative Vi CPR (CPR) <ref type="bibr" target="#b7">[Krallinger et al., 2017]</ref> and Phenotype-Gene relation (PGR) <ref type="bibr" target="#b13">[Sousa et al., 2019]</ref>. The CPR dataset contains the relations between chemical components and human proteins. It has 16,107 training, 10,030 development and 14,269 testing instances, with five regular relations, such as "CPR:3", "CPR:9" and "None" relation. PGR introduces the relations between human phenotypes with human genes, and it contains 11,780 training instances and 219 test instances, with binary class "Yes" and "No" on relation labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Settings</head><p>We tune the hyper-parameters according to the results on the development sets. For the cross-sentence n-ary relation extraction task, we use the same data splits as <ref type="bibr">Song et al. [2018]</ref>, stochastic gradient descent optimizer with a 0.9 decay rate, and 300-dimensional GloVe. The hidden size of both BiL-STM and GCNs are set as 300.</p><p>For cross-sentence task, we report the test accuracy averaged over five cross validation folds <ref type="bibr">[Song et al., 2018]</ref> for the n-ary task. For the sentence-level task, we report the F 1 scores <ref type="bibr" target="#b9">[Lifeng et al., 2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results on Cross-Sentence n-ary Task</head><p>To verify the effectiveness of the model in predicting intersentential relations, we compare LF-GCN against state-ofthe-art systems on the cross-sentence n-ary relation extraction task <ref type="bibr" target="#b11">[Peng et al., 2017]</ref>, as shown in Table <ref type="table" target="#tab_3">1</ref>. Previous systems using the same syntax type are grouped together. Full Tree: models use the 1-best dependency graph constructed by connecting roots of dependency trees correspond to the input sentences. DAG LSTM encodes the graph by using graph-structure LSTM, while GRN and GCN encode it using graph recurrent networks and graph convolutional networks, respectively. Pruned Tree: model with pruned trees as inputs, whose dependency nodes and edges are removed based on rules <ref type="bibr" target="#b16">[Zhang et al., 2018]</ref>. GCN is used to encode the resulted structure. Forest: model constructs multiple fully-connected weighted graphs based on the multi-head attention <ref type="bibr" target="#b13">[Vaswani et al., 2017]</ref>, where the graph can be viewed as a dependency forest.</p><p>Models with pruned trees as inputs tend to achieve higher results than models with full trees. Intuitively, longer sentences in the cross-sentence task correspond to more complex dependency structures. Using an out-of-domain parser may introduce more noise to the model. Removing the irrelevant nodes and edges of the parse tree enables the model to perform better prediction. However, a rule-based pruning strategy <ref type="bibr" target="#b16">[Zhang et al., 2018]</ref> may not yield optimal performance. In contrast, LF-GCN induces the dependency structure automatically, which can be viewed as a soft pruning strategy</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syntax Type</head><p>Model F 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>None</head><p>Random-DDCNN <ref type="bibr" target="#b9">[Lifeng et al., 2020]</ref> 45.4 * Att-GRU <ref type="bibr" target="#b9">[Liu et al., 2017]</ref> 49.5 * Bran <ref type="bibr" target="#b14">[Verga et al., 2018]</ref> 50.8 * Tree GCN <ref type="bibr" target="#b16">[Zhang et al., 2018]</ref> 52.2 * Tree-DDCNN <ref type="bibr" target="#b9">[Lifeng et al., 2020]</ref> 50.3 * Tree-GRN <ref type="bibr" target="#b9">[Lifeng et al., 2020]</ref> 51.4 *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forest</head><p>Edgewise-GRN <ref type="bibr" target="#b12">[Song et al., 2019]</ref> 53.4 * KBest-GRN <ref type="bibr" target="#b12">[Song et al., 2019]</ref> 52.4 * AGGCN <ref type="bibr">[Guo et al., 2019a]</ref> 56.7 * ForestFT-DDCNN <ref type="bibr" target="#b9">[Lifeng et al., 2020]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results on Sentence-Level Task</head><p>To examine LF-GCN on sentence-level task, we compare LF-GCN with state-of-the-art models on two medical datasets, i.e., <ref type="bibr">CPR [Krallinger et al., 2017]</ref> and PGR <ref type="bibr" target="#b13">[Sousa et al., 2019]</ref>. These systems are grouped in three types based on the syntactic structure used as shown in Table <ref type="table" target="#tab_4">2</ref>   <ref type="bibr" target="#b16">[Zhang et al., 2018]</ref> 81.3 * Tree-GRN <ref type="bibr" target="#b9">[Lifeng et al., 2020]</ref> 78.9 * Forest Edgewise-GRN <ref type="bibr" target="#b12">[Song et al., 2019]</ref> 83.6 * KBest-GRN <ref type="bibr" target="#b12">[Song et al., 2019]</ref> 85.7 * AGGCN <ref type="bibr">[Guo et al., 2019a]</ref> 89.3 * ForestFT-DDCNN <ref type="bibr" target="#b9">[Lifeng et al., 2020]</ref> 89.3 * LF-GCN (Ours)</p><p>91.9 *  <ref type="bibr" target="#b16">[Zhang et al., 2018]</ref> 84.8</p><p>Forest ForestFT-DDCNN <ref type="bibr" target="#b9">[Lifeng et al., 2020]</ref> 85.5 AGGCN <ref type="bibr">[Guo et al., 2019a]</ref> 85.7 KBest-GRN <ref type="bibr" target="#b12">[Song et al., 2019]</ref> 85.8 Edgewise-GRN <ref type="bibr" target="#b12">[Song et al., 2019]</ref> 86.3 LF-GCN (Ours) 85.7</p><p>Table <ref type="table">4</ref>: Test results on the SemEval dataset.</p><p>Forest: models leverage the dependency forest. Edgewise-GRN constructs a dependency forest by keeping all the edges with scores greater than a pre-defined threshold. KBest-GRN generates a forest by merging K-bests trees. ForestFT-DDCNN builds a forest by a learnable dependency parser. AGGCN computes attention matrices and treats them as the adjacency matrices of forests.</p><p>As shown in Table <ref type="table" target="#tab_6">3</ref>, models with full dependency trees or forests as inputs are able to significantly outperform all models that only consider the text sequence including BioBERT, which is trained on a very large-scale medical corpus. These results demonstrate that modeling structure in the input sentence is beneficial to the relation extraction task. Models with dependency forests as inputs yield better performance than those use 1-best dependency trees, which confirms our hypothesis that the error propagation, which is caused by the low parsing accuracy of an out-of-domain parser, can be alleviated by constructing weighted graphs (forests). Compared with models which encode fixed dependency forests that are generated at the data preprocessing stage (Edgewise-GRN and KBest-GRN), models with dynamic forests including AGGCN, ForestFT-DDCNN and LF-GCN achieve higher performance. On the other hand, LF-GCN also makes predictions without recourse to any pre-trained parsers, while it outperforms Random-DDCNN by a large margin, i.e., 14.5. Furthermore, our LF-GCN model achieves 58.9 and 91.9 scores on CPR and PGR datasets, which are consistently better than all forest generation approaches. These results suggest that the induced latent structure is able to capture task-specific information for better relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results on News Domain</head><p>LF-GCN can also be used in other domain. Table <ref type="table">4</ref> gives the results on SemEval <ref type="bibr">[Hendrickx et al., 2009]</ref> dataset from Figure <ref type="figure">3</ref>: F 1 scores against sentence length. The results on Tree-GRN and ForestFT-DDCNN come from <ref type="bibr" target="#b9">[Lifeng et al., 2020]</ref>.</p><p>Figure <ref type="figure">4</ref>: F 1 scores against the number of forests on the dataset <ref type="bibr" target="#b11">[Peng et al., 2017]</ref> under the Binary-class, Ternary, Cross setting shown in Table <ref type="table" target="#tab_3">1</ref>. The results on AGGCN are reproduced based on its released implementation. the news domain. Using limited training data, LF-GCN outperforms the models with a dependency tree including Tree-GRN and GCN by almost 1 point and is comparable with the models with dependency forests including AGGCN and ForestFT-DDCNN. This demonstrates that LF-GCN is able to learn a comparable expressive structure compared with the structure generated by an in-domain parser. LF-GCN is 0.6 point worse than Edgewise-GRN. The reason is that the parsing performance for newswire is much more accurate than the biomedical domain. We believe that our model is able to achieve higher performance if more training data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Analysis and Discussion</head><p>Performance against Sentence Length To investigate our LF-GCN performance under different sentence lengths, we split the test set of CPR into three categories ((0, 25], (25, 50], &gt;50) based on the lengths. As shown in Figure <ref type="figure">3</ref>, we compare our LF-GCN with Tree-GRN <ref type="bibr" target="#b12">[Song et al., 2019]</ref> and ForestFT-DDCNN <ref type="bibr" target="#b9">[Lifeng et al., 2020]</ref>. In general, LF-GCN outperforms Tree-GRN and ForestFT-DDCNN for each group of instances, showing the effectiveness of our model based on a latent structure induction. The performance gap is enlarged when the instance length increases. Intuitively, longer instances are more challenging since the dependency structure is a more sophisticated tree. These results illustrate that the induced structure is able to capture complex non-local interactions for better prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance against Number of Forests</head><p>Figure <ref type="figure">4</ref> shows the performance of LF-GCN and AGGCN with different number of forests, since AGGCN also lever-ages the multi-head attention mechanism <ref type="bibr" target="#b13">[Vaswani et al., 2017]</ref> to generate multiple weighted graphs. Even though AGGCN is initialized with the 1-best dependency tree generated by a pre-trained parser, LF-GCN consistently outperforms it under the same number of forest without relying on any parsers, where the numbers range from 1 to 4. These results demonstrate that our model is able to construct informative structures only based on the input text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Case Study</head><p>In this section, we use the Chu- <ref type="bibr">Liu-Edmonds [Chu and Liu, 1965]</ref> algorithm to extract N non-projective dependency trees from N latent forests, where each forest is expressed by a weighted adjacency matrix in Equation <ref type="formula">8</ref>. Here N equals to 2. We select an instance from the CPR development set, whose relations can be correctly predicted by our LF-GCN.</p><p>Case I: As shown in Figure <ref type="figure">5</ref>, these two dependency trees are able to capture rich interactions between the entities Schisandrin B (index 0 and 1) and DT-diaphorase (index 10, 11 and 12), which have a "up regulator" relation, denoted as "CPR:3". For example, the token "enhancing" (index 9), which shares the similar semantic as the gold relation "up regulator", is selected in the path between these two entities. Furthermore, these two trees show different dependencies between tokens, which confirms our hypothesis that inducing multiple forests can include more useful information.</p><p>Case II: However, as shown in Figure <ref type="figure">6</ref>, many dependency trees induced by structure attention are shallow and do not resemble to a linguistic syntax structure. Figure <ref type="figure">6</ref> shows two shallow trees extracted from the latent forests before imposing sparsity constraints. We observe that the constructed dependency trees tend to pick the first token of the sentence as the root, and all other tokens as the children. Interestingly, even though such trees have little to no structure, the model is still able to predict the correct relation label. We also notice that adding the α-entmax helps to induce deeper and more informative structures. We leave the investigation of this phenomenon as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Latent Structure Induction: Latent structure models are powerful tools for modeling compositional data and building NLP pipelines <ref type="bibr" target="#b15">[Yogatama et al., 2016;</ref><ref type="bibr" target="#b11">Niculae et al., 2018]</ref>. A challenge with structured latent models is that they involve computing an "argmax" (i.e., finding a best scoring discrete structure such as a parse tree) in the middle of a computation graph. Since this operation has null gradients, back propagation cannot be used. There are three main strategies to solve this issue including reinforcement learning, surrogate gradients and continuous relaxation. In this paper, we mainly focus on continuous relaxations, for which the exact gradient can be computed and back propagated <ref type="bibr" target="#b5">[Kim et al., 2017;</ref><ref type="bibr" target="#b9">Liu and Lapata, 2018;</ref><ref type="bibr" target="#b10">Nan et al., 2020]</ref>.</p><p>Medical Relation Extraction: Early efforts focus on predicting relations between entities by modeling interactions in the 1-best dependency tree <ref type="bibr" target="#b11">[Peng et al., 2017;</ref><ref type="bibr">Song et al., 2018]</ref>. Recently, dependency forests were used to alleviate the error cascading caused by an out-of-domain parser. <ref type="bibr" target="#b12">Song et al. [2019]</ref> build a forest by adding edges and labels that a pre-trained parser is confident about. <ref type="bibr" target="#b9">Lifeng et al. [2020]</ref> construct full forests represented by a 3-dimensional tensor generated by a pre-trained parser fine-tuned by the relation prediction loss. Instead of using an out-of-domain parser, our model dynamically induces multiple dependency forests solely based on the medical dataset in an end-to end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel model that is able to automatically induce a latent structure for better relation extraction, without recourse to any tree supervisions or pre-training. Extensive results on four medical datasets show that our approach is able to better alleviate the error propagation caused by an out-of-domain dependency parser, giving significantly better results than previous state-of-the-art systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) 1-best dependency tree; (b) manually labeled gold tree; (c) a dependency forest generated by the LF-GCN model, where the number for each arc indicates the weight of the edge in the forest.We omit some edges for simplicity. Phrase phenylalanine hydroxylase and catecho are gene and drug entity, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of our proposed LF-GCN model. It is composed of M identical blocks and each block has two components-forest inducer and forest encoder. The forest inducer consists of two sub-modules, where the first sub-module computes N attention matrices based on the multi-head attention, and the second sub-module takes the N attention matrices as inputs to obtain N dependency forests based on the Matrix-Tree Theorem. Then, the forest encoder uses graph neural networks to encode the induced forests.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>evaluate our LF-GCN model with four datasets on two tasks, namely cross-sentence n-ary relation extraction and sentence-level relation extraction.For cross-sentence n-ary relation extraction, we use two datasets generated by<ref type="bibr" target="#b11">Peng et al. [2017]</ref>, which has 6,987 ternary relation instances and 6,087 binary relation instances extracted from PubMed. The relation label contains five categories, e.g., "sensitivity", "resistance" and "none". FollowingSong et al. [2018], we define two sub-tasks for a more detailed evaluation, i.e., binary-class n-ary relation extraction and multi-class n-ary relation extraction. For binary-class extraction, we cluster the four relation classes as "Yes" and treat the label "None" as "No".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: (a) the first and (b) the second non-projective dependency tree, which are extracted from two latent forests induced by LF-GCN. The sentence is "VT recurred with the addition of aminophylline, a competitive adenosine A1-receptor antagonist."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>hydroxylase catalytic domain ... catechol inhibitors</figDesc><table><row><cell>nmod</cell><cell>nsubj</cell><cell>dobj amod</cell><cell></cell><cell></cell><cell>nmod</cell><cell>amod</cell></row><row><cell>nmod</cell><cell>compound</cell><cell cols="2">nmod amod</cell><cell>nmod</cell><cell cols="2">amod</cell></row><row><cell cols="7">analysis ... phenylalanine hydroxylase catalytic domain ... catechol inhibitors</cell></row><row><cell></cell><cell>0.14</cell><cell>0.15</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.09</cell><cell>0.03</cell><cell>0.06</cell><cell>0.15</cell><cell>0.07</cell><cell>0.12</cell></row><row><cell cols="7">analysis ... phenylalanine hydroxylase catalytic domain ... catechol inhibitors</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Average test accuracies on the [Peng et al., 2017] dataset for binary-class n-ary relation extraction and multi-class n-ary relation extraction. "Ternary" and "Binary" denote drug-gene-mutation tuple and drug-mutation pair, respectively. Single and Cross indicate that the entities of relations reside in single sentence or multiple sentences, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Binary-class</cell><cell></cell><cell cols="2">Multi-class</cell></row><row><cell cols="2">Syntax Type Model</cell><cell cols="2">Ternary</cell><cell cols="2">Binary</cell><cell cols="2">Ternary Binary</cell></row><row><cell></cell><cell></cell><cell cols="4">Single Cross Single Cross</cell><cell>Cross</cell><cell>Cross</cell></row><row><cell></cell><cell>DAG LSTM [Peng et al., 2017]</cell><cell>77.9</cell><cell>80.7</cell><cell>74.3</cell><cell>76.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Full Tree</cell><cell>GRN [Song et al., 2018]</cell><cell>80.3</cell><cell>83.2</cell><cell>83.5</cell><cell>83.6</cell><cell>71.7</cell><cell>71.7</cell></row><row><cell></cell><cell>GCN [Zhang et al., 2018]</cell><cell>84.3</cell><cell>84.8</cell><cell>84.2</cell><cell>83.6</cell><cell>77.5</cell><cell>74.3</cell></row><row><cell cols="2">Pruned Tree GCN [Zhang et al., 2018]</cell><cell>85.8</cell><cell>85.8</cell><cell>83.8</cell><cell>83.7</cell><cell>78.1</cell><cell>73.6</cell></row><row><cell>Forest</cell><cell>AGGCN [Guo et al., 2019a]</cell><cell>87.1</cell><cell>87.0</cell><cell>85.2</cell><cell>85.6</cell><cell>79.7</cell><cell>77.4</cell></row><row><cell></cell><cell>LF-GCN (Ours)</cell><cell>88.0</cell><cell>88.4</cell><cell>86.7</cell><cell>87.1</cell><cell>81.5</cell><cell>79.3</cell></row><row><cell cols="2">We also use SemEval-2010 Task 8 [Hendrickx et al., 2009]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">dataset from the news domain to evaluate the generalization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">capability of our model. It has 10,717 instances with 9 types</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">of relations and a special "Other" relation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Test results on the CPR dataset. Results on AGGCN and GCN are reproduced based on their released implementation.</figDesc><table><row><cell>55.7  *</cell></row></table><note><p>learned from the data. Compared to GCN models, our model obtains 2.2 and 2.6 points improvement over the best performing model with pruned trees for the ternary relation extraction. For binary relation extraction, our model achieves accuracy of 86.7 and 87.1 under Single and Cross settings, respectively, which surpasses the state-of-the-art AG-GCN model. We believe that our LF-GCN is able to distill relevant information and filter out noises from the representation for better prediction.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>and Table3. Results labeled with " * " are obtained based on the retrained models using their released implementations, as we don't have published results for the dataset.</figDesc><table><row><cell>Syntax Type</cell><cell>Model</cell><cell>F 1</cell></row><row><cell>None</cell><cell>BioBERT [Lee et al., 2019]</cell><cell>67.2  *</cell></row><row><cell></cell><cell>BO-LSTM [Lamurias et al., 2019]</cell><cell>52.3</cell></row><row><cell>Tree</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>None: models do not use any pre-trained parsers. Random-</cell></row><row><cell></cell><cell></cell><cell>DDCNN uses a randomly initialized parser [Dozat and Man-</cell></row><row><cell></cell><cell></cell><cell>ning, 2017] fine-tuned by the relation prediction loss. Att-</cell></row><row><cell></cell><cell></cell><cell>GRU stacks a self-attention layer on top of the gated recur-</cell></row><row><cell></cell><cell></cell><cell>rent units and Bran relies on a bi-affine self-attention model</cell></row><row><cell></cell><cell></cell><cell>to capture the interactions in the sentence. BioBERT is a pre-</cell></row><row><cell></cell><cell></cell><cell>trained language representation model for biomedical text.</cell></row></table><note><p>Tree: models use the 1-best dependency tree. Full trees are encoded by GCN, GRN and DDCNN, respectively. BO-LSTM only encodes words on the shortest dependency path. * GCN</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Test results on the PGR dataset. Results on AGGCN and GCN are reproduced based on their released implementations.</figDesc><table><row><cell>Syntax Type</cell><cell>Model</cell><cell>F 1</cell></row><row><cell>Tree</cell><cell>Tree-GRN [Song et al., 2019] GCN</cell><cell>84.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their thoughtful and constructive comments. This research is supported by <rs type="funder">Ministry of Education, Singapore</rs>, under its <rs type="programName">Academic Research Fund (AcRF) Tier 2 Programme (MOE AcRF Tier 2 Award</rs> No: <rs type="grantNumber">MOE2017-T2-1-156</rs>). Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not reflect the views of the <rs type="institution">Ministry of Education, Singapore</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KxhRt2Z">
					<idno type="grant-number">MOE2017-T2-1-156</idno>
					<orgName type="program" subtype="full">Academic Research Fund (AcRF) Tier 2 Programme (MOE AcRF Tier 2 Award</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning classifiers with fenchel-young losses: Generalized entropies, margins, and algorithms</title>
		<author>
			<persName><surname>Blondel</surname></persName>
		</author>
		<editor>AISTATS</editor>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the shortest arborescence of a directed graph</title>
		<author>
			<persName><forename type="first">Chu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Liu ; Yoeng-Jin</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tseng-Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientia Sinica</title>
		<imprint>
			<date type="published" when="1965">1965. 1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dozat and Manning, 2017] Timothy Dozat and Christopher D Manning. Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName><surname>Correia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017">2019. 2019. 2017</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Three new probabilistic models for dependency parsing: An exploration</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">M</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COL-ING</title>
		<imprint>
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Densely connected graph convolutional networks for graph-to-sequence learning</title>
		<author>
			<persName><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<title level="s">Stan Szpakowicz. Semeval-2010 task</title>
		<editor>
			<persName><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Hendrickx</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zornitsa</forename><surname>Kim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Preslav</forename><surname>Kozareva</surname></persName>
		</editor>
		<editor>
			<persName><surname>Nakov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ó</forename><surname>Diarmuid</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marco</forename><surname>Séaghdha</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lorenza</forename><surname>Pennacchiotti</surname></persName>
		</editor>
		<editor>
			<persName><surname>Romano</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009">2019. 2019. 2019. 2019. 2009. 2009</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structured attention networks</title>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Overview of the biocreative vi chemical-protein interaction track</title>
		<author>
			<persName><forename type="first">Koo</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2007">2007. 2007. 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Structured prediction models via the matrix-tree theorem. In BioCreative challenge evaluation workshop</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bo-lstm: classifying relations via long short-term memory networks along biomedical ontologies</title>
		<author>
			<persName><surname>Lamurias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<date type="published" when="2019">2019. 2019. 2019. 2019</date>
		</imprint>
	</monogr>
	<note>Bioinformatics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From softmax to sparsemax: A sparse model of attention and multi-label classification</title>
		<author>
			<persName><surname>Lifeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BioCreative VI Workshop</title>
		<editor>
			<persName><forename type="first">Feichen</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yanshan</forename><surname>Shen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Majid</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ravikumar</forename><surname>Rastegar-Mojarad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vipin</forename><surname>Komandur Elayavilli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hongfang</forename><surname>Chaundary</surname></persName>
		</editor>
		<editor>
			<persName><surname>Liu</surname></persName>
		</editor>
		<meeting>the BioCreative VI Workshop</meeting>
		<imprint>
			<publisher>André F. T. Martins and Ramón Fernández Astudillo</publisher>
			<date type="published" when="2014">2020. 2020. 2018. 2018. 2017. 2017. 2014. 2014. 2016. 2016</date>
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName><forename type="first">Nan</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards dynamic computation graphs via sparse latent structure</title>
		<author>
			<persName><surname>Niculae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2007">2018. 2018. 2017. 2017. 2007. 2018. 2018</date>
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Leveraging dependency forest for neural medical relation extraction</title>
		<author>
			<persName><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A silver standard corpus of human phenotype-gene relations</title>
		<author>
			<persName><surname>Sousa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<editor>
			<persName><surname>Neurips</surname></persName>
		</editor>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="1984">2019. 2019. 1984. 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simultaneously self-attending to all mentions for full-abstract biological relation extraction</title>
		<author>
			<persName><surname>Verga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to compose words into sentences with reinforcement learning</title>
		<author>
			<persName><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
