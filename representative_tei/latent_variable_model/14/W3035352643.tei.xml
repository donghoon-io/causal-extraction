<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text Style Transfer via Learning Style Instance Supported Latent Space</title>
				<funder>
					<orgName type="full">Prime Minister&apos;s Office, Singapore</orgName>
				</funder>
				<funder ref="#_nmZAG2D">
					<orgName type="full">Major Program of the National Social Science Fund of China</orgName>
				</funder>
				<funder>
					<orgName type="full">IRC@Singapore Funding Initiative</orgName>
				</funder>
				<funder ref="#_bdw8e7F">
					<orgName type="full">National Research Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaoyuan</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenhao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Jiangsu Collaborative Innovation Center for Language Ability</orgName>
								<orgName type="institution">Jiangsu Normal University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Text Style Transfer via Learning Style Instance Supported Latent Space</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T18:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text style transfer pursues altering the style of a sentence while remaining its main content unchanged. Due to the lack of parallel corpora, most recent work focuses on unsupervised methods and has achieved noticeable progress. Nonetheless, the intractability of completely disentangling content from style for text leads to a contradiction of content preservation and style transfer accuracy. To address this problem, we propose a style instance supported method, StyIns. Instead of representing styles with embeddings or latent variables learned from single sentences, our model leverages the generative flow technique to extract underlying stylistic properties from multiple instances of each style, which form a more discriminative and expressive latent style space. By combining such a space with the attention-based structure, our model can better maintain the content and simultaneously achieve high transfer accuracy. Furthermore, the proposed method can be flexibly extended to semi-supervised learning so as to utilize available limited paired data. Experiments on three transfer tasks, sentiment modification, formality rephrasing, and poeticness generation, show that StyIns obtains a better balance between content and style, outperforming several recent baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text style transfer aims to endow a sentence with a different style and meanwhile keep its main semantic content unaltered, which could benefit various downstream applications such as text polishing <ref type="bibr" target="#b6">[Rao and Tetreault, 2018]</ref>, poetic writing <ref type="bibr" target="#b10">[Yi et al., 2018]</ref> and dialog generation <ref type="bibr">[Zhou et al., 2018]</ref>.</p><p>Owing to the lack of large parallel corpora, recent work mainly pays attention to unsupervised transfer and generally achieves this goal by fusing content and representations of target styles. However, both literary theory <ref type="bibr" target="#b3">[Embler, 1967]</ref> and machine learning study <ref type="bibr" target="#b4">[Lample et al., 2019]</ref> manifest that style is coupled with content to some degree, causing a contradiction of content preservation and style accuracy. *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corresponding author</head><p>In this work, we mainly study two research paradigms of text style transfer and discuss their influence on performance. One paradigm is a typical disentanglement approach, which explicitly strips style from content, and then incorporates a separated target-style representation <ref type="bibr" target="#b6">[Shen et al., 2017;</ref><ref type="bibr">Fu et al., 2018;</ref><ref type="bibr" target="#b3">John et al., 2019]</ref>. Because of the intractability of disentanglement, specifying a target style often brings some unexpected content attached to the source style. As a result, this approach usually obtains high style transfer accuracy but fails to preserve full source content.</p><p>To improve content preservation, another paradigm adopts attention-based structures <ref type="bibr" target="#b0">[Bahdanau et al., 2015;</ref><ref type="bibr">Vaswani et al., 2017]</ref> to maintain all word-level source information. Instead of disentangling, this method forces the model to focus on style-independent words by cycle reconstruction and uses style embeddings to encourage the fusion of style-related phrases <ref type="bibr" target="#b4">[Lample et al., 2019;</ref><ref type="bibr" target="#b1">Dai et al., 2019]</ref>. Nevertheless, for text, style is a highly complex concept involving various linguistic features and individualities <ref type="bibr">[Crystal, 1970]</ref>. It is hard to learn expressive and flexible style embeddings to represent such a concept. Consequently, these models tend to overemphasize content preservation and evade the difficult of embedding learning, incurring unsatisfactory style accuracy.</p><p>Related linguistic research demonstrates that stylistic syndromes can be better observed in multiple instances by making broader comparisons <ref type="bibr">[Ide, 2004]</ref>. Inspired by this idea, we propose a style instance supported method, called StyIns, to alleviate the contradiction mentioned above. When transferring each sentence, StyIns adopts the attention mechanism to preserve complete source information. Then instead of using simple style embeddings, our model incorporates a set of instances sharing the same style, and learns to extract underlying stylistic properties with the powerful generative flow technique <ref type="bibr">[Rezende and Mohamed, 2015]</ref> to form a more discriminative latent space. Samples drawn from this space are fed to the decoder to strengthen style signals, yielding a better balance between content preservation and style accuracy. Besides, StyIns can be extended to a semi-supervised version to utilize limited parallel data for further improvement.</p><p>In summary, our contributions are as follows:</p><p>• We propose a style instance supported method to learn a more discriminative and expressive latent space, which enhances style signals and makes a better balance between style transfer accuracy and content preservation.</p><p>• Our model can flexibly switch to a semi-supervision manner to take advantage of limited parallel data, without extra parameters or structures.</p><p>• On three text transfer tasks, sentiment, formality and poeticness, both automatic and human evaluations demonstrate that our model achieves better general performance, against several recent baselines 1 . The first paradigm explicitly disentangles text as separated content and style representations, respectively, then combines the content with a target style to achieve transfer. <ref type="bibr" target="#b6">Shen et al. [2017]</ref> take a pair of adversarial discriminators to align the source and transferred content distributions. <ref type="bibr">Fu et al. [2018]</ref> concatenate the extracted content with a learned target-style embedding. These methods are also improved by utilizing locally-normalized language models as discriminators <ref type="bibr" target="#b9">[Yang et al., 2018]</ref>. <ref type="bibr">More recently, John et al. [2019]</ref> design multiple losses to pack a sentence into a latent space, which is then split into sub-spaces of content and style. Since complete disentanglement is impracticable, this paradigm usually results in satisfactory style accuracy but poor content preservation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The second paradigm takes multi-generator structures and generates sentences in each style with a corresponding generator. Namely, each style is implicitly represented as the generator parameters. <ref type="bibr">Fu et al. [2018]</ref> make an attempt on this paradigm and adversarially train one encoder to disentangle content. <ref type="bibr">Prabhumoye et al. [2018]</ref> utilize a back-translation technique to translate a sentence to another language to corrupt its stylistic properties. Then adversarially trained decoders are used to create transferred sentences in the original language. Based on Reinforcement Learning (RL), <ref type="bibr" target="#b3">Gong et al. [2019]</ref> represent each transfer direction between two styles as one encoder-decoder model, and <ref type="bibr" target="#b5">Luo et al. [2019]</ref> pair the two transfer directions with a dual learning schema for further improvement. Generally, this paradigm is effective but also resource-consuming since each style or transfer direction needs to be modelled by a separated generator.</p><p>The third paradigm is a locate-and-replace schema, which locates style-dependent words and then replaces them with the target-style ones. We can consider the content and style to be represented as corresponding sets of words. <ref type="bibr">Li et al. [2018]</ref> design a delete-and-retrieve method to combine content words in a source sentence with stylistic words in a retrieved semantically similar sentence. <ref type="bibr">Wu et al. [2019b]</ref> mask all sentimental tokens in a source sentence, then use a pre-trained BERT <ref type="bibr" target="#b2">[Devlin et al., 2019]</ref> to infill targetsentiment ones. <ref type="bibr">Wu et al. [2019a]</ref> take a hierarchical RL method, which uses two agents to locate style-related words 1 Our source code is available at github.com/XiaoyuanYi/StyIns. and alter the sentence, respectively. To sum up, this paradigm is more accurate since it maintains all word-level information, but the lack of stylistic vocabularies limits its locating performance. Moreover, it doesn't apply to the scenarios that styles are expressed beyond word level, e.g., poeticness.</p><p>The last paradigm adopts one single attention-based encoder-decoder model <ref type="bibr" target="#b0">[Bahdanau et al., 2015]</ref> and feeds a style embedding to the decoder to provide style signals, without explicit disentanglement. <ref type="bibr" target="#b4">Lample et al. [2019]</ref> take this paradigm and try to control multiple attributes of text with an attention-based LSTM model. Instead of LSTM, <ref type="bibr" target="#b1">Dai et al. [2019]</ref> use the more powerful Transformer <ref type="bibr">[Vaswani et al., 2017]</ref>. Such a design helps better preserve source information, avoids structural redundancy of paradigm 2, and could cover broader cases compared to paradigm 3. Nevertheless, since the learned embedding is not expressive enough to model the highly complex concept of style, this paradigm usually causes unsatisfactory style transfer accuracy.</p><p>In addition, Shang et al.</p><p>[2019] devise a semi-supervised method that projects the latent spaces of different styles. Despite achieving further improvement, this model is sensitive to parallel data size and not suitable for unsupervised cases.</p><p>Absorbing advantages of paradigm 1 &amp; 4, StyIns learns a more discriminative latent style space to better balance style and content, and it could flexibly switch to a semi-supervised version, compatible with a broader range of scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formalization and Overview</head><p>We first formalize the unsupervised text style transfer task. Suppose there are M datasets {D i } M i=1 , and sentences in D i share the same style s i . Given an arbitrary sentence x with a source style s i , our goal is to rephrase x to a new one y with a target style s j (j = i) while preserving its main content.</p><p>As discussed before, to produce strong style signals we provide a set of sentences, Φ j K = {ŷ k } K k=1 ⊂ D j , called style instances, to represent an empirical distribution of style s j , which helps the model better learn underlying stylistic properties. For this sake, we incorporate a latent variable z constructed by these instances to represent the complex concept of style. Since sentences of the same style are conditionally Update the parameters of G;</p><p>10:</p><p>for n C steps do 11:</p><p>Update the parameters of C with L C ;</p><p>12:</p><p>end for 13: end for independent w.r.t. z, we can derive a new parametric form of text style transfer:</p><formula xml:id="formula_0">p(y|x, Φ j K ) = p(y, z|x, Φ j K )dz = p(y|x, z) * p(z|Φ j K )dz = E z∼p(z|Φ j K ) [p(y|x, z)].</formula><p>(1)</p><p>Eq. ( <ref type="formula">1</ref>) differs from previous work <ref type="bibr" target="#b6">[Shen et al., 2017;</ref><ref type="bibr" target="#b9">Yang et al., 2018]</ref> and suggests the architecture of our StyIns, as presented in Figure <ref type="figure">1</ref>.</p><p>Define E src (x) as a bidirectional LSTM encoder, called source encoder, E sty (Φ j K ) as a style encoder to model the distribution p(z|Φ j K ), and D(H, z) as a decoder with the attention mechanism <ref type="bibr" target="#b0">[Bahdanau et al., 2015]</ref>. The source encoder maps a given sentence x to a sequence of hidden states H. Then the decoder generates a transferred sentence y with H and z as inputs, where z is sampled from p(z|Φ j K ). These three components together form our generator G(x, Φ j K ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Latent Style Space</head><p>The style encoder E sty (Φ j K ) takes style instances as inputs, constructs a latent style space, and then outputs a sampled style representation z for the decoder to guide stylistic generation. Previous work <ref type="bibr" target="#b3">[John et al., 2019]</ref> usually adopts the Variational Auto-Encoder (VAE) <ref type="bibr" target="#b3">[Kingma and Welling, 2014]</ref> to build latent spaces. On the basis of the mean-field approximation, VAE assumes the independence of sentences and allocates each a corresponding isotropic Gaussian latent space. Despite the tractability of computation, this approach is implausible. For one thing, the dimension-independent Gaussian distribution is not expressive enough, which has been explored in various work <ref type="bibr" target="#b0">[Atanov et al., 2019]</ref>. For another, sentences with the same style are not dependent but connected by sharing one global style space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generative Flow</head><p>To avert these problems, we resort to the generative flow (GF) <ref type="bibr">[Rezende and Mohamed, 2015]</ref>, a potent technique to construct sophisticated distributions. Put simply, GF maps a simple initial latent variable z 0 to a complex one z T by applying a chain of parameterized mapping functions f t :</p><formula xml:id="formula_1">z t = f t (z t-1 , c), z 0 ∼ p(z 0 |c), t ∈ {1, 2, . . . , T }, (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where c is a given condition and T is the length of the chain. GF requires each f t to be invertible and its Jacobian determinant to be computable. Then we can get the probability density of the final distribution by:</p><formula xml:id="formula_3">log p(z T |c) = log p(z 0 |c) - T t=1 log det dz t dz t-1 .<label>(3)</label></formula><p>Various choices of f t have been proposed these years. We use a simple but effective one here, the Inverse Autoregressive Flow (IAF) <ref type="bibr" target="#b3">[Kingma et al., 2016]</ref>. More concretely, we have:</p><formula xml:id="formula_4">[m t , o t ] ← g t (z t-1 , c), σ t = sigmoid(o t ), (4) z t = σ t z t-1 + (1 -σ t ) m t ,<label>(5)</label></formula><p>where is element-wise multiplication. g t is an autoregressive network, in which the i-th element of output vectors is calculated with the first i -1 elements of z t-1 . We use the structure proposed in <ref type="bibr">[Germain et al., 2015]</ref> as g t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Style Instance Supported Latent Space</head><p>As mentioned in Sec. 3.1, to construct a more expressive latent space, we discard the mean-filed assumption by utilizing K style instances Φ j K = {ŷ k } K k=1 rather than only one single sentence. In detail, we feed each ŷk to another bidirectional LSTM, and represent it as v k , the concatenated final hidden state. Then we assume the initial latent variable z 0 in Eq. ( <ref type="formula" target="#formula_1">2</ref>) follows the isotropic Gaussian distribution:</p><formula xml:id="formula_5">z 0 ∼ p(z 0 |Φ j K ) = N (µ 0 , σ 2 0 I),<label>(6)</label></formula><formula xml:id="formula_6">µ 0 ≈ 1 K K k=1 v k , σ 2 0 ≈ 1 K -1 K k=1 (v k -µ 0 ) 2 , (<label>7</label></formula><formula xml:id="formula_7">) c = M LP (µ 0 ),<label>(8)</label></formula><p>where the mean of z 0 is approximated by Maximum Likelihood Estimation and we use the unbiased estimator for variance. c is a global representation of Φ j K which is computed by a Multi-Layer Perceptron (MLP) and used in Eq. ( <ref type="formula">4</ref>).</p><p>With the modules introduced above, we can get an output z of the style encoder E sty (Φ j K ) by sampling z 0 with Eq. ( <ref type="formula" target="#formula_5">6</ref>) and mapping it with Eq. ( <ref type="formula" target="#formula_1">2</ref>). Then the sampled z is concatenated with the embedded word and fed to the decoder at each time step. We will show that such a learned latent space is highly discriminative in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unsupervised Training</head><p>Given a source sentence x, two sets of style instances, Φ i K (x ∈ Φ i K ) and Φ j K , with the source style s i and the target style s j , we adopt the following losses to create indirect signals.</p><p>Reconstruction Loss. This loss is used by different paradigms of model <ref type="bibr" target="#b6">[Shen et al., 2017;</ref><ref type="bibr" target="#b5">Luo et al., 2019;</ref><ref type="bibr">Wu et al., 2019b]</ref>, which requires the model to reconstruct the given sentence with source-style signals:</p><formula xml:id="formula_8">L recon = -log p G (x|x, Φ i K ).<label>(9)</label></formula><p>Cycle Consistency Loss. Cycle consistency is first applied to image style transfer <ref type="bibr" target="#b10">[Zhu et al., 2017]</ref> to strengthen content preservation and then also adopted for text <ref type="bibr" target="#b1">[Dai et al., 2019;</ref><ref type="bibr" target="#b4">Lample et al., 2019]</ref>. We transfer a source sentence in two directions with the support of instances in different styles:</p><formula xml:id="formula_9">L cycle = -log p G (x|y, Φ i K ), y ← G(x, Φ j K ).<label>(10)</label></formula><p>Note that in each iteration, we provide different sampled instances to help StyIns better generalize stylistic properties.</p><p>Adversarial Style Loss. Without any parallel corpus, adversarial training <ref type="bibr" target="#b3">[Goodfellow et al., 2014]</ref> is utilized to build style supervision. Following <ref type="bibr" target="#b1">[Dai et al., 2019]</ref>, we use a classifier with M +1 classes as the discriminator C to tell the style of an input sentence (M +1-th class indicates a generated fake). The generator is expected to fool the discriminator by:</p><formula xml:id="formula_10">L style = -log p C (j|y),<label>(11)</label></formula><p>and the discriminator is alternately optimized by:</p><formula xml:id="formula_11">L C = -[log p C (i|x)+log p C (i|x)+log p C (M +1|y)],<label>(12)</label></formula><p>where x ← G(x, Φ i K ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Semi-Supervised Training</head><p>Our method can be regarded as the construction of target-style information with the support of style instances. When the ground truth y * ∈ Φ j K is available, we can create supervision by maximizing log p(y * |x, Φ j K ). We drive a lower bound as:</p><formula xml:id="formula_12">log p(y * |x, Φ j K ) ≥ E q(z|y * ,Φ j K ) [log p(y * |z, x)] -KL[q(z|y * , Φ j K )||p(z|Φ j K )].<label>(13)</label></formula><p>Based on this lower bound, we get the final supervised loss:</p><formula xml:id="formula_13">L super = -α * E q(z|y * ,Φ j K ) [log p(y * |z, x) + log p(z|Φ j K ) -log q(z|y * , Φ j K )] + β * E q(z|Φ j K ) [-log p(y * |z, x)],<label>(14)</label></formula><p>where α and β are hyper parameters to re-scale the loss. By optimizing Eq. ( <ref type="formula" target="#formula_13">14</ref>), we simultaneously maximize a lower bound of log p(y * |x, Φ j K ) and minimize an upper bound of -log p(y * |x, Φ j K ) (the second term in Eq. ( <ref type="formula" target="#formula_13">14</ref>)). The KL term, which is approximately estimated with Eq. ( <ref type="formula" target="#formula_3">3</ref>), could benefit alignment of latent style distributions learned with/without ground truth, and thus helps better extract common stylistic properties. We describe the complete training process in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We conduct experiments on three text style transfer tasks.</p><p>Sentiment Modification. We use the Yelp dataset processed by <ref type="bibr">[Li et al., 2018]</ref>, which consists of restaurant reviews with two sentiments, namely negative and positive. We only evaluate the transfer direction from vernacular to poetic. Since the opposite direction is more difficult requiring more sophisticated structures, we leave it for future work. We use Yelp and GYAFC for unsupervised transfer; CPVT and GYAFC for semi-supervised transfer. English sentences are tokenized with the NLTK tool, and Chinese sentences are segmented as Chinese characters. All digits are replaced with a &lt;NUM&gt; symbol. Table <ref type="table" target="#tab_1">1</ref> presents detailed data statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Setups</head><p>We set word embedding size, hidden state size, the number of style instances K and the length of generative flow chain T to 256, 512, 10 and 6 respectively. The encoder and decoder share the same word embedding. The prior and posteriori distributions of z in Eq. ( <ref type="formula" target="#formula_12">13</ref>) share parameters to reduce model size. The discriminator is a Convolutional Neural Network (CNN) based classifier with Spectral Normalization <ref type="bibr" target="#b5">[Miyato et al., 2018]</ref>. To handle the discrete nature of sentences, as in <ref type="bibr" target="#b1">[Dai et al., 2019]</ref>, we multiply the softmax distribution by the word embedding matrix, to get a soft generated word and feed this weighted embedding to the discriminator. Adam with mini-batches (batch size=64) is used for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We conduct comprehensive comparisons with several stateof-the-art style transfer models. For unsupervised transfer, we consider CrossAlign <ref type="bibr" target="#b6">[Shen et al., 2017]</ref>, <ref type="bibr">MultiDec [Fu et al., 2018]</ref>, <ref type="bibr">DelRetri [Li et al., 2018]</ref>, Template <ref type="bibr">[Li et al., 2018]</ref>, <ref type="bibr">Disentangled [John et al., 2019]</ref> and StyleTransformer <ref type="bibr" target="#b1">[Dai et al., 2019]</ref>. These models cover the four paradigms described in Sec. 2. We emphasize Disentangled and StyleTransformer (abbreviated as Disent. and StyleTr.) as the representatives  of paradigms 1 &amp; 4 respectively. For semi-supervised transfer, we compare CPLS <ref type="bibr" target="#b6">[Shang et al., 2019]</ref>, which is the only one semi-supervised transfer model to our best knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Metrics</head><p>We consider three criteria: Style Transfer Accuracy (Sty.), Content Preservation (Con.) and Fluency (Flu.).</p><p>Automatic Evaluation. Following previous work <ref type="bibr">[Fu et al., 2018;</ref><ref type="bibr" target="#b5">Luo et al., 2019;</ref><ref type="bibr" target="#b3">John et al., 2019;</ref><ref type="bibr" target="#b1">Dai et al., 2019]</ref>, we use a classifier's accuracy (Acc) to measure style accuracy. For Yelp and GYAFC, we fine-tuned a pre-trained BERT <ref type="bibr" target="#b2">[Devlin et al., 2019]</ref> with each dataset. For CPVT, we train a CNN-based classifier. The three classifiers achieve 98%, 88% and 98% accuracy, respectively. The BLEU score between transferred sentences and human-authored references, and the cosine distance (Cos) between the source and transferred embeddings <ref type="bibr">[Fu et al., 2018]</ref>, are utilized to measure content preservation. Cos is multiplied by 100 to match the scale of other metrics. We train a 5-gram language model KenLM <ref type="bibr">[Heafield, 2011]</ref> with sentences of each style, and measure fluency by perplexity (PPL) of transferred sen-tences. We also report the geometric mean (GM) of Acc, BLEU, Cos and 1 log PPL as the overall performance. Human Evaluation. We conduct human rating on our StyIns and four baselines with the highest GM scores under automatic metrics. Due to the limitation of manual labour involved, we access unsupervised results of Yelp and GYAFC, and semi-supervised results of CPVT. For each model with each transfer direction, we sample 50 sentences and get 1,100 generated sentences in total. We invite three annotators to evaluate in a blind review manner. Each of the three criteria is scored on a 5-point scale ranging from 1 (worst) to 5 (best).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experimental Results</head><p>As shown in Table <ref type="table" target="#tab_2">2</ref> (upper part), our model achieves the best overall performance (GM). Disentangled gets satisfactory accuracy and PPL on both Yelp and GYAFC datasets, but performs worse for other metrics. These models belonging to paradigm 1 (e.g., Disentangled and CrossAlign) try to separate content and style. However, as discussed in Sec. 1 &amp; 2, due to the intractability of disentanglement, specifying one style may also drag some attached content out from the content space, resulting in fluent transferred sentences with the desired style but irrelevant phrases. On the contrary, StyleTransformer is better at content preservation, benefiting from its powerful attention structures. Nevertheless, the simple style embedding hinders this model from higher transfer accuracy. Moreover, we can found our StyIns also outperforms StyleTransformer on BLEU and Cos for GYAFC. With less data, the complicated Transformer can't be adequately trained, while our model is relatively insensitive to data size.</p><p>Table <ref type="table" target="#tab_2">2</ref> (lower part) gives the results of semi-supervised transfer. Our model gets better overall results and excels at content preservation, while CPLS performs better in style control. CPLS also achieves lower PPL. The reason lies in that CPLS adopts multiple decoders, but our model only contains one decoder. With more paired data, both CPLS and StyIns obtain further improvement. Besides, CPLS is more sensitive to the size of parallel data on BLEU but not on accuracy, opposite to our model. Take poeticness transfer as an example. When the number of paired data increases from informal  formal formal  informal they were extremely friendly and reasonably priced. they were extremely rude and extremely slow. they were extremely rude and reasonably priced. they were extremely rude and over priced.</p><p>that is strange but perhaps. i do not know. i will speak to my friend later next to his locker. this is a friend that i know i like him and i know that. that 's strange but u. 1k to 4k, CPLS quintuples its BLEU score, but our model gets limited improvement. Please note that our model is suitable for both unsupervised and semi-supervised cases, while CPLS can be applied to semi-supervised transfer only.</p><p>Table <ref type="table" target="#tab_3">3</ref> presents human evaluation results. Again, Style-Transformer gets worse style accuracy but better content preservation than Disentangled. In addition, StyIns achieves comparable or even better fluency compared to baselines under human rating. This result indicates that overly low PPL may be obtained by ignoring the required content, and a moderate PPL value is enough to reflect acceptable fluency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Further Analysis</head><p>In Figure <ref type="figure" target="#fig_3">2</ref> (a), we vectorize sentences by a pre-trained Au-toEncoder and the LSTM in our style encoder (E sty ), respectively. We can observe that the former fails to distinguish different sentiments while our style encoder can separate these data points to some extent. We also visualize samples from the latent style space of StyIns. Compared to original sentence representations, this space is much more discriminative, which could produce flexible and strong style signals.</p><p>In Figure <ref type="figure" target="#fig_3">2</ref> (b), we plot the accuracy and sentence-BLEU (calculated with NLTK) of sentences transferred by different models. We can see for Disentangled, sentences fall in a low-BLEU and high-Acc area. For StyleTransformer, more sentences spread in the lower-Acc region compared to StyIns. Such results manifest that StyIns makes a better balance between style accuracy and content preservation.</p><p>In Figure <ref type="figure" target="#fig_3">2</ref> (c), we investigate the effect of different numbers of style instances K. We found when we use a small K (e.g., 4) in the training phase, setting different K in the testing phase makes a negligible difference. When we use a larger K in training, it's a better choice to take the same one for testing. In general, increasing K could facilitate the learning of latent style space and hence leads to better performance, which could support our claim that the independent assumption of sentences mentioned in Sec. 3.2 is implausible. However, larger K also requires more resources and slows down the training. As a compromise, we set K=10.</p><p>Figure <ref type="figure" target="#fig_3">2</ref> (d) shows some transferred samples from differ-ent datasets. We can see that Disentangled creates quite fluent sentences in apparent target style, but often loses source information, as discussed before. On Yelp, StyleTransformer can copy most style-independent source phrases but sometimes fails to generate required stylistic words. On the contrary, our StyIns makes a better balance on the two criteria. On CPVT, we can observe more interesting results. As discussed in Sec. 4.1, expressed beyond the use of stylistic words, poeticness is a more complex style than sentiment. CPLS chooses to sidestep this obstacle at times by generating fluent but irrelevant phrases. Our model, by contrast, learns to delete some vernacular words and reorder the remaining ones to better meet the syntactic requirements of classical poetry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this work, we propose a style instance supported method, StyIns, to alleviate the contradiction of content preservation and style accuracy in text style transfer tasks. StyIns adopts the generative flow technique to construct a more discriminative and expressive latent style space with the support of multiple style instances, which provides strong style signals to an attention-based decoder. Besides, our model can be flexibly extended to the semi-supervised version to utilize limited parallel data for further improvement. Experiments on three transfer tasks show that our model achieves a better balance between content and style, against several state-of-the-arts. We plan to explore few-instance text style transfer, in which case a new style and a few instances of it are available only in the testing phase. Without explicitly defined style categories, our model possesses the potential to achieve such transfer. We highlight this task and leave it for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Style transfer has been widely explored in Computer Vision (CV) filed<ref type="bibr" target="#b10">[Zhu et al., 2017]</ref> but remained challenging for text due to the discrete nature and vague style definition of language. Without sufficient parallel text data, recent research interests mainly concentrate on unsupervised transfer methods. According to the way of representing content and style, we can categorize most existing models into four paradigms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 1: A graphical illustration of the proposed model.</figDesc><graphic coords="2,322.82,55.40,171.64,137.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Input do not go to this practice they are horrible! Disent. love this practice. StyleTr. do not go to this practice they are amazing! StyIns i always go to this practice they are great</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>:Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Visualization of data points and samples from the latent style space on Yelp with t-SNE. (b) The logarithm of the number of transferred sentences in different ranges of style accuracy and sentence-BLEU. We present those with accuracy≥0.2 on Yelp. (c) The geometric mean of Acc, BLEU and 1 log PPL on Yelp with different numbers of style instances. (d) Transferred samples from the three datasets. Phrases with different styles are marked in blue and red. Brown words are content irrelevant to source sentences.</figDesc><graphic coords="6,188.81,54.89,90.90,125.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Training Process 1: for number of iterations do</figDesc><table><row><cell>7:</cell><cell>Accumulate L super ;</cell></row><row><cell>8:</cell><cell>end if</cell></row><row><cell>9:</cell><cell></cell></row></table><note><p><p><p><p><p>2:</p>Sample a source style s i and a target style s j ; 3:</p>Sample instances Φ i K from D i and Φ j K from D j ; 4: Sample x from D i , x / ∈ Φ i K ; 5:</p>Accumulate L recon , L cycle , L style ; 6:</p>if y * exits then</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Data Statistics.</figDesc><table><row><cell>Dataset</cell><cell>Styles</cell><cell cols="2">Paired Train Valid</cell><cell>Test</cell><cell cols="2">Unpaired Train Valid</cell></row><row><cell>Yelp</cell><cell>Neg. Pos.</cell><cell>N/A</cell><cell>N/A</cell><cell>500 500</cell><cell cols="2">180k 2,000 270k 2,000</cell></row><row><cell>GYAFC</cell><cell>Inf. For.</cell><cell>52k 52k</cell><cell cols="2">2,788 1,332 2,247 1,019</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>CPVT</cell><cell>Ver. Poe.</cell><cell>4k 4k</cell><cell cols="3">1,000 2,000 200k 1,000 2,000 200k</cell><cell>10k 10k</cell></row><row><cell cols="7">Formality Rephrasing. The recently released dataset</cell></row><row><cell cols="7">GYAFC [Rao and Tetreault, 2018] contains paired formal</cell></row><row><cell cols="7">and informal sentences in two domains. We use the Family &amp;</cell></row><row><cell cols="3">Relationships domain.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Poeticness Generation. We also consider Chinese poetic-</cell></row><row><cell cols="7">ness generation, as in [Shang et al., 2019], which seeks to</cell></row><row><cell cols="7">transfer a vernacular sentence to a classical poetic one. As</cell></row><row><cell cols="7">Chinese vernacular text and classical poetry share similar vo-</cell></row><row><cell cols="7">cabulary, differences between them, e.g., grammar and syn-</cell></row><row><cell cols="7">tax, lie beyond simple word usage. Hence, this task is more</cell></row><row><cell cols="7">challenging than the above two. We build a corpus called</cell></row><row><cell cols="7">Chinese Poetic and Vernacular Text (CPVT) with vernac-</cell></row><row><cell cols="7">ular sentences from Chinese prose and poetic sentences from</cell></row><row><cell cols="7">classical poems. Besides, we collect 7,000 pairs of human-</cell></row><row><cell cols="7">authored sentences for testing and semi-supervised training.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Automatic evaluation results of unsupervised transfer and semi-supervised transfer with different numbers of paired data.</figDesc><table><row><cell>Models</cell><cell></cell><cell></cell><cell></cell><cell cols="5">Acc↑ BLEU↑ Cos↑ PPL↓ GM↑ Yelp (Unsupervised)</cell><cell cols="3">Acc↑ BLEU↑ Cos↑ PPL↓ GM↑ GYAFC (Unsupervised)</cell></row><row><cell cols="3">MultiDec [Fu et al., 2018]</cell><cell></cell><cell>46.0</cell><cell>15.09</cell><cell>91</cell><cell>175</cell><cell>10.52</cell><cell>24.9</cell><cell>11.53</cell><cell>91</cell><cell>97</cell><cell>8.69</cell></row><row><cell cols="4">CorssAlign [Shen et al., 2017]</cell><cell>73.2</cell><cell>9.41</cell><cell>90</cell><cell>76</cell><cell>10.94</cell><cell>66.8</cell><cell>3.18</cell><cell>88</cell><cell>35</cell><cell>8.52</cell></row><row><cell cols="3">DelRetri [Li et al., 2018]</cell><cell></cell><cell>88.5</cell><cell>16.61</cell><cell>93</cell><cell>136</cell><cell>12.92</cell><cell>61.1</cell><cell>21.20</cell><cell>91</cell><cell>110</cell><cell>12.58</cell></row><row><cell cols="3">Template [Li et al., 2018]</cell><cell></cell><cell>81.6</cell><cell>22.62</cell><cell>92</cell><cell>296</cell><cell>13.14</cell><cell>49.2</cell><cell>34.75</cell><cell>94</cell><cell>249</cell><cell>13.06</cell></row><row><cell cols="4">Disentangled [John et al., 2019]</cell><cell>91.7</cell><cell>6.71</cell><cell>89</cell><cell>26</cell><cell>11.39</cell><cell>67.5</cell><cell>8.16</cell><cell>90</cell><cell>24</cell><cell>11.18</cell></row><row><cell cols="5">StyleTransformer [Dai et al., 2019] 86.2</cell><cell>27.45</cell><cell>96</cell><cell>231</cell><cell>14.29</cell><cell>63.1</cell><cell>40.91</cell><cell>95</cell><cell>180</cell><cell>14.74</cell></row><row><cell cols="2">StyIns (Ours)</cell><cell></cell><cell></cell><cell>90.8</cell><cell>26.03</cell><cell>96</cell><cell>109</cell><cell>14.83</cell><cell>67.8</cell><cell>46.73</cell><cell>96</cell><cell>92</cell><cell>16.11</cell></row><row><cell>Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CPVT (Semi-Supervised 1k)</cell><cell></cell><cell></cell><cell cols="2">GYAFC (Semi-Supervised 2.5k)</cell></row><row><cell cols="3">CPLS [Shang et al., 2019]</cell><cell></cell><cell>99.0</cell><cell>0.77</cell><cell>89</cell><cell>329</cell><cell>5.85</cell><cell>71.2</cell><cell>36.99</cell><cell>93</cell><cell>41</cell><cell>16.03</cell></row><row><cell cols="2">StyIns (Ours)</cell><cell></cell><cell></cell><cell>97.4</cell><cell>3.74</cell><cell>95</cell><cell>443</cell><cell>8.68</cell><cell>68.0</cell><cell>47.30</cell><cell>96</cell><cell>93</cell><cell>16.16</cell></row><row><cell>Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CPVT (Semi-Supervised 4k)</cell><cell></cell><cell></cell><cell cols="2">GYAFC (Semi-Supervised 10k)</cell></row><row><cell cols="3">CPLS [Shang et al., 2019]</cell><cell></cell><cell>98.3</cell><cell>3.13</cell><cell>90</cell><cell>283</cell><cell>8.37</cell><cell>71.4</cell><cell>39.25</cell><cell>94</cell><cell>44</cell><cell>16.24</cell></row><row><cell cols="2">StyIns (Ours)</cell><cell></cell><cell></cell><cell>97.5</cell><cell>4.00</cell><cell>95</cell><cell>410</cell><cell>8.86</cell><cell>70.6</cell><cell>47.81</cell><cell>96</cell><cell>92</cell><cell>16.36</cell></row><row><cell>Models</cell><cell>Sty.</cell><cell>Yelp Con.</cell><cell>Flu.</cell><cell>Sty.</cell><cell>GYAFC Con.</cell><cell>Flu.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DelRetri</cell><cell>3.26</cell><cell>3.24</cell><cell>3.46</cell><cell>2.31</cell><cell>2.37</cell><cell>2.39</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Template 3.03</cell><cell>3.34</cell><cell>3.12</cell><cell>2.16</cell><cell>3.56</cell><cell>2.97</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Disent.</cell><cell>3.95</cell><cell>3.20</cell><cell>4.46</cell><cell>2.84</cell><cell>1.85</cell><cell>3.79</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>StyleTr.</cell><cell>3.51</cell><cell>4.35</cell><cell>3.78</cell><cell>3.03</cell><cell>3.27</cell><cell>3.14</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>StyIns</cell><cell cols="4">4.52  *  4.41  *  4.41 3.97  Models Sty. Con. Flu</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CPLS</cell><cell cols="2">3.13 2.18</cell><cell cols="2">2.41</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>StyIns</cell><cell cols="2">2.82 3.67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* 4.41 * 4.48 * (a) Unsupervised transfer of sentiment and formality. * 2.91 * (b) Semi-supervised transfer of poeticness with 4k paired data.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Human evaluation results. The Krippen-dorff's alpha of human rating is 0.64, indicating acceptable inter-annotator agreement. The diacritic * (p &lt; 0.01) represents that StyIns significantly outperforms baseline models.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank anonymous reviewers for their insightful comments. This work is supported by the <rs type="funder">Major Program of the National Social Science Fund of China</rs> (Grant No. <rs type="grantNumber">18ZDA238</rs>), as well as the <rs type="projectName">NSFC</rs> project (Grant No. <rs type="grantNumber">61661146007</rs>) and the <rs type="projectName">NExT++</rs> project, the <rs type="funder">National Research Foundation</rs>, <rs type="funder">Prime Minister's Office, Singapore</rs> under its <rs type="funder">IRC@Singapore Funding Initiative</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_nmZAG2D">
					<idno type="grant-number">18ZDA238</idno>
					<orgName type="project" subtype="full">NSFC</orgName>
				</org>
				<org type="funded-project" xml:id="_bdw8e7F">
					<idno type="grant-number">61661146007</idno>
					<orgName type="project" subtype="full">NExT++</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><surname>Atanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">David Crystal. New perspectives for language study. 1:stylistics. English Language Teaching</title>
		<imprint>
			<date type="published" when="1970">2019. 2019. 2015. 2015. 1970</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Style transformer: Unpaired text style transfer without disentangled latent representation</title>
		<author>
			<persName><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="5997" to="6007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reinforcement learning based text style transfer without parallel training corpus</title>
		<author>
			<persName><forename type="first">Weller</forename><surname>Embler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Embler</surname></persName>
		</author>
		<author>
			<persName><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<editor>
			<persName><forename type="first">Lili</forename><surname>John</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hareesh</forename><surname>Mou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Olga</forename><surname>Bahuleyan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Vechtomova</surname></persName>
		</editor>
		<meeting>the Sixth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<publisher>Kingma and Welling</publisher>
			<date type="published" when="1967">1967. 1967. 2018. 2018. 2015. 2019. 2019. 2014. 2014. 2011. 2004. 2019. 2019. 2014. 2014. 2016</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="424" to="434" />
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Delete, retrieve, generate: a simple approach to sentiment and style transfer</title>
		<author>
			<persName><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018">2019. 2019. 2018. 2018</date>
			<biblScope unit="page" from="1865" to="1874" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A dual reinforcement learning framework for unsupervised text style transfer</title>
		<author>
			<persName><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<publisher>Yulia</publisher>
			<date type="published" when="2018">2019. 2019. 2018</date>
			<biblScope unit="page" from="5116" to="5122" />
		</imprint>
	</monogr>
	<note>ICLR. Prabhumoye et al., 2018] Shrimai Prabhumoye</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rezende and Mohamed, 2015] Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudha</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<editor>
			<persName><forename type="first">Noam</forename><surname>Vaswani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Niki</forename><surname>Shazeer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jakob</forename><surname>Parmar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Llion</forename><surname>Uszkoreit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lukasz</forename><surname>Gomez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</editor>
		<editor>
			<persName><surname>Polosukhin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2018. 2018. 2018. 2015. 2019. 2019. 2017. 2017. 2017. 2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
		<respStmt>
			<orgName>Rao and Tetreault</orgName>
		</respStmt>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A hierarchical reinforced sequence operation method for unsupervised text style transfer</title>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="4873" to="4883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask and infill: Applying masked language model to sentiment transfer</title>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="5271" to="5277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised text style transfer using language models as discriminators</title>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Emotional chatting machine: Emotional conversation generation with internaland external memory</title>
		<author>
			<persName><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<editor>
			<persName><surname>Zhu</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2018. 2018. 2018. 2018. 2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
