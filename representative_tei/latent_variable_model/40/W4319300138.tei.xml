<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analysis of Master Vein Attacks on Finger Vein Recognition Systems</title>
				<funder>
					<orgName type="full">AIP challenge program, Japan</orgName>
				</funder>
				<funder ref="#_YvsuZ3f #_faaWWud">
					<orgName type="full">JST CREST</orgName>
				</funder>
				<funder ref="#_hcDJKDR #_T9aBGvg #_hhFtTrt #_PXmhvD9 #_BXXXmbN">
					<orgName type="full">JSPS KAK-ENHI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-10-18">18 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Huy</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Trung-Nghia</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">University of Science</orgName>
								<orgName type="institution" key="instit2">VNU-HCM</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Vietnam National University</orgName>
								<address>
									<settlement>Ho Chi Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
							<email>jyamagis@nii.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Isao</forename><surname>Echizen</surname></persName>
							<email>iechizen@nii.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Tokyo</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Analysis of Master Vein Attacks on Finger Vein Recognition Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-18">18 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.10667v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Finger vein recognition (FVR) systems have been commercially used, especially in ATMs, for customer verification. Thus, it is essential to measure their robustness against various attack methods, especially when a handcrafted FVR system is used without any countermeasure methods. In this paper, we are the first in the literature to introduce master vein attacks in which we craft a vein-looking image so that it can falsely match with as many identities as possible by the FVR systems. We present two methods for generating master veins for use in attacking these systems. The first uses an adaptation of the latent variable evolution algorithm with a proposed generative model (a multi-stage combination of β-VAE and WGAN-GP models). The second uses an adversarial machine learning attack method to attack a strong surrogate CNN-based recognition system. The two methods can be easily combined to boost their attack ability. Experimental results demonstrated that the proposed methods alone and together achieved false acceptance rates up to 73.29% and 88.79%, respectively, against Miura's hand-crafted FVR system. We also point out that Miura's system is easily compromised by non-vein-looking samples generated by a WGAN-GP model with false acceptance rates up to 94.21%. The results raise the alarm about the robustness of such systems and suggest that master vein attacks should be considered an important security measure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Finger vein authentication (using a FVR system <ref type="bibr" target="#b23">[24]</ref>) was first commercially implemented in Japan in 1997 and has become well-recognized because of its application in ATMs to authenticate users <ref type="bibr" target="#b27">[28]</ref>. Its usage frees users from remembering and regularly changing passwords to maintain security. Due to their convenience, biometric authentication methods (including finger vein ones) have become widely used. Therefore, it is essential to evaluate their robustness and identify potential harms. A presentation attack is a common way to attack biometric recognition systems <ref type="bibr" target="#b16">[17]</ref>. Besides presenting a captured biometric trait of the victim, the attacker can use a wolf sample <ref type="bibr" target="#b26">[27]</ref>, which can match enrolled models of multiple identities. Master prints <ref type="bibr" target="#b1">[2]</ref> and master faces <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20]</ref> are examples of wolf samples generated using generative models. In reality, not all FVR systems have countermeasure methods deployed properly, allowing master vein attacks to compromise them.</p><p>Besides presentation attacks, there are several other ways that an attacker can compromise a biometric recognition system <ref type="bibr" target="#b22">[23]</ref>, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Moreover, in theory, it is possible to craft a physical object (known as a presentation attack instrument, or PAI) from a synthetic master vein (an image clearly showing the center lines of the veins) and use it to perform a presentation attack (attack 1 in Fig. <ref type="figure" target="#fig_0">1</ref>) <ref type="bibr" target="#b25">[26]</ref>. It is also possible to translate the synthetic master vein into a captured vein sample using a convolutional neural network (CNN) <ref type="bibr" target="#b21">[22]</ref> and use it to perform a logical attack (attack 2 in Fig. <ref type="figure" target="#fig_0">1</ref>). Due to these reasons, we focus on a logical attack with a clear vein image (attack 4 in Fig. <ref type="figure" target="#fig_0">1</ref>) in this work. We craft a master vein image and then inject it as a probe to attack FVR systems built based on Miura et al.'s design <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. A master vein image is a probe vein-looking image that can be falsely accepted as a match with enrolled models of multiple identities by a FVR system. Although non-vein-looking images may have better attack ability, it is harder for them to generalize on other systems and other attack scenarios. There are two possible solutions to craft such master veins: using the latent variable evolution (LVE) algorithm <ref type="bibr" target="#b1">[2]</ref> and using an adversarial machine learning (AdvML) attack <ref type="bibr" target="#b10">[11]</ref>.</p><p>The LVE algorithm is a common way to generate master biometric samples <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20]</ref>: a pre-trained generative model is used along with an evolutionary algorithm. The original work on generating master prints used a traditional generative adversarial network (GAN) model <ref type="bibr" target="#b4">[5]</ref> while the work on generating master faces <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20]</ref> used an advanced GAN model trained on a large facial database <ref type="bibr" target="#b11">[12]</ref>. With the original variational autoencoder (VAE) <ref type="bibr" target="#b12">[13]</ref> and β-VAE <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b2">3]</ref> generative models, there is a trade-off between image quality and the ability to learn disentangled representations. The traditional VAE and GAN <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> models have trouble generating large images (320 × 240 pixels in our case), while advanced models are data-hungry. The ability of the LVE algorithm to achieve good results depends on the disentanglement ability of the generative model. Moreover, compatibility with modern vein capturing devices depends on the model's ability to generate high-resolution vein images.</p><p>An AdvML attack using an adversarial example can be used to change the output of a CNN <ref type="bibr" target="#b10">[11]</ref>. It is assumed that an attacker using a master vein attack does not know the identities of the enrolled models. The attacker thus attempts to generate a master vein that can match as many enrolled models as possible. State-of-the-art CNN-based FVR systems use different approaches between training and testing. For example, for a system <ref type="bibr" target="#b13">[14]</ref> that uses the additive angular margin loss <ref type="bibr" target="#b3">[4]</ref> in training, the task is to minimize the crossentropy loss or focal loss <ref type="bibr" target="#b14">[15]</ref> using the provided labels. In evaluation, cosine similarity is used to calculate the distance between the two embedded features of the probe and the model veins. Therefore, traditional adversarial attacks could not be applied in this case. Moreover, adversarial attacks are exclusive to machine-learning-based recognition systems. They are unlikely to generalize well to handcrafted recognition systems.</p><p>This work aims to solve to two above problems and then combine the two newly proposed solutions to generate master vein images that can attack both hand-crafted and deeplearning-based vein recognition systems. For the generative model used by the LVE algorithm, we proposed a method to combine the β-VAE model and the Wasserstein GAN with a gradient penalty (WGAN-GP) <ref type="bibr" target="#b4">[5]</ref> model. The combination model can effectively learn disentanglement latent rep-resentations essential for the LVE algorithm and is capable of generating images with higher quality than the single models. Using this setting, we can successfully attack a hand-crafted system with about 70% of false acceptance rates (FARs). However, this LVE-based method could not work on the CNN-based FVR systems, leading to the development of the adversarial-attack-based one. Unlike traditional adversarial attack methods, we propose using k labels as targets. Since the target system uses cosine distance between the two embedded features in inference mode, we attack its training configuration, which uses an advanced addictive angular margin loss function. To make the attack more general, we combine these two proposed methods. By performing an adversarial attack on the master vein generated by the LVE-based method, the crafted master vein can fool both hand-crafted and CNN-based recognition systems with higher FARs (up to 88.79% for the hand-crafted system) than those of master veins created by single methods.</p><p>In summary, the contributions of this work are four-fold:</p><p>• We point out that a hand-crafted vein recognition system without any countermeasure methods can be easily compromised by non-vein-looking images generated by a WGAN-GP model. We are also the first in the literature to investigate synthesized vein-looking images to perform master vein attacks.</p><p>• We introduce a way to combine a β-VAE model and a WGAN-GP model to generate large, good-quality vein-looking images with better disentanglement. The trained β-VAE decoder extracted from this combination is then used in the LVE algorithm.</p><p>• We present a k-label targeted adversarial attack for use in attacking a CNN-based FVR system. This target CNN-based system was trained using an advanced loss function (additive angular margin), outperforming a hand-crafted system.</p><p>• We describe a highly successful attack that combines a latent LVE-based attack with an adversarial attack on a hand-crafted FVR system. We show that robustness against master vein attacks is an important measure for FVR systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Finger Vein Recognition Systems</head><p>A typical vein recognition system usually has four modules (visualized in Fig. <ref type="figure" target="#fig_0">1</ref>): a data capturer, a feature extractor, a matcher, and a decision maker <ref type="bibr" target="#b22">[23]</ref>. Pre-processing operations may be applied before feature extraction. In the original work of Miura et al. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, the maximum curvature method and the repeated line tracking method were used for the feature extractor module and the crosscorrelation method was used for the matcher module. The maximum curvature method was designed to be robust against varying vein widths and non-uniform brightness. We used it in a baseline handcrafted FVR system, which we refer to as "Miura's system."</p><p>Besides fully handcrafted systems as introduced above, machine learning methods have been used to build the feature extractor and/or the matcher <ref type="bibr" target="#b23">[24]</ref>. For instance, Kuzu et al. <ref type="bibr" target="#b13">[14]</ref> used a modified version of the DenseNet-161 model <ref type="bibr" target="#b9">[10]</ref> to build a feature extractor and used cosine distance as the metric for matching. The modified DenseNet-161 model was trained using the additive angular margin loss <ref type="bibr" target="#b3">[4]</ref> on the provided ground-truth labels. When testing, it was used to calculate the embedded features of the probe and the model finger vein. An overview of this kind of system is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. To build a conceptual attackable CNN-based model for evaluation, we used this kind of CNN as an additional feature extractor to take the output of the maximum curvature-based feature extractor. We combined the additional CNN-based feature extractor with the cosine similarity-based matcher to form a new matcher. In our experiments, we use two modified versions of ResNet-18 <ref type="bibr" target="#b6">[7]</ref> and a modified version of MobileNetV3-Large <ref type="bibr" target="#b8">[9]</ref> (representing small networks) and ResNeXt-50 <ref type="bibr" target="#b28">[29]</ref> (representing a large network) as the additional CNN feature extractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attacks on Biometric Recognition Systems</head><p>A sample is considered a "wolf" if it can be falsely accepted as a match with models from multiple identities in a biometric recognition system <ref type="bibr" target="#b26">[27]</ref>. A wolf sample can be either biometric or non-biometric. Wolf attacks using wolf samples were initially used to target fingerprint recognition systems <ref type="bibr" target="#b22">[23]</ref>. A master biometric attack is a particular case of a "wolf attack" in which the wolf sample looks similar to a biometric trait. A non-biometric wolf sample does not have any constraint on it, hence can be in any appearance. Therefore, non-biometric wolf samples are easier to craft and may have better attack ability than master biometric samples. However, a spoofing detector or a quality assessor integrated into a biometric recognition system can quickly reject them before being recognized. Moreover, since most non-biometric wolf samples mainly focus on a specific flaw in a particular system, they may not generalize well. Therefore, in this paper, we choose to investigate a master finger vein attack.</p><p>Master biometric attacks using master biometric samples have been recently used to attack partial fingerprint recognition systems <ref type="bibr" target="#b1">[2]</ref> and face recognition systems <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20]</ref>. A latent variable evolution algorithm <ref type="bibr" target="#b1">[2]</ref> is used to generate such master biometric traits by combining an evolutionary algorithm with a pre-trained generative model. The covariance matrix adaptation evolution strategy (CMA-ES) <ref type="bibr" target="#b5">[6]</ref> is a popular choice for the evolution algorithm due to its novel design for non-linear and non-convex functions. It is sufficient for a low-resolution biometric trait (like partial fingerprints) generator to use a traditional generative model like WGAN-GP <ref type="bibr" target="#b4">[5]</ref>. For high-resolution biometric traits like faces, it requires a more complex generative model like StyleGAN <ref type="bibr" target="#b11">[12]</ref>. In this work, vein images do not need very high-resolution like facial ones, but low-resolution images like partial fingerprints are not sufficient. Thus, we could not simply use WGAN-GP as the generative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Methods</head><p>We first discuss the attack strategy used in this paper. We then introduce two methods for generating master veins, one using the LVE algorithm and one using an adversarial attack, and describe a way to combine them. We assume that the target FVR systems do not use any spoofing detector or quality assessor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Attack Strategy Analysis</head><p>There are several positions where an attack can be carried out on a FVR system, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We aim to maximize the scope and effectiveness of master vein attacks given limited resources. It is crucial to ensure that the crafted master veins can be used to attack various systems under various conditions. The captured finger vein images are sensor-dependent, and the structure of the veins is unclear because of noise and lack of pre-processing. If we generate coarse master veins to carry out attack 2, the generative model cannot effectively learn the vein representations. The generated master veins also do not work well with other data capture devices. The vein images used to perform attack 4 are more precise, which is more suitable for training the generative model, performing attacks, and analysis. It is possible to translate a master vein into a corresponding captured image using a CNN <ref type="bibr" target="#b21">[22]</ref> to perform attack 2. Moreover, an attacker can craft a corresponding PAI given an image of finger veins <ref type="bibr" target="#b25">[26]</ref> that can be used to carry out a presentation attack (attack 1). In summary, in theory, it is possible to carry out attacks 1 and 2 if we can carry out attack 4.</p><p>Miura's system can perform both symmetric matching (or full matching) and asymmetric matching (or partial matching). For partial matching, the probe is a randomly cropped image of the complete one. This is similar to the scenario in the work of Bontrager et al. <ref type="bibr" target="#b1">[2]</ref>. Before performing random cropping on an input vein image, the system uses an algorithm to calculate a mask to extract the veinonly region first. For simplicity, we assume that this region is provided. In reality, because of this algorithm, non-veinlooking master vein images may not be cropped appropriately, reducing their attack ability. For CNN-based systems, the networks can only perform full matching. Therefore, to ensure generalizability, we focus on generating full master vein images. Furthermore, the full master vein images can be cropped for partial matching in Miura's system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attack Using LVE-Based Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Method's Description</head><p>The work of Bontrager et al. <ref type="bibr" target="#b1">[2]</ref> used a WGAN-GP <ref type="bibr" target="#b4">[5]</ref> to generate partial fingerprints (hereafter LVE 1 ). However, a WGAN-GP is hard to train, especially with limited training data. A β-VAE is easier to train and could learn better disentangled representations (hereafter LVE 2 ). However, its generated images have low quality. Therefore, we fuse their strengths in our proposed generator and use the LVE algorithm <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref> to generate master veins (hereafter LVE 3 ).</p><p>An overview of our proposed LVE 3 method is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. To achieve a better generative model with better learned disentanglement representations and highresolution output, we first train a β-VAE model <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b2">3]</ref> by b. WGAN-GP (LVE 1 ) d. Our method (LVE 3 ) Figure <ref type="figure">4</ref>. Original image and images generated using WGAN-GP method, β-VAE method, and our proposed method (best viewed in the digital version with zoom-in). More samples are shown in the Supplementary Material. The WGAN-GP method failed to generate a vein-looking image while the β-VAE method generated a blurry image. Our proposed method generated a clearer image than the other two methods.</p><p>minimizing Eq. 1.</p><formula xml:id="formula_0">L β-VAE (θ, φ; x, z, C) = E q φ (z|x) [log p θ (x|z)] -γ|D KL (q φ (z|x) p θ (z)) -C|,<label>(1)</label></formula><p>where φ and θ parameterize the distributions of the encoder q φ and decoder p θ , respectively, and D KL ( ) represents Kullback-Leibler divergence.</p><p>Then we fine-tune the decoder by using the WGAN-GP discriminator (minimizing Eq. 2). Using this discriminator improves the quality of the generated images. To ensure stability, we freeze the parameters of q φ and most of p θ except for the last three convolutional layers of p θ when minimizing L GAN . Finger vein images generated using the WGAN-GP method, the β-VAE method, and our proposed method are shown in Fig. <ref type="figure">4</ref> in this paper and in Fig. <ref type="figure" target="#fig_0">1</ref> in the Supplementary Material. The WGAN-GP method failed to generate a realistic vein image while our method generated a clearer image than β-VAE.</p><formula xml:id="formula_1">L GAN = E x∼Pg [D(x)] -E x∼Pr [D(x)] +λE x∼P x [( ∇ xD(x) 2 -1) 2 ],<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">• x = p θ (x|z) = p θ (x|q φ (z|x)</formula><p>).</p><p>• P r and P g are the real and generated data distributions, respectively.</p><p>• P x is sampled uniformly along straight lines between pairs of points sampled from P r and P g .</p><p>The LVE algorithm is described in Alg. 1 in the Supplementary Material. For simplicity, we use the CMA-ES <ref type="bibr" target="#b5">[6]</ref> for the evolutionary algorithm. Only the decoder p θ of the β-VAE is used when running the LVE algorithm. It plays the role of the generator to generate vein images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Preliminary Analysis</head><p>The false acceptance rate (FAR -the rate at which unauthorized or illegitimate users are verified) calculated when running the LVE algorithm are plotted in Fig. <ref type="figure" target="#fig_4">5</ref>. The master vein generated using the LVE 3 method on Miura's system is shown in Fig. <ref type="figure" target="#fig_6">6</ref>.b. Surprisingly, random non-veinlooking finger veins generated by the LVE 1 method (using WGAN-GP) easily fooled Miura's system with the FARs higher than 90%, even without the help of the LVE algorithm. Its cross-correlation-based matcher module failed to work correctly with these wolf samples. This finding raises an urgent alarm on the reliability of the Miura's system without a spoofing detector or a quality assessor integrated.</p><p>Besides the above irregular case, the proposed LVE 3 method worked better than the LVE 2 method (using β-VAE) on Miura's system (with the FARs about 70% and 50%, respectively). This result confirms the effectiveness of our multi-stage combination of β-VAE and WGAN-GP for the generator. Although they are not perfect-looking, finger veins generated by the LVE 3 method are more natural than those generated by the LVE 1 and the LVE 2 methods, reducing the possibility of being detected by the spoofing detectors or being rejected by the quality assessors. For a CNN-based recognition system, the LVE 1 and LVE 3 methods failed to work on the ResNeXt-50-based system with near-zero FARs. This failure may be due to the ResNeXt-50-based system being a large network trained on a welldesigned loss function <ref type="bibr" target="#b3">[4]</ref>, preventing the formation of dense clusters in its embedding space (discussed in Nguyen et al.'s work <ref type="bibr" target="#b19">[20]</ref>). We thus investigated another method to attack the CNN-based system, as described in the next section.  When the number of iterations increases, only the FARs of Miura's system increase, implying that the LVE-based method only works on this system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Attack Using Adversarial Machine Learning Method</head><p>We propose using a modified version of the l ∞ projected gradient descent attack <ref type="bibr" target="#b15">[16]</ref> described by Eq. 3. We use a filter K to control the shape of the perturbations, a mask M to limit the area of the perturbations to the area containing the veins, and a soft-label vector y to control the target identities of the attack. Since the synthesized veins lay on fingers with similar shapes and locations, the mask M can be easily approximated and crafted beforehand by hand or using a gap-filling algorithm.</p><formula xml:id="formula_3">x t+1 = Clip x, (x t + α(ζ * K) M ) with ζ = ∇ x L(θ, x t , y),<label>(3)</label></formula><p>where x is the input image, y is a target soft-label vector, θ is the set of target model parameters, K is the filter kernel, M is the finger vein mask, * is the convolutional operator, and is the element-wise multiplication operator. Unlike traditional adversarial targeted attacks, we target multiple labels instead of a single label. We call this k-label targeted attack. In more detail, we choose 1 &lt; k &lt; N of the total N labels as target labels and set their probabilities to 1/k, with k as a hyper-parameter. For example, if we choose 3/5 of all labels, the target vector y is set to [0.33, 0, 0.33, 0.33, 0]. Since the FVR system does not calculate class probabilities during its testing phase but embeddings (see Fig. <ref type="figure" target="#fig_1">2</ref>), we need to attack the configuration of the training phase. In addition to randomly selecting k target labels, we can choose the top-k labels with the highest predicted probabilities. Doing so can make the optimizer process converge faster with fewer perturbations. Examples of randomly selected k labels and top-k labels are shown  in Fig. <ref type="figure" target="#fig_6">6</ref>.d and Fig. <ref type="figure" target="#fig_6">6</ref>.e, respectively. If k is close to 1, the attack is meaningless. Otherwise, it is hard to optimize the perturbations. In our experiments, the target CNN-based FVR system was powerful, with 0% in both false acceptance rate and false rejection rate on the training set. Therefore, we used a small top-k = 5% in our experiments so that the optimization could successfully converge.</p><p>Besides the well-known hyper-parameter , the number of iterations and the kind of filter K are also important hyper-parameters. If the number of iterations is too small, the optimization will not converge. If it is too large, the perturbations will be too strong, damaging the original image. A damaged master vein image may be rejected by the quality assessor if implemented. Furthermore, it could not be generalized to other systems. Fig. <ref type="figure" target="#fig_1">2</ref> in the Supplementary Material shows such an effect. When the number of iterations is around 200, the image is noisy, and when it is 500 or 1000, it is almost impossible to perceive the veins. We used 100 iterations in our experiments.</p><p>Regarding filter K, it is used to control the shape of the perturbations. Ideally, the perturbations should look like veins rather than random patterns or noise. In reality, it is very challenging to achieve this goal. We initially used a (differentiable) CNN-based vein/non-vein classifier as a loss function in optimizing the AdvML attack. However, it was easily fooled. On the other hand, a non-differentiable hand-crafted one was not useful for optimization with backpropagation. Therefore, we simply use a filter kernel for K to regularize the perturbations. The goal is to avoid tiny-dot noise, which can be easily destroyed and is harder to adapt to other attack scenarios. We evaluated Gaussian blur, lowpass, high-pass, and Laplacian kernel. They have different effects on the rate of convergence of the optimization process and the quality of the master vein images. Examples are shown in Fig. <ref type="figure" target="#fig_2">3</ref> in the Supplementary Material. The Gaussian blur kernel helped the optimization process converge the fastest and produced large-size perturbations with the least amount of tiny-dot noise (except the low-pass kernel), so it is the most suitable candidate kernel. The low-pass kernel destroyed almost all adversarial perturbations, preventing the adversarial attack. The high-pass and Laplacian kernels allowed too much tiny-dot noise. When a filter was not used, the optimizer process took a long time to converge, and the crafted perturbations were also tiny-dot noise.</p><p>Instead of using a bona fide image, we can use a master vein image as the input image x. If we use a master vein image crafted using the LVE-based method, the corresponding adversarial image can work on both handcrafted recognition systems like Miura's one and CNN-based recognition systems, resulting in better generalizability. An example result of this combination attack is shown in Fig. <ref type="figure" target="#fig_6">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We investigated the attack abilities of master veins crafted using the LVE-based method, the adversarial machine learning method, and their combination in white-box, gray-box, and black-box scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Databases</head><p>We used two finger vein databases:</p><p>• The SDUMLA-HMT Database <ref type="bibr" target="#b29">[30]</ref> contains images of six fingers per subject (six images per finger). We divided it into a training set containing the images for 80 subjects and a test set containing the images for 26 subjects. The training set was used to train the CNN-based recognition systems, set the recognition systems' matching thresholds, train the generative models, and generate master veins. We used both the training and test sets for evaluation.</p><p>• The VERA FingerVein Database <ref type="bibr" target="#b25">[26]</ref> contains bona fide images of two fingers of 110 subjects (two images per finger). We used the entire database to evaluate black-box attacks (in terms of database). Since its distribution is different from that of the SDUMLA-HMT Database, the recognition systems' matching thresholds calculated on the SDUMLA-HMT Database could not be used and needed to be re-set for this database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Finger Vein Recognition Systems</head><p>We used three FVR systems: one hand-crafted system (Miura's system) and three CNN-based systems (one based on ResNet-18, one based on ResNeXt-50, and one based on MobileNetV3-Large). We chose MobileNetV3-Large (MobileNetV3-L) from the MobileNet family since it performed the best on FVR. For Miura's system (customized from Idiap bob's implementation), we evaluated both partial matching and full matching. We are aware of other hand-crafted recognition systems <ref type="bibr" target="#b23">[24]</ref> using other features extracted from the local binary patterns, principal component analysis, or Gabor filters. This paper focuses on attack 4 in Fig. <ref type="figure" target="#fig_0">1</ref>, however, these systems have different feature extractors. Attacking them requires building a mapping network to convert the master veins to their raw forms and perform attack number 2. We treated it as future work.</p><p>To generate master veins, we used Miura's system and the ResNeXt-50-based system as surrogate FVR systems. Since the LVE-based method failed to generate master veins on the ResNeXt-50-based recognition system, we ignored this case in all of our experiments. Hereafter, the LVEbased method is assumed to be the one running on Miura's system.</p><p>To evaluate performance, we used Miura's system and the ResNeXt-50-based system for evaluating white-box attacks and the ResNet-18-and MobileNetV3-L-based ones for evaluating black-box attacks in terms of FVR systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Methodology</head><p>We use FAR as the primary metric to define the effectiveness of master vein attacks. We compare the FARs of a FVR system on a normal dataset (without master veins) and a master vein dataset where the zero-effort imposter's probes were replaced by the master vein probes. If the FAR on the master vein dataset is moderately higher than the FAR on the normal dataset, the master vein attack is considered successful.</p><p>For the partial matching mode of Miura's system, we first randomly cropped the full vein images to the size of 128 × 128 pixels and then used them as the probes. Modal images are always full-size ones (320 × 240 pixels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Attacks on Known Databases and Systems</head><p>We first evaluated master vein attacks on the same and similar configurations we used to craft the master vein. In terms of databases, we attacked the SDUMLA-HMT one, resulting in a white-box attack. In terms of FVR systems, we attacked two types of systems, resulting in both white-box and black-box attacks. In more detail, attacks using master veins generated using Miura's system on CNN-based systems are black-box ones and vice versa, while attacking the same system are white-box ones. Merging both terms, we have white-box and gray-box attacks.</p><p>The FARs on bona fide veins and master veins are shown in Table <ref type="table" target="#tab_0">1</ref>. Miura's system is extremely vulnerable to nonvein-looking wolf attack generated by the LVE 1 method, with about 69% for partial matching and 93.5% for full matching, as mentioned earlier in section 3.2.2. The pos-sible reason is that when developing this system, wolf attacks had not been introduced. Therefore, the designation of its matching algorithm (based on cross-correlation) only considered vein-looking probes.</p><p>Miura's system is also vulnerable to vein-looking master vein attacks using the LVE 2,3 methods, AdvML method, and their combination. The LVE 3 method outperformed the LVE 2 one and achieved the FARs of about 70% on both train and test sets. It means that nearly two-thirds of the identities were falsely matched with the master veins. It happened because these generators can generate non-exist random finger vein images, and Miura's matcher algorithm has flaws. The LVE algorithm then guided them to generate finger vein images with wolf characteristics with largeenough iterations.</p><p>Although it is a black-box attack in terms of FVR systems, the AdvML method achieved the FARs of about 12% for partial matching and about 40% for full matching by Miura's system. The plausible explanation is that the Ad-vML master veins were optimized with full matching mode (CNN-based systems' only mode), their performance on partial matching mode is suboptimal.</p><p>A combination of the AdvML method and the LVE 3 method sustainably raised the FARs of Miura's system in the full matching mode, which is about 85%. However, it was not effective for the partial matching mode. Using the top probabilities label (denoted as (Top) in the table) helped increase the FARs on Miura's system in this partial matching mode and on the CNN-based systems. Its combination with the LVE 1 method has the reverse tendency (possibly because LVE 1 and LVE 3 have different characteristics). Regarding robustness, CNN-based systems resist master vein attacks better than Miura's system. Their FARs only slightly increased (about 1 to 3%) when attacks occurred. Regarding generalization, master vein attacks could not work well on unseen CNN-based systems (ResNet-18 and MobileNetV3-L).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Cross-Database and Cross-System Attacks</head><p>Next, we evaluated master vein attacks on more challenging scenarios. In terms of database, we attack a different database -the VERA FingerVein Database. In terms of FVR systems, attacks on Miura's system and the ResNeXt-50 system are white-box while attacks on ResNet-18 and MobileNetV3-L are black-box. Table <ref type="table" target="#tab_1">2</ref> shows the FARs on bona fide and master veins. Miura's system continued to be vulnerable to wolf attacks and master veins attacks. For master vein attacks, the FARs were around 20% and could reach 47.73% when we used the combination method to attack the full matching mode. Using top labels helped increase the FAR of the attack on partial matching mode to 22.25%. On the other hand, the CNN-based recognition systems were robust against master vein attacks. However, it is important to note that the CNN-based recognition systems could not generalize well on the VERA FingerVein Database, resulting in higher FARs for bona fide vein attacks. It implies that if we want to use the CNN-based recognition systems effectively (so that bona fide users are not falsely rejected), we need to train them on the current dataset. However, doing so also opens a chance for master vein attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Summary</head><p>Miura's system in partial matching and full-matching modes was vulnerable to non-vein-looking wolf attacks and vein-looking master vein attacks in white-box and gray-box scenarios. Both attacks substantially increased the FARs for Miura's system while barely increasing them for the CNNbased systems. A combination of the LVE 3 method and the AdvML method can reach 88.79% FAR on the full matching mode of Miura's system. Small increments of FARs on CNN-based systems indicate that they are more robust on master vein attacks.</p><p>It was challenging to perform master vein black-box attacks when both the target recognition system and the database were unknown. However, in reality, handcrafted FVR systems have already been deployed in ATMs, and the replacement cost is high. Since their variety is limited, attackers can narrow the scope of their attacks to gray-box or even white-box. Attackers can also prepare a set of potentially effective master veins in advance. Due to these reasons, master vein attacks still be a viable threat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Social Impacts</head><p>To avoid possible harm, we use academic freelyaccessed finger vein databases and open-source FVR systems. We believe our findings are necessary to raise awareness and promote improving the robustness of such systems. Besides robustness, we suggest using a fake finger vein detector to detect master veins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>We have demonstrated that non-vein-looking wolf samples (generated by WGAN-GP) and vein-looking master veins generated using our proposed methods (LVE-based method, adversarial machine learning attack, and their combination) can successfully perform while-box and gray-box attacks on FVR systems. Miura's handcrafted system is fragile against such attacks, while CNN-based methods are more robust. Since not all commercial FVR systems are deep learning-based, their variety is limited, and the countermeasure methods are not always available, the threat of master vein attacks should not be underestimated.</p><p>Future work will focus on performing adversarial attacks to minimize cosine distance instead of maximizing label probabilities, improving the shape of adversarial perturbations to make them more vein-like, and evaluating more FVR systems and databases. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Overview of FVR system and possible attacks on it, inspired by Ratha et al. [23]. This paper focuses on attack 4 by injecting a master vein probe image.</figDesc><graphic coords="2,68.36,166.31,137.04,77.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Illustration of training and testing phases of CNN-based FVR system. The training phase uses the additive angular margin loss with ground-truth labels while the testing phase uses cosine distance between the two embedded features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Proposed LVE-based method. Only modules in the dashed polygon are used when running the LVE algorithm.</figDesc><graphic coords="4,21.92,247.65,572.71,114.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>method -WGAN-GP Miura's method -Proposed Miura's method -beta-VAE ResNeXt-50 -WGAN-GP ResNeXt-50 -Proposed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. FARs when running the LVE algorithm on Miura's and ResNeXt-50-based systems. When the number of iterations increases, only the FARs of Miura's system increase, implying that the LVE-based method only works on this system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Master veins generated using LVE method, adversarial machine learning method, their combination, and the corresponding mask used for adversarial attacks (best viewed in the digital version with zoom-in).</figDesc><graphic coords="6,196.04,72.49,59.13,78.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .Figure 8 .Figure 9 .Figure 10 .Figure 11 .</head><label>7891011</label><figDesc>Figure7. Examples of real finger veins (first row) and those generated by our proposed method (LVE 3 , second row), β-VAE (LVE 2 , third row), and WGAN-GP (LVE 1 , last row). Since latent codes are sampling from noise, generated images have the randomness property. Among synthetic finger veins, those generated by our method had the best quality, while those generated by WGAN-GP do not look like finger veins.</figDesc><graphic coords="11,50.68,379.83,493.09,98.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>FARs (in %) of three FVR systems on SDUMLA-HMT Database with bona fide and master veins. Gray cells indicate gray-box attacks; white cells indicate white-box attacks. Bold numbers indicate that the master vein attacks have FARs higher than those of the corresponding bona fide cases by at least 1%. Dataset Train set Test set Train set Test set Train set Test set Train set Test set Train set Test set</figDesc><table><row><cell>Matcher</cell><cell cols="2">Miura's system (Partial matching)</cell><cell cols="2">Miura's system (Full matching)</cell><cell cols="2">ResNeXt-50</cell><cell cols="2">ResNet-18</cell><cell cols="2">MobileNetV3-L</cell></row><row><cell>Attack \ Bona fide</cell><cell>07.57</cell><cell>08.02</cell><cell>08.46</cell><cell>08.98</cell><cell>0.00</cell><cell>2.25</cell><cell>0.00</cell><cell>3.37</cell><cell>0.00</cell><cell>1.31</cell></row><row><cell>LVE 1 (WGAN-GP)</cell><cell>68.24</cell><cell>70.41</cell><cell>92.46</cell><cell>94.21</cell><cell>1.85</cell><cell>1.92</cell><cell>1.51</cell><cell>2.25</cell><cell>0.67</cell><cell>1.50</cell></row><row><cell>LVE 2 (β-VAE)</cell><cell>59.63</cell><cell>59.27</cell><cell>54.75</cell><cell>43.89</cell><cell>0.10</cell><cell>1.44</cell><cell>0.90</cell><cell>2.42</cell><cell>0.33</cell><cell>0.33</cell></row><row><cell>LVE 3 (Combination)</cell><cell>70.47</cell><cell>69.85</cell><cell>73.29</cell><cell>71.84</cell><cell>1.46</cell><cell>6.07</cell><cell>0.96</cell><cell>5.86</cell><cell>0.53</cell><cell>2.03</cell></row><row><cell>AdvML</cell><cell>11.34</cell><cell>13.11</cell><cell>32.02</cell><cell>49.52</cell><cell>1.88</cell><cell>3.69</cell><cell>1.44</cell><cell>2.24</cell><cell>0.61</cell><cell>1.46</cell></row><row><cell>LVE 3 + AdvML</cell><cell>48.20</cell><cell>50.00</cell><cell>82.36</cell><cell>88.79</cell><cell>1.82</cell><cell>3.35</cell><cell>1.15</cell><cell>1.93</cell><cell>0.48</cell><cell>0.64</cell></row><row><cell>LVE 3 + AdvML (Top)</cell><cell>62.73</cell><cell>62.52</cell><cell>77.82</cell><cell>80.41</cell><cell>2.37</cell><cell>5.32</cell><cell>1.60</cell><cell>4.00</cell><cell>1.03</cell><cell>3.47</cell></row><row><cell>LVE 1 + AdvML (Top)</cell><cell>76.60</cell><cell>76.95</cell><cell>91.86</cell><cell>93.81</cell><cell>1.68</cell><cell>1.85</cell><cell>1.52</cell><cell>2.09</cell><cell>0.55</cell><cell>0.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>FARs (in %) of three FVR systems on VERA FingerVein Database with bona fide and master veins. Bold numbers indicate that the master vein attacks have FARs higher than those of the corresponding bona fide cases by at least 1%.</figDesc><table><row><cell>Matcher</cell><cell>Miura's</cell><cell>Miura's</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>system</cell><cell>system</cell><cell>ResNeXt</cell><cell>ResNet</cell><cell>Mobile</cell></row><row><cell></cell><cell>(Partial</cell><cell>(Full</cell><cell>50</cell><cell>18</cell><cell>NetV3-L</cell></row><row><cell>Attack</cell><cell>matching)</cell><cell>matching)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bona fide</cell><cell>04.07</cell><cell>03.13</cell><cell>8.22</cell><cell>7.28</cell><cell>8.10</cell></row><row><cell>LVE 1 (WGAN)</cell><cell>38.84</cell><cell>43.86</cell><cell>0.18</cell><cell>0.10</cell><cell>0.18</cell></row><row><cell>LVE 2 (β-VAE)</cell><cell>15.08</cell><cell>02.92</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>LVE 3 (Comb.)</cell><cell>20.84</cell><cell>19.54</cell><cell>0.54</cell><cell>0.00</cell><cell>0.01</cell></row><row><cell>AdvML (A)</cell><cell>03.12</cell><cell>03.57</cell><cell>0.20</cell><cell>0.04</cell><cell>0.18</cell></row><row><cell>LVE 3 +A</cell><cell>16.37</cell><cell>47.73</cell><cell>0.42</cell><cell>0.01</cell><cell>0.18</cell></row><row><cell>LVE 3 +A (Top)</cell><cell>22.25</cell><cell>26.34</cell><cell>0.82</cell><cell>0.52</cell><cell>0.21</cell></row><row><cell>LVE 1 +A (Top)</cell><cell>39.28</cell><cell>44.49</cell><cell>0.18</cell><cell>0.01</cell><cell>0.17</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/CMA-ES/pycma</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was partially supported by <rs type="funder">JSPS KAK-ENHI</rs> Grants <rs type="grantNumber">JP16H06302</rs>, <rs type="grantNumber">JP18H04120</rs>, <rs type="grantNumber">JP20K23355</rs>, <rs type="grantNumber">JP21H04907</rs>, and <rs type="grantNumber">JP21K18023</rs>, and by <rs type="funder">JST CREST</rs> Grants <rs type="grantNumber">JPMJCR18A6</rs> and <rs type="grantNumber">JPMJCR20D3</rs>, including the <rs type="funder">AIP challenge program, Japan</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hcDJKDR">
					<idno type="grant-number">JP16H06302</idno>
				</org>
				<org type="funding" xml:id="_T9aBGvg">
					<idno type="grant-number">JP18H04120</idno>
				</org>
				<org type="funding" xml:id="_hhFtTrt">
					<idno type="grant-number">JP20K23355</idno>
				</org>
				<org type="funding" xml:id="_PXmhvD9">
					<idno type="grant-number">JP21H04907</idno>
				</org>
				<org type="funding" xml:id="_BXXXmbN">
					<idno type="grant-number">JP21K18023</idno>
				</org>
				<org type="funding" xml:id="_YvsuZ3f">
					<idno type="grant-number">JPMJCR18A6</idno>
				</org>
				<org type="funding" xml:id="_faaWWud">
					<idno type="grant-number">JPMJCR20D3</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>The Appendices are organized as follows. First, we provide the detail of the latent variable algorithm (LVE) used to craft master veins in section A. Next, we include some additional visualizations of real and generated finger veins in different settings in section B. Last, we present an ablation study on top-k AdvML attack in section C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Latent Variable Evolution Algorithm</head><p>The LVE algorithm, which is used by the LVE-based methods to generate master veins (visualized in Fig. <ref type="figure">3</ref> in the main paper), is described in detail by Alg. 1. Regarding the implementation of the CMA-ES, we used the pycma library 1 . For simplicity, we used its default parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Visualizations of Generated Finger Veins</head><p>Additional samples of real finger veins and those generated by the LVE-based methods are shown in Fig. <ref type="figure">7</ref>. Effects of the number of iterations and filters used by the AdvML method on the quality of adversarial master veins are visualized in Figs. <ref type="figure">8</ref> and<ref type="figure">9</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study on Top-k AdvML Attack</head><p>Examples of adversarial master veins generated with different k values in the AdvML attack with top-k labels are shown in Fig. <ref type="figure">10</ref>. There are no significant differences in the amounts of perturbations between them. The relationship between k and FARs is visualized in Fig. <ref type="figure">11</ref>. There are no significant differences in the FARs, especially when k is in the [5%, 60%] range. In practice, it is better to avoid the extreme values of k. If k is too small, its attack ability is limited. If k is too large, it makes the optimization hard to converge.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deepmasterprints: Generating masterprints for dictionary attacks via latent variable evolution</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bontrager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Nasir Memon</surname></persName>
		</author>
		<author>
			<persName><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BTAS</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Understanding disentangling in β-VAE</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Christopher P Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><surname>Lerchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>NIPSW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ArcFace: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Completely derandomized self-adaptation in evolution strategies</title>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Ostermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="195" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">beta-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Searching for Mo-bileNetV3</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial machine learning</title>
		<author>
			<persName><forename type="first">Ling</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Ip Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Security and Artificial Intelligence</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="43" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Emanuele Maiorana, and Patrizio Campisi. Loss functions for cnn-based biometric vein recognition</title>
		<author>
			<persName><forename type="first">Rιdvan</forename><surname>Salih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuzu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUSIPCO</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="750" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Handbook of biometric anti-spoofing: Presentation attack detection</title>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mark S Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Fierrez</surname></persName>
		</author>
		<author>
			<persName><surname>Evans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Feature extraction of finger-vein patterns based on repeated line tracking and its application to personal identification. Machine Vision and Applications</title>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akio</forename><surname>Nagasaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takafumi</forename><surname>Miyatake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="194" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Extraction of finger-vein patterns using maximum curvature points in image profiles</title>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akio</forename><surname>Nagasaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takafumi</forename><surname>Miyatake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1185" to="1194" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Master face attacks on face recognition systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junichi</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isao</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><surname>Echizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biometrics, Behavior, and Identity Science</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generating master faces for use in performing wolf attacks on face recognition systems</title>
		<author>
			<persName><forename type="first">Junichi</forename><surname>Huy H Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isao</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Echizen</surname></persName>
		</author>
		<author>
			<persName><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCB</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image-to-image translation: Methods and applications</title>
		<author>
			<persName><forename type="first">Yingxue</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Enhancing security and privacy in biometrics-based authentication systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nalini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Ratha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruud</forename><forename type="middle">M</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName><surname>Bolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM systems Journal</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="614" to="634" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A systematic review of finger vein recognition techniques</title>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Shaheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gongping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imran</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilong</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">213</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generating master faces for dictionary attacks with a networkassisted latent space evolution</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Shmelkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liar</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Friedlander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG 2021</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The 1st competition on counter measures to finger vein spoofing attacks</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramachandra</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Tirunagari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Poh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Bh Shekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Gragnaniello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICB</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="513" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wolf attack probability: A new security measure in biometric authentication systems</title>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Une</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akira</forename><surname>Otsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideki</forename><surname>Imai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICB</title>
		<imprint>
			<biblScope unit="page" from="396" to="406" />
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Biometric authentication in relation to payment systems and atms</title>
		<author>
			<persName><forename type="first">Gerik</forename><surname>Alexander Von Graevenitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Datenschutz und Datensicherheit-DuD</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="681" to="683" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sdumla-hmt: a multimodal biometric database</title>
		<author>
			<persName><forename type="first">Yilong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiwei</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Biometric Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="260" to="268" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
