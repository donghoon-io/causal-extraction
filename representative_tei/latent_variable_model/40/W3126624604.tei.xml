<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using deep LSD to build operators in GANs latent space with meaning in real space</title>
				<funder ref="#_MjdtNWS">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_fJp6yWK">
					<orgName type="full">National Institutes of 372 Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-06-29">June 29, 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">J</forename><surname>Quetzalco ´atl Toledo-Marı ´n</surname></persName>
							<idno type="ORCID">0000-0001-6212-1033</idno>
							<affiliation key="aff0">
								<orgName type="department">Luddy School of Informatics, Computing and Engineering</orgName>
								<orgName type="institution" key="instit1">Biocomplexity Institute</orgName>
								<orgName type="institution" key="instit2">Indiana University</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">United States of America</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">TRIUMF</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Glazier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Luddy School of Informatics, Computing and Engineering</orgName>
								<orgName type="institution" key="instit1">Biocomplexity Institute</orgName>
								<orgName type="institution" key="instit2">Indiana University</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Maritime University</orgName>
								<address>
									<country key="CN">CHINA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Using deep LSD to build operators in GANs latent space with meaning in real space</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-29">June 29, 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1371/journal.pone.0287736</idno>
					<note type="submission">Received: November 7, 2022 Accepted: June 12, 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative models rely on the idea that data can be represented in terms of latent variables which are uncorrelated by definition. Lack of correlation among the latent variable support is important because it suggests that the latent-space manifold is simpler to understand and manipulate than the real-space representation. Many types of generative model are used in deep learning, e.g., variational autoencoders (VAEs) and generative adversarial networks (GANs). Based on the idea that the latent space behaves like a vector space Radford et al. ( <ref type="formula">2015</ref>), we ask whether we can expand the latent space representation of our data elements in terms of an orthonormal basis set. Here we propose a method to build a set of linearly independent vectors in the latent space of a trained GAN, which we call quasi-eigenvectors. These quasi-eigenvectors have two key properties: i) They span the latent space, ii) A set of these quasi-eigenvectors map to each of the labeled features one-to-one. We show that in the case of the MNIST image data set, while the number of dimensions in latent space is large by design, 98% of the data in real space map to a sub-domain of latent space of dimensionality equal to the number of labels. We then show how the quasi-eigenvectors can be used for Latent Spectral Decomposition (LSD). We apply LSD to denoise MNIST images. Finally, using the quasi-eigenvectors, we construct rotation matrices in latent space which map to feature transformations in real space. Overall, from quasi-eigenvectors we gain insight regarding the latent space topology.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Generative models (GMs) are a class of Machine Learning (ML) model which excel in a wide variety of tasks <ref type="bibr" target="#b0">[1]</ref>. The optimization of a GM finds a function G that maps a set of M latent variables in latent space to a set of d variables in real space representing the data of interest (e. g., sets of images, music, videos, etc.), i.e. G : R M ! R d where d &gt;&gt; M &gt; 1. When building a GM, we first define the support of the latent variables, then obtain the function G by iteratively optimizing a loss function. Loss function choice depends on application, e.g., maximum loglikelihood is common in Bayesian statistics <ref type="bibr" target="#b1">[2]</ref>, Kullback-Leibler divergence is common for variational autoencoders (VAEs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, and the Jensen-Shannon entropy and the Wasserstein distance are common with generative adversarial networks (GANs) <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. The last two models are deep learning models. Deep learning is a field in artificial intelligence which has had great success in recent years and has pervaded many fields in science and health as well as our day to day lives. When we fit a latent variable model to a data set, we are finding a description of the data in terms of "independent components". Latent variables, |z i i, have a simple distribution, often a separable distribution (i.e., Pðfz i g <ref type="bibr" target="#b1">[2]</ref>. Often the latent representation of data lives in a simpler manifold than the original data while preserving relevant information. There are many examples of latent representation used to understand or describe more complicated features, ranging from statistical methods like Latent Class Analysis to examples in statistical physics and condensed matter such as order parameters for phase classification and even the long standing problem of the genotype-phenotype where the genome is taken as the latent representation of the phenotype. Other examples are, for instance, Ref. <ref type="bibr" target="#b9">[10]</ref> proposes a time-frequency representation of a signal that allows the reconstruction of the original signal, which relies in what they define as "consensus". Their proposed method generates sharp representations for complex signals.</p><formula xml:id="formula_0">M i¼1 Þ ¼ Q M i¼1 Pðz i Þ)</formula><p>Deep neural networks can function as surrogate propagators for time evolution of physical systems <ref type="bibr" target="#b0">[1]</ref>. While the latent variables are constructed to be independent identically distributed (i.i.d.) random variables, the training process entangle these latent variables. Latent variable disentanglement is an active area of research employing a wide variety of methods. For instance, in Ref. <ref type="bibr" target="#b10">[11]</ref>, the authors train a GAN including the generator's Hessian as a regularizer in the loss function, leading, in optimum conditions, to linearly independent latent variables, where each latent variable independently controls the strength of a single feature. Ref. <ref type="bibr" target="#b11">[12]</ref> constructs a set of quantized vectors in the latent space using a VAE, known as vector quantized variational autoencoder (VQ-VAE). Each quantized vector highlights a specific feature of the data set. This approach has been used in OpenAI's jukebox <ref type="bibr" target="#b12">[13]</ref>. A major drawback of these approaches is the lack of freedom in relating specific features in real space with specific latent space directions. This can be overcome by conditionalizing the generative model <ref type="bibr" target="#b13">[14]</ref>. However, conditionalization can reduce the latent space smoothness and interpolation capacity, since the condition is usually enforced by means of discrete vectors as opposed to a continuous random latent vector. Diffusion-based models <ref type="bibr" target="#b14">[15]</ref> have shown they can equate to GANs in performance and have become highly popular in recent times.</p><p>Here we propose a method to relate a specific chosen labeled feature with specific directions in latent space such that these directions are linearly independent. Having a set of linearly-independent latent vectors associated with specific labeled features allows us to define operators that act on latent space (e.g. a rotation matrix) and correspond to feature transformations in real space. For instance, suppose a given data set in real space corresponds to the states of a molecular dynamic simulation, i.e., the i-th data point in the data set can be the positions of the molecules at time t i , |x i i ! |x(t i )i, where |x(t i )i is a vector. Let us suppose that jxðt i Þi ¼ Gjz i i and jxðt i þ DtÞi ¼ Gjz j i, as depicted in Fig 1 . How can we construct an operator in latent space, O Dt , such that jz j i ¼ O Dt jz k i?. For this construction to be possible, we argue the operator G must be locally linear. Furthermore, in order to build the operator O, we need a basis that spans latent space. While linearity might seem counterintuitive given how NNs work, growing evidence suggests such linearity in practice. For instance, on the one hand there is an ongoing debate on how deep should a NN be to perform a specific task, on the other hand, it has been proposed the equivalence between deep NNs and shallow wide NNs <ref type="bibr" target="#b15">[16]</ref>. For at least one image-related GAN, simple vector arithmetic in latent space leads to feature transformations in real space (e.g., removal of sunglasses, change in hair color, gender, etc.) <ref type="bibr" target="#b16">[17]</ref>. However, a complete understanding on how specific features in real space map to latent space and how are these features arranged in latent space (latent space topology) or why some GANs' latent space behave like linear operators is lacking. It is believed that the latent representation of data with a given labeled feature forms a cluster. However, the tools employed to show this clustering effect quite often consist in a dimensional reduction e.g., t-SNE <ref type="bibr" target="#b17">[18]</ref> which collapses the latent representation into two or three dimensions. Other methods include principal component analysis, latent component analysis and important component analysis <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Our method does not collapse or reduce the latent space, allowing us to inspect latent space topology by spanning all latent space directions. We strongly believe the need of a set of basis vectors for understanding the topology of the latent space. Given the typical high-dimensionality of the latent space, we employ the Gram-Schmidt method to construct linearly independent vectors from a set of vectors that map to specific features. This approach enables us to visualize the feature entanglement in the latent space. We contend that our work contributes to a better understanding of latent space topology in two key ways: 1) through the method itself, which involves constructing a set of basis vectors in the latent space that map to specific features in the real space using Gram-Schmidt, and 2) by possessing the latent space basis vectors that map to specific features in the real space, which enables data manipulation in the latent space via linear algebra. As a proof of concept, we demonstrate the method by applying it to MNIST.</p><p>In the next section we introduce our mathematical method and notation and apply the method to the MNIST data set. In the Results section we show how we can use this method to understand the topology of the latent space by performing classification via principal component analysis; we apply this method to denoise images; and finally we show how to perform matrix operations in latent space which map to image transformations in real space. We discuss future steps and limitations in the last section. P is an operator in real space that evolves the state |x(t i )i to |x(t i + Δt)i. E is an Encoder and G is the Generator that maps latent variables to real space. O is an operator in latent space. The black arrow shows the time propagation done by applying the operator P to |x (t i )i which yields |x(t i + Δt)i. The blue arrows show the path where the data |x(t i )i gets encoded into latent space, |z i i, then the operator O is applied to the latent vector yielding a new latent vector |z j i. Finally, the new latent vector get decoded and yields |x(t i + Δt)i. <ref type="url" target="https://doi.org/10.1371/journal.pone.0287736.g001">https://doi.org/10.1371/journal.pone.0287736.g001</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods and materials</head><p>Assume a vector space which we call real space and denote the vectors in this space |xi with |xi 2 &lt; d . Assume a set fjx i ig N i¼1 , which we call the dataset with N the dataset size. Similarly, we assume a vector space, which we call the latent space and denote these vectors |zi with |zi 2 &lt; M (in general, M � d). We also consider three deep neural networks, a Generator G, an Encoder E and a Classifier C. We can interpret G as a projector from latent space to real space, i.e., jx i i ¼ Gjz i i, and interpret E as the inverse of G. However, this last statement has to be taken with a grain of salt, due to how variational autoencoders work. In fact, if jz a i ¼ Ejx i i and jz a 0 i ¼ Ejx i i, in general, |z a i 6 ¼ |z a 0 i, since these vectors are i.i.d. random vectors, sampled from a Gaussian distribution with mean and standard deviation dependent on |x i i (the correct mathematical notation to use would be jz a i � N ðjmi; jsiÞ, where N is a multivariate Gaussian distribution with mean and standard deviation |μi and |σi, respectively, which depend on Ejx i i) <ref type="bibr" target="#b3">[4]</ref>. Finally, the Classifier projects real-space vectors into the label space, i.e., jy k i ¼ Cjx i i, where |y k i 2 L, where L denotes the label space. We assume that each vector |y k i is a one-hot-vector. The length of |y k i equals the number of labels |L| = l and k = 1, . . ., l. Henceforth, we assume that l &lt; M.</p><p>We define fjx i ig M i¼1 to be a set of basis vectors in latent space such that the inner product between them yields hξ i |ξ j i = Cδ ij , where C is the norm and δ ij is the Kronecker delta function. Henceforth we call the set of basis vectors fjx i ig M i¼1 the quasi-eigenvectors since they form a basis and each one represents a feature state in latent space. Notice that we can define the operator A ¼ P M j¼1 jx j ihx j j (here |κihγ| denotes the outer product between vectors |κi and |γi), which implies Ajx i i ¼ Cjx i i. Any vector in latent space can be expressed as a linear superposition of these quasi-eigenvectors, viz, jzi ¼</p><formula xml:id="formula_1">X M j¼1 c j jx j i :<label>ð1Þ</label></formula><p>where |c i | = |hξ i |zi| is the amplitude of |zi with respect to |ξ i i and gives a measure of |zi's projection with the quasi-eigenvector |ξ i i. Constructing a set of basis vectors is straightforward. However, we wish each labeled feature to corresponds one-to-one with a quasi-eigenvector. Since we are assuming that l &lt; M, there will be a set of quasi-eigenvectors that do not correspond to any labeled feature.</p><p>To obtain a set of orthogonal quasi-eigenvectors, we use the Gram-Schmidt method. Specifically:</p><p>1. We train the GAN, which is composed by two NNs, namely, the Generator and the Discriminator, using the training set fjx i ig N i¼1 as in Ref. <ref type="bibr" target="#b5">[6]</ref>. 2. We train the Classifier independently, using the training set.</p><p>3. We train a VAE using the trained Generator as the decoder. We also use the Classifier to classify the output of the VAE. We include in the loss function a regularizer l � L class , where λ is a hyperparameter and L class denotes the Classifier's loss function. At this stage, we only train the Encoder, keeping the Generator and Classifier fixed. There are several options to choose from for the L class loss function. In our case, we used the Cross Entropy with a softmax activation function, i.e.,</p><formula xml:id="formula_2">L class ðjyi; jy GT iÞ ¼ À X L i¼1 y GT i log e y i P L j¼1 e y j ;<label>ð2Þ</label></formula><p>where y i and y GT i are the ith components of the vectors |yi and |y GT i, respectively, and |y GT i is the ground truth vector.</p><p>4. Define n to be an integer such that M = n × l. Then, for each label, we allocate n sets of latent vectors and we denote each of these latent vectors as jz k a;i i, where α denotes the label, i = 1, . . ., n and k = 1, . . ., V. Here V is the number of elements (latent vectors) in each set corresponding to the pair (i, α) 2 n × l. We build each of these sets fjz k a;i ig V k¼1 in two ways:</p><p>a. Using the training set, we encode each vector jx a i ! jz a i ¼ Ejx a i, then we decode the latent vector, i.e., jz a i ! jx a i ¼ Gjz a i, and then we classify the output, i.e., jx a i ! jy a i ¼ Cjx a i. For each label l, there is a set of latent vectors. The goal is to have a large number of the latent vectors representation of the data set arranged by label. Due to the large latent space dimensionality, we may require additional latent vectors besides those generated directly by encoding the training set. For this reason, we do the following.</p><p>b. We generate random latent vectors and map each of these latent vectors to their labels using the Generator and the Classifier as in 4(a), i.e., once we generated the random latent vector |z a 0i using a random multivariate Gaussian generator, we project it to real space jz a 0 i ! jx a 0 i ¼ Gjz a 0 i, and then we classify the output, i.e., jx a 0 i ! jy a 0 i ¼ Cjx a 0 i. Notice that with this approach we can generate as many latent vectors as desired.</p><p>We denote as V the number of latent vectors per set (i, α). 5. We take the average over V for each set of latent vectors fjz a;i ig V k¼1 and denote that average | ηi α,i , i.e.,</p><formula xml:id="formula_3">jZi a;i ¼ 1 V X V j¼1 jz j a;i i :<label>ð3Þ</label></formula><p>It is worth noticing that since the latent vectors are sampled from a multivariate Gaussian distribution, the average |η α,i i is finite and unbiased. By defining operators in latent space in terms of outer products of the |η α,i i vectors, these latent space operators will have encoded in them the set of latent vectors jz k a;i i. 6. To impose orthogonality, we use the Gram-Schmidt method. Thus, from the vectors |η α,i i we generate a set of quasi-eigenvectors |ξi α,i , i.e.,</p><formula xml:id="formula_4">jxi 1;1 ¼ jZi 1;1<label>ð4Þ</label></formula><formula xml:id="formula_5">jxi 2;1 ¼ jZi 2;1 À 2;1 hZjxi 1;1 1;1 hxjxi 1;1 jxi 1;1<label>ð5Þ</label></formula><formula xml:id="formula_6">. . .<label>ð6Þ</label></formula><formula xml:id="formula_7">jxi l;n ¼ jZi l;n À X lÀ 1 a¼1 X nÀ 1 i¼1 l;n hZjxi a;i a;i hxjxi a;i jxi a;i :<label>ð7Þ</label></formula><p>Such that:</p><formula xml:id="formula_8">a;i hxjxi b;j ¼ Cd ab d ij<label>ð8Þ</label></formula><p>In Eq <ref type="bibr" target="#b7">(8)</ref>, C is the value of the norm. The set of quasi-eigenvectors fjxi a;i g l;n a¼1;i¼1 span the latent space and, as we will show, a subset of them map to specific features.</p><p>The key point is that the set of quasi-eigenvectors form a basis set in latent space and each direction corresponds to a feature in real space. This structure allows us to give a better topological description of latent space, i.e., how does labeled features map to latent space similar to how molecular configurations map to the energy landscape <ref type="bibr" target="#b20">[21]</ref>. In addition, we can use the set of quasi-eigenvectors as tools for classification, denoising and topological transformations. We demonstrate these applications next using the MNIST dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applying method to MNIST</head><p>We trained a GAN, a Classifier and a VAE using the MNIST dataset which has 60k and 10k one-channel images in the training and test set, respectively, with dimensions 28 × 28 pixels. In Fig <ref type="figure" target="#fig_1">2a</ref> we show a sample of the dataset. The MNIST dataset can be found in many machine learning packages (e.g., PyTorch, Flux for Julia, etc.) as well as in <ref type="bibr" target="#b21">[22]</ref>. We fixed the batch size to 25 and number of epochs to 500 during all training runs. We trained the GAN using the training set, used the Jensen-Shannon entropy as the loss function <ref type="bibr" target="#b5">[6]</ref>, the ADAM optimizer with hyperparameters η = 0.0002, β 1 = 0.9, β 2 = 0.999 for both the Generator and the Discriminator, fixed the latent space dimensionality to M = 100 and sampled the random latent vectors from a multivariate Gaussian distribution centered at the origin with standard deviation equal to 1 in all M dimensions. Independently, we trained a Classifier using the training set, used crossentropy as loss function and a softmax as the activation function in the last layer, the ADAM optimizer with hyperparameters η = 3 � 10 -5 , β 1 = 0.5, β 2 = 0.99. The accuracy of the classifier on the test set reached �98.9%. Using the training set, we then trained the Encoder in a VAE and used the trained Generator as the Decoder. We used as loss function the Kullback-Leibler divergence and the hinge loss function. We also added as a regularizer the Classifier's loss function and the Lagrange multiplier, λ, as hyperparameter set to λ = 100. During the training of the Encoder, we kept both the Generator and the Classifier fixed. In Fig 2 <ref type="figure">we</ref> show the training results. To train the NNs we used Flux <ref type="bibr" target="#b22">[23]</ref> in Julia <ref type="bibr" target="#b23">[24]</ref> and the code can be found in Ref. <ref type="bibr" target="#b24">[25]</ref>.</p><p>The latent space dimension is M = 100, while the number of labels is |L| = 10. Thus, following step 4, for each label we generated n = M/|L| sets of latent vectors, each set containing V = 5000 latent vectors. In Fig <ref type="figure" target="#fig_3">3a</ref> we show a sample of latent vectors for labels 0, 1, 2, 6, 7 and 8, projected to real space using the Generator G. Then we take the average over each set as in step 5. We checked that the average and standard deviation over each of the entries in the set of vectors {|ηi α,i } α,i converges. Interestingly, when taking the average over the set of latent vectors corresponding to a label and projecting back to real space, the label holds. For instance, in Fig <ref type="figure" target="#fig_3">3b</ref>  directly from latent space are, by definition, sampled from a multivariate Gaussian distribution with mean and standard deviation equal to 0 and 1, respectively. On the contrary, encoding real space vectors yields Gaussian vectors overall (i.e., the PDF over all latent vectors over all labels yields a Gaussian distribution, by definition) but the mean and standard deviation can differ from 0 and 1, respectively <ref type="bibr" target="#b3">[4]</ref>.</p><p>Step 4(a) gives robustness to this method and step 4(b) allows us to generate as many latent vectors as wanted with a specific label. Since the latent space dimension is M = 100, we need M averaged latent vectors |ηi α,i to generate M orthogonal latent vectors. Since the number of labels is α = {0, . . ., |L| -1}, then n = 10. To this end, we generate one set (i.e., i = 1) following step 4(a) and nine sets (i.e., i = 2, 3, . . ., n) following step 4(b).</p><p>Fig <ref type="figure" target="#fig_4">4a</ref> shows the projection to real space of all the |ηi α,i vectors while Fig <ref type="figure" target="#fig_4">4c</ref> shows the inner product α,i hη|ηi α 0 ,i 0 as a heatmap, which shows they are non-orthogonal. At this point, we have M vectors |ηi α,i in latent space each i) composed of the sum of V latent vectors, and ii) maps to a specific feature in real space (the image of a number). However, these vectors are not orthogonal. Using the Gram-Schmidt method described in step 6, we obtain a set of vectors, |ξi α,i , in latent space such that each |ξi α,i vector i) encodes V latent vectors, ii) maps to a specific labeled feature (see Fig <ref type="figure" target="#fig_4">4b</ref>) and iii) the |ξi α,i vectors are orthogonal, as shown in Fig <ref type="figure" target="#fig_4">4d</ref>). Since the Generator was trained using random vectors sampled from a multivariate Gaussian distribution centered at zero with standard deviation 1, the value of the norm of any random latent vector will be hz|zi � M. Therefore, we fixed the norm of the quasi-eigenvectors to C = M (see Eq <ref type="bibr" target="#b7">(8)</ref>).</p><p>Notice that while the non-orthogonal vectors |ηi α,i for the MNIST GAN map to sharp images of easily-identifiable numbers in real space, not all quasi-eigenvectors map to images of numbers in real space. Only a few of the M linearly-independent directions in latent space (� 20) project to images of numbers in real space. We will show how to apply this property of the quasi-eigenvectors to the MNIST test set to classify images in latent space and to denoise realspace images. We also show how to build a rotation operator in latent space that generates feature transformations in real space.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Using LSD as a classifier in latent space</head><p>We can express any latent vector |zi, in terms of the quasi-eigenvectors, viz.</p><formula xml:id="formula_9">jzi ¼ X M k¼1 c k jx k i ;<label>ð9Þ</label></formula><p>where the coefficients c k are given by,</p><formula xml:id="formula_10">c k ¼ hx k jzi=C :<label>ð10Þ</label></formula><p>Similar to principal component analysis, we are interested in how much information about an image is encoded in the quasi-eigenvector with the largest amplitude |c i |. We encode images from the MNIST test set into latent space, then express the latent vectors in terms of the quasieigenvectors (we call this expression latent spectral decomposition or LSD) and find the maximum amplitude |c i | for each latent vector. Recall that the amplitude |c i | is a measure of the projection of the latent vector with respect to the quasi-eigenvector |ξ i i. Thus, the largest amplitude corresponds to the quasi-eigenvector that contributes the most to the latent vector. Since the quasi-eigenvectors are associated with labeled features in real space, we use the largest amplitude as a way to classify the image. Fig <ref type="figure" target="#fig_6">5a</ref> shows a sample batch of 25 images. The blue dots corresponds to the true labels (see y axis), while the green (red) dots correspond to the case where label associated with the quasi-eigenvector with the largest amplitude is the correct (incorrect) label. In this batch, only batch elements 9 and 22 have true labels that do not agree with the label of the quasi-eigenvalue of the image with the largest amplitude. Since each time the Encoder encodes an image it generates a new random latent vector, then we could obtain a different outcome for batch elements 9 and 22 as well as the rest of the batch elements for each trial. For this reason, we perform an ensemble average over 20 trials. For each trial we take the whole MNIST test set and compute the accuracy of the latent space decomposition (LSD) classifier (see red dots in Fig <ref type="figure" target="#fig_6">5b</ref>). We also computed the accuracy when the test set is encoded through the Encoder, then decoded through the Generator and finally classified (see blue dots in Fig <ref type="figure" target="#fig_6">5b</ref>). We have included the accuracy of the trained Classifier in Fig <ref type="figure" target="#fig_6">5b</ref> as an upper bound. While the trained Classifier has an accuracy of 98.8%, the LSD classifier has an average accuracy of *92%. This difference in accuracy, however, should not be interpreted as showing that the latent-space classifier does a poor job, but that the dominant few quasi-eigenvectors carry most of the information in latent space regarding the individual test-set images. In fact, the encoded 99% of the test-set data requires only the 10 linearly-independent directions in set 1, i.e., the largest amplitude correspond to quasi-eigenvectors in the first set.</p><p>Suppose that when we perform the LSD, we sort the amplitudes such that |c 1 | &gt; |c 2 | &gt; . . . &gt; |c M | and ask the position of the ground-truth label? As previously mentioned, in 92% of the cases the ground-truth label corresponds to the first position (i.e., |c 1 |). In 5% of the cases the ground truth label corresponds to the second largest amplitude (i.e., |c 2 |). In Fig <ref type="figure" target="#fig_6">5c</ref> we have plotted the cumulative of the probability for the ground-truth label being any of the first n positions. The dashed red line corresponds to the trained Classifier accuracy. Notice that the probability of the label being in position 1, 2, 3 or 4 of the LSD equals the accuracy of the trained classifier, i.e., in 98.9% of the MNIST test-set images the ground truth label is associated to a quasi-eigenvector such that the associated coefficient is either c 1 , c 2 , c 3 or c 4 . In this sense, it is possible that even when the amplitude of the quasi-eigenvector associated to the ground-truth label is not the largest one, rather the 2nd or 3rd largest one, then |c  The previous results give us a broad picture of latent space topology: the labeled features project to well-defined compact domains in latent space. Let us now consider how we can use this information to denoise images.</p><p>the the largest amplitude in LSD (� 92%). c) Cumulative probability of the ground truth label being any of the n first largest amplitudes (X axis). For n = 1 the probability is 92%. The probability of the ground truth label being one of the labels with the 4 largest amplitudes is � 98.9%, which is the classifiers accuracy. <ref type="url" target="https://doi.org/10.1371/journal.pone.0287736.g005">https://doi.org/10.1371/journal.pone.0287736.g005</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Denoising with LSD</head><p>The main issue when reducing noise in images is distinguishing noise from information. In this sense, a reliable denoiser has to learn what is noise and what isn't. One reason deep generative models are promising for denoising data is that in optimum conditions the GM has learned the exact data distribution. Of course, if the data set has noise, the GM will also learn the embedded noise in the data set. However, by sampling the latent space we may find regions where the signal to noise ratio is sufficiently large. For large M, this sampling is computationally expensive. To avoid this cost, we propose to LSD as a denoiser.</p><p>Recall that in the previous section we showed that with a 98% accuracy the information needed to assign a label to the image is stored in either the first-, second-, third-or fourth-largest amplitude of the LSD. Therefore, we propose that once the test set is encoded into latent space, we decompose the latent vector in terms of the quasi-eigenvectors and drop the contribution from quasi-eigenvectors with low amplitudes. In In each row, the first image corresponds to the ground-truth image, the second image is the image decoded from all 100 LSD components of the ground truth image. The third, fourth, fifth, sixth and seventh images are the images decoded after truncating the expansion after 1,2,3,4 and 10 LSD components of the ground truth image. In this method, denoising maintains the identity of the labeled feature in the image, e.g., each row shows different representations of the same number. In most cases in Fig <ref type="figure">7</ref>, the denoised image looks clearer and sharper. However, sometimes the LSD components project back to the wrong number. However we can consider as many LSD components as the dimension of the latent space, so even if taking the first n LSD components yields the wrong number, taking the first n + 1 LSD components could yield the correct number. In the previous section we showed that using only the first 4 LSD components gave us a 98.9% chance of obtaining the right number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Operations in latent space</head><p>Here we explore how to build operators in latent space that can yield feature transformations in real space. Having a set of orthogonal vectors that span latent space allows us to perform most operations in latent space as a series of rotations, since we can express the operator as a superposition of the outer product of the quasi-eigenvectors. If we construct a rotation matrix, R, in latent space, we can then recursively apply R to a set of encoded images. After each iteration we project the output to real space to see the effect of the latent-space rotation. We can define a projection operator B x i ;x j , such that,</p><formula xml:id="formula_11">B x i ;x j ¼ 1 hx i jx i i jx j ihx i j :<label>ð11Þ</label></formula><p>This operator projects from |ξ i i to |ξ j i, i.e., B x i ;x j jx k i ¼ d x k ;x k jx j i, where d x k ;x k denotes the Kroenecker delta function. Similarly, we define the operator R x i ;x j ðDy; yÞ as</p><formula xml:id="formula_12">R x i ;x j ðDy; yÞ / ðcosðy þ DyÞjx i i þ sinðy þ DyÞjx j iÞ �ðhx i jcosðyÞ þ hx j jsinðyÞÞ ;<label>ð12Þ</label></formula><p>which projects from cos(θ)|ξ i i + sin(θ)|ξ j i to cos(θ + Δθ)|ξ i i + sin(θ + Δθ)|ξ j i.</p><p>Starting from a set of images with label zero, we first encoded them to latent space, then we applied the rotation operator R recursively, as follows: First, we perform the rotation from the quasi-eigenvector associated with label zero to the quasi-eigenvector associated with label 1, viz., R x a¼0;i¼1 ;x a¼0;i¼1 ðDy; yÞ. Then, we performed a rotation from the quasi-eigenvector associated with label 1 to the quasi-eigenvector associated with label 2, viz., R x a¼1;i¼1 ;x a¼2;i¼1 ðDy; yÞ, and repeat mutatis mutandi until we reach the quasi-eigenvector associated with label α = 9. To keep the individual rotations in latent space small (and maintain the local linearity of the transforms), we fixed the rotation step size Δθ � π/6 so transforming from a direction associated with one quasi-eigenvector to a direction associated with a different quasi-eigenvector requires three sequential rotations. In Alg. 1 we show the pseudocode. To ensure the rotated latent vectors have constant norm value as in Eq <ref type="bibr" target="#b7">(8)</ref>, after each iteration we divide the latent vector |zi by ffi ffi ffi ffi ffi ffi hzjzi M q . After each iteration, we project the latent vector into real space. In Fig 8 <ref type="figure">we</ref> show this projection for a set of sample images. Notice how the numbers transform from 0 to 9. In principle, we could rotate through any other set of sequential features in this way. The key idea is that having a set of quasi-eigenvectors that span latent space each mapping to a  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We have shown that it is possible to build a set of orthogonal vectors (quasi-eigenvectors) in latent space that both span latent space and map to specific labeled features. These orthogonal vectors reveal the latent space topology. We found that for MNIST, almost all the images in the data set map to a small subset of the dimensions available in latent space. We have shown that we can use these quasi-eigenvectors to reduce noise in data. We have also shown that we can perform matrix operations in latent space that map to feature transformations in real space.</p><p>On the one hand, the deeper the NN the better its capacity in learning complex data and as depth increases, the non-linearity increases as well. On the other hand, it has been proposed the equivalence between deep NNs and shallow wide NNs <ref type="bibr" target="#b15">[16]</ref>. From catastrophe theory <ref type="bibr" target="#b25">[26]</ref>, we know that in non-linear dynamical systems small perturbations can be amplified leading to bifurcation points leading to completely different solution families of these non-linear dynamical systems. The results in Ref. <ref type="bibr" target="#b16">[17]</ref> suggest a different picture with what the authors call vector arithmetics in which adding or subtracting vectors in latent space can yield a feature addition, removal or modification (e.g., hair color, sunglasses, facial hair in the case of a headshot image data set). This behavior hints at the possibility of building a vector basis in latent space. It is not obvious why or how the label embeddings cluster in latent space or why they do so in a linearly independent manner. To put it in different terms, it would appear that the training of the GAN is reminiscent of a symmetry breaking mechanism from a rotationally invariant latent space to one where the label embeddings are linearly independently clustered. We consider that understanding why this pattern of clustering occurs is of great relevance and we intend to explore it in future work. Our intuition behind using the Gram-Schmidt method comes from the latent-space vector arithmetic <ref type="bibr" target="#b16">[17]</ref> and the flexibility of the method whereby one first chooses a set of vectors from which the vector basis is built.</p><p>Our work contributes to this discussion of the emergent effective linearity of NNs as transformations. While the NNs we used are intrinsically non-linear, they exhibit local linearity over a region of interest in latent space. This subspace maps to labeled features. In this sense, we say the non-linear NNs are effectively linear over the domain of interest. As a proof of concept, we have shown this for MNIST successfully, and our results serve as a proof of concept. Future work is aimed at testing this method in broader data sets, such as, CIFAR <ref type="bibr" target="#b26">[27]</ref> and Ima-geNet <ref type="bibr" target="#b27">[28]</ref>. Similarly, we plan to test this method for different latent space dimensionality and the effect it can have on feature entanglement.</p><p>We have considered labeled data which is a strong assumption in real problems since it is usually difficult to have that type of information. However, having a set of quasi-eigenvectors potentially allows us to recreate unlabelled data through latent superposition. We have not tested this here and we leave it for further work as well as testing this framework in other wellknown datasets. Fundamentally, we have shown that the data clustered in the GAN's latent space is linearly independent by building a set of quasi-eigenvectors pointing to each of these clusters. Further work is needed to understand the relationship between labels and linearlyindependence when the latent space dimensionality varies. The classifier and encoder were merely tools used to be able to span latent space and further work is aimed at simplifying this framework.</p><p>From an application standpoint, mapping to dominant quasi-eigenvectors could be useful for medical imaging, diagnosis and prognosis if, e.g., the labels denoted the severity of a disease; for predicting new materials if the labels denoted specific material features or external physical parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig 1 .</head><label>1</label><figDesc>Fig 1. Schematic of spaces and operators.P is an operator in real space that evolves the state |x(t i )i to |x(t i + Δt)i. E is an Encoder and G is the Generator that maps latent variables to real space. O is an operator in latent space. The black arrow shows the time propagation done by applying the operator P to |x (t i )i which yields |x(t i + Δt)i. The blue arrows show the path where the data |x(t i )i gets encoded into latent space, |z i i, then the operator O is applied to the latent vector yielding a new latent vector |z j i. Finally, the new latent vector get decoded and yields |x(t i + Δt)i.</figDesc><graphic coords="3,81.58,78.01,494.36,252.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig 2 .</head><label>2</label><figDesc>Fig 2. A batch from the a) dataset, b) the same dataset encoded and decoded using the Generator as Decoder and c) random latent vectors given as input to the Generator. https://doi.org/10.1371/journal.pone.0287736.g002</figDesc><graphic coords="6,84.02,439.65,491.98,226.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>we show the projected image of the average over V for each set of latent vectors fjz a;i ig V k¼1 in the case where the latent vectors were obtained following step 4(a), whereas Fig 3c corresponds to the case following step 4(b). We have also plotted the probability density function (PDF) per label in latent space for both cases and added a Gaussian distribution with mean and standard deviation equal to 0 and 1, respectively, for reference. Notice that the PDF in Fig 3b is shifted away from the Normal distribution, whereas in Fig 3c all PDFs are bounded by the Normal distribution, because latent vectors generated</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig 3 .</head><label>3</label><figDesc>Fig 3. a) Samples of latent vectors jz k a;i i, for labels α = 0, 1, 2, 6, 7 and 8. Gjz k a;i i yields images of numbers with label α. We show 1200 latent vectors, per label, projected into real space. Average over the latent vectors per label yields |ηi α,i . b) left panel Decoded latent vectors |ηi α,i . The vectors |ηi α,i were obtained as described in step 4(a). b) right panel The histogram for each label is Gaussian with non-zero mean. c) left panel Decoded latent vectors |ηi α,i . The vectors |ηi α,i were obtained as described in step 4(b). c) right panel The histogram for each label is Gaussian with zero mean. https://doi.org/10.1371/journal.pone.0287736.g003</figDesc><graphic coords="8,200.01,78.01,275.98,526.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig 4 .</head><label>4</label><figDesc>Fig 4. a) Projection to real space images of the latent vectors fjZi a;i g 9;10 a¼0;i¼1 obtained as described in step 5. b) Projection to real space images of the quasi-eigenvectors fjxi a;i g 9;10 a¼0;i¼1 obtained as described in step 6. The α index corresponds to the label (row) while the i index correspond to the set (column). c) The inner product of vectors fjZi a;i g 9;10 a¼0;i¼1 . d) The inner product of the quasi-eigenvectors fjxi a;i g 9;10 a¼0;i¼1 . https://doi.org/10.1371/journal.pone.0287736.g004</figDesc><graphic coords="9,180.00,78.01,396.00,492.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>1 | ≳ |c 2 | or |c 1 | ≳ |c 2 | ≳ |c 3 |. To test this idea, in Fig 6 we have plotted the normalized amplitude (i.e., |c i |/max{|c j |}) vs the rank (i.e., sorted</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig 5 .</head><label>5</label><figDesc>Fig 5. a) A batch of the MNIST test set classified by LSD using the largest amplitude. The largest amplitude |c i | corresponds to the quasi-eigenvector |ξ i i that contributes the most to the latent vector |zi, and a subset of the quasi-eigenvectors map to each label one-on-one. Y axis corresponds to the label, X axis to the image in the batch. Blue dots, ground truth. Green (red) dots correspond to the case(s) where the label associated with the quasi-eigenvector with the highest amplitude is the correct (incorrect) label. b) Accuracy for different trials using the MNIST test set. The green curve is the Classifier's accuracy (98.9%), the blue dots are the accuracy over the encoded-decoded MNIST test set (� 94%) and the red dots corresponds to the accuracy using</figDesc><graphic coords="11,95.53,78.01,480.47,576.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig 6 .</head><label>6</label><figDesc>Fig 6. LSD normalized amplitude ranking for the cases where the true label corresponds to the largest amplitude quasi-eigenvector a), second largest amplitude b) and third largest amplitude c). Probability-density function of the second-, third-and fourth-largest LSD normalized amplitude when the true label corresponds to the largest amplitude d), second-largest amplitude e) and third-largest amplitude f). https://doi.org/10.1371/journal.pone.0287736.g006</figDesc><graphic coords="12,84.02,382.11,491.98,268.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig 7 we show the results of this truncation for 125 random sample images. In Fig 7a we describe how to understand these images. Fig 7b shows 5 columns, where each column has 25 rows and each row has 7 images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig 7 . 1 :</head><label>71</label><figDesc>Fig 7. a) Image of the number 5 taken from the MNIST test set. The first image correspond to the ground truth (GT), the second image corresponds to the projected image of the 100 LSD components, the third, fourth, fifth, sixth and seventh images correspond to the projected images from the sum of the one, two, three, four and ten LSD components, respectively. b) 125 samples from the MNIST test set. Each sample is a row with seven images as shown in a). https://doi.org/10.1371/journal.pone.0287736.g007</figDesc><graphic coords="14,180.00,78.01,396.00,370.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig 8 .</head><label>8</label><figDesc>Fig 8. Ten latent vectors projected into real space after each iteration where the latent vectors are rotated an angle Δθ � π/6 from between two linearly independent directions associated with a quasi-eigenvector each. Rotations in latent space map to real space as label feature transformation, i.e., the images transform from the number 0 to number 1 and then from 1 to 2 until reaching number 9. https://doi.org/10.1371/journal.pone.0287736.g008</figDesc><graphic coords="15,84.02,78.01,491.98,161.75" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>PLOS ONE | https://doi.org/10.1371/journal.pone.0287736 June 29, 2023</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>JAG <rs type="funder">National Science Foundation</rs> grant <rs type="grantNumber">1720625</rs> and <rs type="funder">National Institutes of 372 Health</rs> grant <rs type="grantNumber">NIGMS R01 GM122424</rs>. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MjdtNWS">
					<idno type="grant-number">1720625</idno>
				</org>
				<org type="funding" xml:id="_fJp6yWK">
					<idno type="grant-number">NIGMS R01 GM122424</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The data set is publicly available at <ref type="url" target="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/ mnist/</ref>. In addition, the data set is part of many standard deep learning libraries, such as, PyTorch, Flux/Julia, Tensorflow, etc.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head><p>Conceptualization: J. Quetzalco ´atl Toledo-Marı ´n.</p><p>Data curation: J. Quetzalco ´atl Toledo-Marı ´n. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Machine learning for molecular simulation</title>
		<author>
			<persName><forename type="first">´f</forename><surname>Noe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><surname>Mu ¨ller Kr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clementi</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-physchem-042018-052331</idno>
		<idno type="PMID">32092281</idno>
		<ptr target="https://doi.org/10.1146/annurev-physchem-042018-052331" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="361" to="390" />
		</imprint>
	</monogr>
	<note>Annual review of physical chemistry</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Information theory, inference and learning algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mac</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>arXiv:13126114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Variational inference &amp; deep learning: A new synthesis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>arXiv:170100160</idno>
		<title level="m">NIPS 2016 tutorial: Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1145/3422622</idno>
		<ptr target="https://doi.org/10.1145/3422622" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="214" to="223" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial network: An overview of theory and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Battineni</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jjimei.2020.100004</idno>
		<ptr target="https://doi.org/10.1016/j.jjimei.2020.100004" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Information Management Data Insights</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">100004</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse time-frequency representations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Magnasco</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0601707103</idno>
		<idno type="PMID">16601097</idno>
		<ptr target="https://doi.org/10.1073/pnas.0601707103" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="6094" to="6099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The hessian penalty: A weak prior for unsupervised disentanglement</title>
		<author>
			<persName><forename type="first">W</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>arXiv:200810599</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14866" to="14876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Jukebox</surname></persName>
		</author>
		<idno>arXiv:200500341</idno>
		<title level="m">A generative model for music</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Perarnau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raducanu</surname></persName>
		</author>
		<idno>arXiv:161106355</idno>
		<title level="m">A ´lvarez JM. Invertible conditional gans for image editing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno>arXiv:201113456</idno>
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Statistical mechanics of deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kadmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-conmatphys-031119-050745</idno>
		<ptr target="https://doi.org/10.1146/annurev-conmatphys-031119-050745" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Condensed Matter Physics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>arXiv:151106434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Maaten</forename><surname>Lvd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">2008. Nov</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Principal components in regression analysis</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Principal component analysis</title>
		<imprint>
			<biblScope unit="page" from="129" to="155" />
			<date type="published" when="1986">1986</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Latent variable analysis. The Sage handbook of quantitative methodology for the social sciences</title>
		<author>
			<persName><forename type="first">B</forename><surname>Muthe ´n</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">345</biblScope>
			<biblScope unit="page" from="106" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Energy landscapes: Applications to clusters, biomolecules and glasses</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MNIST handwritten digit database</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yannlecuncom/exdb/mnist" />
	</analytic>
	<monogr>
		<title level="j">ATT</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Elegant machine learning with Julia</title>
		<author>
			<persName><forename type="first">M</forename><surname>Innes</surname></persName>
		</author>
		<author>
			<persName><surname>Flux</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.00602</idno>
		<ptr target="https://doi.org/10.21105/joss.00602" />
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page">602</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><surname>Julia</surname></persName>
		</author>
		<ptr target="https://julialang.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Latent Spectral Decomposition in GANs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Toledo-Marin</surname></persName>
		</author>
		<ptr target="https://github.com/jquetzalcoatl/LSD-GANs" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Il&apos;yashenko YS, Shil&apos;nikov L. Dynamical systems V: bifurcation theory and catastrophe theory</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Afrajmovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><surname>Imagenet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
