<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gaussian Process Latent Variable Model Factorization for Context-aware Recommender Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-12-19">19 Dec 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Yi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Da</forename><surname>Xu</surname></persName>
						</author>
						<title level="a" type="main">Gaussian Process Latent Variable Model Factorization for Context-aware Recommender Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-19">19 Dec 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1912.09593v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Context-aware recommender systems (CARS) have gained increasing attention due to their ability to utilize contextual information. Compared to traditional recommender systems, CARS are, in general, able to generate more accurate recommendations. Latent factors approach accounts for a large proportion of CARS. Recently, a non-linear Gaussian Process (GP) based factorization method was proven to outperform the state-of-the-art methods in CARS. Despite its effectiveness, GP model-based methods can suffer from over-fitting and may not be able to determine the impact of each context automatically. In order to address such shortcomings, we propose a Gaussian Process Latent Variable Model Factorization (GPLVMF) method, where we apply an appropriate prior to the original GP model. Our work is primarily inspired by the Gaussian Process Latent Variable Model (GPLVM), which was a non-linear dimensionality reduction method. As a result, we improve the performance on the real datasets significantly as well as capturing the importance of each context. In addition to the general advantages, our method provides two main contributions regarding recommender system settings: (1) addressing the influence of bias by setting a nonzero mean function, and (2) utilizing real-valued contexts by fixing the latent space with real values.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the advent of the era of big data, users are suffering from the information overload problem. Recommender systems are designed to help users find out items of interest, while context-aware recommender systems (CARS) refine recommendations by exploiting additional contextual information that can have an impact on users' behavior. For example, a user may have preferences on <ref type="bibr" target="#b0">(1)</ref> the time of weekday (or weekend) when he/she watches a movie, and (2) the location of office (or home) where he/she uses a mobile phone app. Thus, researchers have introduced CARS that extend the user-item modeling to a more complex user-item-context modeling.</p><p>Among existing paradigms for recommender systems, collaborative filtering approaches that predict the interests of a user by collecting preferences from other users are most popular due to their good accuracy and scalability <ref type="bibr" target="#b11">[12]</ref>. Because of this reason, it is also the focus of this paper. We concentrate on the class of the latent factor methods based on collaborative filtering that typically involves matrix factorization methods <ref type="bibr" target="#b12">[13]</ref> for traditional recommender systems and tensor factorization methods <ref type="bibr" target="#b10">[11]</ref> for CARS.</p><p>In the realm of latent factor methods for CARS, there are two popular classes of approaches. The first class is based on tensor factor-ization methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref>, which extend the classical two-dimensional matrix factorization to an n-dimensional tensor factorization. The second class is based on factorization machines <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4]</ref> which originated from a general predictor <ref type="bibr" target="#b25">[26]</ref> that can use second-order feature combinations efficiently. However, both classes adopt the linear combination of second-order or higher-order latent factors to represent user-item-context interactions. In this work, we seek a natural way to capture the inherent non-linear structure of real-world data by the Gaussian process (GP).</p><p>GP is a widely used stochastic process in machine learning <ref type="bibr" target="#b24">[25]</ref>. Standard GP models can only deal with supervised machine learning tasks while an approach called Gaussian process latent variable model (GPLVM) <ref type="bibr" target="#b14">[15]</ref> is designed to address un-supervised problems. Supervised GP learning of user preferences and un-supervised dimensionality reduction from ratings to latent factors constitute the foundation of GP-based collaborative factorization methods. Two works that investigate non-linear matrix factorization for conventional recommender systems <ref type="bibr" target="#b15">[16]</ref> and non-linear factorization for CARS <ref type="bibr" target="#b20">[21]</ref> are proposed. Both of them use the Gaussian process to realize non-linear factorization and have gained state-of-the-art performances compared to linear methods.</p><p>Despite the successful application of the Gaussian process latent variable model to context-aware recommender systems, the model itself cannot infer the impact of each context and is sensitive to overfitting because it does not marginalize out the latent variables <ref type="bibr" target="#b4">[5]</ref>. To address these two drawbacks of GP-based factorization for CARS, we introduce a prior to the latent variables. Inspired by Gaussian process latent variable model <ref type="bibr" target="#b29">[30]</ref>, where a variational inference framework for training is applied, we further implement a generalized variational inference that includes a non-zero mean function during the GP to solve the model. Besides, our model is flexible enough to integrate both categorical and real-valued contexts by fixing the latent variable with corresponding real values for real-valued contexts while adding a prior to the latent variable for categorical contexts.</p><p>As a result, we have developed a powerful non-linear collaborative filtering method, which we name, Gaussian Process Latent Variable Model Factorization (GPLVMF), to improve the performance of GPbased factorization methods for CARS further . To summarize, the main contributions of this paper are:</p><p>‚Ä¢ We propose a novel algorithm named GPLVMF to achieve a Bayesian non-linear factorization for CARS. Different from GPbased methods, GPLVMF aims to address the over-fitting problem via implementing a prior distribution to the latent variables. ‚Ä¢ We model the non-zero mean function during Gaussian processes to capture the bias and provide a generalized variational inference solution. Also, our method can flexibly deal with both real-valued and categorical contexts.</p><p>‚Ä¢ Both scaled conjugate gradient (SCG) and stochastic gradient descent (SGD) optimization methods are applied to solve the model. The experiment results show that GPLVMF not only improves the accuracy of the real datasets but also can automatically infer the influence of each context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are three main approaches to integrate context information into recommender systems <ref type="bibr" target="#b1">[2]</ref>, namely: (1) contextual pre-filter where contextual information is used as a filter before applying context <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>, (2) contextual post-filtering where contextual information is initially ignored and is then filtered after using traditional recommender algorithms <ref type="bibr" target="#b23">[24]</ref>, and (3) contextual modeling where contextual information is integrated into the process of modeling directly, which is what this paper is focusing on, since it does not require supervision and fine-tuning in all steps <ref type="bibr" target="#b26">[27]</ref>.</p><p>In contexture modeling, we introduce three classes of approaches. The first class is based on the matrix factorization <ref type="bibr" target="#b2">[3]</ref>. Later, <ref type="bibr" target="#b17">[18]</ref> used biased matrix factorization as the base model while <ref type="bibr" target="#b31">[32]</ref> adopted matrix completion modeling to address CARS. The second class is based on tensor factorization <ref type="bibr" target="#b10">[11]</ref>. Then <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29]</ref> proposed tensor factorization-based methods for implicit feedback data. The final one is based on factorization machines <ref type="bibr" target="#b25">[26]</ref>. A factorization machinesbased method to solve the cross-domain problem was proposed by <ref type="bibr" target="#b18">[19]</ref> while a higher order factorization machine was proposed by <ref type="bibr" target="#b3">[4]</ref>. However, these methods treat user-item-context interactions as linear combinations of latent factors and are insufficient to capture the complex non-linear inter-relationships between the three entities in CARS.</p><p>GP has been applied to the recommendation problems. A nonlinear method for matrix factorization based on GP <ref type="bibr" target="#b15">[16]</ref> for traditional recommender systems was shown to outperform standard matrix factorization methods. Collaborative Gaussian processes for preference learning <ref type="bibr" target="#b8">[9]</ref> was proposed to learn pairwise preferences expressed by multiple users. Besides, the GP was used to address the challenge of ranking recommendation on click feedback recommender system <ref type="bibr" target="#b30">[31]</ref>. Recently, a GP-based factorization machine for context-aware recommender systems was proposed <ref type="bibr" target="#b20">[21]</ref>, and it can outperform factorization machines and tensor factorization methods. However, none of these methods have studied the case of applying Gaussian process latent variable model to the CARS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Gaussian Process Latent Variable Model Factorization</head><p>In this section, we elaborate on our proposed Bayesian Gaussian process factorization method. First, we explain how to use latent factors to represent the user-item-context interactions for CARS. Then we describe the Gaussian process latent variable model factorization via introducing a prior to latent factors. Finally, we derive a variational inference for solving the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Latent Representation</head><p>A conventional recommender system is an information filtering system that seeks to predict the rating a user would give to an item. We denote users and items by U = {un} N n=1 and V = {v l } L l=1 respectively, where N = |U | and L = |V |. When it comes to the contextaware setting, multiple contexts can be denoted by C1, . . . , CD to describe their contextual information (location, time etc.), where</p><formula xml:id="formula_0">C d = {c d l } L d l=1 , and L d = |C d |, for each d ‚àà [1, D].</formula><p>Latent factors approach defines transformation from observation to its latent representation. Let each element of the entities, including user, item, and each of the multiple contexts be represented by a real-valued vector in the latent space U = {un} N n=1 , V = {v l } L l=1 , and</p><formula xml:id="formula_1">V d = {v d l } L d l=1 for each d ‚àà [1, D]</formula><p>. Suppose the dimension of latent spaces for users, items, and contexts are Qu, Qv, and {Q d } D d=1 respectively. Then, we have</p><formula xml:id="formula_2">U ‚àà R N √óQu , V ‚àà R L√óQv and V d ‚àà R L d √óQ d .</formula><p>Here, we use bold capital letters to denote matrices, bold letters for vectors, and regular letters for scalars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gaussian Process</head><p>A Gaussian process prior defines a distribution over a real-valued function f (x). Formally, the set of function values f = {f (xt)} T t=1 on a collection of any finite inputs X = {xt} T t=1 should satisfy the multivariate Gaussian distribution. A GP is completely specified by its mean function m(x) and covariance function k(x, x ), which denotes as f (x) ‚àº GP(m(x), k(x, x )), and distribution can be written as:</p><formula xml:id="formula_3">p(f|X) = N (f|m, K),<label>(1)</label></formula><p>where m = {m(xt)} T t=1 is a vector and K is a covariance matrix where each element corresponds to the value of covariance function evaluated between all pairs of xt, x t . For common usage of GP, the mean function is usually assumed to be zero, m(x) = 0, which is not the case in our work. Different covariance functions will be applied to different applications while a popular example is the RBF kernel,</p><formula xml:id="formula_4">k(x, x ) = œÉ 2 exp(- 1 2 2 (x -x ) T (x -x )),<label>(2)</label></formula><p>where œÉ is known as the signal variance and the length-scale. A Gaussian likelihood function between observations y = {yt} T t=1 and prior f accounts for the noise,</p><formula xml:id="formula_5">p(y|f) = N (y|f, Œ≤ -1 I),<label>(3)</label></formula><p>where Œ≤ is the noise precision and I is an identity matrix with size T . Integrating out vector functions f, we can obtain the marginal likelihood function,</p><formula xml:id="formula_6">p(y|X) = p(y|f)p(f|X)df = N (y|m, K + Œ≤ -1 I).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mean and Kernel for CARS</head><p>For standard GP regression and classification tasks, the mean function for bias is usually assumed to be zero, since data bias can be eliminated by pre-procession. However, in recommendation system bias is a well-known phenomenon. For example, some users may have their preferences on certain items and consistently give high ratings to them. To mitigate this problem, we allocate additional variables to users, items, and contexts. Let B = {b l } L l=1 , and For each rating associated with user n to item l under contexts d l , we define a mean function:</p><formula xml:id="formula_7">B d = {b d l } L d l=1 for each d ‚àà [1, D]</formula><formula xml:id="formula_8">mn(x b ) = bn + Q b v q=1 b l,q + D d=1 Q b d q=1 b d l ,q ,<label>(5)</label></formula><formula xml:id="formula_9">ùë¢ 1 ùë£ 1 ùëê 1 1 ùëê 2 1 ùë¢ 1 ùë£ 2 ùëê 1 2 ùëê 2 1 ùë¢ 1 ùë£ 3 ùëê 1 1 ùëê 2 2 ùë¢ 2 ùë£ 1 ùëê 1 2 ùëê 2 1 ùë¶ 1 ùë¶ 2 ùëê 2 1 ùëê 1 3 ùë£ 3 ùë¢ 2 ùë¶ 3 ùë¶ 4 ùë¶ 5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collection of latent vectors for the kernel</head><formula xml:id="formula_10">Factorization ùêó ùüè ùêó ùüè ùêõ ùêØ 1 ùêØ 2 ùêØ 3 ùêõ 1 ùêõ 2 ùêõ 3 ùêØ 1 1 ùêØ 1 2 ùêØ 1 3 ùêõ 1 1 ùêõ 1 2 ùêõ 1 3 ùêØ 2 1 ùêØ 2 2 ùêõ 2 1 ùêõ 2 2 ùêØ 1 ùêØ 2 ùêØ 3 ùêØ 1 1 ùêØ 1 2 ùêØ 1 1 ùêØ 2 1 ùêØ 2 1 ùêØ 2 2 ùêØ 1 ùêØ 3 ùêØ 1 2 ùêØ 1 3 ùêØ 2 1 ùêØ 2 1 ùêõ 1 ùêõ 3 ùêõ 1 2 ùêõ 1 3 ùêõ 2 1 ùêõ 2 1 ùêõ 1 ùêõ 2 ùêõ 3 ùêõ 1 1 ùêõ 1 2 ùêõ 1 1 ùêõ 2 1 ùêõ 2 2 ùêõ 2 1</formula><p>Latent space for the mean . Overview of our proposed GPLVMF model. First, the recommendation dataset consisted of two users is shown on the left side. Grey is the user, green is the item, yellow is the first context, blue is the second context, and orange is the rating. The latent space for the mean and kernel are in the middle part, the width of each rectangle represents the dimensionality of each latent vector. We use the identical color and subscript as the items, first contexts, and second contexts in the dataset to indicate the corresponding latent vectors. Finally, GPs from the collection of latent vectors to ratings are illustrated on the right side. The latent space is factorized into two groups for both the mean and kernel according to two users.</p><p>where bn is a parameter regarding the bias of user n and x b represents a latent vector that consists of b</p><formula xml:id="formula_11">l , b d l , for each d ‚àà [1, D].</formula><p>Besides, we focus on the automatic relevance determination (ARD) squared exponential kernel between latent vectors associated with two ratings for user n:</p><formula xml:id="formula_12">kn(x, x ) = œÉ 2 n exp - 1 2 Qv q=1 Œ±q(v l,q -v l ,q ) 2 - 1 2 D d=1 Q d q=1 Œ±q(v d l ,q -v d l ,q ) 2 , (<label>6</label></formula><formula xml:id="formula_13">)</formula><p>where Œ±q is inverse length-scale and x represents a latent vector that consists of vm, v dz for each d ‚àà <ref type="bibr">[1, D]</ref>. ARD squared exponential kernel can help the system to automatically select the dimensionality of its latent features <ref type="bibr" target="#b29">[30]</ref>. Therefore, these weights Œ±q can give us the insights into the impact of each context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Probabilistic Factorization</head><p>After introducing Gaussian process, we seek a natural probabilistic interpretation of factorization based on GPLVM. We show the overview of GPLVMF in Figure <ref type="figure">1</ref>. Let Xn ‚àà R Nn√óQ and X b n ‚àà R Nn√óQ b denote the collection of latent variables regarding rated entities by user n for the kernel and mean respectively, where Nn is the number of ratings by user n, Q = Qv + D d=1 Q d , and</p><formula xml:id="formula_14">Q b = Q b v + D d=1 Q b d .</formula><p>We use y n ‚àà R Nn to denote the corresponding ratings by user n. Our user-centric factorization method for the context-aware recommender systems takes the probabilistic form:</p><formula xml:id="formula_15">P (Y|X) = N n=1 p(y n |Xn, X b n ), (<label>7</label></formula><formula xml:id="formula_16">)</formula><p>where X is the collection of all the latent vectors, and Y = {y n } N n=1 . GPs are taken to be independent across of different users and the likelihood function for user n is written as:</p><formula xml:id="formula_17">p(y n |Xn, X b n , Œ±, bn, Œ∏n) = N (y n |mn, Kn + Œ≤ -1 n IN n ),<label>(8)</label></formula><p>where Œ± = {Œ±q} Q q=1 and Œ∏n = (œÉn, Œ≤n). We'd like to mention that the latent space of users are marginalized out in the likelihood function, while we could integrate latent space of items instead to obtain another likelihood function as well <ref type="bibr" target="#b15">[16]</ref>.</p><p>The optimization method adopted by GP factorization for both the conventional <ref type="bibr" target="#b15">[16]</ref> and context-aware <ref type="bibr" target="#b20">[21]</ref> recommender system is to find the maximum likelihood estimation of latent variables X whilst jointly maximizing with respect to the hyper-parameters (Œ±, Œ∏n). However, this method is sensitive to over-fitting and cannot determine the dimensionality of latent space automatically for GPLVM <ref type="bibr" target="#b29">[30]</ref>.</p><p>To adopt a fully Bayesian treatment for the latent space, we assign to it, a prior density over X. In this work, we use the standard normal density for the latent variables while we use a Œ¥ function prior to realize fixing the latent variables regarding real-valued contexts. The normal distribution for the X can be written as:</p><formula xml:id="formula_18">p(X) = L l=1 N (v l |0, IQ v ) D d=1 L d l=1 N (v d l |0, IQ d ).<label>(9)</label></formula><p>We note the distribution of the latent variables for the mean function has the same form as the kernel function and is dropped from the expression for simplification. The joint probability for the model is:</p><formula xml:id="formula_19">p(Y, X) = N n=1 p(y n |Xn, X b n )p(X).<label>(10)</label></formula><p>However, this is not analytically tractable. While a sampling method was proposed by <ref type="bibr" target="#b27">[28]</ref> for Bayesian probabilistic matrix factorization problem, a variational inference approach to marginalize the latent variables was proposed by <ref type="bibr" target="#b29">[30]</ref>. Inspired by the latter work, we derive a generalized variational inference for Gaussian process latent variable model factorization to include the non-zero mean Equation (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Variational Inference</head><p>We aim to compute the marginal data likelihood:</p><formula xml:id="formula_20">p(Y) = p(Y|X)p(X)dX.<label>(11)</label></formula><p>However, the integral of Equation ( <ref type="formula" target="#formula_20">11</ref>) is intractable due to the nonlinearity inside the inverse of the covariance matrix. Following the VI framework, we introduce a variational distribution q(X) to approximate the true posterior p(X|Y),</p><formula xml:id="formula_21">q(X) = L l=1 N (v l |¬µ l , S l ) D d=1 L d l=1 N (v d l |¬µ d l , S d l ),<label>(12)</label></formula><p>where the variational parameters for items and contexts are ¬µ = ({¬µ l }, {¬µ d l }) and S = ({S l }, {S d l }). Again, we drop the variables regarding the mean from expressions for simplification. We apply Jensen's inequality to find a lower bound F (q) log p(Y), and the lower bound F (q) takes the form: F (q) = q(X) log p(Y|X)p(X) q(X) dX = q(X) log p(Y|X)dX -q(X) log q(X) p(X) dX</p><formula xml:id="formula_22">= F (q) -KL(q||p),<label>(13)</label></formula><p>where KL denotes the Kullback -Leibler divergence, which can be computed analytically thanks to the fact that distributions of q(X) and p(X) are both Gaussians. The first term can further be broken down to separate form for each user,</p><formula xml:id="formula_23">F (q) = N n=1 Fn(q) = N n=1 q(X) log p(y n |Xn)dX = N n=1 q(Xn) log p(y n |Xn)dXn.<label>(14)</label></formula><p>Note that for CARS, Xn ‚äÜ X, so that the complementary latent variables can be integrated out.</p><p>To further solve the Equation ( <ref type="formula" target="#formula_23">14</ref>), we need to introduce a variational spare Gaussian process framework <ref type="bibr" target="#b29">[30]</ref> to modify Gaussian process prior. For each vector of latent function values fn, we introduce a separate set of M inducing variables un ‚àà R M evaluated at a set of inducing input locations given by Z ‚àà R M √óQ . The likelihood associated with GP latent function p(y n , fn|Xn) = p(y n |fn)p(fn|Xn), is augmented by inducing variables: p(y n , fn, un|Xn, Z) = p(y n |fn)p(fn|un, Xn, Z)p(un|Z). <ref type="bibr" target="#b14">(15)</ref> Both fn and un are from same distribution, thus p(fn|un, Xn, Z) = N (fn|Œ±n, Œ£n), where</p><formula xml:id="formula_24">Œ±n = mn + KN nM K -1 M M un, Œ£n = KN n Nn -KN n M K -1 M M KMN n .<label>(16)</label></formula><p>Note we include the bias term mn in Œ±n, which is the key of generalized form for VI. The next step is to adopt a variational distribution:</p><p>q(fn, un) = p(fn|un, Xn)q(un),</p><p>where q(un) is a variational distribution over the inducing variables un. Here we simplify our notation by dropping Z from our expressions. By Jensen's inequality, we obtain a lower bound for the loglikelihood:</p><formula xml:id="formula_26">log p(y n |Xn) q(un) log p(un)N (y n |Œ±n, Œ≤ -1 n IN n ) q(un) dun - Œ≤ 2 Tr(KN n Nn -KN nM K -1 M M KMN n ).<label>(18</label></formula><p>) Substituting Equation ( <ref type="formula" target="#formula_26">18</ref>) back to Equation ( <ref type="formula" target="#formula_23">14</ref>) and swapping the integration order, we have:</p><formula xml:id="formula_27">Fn(q) q(un) log N (y n |Œ±n, Œ≤ -1 n IN n ) + log p(un) q(un) dun - Œ≤n 2 Tr( KN n Nn ) + Œ≤n 2 Tr(K -1 M M KMN n KN n M ),<label>(19)</label></formula><p>where ‚Ä¢ denotes expectation under the distribution q(X). The last step is to adopt q(un) as a variational distribution to maximize the above lower bound <ref type="bibr" target="#b29">[30]</ref>. The optimal setting of distribution is q(un) ‚àù e log N (y n |Œ±n,Œ≤ -1</p><formula xml:id="formula_28">n I Nn ) p(un),<label>(20)</label></formula><p>which is a Gaussian distribution q(un) = N (un|¬µu n , Œ£u n ). By calculating, we obtain its mean and covariance:</p><formula xml:id="formula_29">¬µu n = KMM (Œ≤ -1 n KMM + Œ®2 n ) -1 Œ® T 1n (y n -œÜ1 n ), Œ£u n = Œ≤ -1 n KMM (Œ≤ -1 n KMM + Œ®2 n ) -1 KMM ,<label>(21)</label></formula><p>where Œ®1 n = KN n M q(Xn) , Œ®2 n = KMN n KN n M q(Xn) and œÜ1 n = mn q(X b n ) . In the following computation, we would use another two statistics: œà0 n = Tr( KNN q(Xn) ) and œÜ0 n = m T n mn q(X b n ) . Our contribution to the generalized variational inference form is introducing the statistics œÜ = (œÜ0, œÜ1) with regard to the mean, while the original statistics Œ® = (œà0, Œ®1, Œ®2) are accounting for the kernel. Since all terms are tractable now, we finally get the closedform of the lower bound for Fn(q), Fn(q) log Œ≤</p><formula xml:id="formula_30">Nn 2 n |KMM | 1 2 (2œÄ) Nn 2 |Œ≤nŒ®2 n + KMM | 1 2 e -1 2 (W 1 -W 2 ) - Œ≤nœà0 n 2 + Œ≤n 2 Tr(K -1 M M Œ®2 n ),<label>(22)</label></formula><p>where</p><formula xml:id="formula_31">W1 = Œ≤n(y T n y n -2y T n œÜ1 n + œÜ0 n ) and W2 = Œ≤ 2 n ·ªπT n Œ®1 n (KMM + Œ≤nŒ®2 n ) -1 Œ® T</formula><p>1n ·ªπn , here ·ªπn = y n -œÜ1 n . The bound can be jointly maximized over the variational parameters (¬µ, ¬µ b , S, S b , Z) and model parameters (Œ±, {bn, Œ∏n} N n=1 ) by applying gradient-based optimization techniques.</p><p>The bound can be jointly maximized over the variational parameters (¬µ, ¬µ b , S, S b , Z) and model parameters (Œ±, {bn, Œ∏n} N n=1 ) by applying gradient-based optimization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Optimization and complexity analysis</head><p>In this work, we apply stochastic gradient descent (SGD) to solve the optimization problem. We note that the gradient of the KL divergence of each step is averaged over the number of users since there is no independent form for the users. In addition, we adopt the scaled conjugate gradient (SCG) optimization <ref type="bibr" target="#b19">[20]</ref> to compare with SGD. To counter one epoch of SGD, we use an iteration that processes the whole data once for SCG.</p><p>Bayesian Gaussian process latent variable model cannot handle big data problem, since the computational complexity of O(N M 2 ) and storage demands of O(N M ) for general application, where N is the size of data and M is the number of inducing point <ref type="bibr" target="#b6">[7]</ref>. However, we would not encounter this challenge for GPLVMF thanks to the structure of the dataset in the process of factorization, as shown in Figure <ref type="figure">1</ref>. The maximum kernel size is decided by the user who has the most ratings, which is usually no more than millions or billions in the real-world dataset. Finally, the computational complexity and storage demands in the GPLVMF are O( N n=1 NnM 2 ) and O( N n=1 NnM ) at an iteration respectively. SGD would be a better choice when the number of users becomes millions or billions. A complete comparison between SCG and SGD will be presented in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Prediction</head><p>Once all the variational parameters and model parameters have been learned, they can be used predict preferences for users. We denote the collection of latent variables for user n for prediction as X * n . Then we have the predictive function f * n ,</p><formula xml:id="formula_32">q(f * n ) = p(f * n |un, X * n )q(un)dun q(X * n )dX * n ,<label>(23)</label></formula><p>which is a Gaussian distribution N (f * n |¬µ * fn , Œ£ * fn ) with its mean and covariance:</p><formula xml:id="formula_33">¬µ * fn = Œ® * 1n (Œ≤ -1 n KMM + Œ®2 n ) -1 Œ® T 1n ·ªπn + œÜ * 1n , Œ£ * fn = Œ® * 1n (K -1 M M + (KMM + Œ≤nŒ®2 n ) -1 )(Œ® * 1n ) T ,<label>(24)</label></formula><p>where</p><formula xml:id="formula_34">Œ® * 1n = KN * n M q(X * n ) and œÜ * 1n = m * n q(X b * n ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we empirically investigate the performance of GPLVMF. First we describe the datasets and settings in our experiments, then report and analyze the experiment results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>In this work, we use 4 real datasets. The statistics of all datasets are presented in Table <ref type="table" target="#tab_1">1</ref>.</p><p>‚Ä¢ Comoda <ref type="bibr" target="#b13">[14]</ref> contains 2296 ratings of 1232 movies by 121 users. We use 12 provided contexts: time of the day, day type, season, location, weather, social, ending emotions and dominant emotions, mood, physical conditions, decision, and interaction. ‚Ä¢ Food <ref type="bibr" target="#b21">[22]</ref> contains 5554 ratings by 212 users on 20 food menus.</p><p>We use 2 contexts: three different levels of hunger and real or supposed situation. We have eliminated some conflicted ratings followed by <ref type="bibr" target="#b20">[21]</ref>.</p><p>‚Ä¢ Sushi <ref type="bibr" target="#b9">[10]</ref> contains 50,000 ratings of 100 types of Sushi by 5000 Japanese users. We use 7 contexts: style, major group, minor group, heaviness/oiliness in the state, popularity, price, and availability in shops. All the contextual information are attributions of the item, and the last four contexts are real-valued contexts. ‚Ä¢ Movielens-1M <ref type="bibr" target="#b5">[6]</ref> contains 1,000,209 ratings of 3706 movies by 6040 users. In this work, we adopt the hour and day as 2 contexts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>We compare our results with state-of-the-art methods, namely,</p><p>‚Ä¢ Const, a naive predictor that predicts for every user the mean of his/her ratings. ‚Ä¢ Multiverse <ref type="bibr" target="#b10">[11]</ref>, a state-of-the-art tensor factorization method.</p><p>‚Ä¢ FM <ref type="bibr" target="#b26">[27]</ref>, one of the most popular factorization method, which is famous for its fast training speed. We use LibFM<ref type="foot" target="#foot_2">foot_2</ref> to implement the method.</p><p>‚Ä¢ GPFM <ref type="bibr" target="#b20">[21]</ref>, a Gaussian process based factorization method and has been shown to outperform other context-aware recommendation models on the Comoda dataset, the Food dataset, and the Sushi dataset. ‚Ä¢ GPLVM-MF, a Gaussian process latent variable model based matrix factorization method. We apply our model to a setting where no contextual information is available. Thus GPLVM-MF can be seen as a context-agnostic variant of GPLVMF.</p><p>For each dataset, we split it 5 folds and repeat the experiments 5 times using 1 fold as the test set and the remaining 4 folds as the training set. We tune the parameters using 1 of the 5 folds as the validation set and fix the tunes parameters for other 4 folds. To evaluate the prediction of ratings, we use two evaluations metrics: mean absolute error (MAE), root-mean-square error (RMSE). For all datasets, the performance is averaged over the 5 different folds. The results are statistically significant and the variances are small so not reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance comparison</head><p>The performance comparison of all methods are shown in Table <ref type="table" target="#tab_2">2</ref> in terms of MAE and RMSE. This table shows our approach achieves the best results on all datasets, which demonstrates the effectiveness of using Gaussian process latent variable model factorization to model contextual-aware recommendation.</p><p>First, we show the effect of having contextual information by comparing several context-aware methods with a context-agnostic method, in here, the context-agnostic method is GPLVM-MF. We originally intended to list other context-agnostic methods, such as standard matrix factorization adopted by <ref type="bibr" target="#b20">[21]</ref>. However, our Bayesian non-linear matrix factorization method (GPLVM-MF) outperform the "Const" method on all datasets, while the same superiority is not guaranteed by the standard matrix factorization method reported in <ref type="bibr" target="#b20">[21]</ref>. Thus we adopt GPLVM-MF as an appropriate context-agnostic method.</p><p>It is evident that all the context-aware methods except (1) "Multiverse" method and (2) the Movielens 1M dataset, achieve better performance than GPLVM-FM. Since "Multiverse" suffers from the high dimensionality of contexts on the Comoda and Sushi dataset, we use 4 out of 12 contexts for the Comoda and 3 out 7 contexts for the Sushi dataset. While there is no explicit contextual information for the Movielens-1M, the choice of model can be the most determining factor in terms of achieving a robust result. It can be even more so than incorporating contextual information. It's also noteworthy to state that by using contexts, our GPLVMF further improves performance.</p><p>Then, we compare GPLVMF with other state-of-the-art contextaware methods. The performance in Table <ref type="table" target="#tab_2">2</ref> shows that GPLVMF significantly outperforms other context-aware methods. Comparing with the best performance of other models, GPLVMF improves the MAE values by 4.0%, 13.4%, 1.2%, and 4.7% on the Comoda, Food, Sushi, and Movielens-1M datasets, while improves the RMSE values by 3.7%, 12.8%, and 4.5% on the Comoda, Food, and Movielens-1M datasets respectively.</p><p>Finally, we'd like to elaborate on the Sushi dataset. There are four real-valued contexts which cannot be modeled directly into GPFM model while they quantize real values into several categories <ref type="bibr" target="#b20">[21]</ref>. In the process of quantizing, valuable information may be filtered out. However, we use these raw contextual values by fixing them as the corresponding latent variables to utilize the given data as much as possible. To confirm that the performance differences between GPLVMF and GPFM are due to directly utilizing realvalued contexts, we conduct an experiment using categorical contexts by GPLVMF, and the results are nearly the same as the GPFM, which concludes the efficiency of integrating real-valued contexts in GPLVMF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Optimization comparison</head><p>We demonstrate the comparison of stochastic gradient descent (SGD) and scaled conjugate gradient (SCG) methods in two perspectives. First, we measure the running time for one epoch (SGD) and one iteration (SCG) against the amount of data used for training from the Sushi dataset and the dimensionality of the latent space for both SCG and SGD, where all hyperparameters are identical. We show the running time per epoch (SGD) and per iteration (SCG) in Figure <ref type="figure" target="#fig_1">2</ref>, from which we can observe the linear growth of training time with data increasing. Besides, the number of optimization variables increases linearly with d. Thus the training time also scales with d. Overall, both SGD and SCG have the linear computational complexity for GPLVMF, while SGD is more efficient in terms of training time. Second, the converge curves of SGD and SCG on the four datasets are illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. We show that the MAE of SGD converges after about 100 epochs for all the datasets while MAE of SCG should stop at a specific iteration to obtain the best performance for the small dataset: Comoda and Food. On the other hand, the best performance between SGD and SCG has no difference. To conclude, SGD optimization has a satisfactory convergence rate and can be trained rapidly and effectively in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis of Contexts</head><p>The ability to discover the importance of each context is essential in many regards. In this work, we seek to extract this information by performing a qualitative analysis of the inverse length-scale Œ±. The original meaning of the inverse length-scale of Gaussian process latent variable model is to determine the strength of each latent dimension. For the application to CARS, the inverse length-scale can be used to automatically determine the strength of each context, as shown in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>For the Comoda dataset, 2 inverse length-scale (7 and 8) out of 12 contexts, are significant. These two contexts represent "ending" and "dominant emotions" respectively. The same conclusion has been obtained by <ref type="bibr" target="#b20">[21]</ref> where they run experiments using one of the contexts at a time, which is computationally expensive compared to our method. On the Food dataset, the inverse length-scale of "situation" is smaller than that of "hunger degree", which indicates the context "hunger degree" is more important. The same conclusion has been achieved by <ref type="bibr" target="#b16">[17]</ref> as well. The most important context for the Sushi dataset is "minor group". It is not too surprising since the users typically choose Sushi according to the type of it. Finally, the "day" is less significant than "hour" on the Movielens-1M dataset.  Finally, the impact of parameters, i.e. different number of inducing points and dimensionalities of the latent space of the item and each context, can be found in the supplementary material.  Finally, we study the performance with using mean funtion (bias) or not, different number of inducing points M and dimensionalities d of the latent space of the item and each context. We first compare the performances of GPLVMF models with using mean function and different dimensionalities and list the preference to achieve the best performance in Table <ref type="table" target="#tab_3">3</ref>. We find that datasets with plenty of contexts (Comoda and Sushi) have a preference on the model with mean and low latent dimension while datasets with fewer contexts (Food and Movielens-1M) prefer model without the mean and high latent dimension. Then we use the Food dataset as an example to plot the value of RMSE versus M and d, as shown in Figure <ref type="figure" target="#fig_4">5</ref>. With increasing d and M , the value of RMSE decreases at first, then stays nearly stable after d = 5 and M = 15. For all the datasets, the parameter M can be selected in a broad range, which means that the performance of GPLVMF doesn't rely on the number of inducing point very much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Impact of parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a novel Gaussian process latent variable model factorization algorithm for context-aware recommendation called GPLVMF, to effectively utilize the contextual information. Both the mean and kernel in the Gaussian process are carefully re-modeled to capture rich modeling. The experimental results of four real datasets show that GPLVMF outperforms the state-of-theart models and can analyze the influence of each context in various datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>represent latent space of corresponding entities for the mean, and Q b v and Q b d are the dimension of corresponding latent space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Training time per iteration for the Sushi dataset as a function of training size the dimensionality of latent space.</figDesc><graphic coords="6,331.13,207.45,193.08,144.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparison of the convergence curves of MAE of SGD and SCG on the four datasets.</figDesc><graphic coords="7,42.11,71.12,508.09,102.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Inverse length-scale of the contexts on the four real datasets</figDesc><graphic coords="7,67.52,212.91,457.29,81.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. RMSE of GPLVMF on the Food dataset with different d and M .</figDesc><graphic coords="7,75.73,408.75,177.84,133.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Statistics of Real Datasets</figDesc><table><row><cell>dataset</cell><cell>user</cell><cell>item</cell><cell>context</cell><cell cols="2">observables rating</cell></row><row><cell>Comoda</cell><cell>121</cell><cell>1232</cell><cell>12</cell><cell>2296</cell><cell>1-5</cell></row><row><cell>Food</cell><cell>212</cell><cell>20</cell><cell>2</cell><cell>5554</cell><cell>1-5</cell></row><row><cell>Sushi</cell><cell>5000</cell><cell>100</cell><cell>7</cell><cell>50000</cell><cell>0-4</cell></row><row><cell>Movielens-1M</cell><cell cols="2">6040 3706</cell><cell>2</cell><cell>1000209</cell><cell>1-5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison on the 4 real datasets in terms of MAE and RMSE</figDesc><table><row><cell>Dataset</cell><cell cols="2">Comoda</cell><cell cols="2">Food</cell><cell cols="2">Sushi</cell><cell cols="2">Movielens-1M</cell></row><row><cell>Performance</cell><cell>MAE</cell><cell>RMSE</cell><cell>MAE</cell><cell>RMSE</cell><cell>MAE</cell><cell>RMSE</cell><cell>MAE</cell><cell>RMSE</cell></row><row><cell>GPLVMF</cell><cell cols="8">0.6708 0.8548 0.6328 0.8356 0.9006 1.1633 0.6701 0.8543</cell></row><row><cell>GPFM</cell><cell cols="8">0.6991 0.8879 0.7311 0.9580 0.9111 1.1643 0.7266 0.9228</cell></row><row><cell>FM</cell><cell cols="8">0.7702 0.9879 0.7853 0.9947 0.9177 1.1645 0.7035 0.8943</cell></row><row><cell>Multiverse</cell><cell cols="8">0.8667 1.1135 0.8290 1.0430 0.9316 1.1665 0.7275 0.9173</cell></row><row><cell cols="9">GPLVM-MF 0.8163 1.0144 0.8587 1.0841 0.9249 1.1715 0.6845 0.8758</cell></row><row><cell>Const</cell><cell cols="8">0.8322 1.0338 0.8991 1.1245 1.0077 1.2514 0.8290 1.0355</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Empirical optimal parameters ("use mean", "dimensionality") for each dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Use mean Dimensionality</cell></row><row><cell>Comoda</cell><cell>Yes</cell><cell>1</cell></row><row><cell>Food</cell><cell>No</cell><cell>7</cell></row><row><cell>Sushi</cell><cell>Yes</cell><cell>2</cell></row><row><cell>Movielens-1M</cell><cell>No</cell><cell>6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>University of Technology Sydney, Australia, email: Wei.Huang-6@student.uts.edu.au</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>University of Technology Sydney, Australia, email: YiDa.Xu@uts.edu.au</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.libfm.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incorporating contextual information in recommender systems using a multidimensional approach</title>
		<author>
			<persName><forename type="first">Gediminas</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahana</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Tuzhilin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="145" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Context-aware recommender systems</title>
		<author>
			<persName><forename type="first">Gediminas</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Tuzhilin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender systems handbook</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="217" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for context aware recommendation</title>
		<author>
			<persName><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth ACM conference on Recommender systems</title>
		<meeting>the fifth ACM conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="301" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Higher-order factorization machines</title>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masakazu</forename><surname>Ishihata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3351" to="3359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep Gaussian processes and variational propagation of uncertainty</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Damianou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>University of Sheffield</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The movielens datasets: History and context</title>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm transactions on interactive intelligent systems (tiis)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gaussian processes for big data</title>
		<author>
			<persName><forename type="first">James</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolo</forename><surname>Fusi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.6835</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback</title>
		<author>
			<persName><forename type="first">Bal√°zs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="67" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Collaborative Gaussian processes for preference learning</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Hern√°ndez-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2096" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nantonac collaborative filtering: recommendation based on order responses</title>
		<author>
			<persName><forename type="first">Toshihiro</forename><surname>Kamishima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the ninth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="583" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering</title>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Amatriain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuria</forename><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM conference on Recommender systems</title>
		<meeting>the fourth ACM conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Advances in collaborative filtering</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender systems handbook</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="77" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Database for contextual personalization</title>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Ko≈°ir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ante</forename><surname>Odic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matevz</forename><surname>Kunaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marko</forename><surname>Tkalcic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jurij</forename><forename type="middle">F</forename><surname>Tasic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="270" to="274" />
		</imprint>
	</monogr>
	<note>Elektrotehni≈°ki vestnik</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gaussian process latent variable models for visualisation of high dimensional data</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Neil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="329" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Non-linear matrix factorization with gaussian processes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">COT: Contextual operating tensor for context-aware recommender systems</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="203" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning context-aware latent representations for context-aware collaborative filtering</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 38th international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="887" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Crossdomain collaborative filtering with factorization machines</title>
		<author>
			<persName><forename type="first">Babak</forename><surname>Loni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Hanjalic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="656" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A scaled conjugate gradient algorithm for fast supervised learning</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Fodslette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M√∏ller</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="525" to="533" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gaussian process factorization machines for context-aware recommendations</title>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Trung V Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linas</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><surname>Baltrunas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Context-aware preference model based on a study of difference between real and supposed situation data</title>
		<author>
			<persName><forename type="first">Chihiro</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuhiro</forename><surname>Takishima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoichi</forename><surname>Motomura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideki</forename><surname>Asoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on User Modeling, Adaptation, and Personalization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="102" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contextaware media recommendations for smart devices</title>
		<author>
			<persName><forename type="first">Abayomi</forename><surname>Moradeyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Otebolaku</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">Teresa</forename><surname>Andrade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Ambient Intelligence and Humanized Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="36" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Post-filtering for a restaurant context-aware recommender system</title>
		<author>
			<persName><forename type="first">Xochilt</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Garc√≠a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Valdez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances on Hybrid Approaches for Designing Intelligent Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="695" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Gaussian process for machine learning</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmussen</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM), 2010 IEEE 10th International Conference on</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast context-aware recommendations with factoriza-tion machines</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bayesian probabilistic matrix factorization using MCMC</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">08</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TFMAP: Optimizing MAP for top-n context-aware recommendation</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuria</forename><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 35th international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bayesian gaussian process latent variable model</title>
		<author>
			<persName><forename type="first">Michalis</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="844" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Explore-exploit in top-n recommender systems via Gaussian processes</title>
		<author>
			<persName><forename type="first">Isidor</forename><surname>Hastagiri P Vanchinathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><forename type="middle">De</forename><surname>Nikolic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Bona</surname></persName>
		</author>
		<author>
			<persName><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM Conference on Recommender systems</title>
		<meeting>the 8th ACM Conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Low-rank matrix completion over finite Abelian group algebras for context-aware recommendation</title>
		<author>
			<persName><forename type="first">Chia-An</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tak-Shing</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2415" to="2418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CMPTF: Contextual modeling probabilistic tensor factorization for recommender systems</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meina</forename><surname>Haihong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junde</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
