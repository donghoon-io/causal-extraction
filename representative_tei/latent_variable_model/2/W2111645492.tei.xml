<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MIT Open Access Articles Multi-view latent variable discriminative models for action recognition</title>
				<funder>
					<orgName type="full">U.S. Army Research, Development, and Engineering Command (RDE-COM</orgName>
				</funder>
				<funder ref="#_jxcyre7">
					<orgName type="full">Office of Naval Research Science of Autonomy program</orgName>
				</funder>
				<funder ref="#_aJNjzVH">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yale</forename><surname>Song</surname></persName>
							<email>yalesong@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
							<email>morency@ict.usc.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">USC Institute for Creative Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Randall</forename><surname>Davis</surname></persName>
							<email>davis@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MIT Open Access Articles Multi-view latent variable discriminative models for action recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/cvpr.2012.6247918</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Institute of Electrical and Electronics Engineers (IEEE)</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many human action recognition tasks involve data that can be factorized into multiple views such as body postures and hand shapes. These views often interact with each other over time, providing important cues to understanding the action. We present multi-view latent variable discriminative models that jointly learn both view-shared and viewspecific sub-structures to capture the interaction between views. Knowledge about the underlying structure of the data is formulated as a multi-chain structured latent conditional model, explicitly learning the interaction between multiple views using disjoint sets of hidden variables in a discriminative manner. The chains are tied using a predetermined topology that repeats over time. We present three topologies -linked, coupled, and linked-coupled -that differ in the type of interaction between views that they model. We evaluate our approach on both segmented and unsegmented human action recognition tasks, using the ArmGesture, the NATOPS, and the ArmGesture-Continuous data. Experimental results show that our approach outperforms previous state-of-the-art action recognition models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many real-world human action recognition tasks involve data that can be factorized into multiple views. For example, the gestures made by baseball coaches involve complex combinations of body and hand signals. The use of multiple views in human action recognition has been shown to improve recognition accuracy <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. Evidence from psychological experiments provides theoretical justification <ref type="bibr" target="#b24">[25]</ref>, showing that people reason about interaction between views (i.e., causal inference) when given combined input signals.</p><p>We introduce the term multi-view dynamic learning as a mechanism for such tasks. The task involves sequential data, where each view is generated by a temporal process and encodes a different source of information. These views often exhibit both view-shared and view-specific substructures <ref type="bibr" target="#b10">[11]</ref>, and usually interact with each other over time, providing important cues to understanding the data.</p><p>Single-view latent variable discriminative models (e.g., HCRF <ref type="bibr" target="#b17">[18]</ref> for segmented sequence data, and LDCRF <ref type="bibr" target="#b14">[15]</ref> for unsegmented sequence data) have shown promising results in many human activity recognition tasks such as gesture and emotion recognition. However, when applied to multi-view latent dynamic learning, existing latent models (e.g., early fusion <ref type="bibr" target="#b26">[27]</ref>) often prove to be inefficient or inappropriate. The main difficulty with this approach is that it needs a set of latent variables that are the product set of the latent variables from each original view <ref type="bibr" target="#b15">[16]</ref>. This increase in complexity is exponential: with C views and D latent variables per view, the product set of all latent variables is O(D C ). This in turn causes the model to require much more data to estimate the underlying distributions correctly (as confirmed in our experiment shown in Section 4), which makes this solution impractical for many real world applications. The task can get even more difficult when, as shown in <ref type="bibr" target="#b4">[5]</ref>, one process with high dynamics (e.g., high variance, noise, frame rate) masks another with low dynamics, with the result that both the view-shared and view-specific substructures are dominated by the view with high dynamics.</p><p>We present here multi-view latent variable discriminative models that jointly learn both view-shared and viewspecific sub-structures. Our approach makes the assumption that observed features from different views are conditionally independent given their respective sets of latent variables, and uses disjoint sets of latent variables to capture the interaction between views. We introduce multi-view HCRF (MV-HCRF) and multi-view LDCRF (MV-LDCRF) models, which extend previous work on HCRF <ref type="bibr" target="#b17">[18]</ref> and LD-CRF <ref type="bibr" target="#b14">[15]</ref> to the multi-view domain (see Figure <ref type="figure" target="#fig_1">1</ref>). Knowledge about the underlying structure of the data is represented as a multi-chain structured conditional latent model. The chains are tied using a predetermined topology that repeats over time. Specifically, we present three topologies -linked, coupled, and linked-coupled-that differ in the type of interaction between views that they model. We demonstrate the superiority of our approach over existing single-view models using three real world human action datasets -the ArmGesture <ref type="bibr" target="#b17">[18]</ref>, the NATOPS <ref type="bibr" target="#b21">[22]</ref>, and the ArmGesture-Continuous datasets -for both segmented and unsegmented human action recognition tasks.</p><p>Section 2 reviews related work, Section 3 presents our models, Section 4 demonstrates our approach using synthetic example, and Section 5 describes experiments and results on the real world data. Section 6 concludes with our contributions and suggests directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Conventional approaches to multi-view learning include early fusion <ref type="bibr" target="#b26">[27]</ref>, i.e., combining the views at the input feature level, and late fusion <ref type="bibr" target="#b26">[27]</ref>, i.e., combining the views at the output level. But these approaches often fail to learn important sub-structures in the data, because they do not take multi-view characteristics into consideration.</p><p>Several approaches have been proposed to exploit the multi-view nature of the data. Co-training <ref type="bibr" target="#b1">[2]</ref> and multiple kernel learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23]</ref> have shown promising results when the views are independent, i.e., they provide different and complementary information of the data. However, when the views are not independent, as is common in human activity recognition, these methods often fail to learn from the data correctly <ref type="bibr" target="#b11">[12]</ref>. Canonical correlation analysis (CCA) <ref type="bibr" target="#b7">[8]</ref> and sparse coding methods <ref type="bibr" target="#b10">[11]</ref> have shown a powerful generalization ability to model dependencies between views. However, these approaches are applicable only to classification and regression problems, and cannot be applied directly to dynamic learning problems.</p><p>Probabilistic graphical models have shown to be extremely successful in dynamic learning. In particular, multiview latent dynamic learning using a generative model (e.g., HMM) has long been an active research area <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref>. Brand et al. <ref type="bibr" target="#b2">[3]</ref> introduced a coupled HMM for action recognition, and Murphy introduced Dynamic Bayesian Networks <ref type="bibr" target="#b15">[16]</ref> that provide a general framework for modeling complex dependencies in hidden (and observed) state variables.</p><p>In a discriminative setting, Sutton et al. <ref type="bibr" target="#b23">[24]</ref> introduced a dynamic CRF (DCRF), and presented a factorial CRF as an instance of the DCRF, which performs multi-labeling tasks. However, their approach only works with single-view input, and may not capture the sub-structures in the data because it does not use latent variables <ref type="bibr" target="#b17">[18]</ref>. More recently, Chen et al. presented a multi-view latent space Markov Network for multi-view object classification and annotation tasks <ref type="bibr" target="#b3">[4]</ref>.</p><p>Our work is different from the previous work in that, instead of making the view independence assumption as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>, we make a conditional independence assumption between views, maintaining computational efficiency while capturing the interaction between views. Unlike <ref type="bibr" target="#b23">[24]</ref>, we formulate our graph to handle multiple input streams independently, and use latent variables to model the substructure of the multi-view data. We also concentrate on multi-view dynamic sequence modeling, as compared to multi-view object recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Multi-view Models</head><p>In this section we describe our multi-view latent variable discriminative models. In particular, we introduce two new family of models, called multi-view HCRF (MV-HCRF) and multi-view LDCRF (MV-LDCRF) that extend previous work on HCRF <ref type="bibr" target="#b17">[18]</ref> and LDCRF <ref type="bibr" target="#b14">[15]</ref> to the multi-view domain. The main difference between the two models lies in that the MV-HCRF is for segmented sequence labeling (i.e., one label per sequence) while the MV-LDCRF is for unsegmented sequence labeling (i.e., one label per frame). We first introduce the notation, describe MV-HCRF and MV-LDCRF, present three topologies that define how the views interact, and explain inference and parameter estimation.</p><p>Input to our model is a set of multi-view sequences x = {x (1) </p><formula xml:id="formula_0">, • • • , x (C) }, where each x (c) = {x (c) 1 , • • • , x (c)</formula><p>T } is an observation sequence of length T from the c-th view. Each xt is associated with a label y t that is a member of a finite discrete set Y; for segmented sequences, there is only one y for all t. We represent each observation x (c) t with a feature vector φ(x</p><formula xml:id="formula_1">(c) t ) ∈ R N .</formula><p>To model the sub-structure of the multi-view sequences, we use a set of latent variables ĥ = {h (1) </p><formula xml:id="formula_2">, • • • , h (C) }, where each h (c) = {h (c) 1 , • • • , h (c) T } is a hidden state sequence of length T . Each random variable h (c)</formula><p>t is a member of a finite discrete set H (c) of the c-th view, which is disjoint from view to view. Each hidden variable h </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-view HCRF</head><p>We represent our model as a conditional probability distribution that factorizes according to an undirected graph G = (V, E P , E S ) defined over a multi-chain structured stochastic process, where each chain is a discrete representation of each view. A set of vertices V represents random variables (observed or unobserved) and the two sets of edges E P and E S represent dependencies among random variables. The unobserved (hidden) variables are marginalized out to compute the conditional probability distribution.</p><p>We call E P a set of view-specific edges; they encode temporal dependencies specific to each view. E S is a set of view-shared edges that encode interactions between views.</p><p>Similar to HCRF <ref type="bibr" target="#b17">[18]</ref>, we construct a conditional probability distribution with a set of weight parameters Λ = {λ, ω} as p(y | x; Λ) = ĥ p(y, ĥ | x; Λ) = 1 Z ĥ e Λ Φ(y, ĥ,x) (1) The topologies (i.e., linked and coupled) differ by modeling the type of interaction between views; see the text for detail. The linked-coupled multi-view topologies (not shown here) are a combination of the linked and coupled topologies. Note that we illustrate two-view models for simplicity, generalization to &gt;2 views can be done easily by following the rules stated in Equation <ref type="formula" target="#formula_12">5</ref>.</p><formula xml:id="formula_3">y h (c) t-1 h (c) t h (c) t+1 x (c) t-1 x (c) t x (c) t+1 x (d) t-1 x (d) t x (d) t+1 h (d) t-1 h (d) t h (d) t+1 (a) Linked HCRF y h (c) t-1 h (c) t h (c) t+1 x (c) t-1 x (c) t x (c) t+1 x (d) t-1 x (d) t x (d) t+1 h (d) t-1 h (d) t h (d) t+1 (b) Coupled HCRF y t-1 yt y t+1 h (c) t-1 h (c) t h (c) t+1 x (c) t-1 x (c) t x (c) t+1 h (d) t-1 h (d) t h (d) t+1 x (d) t-1 x (d) t x (d) t+1 (c) Linked LDCRF y t-1 yt y t+1 h (c) t-1 h (c) t h (c) t+1 x (c) t-1 x (c) t x (c) t+1 h (d) t-1 h (d) t h (d) t+1 x (d) t-1 x (d) t x (d) t+1 (d) Coupled LDCRF</formula><p>where Λ Φ(y, ĥ, x) is a potential function and Z = y ∈Y, ĥ e Λ T Φ(y , ĥ,x) is a partition function for normalization. The potential function is factorized with feature functions</p><formula xml:id="formula_4">f k (•) and g k (•) as Λ Φ(y, ĥ, x) = (s,c)∈V k λ k f k (y, h (c) s , x (c) )<label>(2)</label></formula><formula xml:id="formula_5">+ (s,t,c,d)∈E k ω k g k (y, h (c) s , h<label>(d) t , x)</label></formula><p>where E = E P ∪ E S . The first term λ k f k (•) represents singleton potentials defined over a single hidden variable h </p><formula xml:id="formula_6">c |H (c) | × |φ(x (c) )|. Note that f k (•) are modeled under</formula><p>the assumption that views are conditionally independent given hidden variables, and thus encode the view-specific sub-structures.</p><p>The feature function g k (•) encodes both view-shared and view-specific sub-structures. The definition of g k (•) depends on how the two sets of edges E P and E S are defined; we detail these in Section 3.3.</p><p>Once we obtain the optimal set of parameters Λ * (described in Section 3.4), a class label y * for a new observation sequence x is determined as y * = arg max y∈Y p(y | x; Λ * ).</p><p>Note that multi-view HCRF is similar to HCRF <ref type="bibr" target="#b17">[18]</ref>; the difference lies in the multi-chain structured model (c.f., the tree-structured model in HCRF), which makes our model capable of multi-view dynamic learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-view LDCRF</head><p>The MV-HCRF models described above have focused on the task of segmented sequence labeling, where only one label y is assigned to the whole sequence. In this section, we propose a second family of multi-view models tailored to unsegmented sequence labeling, called multi-view LDCRF (MV-LDCRF), which is inspired by LDCRF <ref type="bibr" target="#b14">[15]</ref>.</p><p>An MV-LDCRF is a multi-view discriminative model for simultaneous sequence segmentation and labeling that can capture both intrinsic and extrinsic class dynamics. Similar to <ref type="bibr" target="#b14">[15]</ref>, we assume that each class label y t ∈ Y has a disjoint set of associated hidden states Ĥy , which makes p(y | ĥ, x; Λ) = 0 for any h</p><formula xml:id="formula_7">(c) s / ∈ H (c)</formula><p>ys . Therefore, a conditional probability distribution is written as:</p><formula xml:id="formula_8">p(y | x; Λ) = ĥ:∀h (c) s ∈H (c) ys p( ĥ | x; Λ)<label>(3)</label></formula><p>The definition of feature functions f k (•) and g k (•) are similar to MV-HCRF (see Section 3.1); the only difference is that we include only the observation function for f k (•), i.e., no label feature function f k (y, h</p><p>s ). For testing, instead of estimating a single most probable sequence label y * , we want to estimate a sequence of most probable labels y * . This is obtained as:</p><formula xml:id="formula_10">y * = arg max y∈Y C c=1 h (c) :∀h (c) s ∈H (c) ys α c p(h (c) | x; Λ * ) (4)</formula><p>where C is the number of views, and c α c = 1 sets relative weights on the marginal probability from the c-th view.</p><p>In our experiments, since we had no prior knowledge about the relative importance of each view, we set all α c to 1/C. To estimate the label y * t of the t-th frame, we compute the marginal probabilities p(h</p><formula xml:id="formula_11">(c) t = h | x; Λ * )</formula><p>for all views c ∈ C and for all hidden states h ∈ H (c) of each view.</p><p>Then, for each view c, we sum the marginal probabilities according to H (c) ys , and compute a weighted mean of them across all views. Finally, the label y t that is associated with the optimal set is chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Topologies: Linked and Coupled</head><p>The configuration of E P and E S encodes the view-shared and view-specific sub-structures. Inspired by <ref type="bibr" target="#b15">[16]</ref>, we present three topologies that differ in defining the viewshared edges E S : linked, coupled, and linked-coupled. Because these topologies have repeating patterns, they make the algorithm simple yet powerful, as in HCRF <ref type="bibr" target="#b17">[18]</ref>. Figure <ref type="figure" target="#fig_1">1</ref> illustrates graphical representations of linked and coupled topologies for both the MV-HCRF and MV-LDCRF families.</p><p>The linked multi-view topologies (Figure <ref type="figure" target="#fig_1">1</ref>(a) and 1(c)) model contemporaneous connections between views, i.e., the current state in one view concurrently affects the current state in the other view. Intuitively, this captures the synchronization points between views. The coupled multi-view topologies (Figure <ref type="figure" target="#fig_1">1(b</ref>) and 1(d)) model first-order Markov connections between views, i.e., the current state in one view affects the next state in the other view. Intuitively, this captures the "poker game" interaction, where one player's move is affected by the other player's previous move (but no synchronization between players at the current move). The linked-coupled multi-view topologies are a combination of the linked and coupled topologies, i.e., it models the "full" interaction between each pair of multiple sequential chains. In all our models, we assume view-specific firstorder Markov chain structure, i.e., the current state affects the next state in the same view.</p><p>We encode these dependencies by defining the transition feature functions g k (•) as</p><formula xml:id="formula_12">g k (y, h (c) s , h (d) t ) = 1 ⇐⇒    (s + 1 = t ∧ c = d) ∨ (s = t ∧ c = d) (linked) (s + 1 = t) (coupled) (s + 1 = t) ∨ (s = t ∧ c = d) (linked-coupled)<label>(5)</label></formula><p>In other words, each feature g k (•) is non-zero only when there is an edge between h</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(c)</head><p>s and h</p><formula xml:id="formula_13">(d) t</formula><p>as specified in the union of E P and E S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Parameter Estimation and Inference</head><p>Given a training dataset D = {y i , xi } N i=1 , we find the optimal parameter set Λ * = {λ * , ω * } by minimizing the conditional log-likelihood<ref type="foot" target="#foot_0">foot_0</ref> </p><formula xml:id="formula_14">min Λ L(Λ) = γ 2 Λ 2 - N i=1 log p(y i | xi ; Λ)<label>(6)</label></formula><p>where the first term is an L2-norm regularization factor. The second term, inside the summation, can be re-written as: log p(y i |x i ; Λ) = ĥ Λ Φ(y, ĥ, x)y , ĥ Λ Φ(y , ĥ, x) <ref type="bibr" target="#b6">(7)</ref> As with other latent models (e.g., HMMs <ref type="bibr" target="#b18">[19]</ref>), introducing hidden variables in Equation 7 makes our objective function non-convex. We find the optimal parameters Λ * using the recently proposed non-convex regularized bundle method (NRBM) <ref type="bibr" target="#b6">[7]</ref>, which has been proven to converge to a solution with an accuracy at the rate O(1/ ). The method aims at iteratively building an increasingly accurate piecewise quadratic lower bound of L(Λ) based on its subgradient</p><formula xml:id="formula_15">∂ Λ L(Λ) = ∂ λ L(Λ) + ∂ ω L(Λ). ∂ λ L(Λ)</formula><p>can be computed as follows (∂ ω L(Λ) omitted for space):</p><formula xml:id="formula_16">∂L(Λ) ∂λ k = (s,c),h p(h (c) s = h | y, x (c) ; Λ)f k (•)<label>(8)</label></formula><p>-</p><formula xml:id="formula_17">(s,c),h ,y p(y , h (c) s = h | x (c) ; Λ)f k (•)</formula><p>The most computationally intensive part of solving Equation 6 is the inference task of computing the marginal probabilities in Equation <ref type="formula" target="#formula_16">8</ref>. We implemented both the junction tree (JT) algorithm <ref type="bibr" target="#b5">[6]</ref> for an exact inference and the loopy belief propagation (LBP) <ref type="bibr" target="#b16">[17]</ref> for an efficient approximate inference. For LBP, the message update is done with random scheduling. The update is considered as "converged" when the previous and the current marginals differ by less than 10 -4 . Note that we can easily change our optimization problem in Equation 6 into the max-margin approach <ref type="bibr" target="#b25">[26]</ref> by replacing ĥ and y , ĥ in Equation <ref type="formula">7</ref>with max ĥ and max y , ĥ and solving MAP inference problem. Max-margin approaches have recently been shown to improve the performance of HCRF <ref type="bibr" target="#b25">[26]</ref>; we plan to implement the maxmargin approach for our multi-view models in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments on Synthetic Data</head><p>As an initial demonstration of our approach, we use a synthetic example and compare three MV-HCRF models (LHCRF, CHCRF, and LCHCRF) to a single-view HCRF. Specifically, we focus on showing the advantage of exponentially reduced model complexity of our approach by comparing performance with varying training dataset size.</p><p>Dataset: Synthetic data for a three-view binary classification task was generated using the Gibb's sampler <ref type="bibr" target="#b20">[21]</ref>. Three first-order Markov chains, with 4 hidden states each, were tied using the linked-coupled topology (i.e., LCHCRF; see Equation <ref type="formula" target="#formula_12">5</ref>). In order to simulate three views having strong interaction, we set the weights on view-shared edges to random real values in the range [-10, 10], while viewspecific edge weights were [-1, 1]. The observation model for each view was defined by a 4D exponential distribution; to simulate three views having different dynamics, the minmax range of the distribution was set to [-1, 1], <ref type="bibr">[-5, 5]</ref>, and [-10, 10], respectively. To draw samples, we randomly initialized model parameters, and iterated 50 times using the Gibb's sampler. The sequence length was set to 30.</p><p>Methodology: Inference in the MV-HCRF models was performed using both the junction tree and the loopy BP algorithms. Since we know the exact number of hidden states per view, the MV-HCRF models were set to have that many hidden states. The optimal number of hidden states for the HCRF was selected automatically based on validation, varying the number from 12 to 72 with an increment of 12. The size of the training and validation splits were varied from 100 to 500 with an increment of 100, the size of the test split was always 1,000. For each split size, we performed 5-fold cross validation except that the test split size stayed constant at 1,000. Each model was trained with five random initializations, and the best validation parameter was selected based on classification accuracy.</p><p>Result: Figure <ref type="figure" target="#fig_3">2</ref> and Table <ref type="table" target="#tab_0">1</ref> show classification accuracy as a function of dataset size, comparing HCRF and the three MV-HCRF models. The results are averaged values over the 5 splits. The MV-HCRF models always outperformed the HCRF; these differences were statistically significant for the dataset size of 100 and 300. Our result also shows that an approximate inference method (i.e., LBP) on the multi-view models achieves as good accuracy as done with an exact inference method (i.e., JT).</p><p>The optimal number of hidden states for the best performing HCRF was 48, which resulted in a sizable difference in the model complexity, with 6,144 parameters to estimate using HCRF, compared to the number of parameters needed for LHCRF (240), CHCRF (336), and LCHCRF (432). Consequently, the MV-HCRF models outperformed the HCRF consistently even with one third of the training dataset size (see Table <ref type="table" target="#tab_0">1</ref>, dataset size 100 and 300). Note that the performance difference between multi-view and single-view HCRFs is larger at a smaller training split size. This implies that our multi-view approach is advantageous especially when there is not enough training data, which is often the case in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments on Real-world Data</head><p>We evaluated our multi-view models on segmented and unsegmented human action recognition tasks using three datasets: the ArmGesture dataset <ref type="bibr" target="#b17">[18]</ref>, the NATOPS dataset <ref type="bibr" target="#b21">[22]</ref>, and the ArmGesture-Continuous dataset. <ref type="foot" target="#foot_1">2</ref> The first two datasets involve segmented gestures, while the third involves unsegmented gestures that we created based  on <ref type="bibr" target="#b17">[18]</ref>. Below we describe the datasets, detail our experimental methods with baselines, and report and discuss the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>ArmGesture <ref type="bibr" target="#b17">[18]</ref>: This dataset includes the six arm gestures shown in Figure <ref type="figure" target="#fig_4">3</ref>. Observation features include automatically tracked 2D joint angles and 3D euclidean coordinates for left/right shoulders and elbows; each observation is represented as a 20D feature vector. The dataset was collected from 13 participants with an average of 120 samples per class. <ref type="foot" target="#foot_2">3</ref> Following <ref type="bibr" target="#b19">[20]</ref>, we subsampled the data by the factor of 2. For multi-view models, we divided signals into the left and right arms.</p><p>NATOPS <ref type="bibr" target="#b21">[22]</ref>: This dataset includes twenty-four bodyhand gestures used when handling aircraft on the deck of an aircraft carrier. We used the six gestures shown in Fig- <ref type="figure" target="#fig_5">ure 4</ref>. The dataset includes automatically tracked 3D body postures and hand shapes. The body feature includes 3D joint velocities for left/right elbows and wrists, and represented as a 12D input feature vector. The hand feature includes probability estimates of five predefined hand shapes -opened/closed palm, thumb up/down, and "no hand". The fifth shape, no hand, was dropped in the final representa-  tion, resulting in an 8D input feature vector (both hands). Note that, in the second gesture pair (#3 and #4), the hand signals contained mostly zero values, because these hand shapes were not included in the predefined hand poses. We included this gesture pair to see how the multi-view models would perform when one view has almost no information. We used 10 samples per person, for a total of 200 samples per class. Similar to the previous experiment, we subsampled the data by the factor of 2. For multi-view models, we divided signals into body postures and hand shapes. <ref type="foot" target="#foot_3">4</ref>ArmGesture-Continuous: The two above mentioned datasets include segmented sequences only. To evaluate the MV-LDCRF models on unsegmented sequences, we created a new dataset based on the ArmGesture dataset, called ArmGesture-Continuous. To generate an unsegmented sequence, we randomly selected 3 to 5 (segmented) samples from different classes, and concatenated them in random order. This resulted in 182 samples in total, with an average of 92 frames per sample. Similar to the previous experiment, we subsampled the data by the factor of 2, and divided signals into the left and right for multi-view models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Methodology</head><p>Based on the previous experiment on the synthetic data, we selected two topologies, linked and coupled, to compare to several baselines. Also, the junction tree algorithm is selected as an inference method for the multi-view models. In all experiments, we performed four random initializations of the model parameters, and the best validation parameter was selected based on classification accuracy. Below we detail experimental methods and baselines used in each experiment.</p><p>ArmGesture: Five baselines were chosen: HMM <ref type="bibr" target="#b18">[19]</ref>, CRF <ref type="bibr" target="#b12">[13]</ref>, HCRF <ref type="bibr" target="#b17">[18]</ref>, max-margin HCRF <ref type="bibr" target="#b25">[26]</ref>, and S-KDR-SVM <ref type="bibr" target="#b19">[20]</ref>. Following <ref type="bibr" target="#b19">[20]</ref>, we performed 5-fold cross validation. <ref type="foot" target="#foot_4">5</ref> The number of hidden states was automatically validated; for the single-view model (i.e., MM-HCRF), we varied it from 8 to 16, increasing by 4; and for multi-view models, we varied it from 8 (4 per view) to 16 (8 per view), increasing by 4 (2 per view). No regularization was used in this experiment.</p><p>NATOPS: Three baselines were chosen: HMM <ref type="bibr" target="#b18">[19]</ref>, CRF <ref type="bibr" target="#b12">[13]</ref>, and HCRF <ref type="bibr" target="#b17">[18]</ref>. We performed hold-out testing, where we selected samples from the last 10 subjects for training, the first 5 subjects for testing, and the remaining 5 subjects for validation. The number of hidden states was automatically validated; for single-view models (i.e., HMM and HCRF), we varied it from 12 to 120, increasing by 12; for multi-view models, we varied it from 6 (3 per view) to 60 (30 per view). No regularization was used in this experiment.</p><p>ArmGesture-Continuous: Unlike the two previous experiments, the task in this experiment was continuous sequence labeling. We compared a linked LDCRF to two baselines: CRF <ref type="bibr" target="#b12">[13]</ref> and LDCRF <ref type="bibr" target="#b14">[15]</ref>. We performed holdout testing, where we selected the second half of the dataset for training, the first quarter for testing, and the remaining for validation. The number of hidden states was automatically validated; for the single-view model (i.e., LDCRF), we varied it from 2 to 4; for multi-view models, we varied it from 4 (2 per view) to 8 (4 per view). The regularization coefficient was also automatically validated with values 0 and 10 k , k=[-4:2:4].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results and Discussion</head><p>Table <ref type="table" target="#tab_1">2</ref> shows classification accuracy on the ArmGesture dataset. For comparison, we include the results reported in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>. The results in MM-HCRF, S-KDR-SVM, LHCRF, and CHCRF are averaged values over the 5 splits. Our MV-HCRF models (LHCRF and CHCRF) outperformed all other baselines. This shows that our approach more precisely captures the hidden interaction between views, e.g., Models Accuracy (%) HMM <ref type="bibr" target="#b17">[18]</ref> 84.22 CRF <ref type="bibr" target="#b17">[18]</ref> 86.03 HCRF <ref type="bibr" target="#b17">[18]</ref> 91.64 HCRF (ω = 1) <ref type="bibr" target="#b17">[18]</ref> 93.86 MM-HCRF 93.79 S-KDR-SVM <ref type="bibr" target="#b19">[20]</ref> 95.30</p><p>Linked HCRF 97.65 Coupled HCRF 97.24 when the left arm is lifted (or lowered), the right arm is lowered (or lifted) (see the gestures EV and SV in Figure <ref type="figure" target="#fig_4">3</ref>). We note that S-KDR-SVM was trained on a smaller dataset size (N=10) <ref type="bibr" target="#b19">[20]</ref>. To the best of our knowledge, our result is the best classification accuracy reported in the literature.</p><p>Table <ref type="table" target="#tab_2">3</ref> shows classification accuracy on the NATOPS dataset. All of our multi-view models outperformed the baselines. The accuracy of CRF was significantly lower than other latent models. This suggests that, on this dataset, it is crucial to learn the sub-structure of the data using latent variables. Figure <ref type="figure">5</ref> shows an ROC plot averaged over all 6 classes, and a confusion matrix from the result of CHCRF. As expected, most labeling errors occurred within each gesture pair (i.e., #1-#2, #3-#4, and #5-#6). We can see from the ROC plot that our multi-view models reduce both false positives and false negatives.</p><p>Detailed analysis from the NATOPS experiment revealed that the multi-view models dramatically increased the per gesture pair classification accuracy; for the first and the third gesture pairs (i.e., #1-#2 and #5-#6), CHCRF achieved an accuracy of 91.5% and 93.5%, while for HCRF they were 75% and 76%. We claim that this result empirically demonstrates the benefit of our approach when each view has different dynamics (i.e., one view with velocity measures vs. another view with probability measures). The second gesture pair showed inferior performance in our multi-view models; CHCRF achieved 70%, while for HCRF it was 80%. This raises an interesting point: when one view (hand) contains almost no information, forcing the model to capture the interaction between views results in inferior performance, possibly due to the increased complexity. This observation suggests automatically learning the optimal topology of E P and E S could help solve this problem; we leave this as future work. Table <ref type="table" target="#tab_3">4</ref> shows per-frame classification accuracy on the ArmGesture-Continuous dataset. Consistent with our previous experimental results, the linked LDCRF outperformed the single-view LDCRF. This shows that our approach outperforms single-view discriminative models on both segmented and unsegmented action recognition tasks.</p><p>Although the linked and coupled HCRF models capture different interaction patterns, these models performed almost similarly (see Table <ref type="table" target="#tab_1">2</ref> and<ref type="table" target="#tab_2">Table 3</ref>). We believe this is due to the high sampling rate of the two datasets, making the two models almost equivalent. We plan to investigate the difference between these models using higher order Markov connections between views for the coupled models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduced multi-view latent variable discriminative models that jointly learn both view-shared and viewspecific sub-structures, explicitly capturing the interaction between views using disjoint sets of latent variables. We evaluated our approach using synthetic and real world data, for both segmented and unsegmented human action recognition, and demonstrated empirically that our approach successfully captures the latent interaction between views, achieving superior classification accuracy on all three human action datasets we evaluated.</p><p>Our multi-view approach has a number of clear advantages over the single-view approach. Since each view is treated independently at the input feature level, the model captures different dynamics from each view more precisely. It also explicitly models the interaction between views using disjoint sets of latent variables, thus the total number of latent variables increases only linearly in the number of views, as compared to the early fusion approach where it increases exponentially. Since the model has fewer parameters to estimate, model training requires far less training data, making our approach favorable in real world applications.</p><p>In the future, we plan to extend our models to work with data where the views are not explicitly defined. Currently we assume a priori knowledge about the data and manually define the views. However, in many real world tasks the views are not explicitly defined. In this case, we may be able to perform independent component analysis <ref type="bibr" target="#b8">[9]</ref> or data clustering <ref type="bibr" target="#b9">[10]</ref> to automatically learn the optimal view configuration in an unsupervised manner. We look forward to exploring this in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>st</head><label></label><figDesc>is indexed by a pair (s, c). An edge between two hidden variables h is indexed by a quadruple (s, t, c, d), where {s, t} describes the time indices and {c, d} describes the view indices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Graphical representations of multi-view latent variable discriminative models: (a) linked HCRF (LHCRF), (b) coupled HCRF (CHCRF), (c) linked LDCRF (LLDCRF), and (d) coupled LDCRF (CLDCRF). Grey nodes are observed variables and white nodes are unobserved variables.The topologies (i.e., linked and coupled) differ by modeling the type of interaction between views; see the text for detail. The linked-coupled multi-view topologies (not shown here) are a combination of the linked and coupled topologies. Note that we illustrate two-view models for simplicity, generalization to &gt;2 views can be done easily by following the rules stated in Equation5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, and the second term ω k g k (•) represents pairwise potentials over a pair of hidden variables (h (c) s , h (d) t ) ∈ E. We define two types of f k (•) feature functions. The label feature function f k (y, h (c) s ) models the relationship between a hidden state h (c) s ∈ H (c) and a label y ∈ Y; thus, the number of the label feature functions is c |Y| × |H (c) |. The observation feature function f k (h (c) s , x (c) ) represents the relationship between a hidden state h (c) s ∈ H (c) and observations φ(x (c) ), and is of length</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Accuracy graph as a function of the training dataset size on the synthetic dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. ArmGesture dataset [18]. Illustration of the 6 gesture classes (Flip Back, Shrink Vertically, Expand Vertically, Double Back, Point and Back, Expand Horizontally). The green arrows are the motion trajectory of the fingertip.</figDesc><graphic coords="7,56.86,83.96,222.75,57.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. NATOPS dataset [22]. Illustration of the 6 aircraft handling signal gestures. Body movements are illustrated in yellow arrows, and hand poses are illustrated with synthesized images of hands. Red rectangles indicate hand poses are important in distinguishing the gesture pair.</figDesc><graphic coords="7,57.21,275.85,73.85,58.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="1,113.50,16.20,385.00,140.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Experimental</figDesc><table><row><cell>Models</cell><cell>N=100</cell><cell>Accuracy (%) N=300</cell><cell>N=500</cell></row><row><cell>LHCRF-JT</cell><cell>69.58 (p&lt;.01)</cell><cell>72.76 (p=.01)</cell><cell>73.52 (p=.12)</cell></row><row><cell>CHCRF-JT</cell><cell>69.88 (p&lt;.01)</cell><cell>72.94 (p&lt;.01)</cell><cell>72.66 (p=.56)</cell></row><row><cell>LCHCRF-JT</cell><cell>69.10 (p&lt;.01)</cell><cell>72.92 (p&lt;.01)</cell><cell>75.56 (p=.03)</cell></row><row><cell>LHCRF-LBP</cell><cell>69.94 (p&lt;.01)</cell><cell>72.70 (p=.01)</cell><cell>75.28 (p=.01)</cell></row><row><cell>CHCRF-LBP</cell><cell>69.66 (p&lt;.01)</cell><cell>71.50 (p=.01)</cell><cell>72.60 (p=.59)</cell></row><row><cell>LCHCRF-LBP</cell><cell>68.86 (p&lt;.01)</cell><cell>72.22 (p=.03)</cell><cell>73.44 (p=.17)</cell></row><row><cell>HCRF</cell><cell>60.18</cell><cell>67.04</cell><cell>72.14</cell></row></table><note><p>results on the synthetic dataset. The MV-HCRF models statistically significantly outperformed the singleview HCRF at the dataset size of 100 and 300. Values in parenthesis show p-values from t-tests against HCRF. Bold faced values indicate the difference against HCRF was statistically significant.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Experimental results on the ArmGesture dataset. We include the classification accuracy reported in<ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>. Our multiview HCRF models (LHCRF and CHCRF) outperformed all the baselines; to the best of our knowledge, this is the best classification accuracy reported in the literature. ω = 1 means that the previous and next observations are concatenated to produce an observation.</figDesc><table><row><cell>Models</cell><cell>Accuracy (%)</cell></row><row><cell>HMM</cell><cell>77.67</cell></row><row><cell>CRF</cell><cell>53.30</cell></row><row><cell>HCRF</cell><cell>78.00</cell></row><row><cell>Linked HCRF</cell><cell>87.00</cell></row><row><cell>Coupled HCRF</cell><cell>86.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Experimental results on the NATOPS dataset. Our MV-HCRF models outperformed all the baselines. The only nonlatent model, i.e., CRF, performed the worst, suggesting that it is crucial to learn a model with latent variables on this dataset.</figDesc><table><row><cell>Models</cell><cell>Accuracy (%)</cell></row><row><cell>CRF</cell><cell>90.80</cell></row><row><cell>LDCRF</cell><cell>91.02</cell></row><row><cell>Linked LDCRF</cell><cell>92.51</cell></row><row><cell>Coupled LDCRF</cell><cell>92.44</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Experimental results on the ArmGesture-Continuous dataset. Our linked LDCRF outperformed the two baselines.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ROC curve and a confusion matrix from the NATOPS experiments. As expected, most labeling errors occurred within each gesture pair. Top of the confusion matrix shows the F1 score and the number of hidden states of CHCRF. See the text for detail.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">CHCRF (F1=0.86, |H * |=30(15+15))</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell>0.88</cell><cell></cell><cell></cell></row><row><cell>True Positive Rate</cell><cell>0.4 0.5 0.6 0.7 0.3 0.2</cell><cell></cell><cell></cell><cell></cell><cell>HMM CRF HCRF</cell><cell>Ground Truth</cell><cell>3 4 5</cell><cell></cell><cell></cell><cell>0.85</cell><cell>0.55</cell><cell>0.90</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LHCRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell>CHCRF</cell><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.97</cell></row><row><cell></cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">False Positive Rate</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Prediction</cell></row><row><cell cols="2">Figure 5.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The derivations in this section is for MV-HCRF. This can be changed easily for MV-LDCRF by replacing y i with y i .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The dataset and the source code of our model are available at http://people.csail.mit.edu/yalesong/cvpr12/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The exact sample counts per class were[88, 117, 118, 132, 179, 90].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We divided the views in this way because the two views had different dynamics (i.e., scales); hand signals contained normalized probability estimates, where as body signals contained joint velocities.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>We did this for the direct comparison to the state-of-the-art result on this dataset<ref type="bibr" target="#b19">[20]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was funded by the <rs type="funder">Office of Naval Research Science of Autonomy program</rs> #<rs type="grantNumber">N000140910625</rs>, the <rs type="funder">National Science Foundation</rs> #<rs type="grantNumber">IIS-1018055</rs>, and the <rs type="funder">U.S. Army Research, Development, and Engineering Command (RDE-COM</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jxcyre7">
					<idno type="grant-number">N000140910625</idno>
				</org>
				<org type="funding" xml:id="_aJNjzVH">
					<idno type="grant-number">IIS-1018055</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multimodal fusion for multimedia analysis: a survey</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Atrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El-Saddik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Syst</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="345" to="379" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled sata with co-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coupled hidden markov models for complex action recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predictive subspace learning for multi-view data: a large margin approach</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view learning in presence of view disagreement</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Christoudias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Cowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename></persName>
		</author>
		<title level="m">Spiegelhalter. Probabilistic Networks and Expert Systems</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large margin training for hidden markov models with partially observed states</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Artières</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comp</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<title level="m">Independent Component Analysis</title>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data clustering: A review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Factorized latent spaces with structured sparsity</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-relational learning, text mining, and semi-supervised learning for functional genomics</title>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Krogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="61" to="81" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning the kernel matrix with semidefinite programming</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="27" to="72" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Latent-dynamic discriminative models for continuous gesture recognition</title>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2007. 1, 2, 3, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic Bayesian Networks: Representation, Inference and Learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UCB</title>
		<imprint>
			<date type="published" when="2002">2002. 1, 2, 4</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Loopy belief propagation for approximate inference: An empirical study</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Hidden conditional random fields. T-PAMI</title>
		<imprint>
			<date type="published" when="2007">2007. 1, 2, 3, 4, 5, 6, 7</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1848" to="1852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="267" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sufficient dimension reduction for visual sequence classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shyr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian computation via the gibbs sampler and related markov chain monte carlo methods</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">O</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Roy. Stat. Soc. Series B</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2" to="23" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tracking body and hands for gesture recognition: Natops aircraft handling signals database</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Demirdjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2006">2011. 1, 5, 6</date>
			<biblScope unit="page" from="500" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large scale multiple kernel learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1531" to="1565" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rohanimanesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Goodman. How to grow a mind: Statistics, structure, and abstraction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="issue">6022</biblScope>
			<biblScope unit="page" from="1279" to="1285" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Max-margin hidden conditional random fields for human action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multimodal integration -a statistical view</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
