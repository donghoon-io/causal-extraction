<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding Masked Autoencoders via Hierarchical Latent Variable Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-06-08">8 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lingjing</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><forename type="middle">Q</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuejie</forename><surname>Chi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding Masked Autoencoders via Hierarchical Latent Variable Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-08">8 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2306.04898v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Masked autoencoder (MAE), a simple and effective selfsupervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical insights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masking-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Self-supervised learning (SSL) has achieved tremendous success in learning transferable representations without labels, showing strong results in a variety of downstream tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b48">49]</ref>. As a major SSL paradigm, masked image modeling (MIM) <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b68">69]</ref> performs the reconstruction of purposely masked image pixels as the pretraining task. Among MIM methods, masked autoencoding (MAE) <ref type="bibr" target="#b21">[22]</ref> has gained significant traction due to its computational efficiency and state-of-the-art performance in a wide range of downstream tasks.</p><p>Empirical observations from previous work reveal various intriguing properties of MAE. In particular, aggressive * Joint first author. † Joint senior author. In a hierarchical data-generating process, high-level latent variables (e.g., z 1 ) represent high-level information such as semantics, and low-level latent variables (e.g., [z 2 , z 3 , z 4 ]) represent low-level information such as texture. We show that through proper masking, MAE learns to recover high-level latent variables with identifiability guarantees. masking has been shown critical to downstream task performances <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b62">63]</ref>. It is conjectured that such masking forces the model to learn meaningful high-level semantic understanding of the objects and scenes rather than the low-level information such as texture. However, it remains largely unclear whether such intuitions are sound in principle. Theoretically verifying and characterizing these empirical insights would not only grant a certificate to the current approaches but would also offer theoretical insights for algorithmic advancements.</p><p>In this work, we establish a principled yet intuitive framework for understanding MAE and providing identifiability guarantees. Concretely, we first formulate the underlying data-generating process as a hierarchical latent variable model (Figure <ref type="figure" target="#fig_0">1</ref>), with high-level variables corresponding to abstract and semantic information like classes, and low-level variables corresponding to elaborate and granular information like texture. Such latent variable models have been studied in causal discovery <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b61">62]</ref>. In <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b49">50]</ref>, it is hypothesized that complex data, such as images, follow a hierarchical latent structure.</p><p>Stemming from this formulation, we show that under reasonable assumptions, MAE can recover a subset of the true latent variables within the hierarchy, where the levels of the learned latent variables are explicitly determined by how masking is performed. Our theoretical framework not only unifies existing empirical observations coherently but also gives rise to insights for potential empirical improvements and fundamental limitations of MAE. Our theory improves the existing nonlinear identifiability results <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b57">58]</ref> and can be of independent interest.</p><p>Empirically, we deduce several insights from our theoretical results and verify them with experiments. Unlike common belief, MAE trained with extremely high masking ratios (e.g., 90%) captures low-level information, similar to models trained with extremely low ratios (e.g., 10%). Our results suggest that learning high-level semantic information is only possible in the non-extreme masking regime. We also discuss masking designs that can potentially improve current empirical performance.</p><p>Contributions. We highlight the following contributions:</p><p>• We formulate the underlying data-generating process as a hierarchical latent variable model. Under such a formulation, we provide a theoretical guarantee for MAE by showing that it can recover true latent variables in the hierarchical model.</p><p>• Based on our theoretical results, we establish the connection between masking hyperparameters (i.e., masking ratios and patch sizes) and the learned representation and discuss potential improvements and inherent limitations of MAE.</p><p>• We validate our theoretical insights with extensive experimental results. We illustrate how the semantic level of the learned representation varies with the aggressiveness of the masking strategy. Interestingly, representations learned under overly aggressive masking (e.g.,ß 90% masking ratio) exhibit similar properties to their counterparts learned with overly conservative masking (e.g., 10% masking ratio).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Theoretical Understanding 2.1. A Hierarchical Data-generating Process</head><p>Images, despite their high dimensionality, are well structured -there is a multitude of statistical dependencies among pixels determined by their relative distances and visual semantics. For instance, pixels in close proximity are often highly dependent, whereas pixels far apart typically share less information. There has been a plethora of work adopting this intuition for vision tasks such as image generation <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b66">67]</ref>. Similar insights are also addressed in attempts to learn a part-whole image representation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b49">50]</ref>. In this work, we formulate such an underlying structure of images with a hierarchical data-generating process <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b61">62]</ref> (Figure <ref type="figure" target="#fig_2">2</ref>). Under this formulation, we reveal the underpinning principle of MAE and provide identifiability guarantees. In particular, we show that through masking-reconstruction, MAE learns the long-range statistical dependencies within the image, which renders it capable of extracting high-level semantic representations.</p><p>Formally, the generating process is defined with a graph structure G := (V, E) where E is the set of all directed edges and V := (X, Z) comprises all observable variables X := {x 1 , . . . , x m } (i.e., all pixels) and all latent variables Z := {z 1 , . . . , z n }. Each variable x i or z j represents a multidimensional vector. <ref type="foot" target="#foot_0">1</ref> The hierarchical latent structure G fulfills the following assumption: Assumption 1. (Data-generating process): There is no direct edge between any two observables: ∀x i , x j ∈ X , (x i , x j ) / ∈ E and (x j , x i ) / ∈ E. Each variable is generated by its parents in a directed acyclic graph (DAG) according to:</p><formula xml:id="formula_0">z i = g zi (Pa(z i ), ε i ), x j = g xj (Pa(x j ), ε j ),<label>(1)</label></formula><p>where g zi and g xj are invertible functions, ε i denotes exogenous random variables, and Pa(•) denotes the parents of a certain node.</p><p>The invertible data-generating-module assumption (g i and g j being invertible) is adopted from prior work identifying latent variables in deep generative models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b57">58]</ref>. We make the following remarks on the hierarchical generating process. First, we note that we impose minimal constraints on the graph structure among the latent variables (i.e., the connectivity among latent variables z); therefore, the hierarchical model class is generic and encompasses all possible DAG structures over latent variables (Figure <ref type="figure" target="#fig_2">2</ref>). Next, we interpret the latent variables z as information related to semantic/content information, such as the shape and contour in the image, whereas the exogenous variables ε injected in each layer represent nuanced information, such as the texture and contrast of the image. Each structural function g i mixes the two sources of information and generates a more low-level variable until pixels x. Lastly, for the upcoming theoretical results, as long as the data-generating process conforms to the hierarchical graph assumption, our theory holds, and the insights do not rely on the knowledge of a specific graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Masked Autoencoders</head><p>As a canonical method of masking-reconstruction learning, MAE <ref type="bibr" target="#b21">[22]</ref> randomly masks a subset of pixel patches in the original image and then reconstructs the masked patches from the encoded representation of the visible part. More formally, we formulate the MAE training as follows.</p><p>Mask sampling: random masks m are sampled from a distribution p m which is parameterized by the masking ratio r (i.e., the ratio between the number of masked pixels and the number of all pixels) and patch size s (i.e., the size of the minimal masking unit).</p><p>MAE encoding: E m c (x m c ) maps the unmasked part x m c to a latent representation ĉ 2 , where m c denotes the complement of the mask index set m and is passed to the encoder as positional embeddings to indicate the positions of the visible patches.</p><p>MAE decoding: D m (ĉ, ŝm ) reconstructs the masked image x m from the estimated latent variable ĉ (i.e., the encoder output), and the auxiliary information ŝm embodying positional embeddings and [MASK] token which are fed to the decoder in MAE. Although ŝm is deterministic in MAE implementation, we view it as a random variable in our analysis.</p><p>With the notation above, the MAE training objective can be expressed as follows:</p><formula xml:id="formula_1">L(E, D) := E m,x,ŝm ∥Dm (Emc (xmc ), ŝm) -xm∥ 2 . (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Identifiability Theory</head><p>Building upon the formalization above, we show in Theorem 1 that each random mask m would induce a specific (sub)set of latent variables that fully captures the statistical dependency between the masked part and the visible part. We denote this relationship as c ⊂ Z where c is the subset of the latent variable set Z.</p><p>Theorem 1. (Locating the shared information c): In a hierarchical latent variable structure G, for each specific mask m, there exists a corresponding minimal set of latent variables c such that the generating process of x can be expressed as in Figure <ref type="figure" target="#fig_1">3</ref> where the following conditions are satisfied: Such c and the corresponding s m are unique and can be located from the hierarchical structure by executing Algorithm 1. Furthermore, s m c can be found through Algorithm 2.</p><formula xml:id="formula_2">x m x m c s m s m c c</formula><p>The proof, Algorithm 1, and Algorithm 2 can be found in Appendix A. We note that although the minimal c and its corresponding s m are unique for a given mask m, there is no unique s m c in general. Algorithm 2 returns one such instance.</p><p>Theorem 1 states that for each mask m, there exists a corresponding c that represents all the information contained in the visible part x m c that is conducive to reconstructing the masked part x m . Algorithm 1 can locate such c in the hierarchy and directly characterizes the impact of masking on the property of c.</p><p>Next, in Theorem 2, we show that MAE learning objective (Equation <ref type="formula" target="#formula_13">2</ref>) estimates c specified in Theorem 1, and MAE attains a form of identifiability of c. We first lay out the assumptions: In the following, we discuss our assumptions and results. The proof can be found in Appendix B.</p><p>Assumption interpretation. Assumption 1 follows prior work identifying latent variables in deep generative models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b57">58]</ref> to ensure that latent variables are recoverable from pixels. Assumption 2 requires the MAE encoder E m c to be part of an invertible function output -this is mild and allows the encoder to be more flexible than invertible functions. The decoder D m (ĉ, ŝm ) is assumed to be locally invertible in ĉ almost surely, allowing for a broader class than invertible functions, e.g., nondegenerate polynomials. The joint invertibility of (D m , gm c ) ensures no information loss during the estimation process.</p><p>How does MAE work? Theorem 2 states that the MAE objective (Equation <ref type="formula" target="#formula_13">2</ref>) essentially serves to estimate the shared variable c and is able to restore all information in c. Therefore, the efficacy of MAE stems from its ability to extract high-level semantic representations from low-level features like image pixels. Moreover, our theory indicates the possibility of fully identifying a latent hierarchical structure via properly designed self-supervised objectives, opening up research avenues for future work.</p><p>Takeaway: MAE provably recovers high-level representations from low-level features like pixels.</p><p>How does masking influence the learned representation? Theorem 1 establishes a direct connection between the mask m and the shared information c, which is further connected to the MAE estimate ĉ in Theorem 2. We can observe that conservative masking with overly small masking ratios and masking patch sizes inevitably leads to low-level latent variables. To see this, in Figure <ref type="figure" target="#fig_3">4a</ref>, the mask is not large enough to cover all observable descendants of a desirable high-level variable z 1 , thus following Algorithm 1 a low-level variable z 3 will mix in ĉ, preventing the model from learning z 1 . This insight highlights the necessity of nontrivial masking ratios and patch sizes and resonates with the empirical observations in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b62">63]</ref>.</p><p>Surprisingly, the above reasoning can be applied to the case with extremely aggressive masking: in Figure <ref type="figure" target="#fig_3">4b</ref> lowlevel latent variables z 6 will be learned by MAE when the visible part is too small to cover all observable descendants of a desirable high-level variable z 2 . Thus, the learned representation does not become monotonically more high-level with increasing masking aggressiveness -overly aggressive masking also gives rise to low-level representations. This insight echoes the empirical finding in <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b62">63]</ref> where the extremely large masking degrades the performance of highlevel downstream tasks like classification <ref type="bibr" target="#b62">[63]</ref> but yields relatively low-level representations like the object loca- We label the masked pixels with x. We locate the MAE learned latent variables with Algorithm 1 and label them with blue. We can observe that extremely low (left) and high (middle) masking intensities lead to low-level representations, whereas the desirable masking intensity that yields a high-level representation lies in the intermediate masking aggressiveness.</p><formula xml:id="formula_3">z 1 z 2 z 3 z 4 z 5 z 6 x 1 x 2 x 3 x 4 x 5 x 6 (a) Conservative mask z 1 z 2 z 3 z 4 z 5 z 6 x 1 x 2 x 3 x 4 x 5 x 6 (b) Aggressive mask z 1 z 2 z 3 z 4 z 5 z 6 x 1 x 2 x 3 x 4 x 5 x 6 (c) Ideal mask</formula><p>tions/scales in the image <ref type="bibr" target="#b60">[61]</ref>. In Section 3, we present empirical evidence to verify our theoretical insights.</p><p>Takeaway: (1) MAE under different masking intensities learns representations of different abstraction levels;</p><p>(2) Learning high-level representations is very hard with extreme masking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is current MAE optimal for representation learning?</head><p>As reflected in the discussion above, although MAE offers the flexibility of tuning the masking scheme to learn representations of various levels, it is inherently challenging to learn high-level representations by random masking without prior knowledge of the latent structure. In contrast, contrastive learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b63">64]</ref> actively leverages the prior knowledge encoded in data augmentations to extract the augmentation-invariant latent variables <ref type="bibr" target="#b57">[58]</ref> which correspond to the high-level latent variables in our hierarchical model. Our theory suggests an explanation for why representations learned by contrastive learning are superior to those of MAE on high-level tasks like linear-probing classification.</p><p>Takeaway: Learning high-level representations can be challenging for random masking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We conduct five sets of experiments and then provide insights into possible empirical improvements over MAE. We investigate the following question: how does the masking aggressiveness influence the representation? To this end, we pretrain MAE using different masking ratios and making patch sizes, and then conduct the following evaluations: 1) measuring structure-level and pixel-level similarities between the reconstructed and the original images; 2) visualizing self-attentions to understand what is learned; 3) performing linear probing on ImageNet-1K (IN1K) and different ImageNet variants; 4) measuring the shape bias <ref type="bibr" target="#b18">[19]</ref> which estimates how much a network leverages high-level shape information over low-level texture information; and Pretraining overview. We conduct pretraining on IN1K using the MAE pipeline <ref type="bibr" target="#b21">[22]</ref>, with ViT-Base as the backbone of our study. We conduct two sets of pretraining: 1) fixing patch size at 16 and varying the masking ratios from {0.1, 0.25, 0.5, 0.75, 0.9}. Larger masking ratios suggest larger portions of pixels being masked, i.e., 0.9 suggests 90% of pixels being randomly masked for the encoder. 2) Fix the masking ratio at 0.75 and vary the patch size from {8, 16, 32}. To decouple the patch size for masking images and the patch size hyperparameter in the Vision Transformer, we adopt the implementation from <ref type="bibr" target="#b27">[28]</ref>. The patch size studied in this paper refers to the minimal masking unit size, and the hyperparameter of the ViT patch size remains fixed at 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Reconstructing High-level or Low-level Representations</head><p>Setup. We begin our study by evaluating the high-level structural and low-level pixel-wise similarities between the reconstructed images from MAE and the original inputs. We choose two metrics for high-level similarities and two metrics for low-level similarities. If the structural similarities are high, MAE captures more perceivable structural semantics from the input. The two high-level similarities are structural similarity index measure <ref type="bibr" target="#b59">[60]</ref> (SSIM) and feature similarity index measure <ref type="bibr" target="#b64">[65]</ref> (FSIM). Both metrics consider the change of perceptions in structural information <ref type="bibr" target="#b32">[33]</ref>. SSIM considers the normalized mean value of the structural similarity between the original and reconstructed images, and FSIM considers the normalized mean value of the feature similarity between the two images. A higher SSIM or a higher FSIM suggests a better reconstruction of high-level information (structural or feature-wise). On the other hand, if the pixel-level similarity between recon-structed images and the original input is high, then MAE is deemed to capture the low-level information about the input better. The two low-level metrics are the mean squared error (MSE), which is the squared differences between the original and reconstructed images in the pixel space, and the peak signal-to-noise ratio (PSNR), which measures the ratio between the power of the maximum possible pixel value and the power of corruption noise. A lower MSE or a higher PSNR suggests a better reconstruction at the pixel level. Note that a very low MSE or a very high PSNR may also suggest that the model captures high-level information well. All four metrics are full reference, meaning the assessment is based on comparing original and reconstructed images rather than the reconstructed output. We introduce the high-level and low-level metrics below and perform the reconstructions on the IN1K evaluation set. The full details and comparisons of the four metrics can be found in <ref type="bibr" target="#b50">[51]</ref>.</p><p>Evaluation of image reconstructions. We include the results in Figure <ref type="figure" target="#fig_4">5</ref>. We plot the negative of the MSE to show a consistent trend with PSNR, so higher means better lowlevel reconstruction. From the first row, varying masking ratios from 0.1 to 0.75, higher masking ratios produce reconstructions with higher structural information similarities with the original image (higher SSIM and FSIM), but the model trained with the extremely high ratio 0.9 captures more low-level information (higher PSNR and higher negative MSE). On the other hand, lower masking ratios tend to reconstruct images that capture low-level information better. From the second row, larger patch sizes produce image reconstructions that capture high-level similarities better, while smaller patch sizes have low-level metrics. The empirical observations validate our insight from Section 2.3: higher masking ratios and patch sizes capture high-level structural information better, but extreme masking ratios (both low and high) capture less high-level and more low-level information.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention Analysis</head><p>In this section, we measure the property of the learned representations of MAE by probing the attention heads. We would like to understand visually how masking ratios and patch sizes influence MAE's capacity to capture objectcentric semantics. We provide two types of visualization: self-attention on the [CLS] token and self-attention on an object-related token.</p><p>[CLS] has been considered a compact token to represent the whole image for downstream tasks, although recent work <ref type="bibr" target="#b21">[22]</ref> suggests that the average pooling of all tokens may achieve slightly better results. Therefore, we also provide an analysis of object-related tokens to evaluate if MAE can contextualize object informa-tion across tokens.</p><p>We plot examples of self-attention of the [CLS] token in Figure <ref type="figure" target="#fig_5">6</ref> and self-attention of non-CLS tokens related to the object in Figure <ref type="figure" target="#fig_6">7</ref>. From the visualizations, as the masking ratio increases from 10% to 90%, the model is increasingly more able to grasp succinct information about the holistic objects rather than only focusing on the regions around the chosen token. However, extreme ratio 0.9 contains more low-level information and background information and cannot capture most of the remaining tokens related to objects (e.g., the dog, cat, and bee images in Figure <ref type="figure" target="#fig_6">7</ref>). Extremely low masking ratios such as 0.1 capture both object-related and background tokens. Similarly, extreme masking ratios contextualize over other object-related tokens worse than intermediate masking ratios. We include the visualizations for patch sizes in Appendix. We observe that models trained with larger patch sizes better capture high-level information, but extreme patch size hurts, which validates our theoretical insight that moderate masking ratios and patch sizes are critical for MAE to learn succinct and comprehensive object information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Representation Linear Separability</head><p>T-SNE embedding visualizations. To gain a visual understanding of how masking ratios and patch sizes influence the representation structure, we visualize T-SNE <ref type="bibr" target="#b56">[57]</ref> embeddings of different models. We randomly select ten classes from ImageNet. The results are shown in Figure <ref type="figure" target="#fig_7">8</ref>. From 0.1 to 0.75, a larger masking ratio consistently produces a more linearly separable representation, while the linear separabilities of representations with masking ratios 0.75 and 0.9 are visually similar. For different patch sizes, the embeddings are more separated as the patch sizes grow. Non-extreme masking ratios and larger patch sizes generate more linearly separable embeddings.</p><p>Linear probing on IN1K. We use linear probing to test how linearly separable the features are in the learned MAE representation. We show the linear probing results in Table <ref type="table" target="#tab_1">1</ref> in row 1N1K. For different masking ratios, similar to the observation in <ref type="bibr" target="#b21">[22]</ref>, the accuracy increases steadily until the masking ratio reaches the sweet point of 0.75. An extremely large masking ratio (0.9) hurts performance. For different patch sizes, which are not shown in <ref type="bibr" target="#b21">[22]</ref>, we observe that the accuracy increases first from 8 to 16, then decreases significantly when the patch size is 32. From the results, higher masking ratios and larger patch sizes perform better at linear probing than lower masking ratios, but extreme masking hurts linear probing.</p><p>Robustness evaluation on ImageNet variants. We evaluate the robustness of the MAE models on different variants of ImageNet validation datasets, or object detection datasets  that share similar class information with ImageNet-1K: ImageNet-v2 (INV2) <ref type="bibr" target="#b51">[52]</ref>, ObjectNet (OJN) <ref type="bibr" target="#b3">[4]</ref>, ImageNet-Adversarial (IN-A) <ref type="bibr" target="#b24">[25]</ref>, ImageNet-Rendition <ref type="bibr" target="#b3">[4]</ref>, and ImageNet-Sketch (IN-S) <ref type="bibr" target="#b58">[59]</ref>. These datasets share similar semantics and labels with ImageNet but are under different data distributions. The MAE models are first trained in a supervised fashion on IN1K for linear probing, and inference is run on the evaluation sets without any training. Table <ref type="table" target="#tab_1">1</ref> shows for all evaluation datasets, a reasonably large masking ratio (i.e., 0.75) achieves better robustness than smaller (i.e., 0.25) masking ratios, although extremely large (0.9) or small (0.1) masking ratios hurt the performance. For patch sizes, larger patch sizes yield better robustness evaluations on IN-v2, OJN, IN-R, and IN-S. Non-extreme masking ratios and large patch sizes have stronger robustness performances than extreme masking ratios or patch sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Shape Bias</head><p>Texture vs. shape bias. Next, we analyze to what extent different MAE models rely on high-level vs. low-level information. We follow the analysis in <ref type="bibr" target="#b18">[19]</ref>, where the authors study whether a model leverages more low-level textures than high-level shapes for classification. As shown in Table <ref type="table" target="#tab_2">2</ref>, intermediate masking ratios (i.e., 0.25, 0.5, and 0.75) show a high level of shape bias, suggesting that the corresponding models exploit more high-level shape information. In contrast, extreme masking ratios (i.e., 0.1 and 0.9) leverage more low-level textures. This suggests that extreme masking schemes make it more difficult to capture high-level shapes for MAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Transfer Learning</head><p>Next, we evaluate the quality of MAE models on different downstream tasks. Specifically, we look at object de-   tection and segmentation on the COCO dataset <ref type="bibr" target="#b42">[43]</ref>, which requires a strong semantic understanding of the scenes. We finetune Mask R-CNN <ref type="bibr" target="#b23">[24]</ref> end-to-end using MAEpretrained ViT weights. Following the practice in <ref type="bibr" target="#b21">[22]</ref>, we adapt the ViT backbone to make it compatible with FPN <ref type="bibr" target="#b41">[42]</ref>. In Table <ref type="table" target="#tab_3">3</ref>, we report box AP for object detection and mask AP for instance segmentation. We reduce the number of epochs to 45 due to computational constraints. We observe that the 0.75 masking ratio yields the best detection and segmentation average precision, suggesting that the masking ratio 0.75 generates representation with the best semantic understanding. The extremely high masking ratio of 0.9 and a low masking ratio of 0.1 hurt the performance. Results of different patch size experiments are included in Appendix. The results suggest that higher, but not extreme, masking ratios generate the best representation of object detection and segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Potential Algorithmic Improvements</head><p>Lastly, we discuss empirical suggestions based on our results that could benefit the performance of MAE.</p><p>First, as discussed in Section 2, when reconstructing the masked pixels near the boundary between the masked and unmasked regions, the model uses nearby visible pixels to interpolate, therefore capturing low-level pixel information. If high-level representation is desired for downstream tasks, the boundary pixels may be ignored when calculating the objective function.</p><p>Next, in light of the limitation of random masking in Section 2, one may leverage the latent structure of the underlying data-generating process for masking designs, which can serve as a more principled approach than recent work that exploits auxiliary information for masking <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>To this end, one may take advantage of the recent development of causal discovery <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b61">62]</ref> to identify the latent structure.</p><p>Lastly, if low-level information is preferable for downstream tasks, an extremely high masking ratio can retain such information and is more computationally efficient than its low masking ratio counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related work 4.1. Masked Autoencoders</head><p>Masked image modeling (MIM) <ref type="bibr">[1-3, 11, 13, 22, 41, 63, 69]</ref> has been gaining momentum recently due to their sotaof-the-art performances over many downstream tasks. The pretraining objective is simple in its basic form: the model is tasked to predict the masked-out image pixels with the information of the unmasked part. Despite the simplicity of the task, many intriguing properties have been observed on MIM that escape rigorous analysis. For instance, small masking ratios and masking patch sizes are empirically shown detrimental to downstream tasks like classification <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref>. It is hypothesized that aggressive masking forces to model to leverage more global information, rather than local interpolation <ref type="bibr" target="#b21">[22]</ref>. However, whether such intuition is theoretically justifiable remains elusive. In this work, we provide theoretical verification of such intuitions and further derive insights into MAE's empirical behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Theoretical Understanding of MAE</head><p>Despite the prominent success of MAE, only a limited number of papers are dedicated to understanding its underlying mechanism in a principled manner <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b65">66]</ref>. Lee et al. <ref type="bibr" target="#b38">[39]</ref> establish the connection between the inpainting pretraining task and downstream tasks by assuming that the downstream task target captures the statistical dependency between the visible part and the masked part in the inpainting. Under this assumption, they show that the sampling complexity of the downstream task can be largely reduced by pretraining. Cao et al. <ref type="bibr" target="#b7">[8]</ref> inquire into the interactions between the transformer architecture and the MAE representation, highlighting the critical role of the attention mechanism in the success of MAE. Pan et al. <ref type="bibr" target="#b47">[48]</ref> make a multi-view assumption on the samples, showing that MAE can extract class-relevant semantics with shallow convolutional models. Zhang et al. <ref type="bibr" target="#b65">[66]</ref> study masking through the data-augmentation perspective and employ the augmentation graph <ref type="bibr" target="#b20">[21]</ref> to illustrate the impact of masking on downstream task performance. In contrast, our work employs the hierarchical latent variable model, which lets us directly examine the relationship between the masking operation and the learned representations. Also, our theoretical guarantee is on the statistical identifiability of the true data-generating process rather than the statistical/optimization complexities as in most prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Identifiability Guarantees for Nonlinear Latent-variable Models</head><p>In unsupervised learning, identifiability means latent variables involved in the underlying data-generating process can be estimated from observational data. This is critical to tasks like feature disentanglement <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b61">62]</ref> in the image generation community. However, principled disentanglement in the non-linear regime is challenging and even proved impossible without additional assumptions on the data-generating process <ref type="bibr" target="#b43">[44]</ref>. Recent advances in independent component analysis (ICA) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31]</ref> obtain identifiability in the non-linear regime by imposing additional constraints on either the latent variable distribution or the function class variables <ref type="bibr">[20, 32, 36-38, 45, 54, 58, 68]</ref>. Most relevant to ours are the identifiability theories in <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b57">58]</ref> in which similar latent causal models (Figure <ref type="figure" target="#fig_1">3</ref>) are studied. Specifically, our model allows the generating functions g m ̸ = g m c to be distinct (cf. identical functions assumed in <ref type="bibr" target="#b57">[58]</ref>) and statistical dependence between c and s m c (cf. independence assumed in <ref type="bibr" target="#b45">[46]</ref>). Additionally, both works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b57">58]</ref> focus on contrastive learning with data augmentation, while our subject is MAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we formulate the data-generating process as a hierarchical latent variable model and provide guarantees that MAE can identify the true variables in such a hierarchical latent model. We then show how different masking ratios and patch sizes determine the set of true latent variables to be recovered, which influences the representation abstractions learned in MAE. Empirically, we show that non-extreme masking ratios or patch sizes often capture succinct and robust high-level information, while extreme masking ratios capture more low-level information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof for Theorem 1</head><p>In this section, we provide the proof for Theorem 1.</p><p>Theorem 1. (Locating the shared information c): In a hierarchical latent variable structure G, for each specific mask m, there exists a corresponding minimal set of latent variables c such that the generating process of x can be expressed as in Figure <ref type="figure" target="#fig_1">3</ref> where the following conditions are satisfied:   for p ∈ P do 7:</p><formula xml:id="formula_4">for p ′ ∈ LocateParents(p) do 8:</formula><p>if p ′ is exogenous then</p><formula xml:id="formula_5">9: S m c ← S m c ∪ {p ′ } 10: else if p ′ ∈ C then 11: S m c ← S m c ∪ (LocateParents(p) \ {p ′ }) 12:</formula><p>else 13:</p><formula xml:id="formula_6">P ′ ← P ′ ∪ {p ′ } 14: P ← P ′ return S m c</formula><p>Proof. We will show that Algorithm 1 returns the minimal set of variables that satisfy all conditions in Theorem 1, which implies its existence. We will then argue that such C is unique for a specific mask m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Condition 1:</head><p>We first discuss the invertibility of gx m . Due to the invertibility assumption of the generating process, each backtrack step in Algorithm 1 is invertible (lossless). Thus, before the pruning stage, the mapping between (C, Sm) and Xm is invertible, as the information of Xm is either stored in either C or Sm. We now show that the pruning stage does not break this invertibility. To see this, we note that for each c that is removed in the pruning stage, there exists c ′ ∈ C on the directed path from c to Xmc (per Algorithm 1). Therefore, c is a parent/ancestor of c ′ and can thus be retrieved by backtracking from c ′ thanks to the invertibility of the generating process. Therefore, the mapping between (C, Sm) and Xm is invertible.</p><p>We now address the invertibility of gx m c , i.e., the mapping between (C, Smc ) and Xmc . We observe that a similar argument applies: Algorithm 2 dictates that the latent variables from the backtracking from Xmc are either stored in either C or Smc . It follows that gx m c is invertible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Condition 2:</head><p>We show that (c, sm, smc ) returned by Algorithm 1 and Algorithm 2 satisfies Condition 2 by contradiction. We suppose that sm ̸ ⊥ ⊥ (c, smc ). Then it implied that ∃d ∈ (c, smc ), ∃ε ∈ sm, such that d ∈ Descendants(ε). More precisely, it followed that there was a directed path that started from ε and ended at d, and a child of ε, denoted as δ, was located on this path. If d ̸ ∈ Descendants(ε), there would be no directed paths from ε to d and thus at least one V-structure would sit on each path between ε and d that blocked the path. According to Algorithm 1, as ε ∈ sm, it implied that δ ̸ ∈ c and δ ̸ ∈ Ancestors(xm) ∩ Ancestors(xmc ).</p><p>We first investigate the case where d ∈ c, i.e., sm ̸ ⊥ ⊥ c. The fact that d ∈ c implied that d ∈ Ancestors(xm) ∩ Ancestors(xmc ) which further implied that δ ∈ Ancestors(xm) ∩ Ancestors(xmc ) as δ was an ancestor of d. Therefore, we have arrived at a contraction to the observation that δ ∈ Ancestors(xm)∩ Ancestors(xmc ).</p><p>We now discuss the scenario where d ∈ smc . By design, Algorithm 2 ensures that smc contains two types of latent variables, exogenous variables and a spouse of latent variables in c. As sm consists solely of exogenous variables and exogenous variables are independent mutually, it could only be the case that d was a spouse of a latent variable in c. By Algorithm 1, there would be a directed path from δ to xm. Also, Algorithm 2 ensured that d lied on a path directed to xmc . As there existed a directed path from δ to d, there must exist a directed path from δ to xmc . Therefore, δ ∈ Ancestors(xm) ∩ Ancestors(xmc ) which contradicts the fact established above.</p><p>Therefore, these contradiction implies that sm ⊥ ⊥ (c, smc ).</p><p>So far, we have shown that Algorithm 1 and Algorithm 2 yield (c, sm, smc ) that fulfills the conditions of Figure <ref type="figure" target="#fig_1">3</ref>. In the following, we show that (c, sm) is the minimal solution and is unique.</p><p>Uniqueness and minimality of (c, s m ): We now reason about that given the mask and the hierarchical structure, (c, sm) returned by Algorithm 1 is the set of minimal dimensionality that can fulfill the conditions, and such a minimal set is unique.</p><p>By construction, Algorithm 1 ensures that for each c ∈ C there exists an undirected path that is made up of a directed path from c to the masked variable xm and a directed path from c to the unmasked variable xmc and no other c ′ ∈ C sits on this entire undirected path. To see this, there must exist a directed path from c to xm without any other c ′ ∈ C on it, otherwise c would not be placed in C in Algorithm 1. In addition, the pruning stage of Algorithm 1 mandates that there must exist xmc such that the path from c to xmc does not contain other c ′ ∈ C. We note that c chosen by Algorithm 1 is the variable with the smallest possible dimension to block such a path, as it resides on the highest level compared to other variables on the path and the variable dimension increases monotonically along directed paths.</p><p>Therefore, the choice of each c is minimal, and such a choice is unique. As Sm is the set of exogenous variables necessary for C to restore Xm, the selection of Sm is also unique. Hence, we conclude that the (C, Sm) returned by Algorithm 1 is the minimal choice and is unique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Identifiability proof</head><p>In this section, we present the proof for Theorem 2. We first give a general identifiability theory (i.e., Theorem 3) for the generating process in Figure <ref type="figure" target="#fig_1">3</ref> and then make the connection to the proof of Theorem 2.</p><p>Theorem 3. The generating process in Figure <ref type="figure" target="#fig_1">3</ref> is defined as fol-lows:</p><formula xml:id="formula_7">[v1, v2] = g(c, s1, s2),<label>(3)</label></formula><formula xml:id="formula_8">v1 = g1(c, s1),<label>(4)</label></formula><formula xml:id="formula_9">v2 = g2(c, s2),<label>(5)</label></formula><p>where c ∈ C ⊂ R dc , s1 ∈ S ⊂ R ds 1 , and s2 ∈ S2 ⊂ R ds 2 . Both g1 and g2 are smooth and have non-singular Jacobian matrices almost anywhere, and g is invertible.</p><p>If ĝ1 : Z → V1 and ĝ2 : Z → V2 assume the generating process of the true model (g1, g2) and match the joint distribution pv 1 ,v 2 , then there is a one-to-one mapping between the estimate ĉ and the ground truth c over C × S × S, that is, c is blockidentifiable.</p><p>Proof. For (v1, v2) ∼ pv 1 ,v 2 , because of the matched joint distribution, we have the following relations between the true variables (c, s1, s2) and the estimated ones (ĉ, ŝ1, ŝ2):</p><formula xml:id="formula_10">v1 = g1(c, s1) = ĝ1( ĉ, ŝ1),<label>(6)</label></formula><formula xml:id="formula_11">v2 = g2(c, s2) = ĝ2( ĉ, ŝ2),<label>(7)</label></formula><formula xml:id="formula_12">(ĉ, ŝ1, ŝ2) = ĝ-1 (v1, v2) = ĝ-1 (g(c, s1, s2)) := h(c, s1, s2),<label>(8)</label></formula><p>where we define the smooth and invertible function h := ĝ-1 • g that transforms the true variables (c, s1, s2) to estimates (ĉ, ŝ1, ŝ2).</p><p>Plugging Equation <ref type="formula" target="#formula_12">8</ref>into Equation 6 yields the following:</p><formula xml:id="formula_13">g1(c, s1) = ĝ1(hc,s 1 (c, s1<label>, s2)).</label></formula><p>For i ∈ {1, . . . , dv 1 } and (j ∈ {1, . . . , ds 2 }), taking partial derivative of the i-th dimension of both sides w.r.t. s2,j:</p><formula xml:id="formula_14">0 = ∂g1,i(c, s1) ∂s2,j = ∂ĝ1,i(hc,s 1 (c, s1, s2)) ∂s2,j .</formula><p>The equation equals zero because there is no s2,j in the left-hand side of the equation. Expanding the derivative on the right-hand side gives:</p><formula xml:id="formula_15">k∈{1,...,dc+ds 1 } ∂ĝ1,i ∂h (c,s 1 ),k • ∂h (c,s1),k ∂s2,j (c, s1, s2) = 0<label>(9)</label></formula><p>For (ĉ, ŝ1) ∈ C × S \ E1 where E1 denotes some subset with zero measure, there are at least dc + ds 1 values of i for which vectors [ ∂ ĝ1,i ∂h (c,s 1 ),1 (ĉ, ŝ1), . . . , ∂ ĝ1,i ∂h (c,s 1 ),dc +ds 1 (ĉ, ŝ1)] are linearly independent, which is equivalent to the non-singular Jacobian matrix condition. Therefore, the (dc +ds 1 )×(dc +ds 1 ) linear system is invertible and the solution states that:</p><formula xml:id="formula_16">∂h (c,s 1 ),k ∂s2,j (c, s1, s2) = 0,</formula><p>for any k ∈ {1, . . . , dc + ds 1 }, j ∈ {1, . . . , ds 2 }, and (ĉ, ŝ1) ∈ C × S \ E1. Therefore, we have shown that hc,s 1 , i.e. (ĉ, ŝ1), does not depend on s2.</p><p>Applying the same reasoning to hc,s 2 , we can obtain that hc,s 2 , i.e. (ĉ, ŝ2) does not depend on s1 on C × S.</p><p>Thus, for (ĉ, ŝ1, ŝ2) ∈ C × S × S, we can observe that ĉ does not depend on s1 and s2, that is, ĉ = hc(c).</p><p>Notice that in all procedures above, the roles of the true quantities (c, s1, s2, g, g1, g2) and the estimated quantities (ĉ, ŝ1, ŝ2, ĝ, ĝ1, ĝ2) are symmetric. Therefore, we can switch the two sets of quantities and derive the relation: for (c, s1, s2) ∈ (C × S × S), c does not depend on ŝ1 and ŝ2, that is, c = h ′ c (ĉ). In sum, we have shown that on (C × S × S), there is a one-toone mapping between c and ĉ.</p><p>We now show that Theorem 2 follows directly from Theorem 3. Proof. We invoke Theorem 3 and establish the connection between the MAE training and the estimation model in Theorem 3. In particular, we show that under Assumption 2, any solution produced by the MAE objective satisfies the conditions in Theorem 3 and consequently is equipped with the identifiability guarantee.</p><p>We establish the correspondence between the MAE configuration and the estimation models in Theorem 3:</p><formula xml:id="formula_17">• v1 ← xm; • v2 ← xmc ; • ĝ1 ← Dm(•, ŝm); • ĝ2 ← gm c , where Emc (•) = [g -1</formula><p>m c (•)] 1:dc . We can observe that the minimizer of MAE satisfies the conditions specified in Theorem 3. This is because for the optimal solution Emc of the MAE objective, we can always construct a gm c , which, together with Dm, matches the joint distribution px m ,x m c and shares ĉ, as stipulated in Theorem 3. Thus, as shown in Theorem 3, there exists a one-to-one mapping between the MAE estimate ĉ := Emc (xmc ) and the true variable c, which concludes our proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Setup</head><p>In this section, we provide the details of the experimental setups for our empirical results. Checkpoints and some codes are in <ref type="url" target="https://github.com/martinmamql/mae_understand">https://github.com/martinmamql/mae_ understand</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Masked Autoencoder</head><p>Masked Autoencoder (MAE) is an auto-encoding approach based on Vision Transformers (ViT) <ref type="bibr" target="#b16">[17]</ref>. It consists of five steps: masking, encoding, unmasking, decoding, and reconstruction. First, an image is divided into non-overlapping patches. Then MAE samples a subset of patches and discards the remaining patches. MAE uses a hyper-parameter, masking ratio, to determine the percentage of patches to discard. For instance, if the masking ratio is 75%, 3  4 of the patches in an image will be discarded, and only 1  4 of the patches will be fed into the encoder. The sampling of patches follows a uniform distribution. Next, a ViT encoder first embeds patches using a linear projection with positional embeddings and then uses the processed embeddings to feed into transformer blocks. For decoding, MAE first re-arranges the encoded embeddings from the visible patches according to their corresponding positions in the original image and then uses a shared learned mask token to fill in the patches that are masked. Essentially, this means the input of the decoder is a combination of encoded visible patches and the mask tokens, where the positions of the mask tokens are the masked patches in the original image. The decoder is another lightweight ViT, and it processes the decoder input through transformer blocks. Lastly, the last layer of the decoder linearly projects output patches to pixels, and the pixel output is reshaped to form a reconstruction of the original image. The objective function is the mean squared error between the reconstruction and the original image. MAE has thrived because of its simple design and strong empirical performance.</p><p>In the main text, inspired by a follow-up work of MAE <ref type="bibr" target="#b27">[28]</ref>, we study the effect of masking by decoupling the patch size for masking images and the patch size hyperparameter in the ViT. Particularly, in the main text, we only vary the masking patch size and fix the ViT patch size at 8. Nevertheless, the original MAE <ref type="bibr" target="#b21">[22]</ref> does not decouple the two patch sizes. Therefore, for the reference of readers, in Appendix, we provide some analysis and results produced based on the patch size design from the original MAE <ref type="bibr" target="#b21">[22]</ref>, where the masking patch size and the ViT patch size are equal. We study three patch sizes: {8, 16, 32}. The experimental setup in <ref type="bibr" target="#b27">[28]</ref> and the setup in <ref type="bibr" target="#b21">[22]</ref> are interchangeable except for whether the patch size for the Vision Transformer varies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Pretraining and Linear Probing</head><p>For pretraining MAE under different masking ratios or patch sizes, we leverage the Tensor Processing Unit (TPU) from Google Cloud. We train separate MAE models for each (masking ratio, patch size) pair, and each pretrained MAE corresponds to a unique masking ratio and patch size. We train all MAEs for 800 epochs. Training time varies, with the shortest (patch size = 32) taking 18 hours on a TPU v3-128 Pod, and the longest (patch size = 8) taking 40 hours on a TPU v3-128 pod. The architecture follows the exact implementation from the original MAE paper <ref type="bibr" target="#b21">[22]</ref>, without any hyper-parameter tuning except masking ratio and patch size, which we study in this paper. Details of augmentation, initialization, and base learning rate scaling can be found in the Appendix section of <ref type="bibr" target="#b21">[22]</ref>, all of which we follow.</p><p>After pretraining, we also follow the original MAE work to use linear probing to evaluate the representation quality. After pretraining, we remove the projection layers and add a supervised learning classifier on frozen features of MAE encoders. The decoders are discarded during linear probing. Other details of linear probing can be found in the Appendix section of <ref type="bibr" target="#b21">[22]</ref>. We use the same hyper-parameters of linear probing as in <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Reconstructing high-level or low-level representations</head><p>To perform reconstruction, we use both the encoder and the decoder from the pretrained MAEs. All samples from ImageNet-1K are passed through the encoder without any masking, and the decoder reconstructs images in the original input space. Since no masking is applied, no masking token is applied to the input of the decoder. We use the reconstructed images and the original images In Fig. <ref type="figure" target="#fig_14">9</ref>, we show the reconstruction analysis using the original patch size design in MAE. Similar to the result in the main text, higher patch sizes produce image reconstructions capturing highlevel similarities better, while low patch sizes have reconstructions better on low-level metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Attention Analysis</head><p>We follow the attention heatmap visualization in DINO <ref type="bibr" target="#b9">[10]</ref>, where the chosen token is the [CLS] token or an object-related token. We visualize the self-attention module from the last block of the MAE encoder ViT. Brighter colors suggest larger attention weights. For easier visualization, attentions below a threshold of activation scores are not shown. We use the same threshold as <ref type="bibr" target="#b9">[10]</ref>. For the self-attention visualization on the [CLS] token, we use an average of all heads in the last layer of the encoder ViT. For the self-attention visualization of the object-related token, we use the first head of the last layer of the encoder ViT, because using the average attention over all heads will result in a heatmap with much higher overall attention scores across pixels, making the visualization hard to interpret. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. Linear separability</head><p>To illustrate the linear separability of different MAEs under varied masking ratios or patch sizes, we sample ten random classes from ImageNet, and then use each MAE encoder to process images in the 10 classes to produce embeddings. We then project embeddings of all samples using PCA to a 50-dimension space before t-SNE, as recommended by <ref type="bibr" target="#b55">[56]</ref>. For t-SNE, we use a perplexity of 20.</p><p>In Fig. <ref type="figure" target="#fig_15">10</ref>, we show the t-SNE plot using the original patch size design in MAE. Similar to the main text, embeddings are more separated in patch sizes 16 and 32 than 8, but differently, there are no significant differences between 16 and 32. Larger patch sizes generate more linearly separable embeddings in this case, although the separability seems indistinguishable for sizes 16 and 32.</p><p>For the robustness evaluation, we evaluate different variants of ImageNet validation datasets: ImageNet-v2 (INV2) <ref type="bibr" target="#b51">[52]</ref>, ImageNet-Adversarial (IN-A) <ref type="bibr" target="#b24">[25]</ref>, ImageNet-Rendition <ref type="bibr" target="#b3">[4]</ref>, and ImageNet-Sketch (IN-S) <ref type="bibr" target="#b58">[59]</ref>. We also include another object classification dataset, ObjectNet (OJN) <ref type="bibr" target="#b3">[4]</ref>. ImageNet-v2 contains three new test sets with 10,000 new images each, sampled a decade after the collection of the original ImageNet dataset, and is independent of existing models to prevent overfitting. Note that for evaluating these datasets, no training is performed; we use the MAE encoders after linear probings, therefore the checkpoints that are pretrained and linear-probed on ImageNet, and evaluate the checkpoints on these validation datasets without any parameter updates.</p><p>In Table <ref type="table" target="#tab_4">4</ref>, we show the robustness analysis using the original patch size design in MAE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6. Shape bias</head><p>The cue-conflict dataset was introduced by <ref type="bibr" target="#b18">[19]</ref> to evaluate how much deep learning models rely on shape information for prediction, which reflects the model's robustness to spurious correlation like textures. This dataset consists of 1280 images synthesized from 160 images of objects and 48 images of textures. The shape accuracy is measured by the fraction of images pre- Table <ref type="table">5</ref>. COCO object detection and segmentation using a ViT Mask R-CNN baseline. We fix the masking ratio at 0.75 to change patch sizes. The patch size refers to the patch size in the original MAE, where the masking patch size and the patch size of ViT are equal.</p><p>dicted correctly by their shape. We directly run the pretrained MAE models with linear probes trained on ImageNet-1K on the cue-conflict dataset to examine the representation resulting from MAE pretraining without any adaptation to the test dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7. Transfer learning</head><p>We use the pretrained MAE ViT encoder as an FPN <ref type="bibr" target="#b41">[42]</ref> backbone in Mask-RCNN <ref type="bibr" target="#b23">[24]</ref>, following <ref type="bibr" target="#b21">[22]</ref>. To do so, <ref type="bibr" target="#b21">[22]</ref> uses a stack of pretrained transformer blocks in MAE to produce feature maps at a single scale; for instance, patch size 16 will produce stride 16 features. Then the features are equally divided, and upsampling or downsampling is applied to create features at different scales. Lastly, the FPN is built on multi-scale features. Below we include the transfer learning results of different patch sizes on COCO object detection and segmentation <ref type="bibr" target="#b42">[43]</ref>. Because different patch sizes in ViT will influence the scale of feature maps in the FPN, we enforce the same combinations of multi-scale features: i.e., stride 4, 8, 16, and 32.</p><p>From Table <ref type="table">5</ref>, we show the transfer learning results of MAE under different patch sizes. Patch size 8 performs the best, and patch size 16 is better than 32. The reason for the better performance at patch size 8 may be due to a smaller batch size used, compared to patch size 16 and 32 (we can only fit batch size 1 for patch size 8 due to the increased number of tokens to process because of a smaller patch size.) We use the same batch size for 32 and 16, and the comparison between the two supports our claim: an extreme masking scheme can hurt the model's capacity to capture high-level information or, in this case, the semantic understanding of the scene.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Masking-reconstruction under a hierarchical generating process. In a hierarchical data-generating process, high-level latent variables (e.g., z 1 ) represent high-level information such as semantics, and low-level latent variables (e.g., [z 2 , z 3 , z 4 ]) represent low-level information such as texture. We show that through proper masking, MAE learns to recover high-level latent variables with identifiability guarantees.</figDesc><graphic coords="1,320.68,219.55,212.63,148.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Information sharing latent models. Here, xm and xmc denote the masked part and the visible part of the image x, respectively. c stands for the maximally shared information between xm and xmc . sm and smc refer to the information specific to xm and xmc respectively. The dashed line indicates the potential existence of statistical dependence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Assumption 2 .</head><label>2</label><figDesc>(MAE model): For any mask m, the MAE decoder D m (ĉ, ŝm ) has a non-singular Jacobian matrix almost anywhere, and there exists an invertible function gm c (•) such that MAE encoder E m c (•) = [g -1 m c (•)] 1:dc where [•] 1:dc denotes the dimensions corresponding to c. Moreover, (D m , gm c ) forms an invertible mapping between (ĉ, ŝm , ŝm c ) and (x m , x m c ) Next, we show MAE identifies the shared information c: Theorem 2. (Identifiability of c): For each mask m, given the dimensions (d c , d sm ) the encoder function E m c (•) recovers all information of c located in Theorem 1, i.e., there exists a one-to-one mapping h, s.t., h(c) = ĉ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The impact of masking on the learned representation.We label the masked pixels with x. We locate the MAE learned latent variables with Algorithm 1 and label them with blue. We can observe that extremely low (left) and high (middle) masking intensities lead to low-level representations, whereas the desirable masking intensity that yields a high-level representation lies in the intermediate masking aggressiveness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Reconstruction evaluation using the validation set without masking, based on two structural-level similarity metrics (SSIM and FSIM) and two pixel-level metrics (PSNR and MSE). We plot negative MSE for easier visualization. Higher SSIM and FSIM indicate high-level information is better captured, while higher PSNR and negative MSE indicates better low-level reconstruction. 5) transfer learning on object detection and segmentation. Details of experiments can be found in Appendix.</figDesc><graphic coords="5,61.24,72.00,470.21,144.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Self-attention of the [CLS] tokens averaged across the heads of the last layer in MAE.</figDesc><graphic coords="6,54.84,71.98,226.82,198.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Self-attention of an object-related token. Chosen tokens are shown in red squares: dog nose, cat chin, bee abdomen, chicken head, and football center, respectively.</figDesc><graphic coords="6,54.84,306.79,226.80,199.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. T-SNE embeddings of different MAE models under varied masking ratios and patch sizes. We fix the patch size at 16 to vary the masking ratios and fix the masking ratio at 0.75 to change the patch sizes. Each color represents one ImageNet class. mask ratio patch size IN1K IN-v2 OJN IN-R IN-A IN-S 0.1 16 47.45 34.72 9.42 14.63 2.00 7.25 0.25 16 53.58 40.34 11.54 18.68 2.49 10.27 0.5 16 60.07 46.71 13.94 22.44 2.89 12.58 0.75 16 67.41 54.23 18.24 25.20 3.76 15.51 0.9 16 62.97 49.52 15.87 19.11 2.76 10.46 0.75 8 62.57 49.17 13.44 19.42 3.73 10.73 0.75 16 68.96 55.94 13.73 24.23 6.29 18.81 0.75 32 73.31 61.35 19.03 27.84 12.69 28.30</figDesc><graphic coords="7,73.62,72.00,445.48,58.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>bias 0.1352 0.2545 0.2458 0.2563 0.2014</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>We thank the Google TRC program for the TPU Research Cloud support, Ronghang Hu and Xinlei Chen for the MAE TPU code, Biwei Huang for technical discussions, Tao Lin for feedback on the manuscript, and anonymous reviewers for valuable feedback. The work of LK and YC is supported in part by NSF under the grants CCF-1901199 and DMS-2134080, and by ONR under the grant N00014-19-1-2404. The work of MM and LP is partially supported by BMW, National Science Foundation awards 1722822 and 1750439, and National Institutes of Health awards R01MH125740, R01MH096951, R21MH130767 and R01MH132225. This project is also partially supported by the National Institutes of Health (NIH) under Contract R01HL159805, by the NSF-Convergence Accelerator Track-D award 2134901, by a grant from Apple Inc., a grant from KDDI Research Inc, and generous gifts from Salesforce Inc., Microsoft Research, and Amazon Research.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>1 .Algorithm 1 1 : 3 :</head><label>1113</label><figDesc>xm = gx m (c, sm) and xmc = gx m c (c, smc ) where both gx m and gx m c are invertible; 2. sm ⊥ ⊥ (c, smc ); 3. c is minimal: ∀c ′ ⊂ Z such that dim(c ′ ) &lt; dim(c), c ′ cannot satisfy the two conditions above. Such c and the corresponding sm are unique and can be located from the hierarchical structure by executing Algorithm 1. Furthermore, smc can be found through Algorithm 2. Search for the minimal c and sm. c and sm discussed in text can be viewed as the concatenations of vectors in C and Sm. LocateParents(•) pins down the locations Z's parents (including exogenous variables) in the graph. DirectedPaths(d, X c m ) returns the set of variables on the directed paths between d and X c m . inputs: The hierarchical graph structure G, and the partitioned observables X m , X m c . 2: C, S m ← ∅, ∅. Selection stage: 4: for x ∈ X m do 5: Z ← {x}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>13 :</head><label>13</label><figDesc>Pruning stage: 14: for d ∈ C do 15: for d ′ ∈ C \ {d} do 16: if d ′ ∈ DirectedPaths(d, X m c ) then 17: C ← C \ {d} return C, S m Algorithm 2 Search for s m c given C. LocateParents(p) pins down the locations p's parents (including exogenous variables) in the graph. 1: inputs: The hierarchical graph structure G, the partitioned observables X m , X m c , and C returned by Algorithm 1. 2: S m c ← ∅. 3: for x ∈ X m c do 4: P, P ′ ← {x}, ∅.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Theorem 2 .</head><label>2</label><figDesc>(Identifiability of c): For each mask m, given the dimensions (dc, ds m ) the encoder function Emc (•) recovers all information of c located in Theorem 1, i.e., there exists a one-toone mapping h, s.t., h(c) = ĉ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Reconstruction evaluation using the validation set without masking, based on two structural-level similarity metrics (SSIM and FSIM) and two pixel-level metrics (PSNR and MSE). We plot negative MSE for easier visualization. Higher SSIM and FSIM indicate high-level information is better captured, while higher PSNR and negative MSE indicates better low-level reconstruction. Here the patch size refers to the patch size in the original MAE, where the masking patch size and the patch size of ViT are equal.</figDesc><graphic coords="15,98.37,72.00,395.98,60.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. T-SNE embeddings of different MAE models under varied masking ratios and patch sizes. We fix the masking ratio at 0.75 to change patch sizes. Each color represents one ImageNet class. The patch size refers to the patch size in the original MAE, where the masking patch size and the patch size of ViT are equal.</figDesc><graphic coords="15,50.11,460.27,236.25,83.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>ImageNet-Adversarial consists of natural images with adversarial filtration, meaning samples that can be classified with spurious cues are removed. Examples in ImageNet-A are harder to classify correctly and can cause mistakes across various models. ImageNet-Rendition contains renditions of ImageNet classes, such as art, cartoons, graffiti, and paintings. These examples share the same high-level object labels as ImageNet examples but differ in style and texture. ImageNet-Sketch contains black and white images of ImageNet classes, also differing in color and texture compared to original ImageNet samples. ObjectNet is a set of images captured at unusual poses in cluttered, natural scenes, which can severely degrade recognition performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>A moderate patch size 16 yields the best robustness evaluation on IN-v2, OJN, IN-R, and IN-S. If we follow the original MAE and do not decouple masking patch size and ViT patch size, a medium patch size has stronger robustness performances than extreme patch sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>z 1 z 2 z 3 z 4 z 5 z 6 z 7 z 8 z 9 z 10 x 1 x 2 x 3 x 4 x 5 x 6 x 7 x 8 x 9 x 10 x 11 Figure 2. A hierarchical data-generating process. z represents the latent variables and x stands for the observable variables (i.e. image pixels). The hierarchical model is generic and is capable of modeling arbitrary DAGs in the latent space.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Accuracy (%) of linear probing and robustness evaluation on ImageNet variants and ObjectNet. We linear-probe MAE via supervised training on IN1K, and then perform inference on IN1K as well as other evaluation sets.</figDesc><table><row><cell>1</cell><cell>16</cell><cell>47.45 34.72 9.42 14.63 2.00 7.25</cell></row><row><cell>0.25</cell><cell>16</cell><cell>53.58 40.34 11.54 18.68 2.49 10.27</cell></row><row><cell>0.5</cell><cell>16</cell><cell>60.07 46.71 13.94 22.44 2.89 12.58</cell></row><row><cell>0.75</cell><cell>16</cell><cell>67.41 54.23 18.24 25.20 3.76 15.51</cell></row><row><cell>0.9</cell><cell>16</cell><cell>62.97 49.52 15.87 19.11 2.76 10.46</cell></row><row><cell>0.75</cell><cell>8</cell><cell>62.57 49.17 13.44 19.42 3.73 10.73</cell></row><row><cell>0.75</cell><cell>16</cell><cell>68.96 55.94 13.73 24.23 6.29 18.81</cell></row><row><cell>0.75</cell><cell>32</cell><cell>73.31 61.35 19.03 27.84 12.69 28.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Shape bias<ref type="bibr" target="#b18">[19]</ref> measurement, a higher metric indicates that the model classifies images relying on the high-level shape feature rather than the low-level texture feature.</figDesc><table><row><cell cols="4">mask ratio mask size AP box AP mask</cell></row><row><cell>0.1</cell><cell>16</cell><cell>30.47</cell><cell>28.24</cell></row><row><cell>0.25</cell><cell>16</cell><cell>32.38</cell><cell>29.95</cell></row><row><cell>0.5</cell><cell>16</cell><cell>34.87</cell><cell>32.11</cell></row><row><cell>0.75</cell><cell>16</cell><cell>39.72</cell><cell>36.35</cell></row><row><cell>0.9</cell><cell>16</cell><cell>37.17</cell><cell>34.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>COCO object detection and segmentation using a ViT Mask R-CNN baseline.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>mask ratio patch size IN1K IN-v2 OJN IN-R IN-A IN-S 0.75 8 62.57 49.17 13.44 19.42 3.73 10.73 0.75 16 67.41 54.23 18.24 25.20 3.76 15.51 0.75 32 55.51 42.35 13.46 18.70 1.89 9.48 Accuracy (%) of linear probing and robustness evaluation on ImageNet variants and ObjectNet. We linear probe MAE via supervised training on IN1K, and then perform inference on IN1K as well as other evaluation sets. We fix the masking ratio at 0.75 to change patch sizes. The patch size refers to the patch size in the original MAE, where the masking patch size and the patch size of ViT are equal.</figDesc><table><row><cell cols="4">mask ratio patch size AP box AP mask</cell></row><row><cell>0.75</cell><cell>8</cell><cell>34.21</cell><cell>32.28</cell></row><row><cell>0.75</cell><cell>16</cell><cell>33.77</cell><cell>32.04</cell></row><row><cell>0.75</cell><cell>32</cell><cell>32.39</cell><cell>30.54</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In high-dimensional data like images, there is a larger degree of information redundancy, e.g., neighboring pixels. Thus, it is sensible to lump one-dimensional variables into vectors.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>To avoid notation cluttering, we adopt • to distinguish the estimated variables from the true ones in the generating process.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning linear bayesian networks with latent variables</title>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adel</forename><surname>Javanmard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Data2vec: A general framework for self-supervised learning in speech, vision and language</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03555</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Vicreg: Variance-invariance-covariance regularization for selfsupervised learning</title>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Bardes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04906</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An informationmaximization approach to blind separation and blind deconvolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1129" to="1159" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Irina</forename><surname>Christopher P Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03599</idno>
		<title level="m">Understanding disentangling in beta vae</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">How to understand masked autoencoders</title>
		<author>
			<persName><forename type="first">Shuhao</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Clifton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno>ArXiv, abs/2006.09882</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Context autoencoder for self-supervised representation learning</title>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shentong</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03026</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Independent component analysis, a new concept? Signal processing</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Comon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="287" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Weakly-supervised disentanglement without compromises</title>
		<author>
			<persName><surname>Locatello</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12231</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hidden markov nonlinear ica: Unsupervised learning from nonstationary time series</title>
		<author>
			<persName><forename type="first">Hermanni</forename><surname>Hälvä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
		<idno>PMLR, 2020. 8</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="939" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Provable guarantees for self-supervised deep learning with spectral contrastive loss</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Jeff Z Haochen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5000" to="5011" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 1, 3, 4, 5, 6, 7, 8, 14</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Natural adversarial examples</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">beta-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">How to represent part-whole hierarchies in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12627</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Exploring long-sequence masked autoencoders</title>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoubhik</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.07224</idno>
		<imprint>
			<date type="published" when="2022">2022. 1, 4, 5, 8, 14</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Latent hierarchical causal structure discovery with rank constraints</title>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.01798</idno>
		<imprint>
			<date type="published" when="2008">2022. 1, 2, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<title level="m">Independent Component Analysis</title>
		<imprint>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nonlinear ica using auxiliary variables and generalized contrastive learning</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="859" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Kakogeorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Psomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Karantzalos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12719</idno>
		<title level="m">What to hide from your students: Attention-guided masked image modeling</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008">June 2019. 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Variational autoencoders and nonlinear ica: A unifying framework</title>
		<author>
			<persName><forename type="first">Ilyes</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2207" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Partial disentanglement for domain adaptation</title>
		<author>
			<persName><forename type="first">Lingjing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Akinwande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Csaba</forename><surname>Szepesvari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sivan</forename><surname>Sabato</surname></persName>
		</editor>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022-07">Jul 2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodríguez</forename><surname>Pau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><surname>Lacoste-Julien</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.10098</idno>
		<title level="m">Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ica</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Predicting what you already know helps: Provable selfsupervised learning</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikunj</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Semmae: Semantic-guided masking for learning masked autoencoders</title>
		<author>
			<persName><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changwen</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.10207</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mst: Masked self-supervised transformer for visual representation</title>
		<author>
			<persName><forename type="first">Zhaowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yousong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Raetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4114" to="4124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Latent correlation-based multiview learning and self-supervision: A unifying perspective</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songtao</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07115</idno>
		<imprint>
			<date type="published" when="2008">2021. 2, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Understanding latent correlation-based multiview learning and self-supervision: An identifiability perspective</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songtao</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Biva: A very deep hierarchy of latent variables for generative modeling. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Liévin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Towards understanding why mask-reconstruction pretraining helps in downstream tasks</title>
		<author>
			<persName><forename type="first">Jiachun</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.03826</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR, 2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note>Dynamic routing between capsules</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Image quality assessment through fsim, ssim, mse and psnr-a comparative study</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and Communications</title>
		<editor>
			<persName><forename type="first">Umme</forename><surname>Sara</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Morium</forename><surname>Akter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mohammad</forename><surname>Shorif Uddin</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="8" to="18" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Evaluating machine accuracy on imagenet</title>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horia</forename><surname>Mania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Adversarial masking for self-supervised learning</title>
		<author>
			<persName><forename type="first">Yuge</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Kosiorek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Disentanglement by nonlinear ica with general incompressible-flow networks (gin)</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sorrenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ullrich</forename><surname>Köthe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04872</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Nvae: A deep hierarchical variational autoencoder</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002">19667-19679, 2020. 2</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Accelerating t-sne using treebased algorithms</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Self-supervised learning with data augmentations provably isolates content from style</title>
		<author>
			<persName><forename type="first">Julius</forename><surname>Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="16451" to="16467" />
			<date type="published" when="2008">2021. 2, 4, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Extreme masking for learning instance and distributed visual representations</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04667</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Identification of linear non-gaussian latent hierarchical structure</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangbo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008">2022. 1, 2, 8</date>
			<biblScope unit="page" from="24370" to="24387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Simmim: A simple framework for masked image modeling</title>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2022. 1, 4, 8</date>
			<biblScope unit="page" from="9653" to="9663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Deny</surname></persName>
		</author>
		<idno>PMLR, 2021. 4</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="12310" to="12320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Fsim: A feature similarity index for image quality assessment</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqin</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2386" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">How mask matters: Towards theoretical understandings of masked autoencoders</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.08344</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Learning hierarchical features from generative models</title>
		<author>
			<persName><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08396</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07751</idno>
		<title level="m">On the identifiability of nonlinear ica: Sparsity and beyond</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Jinghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07832</idno>
		<title level="m">ibot: Image bert pre-training with online tokenizer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
