<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
							<email>avahdat@sfu.ca</email>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Cannons</surname></persName>
							<email>kcannons@sfu.ca</email>
						</author>
						<author>
							<persName><forename type="first">Greg</forename><surname>Mori</surname></persName>
							<email>mori@sfu.ca</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Sangmin Oh, and Ilseo Kim Kitware Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a compositional model for video event detection. A video is modeled using a collection of both global and segment-level features and kernel functions are employed for similarity comparisons. The locations of salient, discriminative video segments are treated as a latent variable, allowing the model to explicitly ignore portions of the video that are unimportant for classification. A novel, multiple kernel learning (MKL) latent support vector machine (SVM) is defined, that is used to combine and re-weight multiple feature types in a principled fashion while simultaneously operating within the latent variable framework. The compositional nature of the proposed model allows it to respond directly to the challenges of temporal clutter and intra-class variation, which are prevalent in unconstrained internet videos. Experimental results on the TRECVID Multimedia Event Detection 2011 (MED11) dataset demonstrate the efficacy of the method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multimedia event detection in unconstrained video collections is a challenging problem. Event categories are diverse and exhibit large intra-class variation. Additionally, videos may be composed of a small number of important segments, while the remaining portions of the video are ineffective for classification.</p><p>Consider the example video from the board trick category in Fig. <ref type="figure" target="#fig_3">1</ref>. This video contains segments focusing on the snowboard, the person jumping, is shot in an outdoor, ski-resort scene, and has fast-paced theme music. Together, all of these pieces of evidence can lead an algorithm to declare that this video is from the relevant category.</p><p>This work was supported by NSERC and the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center contract number D11PC20069. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DOI/NBC, or the U.S. Government. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T est Video</head><p>PDF created with pdfFactory trial version www.pdffactory.com</p><p>Figure <ref type="figure" target="#fig_3">1</ref>: A test video can be described using pieces of similar training videos. Similarity might be defined from different perspectives. In this example, parts of the test video from the board trick event are similar to three different videos in terms of motion and sound (green), pure motion (purple) or motion and texture (yellow).</p><p>Building a model that can correctly categorize this type of video is challenging. Arguably, such a model must reason about which temporal segments within the video contain relevant evidence. Additionally, grouping these segments into different mid-level categories, or "scene types" may be beneficial. For the board trick event, a particular video may involve a surfboard, skateboard, or snowboard trick, but is unlikely to include all three. Grouping segments into their relevant scene types can improve recognition. Finally, the model must utilize a variety of different low-level features in order to make such a decision.</p><p>In this paper we present a novel, compositional model for video event detection. Our model uses a latent variable framework to localize the discriminative temporal segments of a video. These temporal segments are matched to training segments of the same scene type via kernels that combine information from several feature modalities. The test video is explained as a composition of related training videos.</p><p>The main contribution of this paper is the theoretical development of a formulation and learning algorithm for this type of model. The proposed compositional method has two key novel aspects: (1) a weakly supervised method for localizing only the most salient evidence for classification in a video sequence. This method does not require manual marking of the salient segments -they are automatically extracted and labeled by scene type. (2) A novel multiple kernel learning algorithm with structured latent variables that permits the principled combination of multiple different low-level features in a single integrated framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous Work</head><p>Event detection in unconstrained internet videos is an active area of research. We consider the TRECVID MED11 dataset -a large, diverse, and challenging video collection. Among the top ranking methods on this dataset is the work of Natarajan et al. <ref type="bibr" target="#b6">[7]</ref>, which performs a principled combination of many low-level features using a global, video-level representation. It is arguable that engineering a combination of many complementary low-level features is necessary for excellent performance on this dataset, and the method we propose can be used with a multitude of features in this manner. Furthermore, our multiple kernel learning algorithm offers an extension that allows for such feature combination in conjunction with latent SVMs. With this novel approach, more detailed comparisons between latently selected video segments can be considered.</p><p>Other video classification work includes Niebles et al. <ref type="bibr" target="#b7">[8]</ref>, who developed a related model for human action recognition, but used a fixed, single temporal ordering of key poses around anchor points -which may break down in internet videos due to temporal clutter. Tang et al. <ref type="bibr" target="#b11">[12]</ref> extended this line of work to consider temporal segmentation via a variant of an HMM. Cao et al. <ref type="bibr" target="#b0">[1]</ref> considered a "scene aligned pooling" feature representation to capture the different scenes present in a single video. In contrast to the above, our method focuses on intra-class variation and temporal scatter of an event by using latent variables to compose a test video in a kernelized framework. In direct comparisons, we show empirically that our approach outperforms these previous methods.</p><p>The approach we take to modeling internet videos is weakly supervised -only a video-level category label is provided during training. Segments and their associated scene types that compose a video are learned in an unsupervised fashion. Izadinia and Shah <ref type="bibr" target="#b3">[4]</ref> developed a similar method, but with manual annotations on the training data -extending the image-attribute method of Wang and Mori <ref type="bibr" target="#b16">[17]</ref> to the video domain.</p><p>Technically, the proposed approach is most closely related to <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3]</ref>, but differentiates itself by presenting a novel multiple kernel learning approach that accommodates structured latent variables. In comparison, Wu and Jia <ref type="bibr" target="#b17">[18]</ref> and Yang et al. <ref type="bibr" target="#b19">[20]</ref> developed kernelized variants of the latent support vector machine <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref>. However, the algorithms for learning kernelized latent SVMs in these pa-pers have two drawbacks: they are limited to cases where one can enumerate the set of latent variables and they are restricted to a single kernel or a set of summed kernels. Finally, Gu et al. <ref type="bibr" target="#b2">[3]</ref> consider low level concept detection (e.g. flag, car, building) using a bag-instance relationship whereas ours examines high-level event recognition.</p><p>Kernelized classifiers often offer superior performance. A body of work has aimed at providing efficient training and evaluation with kernelized classifiers via algorithmic optimizations or additive linear approximations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10]</ref>. This line of work is promising, but has yet to be extended to latent variable models, as is done here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Compositional Models for Video Retrieval</head><p>We are interested in the classification of high-level complex events in unconstrained internet videos. Two significant challenges in this domain are temporal clutter (i.e., the evidence of a complex event can occur in small, isolated video segments) and intra-class variation. In this paper, we target both the intra-class variation and temporal clutter challenges by leveraging a compositional model.</p><p>Early successes on the TRECVID MED11 dataset have often deferred to an approach where the output of an array of simple classifiers operating on a range of low-level features are combined <ref type="bibr" target="#b6">[7]</ref>. These approaches have tended to employ simple, bag of words (BoW) representations with kernelized SVM classifiers. In such systems, the standard kernelized SVM can be thought of as a form of intelligent template matching, whereby a test video is compared directly against the set of support vectors. Such approaches can perform effective matching on global video-level representations, but are not well-suited for segment-level analysis. By introducing latent variables in our proposed method, kernelized latent SVMs are constructed that select particularly salient video segments. Thus, this intelligent template matching can now be completed not only at the video level, but also at the segment level. This approach provides our compositional model with the additional flexibility to mix and match segments from the pool of training videos when evaluating a test video, directly addressing the challenges of clutter and intra-class variation.</p><p>Additionally, to attain state-of-the-art performance on TRECVID MED11, it appears that multiple feature types must be combined. We further extend our model to combine multiple kernel learning with the kernelized latent SVM framework, adding the ability to weight feature types based on their relative importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Linear Model</head><p>To begin the exposition we describe the linear version of our model, which consists of two parts. The first part is a global model that captures the overall theme or "subcategory" of the video. It is assumed that each event category contains several subcategories (e.g., a wedding   ceremony at a church, house, or park). Further, it is assumed that a particular video corresponds to only one subcategory. The second part of our formulation is a "scene type model" that represents an event by a set of segment-level features. This part of the model is included to identify and localize discriminative segments of interest in a video. The model is depicted graphically in Fig. <ref type="figure" target="#fig_2">2</ref>.</p><p>We consider eight second segments that correspond to scenes observed within the event category (e.g., for wedding ceremony videos, outdoor park scenes or people dancing, cutting a cake, or kissing). A weakly supervised setting is considered, meaning that we are only given a binary event label for each video that indicates the presence of a complex event in the sequence; the subcategory labels, scene type labels, and temporal locations of scene types are not provided. These are modeled as hidden variables and we employ a latent max-margin approach <ref type="bibr" target="#b1">[2]</ref> to infer them during training.</p><p>Concretely, assume we are given a video sequence x, and want to classify it into an event category. The variables C and S denote the number of subcategories and scene types for an event, respectively. The presence of a subcategory c ∈ {1, 2, . . . , C} is defined using the binary variable b c ; similarly, the presence of a scene type s ∈ {1, 2, . . . , S} is denoted using the binary variable z s .</p><p>We define φ g (x), a global feature extracted from the whole sequence, and φ l (x, t) a segment-level feature extracted from a temporal window of fixed size centered at time t in x. Multiple features are incorporated to improve accuracy: G global and L local (segment-level) features. Together, the linear version of our model is defined as:</p><formula xml:id="formula_0">fw(x, b, h) = C c=1 G g=1 w T cg φg(x)bc + S s=1 L l=1 w T sl φ l (x, ts)zs (1)</formula><p>where w cg is the learned weight vector for the c th subcategory model on the global feature φ g (•), and w sl is the weight vector for the s th scene type model defined on the segment-level feature φ l (•). Use of the same set of feature types in the global and segment-level scales can be achieved by setting G = L. However, more generally, our model sup-ports the added flexibility of using different sets of features for the two parts. For notational compactness, we represent the pair (t s , z s ) using h s for s ∈ {1, 2, ..., S}, and group them in vector h = {h 1 , h 2 , ..., h S }. We similarly group subcategory binary variables in</p><formula xml:id="formula_1">b = {b 1 , b 2 , ..., b C }.</formula><p>Note that the model in Eq. 1 assumes the temporal location for the s th scene type is shared among all segment-level features types -they are all extracted from the same temporal window in the sequence.</p><p>It is assumed that a sequence can belong to only one global subcategory, but multiple scene types might be observed in a sequence, corresponding to the various segments. Therefore, two hard constraints are imposed on the selecting binary variables:</p><formula xml:id="formula_2">C c=1 b c = 1, and S s=1 z s = K, where K is a constant parameter.</formula><p>The subcategory variables, b c , and scene model configurations, h s , are latent variables, unobserved on both training and testing data. Next, we develop a novel multiple kernel learning approach for learning with these latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multiple Kernel Latent SVM</head><p>Latent SVMs have been successfully used in many computer vision tasks. They were originally proposed for linear models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b1">2]</ref>, where the similarity of two samples is measured using a simple dot product. Recently, LSVMs were extended to kernelized versions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref> resulting in significant boosts in recognition accuracy. However, both <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref> assumed simple models with few latent variables that could be enumerated during inference. In our proposed model, latent variables are defined in a structured framework such that enumeration is not tractable.</p><p>The use of multiple complementary features can lead to improved recognition accuracy. With multiple features, fusion is a challenge because the importance of feature types is variable. Multiple kernel learning is a standard approach to address this challenge. A linear MKL SVM framework (e.g., <ref type="bibr" target="#b15">[16]</ref>) typically performs such fusion by linearly combining a set of kernels K = i d i K i , which corresponds to re-scaling feature maps of the kernel, Ψ i , by √ d i . The linear model in Eq. 1 is also defined with respect to multiple features. We require a training framework that can accommodate both latent variables and feature re-scaling simultaneously. We propose a novel multiple kernel latent SVM framework that extends standard MKL and can be used to train models of the form proposed in this paper.</p><p>Consider a set {(x 1 , y 1 ), (x 2 , y 2 ), . . . , (x N , y N )} of training videos where x i ∈ X is the i th video and y i ∈ {-1, 1} its label. Our goal is to learn a scoring function F : X → R that can be used to classify a video. Similar to the standard latent SVM, the proposed multiple kernel latent SVM (MKL-KLSVM 1 ) operates upon a set of base feature maps, Ψ i (x, v), defined on a sample x and its latent variables v ∈ V, where V is the set of all possible latent variables. We define the scoring function</p><formula xml:id="formula_3">F (x) = max v I i=1 √ d i w T i Ψ i (x, v)</formula><p>where d i is the normalizing factor for the i th base feature map. Training of the MKL-KLSVM is then formulated as:</p><formula xml:id="formula_4">min w,b,ξ≥0,d≥0 1 2 i w T i wi + ρ n ξn + λ 2 i d 2 i (2) s.t. yn( max v∈Vn i √ diw T i Ψi(xn, v) + b) ≥ 1 -ξn ∀n,</formula><p>where λ is a regularizer on the kernel weights, d i to prevent them from diverging to infinity, and ρ is a trade-off parameter to penalize error on the training data. Note that our multiple kernel latent SVM framework becomes a standard latent SVM <ref type="bibr" target="#b1">[2]</ref> if the kernel coefficients, d i , are set to one and will become a standard MKL classifier if the hidden variables v n are observed. The objective function in Eq. 2 is not convex; however, convexity is attained if the latent variables for positive samples are available (semi-convexity of latent SVM <ref type="bibr" target="#b1">[2]</ref>) and if w i is replaced with √ d i w i . Here we limit the possible latent variables of positive samples to a single configuration V n = {v * n } ∀n : y n = 1, but allow negative samples to consider all possible latent variables, V n ∀n :</p><formula xml:id="formula_5">y n = -1.</formula><p>Given that the latent variable configuration has been specified, the max operator can be omitted from Eq. 2, yielding, min w,b,ξ≥0,d≥0</p><formula xml:id="formula_6">1 2 i w T i wi di + ρ n ξn + λ 2 i d 2 i<label>(3)</label></formula><formula xml:id="formula_7">s.t. yn( i w T i Ψi(xn, v) + b) ≥ 1 -ξn ∀n, ∀v ∈ Vn</formula><p>The objective function in Eq. 3 addresses the problem of learning parameters of a structural SVM with multiple kernels. It has N -|V|+N + constraints, where N -and N + are the number of negative and positive samples respectively. If the latent variables are structured, |V| will be exponential.</p><p>The same problem of exponential constraints is confronted with linear latent SVMs as well. Yu and Joachims <ref type="bibr" target="#b20">[21]</ref> use the cutting plane algorithm <ref type="bibr" target="#b12">[13]</ref> to ameliorate this challenge by mining hard constraints and iteratively optimizing with and updating the current constraints. We use the cutting plane algorithm to extract the set of most violated constraints for negative samples during training, while the latent variables of positive videos remain fixed. Here, Ṽn denotes the set of current active constraints (instead of V n , which represents all the constraints defined over all possible latent variables). The set of active constraints, Ṽn , contains just a single constraint per positive sample, but can have multiple constraints for negative samples, extracted using the cutting plane algorithm.</p><p>Given a current set of constraints, a method is required for optimizing Eq. 3. By forming the Lagrangian of Eq. 3 and minimizing the objective function with respect to w i , ξ and b, we obtain</p><formula xml:id="formula_8">w i = d i n,v∈ Ṽn α n,v y n Ψ i (x n , v)<label>(4)</label></formula><p>where α n,v is the Lagrangian variable for the n th sample and the latent variables, v. Substituting w i in Eq. 3 yields</p><formula xml:id="formula_9">min d≥0 max α L(α, d) = n,v αn,v + λ 2 i d 2 i (5) - 1 2 i di   n,v m,v αn,vα m,v ynymΨi(xn, v) T Ψi(xm, v )   s.t. 0 ≤ n,v αn,v ≤ ρ, n,v ynαn,v = 0,</formula><p>which is an instance of the saddle point problem. In Eq. 5, Ψ i (x n , v) T Ψ i (x m , v ) can be replaced with a kernel k(x n , v, x m , v ) that measures the similarity of x n and x m , given their latent configurations. If the kernel weights, d, are fixed in Eq. 5, the inner maximization will become the Quadratic Program (QP) of a kernelized structural SVM <ref type="bibr" target="#b12">[13]</ref>. We solve the saddle point problem by iteratively updating d and subsequently performing QP optimization for α with a fixed d. The kernel weights can be updated using a Newton descent step or the cutting plane approach <ref type="bibr" target="#b4">[5]</ref>. Alternatively, the Lagrangian of Eq. 5 can be derived to form the dual problem, which is differentiable and can be optimized using the sequential minimal optimization (SMO) algorithm <ref type="bibr" target="#b10">[11]</ref>, similar to <ref type="bibr" target="#b15">[16]</ref>.</p><p>Here, we elect to use the simple Newton descent approach. Given the optimum, α * , from iteration τ , in iteration τ +1 an update is computed as</p><formula xml:id="formula_10">d τ +1 = d τ -µH -1 ∇L, where µ = 1</formula><p>τ is the step size. Additionally, H = λI is the Hessian matrix of L(α * , d) (I is the identity matrix), and</p><formula xml:id="formula_11">∇L i (α * , d) = λd τ i -1 2 n,v y n α * n,v Ψ i (x n , v n ) 2</formula><p>is the the derivative of L with respect to d τ . If a Newton descent update results in a negative kernel weight, it is back projected using</p><formula xml:id="formula_12">d τ +1 i = 0 if d τ +1 i &lt; 0.</formula><p>After updating the kernel weights, the inner quadratic program in Eq. 5 is solved by assuming d is fixed. We iterate between these two steps until the optimization converges and the objective function does not change. Given the final α * and d * (which together represent w), we infer the latent variables on the positive examples using v * n = arg max v i w T i Ψ i (x n , v). It has been shown for standard linear latent SVMs that iteratively updating the latent variables of positive samples and learning the latent SVM model parameters will minimize the objective function to a local optimum <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b1">2]</ref>. The same argument holds for multiple kernel latent SVM. Algorithm 1 provides a summary of our proposed training algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Kernelized Model</head><p>We use multiple kernel latent SVM to train the parameters of our model defined in Eq. 1. However, we still must define Ψ i (x, v), the base features, and their corresponding kernels that have an associated re-scaling coefficient d i as in Eq. 2. For the linear model defined in Eq. 1 global models were defined on G global features while scene type models employed L segment-level feature types. Specifically, the base features in Eq. 2, Ψ i , are defined as C c=1 φ g (x)b c for the global features and S s=1 φ l (x, t s )z s for the segmentlevel features, which are derived from Eq. 1. Thus, G + L kernels are defined as</p><formula xml:id="formula_13">Kg(x, b, x , b ) = C c=1 bckg(x, x )b c , K l (x, h, x , h ) = S s=1 zsk l (x, ts, x , t s )z s .<label>(6)</label></formula><p>Given two videos, x and x , K g measures the kernelized similarity of their global feature if they belong to the same subcategory; otherwise, it assigns zero similarity. Analogously, K l measures the kernelized similarity of segmentlevel feature l for sequences x and x at times t s and t s for the scene models that are present in both x and x .</p><p>Given the kernels defined in Eq. 6, Alg. 1 is used to learn α * and d * , the parameters of the proposed kernelized model. We can substitute these parameters in Eq. 1 to rewrite our scoring function for the kernelized model: where (h n , b n ) ∈ Ṽn are latent variables defined for the n th training sample. The completed model in Eq. 7 is the full, proposed compositional model. Given the sequence, x, maximization matches the sequence to the training videos by choosing segment locations, h, and the subcategory model, b, that are well-explained by the training videos. A test video, x, is assigned a high score for an event category if it is similar to its associated positive training videos using two criteria. First, the global features from the test video should be similar to the global features from training videos. Second, the test video should contain segments that are similar to those in the training set. Under this framework, the test video can be composed using components from numerous training videos at both the global and segment scale. The learned kernel coefficients, d, allow for the re-scaling of the similarity measures on different parts of model. This rescaling can give higher weights to important feature types while allowing for the extraction of the most discriminative evidence from the training set, using (h n , b n ).</p><formula xml:id="formula_14">F (x) = max b,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>Simple heuristics are used to initialize the latent variables for the positive samples. For the subcategory labels, we cluster the concatenated global features of the positive videos into C clusters. Subsequently, we assign a video to the closest cluster. For the scene models, we similarly cluster the concatenated segment-level features of all segments from the positive training videos. Then, we choose the K closest clusters to the video segments, and set the temporal location of each, t s , to the closest segment.</p><p>Inference: For inferring latent variables, we first need to compute the global and scene model scores for each subcategory and scene type. For a general kernel type, there is no explicit form of w i and direct comparison to support vectors is necessary to compute the scores. Kernel comparison can significantly slow down the inference. Given N s support vectors, considering Eq. 7, Eq. 6 and sparsity of b n and z in h n , O(N s G + N s KLT ) kernel comparisons will be required to compute the scores for a sequence. However, with additive kernels we can approximate the embedding feature <ref type="bibr" target="#b13">[14]</ref>, and form an approximated w i using Eq. 4. Thus, the number of linear kernel computations becomes O(CG + SLT ).</p><p>Consider the model in Fig <ref type="figure" target="#fig_3">1</ref>. Now, given global and scene type model scores, we need to infer the subcategory variables b c and temporal locations t s of the K best scene type models. The subcategory can be found in O(C). For a video with T segments, the best location for each scene type is found in O(T ), and then the K best scenes are selected in O(S log(K)) using a min heap. So, the complexity of inference is O(C + ST + S log(K)) in addition to the score computation. In our experiments, this inference takes 0.05 seconds for a 120-second video on an Intel CPU E7450 @2.40GHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our model on the challenging TRECVID MED11 dataset <ref type="bibr" target="#b8">[9]</ref>, following a standard evaluation protocol used in previous work <ref type="bibr" target="#b11">[12]</ref>. The TRECVID MED11 dataset contains 15 events that are divided across two collections, DEV-T and DEV-O. The DEV-T dataset consists of 10,723 videos including videos from five event categories: board trick (E1), feeding animal (E2), landing fish (E3), wedding ceremony (E4), and woodworking project (E5). The DEV-O collection is significantly larger, 32,061 videos, and includes ten categories: birthday party (E6), changing a tire (E7), flash mob (E8), getting a vehicle unstuck (E9), grooming animal (E10), making sandwich Table <ref type="table">1</ref>: Performance variation on the DEV-T dataset as a function of model parameters: the number of subcategories (C), number of scene types (S), and number of selected scenes (K). Selection is done for each parameter in turn and is fixed for subsequent parameters, as shown in red. (E11), parade (E12), parkour (E13), repairing appliance (E14), and sewing project (E15). Both DEV-T and DEV-O are dominated by videos of the null category (i.e., background videos that do not contain the events of interest). For training, an Event-Kit data collection, containing roughly 150 positive videos per category, is also provided. A classifier is trained for each event category versus all other categories, similar to <ref type="bibr" target="#b11">[12]</ref>. For TRECVID MED11, DEV-T is used for development, whereas DEV-O is utilized for testing. Thus, we performed cross validation of all system parameters and hyper parameters on DEV-T and held them constant when considering DEV-O. We use mean average precision (mAP) as the performance metric to remain comparable with recently published works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparisons using HOG3D Features</head><p>First, we evaluated our proposed method against several baselines. This evaluation uses HOG3D features, k-means quantized into a 1,000 word codebook for all methods. For this experiment, we use the following set of baselines: Linear-SVM, a linear SVM using HOG3D BoW features; KSVM, same video-level features with histogram intersection kernel (HIK) SVM; Niebles <ref type="bibr" target="#b7">[8]</ref>; Tang <ref type="bibr" target="#b11">[12]</ref>; Linear-SAP, the scene-aligned pooling method [1] using a linear SVM; and K-SAP, the same method using a HIK-SVM. Results for Niebles and Tang are reproduced from <ref type="bibr" target="#b11">[12]</ref> and we obtained exactly the same quantized features to be directly comparable. Also, note that we re-implemented the scene aligned pooling method <ref type="bibr" target="#b0">[1]</ref> using parameters suggested by the authors to permit direct comparisons.</p><p>Two variants of our proposed model were considered: Linear-LSVM, using a linear latent SVM, and KLSVM, using a HIK latent SVM. For the proposed models, selection of appropriate parameters is required, including the number of subcategories (C), number of scene types (S), and number of selected scenes (K). We used the kernelized version of our model with a HIK kernel to choose the best parameters on DEV-T (E1 to E5) and fixed them for all subsequent experiments using our model in this paper. Parameters were selected based on the criteria of mAP performance and model complexity. Interestingly, as Table <ref type="table">1</ref> shows, as the various components of our model are added, mAP is improved. In particular, our latent model with selected parameters (C = 8, S = 16, K = 4) outperforms the standard kernelized SVM (C = 1, S = 0) by 6.6% in mAP.</p><p>In this section, our novel multiple kernel learning formulation is not employed, since the number of kernels used is very small. Section 4.2 considers experiments with the full model, using MKL for multi-feature fusion.</p><p>Results for the six baselines and two variants of the proposed method on DEV-O are shown in Table <ref type="table" target="#tab_3">2</ref>. When considering only models that employ linear SVMs (i.e., Linear-SVM, Niebles, Tang, Linear-SAP, and Linear-LSVM), the recently proposed scene aligned pooling method provides highest performance with a mean AP of 6.28%. The linear variant of the proposed model offers midrange performance. However, the simple KSVM baseline significantly outperforms all variants that use a linear SVM classifier, including Niebles and Tang, which model complex structure. It appears that use of a kernelized SVM is critical for the task of accurate event detection.</p><p>A second performance trend can be identified from considering the models that use kernelized SVMs (i.e., KSVM, K-SAP, and KLSVM). Specifically, the proposed model, KLSVM, outperforms all other baselines, including K-SAP by 3.72% and KSVM by 4.22%. Further, KLSVM attains best performance on eight out of ten event categories, often by a significant margin (e.g., 11.43% gap for E14). These results emphasize the importance of using a compositional framework. Note that a kernelized version of Tang was not considered because it is not clear how the computationally expensive inference could be done for an extension to kernel SVMs, especially for a large data collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons using Multiple Features</head><p>In this section, we demonstrate the effectiveness of the full, multiple kernel learning-based model by extending from a single feature modality to six features.</p><p>To demonstrate the full MKL-KLSVM model, HOG3D was supplemented with five additional features from the Sun09 set <ref type="bibr" target="#b18">[19]</ref>. The additional features were: sparse SIFT, dense SIFT, HOG2x2, self-similarity descriptors (SSIM), and color histograms. Here, the same set of features was used for both the global and scene type parts of our model (i.e., G = L = 6). These particular features were selected because we empirically found them to offer best performance on TRECVID MED11. Features were extracted at four second time increments, synchronized with the HOG3D features. The two coarser scales of a three level spatial pyramid were retained for dense SIFT, HOG2x2, and SSIM. Sparse SIFT and color histograms were extracted on the whole frame. Global and segment-level features are formed by averaging the histograms. Three baselines are compared against the full MKL-KLSVM, all systems using the identical set of six features. The first baseline, KSVM, is trained on a summation of six χ 2 kernels on the global features. The second baseline, MKL-SVM, is similar to KSVM, but the weights on the kernels are trained. KLSVM and MKL-KLSVM are variants of our model that consider both the global and segmentlevel features. Global models and scene type models are formed using χ 2 and HIK, respectively. In the KLSVM, the weights of all kernels are fixed to one, while in the MKL-KLSVM, the kernel weights are learned.</p><p>Table <ref type="table" target="#tab_4">3</ref> presents the results of these systems for DEV-O. A progression in the mAP performance is demonstrated as the different components of our model are added. By allowing the model to learn the kernel weights for the various feature modalities, MKL-SVM shows slight performance gains over KSVM. KLSVM improves performance by incorporating our proposed compositional model that performs latent segment selection. Finally, when considering the full model, MKL-KLSVM, which allows the various kernel weights to be adapted for the global and segment components across multiple features, highest overall accuracy is attained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results Visualizations</head><p>Figure <ref type="figure">3</ref> shows qualitative results for our model on four test videos, where eight second segments are visualized using their center frames. The frames that are latently selected tend to be discriminative and ignore temporal clutter inherent in many test videos. For example, in the sewing project video, the latter frames where the individual is walking in an outdoor environment are not selected because such scenes are not typically associated with a video of a sewing project.</p><p>Latently selected frames of the same scene type model also often have similar overall appearance characteristics. For instance, in the grooming animal test video, the frame in the green box shows a view of a dog's backside with human hands moving its tail. A support vector containing a frame for this scene type showing a comparable view of a dog with extended human arms is also selected. The visualizations also demonstrate the compositional approach. For example, in the changing a tire test sequence, two of the top three support vector videos offer good matches for three of the latently selected frames in the test sequence (corresponding to the test frames highlighted with red, yellow, and blue boxes). However, for the fourth test frame that was selected (green box), only one of the top three support vectors provides a particularly discriminative match. The proposed model is able to accumulate evidence for classification from different video segments in the pool of training videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a novel, compositional model for video event detection that leverages a novel multiple kernel learning algorithm that incorporates structured latent variables. The kernelized latent variable framework allows the model to select and match test video segments with those that are extracted from the pool of training of videos. The compositional nature of the model allows it to respond to the challenges of intra-class variation and temporal clutter, which Figure <ref type="figure">3</ref>: Qualitative visualization of results. Individual images denote the center frame from an eight second window. Each subfigure shows frames from a testing video along with frames from the three support vectors that produce the overall best match to that test video (i.e., frames from only three support vector videos are shown for each test sequence). For a test video, the K = 4 frames that were latently selected are highlighted with colored boxes, where color denotes the particular scene type model. Latently selected frames from the the top three support vectors are grouped using colored boxes, where color corresponds to the same scene types selected for the test video. From top-to-bottom, left-to-right, the testing videos correspond to changing tire (E7), grooming animal (E10), repairing appliance (E14), and sewing project (E15). Faces have been obscured for privacy considerations. Best viewed magnified and in color. are inherent in unconstrained internet videos. Additionally, since multiple feature types are required to attain state-ofthe-art performance on TRECVID MED11, a principled approach to feature fusion via multiple kernel learning with structured latent variables is proposed. Experimental results showed that this approach outperforms state-of-the-art baselines on the challenging TRECVID MED11 dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>pdfFactory trial version www.pdffactory.com</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Depiction of our proposed model. The global model captures the subcategories of an event, and the scene model represents the different scene types observed in the category. The presence of a subcategory or scene type is represented using binary variables (b c , z s ). The temporal position of scene types in a video is denoted by t s .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1</head><label>1</label><figDesc>Training a multiple kernel latent SVM Input : {(x 1 , y 1 ), (x 2 , y 2 ) . . . , (x N , y N )} Output : α * , d * Ṽn = {v 0 n } ∀n : y n = 1, Ṽn = {} ∀n : y n = -1 repeat repeat Optimize Eq. 3 using iterative Newton descent and QP given the current Ṽn ∀n : y n = -1 add the most violated constraint to Ṽn until no change in objective function of Eq. 3 ∀n : y n = 1 update Ṽn = arg max v i w T i Ψ i (x n , v) until no change in Ṽn ∀n : y n = 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison against several baselines using HOG3D features on DEV-O for E6-E15. Numbers denote the average precision, in %. Best results for a particular event category are shown in bold.</figDesc><table><row><cell>Event</cell><cell>Chance</cell><cell>Linear-SVM</cell><cell>Niebles [8]</cell><cell>Tang [12]</cell><cell>Linear-SAP [1]</cell><cell>Linear-LSVM</cell><cell>KSVM</cell><cell>K-SAP [1]</cell><cell>KLSVM</cell></row><row><cell>E6</cell><cell>0.54</cell><cell>1.97</cell><cell>2.25</cell><cell>4.38</cell><cell>2.77</cell><cell>2.34</cell><cell>6.08</cell><cell>4.73</cell><cell>5.73</cell></row><row><cell>E7</cell><cell>0.35</cell><cell>1.25</cell><cell>0.76</cell><cell>0.92</cell><cell>2.11</cell><cell>1.33</cell><cell>2.87</cell><cell>2.26</cell><cell>4.81</cell></row><row><cell>E8</cell><cell>0.42</cell><cell>6.48</cell><cell>8.30</cell><cell>15.29</cell><cell>25.48</cell><cell>10.30</cell><cell>20.75</cell><cell>22.99</cell><cell>35.82</cell></row><row><cell>E9</cell><cell>0.26</cell><cell>2.15</cell><cell>1.95</cell><cell>2.04</cell><cell>4.14</cell><cell>1.79</cell><cell>6.25</cell><cell>7.61</cell><cell>8.38</cell></row><row><cell>E10</cell><cell>0.25</cell><cell>0.81</cell><cell>0.74</cell><cell>0.74</cell><cell>1.03</cell><cell>0.76</cell><cell>1.43</cell><cell>1.34</cell><cell>2.12</cell></row><row><cell>E11</cell><cell>0.43</cell><cell>1.10</cell><cell>1.48</cell><cell>0.84</cell><cell>1.93</cell><cell>1.41</cell><cell>2.29</cell><cell>2.65</cell><cell>4.65</cell></row><row><cell>E12</cell><cell>0.58</cell><cell>5.83</cell><cell>2.65</cell><cell>4.03</cell><cell>7.06</cell><cell>5.71</cell><cell>8.44</cell><cell>8.70</cell><cell>10.99</cell></row><row><cell>E13</cell><cell>0.32</cell><cell>2.58</cell><cell>2.05</cell><cell>3.04</cell><cell>10.38</cell><cell>2.57</cell><cell>9.44</cell><cell>10.43</cell><cell>13.11</cell></row><row><cell>E14</cell><cell>0.27</cell><cell>1.18</cell><cell>4.39</cell><cell>10.88</cell><cell>6.69</cell><cell>4.58</cell><cell>10.00</cell><cell>11.89</cell><cell>23.32</cell></row><row><cell>E15</cell><cell>0.26</cell><cell>0.92</cell><cell>0.61</cell><cell>5.48</cell><cell>1.21</cell><cell>1.09</cell><cell>2.49</cell><cell>2.4</cell><cell>3.29</cell></row><row><cell>mAP</cell><cell>0.37</cell><cell>2.43</cell><cell>2.52</cell><cell>4.77</cell><cell>6.28</cell><cell>3.19</cell><cell>7.00</cell><cell>7.50</cell><cell>11.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison against several baselines using multiple features on DEV-O for E6-E15. Numbers denote the average precision, in %.</figDesc><table><row><cell>Event</cell><cell>KSVM</cell><cell>MKL-SVM</cell><cell>KLSVM</cell><cell>MKL-KLSVM</cell></row><row><cell>E6</cell><cell>6.36</cell><cell>6.77</cell><cell>5.36</cell><cell>6.24</cell></row><row><cell>E7</cell><cell>22.04</cell><cell>22.22</cell><cell>23.47</cell><cell>24.62</cell></row><row><cell>E8</cell><cell>31.23</cell><cell>31.40</cell><cell>31.99</cell><cell>37.46</cell></row><row><cell>E9</cell><cell>18.13</cell><cell>17.49</cell><cell>16.18</cell><cell>15.72</cell></row><row><cell>E10</cell><cell>2.48</cell><cell>2.55</cell><cell>2.36</cell><cell>2.09</cell></row><row><cell>E11</cell><cell>3.88</cell><cell>4.03</cell><cell>7.98</cell><cell>7.65</cell></row><row><cell>E12</cell><cell>10.90</cell><cell>11.00</cell><cell>10.77</cell><cell>12.01</cell></row><row><cell>E13</cell><cell>13.31</cell><cell>14.54</cell><cell>13.70</cell><cell>10.96</cell></row><row><cell>E14</cell><cell>12.97</cell><cell>12.34</cell><cell>31.22</cell><cell>32.67</cell></row><row><cell>E15</cell><cell>3.98</cell><cell>3.81</cell><cell>7.47</cell><cell>7.49</cell></row><row><cell>mAP</cell><cell>12.53</cell><cell>12.62</cell><cell>15.05</cell><cell>15.69</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We use MKL-KLSVM for Multiple Kernel Latent SVM to prevent confusion with Multiple Kernel Learning SVM (MKL SVM)</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scene aligned pooling for complex video recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007">2012. 2, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-layer multiinstance learning for video concept detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1605" to="1616" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognizing complex events using large margin joint low-level event model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient and accurate lp-norm multiple kernel learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Brefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Max-margin additive classifiers for detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal feature fusion for robust event detection in web videos</title>
		<author>
			<persName><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N P</forename><surname>Vitaladevuni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsakalidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling temporal structure of decomposable motion segments for activity classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Trecvid 2011 -an overview of the goals, tasks, data, evaluation mechansims and metrics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Quenot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TRECVID 2011</title>
		<meeting>TRECVID 2011</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale image categorization with explicit data embedding</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast training of support vector machines using sequential minimal optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="185" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning latent temporal structure for complex event detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2012. 2, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Support vector machine learning for interdependent and structured output spaces</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiple kernels for object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient additive kernels via explicit feature maps</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="480" to="492" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiple kernel learning and the SMO algorithm</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ampornpunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A discriminative latent model of object classes and attributes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">View-invariant action recognition using latent kernelized structural SVM</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SUN database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kernel latent svm for visual recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning structural SVMs with latent variables</title>
		<author>
			<persName><forename type="first">C.-N</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
