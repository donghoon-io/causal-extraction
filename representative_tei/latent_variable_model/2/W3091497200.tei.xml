<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sample-Efficient Robot Motion Learning using Gaussian Process Latent Variable Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Juan</forename><forename type="middle">Antonio</forename><surname>Delgado-Guerrero</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adrià</forename><surname>Colomé</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Carme</forename><surname>Torras</surname></persName>
						</author>
						<title level="a" type="main">Sample-Efficient Robot Motion Learning using Gaussian Process Latent Variable Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robotic manipulators are reaching a state where we could see them in household environments in the following decade. Nevertheless, such robots need to be easy to instruct by lay people. This is why kinesthetic teaching has become very popular in recent years, in which the robot is taught a motion that is encoded as a parametric function -usually a Movement Primitive (MP)-. This approach produces trajectories that are usually suboptimal, and the robot needs to be able to improve them through trial-and-error. Such optimization is often done with Policy Search (PS) reinforcement learning, using a given reward function. PS algorithms can be classified as model-free, where neither the environment nor the reward function are modelled, or model-based, which can use a surrogate model of the reward function and/or a model for the dynamics of the task.</p><p>However, MPs can become very high-dimensional in terms of parameters, which constitute the search space, so their optimization often requires too many samples. In this paper, we assume we have a robot motion task characterized with an MP of which we cannot model the dynamics. We build a surrogate model for the reward function, that maps an MP parameter latent space (obtained through a Mutual-information-weighted Gaussian Process Latent Variable Model) into a reward. While we do not model the task dynamics, using mutual information to shrink the task space makes it more consistent with the reward and so the policy improvement is faster in terms of sample efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Learning through demonstration using kinesthetic teaching (see Fig. <ref type="figure" target="#fig_0">1</ref>) and then improving over executions through reinforcement learning has proved to be a successful approach in several situations <ref type="bibr" target="#b0">[1]</ref>. In Policy Search (PS), the initial demonstration is fitted into a parametric policy, which is a generative model. This model -a Movement Primitive (MP) -is used to generate samples that are evaluated with a given reward function capable of telling how good a robotic execution was. These samples and rewards are then used to obtain a new policy, which will usually maximize the expected value of the reward function given the policy. However, these approaches often require a high number of samples in order to learn sufficiently good motions in highdimensional parameter spaces. Therefore, model-based PS is often used <ref type="bibr" target="#b1">[2]</ref>, where either a surrogate function estimating This work was partially developed in the context of the project CLOTHILDE ("CLOTH manIpulation Learning from DEmonstrations"), which has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (Advanced Grant agreement No 741930). This work is also supported by the Spanish State Research Agency through the María de Maeztu Seal of Excellence to IRI MDM-2016-0656. 1 Institut de Robòtica i Informàtica Industrial (IRI), CSIC-UPC, Barcelona, Spain. [jdelgado,acolome,torras]@iri.upc.edu. reward values or a dynamics model of the environment/robot behaviour are built. Examples of model-based PS that use both models for the dynamics and the reward functions are PILCO <ref type="bibr" target="#b2">[3]</ref> and Black-DROPS <ref type="bibr" target="#b3">[4]</ref>.</p><p>In this paper, we assume that we are provided with an MP of a task which can be executed and evaluated, but whose dynamics can't be modelled. In such situations, building a surrogate model with Gaussian Processes for the reward function and using Upper Confidence Bound (UCB) optimization can help to converge faster to an improved solution. However, Bayesian Optimization (BO) techniques such as UCB do not scale easily to a high dimension, thus we need to perform dimensionality reduction in the MP parameter space so as to be able to carry out the optimization.</p><p>Dimensionality Reduction (DR) techniques have been used for learning robot motion <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> and human movements <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. These techniques led to significant improvements by learning tasks in latent spaces. However, the smallest dimensionality that can be used is strongly limited by the DR technique used, often a linear one. This is why, in this work, we resort to Gaussian Process Latent Variable Models (GPLVM) <ref type="bibr" target="#b12">[13]</ref> to perform such DR.</p><p>GPLVM has the advantage that it can fine-adjust the latent space dimension much more than other methods, without losing too much information. This is because of the way GPLVM is built: instead of looking for a projection from the original space to the latent space, it generalizes Dual Probabilistic Principal Component Analysis (DPPCA) and Fig. <ref type="figure">2</ref>: Global scheme of the proposed approach. Trajectories are evaluated (0) and fit <ref type="bibr" target="#b0">(1)</ref> into data vectors Y data as ProMP parameters, these data are weighted (2) dimension-wise with their mutual information with rewards. GPLVM (3) finds a latent data model with variables X data . These data, together with their rewards, are used to build a surrogate model of the reward function in the latent space, which can estimate the reward directly from the latent space. UCB exploration is used (4) to generate new samples in the latent space, which are then used to predict (5) their respective highdimensional state value and then executed <ref type="bibr" target="#b5">(6)</ref> as trajectories and evaluated. The outputs of these evaluations and their generators in the latent space are sent back to the surrogate model of the reward, which will be updated.</p><p>finds the values of the latent space variables that maximize the likelihood with respect to the data. This, together with the sampling efficiency of Gaussian Processes (GP), allows us to reduce the dimensionality of the search space by at least one order of magnitude, significantly speeding up the convergence to a better motion policy. Additionally, we use the Mutual Information (MI) <ref type="bibr" target="#b13">[14]</ref> between the data dimensions and the reward in order to weigh data and prioritize the fitting of the reward function instead of the MP parameters. Finally, we can use UCB to decide which samples to evaluate from the latent space, project them upwards, execute, evaluate and then update a surrogate model of the reward function that maps the latent space parameters to the original reward function, considered as a black box. Fig. <ref type="figure">2</ref> shows a schematic view of the proposed method.</p><p>This paper is organized as follows: Section II briefly introduces the concepts used in the paper, such as Probabilistic Movement Primitives (ProMPs) <ref type="bibr" target="#b14">[15]</ref>, Gaussian Processes (GP) <ref type="bibr" target="#b15">[16]</ref>, Gaussian Process Latent Variable Models (GPLVM) <ref type="bibr" target="#b12">[13]</ref>, Mutual Information (MI) <ref type="bibr" target="#b13">[14]</ref> and Upper Confidence Bound (UCB) <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Section III defines the proposed approach and Section IV presents the results obtained with this method. Section V concludes the paper and proposes future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES A. Probabilistic Movement Primitives</head><p>ProMPs <ref type="bibr" target="#b14">[15]</ref> are an overall approach to model, encode and learn a set of similar motion trajectories that present time-dependent variances over time. Given a number of basis functions per Degrees of Freedom (DoF), N f , ProMPs use time-dependent Gaussian kernels Φ t to encode the state of a trajectory, Φ t being the vector of normalized kernel basis functions (e.g., uniformly distributed Gaussian basis function over time). Thus, the position and/or velocity state vector z t can be represented as</p><formula xml:id="formula_0">z t " Ψ T t ω ` z ,<label>(1)</label></formula><p>where Ψ T t " I r b Φ T t , I r being the r-dimensional identity matrix, with r the number of DoFs of the robot, and Φ t an N f -dimensional column vector with the Gaussian kernels associated to one DoF at time t. Furthermore, z is a zero-mean Gaussian noise and the weights ω are also treated as random variables with a distribution ppωq " N pω|µ ω , Σ ω q. Given a set of demonstration trajectories τ n " tz n t u t"1..Nt , n " 1..N , this distribution can be fitted by obtaining the weights ω n of each demonstration through least squares. Afterwards, the parameters of the distribution θ " tµ ω , Σ ω , Σ z u, Σ z being the state noise covariance, are fitted by means of a maximum likelihood estimate, i.e., computing the mean and covariance of the samples tω 1 , ..., ω N u. Therefore, the probability of observing a z t is:</p><formula xml:id="formula_1">p pz t ; θq " N ´zt |Φ T t µ ω , Σ z `ΦT t Σ ω Φ t ¯(2)</formula><p>Due to the probabilistic representation, ProMPs can represent motion variability while keeping other MP properties such as rescalation and linear representation with respect to parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Gaussian Processes</head><p>A Gaussian Process (GP) <ref type="bibr" target="#b15">[16]</ref> f is an infinite-dimension stochastic process such that, for any finite set of indices x 1 , ..., x n , the random variables f px 1 q, ..., f px n q have joint Gaussian distributions. A GP is completely defined by its mean function m and covariance function k, which is symmetric and positive semi-definite:</p><formula xml:id="formula_2">f pxq " GPpmpxq, kpx, x 1 qq<label>(3)</label></formula><p>This is the generalization of the multivariate Gaussian distribution, over vectors, to an infinite-dimension stochastic process, over functions.</p><p>For convenience, the mean function is usually assumed to be the zero function, mpxq " 0. On the other hand, many possibilities can be found in the literature for defining the covariance function k. In this work, the squared exponential kernel, combined with a vector of automatic relevance determination, has been used for this purpose:</p><formula xml:id="formula_3">kpx i , x j q " σ 2 exp ˆ´1 2 px i ´xj q T diagp q ´2px i ´xj q ˙, (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where σ is the kernel variance parameter and is the lengthscale vector parameter. Furthermore, Gaussian processes are useful for regression models, y " f pxq ` . Thus, considering a set of N observations in matrix form: tX, Yu, with X P R N ˆQ, Y P R N ˆD , f can be used to predict the value of y N `1, given x N `1, and a noise Gaussian distribution . By leveraging the properties of f and the Gaussian identities, one can arrive to the expression:</p><formula xml:id="formula_5">P py N `1|X, Y, x N `1q " " N pµ t px N `1q, σ 2 t px N `1q `σ2 noise q,<label>(5)</label></formula><p>where</p><formula xml:id="formula_6">µ t px N `1q " k T rK `σ2 noise I N s ´1Y<label>(6)</label></formula><formula xml:id="formula_7">σ 2 t px N `1q " kpx N `1, x N `1q ´kT rK `σ2 noise I N s ´1k (7) K i,j " kpx i , x j q i, j " 1..N (8) k i " kpx N `1, x i q i " 1..N<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Gaussian Process Latent Variable Models</head><p>GPLVMs can be thought as the combination of GPs, explained in section II-B, and latent variable models. These models, as a type of feature extraction approach, relate through a set of parameters the observed data, Y P R N ˆD , with a set of so-called latent or hidden variables, X P R N ˆQ, being Q ! D for the purpose of dimensionality reduction. In this way, GPLVMs define a generative mapping from the latent space to the observation space, whose variable responses are said to be governed by the latent ones.</p><p>Besides, GPLVMs are formulated as a non-linear generalization of Probabilistic Principal Component Analysis (PPCA) <ref type="bibr" target="#b12">[13]</ref>. Specifically, GPLVMs arise directly from the formulation of Dual PPCA models, by replacing the inner product kernel with a non-linear covariance function. These approaches marginalize the parameters and optimize the latent variables. Therefore, the marginal likelihood function ppY|X, θq can be expressed as:</p><formula xml:id="formula_8">ppY|X, θq " D ź d"1 ppy :,d |Xq,<label>(10)</label></formula><p>where y :,d is the d´th column of the data matrix Y, corresponding to the d´th dimension, and y :,d |X " N py :,d |0, K `σ2 noise Iq Finally, the methodology for training the GPLVM is to find the maximum a posteriori estimate of X, maximizing Eq. ( <ref type="formula" target="#formula_8">10</ref>) with respect to the latent variable values and noise parameters.</p><p>GPLVM results in a projection from the higher dimensional space to the latent space, without providing a projection mapping by itself. Not restricting the mapping to a certain expression allows GPLVM to fine-tune the values of the latent space further. Besides, GPLVM does provide a tool for estimating the higher-dimensional variable y with respect to the lower dimensional x, by also using Eq. ( <ref type="formula" target="#formula_5">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Mutual Information</head><p>Mutual Information (MI) <ref type="bibr" target="#b13">[14]</ref> is a widespread nonnegative symmetric statistic for quantifying the degree of dependence between two random variables, being zero if and only if such variables are independent. In other words, it measures the amount of information shared by the variables, reflecting how much information about one of them may arise, from the knowledge of the other one.</p><p>Formally, this concept, which is grounded in information theory, is defined as the relative entropy between the joint probability, and the product distributions. Therefore, the mutual information IpX; Y q between two continuous random variables with joint density ppX, Y q is:</p><formula xml:id="formula_9">IpX; Y q " ş ş ppx, yq logp ppx,yq ppxqppyq qdxdy<label>(11)</label></formula><p>where ppxq and ppyq are the marginal probabilities with respect to X and Y , respectively. Normally, and so it happens in our case, the joint probability distribution is not known, being only available some sampled data of the form tx i , y i u iPrN s . In such cases, MI must be estimated from this data set. One straightforward approach for this purpose consists in partitioning the variables domain into bins of finite size, and approximate Eq. ( <ref type="formula" target="#formula_9">11</ref>), as in <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Bayesian Optimization and Upper Confidence Bound</head><p>Bayesian Optimization <ref type="bibr" target="#b17">[18]</ref> addresses the issue of finding the extrema of objective functions, which have no closedform expression, unknown derivatives and convexity, or are costly to evaluate, as in our case. In such cases, these approaches result efficient in terms of the number of function evaluations required to reach convergence <ref type="bibr" target="#b19">[20]</ref>.</p><p>These methods mainly consist of two components: a stochastic surrogate model fitting the target function f , and an acquisition function. On the one hand, the surrogate model takes advantage of the information of accumulated observations to generate a posterior distribution from a prior distribution, by means e.g. of Gaussian process, as in section II-B. The surrogate function will usually have more uncertainty (variance) on those unexplored areas or where its value is much non-deterministic.</p><p>On the other hand, the acquisition function, defined in a search space Ω X Ă R N ˆQ, uses the surrogate model to define the utility of evaluating each point of Ω X , giving more importance to points which are likely to have high objective function values, considering both the surrogate model prediction and its uncertainty.</p><p>Thus, the result of the maximization of the acquisition function is selected as the next point of the objective function to be evaluated, being the surrogate model updated accordingly right afterwards. In this way, the acquisition function is responsible to lead the search for the optimum of the objective function, in a trade-off frame between exploration and exploitation.</p><p>Upper Confidence Bound is a very intuitive <ref type="bibr" target="#b1">[2]</ref> and efficient <ref type="bibr" target="#b20">[21]</ref> acquisition function method, defined by:</p><formula xml:id="formula_10">UCBpxq " µpxq `κσpxq, (<label>12</label></formula><formula xml:id="formula_11">)</formula><p>where κ is a parameter (left to the user) which controls the importance of exploitation. The new samples are then generated as:</p><p>x sample " argmax xPΩ X UCBpxq <ref type="bibr" target="#b12">(13)</ref> This method results in choosing to sample the point which presents the highest mean plus κ standard deviations value on the surrogate function model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LEARNING IN LATENT SPACES THROUGH GPLVM</head><p>In this section, using the concepts defined in Sec. II we present an approach that is capable of learning highdimensional robot motion policies with very few samples.</p><p>Given a set of trajectories τ n " tz n j u, for trajectories n " 1..N and timesteps j " 1..N t , we will firstly fit those trajectories to MPs parameters by obtaining, for each trajectory, ω n that adjust the trajectory according to (1) (by using least-squares). These weight vectors, capable of fitting each individual trajectory given the previously fixed ProMP kernels, are then gathered together as the rows of our data matrix Y. Therefore, we are performing Dimensionality Reduction (DR) in the parameter space of the MP, rather than directly in the space of degrees of freedom of the robot, as other approaches do <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>. While performing DR in the space of degrees of freedom is advantageous in that it provides qualitative information that is directly interpretable, performing DR in the MP's parameter space permits to reduce further and fine-tune the dimensionality of the latent space <ref type="bibr" target="#b21">[22]</ref>, which is one of the goals of this work.</p><p>We will then use GPLVM on the data stored in matrix Y. However, while GPLVM was initially conceived for data visualization <ref type="bibr" target="#b22">[23]</ref>, we are aiming at further improving the model given the evaluations of each trajectory through a reward function:</p><formula xml:id="formula_12">R : R D ÝÑ R y Þ ÝÑ Rpyq. (<label>14</label></formula><formula xml:id="formula_13">)</formula><p>Therefore, when deriving the GPLVM, we will weigh the data by using Y MI " Y ¨MI 1{2 instead of data Y. Here, MI is the mutual information between the reward function and each column of the data -i.e., each parameter of the parameter vector ω of the MP. The MI will then be higher when there is a relation between such parameter (column) and the reward, and smaller when such parameter does not have much influence on the reward. Indeed, if we use this weighting M d " MIpy :,d , Rq on the log-likelihood, we can see that optimizing the weighted log-likelihood is equivalent to using such weighted data as input: which can be developed into:</p><formula xml:id="formula_14">log ppY|X, θq " log D ź d"1 ppy :,d |Xq M d ,<label>(15)</label></formula><formula xml:id="formula_15">log D ź d"1 ppy :,d |Xq M d " D ÿ d"1 M d " ´N 2 log2π ´log|K| ´1 2 y T :,d K ´1y T :,d  " " p N 2 log 2π ´log |K|q D ÿ d"1 M d ´D ÿ d"1 ´y:,d a M d ¯T K ´1 ´y:,d a M d ¯" " D ÿ d"1 M d ¨C ´D ÿ d"1 tr ´YMI T K ´1Y MI T ¯(16)</formula><p>where C " N 2 log 2π ´log |K| is a constant, Y MI " Y diagpM 1..D q, and diagpM 1..D q is a diagonal matrix with the values of the mutual information between column d of Y and the rewards at each d-th diagonal position, for d " 1..D.</p><p>Therefore, using such weighting on the input data for the GPLVM allows us to have a significantly smaller error when calculating the reward of the mean predicted reprojection for each training motion y n , while not losing much precision on such reprojection. In Fig. <ref type="figure" target="#fig_2">3</ref>, we see the reprojection error from a dataset using GPLVM with and without MI weighting for the training dataset, while 30% of data was left for validation purposes as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. There, we can observe that, while the reprojected training points present a slightly better fitting with MI, this result is worse for the validation datapoints. However, if we pay attention to the reward evaluation of the latent space datapoints, reprojected again to the higher-dimensional space, this gives an idea of how the rewards are affected by the coding/decoding operation. We see in Fig. <ref type="figure" target="#fig_4">5</ref> -in log10 scale -that there is a slight improvement on the training dataset by using MI, and at least one order of magnitude of improvement on the reward evaluations for the validation dataset, as observed in Fig. <ref type="figure" target="#fig_5">6</ref>. This means that using such MI to weigh the GPLVM input results in a model that more reliably represents the reward function, for which we will build a surrogate model.</p><p>Once having built a GPLVM to obtain a proper latent space representation of data, we will build a surrogate model that   estimates the reward function to be evaluated for the latentspace variables rather than in the original higher-dimensional space:</p><p>R :</p><formula xml:id="formula_16">R Q ÝÑ R x Þ ÝÑ Rpxq.<label>(17)</label></formula><p>In order to build the surrogate model for the latent space reward function, we will rely on a Gaussian Process (see Sec. II-B) that uses the latent space variables and their evaluations.</p><p>For learning and improving the trajectories, we will then use UCB (see Sec. II-E) to generate new samples. The UCB will be using the mean and variance provided by the GP Update f , µ k , and σ k with x k and Rpx k q 14: end for surrogate model. As already mentioned in Sec. II-E, UCB suggests to evaluate points, according to the maximization of Eq. ( <ref type="formula" target="#formula_10">12</ref>), in a certain search space Ω X .</p><p>Each sample x sample in the latent space will be used to estimate the corresponding value in the higher-dimensional space ŷpxq with Eq. ( <ref type="formula" target="#formula_5">5</ref>), which will be executed and evaluated, giving us the real value of the reward function R associated with x sample . This new sample, and the associated reward, will then be added to the surrogate model, that will be updated, before generating new samples. The process is repeated until convergence or a certain number of samples have been executed. Algorithm 1 displays the procedure of the proposed method, while Fig. <ref type="figure">2</ref> shows a more schematic view.</p><p>In this work, the search space Ω X has been defined as the minimum axis-aligned hyperrectangle that contains all latent data, as suggested in <ref type="bibr" target="#b17">[18]</ref>. In order to select candidates, the exploration parameter κ in Eq. ( <ref type="formula" target="#formula_10">12</ref>) has been fixed for simplification purposes (κ " 1q. However, less naive methods for selecting this parameter can be found, as in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTATION</head><p>In order to show the performance of the proposed method, we taught a Barret WAM robot 50 trajectories of feeding a mannequin (see Fig. <ref type="figure" target="#fig_0">1</ref>) with the robot going from one initial area (start position) to a final position area (mannequin head position), getting food from random positions on the table. The trajectories of the robot's end-effector can be seen in Fig. <ref type="figure" target="#fig_7">7</ref>. As input data, we used the position tx, y, zu of the robot's end-effector to represent the trajectory, and 11 Gaussians per Cartesian dimension, resulting in a 33-dimensional parameter space. Then, we placed a bowl at a particular position and we defined a reward function by calculating the Euclidean </p><p>In order to use our proposed method in Algorithm 1, we initially found the problem that the X-R regression model was identifying the latent data with both originally demonstrated motions' rewards and reprojected samples' rewards, with poor results. Therefore, as reprojected trajectories were similar, we then performed and evaluated the reprojection of 10 out of the 50 original demonstrations, by reprojecting their associated latent space point to the higher-dimensional space and evaluating it. Then, we built the reward's surrogate model with those 10 resampled motions, and iteratively updated it, with new samples and evaluations through UCB. The results, both with the MI weighting on the data for GPLVM and without it, are shown in Fig. <ref type="figure" target="#fig_8">8</ref>, where we see there is an improvement when using MI to correct data.</p><p>Moreover, we compared these two methods against a stateof-the-art policy search algorithm: Relative Entropy Policy Search (REPS) <ref type="bibr" target="#b24">[25]</ref>. REPS's optimization maximizes the expected reward of a stochastic policy, subject to obtaining a stochastic policy and keeping the Kullback-Leibler divergence between the old and new policy bounded by a certain parameter KL . Same scaled Fig. <ref type="figure" target="#fig_9">9</ref>, shows that, while REPS optimizes the trajectory and reaches a similar long-term reward, we see that it requires more samples than our proposed method for KL " 0.25, 1.0. We could check that trajectories with a high reward were desirable. In fact, log 10 p´Rq ą 4 implied millimetric precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have proposed an approach to use Bayesian optimization-based policy search to optimize robot motion trajectories with a surrogate model of the reward function. To this end, we have used GPLVM to reduce the parameter space of robot movement primitives to a much lower-dimensional space, e.g., going down from a 33dimensional parameter space to a 4-dimensional space. In  such latent space, optimizing the robot's policy is much more sample efficient, resulting in a much faster convergence to an optimal solution, also thanks to weighting the GPLVM with the mutual information of each dimension with respect to the rewards observed. The experimental results provided show that this optimization method converges much faster than other state-of-the-art methods, such as REPS.</p><p>While the initial latent space may restrict the search space for the policy learning, the prediction used to obtain the higher-dimensional variables from the latent space samples can be perturbed, so that exploration would also take place outside the initial Q-dimensional latent space, and the GPLVM could be updated after a certain number of samples. This is left for future work, as well as other approaches to adapt UCB to higher-dimensional spaces without sampling too much on the extremes of the sampling regions. Moreover, we plan to expand this work to make it adaptable to environmental changes that the robot might perceive, using GPLVM extensions such as Bayesian GPLVM <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, and to improve the discussion regarding the advantages of our approach with respect to other state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Kinesthetic teaching of a feeding task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Mean Euclidean norm of the error on the reprojection of each data trajectory within a training dataset, comparing GPLVM with MI-weighted GPLVM.</figDesc><graphic coords="4,322.20,54.28,244.87,122.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Mean Euclidean norm of the error on the reprojection of each data trajectory within a validation dataset, comparing GPLVM with MI-weighted GPLVM.</figDesc><graphic coords="5,65.52,54.00,239.44,119.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Mean Euclidean norm of the error (in log10 scale) on the evaluation of the reward function of the reprojected value of each data trajectory within a training dataset, comparing GPLVM with MI-weighted GPLVM.</figDesc><graphic coords="5,65.52,221.48,239.44,119.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Mean Euclidean norm of the error (in log10 scale) on the evaluation of the reward function of the reprojected value of each data trajectory within a validation dataset, comparing GPLVM with MI-weighted GPLVM.</figDesc><graphic coords="5,65.52,397.93,239.44,119.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Trajectory data τ n jl , n " 1..N , j " 1..N t , l " 1..N DOF Demonstrated trajectories rewards R n , n " 1..N GPLVM latent space dimension Q ProMPs' kernel matrix Ψ 1: Compute weights ω n with Eq.(1) 2: Assign Y n Ð Ý ω n 3: Compute MI: M d " MIpy :,d , Rq , d " 1..D 4: Reassign GPLVM input data Y MI Ð Ý Y ¨diagpM 1..D q 5: Perform GPLVM(Y MI n ) and obtain X n 6: Train X-R Regression Model R " GPpmp¨q, kp¨, ¨qq 7: Define Search region Ω X Ă R N ˆQ 8: for k " 1..N new do 9: Define UCBpxq " µ k´1 pxq `κσ k´1 pxq 10: Generate new sample x k " arg max xPΩ X UCBpxq 11: Project x k to ỹk with Eq. (5) 12: Execute ỹk and evaluate Rpỹ k px k qq 13:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Robot trajectories obtained through kinesthetic teaching in the 3D space. Trajectories start from the left side.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Samples and their real rewards (in log10 scale) with GPLVM and UCB learning, by using MI weighting and not. The initial 50 samples shown in green correspond to the training dataset. Note that the first 10 samples are re-sampled demonstrations in order to better fit the surrogate function.</figDesc><graphic coords="6,7.51,-1.41,307.51,230.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Samples and their real rewards (in log10 scale) with REPS with two different KL divergence bounds, KL " 0.25 and KL " 1.0. The initial 50 samples shown in green correspond to the training dataset.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on Policy Search for Robotics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Robotics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="142" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A survey on policy search algorithms for learning robot controllers in a handful of trials</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatzilygeroudis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vassiliades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stulp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calinon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Mouret</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02303</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PILCO: A model-based and data-efficient approach to policy search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on machine learning (ICML-11)</title>
		<meeting>the 28th International Conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Black-box data-efficient policy search for robotics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatzilygeroudis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goepp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vassiliades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Mouret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent spaces for dynamic movement primitives</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE-RAS Int. Conf. on Humanoid Robots</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="574" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dimensionality reduction for dynamic movement primitives and application to bimanual manipulation of clothes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Colomé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="602" to="615" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dimensionality reduction for probabilistic movement primitives</title>
		<author>
			<persName><forename type="first">A</forename><surname>Colomé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>IEEE-RAS Humanoid Robots</publisher>
			<biblScope unit="page" from="794" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Latent space policy search for robotics</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Luck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben Amor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots (IROS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1434" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative funcitonal analysis of human movements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sadamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kulic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1829" to="1839" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unvealing the principal modes of human upper limb movements through functional analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Averta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Della Santina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Felici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bicchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Robotics and AI</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Common functional principal components analysis: A new approach to analyzing human movement data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Coffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Donoghue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human movement science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1144" to="1166" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">FPCA Based Human-like Trajectory Generating</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic non-linear principal component analysis with Gaussian process latent variable models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1783" to="1816" />
			<date type="published" when="2005-11">Nov. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sample Estimate of the Entropy of a Random Vector</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Kozachenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Leonenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Probl. Peredachi Inf.</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="9" to="16" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Probabilistic movement primitives</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paraschos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2616" to="2624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gaussian process optimization in the bandit setting: no regret and experimental design</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1015" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Brochu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Cora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1012.2599</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating Mutual Information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kraskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stögbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="issue">69</biblScope>
			<biblScope unit="page">66138</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Application of Bayesian approach to numerical methods of global and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mockus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="347" to="365" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Entropy search for information-efficient global optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1809" to="1837" />
			<date type="published" when="2012-06">Jun. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dimensionality reduction in learning Gaussian mixture models of movement primitives for contextualized action selection and adaptation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Colomé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3922" to="3929" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<title level="m">Gaussian Process Latent Variable Models for Visualisation of High Dimensional&quot; International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Regret Bounds for Gaussian Process Bandit Problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grünewälder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="273" to="280" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mülling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Altün</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Relative Entropy Policy Search</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="182" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A review on Gaussian Process Latent Variable Models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAAI Transactions on Intelligence Technology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="366" to="376" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayesian Gaussian Process Latent Variable Model</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<idno>JMLR:W&amp;CP 9</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
