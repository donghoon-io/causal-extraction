<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Bayesian Latent Variable Model of User Preferences with Item Context</title>
				<funder ref="#_n6AmxXb">
					<orgName type="full">NRF</orgName>
				</funder>
				<funder>
					<orgName type="full">National Research Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Prime Minister&apos;s Office, Singapore</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aghiles</forename><surname>Salah</surname></persName>
							<email>asalah@smu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Systems</orgName>
								<orgName type="institution">Singapore Management University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hady</forename><forename type="middle">W</forename><surname>Lauw</surname></persName>
							<email>hadywlauw@smu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Systems</orgName>
								<orgName type="institution">Singapore Management University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Bayesian Latent Variable Model of User Preferences with Item Context</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Personalized recommendation has proven to be very promising in modeling the preference of users over items. However, most existing work in this context focuses primarily on modeling user-item interactions, which tend to be very sparse. We propose to further leverage the item-item relationships that may reflect various aspects of items that guide users' choices. Intuitively, items that occur within the same "context" (e.g., browsed in the same session, purchased in the same basket) are likely related in some latent aspect. Therefore, accounting for the item's context would complement the sparse user-item interactions by extending a user's preference to other items of similar aspects. To realize this intuition, we develop Collaborative Context Poisson Factorization (C 2 PF), a new Bayesian latent variable model that seamlessly integrates contextual relationships among items into a personalized recommendation approach. We further derive a scalable variational inference algorithm to fit C 2 PF to preference data. Empirical results on realworld datasets show evident performance improvements over strong factorization models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recommender systems are essential in guiding users as they navigate the myriads of options offered by modern applications. They rely chiefly on information about which items users have consumed-rated, purchased, etc.-in the past, which can be represented as user-item preference matrix. A predominant framework for recommendation is Matrix Factorization (MF) <ref type="bibr" target="#b11">[Mnih and Salakhutdinov, 2008;</ref><ref type="bibr" target="#b7">Hu et al., 2008;</ref><ref type="bibr" target="#b9">Koren et al., 2009]</ref>. The principle is to decompose the preference matrix into low-dimensional user and item latent factor matrices. The bilinear combination of user and item's latent factors can be used to predict unknown preferences.</p><p>Classical probabilistic MF models <ref type="bibr" target="#b11">[Mnih and Salakhutdinov, 2008]</ref> typically assume that a user's preference for an item is drawn from a Gaussian distribution centered at the inner product of their latent factor vectors. A distinct form of probabilistic MF, referred to as Poisson Factorization (PF) <ref type="bibr" target="#b3">[Canny, 2004;</ref><ref type="bibr" target="#b4">Cemgil, 2009]</ref>-where the Poisson distribution is substituted for the usual Gaussian-recently demonstrates natural aptness for modeling discrete data such as ratings or purchases commonly found in recommendation scenarios. As documented in <ref type="bibr" target="#b7">[Gopalan et al., 2015]</ref>, thanks to the properties of the Poisson distribution, PF realistically models user preferences, fits well to sparse data, enjoys scalable variational inference with closed-form updates, and substantially outperforms previous state-of-the-art MF models based on Gaussian likelihoods <ref type="bibr" target="#b11">[Mnih and Salakhutdinov, 2008;</ref><ref type="bibr" target="#b13">Shan and Banerjee, 2010;</ref><ref type="bibr" target="#b9">Koren et al., 2009]</ref>.</p><p>Nevertheless, existing PF models for recommendation are primarily focused on user-item interactions, which are very sparse. A PF model that relies on user-item interactions alone may not necessarily associate similar items with similar representations in the latent space. This is due to the fact that such items are not necessarily rated by exactly the same users. Furthermore, on average, any given user may have had the opportunity to rate or purchase relatively few items. Thus, modeling and generalizing her preference across the large vocabulary of items based on the few user-item interactions alone is an onerous task. Fortunately, there are auxiliary information that could augment user-item interactions. One that we focus on in this paper is the contextual relationships among items.</p><p>Real-world behavior data often hold clues on how items may be related to one another. For instance, items found in the same shopping cart may work well together, e.g., shirt and matching pair of jeans. Items clicked or viewed on an e-commerce site in the same session may be alternatives for a particular need, e.g., shopping for a phone. Songs found in the same playlist probably share a coherent theme, e.g., country music of the 90s. As an abstraction of such scenarios, we introduce the notion of "context", which may refer to a shopping cart, session, playlist, etc., depending on the specific problem instance. Intuitively, items that share similar contexts are implicitly related to one another in terms of some aspect that guides the choices one makes, such as specification, functionality, visual appearance, compatibility, etc. Note that contextual relatedness is not necessarily synonymous with feature-based similarity, e.g., shirt and jeans may share similar contexts, though they have different features.</p><p>The question is how to exploit and incorporate such contextual relationships among items within the PF framework. In this work, we posit that there could be two reasons that might explain the preference of a user for an item. The first reason is that the user's latent preference matches the latent attributes of the item of interest. The second reason is that the user's latent preference matches those of other related items, i.e., those sharing similar contexts with the item of interest.</p><p>Based on the above assumption, we propose Collaborative Context Poisson Factorization (C 2 PF), a new Bayesian latent variable model of user preferences which takes into account contextual relationships between items; this is our first contribution. Under C 2 PF, the preference of a user for an item is driven by two components. One component is the interaction between the user's and item's latent factors, as in traditional PF. The other component consists of interactions between the user's latent factor and item's context latent factors. In this paper, "the context set of an item i" refers to the set of items sharing the same contexts (e.g., browsing sessions) with i. As the second contribution, we derive a scalable variational algorithm for approximate posterior inference, to fit our model to preference data. As the third contribution, through extensive experiments on six real-world datasets, we demonstrate the benefits of leveraging item context; C 2 PF noticeably improves upon the performance of Poisson factorization models, especially in the sparse scenario in which users express few ratings only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Given the breadth of scope of recommender systems in the literature <ref type="bibr" target="#b2">[Bobadilla et al., 2013]</ref>, we focus on those closely related to ours, to sharpen and clarify our contributions.</p><p>Approaches based on matrix factorization rely primarily on user-item interactions <ref type="bibr" target="#b11">[Mnih and Salakhutdinov, 2008;</ref><ref type="bibr" target="#b7">Hu et al., 2008;</ref><ref type="bibr" target="#b9">Koren et al., 2009]</ref>. The sparsity of such information motivates the exploration of side information in several directions. On the users' side, these include leveraging social networks <ref type="bibr" target="#b10">[Ma et al., 2008;</ref><ref type="bibr" target="#b13">Zhou et al., 2012]</ref> or common features <ref type="bibr" target="#b12">[Rao et al., 2015]</ref> to bring related users' latent factors closer. On the items' side, these include exploiting item content [Wang and <ref type="bibr">Blei, 2011]</ref> or product taxonomy <ref type="bibr" target="#b9">[Koenigstein et al., 2011]</ref> to pull together item latent factors.</p><p>In this work, we focus on item-item relationships. The closest such work to ours is <ref type="bibr" target="#b12">[Park et al., 2017]</ref>, which proposes Matrix Co-Factorization (MCF) model. The latter falls into the large class of collective matrix factorization <ref type="bibr">[Singh and Gordon, 2008]</ref>, which consists in jointly decomposing multiple data matrices, user-item and item-item matrices in MCF, with shared latent factors. This is a widely used approach in the recommendation literature to exploit different sources of data. The model we propose is radically different from MCF. First, here we investigate another architecture for leveraging item relationships with new modeling perspectives. More precisely, as opposed to collective MF-based models like MCF, in our approach, the user-item preferences are the only observations being factorized, and the auxiliary information (item-item relationships) is embedded into the model's architecture. Second, MCF relies on the Gaussian distribution and uses stochastic gradient descent for learning, whereas our model builds on the Poisson distribution and enjoys scalable variational inference with closed-form updates. The benefits of our model are reflected in experiments.</p><p>In contrast, the CoFactor model <ref type="bibr" target="#b9">[Liang et al., 2016]</ref> induces item relationships from the same user-item matrix, instead of a separate item-item matrix. It is also an instance of collective MF, relies on a Gaussian likelihood, and designed specifically for implicit feedback data <ref type="bibr" target="#b7">[Hu et al., 2008]</ref>.</p><p>Our model is also a novel contribution to the body of work on recommendation models based on Poisson factorization <ref type="bibr" target="#b3">[Canny, 2004;</ref><ref type="bibr" target="#b4">Cemgil, 2009]</ref>. To our best knowledge, item context has not been explored within the PF framework.</p><p>Various other extensions of PF have been proposed. <ref type="bibr">Gopalan et al. [2014a]</ref> develop Bayesian non-parametric PF, which does not require the dimension of the latent factors to be specified in advance. <ref type="bibr">Gopalan et al. [2014b]</ref> propose Collaborative Topic Poisson Factorization (CTPF) to model both article contents and reader preferences. CTPF is also an instance of collective MF and could be viewed as a "Poisson" alternative to MCF. <ref type="bibr" target="#b5">Chaney et al. [2015]</ref> extend PF to incorporate social interactions. <ref type="bibr">Charlin et al. [2015]</ref> propose a model which accounts for user and item evolution over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Collaborative Context Poisson Factorization</head><p>This section describes Collaborative Context Poisson Factorization (C 2 PF), a Bayesian latent variable model of user preferences that accounts for the item's context.</p><p>Let X = (x ui ) denote the user-item preference matrix of size U × I, where x ui is the integer rating<ref type="foot" target="#foot_1">foot_1</ref> that user u gave to item i, or zero if no preference was expressed. Let C = (c ij ), of size I × J, be the item-context matrix, where c ij = 1 if item j belongs to the context of item i, and c ij = 0 otherwise. Subsequently, we refer to j as the context item, and to the set of items j for which c ij = 1 as the context of item i.</p><p>C 2 PF builds on Poisson factorization <ref type="bibr" target="#b6">[Friedman et al., 2001]</ref> to jointly model user preferences and leverage item's context. Formally, C 2 PF represents each user u with a vector of latent preferences θ u ∈ R K + , each item i with a vector of latent attributes β i ∈ R K + , and each context item j with a vector of latent attributes ξ j ∈ R K + . C 2 PF also assumes additional latent variables κ ij ∈ R + , one for each observed item-context pair, that we shall discuss shortly. Conditional on these latent variables, the user preferences x ui are assumed to come from a Poisson distribution as follows:</p><formula xml:id="formula_0">x ui ∼ Poisson(θ u β i + j c ij κ ij θ u ξ j ).</formula><p>(1)</p><p>The preference x ui is affected by both how well the user u's latent factors θ u matches the target item i's latent factors β i , and how well θ u matches the context latent factors ξ j of other items in i's context. Given that i may have multiple context items, it is natural to expect that different context items may affect i to different degrees. This is the intuition behind each variable κ ij , which represents the effect a context item j has on item i; we refer to these variables as the context effects.</p><p>The user latent preferences θ uk , item attributes β ik , context item attributes ξ jk and context effects κ ij are all drawn from Gamma distributions. The Gamma is an exponential family distribution over positive random variables, governed</p><formula xml:id="formula_1">x ui θ u β i ξ j κ ij γ i δ α θ α ξ α s κ α β j∈ c i U I Figure 1: C 2 PF as a graphical model, α = (α s , α r ), δ = (δ s , δ sc )</formula><p>, the super scripts s, r and sc stand respectively for shape, rate and scale parameters, ci is the context set of item i, i.e., ci = {j | cij = 1}. The context factors ξ j are shared across items, please refer to the generative process for details.</p><p>by a shape and rate parameters <ref type="bibr" target="#b0">[Bishop, 2006]</ref>, which is a conjugate prior to the Poisson distribution. Moreover, in real-world data the items have very unbalanced context sizes, i.e., some have many more items in their context set than others. To account for this diversity in context size, C 2 PF assumes additional priors on the rate parameter of the Gamma distribution over the context effects κ ij , which govern the average magnitude of the latter variables. This induces a hierarchical structure over the κ ij 's that makes it possible to model item context more realistically.</p><p>The graphical model of C 2 PF is depicted in Figure <ref type="figure">1</ref>, and its generative process is as follows:</p><p>1. Draw user preferences:</p><formula xml:id="formula_2">θ uk ∼ Gamma(α s θ , α r θ ). 2. Draw context item attributes: ξ jk ∼ Gamma(α s ξ , α r ξ ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">For each item i:</head><p>(a) Draw attributes: β ik ∼ Gamma(α s β , α r β ). (b) Draw the average magnitude of the context effects:</p><formula xml:id="formula_3">γ i ∼ Inverse-Gamma(δ s , δ sc ). (c) For each context item j of i draw a context effect: κ ij ∼ Gamma(α s κ , α s κ γi</formula><p>). 4. For each user-item pair (u,i) sample a preference:</p><formula xml:id="formula_4">x ui ∼ Poisson(θ u β i + j c ij κ ij θ u ξ j ).</formula><p>Note that C 2 PF includes as special cases other simpler models, such as the original Bayesian PF, which can be derived by modifying C 2 PF's specific components. In experiments, we consider some of such simpler variants of C 2 PF.</p><p>In practice, we are given X and C, and we are interested in reversing the above generative process so as to infer the posterior distribution of the latent user preferences, context effects, item and context item attributes, i.e., p(θ, β, ξ, κ|X, C). The latter will allow us to predict unknown ratings and generate recommendations. Once this posterior is fit, we can estimate the unknown ratings for each user-item pair (u, i) as follows:</p><formula xml:id="formula_5">xui = E(θ u β i |X, C) + j c ij E(κ ij θ u ξ j |X, C), (2)</formula><p>where the expectation is with respect to the posterior. These predicted values are then used to rank unrated items for each user so as to provide him/her with a recommendation list.</p><p>As in many Bayesian models, exact posterior inference is challenging, i.e., the exact inference of the above posterior is intractable. We therefore resort to approximate inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approximate Inference</head><p>Approximating the posterior is central to our work. In this section, we rely on variational inference and develop a scalable approximate inference algorithm for our model C 2 PF.</p><p>Variational Inference (VI) <ref type="bibr" target="#b0">[Bishop, 2006]</ref> is a widely used approach in statistical learning to fit complex Bayesian models. This approach transforms the inference problem into an optimization problem. The key idea is to define a new posterior distribution q, governed by its own free variational parameters ν, that is tractable to work with. The objective is then to find the value of the variational parameters ν * which indexes the distribution closest, in terms of the Kullback-Leibler (KL) divergence, to the exact posterior. Finally, the resulting variational distribution q(.|ν * ) is used as a surrogate to the true posterior in subsequent analysis.</p><p>We start by introducing an additional layer of auxiliary hidden variables, which leave the original model intact when marginalized out. For each observed rating x ui we add K latent variables z x uik ∼ Poisson(θ uk β ik ) and</p><formula xml:id="formula_6">K ×c i. latent vari- ables z c uijk ∼ Poisson(c ij κ ij θ uk ξ jk )</formula><p>, where c i. = j c ij . These auxiliary variables deterministically define the user preference. That is, x ui = k (z x uik + j z c uijk ). The latter result follows from the additive property of Poisson random variables <ref type="bibr" target="#b9">[Kingman, 1993]</ref>, i.e., if</p><formula xml:id="formula_7">x 1 ∼ Poisson(λ 1 ), x 2 ∼ Poisson(λ 2 ) and x = x 1 + x 2 , then x ∼ Poisson(λ 1 + λ 2 ).</formula><p>Note that when x ui is zero, z x uik and z c uijk are not random. This is why we consider these variables for the non-zero elements in X only. As we shall see, with these auxiliary variables in place our model is conditionally conjugate <ref type="bibr" target="#b6">[Ghahramani and Beal, 2001]</ref>, which will ease variational inference.</p><p>We now introduce our variational distribution q. We consider a mean-field family <ref type="bibr" target="#b8">[Jordan et al., 1999]</ref>, q(•|ν) = q(θ, β, ξ, κ, γ, Z x , Z c |ν), with a factorized form, i.e., the latent variables are assumed to be independent and each governed by its own variational parameters, as follows:</p><formula xml:id="formula_8">q(•|ν) = u,k q(θ uk |λ θ uk ) i,k q(β ik |λ β ik ) j,k q(ξ jk |λ ξ jk ) ij q(κ ij |λ κ ij ) cij i q(γ i |η i ) u,i q(z x ui , z c ui |φ ui ),<label>(3)</label></formula><p>where ν = {λ, η, φ}. The form of each factor in the above equation is specified by the corresponding complete conditional: the conditional distribution of each variable given the other variables and observations. That is, the factors over the Gamma variables are also Gamma distributions with variational parameters λ, e.g., λ θ uk = (λ θ,s uk , λ θ,r uk ), the superscripts s and r refer to the shape and rate parameters.The factors over the Inverse-Gamma varibles γ i are also Inverse-Gamma distribution with shape (s) and scale (sc) variational parameters, e.g., η i = (η s i , η sc i ). Finally, the factors over z ui = (z x ui , z c ui ) are Multinomial distributions with free parameters φ ui . The latter result follows from the fact that, the conditional distribution of a set of Poisson variables given their sum is a Multinomial; please refer to <ref type="bibr" target="#b4">[Cemgil, 2009]</ref> for details.</p><p>Given the variational family q, VI is to fit its parameters by solving the following optimization problem:</p><formula xml:id="formula_9">ν * = argmin ν KL(q(•|v)||p(•|X, C))<label>(4)</label></formula><p>This equation makes it clear how the observed data, X and C, enter the variational distribution. Once ν * is found, we use q(•|ν * ) as a surrogate to the true posterior to compute the prediction in (2) and subsequently make recommendations.</p><p>Coordinate ascent learning. We derive an efficient coordinate ascent mean-field algorithm to solve the optimization problem (4). The principle is to alternate the update of each variational parameter while holding the others fixed. Iterating on such updates is guaranteed to monotonically decrease the KL in (4), and to converge into a locally optimal solution. Thanks to the auxiliary variables, our model is conditionally conjugate. That is, each complete conditional is in the exponential family <ref type="bibr" target="#b6">[Ghahramani and Beal, 2001;</ref><ref type="bibr" target="#b1">Blei et al., 2017]</ref>. Thereby, each coordinate update can be performed in closed form, by setting the variational parameter equal to the expected natural parameter (w.r.t. q) of the corresponding complete conditional. This is indeed the optimal update for the variational parameter.</p><p>The complete conditional for the user preference, p(θ uk |•), is a Gamma with shape and rate parameters given by:</p><formula xml:id="formula_10">(α s θ + i z x uik + i,j z c uijk , α r θ + i β ik + i,j c ij κ ij ξ jk ). (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>The complete conditionals for the other Gamma variables are:</p><formula xml:id="formula_12">p(β ik |•) = Gamma(α s β + u z x uik , α r β + u θ uk ). (6) p(ξ jk |•) = Gamma(α s ξ + u,i z c uijk , α r ξ + u,i c ij κ ij θ uk ). (7) p(κ ij |•) = Gamma(α s κ + u,k z c uijk , α s κ γi + u,k θ uk ξ jk ).<label>(8)</label></formula><p>The complete conditional for the average intensity of the context effect is as follows:</p><formula xml:id="formula_13">p(γ i |•) = Inv-Gamma(δ s + α s κ j c ij , δ sc + α s κ j κ ij ). (9)</formula><p>The complete conditional for the auxiliary variables is:</p><formula xml:id="formula_14">p(z ui |θ, β, ξ, κ, C, X) = Multinomial(x ui , log p ui ),<label>(10)</label></formula><p>where z ui = (z x ui , z c ui ), p ui = (p x ui , p c ui ) is a point on the (K + K × c i. )-simplex, and for all k, j: p x uik ∝ θ uk β ik and p c uijk ∝ c ij κ ij θ uk ξ jk .</p><p>The expected natural parameters (w.r.t. q) of these conditionals give the optimal updates for the variational parameters, e.g., the update for Gamma variational parameter λ θ uk is obtained by taking expectation of (5), which yields:</p><formula xml:id="formula_15">λ θ,s uk = α s θ + i x ui (φ x uik + j φ c uijk ), λ θ,r uk = α r θ + i λ β,s ik λ β,r ik + i,j c ij λ ξ,s jk λ ξ,r jk λ κ,s ij λ κ,r ij ,<label>(11)</label></formula><p>where we have used the standard results about the expectation of Gamma and Multinomial random variables. That is, if θ ∼ Gamma(λ s , λ r ), then E(θ) = λ s λ r , and if z ui ∼ Multinomial(x ui , φ ui ), then the expectation of the k th component of z ui is E(z uik ) = x ui φ uik . Using the standard results of the expectation of the log of a Gamma variable, i.e., E(log θ) = ψ(λ s ) -log λ r with ψ(•) denoting the digamma function, the updates for the components of the variational Multinomial parameter φ ui = (φ x ui , φ c ui ) are:</p><formula xml:id="formula_16">φ x uik ∝ exp ψ(λ θ,s uk ) -log λ θ,r uk + ψ(λ β,s ik ) -log λ β,r ik .<label>(12)</label></formula><p>φ c uijk ∝ exp (E(log θ uk ) + E(log ξ jk ) + E(log κ ij )) , (13) for brevity we did not develop the expectations in (13).</p><p>The updates for the remaining variational parameters can be derived in the same way. The full variational inference for C 2 PF is depicted in Algorithm 1.</p><p>Algorithm 1 Variational inference for C 2 PF.</p><p>Input: X, C, K, δ , α θ , α β , α ξ , α s κ Output: The set of variational parameters ν * Steps: 1. Initialization: η s i = δ s + ci. × α s κ , randomly initialize the remaining Gamma variational parameters λ s , λ r repeat 2. For each observed preference xui, update the variational Multinomial parameter φ ui using equations ( <ref type="formula" target="#formula_16">12</ref>) and ( <ref type="formula">13</ref>).</p><p>3. Update the user related parameters, ∀u, k:</p><formula xml:id="formula_17">λ θ,s uk = α s θ + i xuiφ x uik + i,j xuiφ c uijk λ θ,r uk = α r θ + i λ β,s ik λ β,r ik + i,j cij λ ξ,s jk λ ξ,r jk λ κ,s ij λ κ,r ij</formula><p>4. Update the item related parameters, ∀i, k:</p><formula xml:id="formula_18">λ β,s ik = α s β + u xuiφ x uik ; λ β,r ik = α r β + u λ θ,s uk λ θ,r uk</formula><p>5. Update the context item related parameters, ∀j, k:</p><formula xml:id="formula_19">λ ξ,s jk = α s ξ + u,i xuiφ c uijk ; λ ξ,r jk = α r ξ + u,i cij λ θ,s uk λ θ,r uk λ κ,s ij λ κ,r ij</formula><p>6. Update the context effects, ∀i, j, such that cij &gt; 0:</p><formula xml:id="formula_20">η sc i = δ sc + α s κ j λ κ,s ij λ κ,r ij ; λ κ,s ij = α s κ + u,k xuiφ c uijk λ κ,r ij = α s κ η s i η sc i + u,k λ θ,s uk λ θ,r uk λ ξ,s jk λ ξ,r jk until convergence</formula><p>Efficient implementation. A key property of the variational C 2 PF algorithm is efficiency. The operations involving users and items need to be carried out for only the non-zero elements in X and C. Furthermore, we can avoid explicitly computing and storing the Multinomial parameters φ. We need to store only the following matrices, L θ = (exp{E q (log θ uk )}), L β = (exp{E q (log β ik )}), L ξ = (exp{E q (log ξ jk )}) and L κ = (exp{E q (log κ ij )}). We can then use these quantities directly in the updates of the variational shape parameters.</p><p>Computational time complexity. The Proposition below shows that the computational complexity of Algorithm 1 scales linearly with the number of non-zero entries in X and C. In practice X and C are extremely sparse, and Algorithm 1 converges within 100 iterations. Furthermore, the updates of the variational parameters are trivially parallelizable across users and items, hence our variational inference for C 2 PF can easily scale to large datasets. Proposition 1. Let nz x and nz c denote respectively the number of non-zero in X and C. The computational complexity per iteration of Algorithm 1 is O(K • (nz x + nz c + U + I)).</p><p>Proof. The computation bottleneck of Algorithm 1 is with the update blocks 3 to 6. The computational complexity of updating λ θ,s uk is O(nz u x ), such that nz u x is the number of ratings expressed by user u. This complexity holds since the sum over j can be precomputed once for each i,k and stored in a I × K matrix, the total cost of this operation is O(K • nz c ). The complexity of updating all λ θ,r uk parameters is O(K • (I + U + nz c )). Therefore, the computational complexity of block 3 is O(K • (nz x + nz c + U + I)).</p><p>Similarly, we can show that the complexity of block 4 is O(K • (nz x + U + I)) and that of blocks 5 and 6 is O(K • (nz c + nz x + U + J)). Putting it all together, the complexity per iteration of Algorithm 1 is O(K • (nz x + nz c + U + I)), where we have assumed that I is of the same order as J.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Study</head><p>Our objective is to study the impact of item context, and our modeling assumptions, on personalized recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We use six datasets from Amazon.com 2 , provided by <ref type="bibr" target="#b10">McAuley et al.;</ref><ref type="bibr">McAuley et al. [2015b;</ref><ref type="bibr">2015a]</ref>. These datasets include both the user-item preferences and the "Also Viewed" lists that we treat as the item contexts. We preprocess all datasets so that each user (resp. item) has at least ten (resp. two) ratings, and the sets of items and context items are identical. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparative Models</head><p>We benchmark our model, C 2 PF, against strong comparable generative factorization models.</p><p>• MCF: Matrix Co-Factorization <ref type="bibr" target="#b12">[Park et al., 2017]</ref>, which incorporates item-to-item relationships into Gaussian MF. • PF: Bayesian Poisson Factorization <ref type="bibr" target="#b7">[Gopalan et al., 2015]</ref> which arises as a special case from our model without the item context. Therefore, we can effectively assess the impact of the item context by comparing C 2 PF to PF. • CTPF: Collaborative Topic Poisson Factorization <ref type="bibr">[Gopalan et al., 2014b</ref>] is a co-factorization approach that jointly models user preferences and item topics. It can also be used to leverage the item context by substituting the itemcontext matrix C for the item-word matrix. • CoCTPF: Content-only CTPF <ref type="bibr">[Gopalan et al., 2014b]</ref> is a variant of CTPF without the document topic offsets; please refer to <ref type="bibr">[Gopalan et al., 2014b]</ref> for details.</p><p>2 <ref type="url" target="http://jmcauley.ucsd.edu/data/amazon/">http://jmcauley.ucsd.edu/data/amazon/</ref> Note that the above baselines have been found to perform better than several other models on the task of item recommendation. To examine the contributions of our modeling choices, we also include the results for two simplified variants of C 2 PF.</p><p>• rC 2 PF: reduced C 2 PF that drops the item factors β, resulting in a simpler model where only the context part in (1) is responsible for explaining the user preferences x ui<ref type="foot" target="#foot_2">foot_2</ref> ,</p><p>• tC 2 PF: tied C 2 PF that constrains the context factors ξ to be the same as the item factors β, that is ξ i = β i for all i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Settings</head><p>For each dataset, we randomly select 80% of the ratings as training data and the remaining 20% as test data. Random selection is carried out five times independently on each dataset. The average performance over the five samples is reported as the final result.</p><p>For most experiments, we set the number of latent components K to 100. Later, we will also vary K and indeed find 100 to be a good trade-off between accuracy and model complexity. To encourage sparse latent representations, we set α θ = α β = α ξ = (0.3, 0.3) -resulting in exponentially shaped Gamma distributions with mean equal to 1. We further set δ = (2, 5) and α s κ = 2, fixing the prior mean over the context effects to 0.5. Note that we set α s κ &gt; 1 to avoid sparse distributions over the κ ij variables and thereby encourage C 2 PF to rely on item's context to explain user preferences. For an illustration, please refer to Figure <ref type="figure">2</ref> in <ref type="bibr" target="#b4">[Cemgil, 2009]</ref>. We initialize the Gamma variational parameters, λ s and λ r , to a small random perturbation of the corresponding prior parameters. In order for the comparisons to be fair, we use the same initial parameters for all PF-based models, where it is possible.</p><p>To set the different hyperparameters of MCF, we follow the same strategy, grid search, as in <ref type="bibr" target="#b12">[Park et al., 2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation Metrics</head><p>We assess the recommendation accuracy on a set of held-out items-the test set. We retain four widely used measures for top-M recommendation, namely the Normalized Discount Cumulative Gain (nDCG), Mean Reciprocal Rank (MRR), Precision@M (P@M ) and Recall@M (R@M ), where M is the number of items in the recommendation list <ref type="bibr" target="#b2">[Bobadilla et al., 2013]</ref>. Intuitively, nDCG and MRR measures the raking quality of a model, while Precision@M and Recall@M assess the quality of a user's top-M recommendation list. These measures vary from 0.0 to 1.0 (higher is better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Empirical Results and Discussion.</head><p>Table <ref type="table" target="#tab_2">2</ref> depicts the average performances of the various competing models in terms of different metrics, over all datasets. In order to ease interpretation, we provide another presentation of Recall@20 in Figure <ref type="figure">2</ref>-the results are consistent across all metrics.</p><p>We note that C 2 PF, and its variants, substantially outperforms the other competing models on all datasets and across 0.0394 0.0387 0.0392 P@10 0.0028 0.0019 0.0021 0.0075 0.0087 0.0087 0.0093 R@10 0.0151 0.0088 0.0094 0.0351 0.0417 0.0415 0.0439 P@20 0.0022 0.0015 0.0016 0.0058 0.0069 0.0066 0.0070 R@20 0.0228 0.0132 0.0143 0.0566 0.0645 0.0633 0.0673 Sports nDCG 0.1122 0.1179 0.1189 0.1398 0.1512 0.1521 0.1547 MRR 0.0071 0.0122 0.0119 0.0297 0.0390 0.0409 0.0427 P@10 0.0015 0.0022 0.0026 0.0067 0.0091 0.0093 0.0096 R@10 0.0061 0.0083 0.0101 0.0266 0.0361 0.0374 0.0393 P@20 0.0011 0.0018 0.0022 0.0054 0.0072 0.0073 0.0076 R@20 0.0096 0.0143 0.0170 0.0431 0.0574 0.0591 0.0613</p><p>Pet Supplies nDCG 0.1201 0.1288 0.1317 0.1585 0.1627 0.1628 0.1678 MRR 0.0136 0.0207 0.0237 0.0441 0.0501 0.0516 0.0562 P@10 0.0028 0.0039 0.0048 0.0103 0.0110 0.0113 0.0122 R@10 0.0147 0.0184 0.0219 0.0499 0.0526 0.0550 0.0597 P@20 0.0022 0.0029 0.0034 0.0079 0.0089 0.0091 0.0095 R@20 0.0237 0.0271 0.0314 0.0752 0.0862 0.0897 0.0917 Clothing nDCG 0.0896 0.0885 0.0896 0.0961 0.1014 0.1046 0.1061 MRR 0.0031 0.0018 0.0032 0.0065 0.0118 0.0122 0.0130 P@10 0.0006 0.0003 0.0006 0.0013 0.0020 0.0023 0.0026 R@10 0.0028 0.0014 0.0028 0.0062 0.0111 0.0114 0.0120 P@20 0.0004 0.0002 0.0005 0.0010 0.0016 0.0019 0.0021 R@20 0.0043 0.0022 0.0041 0.0097 0.0175 0.0184 0.0190 all measures. Recall that without the item context information C 2 PF degenerates to the basic PF. We can therefore attribute the performance improvements reached by C 2 PF, relative to PF, to the modeling of the item context. The importance of the item context is also strongly supported by the high performance of rC 2 PF relative to PF, though rC 2 PF relies solely on item's context to make recommendations. Overall, the results from Table <ref type="table" target="#tab_2">2</ref> suggest that the item context underlies different aspects of items that explain the user behaviour. To gain further insights into the performance of the proposed model and the impact of our modeling choices, we now delve into specific research questions.</p><p>• Q1. How important is the Poisson distribution?</p><p>We observe that even though PF does not leverage the relationships among items, it still outperforms MCF in most cases. This provides empirical evidence that the Poisson distribution is a better alternative to Gaussian in modeling user preferences. CoCTPF arises as a special case from CTPF without the item offset. Surprisingly, the former performs better than the latter. A careful investigation reveals that the magnitudes of the item offsets (noted in the original paper) tend to be bigger than those of the shared item attributes θ. This means that, in CTPF, the item offsets, which are specific to the user-item interaction component, dominate the prediction of unknown preferences (please refer to equation 1 in <ref type="bibr">[Gopalan et al., 2014b]</ref>).</p><p>• Q4. When does C 2 PF offer the most improvements?</p><p>In Figure <ref type="figure" target="#fig_0">3</ref>, we report the performances, in terms of Recall@20, of C 2 PF and PF, on users with different number of ratings. C 2 PF consistently achieves the best performance over different scenarios. Though this may be datadependent, C 2 PF seems to provide the most improvement on users with few ratings. The relative difference between C 2 PF and PF tends to decrease with more ratings. It is challenging to infer good user representations when there is a lack of information in the preference matrix. By leveraging additional signals from items' contexts, C 2 PF mitigates this lack of information.</p><p>• Q5. What is the impact of the number of factors on the performance of C 2</p><p>In Figure <ref type="figure" target="#fig_1">4</ref>, we report the performance of different models, on Office, over different K. C 2 PF consistently outperforms the competing methods. It is not very sensitive to the value of K and seems to provide better performances when K ≥ 100. Because the complexity of the models increases with K, we recommend to set the number of factors to 100, which is a good tradeoff between recommendation quality and model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion &amp; Perspectives</head><p>Based on the assumption that items sharing similar contexts are related in some latent aspect that guides one's choices, we develop Collaborative Context Poisson Factorization (C 2 PF), a Bayesian latent factor model of user preferences which takes into account the contextual relationships among items. Under C 2 PF, not only do items (through latent attributes) contribute to explain user behaviour, but so do their contexts. Empirical results on real-world datasets show that C 2 PF noticeably improves the performance of Poisson factorization models, especially in the sparse scenario in which users express few ratings, suggesting that the item context underlies aspects of items that can explain the user preferences.</p><p>A flexible model with strong theoretical foundations, C 2 PF can be extended in several directions. For instance, it would be interesting to extend C 2 PF to account for user-user social relationships to further alleviate the sparsity issue. Another possible line of future work is to compose C 2 PF with other graphical models. For instance, one could combine C 2 PF and CTPF to jointly model item's context and textual content.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of average Recall@20 on users with different number of ratings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Model performance (Recall@20) as a function of K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Table 1 describes the resulting datasets. Statistics of the Datasets.</figDesc><table><row><cell>Datasets</cell><cell></cell><cell></cell><cell cols="2">Characteristics</cell><cell></cell></row><row><cell></cell><cell cols="4">#Users #Items #Ratings nz X (%)</cell><cell>#nz C nz C (%)</cell></row><row><cell>Office</cell><cell>3,703</cell><cell>6,523</cell><cell>53,282</cell><cell>0.22</cell><cell>108,466 0.25</cell></row><row><cell>Grocery</cell><cell cols="3">8,938 22,890 148,735</cell><cell>0.07</cell><cell>480,300 0.09</cell></row><row><cell>Automotive</cell><cell cols="2">7,280 15,635</cell><cell>63,477</cell><cell>0.05</cell><cell>365,634 0.15</cell></row><row><cell>Sports</cell><cell cols="3">19,049 24,095 211,582</cell><cell>0.04</cell><cell>531,148 0.09</cell></row><row><cell cols="4">Pet Supplies 16,462 20,049 164,017</cell><cell>0.05</cell><cell>631,102 0.16</cell></row><row><cell>Clothing</cell><cell cols="3">41,809 97,619 420,377</cell><cell cols="2">0.01 1,080,442 0.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average recommendation accuracy over different datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>• Q2. How important are the C 2 PF's modeling assumptions? CTPF and CoCTPF offer alternative PF-based architectures to C 2 PF for leveraging item's context, with different modeling assumptions. More precisely, CTPF and CoCTPF fall into the class of collective matrix factorization, and consist in jointly factorizing the user-item X and item-context C matrices, with shared item factors. This is a popular strategy in the recommendation literature to model different sources of data. The proposed models, C 2 PF and its variants, substantially outperform CTPF and CoCTPF in all cases, demonstrating the benefits of the assumptions behind C 2 PF. • Q3. Why does CoCTPF performs better than CTPF?</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Other user-item interactions indicative of preferences are also possible, e.g., number of clicks.Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>This variant considers only items with non-empty context sets since the rate of the Poisson cannot be zero Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research is supported by the <rs type="funder">National Research Foundation</rs>, <rs type="funder">Prime Minister's Office, Singapore</rs> under its <rs type="funder">NRF</rs> <rs type="programName">Fellowship Programme</rs> (Award No. <rs type="grantNumber">NRF-NRFF2016-07</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_n6AmxXb">
					<idno type="grant-number">NRF-NRFF2016-07</idno>
					<orgName type="program" subtype="full">Fellowship Programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">;</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recommender systems survey</title>
		<author>
			<persName><surname>Bobadilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-based systems</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="109" to="132" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gap: a factor model for discrete data</title>
		<author>
			<persName><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International ACM SIGIR Conference</title>
		<meeting>the 27th International ACM SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="122" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ali Taylan Cemgil. Bayesian inference for nonnegative matrix factorisation models</title>
		<author>
			<persName><surname>Cemgil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Neuroscience</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A probabilistic model for using social networks in personalized item recommendation</title>
		<author>
			<persName><surname>Chaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM RecSys Conference</title>
		<meeting>the 9th ACM RecSys Conference</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015. 2015</date>
			<biblScope unit="page" from="155" to="162" />
		</imprint>
	</monogr>
	<note>Proceedings of ACM RecSys Conference</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Propagation algorithms for variational bayesian learning</title>
		<author>
			<persName><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<publisher>Ghahramani and Beal</publisher>
			<date type="published" when="2001">2001. 2001. 2001. 2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="507" to="513" />
		</imprint>
	</monogr>
	<note>Springer series in statistics New York</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Yifan Hu, Yehuda Koren, and Chris Volinsky. Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName><surname>Gopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2014. 2014. 2014. 2014. 2015. 2008. 2008</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
	<note>IEEE ICDM</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName><forename type="first">Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Factorization meets the item embedding: Regularizing matrix factorization with item co-occurrence</title>
		<author>
			<persName><forename type="first">John</forename><surname>Kingman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Kingman</surname></persName>
		</author>
		<author>
			<persName><surname>Koenigstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM RecSys Conference</title>
		<editor>
			<persName><forename type="first">Gideon</forename><surname>Koenigstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yehuda</forename><surname>Dror</surname></persName>
		</editor>
		<editor>
			<persName><surname>Koren</surname></persName>
		</editor>
		<meeting>the 10th ACM RecSys Conference</meeting>
		<imprint>
			<publisher>Poisson processes. Wiley Online Library</publisher>
			<date type="published" when="1993">1993. 1993. 2011. 2011. 2009. 2009. 2016. 2016</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
	<note>ACM RecSys</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Imagebased recommendations on styles and substitutes</title>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD Conference</title>
		<meeting>the 21th ACM SIGKDD Conference</meeting>
		<imprint>
			<publisher>Julian McAuley</publisher>
			<date type="published" when="2008">2008. 2008. 2015. 2015. 2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
	<note>Sorec: social recommendation using probabilistic matrix factorization. In Proceedings of the 38th International ACM SIGIR Conference</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName><forename type="first">Salakhutdinov</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Collaborative filtering with graph information: Consistency and scalable methods</title>
		<author>
			<persName><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2017. 2017. 2015. 2015</date>
			<biblScope unit="page" from="2107" to="2115" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Singh and Gordon, 2008] Ajit P Singh and Geoffrey J Gordon. Relational learning via collective matrix factorization</title>
		<author>
			<persName><forename type="first">Shan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Banerjee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hanhuai</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM International Conference on Data mining</title>
		<meeting>the SIAM International Conference on Data mining</meeting>
		<imprint>
			<date type="published" when="2008">2010. 2010. 2008. 2011. 2012. 2012</date>
			<biblScope unit="page" from="403" to="414" />
		</imprint>
	</monogr>
	<note>Proceedings of the 14th ACM SIGKDD International Conference</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
