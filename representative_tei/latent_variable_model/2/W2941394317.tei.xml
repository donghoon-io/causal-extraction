<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data-efficient Learning of Robotic Clothing Assistance using Bayesian Gaussian Process Latent Variable Model</title>
				<funder ref="#_TNJBCTx">
					<orgName type="full">ImPACT Program of Council for Science, Technology, and Innovation (Cabinet Office, Government of Japan</orgName>
				</funder>
				<funder>
					<orgName type="full">NAIST Big</orgName>
				</funder>
				<funder ref="#_zjbyxjH #_rnNPDXb #_XDV9MZN">
					<orgName type="full">Japan Society for the Promotion of Science</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-04-14">April 14, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Nishanth</forename><surname>Koganti</surname></persName>
							<email>nishanth-k@is.naist.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Science and Technology</orgName>
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<country>Japan;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tomohiro</forename><surname>Shibata</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Life Sciences and Systems Engineering</orgName>
								<orgName type="institution">Kyushu Institute of Technology</orgName>
								<address>
									<country>Japan;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tomoya</forename><surname>Tamei</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Mathematical and Data Science Center</orgName>
								<orgName type="institution">Kobe University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kazushi</forename><surname>Ikeda</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Science and Technology</orgName>
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<country>Japan;</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data-efficient Learning of Robotic Clothing Assistance using Bayesian Gaussian Process Latent Variable Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-04-14">April 14, 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Clothing Assistance</term>
					<term>Gaussian Processes</term>
					<term>Latent Variable Models</term>
					<term>Policy Search Reinforcement Learning</term>
					<term>Learning from Demonstration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motor-skill learning for complex robotic tasks is a challenging problem due to the high task variability. Robotic clothing assistance is one such challenging problem that can greatly improve the quality-oflife for the elderly and disabled. In this study, we propose a data-efficient representation to encode task-specific motor-skills of the robot using Bayesian nonparametric latent variable models. The effectivity of the proposed motor-skill representation is demonstrated in two ways: 1) through a real-time controller that can be used as a tool for learning from demonstration to impart novel skills to the robot and 2) by demonstrating that policy search reinforcement learning in such a task-specific latent space outperforms learning in the high-dimensional joint configuration space of the robot. We implement our proposed framework in a practical setting with a dual-arm robot performing clothing assistance tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, there has been a tremendous increase in the elderly population of the world leading to a shortage of caregivers and therapists. To address this issue, we need a greater presence of ICT and assistive robotics. One of the major problems that arise due to aging is the loss of motor functions to perform dexterous tasks such as putting on clothes. Patients with bone and muscle related diseases find it difficult to move their arms beyond a certain range and are not able to wear clothes by themselves. Similarly, patients with conditions like Alzheimers disease face difficulty in performing fine movements such as putting on buttons and are dependent on the caregiver. Although in most cases, the patient is not entirely dependent on the caregiver and can perform the tasks requiring some assistance. Service robots could ideally perform this task.</p><p>On the other hand, an area of active research in the domain of robotics is motor-skills learning that enables robots to perform complex tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. However, existing methods require a large number of interactions to learn the optimal behavior which is not suitable in practical scenarios such as in the field of assistive robotics. Clothing assistance is one such task which is a necessity in the daily life of the elderly and disabled people. Design of a practical framework involves cloth state estimation in real-time and a learning framework that can detect and adapt to various failure scenarios. A promising approach is to formulate robotic clothing assistance as a reinforcement learning (RL) problem wherein the robot learns to recover from failure scenarios and adapt to new settings from experience.</p><p>An RL framework for clothing assistance was proposed by Tamei et al. <ref type="bibr" target="#b2">[3]</ref> where a dual-arm robot was the agent, and a mannequin was used as the subject. Policy search is performed in a kinematic space which is considerably high dimensional for a 7 degree of freedom (DOF) dual-arm robot. The policy was represented using via-points extracted using a minimum jerk criterion. To ensure tractable learning time, policy update was done using the finite difference policy gradient algorithm applied to a single via-point of a single joint in each robot arm. This severely constrained the generalization capability to very different environmental settings such as significant changes in the subject's posture or using a different clothing article.</p><p>In this study, we propose an efficient representation of motor-skills that relies on the use of Bayesian Gaussian Process Latent Variable Model (BGPLVM) <ref type="bibr" target="#b3">[4]</ref>. BGPLVM is capable of learning a data-efficient latent space for clothing tasks performed by a dual-arm robot. We present two applications with the BGPLVM latent space as shown in Figure <ref type="figure" target="#fig_1">2</ref>. Firstly, a userfriendly interface to impart novel motor-skills to a bulky dual-arm robot. We present a real-time controller with input from the latent space that can be used as a user-friendly tool for LfD. Second, we propose a novel RL framework where the BGPLVM latent space is used as a search space in combination with an unbiased policy gradient algorithm, Policy learning by Weighted Exploration with Returns (PoWER) <ref type="bibr" target="#b4">[5]</ref>. We demonstrate that the learned latent space generates robot trajectories that maintain task space constraints required for clothing tasks. We apply our proposed method in a practical setting of robotic clothing assistance as shown in Figure <ref type="figure" target="#fig_0">1</ref>. The experimental results indicate a promising representation with reinforcement learning that can be used for robotic tasks with complex motor-skills.</p><p>The rest of the paper is structured as follows. Section 2 provides an overview of related studies. The proposed framework is presented in Section 3. Section 4 includes experimental results and Section 6 concludes the paper with directions for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we present the related work in three subcategories: application of Latent Variable Models (LVM) to robotics, motor-skills learning, and robotic clothing assistance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Latent Variable Models for Robotics</head><p>Robotic tasks have high-dimensional observations obtained using noisy sensors. One approach is to use dimensionality reduction and obtain a low-dimensional manifold that efficiently captures the task representation as well as variability in different settings. Principal Component Analysis (PCA) <ref type="bibr" target="#b5">[6]</ref> is commonly used with applications including robot localization <ref type="bibr" target="#b6">[7]</ref>, LfD in humanoid robots <ref type="bibr" target="#b7">[8]</ref>, robotic hand grasp planing <ref type="bibr" target="#b8">[9]</ref> and EMG-based robot arm control <ref type="bibr" target="#b9">[10]</ref>.</p><p>With the increasing popularity of deep learning, there have been several studies that use neural network based autoencoders in various robotics applications. Watter et al. <ref type="bibr" target="#b10">[11]</ref> used variational autoencoders and a locally linear approximation of nonlinear dynamical systems to generate high-dimensional image trajectories given current observations. Van Hoof et al. <ref type="bibr" target="#b11">[12]</ref> proposed the use of autoencoders to learn a low dimensional feature space of high dimensional tactile and visual information. Linear models such as PCA are not suitable to handle the nonlinear dynamics of clothing articles. On the other hand, methods such as variational autoencoders are capable of learning complex non-linear mappings but are not sample-efficient.</p><p>There have been several studies that use Gaussian process latent variable model (GPLVM) and their extensions to perform dimensionality reduction in various settings. Shon et al. <ref type="bibr" target="#b12">[13]</ref> proposed an LVM formulation to learn a shared latent space between human joints and humanoid degrees of freedom which enabled robotic imitation of the human poses. Ko et al. <ref type="bibr" target="#b13">[14]</ref> extended Bayes filters using Gaussian process (GP) based observation, prediction models and applied it to various robotic applications. Wang et al. <ref type="bibr" target="#b14">[15]</ref> proposed a dynamical extension to GPLVM and applied it to predict human intention during a human-robot interaction task. Nakamura et al. <ref type="bibr" target="#b15">[16]</ref> performed segmentation of human motion capture data using a hidden semi Markov model where the emission distributions were modeled using a GP. Koskinopoulou et al. <ref type="bibr" target="#b16">[17]</ref> performed LfD using GPLVM where the mapping is learned using human demonstrations. Some of these studies use GPLVM which relies on a Maximum-A-Posteriori (MAP) estimate of the latent space, and so the models tend to overfit to the training data and do not generalize well to drastic changes in the task settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Motor-skill Learning using RL and LVMs</head><p>In recent years, RL has been successfully applied to various robotic tasks <ref type="bibr" target="#b0">[1]</ref>. Several studies have proposed motor skills learning specifically for cloth manipulation to handle the inherent non-rigidity. Doumanoglou et al. <ref type="bibr" target="#b17">[18]</ref> formulated a Partially Observable Markov Decision Process (POMDP) framework for cloth unfolding along with the use of random forests for cloth classification. Huang et al. <ref type="bibr" target="#b18">[19]</ref> used depth and appearance features for detecting graspable regions and generated trajectories through a warp function to bring clothes to the desired configuration. Yang et al. <ref type="bibr" target="#b19">[20]</ref> proposed a deep learning based framework for autonomous cloth folding. They relied on a convolutional autoencoder for task-specific feature extraction from raw images and a time-delayed neural network to learn the dynamics of cloth folding.</p><p>Balaguer et al. <ref type="bibr" target="#b20">[21]</ref> proposed a reinforcement learning framework that exploits the dynamics of clothing articles to perform a high momentum folding task. They used a motion capture system for real-time cloth state estimation as the clothing article did not undergo much occlusion. Another closely related application is the manipulation of deformable objects such as ropes and soft objects. Monso et al. <ref type="bibr" target="#b21">[22]</ref> proposed a probabilistic motion planning framework for cloth separation by formulating the problem as a POMDP to handle uncertainty during manipulation. They defined a low-dimensional state representation to ensure fast and efficient learning of the task. Lee et al. <ref type="bibr" target="#b22">[23]</ref> proposed a force-based manipulation framework for applications such as knot tying. They rely on the use of non-rigid registration to modify expert demonstrations to the current environmental setting. Hu et al. <ref type="bibr" target="#b23">[24]</ref> proposed a visual-servo based control framework using online GP regression where uninformative observations are removed for real-time control.</p><p>Few studies have handled sample-efficiency explicitly which is crucial for our assistive robotics task. Deisenroth et al. <ref type="bibr" target="#b24">[25]</ref> proposed a model-based RL framework that relies on a GP transition model along with explicit incorporation of uncertainty in long-term predictions. This framework suffers from computational intractability for high-dimensional applications. RL usually suffer from the curse of dimensionality and a possible solution is the use of dimensionality reduction (DR). Some studies use DR as a preprocessing step and perform RL in the reduced search space <ref type="bibr" target="#b25">[26]</ref>. Others inherently combine DR and RL wherein the dimensionality reduction is motivated by the rewards obtained during the learning phase <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. However, these studies either use linear models for DR limiting the modeling capability or rely on a MAP estimate of the latent space which tends to overfit to the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Robotic Clothing Assistance</head><p>Recently, there have been several promising studies that tackle this challenging problem. For people with cognitive impairments, dressing assistance mainly involves providing social cues rather than physical support. Burleson et al. <ref type="bibr" target="#b28">[29]</ref> proposed a framework to detect abnormal dressing states by tracking clothing articles that have fiducial markers. Orr et al. <ref type="bibr" target="#b29">[30]</ref> developed a multi-agent system that relied on sensor fusion to provide recommendations for clothing when the users were about to leave their house. Klee et al. <ref type="bibr" target="#b30">[31]</ref> proposed a clothing assistance framework to communicate and coordinate with a human to complete clothing tasks.</p><p>Conducting clothing assistance experiments and future real-world implementations need to be safe and efficient. One approach to sidestep this problem is to develop a realistic simulator and use this as a testbed. Clegg et al. <ref type="bibr" target="#b31">[32]</ref> developed a framework to synthesize dressing motion performed by animated human characters. Erickson et al. <ref type="bibr" target="#b32">[33]</ref> relied on simulations of dressing a human hand in a shirt to infer the forces felt by the human from the end effector forces of</p><formula xml:id="formula_0">x f Φ y Figure 3</formula><p>. Graphical model of BGPLVM <ref type="bibr" target="#b3">[4]</ref>. Φ are the model hyper parameters that need to be optimized.</p><p>the robot. They generated large amounts of data and used it to train deep neural networks for force estimation. Yu et al. <ref type="bibr" target="#b33">[34]</ref> used haptic information obtained from a simulation of dressing assistance to train a classifier that could predict failure scenarios in a real-world implementation of the same task. Yamazaki et al. <ref type="bibr" target="#b34">[35]</ref> proposed an end-to-end framework for bottom-dressing that relies on proprioceptive and visual information to detect failure scenarios. Colomé et al. <ref type="bibr" target="#b35">[36]</ref> relied on a friction-based complaint controller to perform a scarf wrapping task. Kapusta et al. <ref type="bibr" target="#b36">[37]</ref> used hidden Markov models to detect failure scenarios for the task where a linear actuator clothes human subjects with a hospital gown. Gao et al. <ref type="bibr" target="#b37">[38]</ref> proposed a path optimization approach that relies on robot proprioceptive information for assisting human users with dressing. These studies have handled various aspects of the problem. However, they do not handle the scenarios where there is a tight coupling between the human and clothing article. Furthermore, the emphasis was not motor-skills learning, and they usually perform point-to-point motion planning.</p><p>In our previous work, we demonstrated the applicability of BGPLVM for real-time cloth state estimation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>. We performed multi-view learning using Manifold Relevance Determination (MRD) <ref type="bibr" target="#b40">[41]</ref> and perform accurate human-cloth relationship estimation from very highdimensional and noisy point cloud data. In this study, we propose the use of BGPLVM to learn a low-dimensional latent space for encoding motor-skills. The focus of this study is to design an efficient action representation for RL of clothing assistance. The advantage of BGPLVM is that it relies on variational inference to learn a posterior distribution on the latent space rather than a MAP estimate as in GPLVM <ref type="bibr" target="#b41">[42]</ref>. This avoids overfitting to the training data, thereby, improving the generalization capability of the model to unseen environmental settings. We further explore various feature representations and their effect on the resultant latent space, specific to clothing assistance tasks. We implement our framework on a practical setting of clothing assistance as formulated by Tamei et al. <ref type="bibr" target="#b2">[3]</ref> that involves tight coupling between human and the clothing article along with high variability in policy depending on the task settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we present our formulation to encode motor-skills and a generic learning framework that relies on this formulation. Section 3.1 provides the mathematical formulation of BG-PLVM. Section 3.2 provides the application of BGPLVM to encode motor-skills for clothing assistance task. Finally, Section 3.3 describes the reinforcement learning framework which relies on BGPLVM latent space to perform policy search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bayesian Nonparametric Dimensionality Reduction</head><p>BGPLVM is a dimensionality reduction technique proposed by Titsias et al. <ref type="bibr" target="#b3">[4]</ref> derived from the generative model shown in Figure <ref type="figure">3</ref>. The model assumes that the high-dimensional observed data,</p><formula xml:id="formula_1">Y = {y i ∈ R D } N n=1 , is generated from low-dimensional latent inputs X = {x i ∈ R q } N n=1</formula><p>April 14, 2019 Advanced Robotics paper through a noisy process,</p><formula xml:id="formula_2">y i = f (x i ) + , ∼ N (0, β -1 I) (1)</formula><p>where β is the inverse variance for noise random variable . The mapping function f is modeled using a GP which has several useful properties. GP performs data-efficient learning as it can learn informative mappings with few samples and capable of learning complex mappings using non-linear kernel functions. The conditional likelihood for the generative model is given by:</p><formula xml:id="formula_3">p(Y|X, Φ) = D d=1 N (Y :,d |0, K + β -1 I) (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where Y is the observed data and X, Φ are the unknown latent points, hyperparameters for the GP mapping that need to be inferred. The conditional likelihood is factorized with respect to the output dimensionality D with independent GP mapping for each output dimension d. K is the kernel matrix constructed from the latent points. In the generative model, the latent positions need to be marginalized out for having a purely Bayesian treatment,</p><formula xml:id="formula_5">p(X) = N n=1 N (x n |0, I), p(Y|Φ) = p(Y|X, Φ)p(X)dX<label>(3)</label></formula><p>Variational inference is used to compute a tractable lower bound for the marginalization thereby inferring a posterior distribution on the latent space which avoids the problem of overfitting.</p><p>For performing the automatic model selection of the latent space dimensionality, the Automatic Relevance Determination (ARD) Kernel <ref type="bibr" target="#b42">[43]</ref> can be used in the GP mapping:</p><formula xml:id="formula_6">k ard (x i , x j ) = σ 2 ard exp - 1 2 q k=1 α q (x i,k -x j,k ) 2<label>(4)</label></formula><p>The ARD weights α q describe the relevance of each dimension and zero weight indicates complete irrelevance. Maximizing the marginal likelihood w.r.t. these weights allows the inference of latent space dimensionality. The inference for unseen test data can now be performed through a Bayesian formulation instead of relying on a MAP estimation of the latent space. The predictive distribution is given by the ratio of two marginal likelihoods, both of which can be approximated using the variational inference technique:</p><formula xml:id="formula_7">p(y * |Y) = p(y * , Y|x * , X)p(x * , X)dXdx * p(Y|X)p(X)dX<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Motor-skills Representation using BGPLVM</head><p>Motor-skills for robotic clothing assistance is given by joint angle trajectories of a dual-arm robot that lie in a high-dimensional space. The robot also has to maintain several task space constraints such as coupling with a clothing article along with safe human-robot interaction as shown in Figure <ref type="figure" target="#fig_1">2</ref>. To address these problems, we propose the use of BGPLVM for learning a low-dimensional latent space through a nonlinear mapping to the kinematic space. There are several motivating factors for this choice. The Bayesian treatment avoids overfitting to the training data, and the use of an ARD kernel in the GP mapping leads to the inference of the inherent dimensionality to encode motor-skills.</p><p>In this section, we present the formulation used to apply BGPLVM to clothing assistance skills. We consider the clothing task where a dual-arm robot dresses a soft mannequin in a T-shirt which is initially resting on the mannequin's arms. The training dataset is given by human demonstration through kinesthetic movement while controlling the robot under gravity compensation mode. The BGPLVM model learns a mapping from the low dimensional latent space to the robot kinematic space such that a trajectory of points in the latent space generates a trajectory on the dual-arm robot. The GP mapping leads to data-efficient learning, thereby requiring few demonstrations, possibly one, for task generalization.</p><p>The latent features learned by BGPLVM depend on the input dataset provided. We consider two alternate representations i.e. 1) kinematic representation (JA) given by joint angles of 7 DoF dual-arm robot D K = 14 and 2) task space representation (EE) given by the end-effector pose of both arms with Cartesian position P X , P Y , P Z ∈ R 3 and orientation with quaternion representation O X , O Y , O Z , O ω ∈ R 4 forming a 14-dimensional space D T = 14. We set the dimension of the latent space as q = 10 for all our experiments. However, the dimensionality is eventually inferred through the training of the ARD kernel weights as explained in Section 3.1.</p><p>There can be several types of failure scenarios when the robot performs clothing tasks. To recover from these failures, not only is the trajectory of the robot important, but also the speed of execution. Typically such trajectories are provided to the robot through kinesthetic demonstrations where a person holds the robot arms and physical moves them. Imparting these skills through kinesthetic movement can be difficult for inexperienced users especially when the robot is bulky and with specific kinematic constraints. Inexperienced users could impart noisy demonstrations which could lead to suboptimal performance and in some cases unsafe movements as well.</p><p>To address this problem, we implemented a user interface that uses BGPLVM. Our interface involves interaction with a BGPLVM latent space trained using expert demonstrations displayed in a 2D space. A real-time controller is implemented as shown in Figure <ref type="figure" target="#fig_9">12</ref> that maps the cursor coordinates from the BGPLVM latent space to a pose of dual arm 7-DoF robot. This interface can be used as a tool for LfD where the necessary clothing skills are imparted to the robot by using cursor control over the latent space. This can be a user-friendly interface where the user need not physically interact with the robot and can control the robot through intuitive interfaces such as a mouse or even a touchpad.</p><p>Real-time implementation of the controller was designed using the Robot Operating System April 14, 2019 Advanced Robotics paper (ROS) software framework. A pipeline was formed where cursor coordinates in the latent space were mapped through BGPLVM to generate a robot joint angle pose which was provided as input to the low-level controllers of the robot. Using this interface, a path traced in the latent space converts to a trajectory performed on the dual-arm robot in real-time. The implementation of the latent space controller along with the user-friendly interface is provided as an open source repository at <ref type="url" target="https://github.com/ShibataLab/cloth_assist_framework">https://github.com/ShibataLab/cloth_assist_framework</ref> for further reference. An example scenario for this interface is care-givers imparting motor-skills to assistive robots in a real-world health care facility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Latent Space Reinforcement Learning</head><p>In this section, we formulate a policy search framework in the BGPLVM latent space. The objective is to learn a high-dimensional robot trajectory for performing the clothing task on an unseen posture of the mannequin by searching within this latent space. Firstly, a dataset of successful clothing assistance trajectories is used to train a latent space that encodes the motorskills. Each of the trajectories is now transformed into a sequence of points in the latent space forming latent space trajectories. The policy search is performed using PoWER <ref type="bibr" target="#b4">[5]</ref> which is a commonly used policy search algorithm. PoWER can be considered reward-weighted recombination of previous experience. It has an Expectation-Maximization like formulation where the policy is given by a weighted summation of basis functions with state-dependent exploration noise π(a t |s t , t) = θ T µ(s, t) + (µ(s, t)) and the policy parameters are updated as follows:</p><formula xml:id="formula_8">θ = θ + E T t=1 W(s t , t)Q π (s t , a t , t) -1 E T t=1 W(s t , t) t Q π (s t , a t , t)<label>(6)</label></formula><p>where θ are the policy parameters, W (s t , t) is the basis function matrix and t is a time-dependent exploration noise term. Q π (s t , a t , t) is the value function which is approximated using an unbiased estimator, Qπ (s, a, t) = T t=t r(s t, a t, t). We consider Dynamic Movement Primitives (DMP) <ref type="bibr" target="#b43">[44]</ref> as policy representation. It is the combination of a point attractor dynamical system, and a non-linear forcing term (f (s)) learned using Locally Weighted Regression (LWR) <ref type="bibr" target="#b44">[45]</ref>:</p><formula xml:id="formula_9">τ ẍ = K(g -x) -D ẋ + (g -x 0 )f, f (s) = i w i ψ i (s)s i ψ i (s)</formula><p>, where τ ṡ = -αs</p><p>where K, D are the coefficients for the point attractor system, x 0 , g are the initial and goal positions. The non-linear forcing term is described using another canonical system (s) given by a weighted summation (w i ) of basis functions (ψ i (s)). The weight parameters, w i , are treated as the policy parameters in this study where these weights need to be estimated to generate the desired robot trajectory. We train a DMP on the latent points corresponding to one of the training trajectory, and the LWR weight coefficients are used as policy parameters which are modified to generalize to unseen environmental settings. The cost function for policy improvement is designed in the high-dimensional action space. In the current setting, we obtain a demonstration for the unseen posture and consider this as an optimized trajectory that needs to be learned by the DMP controller. The optimized trajectory is efficiently encoded by via-points extracted using the minimum jerk criterion <ref type="bibr" target="#b45">[46]</ref> that the robot needs to pass through. The cost function is given by the sum of all errors between the current policy and the desired via-points:</p><formula xml:id="formula_11">R(τ ) = ndims i=1 nvia j=1 V i,j -x recons i (t i,j ) 2<label>(8)</label></formula><p>where R(τ ) is the total reward for trajectory τ , V i,j is the j th via-point of i th dimension and x recons i (t) is the value at time t for i th dimension of reconstructed trajectory. The number of via-points n via was estimated such that the reconstruction error for the generated trajectory is lower than a user-specified threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In this section, we present the performance of our proposed framework in a practical setting of robotic clothing assistance. The experimental setup includes Baxter research robot, a soft mannequin as the subject and a T-shirt as the clothing article. We consider the performance of the clothing and unclothing tasks of the T-shirt. These tasks follow different dynamics and running a clothing demonstration backward for unclothing usually leads to failure. The evaluation dataset contains demonstrations performed by three experienced users interacting with the Baxter robot. For each demonstrator, clothing and unclothing demonstrations were recorded for 6 different postures of the mannequin wherein the pair of shoulder elevation and head elevations were as follows {(65 o , 30 o ), ( <ref type="formula">70</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison of Latent Variable Models</head><p>In this experiment, we inspect the motor-skills, i.e. latent features learned by BGPLVM and evaluate the predictive performance in comparison to other LVMs such as Principal Component Analysis (PCA) <ref type="bibr" target="#b5">[6]</ref> and GPLVM <ref type="bibr" target="#b41">[42]</ref>. PCA is a linear dimensionality reduction technique where the observations Y are assumed to be generated from the following generative process:</p><formula xml:id="formula_12">Y = WX (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>where X is the unobserved latent position and W ∈ R D×d comprises of a set of orthonormal basis vectors that maximizes the scatter of latent points. On the other hand, GPLVM has a similar generative process to BGPLVM given in Equation <ref type="formula">1</ref>. However, the unknown latent positions X are evaluated using Maximum-A-Posteriori (MAP) estimation along with jointly maximizing the hyperparameters. The objective function for optimizing the latent positions X is derived from the conditional log-likelihood given in Equation <ref type="formula" target="#formula_3">2</ref>:</p><formula xml:id="formula_14">L = - DN 2 ln(2π) - D 2 ln |K| - 1 2 tr(K -1 YY T ) (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>Firstly, we consider the dataset of demonstrations from the evaluation dataset for both clothing and unclothing tasks. For each task, six-fold cross-validation was performed where demonstrations for four postures were used as training data and for two postures were used as test data in each fold. BGPLVM models were trained for both the kinematic space (JA) and task space (EE) representations. The ARD weights for BGPLVM on training resulted in the varying number of active latent dimensions for different feature representations as shown in Figure <ref type="figure" target="#fig_5">6</ref>. For the clothing task, the JA representation resulted in three active latent dimensions on average and two dimensions for the EE representation. This implies that the encoded motor-skills varies depending on the input observations and can be an essential design consideration depending on the task.</p><p>It was observed that the latent space varied for different LVMs. Latent spaces learned for the clothing task, and JA representation is shown in Figure <ref type="figure" target="#fig_3">5</ref>. The grayscale background for GPLVM and BGPLVM indicate the predictive variance of the GP mapping which can be considered as a measure of uncertainty of the model. In general, all the LVMs resulted in latent trajectories for the training data which correspond to the demonstrations. There is a considerable difference in the predictive variance where BGPLVM is more confident than the GPLVM model assigning lower predictive variance over a larger region and hence effectively using the latent space.</p><p>BGPLVM can be used to automatically infer the dimensionality of the latent space through the ARD kernel weights. This can be very useful for complex tasks such as clothing assistance where it is difficult to estimate the inherent dimensionality given high dimensional trajectories of the robot performing the task. Although GPLVM also uses an ARD kernel, the reconstruction error remains high on using only the active dimensions as shown in Figure <ref type="figure" target="#fig_6">7</ref>. This indicates that Bayesian inference proves useful in jointly learning the model hyperparameters along with optimizing the latent positions. On the other hand, MAP inference used by GPLVM leads to poor learning of the kernel hyperparameters such as the ARD relevance weights. The EE representation required more latent dimensions in comparison to JA representation, this could indicate that the variability to adapt to different postures could not be captured in the EE representation.</p><p>The predictive performance of BGPLVM was evaluated by comparing the reconstruction error with two other LVM, i.e. PCA and GPLVM. For each demonstrator, six-fold cross-validation was performed where four demonstrations were used as training data. The remaining two demonstrations (Test Pose) along with demonstrations from an unseen demonstrator (Test Demo) were used as test data. To evaluate generalization capability, we evaluated reconstruction error given by comparing the input data and the reconstructed data from the latent points corresponding to the input, Err = y org -y pred ,</p><formula xml:id="formula_16">y pred = f model (f -1 model (y org ))<label>(11)</label></formula><p>where y org is an input sample from dataset, y pred is the predicted value after reconstruction and f model is the forward mapping from latent space to observation space. The results for reconstruction error are provided in Figure <ref type="figure">8</ref>. We evaluated Normalized Root Mean Square Error (NRMSE) and Pearson correlation as the metrics. The Wilcoxon signed rank sum test <ref type="bibr" target="#b46">[47]</ref> was used to assess the statistical significance and the p-value was evaluated for a one-sided test. It can be seen that BGPLVM has the best predictive performance which can be considered as a measure of generalization capability of BGPLVM latent space to unseen environmental settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation of Latent Space Controller</head><p>In this experiment, we evaluate the latent space controller for imparting motor-skills to the robot. For the evaluation, BGPLVM models were trained on kinesthetic demonstrations of performing clothing and unclothing task for a given posture of the mannequin. The model is used as an interface to control the robot and reproduce the task by performing cursor control on the latent space of the demonstration. An instance of performing cursor control to generate a different trajectory is shown in Figure <ref type="figure">9</ref>. It can be seen that tracing a trajectory different from the expert demonstration used to train the model still leads to the successful execution of the clothing assistance trajectory. Furthermore, the latent trajectory for test data that minimizes reconstruction error could be discontinuous as shown in Figure <ref type="figure" target="#fig_3">5</ref> but tracing a smooth trajectory with cursor control still leads to successful execution. A video demonstration with the exploration of the latent dimensions is also available at <ref type="url" target="https://youtu.be/bul5lq44ru8">https://youtu.be/bul5lq44ru8</ref>. The latent dimensions learned by BGPLVM capture a specific aspect of the clothing motorskills. For the joint angle scenario, the most significant dimension captured the horizontal motion of the arms along the mannequin while maintaining the constraints for clothing. The second dimension captured various vertical motions of pulling up the T-shirt in the beginning and pulling it down along the torso at the end. The third dimension captured variations in joint configurations across the demonstrations and could explain the constraint of safe human-robot interaction. This makes the interface intuitive to the users for planning a desired alteration to the robot trajectory. The robot trajectories generated corresponding to a trajectory along each latent dimension is shown in Figures <ref type="figure" target="#fig_0">10,</ref><ref type="figure" target="#fig_0">11</ref>.</p><p>Five subjects without prior experience of interacting with the robot were asked to use the interface to reproduce the tasks. The joint angles along with proprioceptive information were recorded while the interface was used. In general, the subjects were able to reproduce the clothing demonstration even when the latent trajectory was different from the training latent points. The subjects had fast learning curves with the execution time for performing the demonstration decreasing drastically within five trials of using the interface as shown in Figure <ref type="figure" target="#fig_9">12a</ref>. There were also instances where the execution was slower as the subject felt that the T-shirt was stuck on the mannequin's head. This indicates that the controller can be used to modulate the dynamics of the task depending on the current setting. A video demonstration of an inexperienced subject interacting with the real-time controller is available at <ref type="url" target="https://youtu.be/bul5lq44ru8">https://youtu.be/bul5lq44ru8</ref>. The subjects also learned to modulate the dynamics of performing the task wherein crucial parts were performed slowly and parts without much human-cloth interaction being performed quickly. Figure <ref type="figure" target="#fig_9">12b</ref> shows the modulation in the dynamics of performing the task for one of the subjects. The time normalized trajectories for a single joint of the robot is shown for five trials of using the interface along with the original kinesthetic demonstration. For the regions indicated in the blue squares, it can be seen that the trajectories through the interface vary drastically from the original demonstration. This indicates that novel motor-skills for performing complex tasks can be imparted by inexperienced users as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Latent Space Reinforcement Learning</head><p>In this section, we demonstrate the effectivity of performing policy search in the BGPLVM latent space by comparison with search in the high-dimensional action space and latent spaces learned using PCA and GPLVM. PoWER algorithm was implemented with the same state and reward representations, and only the policy search space varied depending on the scenario. For each demonstrator, six-fold cross-validation was performed where three demonstrations of the clothing task were used as training data for learning latent spaces. The latent space of each LVM was used as a search space for reinforcement learning to learn the policy corresponding to the remaining three unseen demonstrations. Policy search was run for 1000 iterations where 20 rollouts with the best overall rewards were used to update the policy in each iteration. The initial policy was generated from DMP that is fit on a latent space trajectory of one of the human demonstrations used to train the latent space.</p><p>The performance of using BGPLVM as a search space was exhaustively evaluated by considering the complete dataset. The average learning curves over the entire dataset are shown in Figure <ref type="figure" target="#fig_10">13</ref>. It can be seen that BGPLVM scenario outperforms all other search spaces. This indicates that BGPLVM captures the necessary motor-skills most efficiently and also has the best generalizability in comparison to other LVMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In Section 4, we demonstrated the applicability of using BGPLVM to learn meaningful latent representations for the task of robotic clothing assistance. The results in Section 4.1 indicate that non-linear mapping and Bayesian inference are necessary for the most efficient encoding of motor-skills. PCA relies on a linear model thereby constraining its capability. GPLVM has performance similar to PCA as it relies on a MAP estimate of the latent space and it seems to be overfitting to the training data. Overfitting of the GP hyperparameters could lead to worse performance on using a non-linear mapping. GPLVM could achieve better performance over PCA on the careful tuning of the hyperparameter values. On the other hand, BGPLVM relies on full Bayesian inference where a posterior distribution on the latent space is learned. This leads to superior performance as a model of the appropriate complexity can be learned automatically, and the advantage of using a non-linear mapping can also be utilized. This validates our hypothesis that encoding motor-skills using Bayesian nonparametric latent variable models is a useful parametrization for learning. The results in Section 4.2 indicate that BGPLVM is also able to learn interpretable latent features which are very useful for skill-transfer. In the coming years, there will be an increasing presence of assistive robots in healthcare facilities, and it is essential to design user-friendly interfaces for the caregivers to adapt to such technologies. The interface presented in this study could have several extensions making it suitable for other assistive tasks. It could include the pose of the assisted person which can be obtained using skeleton tracking. Section 4.3 presents the feasibility of using BGPLVM latent space as a search space for reinforcement learning. Policy search is performed using the PoWER algorithm which has been shown to perform well in several applications. Currently, dimensionality reduction (DR) and policy search (RL) are performed independently, and the latent features do not adapt to the rewards obtained during policy search. Luck et al. <ref type="bibr" target="#b26">[27]</ref> proposed an Expectation Maximization (EM) formulation which inherently combines DR with policy search. However, they relied on a linear mapping from the latent space to observation space. Interesting future work will be to combine policy search with BGPLVM through such an EM-like formulation.</p><p>Recently, there has been an increasing emphasis on Deep Learning which is capable of solving complex tasks. However, these methods usually fail when presented with small datasets. The primary advantage of using a nonparametric method is its efficiency in the low data regime. This is crucial for assistive robotics due to the difficulty in collecting large datasets. BGPLVM is a memory-based method and has prohibitive training costs. The computational complexity to train a model scales with the size of the training dataset (n) and the number of inducing points used to approximate the dataset (m), i.e. O(nm 2 ) <ref type="bibr" target="#b3">[4]</ref>. However, both the applications of using BGPLVM proposed in this study relies on the inference of high-dimensional joint angles given a latent position. The latent position is specified from a human in LfD and through policy search in RL. During test inference, the training dataset remains fixed and so precomputed values of the kernel matrix can be used to reduce time-complexity to O(n) which makes it much smaller than training instance and suitable for real-time implementation. The focus of this study is on the policy or action representation used for learning clothing assistance skills. A trajectory-centric reward function is used to perform the policy update. This method can be combined along with our previous work <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> which focuses on efficient state estimation to formulate an entire RL framework. The human-cloth relationship estimated by our framework can be used as the state representation. A reward function defined in this state space will be used to perform policy update in the BGPLVM action space proposed in this study.</p><p>Our proposed framework could also be extended to other tasks that involve complex motorskills and require data-efficiency for practical implementation. Related tasks such as dressing of other clothing articles, cloth folding, non-rigid object manipulation and so on can be potential applications. The primary requirement to apply our framework is expert demonstrations which can be used to learn efficient latent space representations. A limitation of the current study is that a soft mannequin is used as a subject. However, a concurrent study <ref type="bibr" target="#b47">[48]</ref> evaluated the usability of performing clothing assistance using the latent space controller with ten subjects and was able to demonstrate its applicability. This framework can be extended towards a practical scenario of robotic dressing assistance implemented in a nursing care facility. A shared mobile and dualarm dressing robot can be made accessible to about 20 residents of the care home. End-to-end dressing tasks can be performed by integrating related methods such as cloth extremity <ref type="bibr" target="#b48">[49]</ref> and cloth grasp point detection <ref type="bibr" target="#b49">[50]</ref>. Factors such as sociability and personalization to each user will also be considered in the implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this study, we have presented the use of BGPLVM as a representation for encoding motorskills to perform clothing assistance task. We demonstrated the applicability of this framework as an intuitive and user-friendly tool for LfD. The experimental results indicate our method as a promising approach for learning in combination with RL. There can be several extensions to this work. A latent space representation can be learned from multiple observation spaces using Manifold Relevance Determination (MRD) <ref type="bibr" target="#b40">[41]</ref>. Our future work will be to learn models that incorporate visual information about the relationship between human and cloth. The long term goal is to develop a data-efficient policy search RL framework that unifies learning the latent space simultaneously with policy learning. We will also evaluate our learning framework on human subjects along with a real-world implementation of RL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>paperFigure 1 .</head><label>1</label><figDesc>Figure 1. Clothing assistance setting: Dual-arm robot clothing soft-mannequin with T-shirt.</figDesc><graphic coords="2,208.02,48.64,181.71,155.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of proposed framework presented as a flowchart. Figure on left shows human demonstration of clothing assistance used to learn a low-dimensional latent space (figure in middle) which is used for controlling the robot (figure on right) in two different ways.</figDesc><graphic coords="3,348.46,264.23,67.94,81.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Interface for imparting motor-skills. Left: Simulator to plan trajectory, Right Top: Robot performing trajectory. Right Bottom: BGPLVM latent space to control robot.</figDesc><graphic coords="7,139.88,62.81,318.00,182.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparison of latent spaces learned by LVMs: a) PCA, b) GPLVM and c) BGPLVM. The axes are given by the most significant dimension ordered using either eigen-values or the ARD kernel weights.</figDesc><graphic coords="9,78.99,58.61,145.38,99.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>o , 30 o ), (75 o , 30 o ), (70 o , 45 o ), (75 o , 45 o ), (80 o , 45 o )}. These postures cover the entire range for which the robot can successfully perform clothing tasks thereby spanning all feasible postures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Comparison of ARD weights and latent space for different feature representations</figDesc><graphic coords="10,76.27,231.92,222.61,148.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Comparison of LVMs as a dependence on number of active dimensions used for reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Figure 8. Comparison of generalizability by LVMs over evaluation dataset a) Normalized RMS error, b) Pearson Correlation</figDesc><graphic coords="12,179.69,265.33,246.12,125.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .Figure 11 .</head><label>1011</label><figDesc>Figure 10. Robot trajectory generated corresponding to horizontal latent dimension.</figDesc><graphic coords="13,91.97,204.06,132.45,96.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Evaluation of latent space controller: a) Variation in dynamics of performing clothing task using latent space controller, b) Execution time of five novice users interacting with the latent space controller over five trials.</figDesc><graphic coords="14,71.74,78.48,227.14,136.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Comparison of different search spaces for policy search shown as a bar plot over the evaluation dataset.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was supported in part by <rs type="funder">ImPACT Program of Council for Science, Technology, and Innovation (Cabinet Office, Government of Japan</rs>) <rs type="grantNumber">2015-PM07-36-01</rs>. This work was also supported in part by the <rs type="grantName">Grant-in-Aid for Scientific Research</rs> from <rs type="funder">Japan Society for the Promotion of Science</rs> (No. <rs type="grantNumber">16H01749</rs>, No. <rs type="grantNumber">16H06534</rs> and No. <rs type="grantNumber">17H05863</rs>) and the <rs type="funder">NAIST Big</rs>-Data Project.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TNJBCTx">
					<idno type="grant-number">2015-PM07-36-01</idno>
					<orgName type="grant-name">Grant-in-Aid for Scientific Research</orgName>
				</org>
				<org type="funding" xml:id="_zjbyxjH">
					<idno type="grant-number">16H01749</idno>
				</org>
				<org type="funding" xml:id="_rnNPDXb">
					<idno type="grant-number">16H06534</idno>
				</org>
				<org type="funding" xml:id="_XDV9MZN">
					<idno type="grant-number">17H05863</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reinforcement Learning in Robotics: A Survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1238" to="1274" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Survey on Policy Search for Robotics. Foundations and Trends R in Robotics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="142" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reinforcement Learning of Clothing Assistance with a Dual-Arm Robot</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tamei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shibata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE-RAS International Conference on Humanoid Robots (Humanoids)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="733" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bayesian Gaussian Process Latent Variable Model</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="844" to="851" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Policy Search for Motor Primitives in Robotics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Principal Component Analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SLAM using Incremental Probabilistic PCA and Dimensionality Reduction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="342" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On Learning, Representing, and Generalizing a Task in a Humanoid Robot</title>
		<author>
			<persName><forename type="first">S</forename><surname>Calinon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Billard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="286" to="298" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction for Hand-independent Dexterous Robotic Grasping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ciocarlie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Goldfeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="3270" to="3275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">EMG-based Control of a Robot Arm using Low-dimensional Embeddings</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Artemiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Kyriakopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="393" to="398" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Embed to control: A locally linear latent dynamics model for control from raw images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Watter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boedecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2746" to="2754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stable Reinforcement Learning with Autoencoders for Tactile and Visual Data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">N</forename><surname>Karl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3928" to="3934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning Shared Latent Structure for Image Synthesis and Robotic Imitation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Shon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grochow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">1233</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning GP-BayesFilters via Gaussian Process Latent Variable Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="23" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Probabilistic Movement Modeling for Intention Inference in Human-Robot Interaction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mülling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Amor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="841" to="858" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Segmenting Continuous Motions with Hidden Semi-markov Models and Gaussian Processes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mochihashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Asoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaneko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neurorobotics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from Demonstration Facilitates Human-Robot Collaborative Task Execution</title>
		<author>
			<persName><forename type="first">M</forename><surname>Koskinopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Piperakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Trahanias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE International Conference on Human Robot Interaction (HRI)</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Autonomous active recognition and unfolding of clothes using random decision forests and probabilistic planning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Doumanoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kargakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malassiotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and automation (icra), 2014 ieee international conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="987" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Leveraging appearance priors in non-rigid registration, with application to manipulation of deformable objects</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent robots and systems (iros), 2015 ieee/rsj international conference on</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="878" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Repeatable folding task by humanoid robot worker using deep learning</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="397" to="403" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Combining imitation and reinforcement learning to fold deformable planar objects</title>
		<author>
			<persName><forename type="first">B</forename><surname>Balaguer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carpin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent robots and systems (iros)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1405" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pomdp approach to robotized clothes separation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Monsó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alenyà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent robots and systems (iros)</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1324" to="1329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning force-based manipulation of deformable objects from multiple demonstrations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>ICRA). IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Three-Dimensional Deformable Object Manipulation Using Fast Online Gaussian Process Regression</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="979" to="986" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gaussian Processes for Data-efficient Learning in Robotics and Control</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="408" to="423" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using Dimensionality Reduction to Exploit Constraints in Reinforcement Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3219" to="3225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Latent Space Policy Search for Robotics</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Luck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Amor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1434" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature Space Decomposition for Effective Robot Adaptation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="441" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Assistive dressing system: A capabilities study for personalized support of dressing activities for people living with dementia</title>
		<author>
			<persName><forename type="first">W</forename><surname>Burleson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Iproceedings</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A multi-agent approach to assist with dressing in a smart environment</title>
		<author>
			<persName><forename type="first">C</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nugent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="page" from="220" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Personalized assistance for dressing users</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Klee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Q</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="359" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Animating human dressing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Clegg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">116</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">What does the person feel? learning to infer applied forces during robot-assisted dressing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clegg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Kemp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and automation</title>
		<imprint>
			<biblScope unit="page" from="6058" to="6065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Haptic simulation for robot-assisted dressing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapusta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and automation</title>
		<imprint>
			<biblScope unit="page" from="6044" to="6051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bottom Dressing by a Life-sized Humanoid Robot provided Failure Detection and Recovery Functions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nagahama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/SICE International Symposium on System Integration (SII)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="564" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Friction-model based Framework for Reinforcement Learning of Robotic Tasks in Non-rigid Environments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Colomé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Planells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>ICRA). IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5649" to="5654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Data-driven Haptic Perception for Robot-Assisted Dressing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kapusta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Kemp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Robot and Human Interactive Communication</title>
		<meeting><address><addrLine>RO-MAN</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="451" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Iterative Path Optimisation for Personalised Dressing Assistance using Vision and Force Information</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Demiris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4398" to="4403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cloth Dynamics Modeling in Latent Spaces and its Application to Robotic Clothing Assistance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Koganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Ngeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tomoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shibata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3464" to="3469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bayesian Nonparametric Learning of Cloth Models for Real-Time State Estimation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Koganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tamei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shibata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="916" to="931" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1425" to="1486" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1783" to="1816" />
			<date type="published" when="2005-11">2005. Nov</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Gaussian Processes for Machine learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>The MIT Press</publisher>
			<biblScope unit="volume">10</biblScope>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dynamical Movement Primitives: Learning Attractor Models for Motor Behaviors</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ijspeert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nakanishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="328" to="373" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Constructive Incremental Learning from only Local Information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Atkeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2047" to="2084" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A Theory for Cursive Handwriting based on the Minimization Principle. Biological Cybernetics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Individual Comparisons by Ranking Methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics Bulletin</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A Framework for Robotic Clothing Assistance by Imitation Learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shibata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Robotics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Under Review</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Clothing Extremity Identification Using Convolutional Neural Network Regressor</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gaurav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shibata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Informatics, Electronics &amp; Vision (ICIEV) and International Conference on Imaging, Vision &amp; Pattern Recognition (icIVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="436" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A Study on Human-Robot Collaboration for Table-setting Task</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Labuguen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shibata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Biomimetics</title>
		<imprint>
			<publisher>ROBIO). IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="183" to="188" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
