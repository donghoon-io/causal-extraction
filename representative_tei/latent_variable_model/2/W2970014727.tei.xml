<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lars</forename><forename type="middle">Maaløe</forename><surname>Corti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark Copenhagen Denmark</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Copenhagen</forename><surname>Denmark</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark Copenhagen Denmark</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark Copenhagen Denmark</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Unumed</forename><forename type="middle">Copenhagen</forename><surname>Denmark</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark Copenhagen Denmark</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Valentin</forename><surname>Liévin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark Copenhagen Denmark</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ole</forename><surname>Winther</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark Copenhagen Denmark</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the key aspirations in recent machine learning research is to build models that understand the world <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b56">57]</ref>. Generative models are providing the means to learn from a plethora of unlabeled data in order to model a complex data distribution, e.g. natural images, text, and audio. These models are evaluated by their ability to generate data that is similar to the input data distribution from which they were trained on. The range of applications that come with generative models are vast, where audio synthesis <ref type="bibr" target="#b54">[55]</ref> and semi-supervised classification <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref> are examples hereof. Generative models can be broadly divided into explicit and implicit density models. The generative adversarial network (GAN) <ref type="bibr" target="#b10">[11]</ref> is an example of an implicit model, since it is not possible to procure a likelihood estimation from this model framework. The focus of this research is instead within explicit density models, for which a tractable or approximate likelihood estimation can be performed.</p><p>The three main classes of powerful explicit density models are autoregressive models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b56">57]</ref>, flowbased models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b15">16]</ref>, and probabilistic latent variable models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b32">33]</ref>. In recent years autoregressive models, such as the PixelRNN and the PixelCNN <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b44">45]</ref>, have achieved superior likelihood performance and flow-based models have proven efficacy on large-scale natural image generation tasks <ref type="bibr" target="#b20">[21]</ref>. However, in the autoregressive models, the runtime performance of generation is scaling poorly with the complexity of the input distribution. The flow-based models do not possess 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</p><p>this restriction and do indeed generate visually compelling natural images when sampling close to the mode of the distribution. However, generation from the actual learned distribution is still not outperforming autoregressive models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Probabilistic latent variable models such as the variational auto-encoder (VAE) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40]</ref> possess intriguing properties that are different from the other classes of explicit density models. They are characterized by a posterior distribution over the latent variables of the model, derived from Bayes' theorem, which is typically intractable and needs to be approximated. This distribution most commonly lies on a low-dimensional manifold that can provide insights into the internal representation of the data <ref type="bibr" target="#b0">[1]</ref>. However, the latent variable models have largely been disregarded as powerful generative models due to blurry generations and poor likelihood performances on natural image tasks. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b9">10]</ref>, amongst others, attribute this tendency to the usage of a similarity metric in pixel space. Contrarily, we attribute it to the lack of overall model expressiveness for accurately modeling complex input distributions, as discussed in <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>There has been much research into explicitly defining and learning more expressive latent variable models. Here, the complementary research into learning a covariance structure through a framework of normalizing flows <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b22">23]</ref> and the stacking of a hierarchy of latent variables <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b49">50]</ref> have shown promising results. However, despite significant improvements, the reported performance of these models has still been inferior to their autoregressive counterparts. This has spawned a new class of explicit density models that adds an autoregressive component to the generative process of a latent variable model <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5]</ref>. In this combination of model paradigms, the latent variables can be viewed as merely a lossy representation of the input data and the model still suffers from the same issues as autoregressive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>In this research we argue that latent variable models that are defined in a sufficiently expressive way can compete with autoregressive and flow-based models in terms of test log-likelihood and quality of the generated samples. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), a model formed by a deep hierarchy of stochastic variables that uses skip-connections to enhance the flow of information and avoid inactive units. To define a flexible posterior approximation, we construct a bidirectional inference network using stochastic variables in a bottom-up and a top-down inference path. The inference model is reminiscent to the stochastic top-down path introduced in the Ladder VAE <ref type="bibr" target="#b49">[50]</ref> and IAF VAE <ref type="bibr" target="#b49">[50]</ref> with the addition that the bottom-up pass is now also stochastic and there are no autoregressive components. We perform an in-depth analysis of BIVA and show (i) an ablation study that analyses the contributions of the individual novel components, (ii) that the model is able to improve on state-of-the-art results on benchmark image datasets, (iii) that a small extension of the model can be used for semi-supervised classification and performs comparably to current state-of-the-art models, and (iv) that the model, contrarily to other state-of-the-art explicit density models <ref type="bibr" target="#b33">[34]</ref>, can be utilized for anomaly detection on complex data distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Variational Autoencoders</head><p>The VAE is a generative model parameterized by a neural network θ and is defined by an observed variable x that depends on a hierarchy of stochastic latent variables z = z 1 , ..., z L so that:</p><formula xml:id="formula_0">p θ (x, z) = p θ (x|z 1 )p θ (z L ) L-1 i=1 p θ (z i |z i+1</formula><p>). The posterior distribution over the latent variables of a VAE is commonly analytically intractable, and is approximated with a variational distribution which is factorized with a bottom-up structure, q φ (z|x) = q φ (z 1 |x) L-1 i=1 q φ (z i+1 |z i ), so that each latent variable is conditioned on the variable below in the hierarchy. The parameters θ and φ can be optimized by maximizing the evidence lower bound (ELBO)</p><formula xml:id="formula_1">log p θ (x) ≥ E q φ (z|x) log p θ (x, z) q φ (z|x) ≡ L(θ, φ) .<label>(1)</label></formula><p>A detailed introduction on VAEs can be found in appendix A in the supplementary material. While a deep hierarchy of latent stochastic variables will result in a more expressive model, in practice the top stochastic latent variables of standard VAEs have a tendency to collapse into the prior. The Ladder VAE (LVAE) <ref type="bibr" target="#b49">[50]</ref> is amongst the first attempts towards VAEs that can effectively leverage multiple layers of stochastic variables. This is achieved by parameterizing the variational approximation with a bottom-up deterministic path followed by a top-down inference path that shares parameters with See Appendix B for a detailed explanation and a graphical model that includes the deterministic variables.</p><p>the top-down structure of the generative model: q φ,θ (z|x) = q φ (z L |x)</p><p>L-1 i=1 q φ,θ (z i |z i+1 , x). See Appendix A for a graphical representation of the LVAE inference network. Thanks to the bottomup path, all the latent variables in the hierarchy have a deterministic dependency on the observed variable x, which allows data-dependent information to skip all the stochastic variables lower in the hierarchy (Figure <ref type="figure">5d</ref> in Appendix A). The stochastic latent variables that are higher in the hierarchy will therefore receive less noisy inputs, and will be empirically less likely to collapse. Despite the improvements obtained thanks to the more flexible inference network, in practice LVAEs with a very deep hierarchy of stochastic latent variables will still experience variable collapse. In the next section we will introduce the Bidirectional-Inference Variational Autoencoder, that manages to avoid these issues by extending the LVAE in 2 ways: (i) adding a deterministic top-down path in the generative model and (ii) defining a factorization of the latent variables z i at each level of the hierarchy that allows to construct a bottom-up stochastic inference path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bidirectional-Inference Variational Autoencoder</head><p>In this section, we will first describe the architecture of the Bidirectional-Inference Variational Autoencoder (Figure <ref type="figure" target="#fig_0">1</ref>), and then provide the motivation behind the main ideas of the model as well as some intuitions on the role of each of its novel components. Finally, we will show how this model can be used for a novel approach to detecting anomalous data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model architecture</head><p>Generative model. In BIVA, at each layer 1, ..., L -1 of the hierarchy we split the latent variable in two components, z i = (z BU i , z TD i ), which belong to a bottom-up (BU) and top-down (TD) inference path, respectively. More details on this will be given when introducing the inference network. The generative model of BIVA is illustrated in Figure <ref type="figure" target="#fig_0">1a</ref>. We introduce a deterministic top-down path d L-1 , . . . , d 1 that is parameterized with neural networks and receives as input at each layer i of the hierarchy the latent variable z i+1 . In the case of a convolutional model, this is done by concatenating (z BU i+1 , z TD i+1 ) and d i+1 along the features' dimension. d i can therefore be seen as a deterministic variable that summarizes all the relevant information coming from the stochastic variables higher in the hierarchy, z &gt;i . The latent variables z BU i and z TD i are conditioned on all the information in the higher layers, and are conditionally independent given z &gt;i . The joint distribution of the model is then given by:</p><formula xml:id="formula_2">p θ (x, z) = p θ (x|z)p θ (z L ) L-1 i=1 p θ (z BU i |z &gt;i )p θ (z TD i |z &gt;i ) ,</formula><p>where θ are the parameters of the generative model. The likelihood of the model p θ (x|z) directly depends on z 1 , and depends on z &gt;1 through the deterministic top-down path. Each stochastic latent variable 1, ..., L is parameterized by a Gaussian distribution with diagonal covariance, with one neural network µ(•) for the mean and another neural network σ(•) for the variance. Since the z BU i+1 and z TD i+1 variables are on the same level in the generative model and of the same dimensionality, we share all the deterministic parameters going to the layer below. See Appendix B for details.</p><p>Bidirectional inference network. Due to the non-linearities in the neural networks that parameterize the generative model, the exact posterior distribution p θ (z|x) is intractable and needs to be approximated. As for VAEs, we therefore define a variational distribution, q φ (z|x), that needs to be flexible enough to approximate the true posterior distribution, as closely as possible. We define a bottom-up (BU) and a top-down (TD) inference path, which are computed sequentially when constructing the posterior approximation for each data point x, see Figure <ref type="figure" target="#fig_0">1b</ref>. The variational distribution over the BU latent variables depends on the data x and on all BU variables lower in the hierarchy, i.e. q φ (z BU i |x, z BU &lt;i ), where φ denotes all the parameters of the BU path. z BU i has a direct dependency only on the BU variable below, z BU i-1 . The dependency on z BU &lt;i-1 is achieved, similarly to the generative model, through a deterministic bottom-up path d 1 , . . . , d L-1 .</p><p>The TD variables depend on the data and the BU variables lower in the hierarchy through the BU inference path, but also on all variables above in the hierarchy through the TD inference path, see Figure <ref type="figure" target="#fig_0">1b</ref>. The variational approximation over the TD variables is thereby q φ,θ (z TD i |x, z BU &lt;i , z BU &gt;i , z TD &gt;i ). Importantly, all the parameters of the TD path are shared with the generative model, and are therefore denoted as θ. The overall inference network can be factorized as follows:</p><formula xml:id="formula_3">q φ (z|x) = q φ (z L |x, z BU &lt;L ) L-1 i=1 q φ (z BU i |x, z BU &lt;i )q φ,θ (z TD i |x, z BU &lt;i , z BU &gt;i , z TD &gt;i ) ,</formula><p>where the variational distributions over the BU and TD latent variables are Gaussians whose mean and diagonal covariance are parameterized with neural networks that take as input the concatenation over the feature dimension of the conditioning variables. Training of BIVA is performed, as for VAEs, by maximizing the ELBO in eq. ( <ref type="formula" target="#formula_1">1</ref>) with stochastic backpropagation and the reparameterization trick.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Motivation</head><p>BIVA can be seen as an extension of the LVAE in which we (i) add a deterministic top-down path and (ii) apply a bidirectional inference network. We will now provide the motivation and some intuitions on the role of these two novel components, that will then be empirically validated with the ablation study of Section 4.1.</p><p>Deterministic top-down path. Skip-connections represent one of the simplest yet most powerful advancements of deep learning in recent years. They allow constructing very deep neural networks, by better propagating the information throughout the model and reducing the issue of vanishing gradients. Skip connections form for example the backbone of deep neural networks such as ResNets <ref type="bibr" target="#b14">[15]</ref>, which have shown impressive performances on a wide range of classification tasks. Our goal in this paper is to build very deep latent variable models that are able to learn an expressive latent hierarchical representation of the data. In our experiments, we however found that the LVAE still had difficulties in activating the top latent variables for deeper hierarchies. To limit this issue, we add skip connections among the latent variables in the generative model by adding the deterministic top-down path, that makes each variable depend on all the variables above in the hierarchy (see Figure <ref type="figure" target="#fig_0">1a</ref> for a graphical representation). This allows a better flow of information in the model and thereby avoids the collapse of latent variables. A related idea was recently proposed by <ref type="bibr" target="#b6">[7]</ref>, that add skip connections among the neural network layers parameterizing a shallow VAE with a single latent variable.</p><p>Bidirectional inference. The inspiration for the bidirectional inference network of BIVA comes from the work on Auxiliary VAEs (AVAE) by <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b30">31]</ref>. An AVAE can be viewed as a shallow VAE with a single latent variable z and an auxiliary variable a that increases the expressiveness of the variational approximation q φ (z|x) = q φ (z|a, x)q φ (a|x)da. By making the inference network q φ (z|a, x) depend on the stochastic variable a, the AVAE adds covariance structure to the posterior approximation over the stochastic unit z, since it no longer factorizes over its components z (k) , i.e. q φ (z|x) = k q φ (z (k) |x). As discussed in the following, by factorizing the latent variables at each level of the hierarchy of BIVA we are able to achieve similar results without introducing additional auxiliary variables in the model. To see this, we can focus for example on the highest latent variable z L . In BIVA, the presence of the z BU i variables makes the bottom-up inference path stochastic, as opposed to the deterministic BU path of the LVAE. While the conditional distribution q φ (z L |x, z BU &lt;L ) still factorizes over the components of z L , due to the stochastic BU variables the marginal distribution over z L no longer factorizes, i.e. q φ (z</p><formula xml:id="formula_4">L |x) = q φ (z L |x, z BU &lt;L )q φ (z BU &lt;L |x)dz BU &lt;L = K k=1 q(z<label>(k)</label></formula><p>L |x) . Therefore, the BU inference path enables the learning of a complex covariance structure in the higher TD stochastic latent variables, which is fundamental in the model to extract good high-level semantic features from the data distribution. Notice that, in BIVA, only z BU 1 will have a marginally factorizing inference network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Anomaly detection with BIVA</head><p>Anomaly detection is considered to be one of the most important applications of explicit density models. However, recent empirical results suggest that these models are not able to distinguish between two clearly distinctive data distributions <ref type="bibr" target="#b33">[34]</ref>, as they can assign a higher likelihood to data points from a data distribution that is very different from the one the model was trained on. Based on a thorough study, <ref type="bibr" target="#b33">[34]</ref> states that the main issue is the fact that explicit density models tend to capture low-level statistics, as opposed to the high-level semantics that are preferable when doing anomaly detection. We hypothesize that the latent representations in the higher layers of BIVA can capture the high-level semantics of the data and that these can be used for improved anomaly detection.</p><p>In the standard ELBO from eq. ( <ref type="formula" target="#formula_1">1</ref>), the main contribution to the expected log-likelihood term is coming from averaging over the variational distribution of the lower level latent variables. This will thus emphasize low-level statistics. So in order to perform anomaly detection with BIVA we instead need to emphasize the contribution from the higher layers. We can achieve this introducing an alternative score function inspired by the ELBO that partly replaces the inference network with the generative model, and uses therefore the generative hierarchy of the stochastic variables. In the following we define the hierarchy of stochastic latent variables as z = z 1 , z 2 , z 3 , ..., z L with z i = (z BU i , z TD i ). Instead of using as in the standard ELBO the variational approximation q φ (z|x) over all stochastic variables in the model, we use the prior distribution for the first k layers and the variational approximation from the inference network for the others, i.e. p θ (z ≤k |z &gt;k )q φ (z &gt;k |x, z BU ≤k ). In the computation of q φ (z &gt;k |x, z BU ≤k ) we use samples z BU ≤k from the inference network. Using this alternative distribution instead of q φ (z|x) in the ELBO in eq. ( <ref type="formula" target="#formula_1">1</ref>), we define the score function for anomaly detection as:</p><formula xml:id="formula_5">L &gt;k = E p θ (z ≤k |z &gt;k )q φ (z &gt;k |x,z BU ≤k ) log p θ (x|z)p θ (z &gt;k ) q φ (z &gt;k |x, z BU ≤k ) .<label>(2)</label></formula><p>L &gt;0 = L is the ELBO in eq. ( <ref type="formula" target="#formula_1">1</ref>). As for the ELBO, we approximate the computation of L &gt;k with Monte Carlo integration. Sampling from p θ (z ≤k |z &gt;k )q φ (z &gt;k |x, z BU ≤k ) can be easily performed by obtaining samples z &gt;k from the inference network, that are then used to sample z ≤k from the conditional prior p θ (z ≤k | z &gt;k ).</p><p>L &gt;k with higher values of k represents a useful metric for anomaly detection, as shown empirically in the experiments of Section 4.4. By only sampling the top L -k variables from the variational approximation, in fact, we are forcing the model to only rely on the high-level semantics encoded in the highest variables of the hierarchy when evaluating this metric, and not on the low-level statistics encoded in the lower variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>BIVA is empirically evaluated by (i) an ablation study analyzing each novel component, (ii) likelihood and semi-supervised classification results on binary images, (iii) likelihood results on natural images, and (iv) an analysis of anomaly detection in complex data distributions. We employ a free bits strategy with λ = 2 <ref type="bibr" target="#b22">[23]</ref> for all experiments to avoid latent variable collapse during the initial training epochs. Trained models are reported with 1 importance weighted sample, L 1 , and 1000 importance weighted samples, L 1e3 <ref type="bibr" target="#b2">[3]</ref>. We evaluate the natural image experiments by bits per dimension (bits/dim), L/(hwc log(2)), where h, w, c denote the height, width, and channels respectively. For a detailed   description of the experimental setup see Appendix C and the source code 12 . In Appendix D we test BIVA on complex 2d densities, while Appendix E presents initial results for the model on text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Study</head><p>BIVA can be viewed as an extension of the LVAE from <ref type="bibr" target="#b49">[50]</ref> where we add (i) extra dependencies in the generative model (p θ (x|z 1 ) → p θ (x|z) and p θ (z i |z i+1 ) → p θ (z i |z &gt;i )) through the skip connections obtained with the deterministic top-down path and (ii) a bottom-up (BU) path of stochastic latent variables to the inference model. In order to evaluate the effects of each added component we define an LVAE with the exact same architecture as BIVA, but without the BU variables and the deterministic top-down path. Next, we define the LVAE+, where we add to the LVAE's generative model the deterministic top-down path. It is therefore the same model as in Figure <ref type="figure" target="#fig_0">1</ref> but without the BU variables. Finally, we investigate a LVAE+ model with 2L -1 stochastic layers. This corresponds to the depth of the hierarchy of the BIVA inference model</p><formula xml:id="formula_6">x → z BU 1 → • • • → z BU L-1 → z L → z TD L-1 → • • • → z TD 1 .</formula><p>If this model is competitive with BIVA then it is an indication that it is the depth that determines the performance. The ablation study is conducted on the CIFAR-10 dataset against the best reported BIVA with L = 15 layers (Section 4.3), which means 2L -1 = 29 stochastic latent layers in the deep LVAE+. Table <ref type="table" target="#tab_0">1</ref> presents a comparison of the different model architectures. The positive effect of adding the skip connections in the generative models can be evaluated from the difference between the LVAE L = 15 and LVAE+ L = 15 results, for which there is close to a 0.2 bits/dim difference in the ELBO. Thanks to the more expressive posterior approximation obtained using its bidirectional inference network, BIVA improves the ELBO significantly w.r.t the LVAE+, by more than 0.3 bits/dim. Notice that a deeper hierarchy of stochastic latent variables in the LVAE+ will not necessarily provide a better likelihood performance, since the LVAE+ L = 29 performs worse than the LVAE+ L = 15 despite having significantly more parameters. In Figure <ref type="figure" target="#fig_2">2</ref> we plot for LVAE, LVAE+ and BIVA the KL divergence between the variational approximation over each latent variable Table <ref type="table">2</ref>: Test log-likelihood on statically binarized MNIST for different number of importance weighted samples. The finetuned models are trained for an additional number of epochs with no free bits, λ = 0. For testing resiliency we trained 4 models and evaluated the standard deviations to be ±0.031 for L 1 .</p><p>-log p(x) With autoregressive components PIXELCNN <ref type="bibr" target="#b56">[57]</ref> = 81.30 DRAW <ref type="bibr" target="#b12">[13]</ref> &lt; 80.97 IAFVAE <ref type="bibr" target="#b22">[23]</ref> ≤ 79.88 PIXELVAE <ref type="bibr" target="#b13">[14]</ref> ≤ 79.66 PIXELRNN <ref type="bibr" target="#b56">[57]</ref> = 79.20 VLAE <ref type="bibr" target="#b4">[5]</ref> ≤ 79.03 Without autoregressive components DISCRETE VAE <ref type="bibr" target="#b41">[42]</ref> ≤ 81.01  3.33% (±0.14) VAT <ref type="bibr" target="#b31">[32]</ref> 2.12% CATGAN <ref type="bibr" target="#b50">[51]</ref> 1.91% (±0.10) SDGM <ref type="bibr" target="#b30">[31]</ref> 1.32% (±0.07) LADDERNET <ref type="bibr" target="#b37">[38]</ref> 1.06% (±0.37) ADGM <ref type="bibr" target="#b30">[31]</ref> 0.96% (±0.02) IMPGAN <ref type="bibr" target="#b43">[44]</ref> 0.93% (±0.07) TRIPLEGAN <ref type="bibr" target="#b28">[29]</ref> 0.91% (±0.58) SSLGAN <ref type="bibr" target="#b5">[6]</ref> 0.80% (±0.10) BIVA 0.83% (±0.02)  We evaluated two different BIVA with various number of layers (L). For testing resiliency we trained 3 models and evaluated the standard deviations to be ±0.013 for L 1 and L = 15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BITS/DIM</head><p>With autoregressive components CONVDRAW <ref type="bibr" target="#b11">[12]</ref> &lt; 3.58 IAFVAE L1 <ref type="bibr" target="#b22">[23]</ref> ≤ 3.15 IAFVAE L1e3 <ref type="bibr" target="#b22">[23]</ref> ≤ 3.12 GATEDPIXELCNN <ref type="bibr" target="#b55">[56]</ref> = 3.03 PIXELRNN <ref type="bibr" target="#b56">[57]</ref> = 3.00 VLAE <ref type="bibr" target="#b4">[5]</ref> ≤ 2.95 PIXELCNN++ <ref type="bibr" target="#b44">[45]</ref> = 2.92 Without autoregressive components NICE <ref type="bibr" target="#b7">[8]</ref> = 4.48 DEEPGMMS <ref type="bibr" target="#b57">[58]</ref> = 4.00 REALNVP <ref type="bibr" target="#b8">[9]</ref> = 3.49 DISCRETEVAE++ <ref type="bibr" target="#b53">[54]</ref> ≤ 3.38 GLOW <ref type="bibr" target="#b20">[21]</ref> = 3.35 FLOW++ <ref type="bibr" target="#b15">[16]</ref> = 3.08 BIVA L=10, L1 ≤ 3.17 BIVA L=15, L1 ≤ 3.12 BIVA L=15, L1e3 ≤ 3.08 and its prior distribution, KL(q||p). This KL divergence is 0 when the two distributions match, in which case we say that the variable has collapsed, since its posterior approximation is not using any data-dependent information. We can see that while the LVAE is only able to utilize its lowest 7 stochastic variables, all variables in both LVAE+ and BIVA are active. We attribute this tendency to the deterministic top-down path that is present in both models, which creates skip-connections between all latent variables that allow to better propagate the information throughout the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Binary Images</head><p>We evaluate BIVA L = 6 in terms of test log-likelihood on statically binarized MNIST <ref type="bibr" target="#b42">[43]</ref>, dynamically binarized MNIST <ref type="bibr" target="#b27">[28]</ref> and dynamically binarized OMNIGLOT <ref type="bibr" target="#b24">[25]</ref>. The model parameterization and optimization parameters have been kept identical for all binary image experiments (see Appendix C). For each experiment on binary image datasets, we finetune each model by setting the free bits to λ = 0 until convergence in order to test the tightness of the L 1 ELBO.</p><p>To the best of our knowledge, BIVA achieves state-of-the-art results on statically binarized MNIST, outperforming other latent variable models, autoregressive models, and flow-based models (see Table <ref type="table">2</ref>). Finetuning the model with λ = 0 improves the L 1 ELBO significantly and achieves slightly better performance for the 1000 importance weighted samples. For dynamically binarized MNIST   <ref type="table" target="#tab_0">10</ref> and<ref type="table" target="#tab_0">11</ref> in Appendix G.</p><p>Semi-supervised learning. BIVA can be easily extended for semi-supervised classification by adding a categorical variable y to represent the class, as done in <ref type="bibr" target="#b21">[22]</ref>. We add a classification model q φ (y|x, z BU &lt;L ) to the inference network, and a class-conditional distribution p θ (x|z, y) to the generative model (see Appendix F for a detailed description). We train 5 different semi-supervised models on MNIST, each using a different set of just 100 randomly chosen and evenly distributed MNIST labels. Table <ref type="table" target="#tab_2">3</ref> presents the classification results on the test set (mean and standard deviation over the 5 runs), that shows that BIVA achieves comparable performance to recent state-of-the-art results by generative adversarial networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Natural Images</head><p>We trained and evaluated BIVA L = 15 on 32x32 CIFAR-10, 32x32 ImageNet <ref type="bibr" target="#b56">[57]</ref>, and another BIVA L = 20 on 64x64 CelebA <ref type="bibr" target="#b26">[27]</ref>. For the output decoding, we employ the discretized logistic mixture likelihood from <ref type="bibr" target="#b44">[45]</ref> (see Appendix C for more details). In Table <ref type="table" target="#tab_3">4</ref> we see that for the CIFAR-10 dataset BIVA outperforms other state-of-the-art non-autoregressive models and performs slightly worse than state-of-the-art autoregressive models. For the 32x32 ImageNet dataset BIVA achieves better performance than flow-based models, but the performance gap to the autoregressive models remains large (Table <ref type="table" target="#tab_2">13</ref> in Appendix G). This may be due to the added complexity (more categories) of the 32x32 ImageNet dataset, requiring an even more flexible model. More research should be invested in defining an improved architecture for BIVA that holds more parameters and thereby achieves better performances.</p><p>Figure <ref type="figure" target="#fig_3">3</ref> shows generated samples from the N (0, I) prior of a BIVA L = 20 trained on the CelebA dataset. From a visual inspection, the samples are far superior to previous natural image generations by latent variable models. We believe that previous claims stating that this type of model can only generate blurry images should be disregarded <ref type="bibr" target="#b26">[27]</ref>. Rather the limited expressiveness/flexibility of previous models should be blamed. Additional samples from BIVA can be found in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Does BIVA know what it doesn't know?</head><p>We test the anomaly detection capabilities of BIVA replicating the most challenging experiments of <ref type="bibr" target="#b33">[34]</ref>. We train BIVA L = 15 on the CIFAR-10 dataset, and evaluate eq. ( <ref type="formula" target="#formula_5">2</ref>) for various values of k on the CIFAR-10 test set, the SVHN dataset <ref type="bibr" target="#b34">[35]</ref> and the CelebA dataset. The results can be found in Table <ref type="table" target="#tab_5">5</ref> and Figure <ref type="figure" target="#fig_4">4</ref>, and are reported in terms of bits per dimension (lower is better). We see that for k = 0, corresponding to the standard ELBO, BIVA wrongly assigns lower values to data points from SVHN. This is in line with the results obtained with other explicit density models in <ref type="bibr" target="#b33">[34]</ref>, and shows that by using the standard ELBO the low-level image statistics prevail and the model is not able to correctly detect out-of-distribution samples. However, for higher values of k, the situation is reversed. We take this as an indication that BIVA uses the high-level semantics inferred from the data to better differentiate between the CIFAR-10 and the SVHN/CelebA distributions. We repeat the experiment training BIVA L = 6 on the FashionMNIST dataset (Table <ref type="table" target="#tab_5">5</ref>), and testing on the FashionMNIST test set and the MNIST dataset. Unlike the flow-based models used in <ref type="bibr" target="#b33">[34]</ref>, BIVA is able to learn a data distribution that can be used to detect anomalies with the standard ELBO (but also k &gt; 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have introduced BIVA, that significantly improves performances over previously introduced probabilistic latent variable models and flow-based models. BIVA is able to generate natural images that are both sharp and coherent, to improve on semi-supervised classification benchmarks and, contrarily to other models, allows for anomaly detection using the extracted high-level semantics of the data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A L = 3 layered BIVA with (a) the generative model and (b) inference model. Blue arrows indicate that the deterministic parameters are shared between the inference and generative models.See Appendix B for a detailed explanation and a graphical model that includes the deterministic variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) LVAE L = 15 (b) LVAE+ L = 15 (c) BIVA L = 15</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The log KL(q||p) for each stochastic latent variable as a function of the training epochs on CIFAR-10. (a) is a L = N = 15 stochastic latent layer LVAE with no skip-connections and no bottom-up inference. (b) is a L = N = 15 LVAE+ with skip-connections and no bottom-up inference. (c) is a L = 15 stochastic latent layer (N = 29 latent variables) BIVA for which 1, 2, ..., N denotes the stochastic latent variables following the order z BU 1 , z TD 1 , z BU 2 , z TD 2 , ..., z L .</figDesc><graphic coords="7,108.00,207.79,396.00,89.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (left) images from the CelebA dataset preprocessed to 64x64 following [27]. (right) N (0, I) generations of BIVA with L = 20 layers that achieves a L 1 = 2.48 bits/dim on the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Histograms and kernel density estimation of the L &gt;k for k = 13, 11, 0 evaluated in bits/dim by a model trained on the CIFAR-10 train dataset and evaluated on the CIFAR-10 and the SVHN test set.</figDesc><graphic coords="8,127.01,305.43,152.06,144.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A comparison of the LVAE with no skip-connections and no bottom-up inference, the LVAE+ with skip-connections and no bottom-up inference, and BIVA. All models are trained on the CIFAR-10 dataset.</figDesc><table><row><cell></cell><cell>PARAM.</cell><cell>BITS/DIM</cell></row><row><cell>LVAE L=15, L1</cell><cell>72.36M</cell><cell>≤ 3.60</cell></row><row><cell>LVAE+ L=15, L1</cell><cell>73.35M</cell><cell>≤ 3.41</cell></row><row><cell cols="2">LVAE+ L=29, L1 119.71M</cell><cell>≤ 3.45</cell></row><row><cell>BIVA L=15, L1</cell><cell>102.95M</cell><cell>≤ 3.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Semi-supervised test error for BIVA on MNIST for 100 randomly chosen and evenly distributed labelled samples. ERROR % M1+M2<ref type="bibr" target="#b21">[22]</ref> </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Test log-likelihood on CIFAR-10 for different number of importance weighted samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>L</head><label></label><figDesc>&gt;L-2 L &gt;L-4 L &gt;L-6 L &gt;0 Model trained on CIFAR-10:</figDesc><table><row><cell>CIFAR-10</cell><cell>79.36</cell><cell>35.34</cell><cell>20.93</cell><cell>3.12</cell></row><row><cell>SVHN</cell><cell cols="2">121.04 58.82</cell><cell>26.76</cell><cell>2.28</cell></row><row><cell>Model trained on FashionMNIST:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FASHIONMNIST</cell><cell cols="3">228.38 107.07 -</cell><cell>94.05</cell></row><row><cell>MNIST</cell><cell cols="3">295.95 130.39 -</cell><cell>128.60</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The test L &gt;k</figDesc><table><row><cell>for different values of</cell></row><row><cell>k and train/test dataset</cell></row><row><cell>combinations evaluated</cell></row><row><cell>in bits/dim for natural</cell></row><row><cell>images and negative log-</cell></row><row><cell>likelihood for binary im-</cell></row><row><cell>ages (lower is better).</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Source code (Tensorflow): https://github.com/larsmaaloee/BIVA.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Source code (PyTorch): https://github.com/vlievin/biva-pytorch.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06349</idno>
		<title level="m">Generating sentences from a continuous space</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Accurate and conservative estimates of mrf log-likelihood using reverse annealing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00519</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Importance Weighted Autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Variational Lossy Autoencoder</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Good semi-supervised learning that requires a bad GAN</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Avoiding latent variable collapse with generative skip models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04863</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<title level="m">Nice: Non-linear independent components estimation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D J</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredric</forename></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.08772</idno>
		<title level="m">Towards conceptual compression</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ali Taiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Pixelvae</surname></persName>
		</author>
		<idno>1611.05013</idno>
		<title level="m">A latent variable model for natural images</title>
		<imprint>
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Flow++: Improving flow-based generative models with variational dequantization and architecture design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00275</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999-11">Nov. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A Method for Stochastic Optimization</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2014</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-Supervised Learning with Deep Generative Models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P;</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2013</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Auto-Encoding Variational Bayes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">One-shot learning by inverting a compositional causal process</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02291</idno>
		<title level="m">Triple generative adversarial nets</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Semi-supervised generation with cluster-aware generative models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00637</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Auxiliary Deep Generative Models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>-I. Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00677</idno>
		<title level="m">Distributional Smoothing with Virtual Adversarial Training</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural variational inference and learning in belief networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1791" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Do deep generative models know what they don</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09136</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">t know? arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Unsupervised Feature Learning, workshop at Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Variational bayesian inference with stochastic search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1363" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hierarchical variational models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Variational Inference with Normalizing Flows</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<title level="m">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00597</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Taming vaes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Discrete variational autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Rolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On the quantitative analysis of deep belief networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03498</idno>
		<title level="m">Improved techniques for training gans</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karparthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno>arXiv preprint:1701.05517</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing Systems</title>
		<meeting>the International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Markov chain monte carlo and variational inference: Bridging the gap</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A hybrid convolutional variational autoencoder for text generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02390</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Generating sentences using a dynamic canvas</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ladder variational autoencoders</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Unsupervised and semi-supervised learning with categorical generative adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06390</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Improving variational auto-encoders using householder flow</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09630</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Variational Gaussian process</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">DVAE++: discrete variational autoencoders with overlapping transformations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khoshaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Andriyash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05328</idno>
		<title level="m">Conditional image generation with pixelcnn decoders</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<title level="m">Pixel recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Factoring variations in natural images with deep gaussian mixture models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Towards deeper understanding of variational autoencoding models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08658</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
