<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Face Recognition across Poses using Fusion of Probabilistic Latent Variable Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Moh</forename><forename type="middle">Edi</forename><surname>Wibowo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dian</forename><surname>Tjondronegoro</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vinod</forename><surname>Chandran</surname></persName>
							<email>v.chandran@qut.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">Science and Engineering Faculty</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<addrLine>2 George St</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Reza</forename><surname>Pulungan</surname></persName>
							<email>pulungan@ugm.ac.id</email>
						</author>
						<author>
							<persName><forename type="first">Jazi</forename><surname>Eko Istiyanto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Electronics</orgName>
								<orgName type="institution">Universitas Gadjah Mada FMIPA Sekip Utara</orgName>
								<address>
									<addrLine>Ph.: +62274546194 2</addrLine>
									<postCode>55281</postCode>
									<settlement>Bulaksumur Sleman Yogyakarta</settlement>
									<country key="ID">Indonesia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Brisbane CBD</orgName>
								<address>
									<postCode>4000</postCode>
									<region>Queensland</region>
									<country>Australia Ph</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Face Recognition across Poses using Fusion of Probabilistic Latent Variable Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.12928/TELKOMNIKA.v15i4.5731</idno>
					<note type="submission">Received August 10, 2017; Revised October 30, 2017; Accepted November 17, 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>face recognition</term>
					<term>pose</term>
					<term>classifier fusion</term>
					<term>video</term>
					<term>probabilistic latent variable</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Uncontrolled environments have often required face recognition systems to identify faces appearing in poses that are different from those of the enrolled samples. To address this problem, probabilistic latent variable models have been used to perform face recognition across poses. Although these models have demonstrated outstanding performance, it is not clear whether richer parameters always lead to performance improvement. This work investigates this issue by comparing performance of three probabilistic latent variable models, namely PLDA, TFA, and TPLDA, as well as the fusion of these classifiers on collections of video data. Experiments on the VidTIMIT+UMIST and the FERET datasets have shown that fusion of multiple classifiers improves face recognition across poses, given that the individual classifiers have similar performance. This proves that different probabilistic latent variable models learn statistical properties of the data that are complementary (not redundant). Furthermore, fusion across multiple images has also been shown to produce better perfomance than recogition using single still image.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>into some pose-invariant representations and infers face identities by matching these representations in the pose-invariant spaces. The 3DMMs are fitted to face images in <ref type="bibr" target="#b16">[15]</ref> to produce 3D shape and texture parameters that serve as pose-invariant representations. Statistical methods, such as subspaces alignment <ref type="bibr" target="#b17">[16]</ref> and kernel discriminant analysis <ref type="bibr" target="#b18">[17]</ref>, have also been employed for the same purpose. Recognition across poses is performed in <ref type="bibr" target="#b19">[18]</ref> through the use of light-fields, i.e., the concatenation of face images of individuals from a number of poses. Probe faces and enrollment samples are viewed as light-fields with missing values whose least-square projections to the Eigenspace serve as pose-invariant representations. More recently, probabilistic latent variable models <ref type="bibr" target="#b20">[19]</ref><ref type="bibr" target="#b21">[20]</ref><ref type="bibr" target="#b22">[21]</ref> have been applied to face recognition across poses with superior performance. These methods assume that there genuinely exists a multidimensional latent variable that uniquely represents the identity of an individual's faces irrespective of their poses. Using these models, the likelihoods that face images with different poses actually correspond to the same identity (latent variable) can be estimated.</p><p>While probabilistic latent variable models have been successfully used in face recognition across poses, it is unclear whether richer parameters always lead to performance improvement. It is yet to know, for example, that the use of pose-specific transformations (tied models) will make the generic transformations (non-tied models) completely void. Similarly, it is important to confirm that explicit modelling of within-class variations (discriminant analysis) will always be a better and complete substitute for non-explicit modelling (factor analysis). This work investigates this issue by comparing the performance of variants of probabilistic latent variable models as well as the fusion of these classifiers. More specifically, three classification models are evaluated: probabilistic linear discriminant analysis (PLDA) <ref type="bibr" target="#b20">[19]</ref>, tied factor analysis (TFA) <ref type="bibr" target="#b21">[20]</ref>, and tied probabilistic linear discriminant analysis (TPLDA) <ref type="bibr" target="#b22">[21]</ref>. Unlike the existing work, the evaluation is performed not only on still images but also on videos. Videos differ from still images in the much larger number of images available for each individuals and the dense face samples within the pose space. It is therefore interesting to see how such rich data benefit the recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Research Method</head><p>Figure <ref type="figure" target="#fig_1">1</ref> shows the proposed framework for face recognition across poses. The framework is composed of two components: front-end and classifier. The front-end serves to localize faces in videos, localize facial landmarks and estimate head poses, extract features, and group the features based on the estimated poses. The classifier matches probe faces, assumed to be non-frontal, against enrollment samples, which are frontal. Matching scores are computed based on probabilistic latent variable models that are constructed in the training stage. The rest of this section describes in more detail each of the processes involved in the proposed framework. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Face and Facial Landmark Localization and Head Pose Estimation</head><p>Given a video frame as input, facial ROIs (regions of interest, i.e., bounding boxes) are localized using a combination of three Viola-Jones face detectors <ref type="bibr" target="#b23">[22]</ref>. These detectors have been trained for three discrete poses: frontal, left-profile, and right-profile. The frontal detector is applied first and is followed by the application of the left-profile detector. If the frontal detector successfully detects a face, the left-profile detector is applied only to a small area around this detection result. The right-profile detector is executed only when the left-profile detector does not give a positive result. This procedure is able to anticipate left-right head rotations and might return multiple detection results for one particular face.</p><p>After facial ROIs have been detected, the process continues with the search of facial landmarks. We train a cascaded regression model that is able to perform simultaneous facial landmark localization and head pose estimation <ref type="bibr" target="#b24">[23,</ref><ref type="bibr" target="#b25">24]</ref>. Cascaded regression has been well known as an accurate and reliable method for facial landmark localization. In this work, the model has also been trained to handle occluded facial landmarks (for faces that rotate away from frontal). This model makes use of multiple facial ROIs as input to produce a single final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Face Normalization and Feature Extraction</head><p>Based on the estimated head poses, faces are classified into frontal, half-profile, and profile, which are defined as 0 -20°, 20° -50°, and 50° -90° of left-right rotation, respectively. Note that faces facing to the left direction are flipped horizontally. Before appearance features are extracted, the faces are normalized and segmented.</p><p>Piece-wise triangular warp is employed to normalize face images. This technique has been observed in this research to work better than the traditional procedures, i.e., similarity transforms. Piece-wise triangular warp employs point distribution models (PDMs) <ref type="bibr" target="#b26">[25]</ref> to perform normalization. A PDM represents 2D facial meshes using a set of orthogonal basis shapevectors. Three PDMs are constructed for frontal, half-profile, and profile faces, respectively. Given a number of facial landmarks returned by the cascaded regression model, least square projection is performed to obtain the complete parameters of the PDMs as well as the corresponding 2D mesh. Figure <ref type="figure" target="#fig_2">2</ref> shows the estimated 2D meshes of different faces as well as the results of piece-wise triangular warp for the normalization. Note that a single reference mesh is used to deform (warp) all faces of a particular pose. Compared to similarity transforms, piecewise triangular warp produces better correspondences of facial parts at the cost of losing facial shape information. The warp faces are resized into 51×51 ROIs, whose intensity values are concatenated to form feature vectors of 2601 elements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Classification</head><p>As mentioned earlier, probabilistic latent variable models are employed in this work to match face images across poses. These include PLDA <ref type="bibr" target="#b20">[19]</ref>, TFA <ref type="bibr" target="#b21">[20]</ref>, and TPLDA <ref type="bibr" target="#b22">[21]</ref>, first proposed by Prince and colleagues. The tied models generalize the "original" models by introducing pose-specific generative transformations over the single latent identity space. More explicitly, PLDA can be described as</p><formula xml:id="formula_0">x ij = µ + Fh i + Gw ij + ε ij (1)</formula><p>while TFA and TPLDA can be expressed as</p><formula xml:id="formula_1">x ijk = µ k + F k h i + ε ijk (2)</formula><p>and</p><formula xml:id="formula_2">x ijk = µ k + F k h i + G k w ijk + ε ijk (3)</formula><p>respectively. The term x ijk represents the j-th observation of class i in pose k. For each pose k, 4 parameters are defined: the mean µ k , the bases F k and G k , and the diagonal covariance matrix Ʃ k of ε ijk . TFA and TPLDA models (analogous to PLDA) can be trained using an EM algorithm that executes two computation steps iteratively until it converges. In the expectation step, the expected values of latent variables h i and w ijk are calculated for each individual i using data of the individual from all poses x ij• . In the maximization step, model parameters F k , G k , and Ʃ k are optimized for each pose k using data of the pose from all individuals x ••k . Interested readers are encouraged to refer to the comprehensive discussion of this algorithm in <ref type="bibr" target="#b20">[19]</ref><ref type="bibr" target="#b21">[20]</ref><ref type="bibr" target="#b22">[21]</ref><ref type="bibr" target="#b27">26]</ref>.</p><p>The trained models are used to recognize probe faces during the recognition phase. Prince and Elder <ref type="bibr" target="#b20">[19]</ref> propose a Bayesian model comparison approach that assumes data points of the same class are generated from the same value of LIV. Given a probe x p and samples of M classes x 1 , x 2 , … , x M , there will be M generation models M 1 , M 2 , … , M M to consider. M m represents the case where x p and x m are bound to the same LIV, which is h m , while the other samples are bound to their own LIVs. The likelihood P(x p , x 1 … M |M m , θ) can then be defined as</p><formula xml:id="formula_3">P(x p , x 1 … M |M m , θ) = P(x p , x m |M m , θ)×Π i = 1 … M, i ≠ m P(x i |M m , θ) (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where θ is the set of model parameters. The posterior of the generation model is obtained as</p><formula xml:id="formula_5">P(M m | x p , x 1 … M , θ) = P(x p , x 1 … M |M m , θ)×P(M m ) which is  P(x p , x 1 … M | M m , θ)  P(x p , x m |M m , θ) / P(x m |M m , θ</formula><p>) if the priors P(M 1 ), P(M 2 ), … , P(M m ) are assumed to be uniform. In this research, only closed-set identification is considered. Classification systems will thus not be probed by individuals who are not enrolled in the systems (impostors). Furthermore, multiple enrollment samples are available for each individual. Suppose that a model θ is employed for the classification. Given a test image x p , matching score of x p and class i is computed using S(x p , x i• |θ) = max j P(x p , x ij |M i , θ) / P(x ij |M i , θ) (matching to the nearest sample). The identity of the probe x p can then be inferred as argmax i = 1 ... M S(x p , x i• ).</p><p>In <ref type="bibr" target="#b20">[19]</ref><ref type="bibr" target="#b21">[20]</ref><ref type="bibr" target="#b22">[21]</ref>, high recognition rates have been achieved by fusing matching scores across different local areas. Inspired by this idea, this research investigates the possibility of improving performance by fusing matching scores from different classifiers. For a fusion to be successful, the fused classifiers must not be redundant. This research conjectures that PLDA, TFA, and TPLDA capture statistical properties of data that are complementary. Matching score of x p and x i under the fusion of classifiers can be expressed as:</p><formula xml:id="formula_6">S(x p , x i |θ 1 , … , θ S ) = Π s = 1 … S S(x p , x i |θ s )<label>(5)</label></formula><p>where θ 1 , … , θ S are the fused classifiers. Later in the experiments, we also apply fusion across video frames f 1 , f 2 , … , f P which can be expressed as </p><formula xml:id="formula_7">S(f 1 … P , x i |θ) = Π p = 1 … P S(f p , x i |θ).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results and Analysis</head><p>Experiments are conducted to evaluate performance of different classification models. The experiments make use of enrollment samples that consist of frontal faces only. The probe faces include half-profile and profile faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets for Evaluation</head><p>Two datasets are collected for the experiments: the VidTIMIT+UMIST dataset <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b29">28]</ref> and the FERET dataset <ref type="bibr" target="#b30">[29]</ref>.</p><p>The VidTIMIT database <ref type="bibr" target="#b28">[27]</ref> contains videos of 43 individuals who are asked to perform an extended sequence of head rotation. The rotation starts with the head facing forward, followed by facing to the right, to the left, back to forward, up, down, and finally return to forward. Three video sequences with a resolution of 512×384 are recorded from each individual in three sessions, respectively. The UMIST database <ref type="bibr" target="#b29">[28]</ref> contains 20 individuals, each of whom appears in various poses ranging from profile to frontal. Faces are captured as grey-scale images with a resolution of 220×220. Eighteen individuals from the UMIST database are merged with those from the VidTIMIT database to yield a total of 61 individuals. Using the merged data, three pairs of training and test sets are constructed. The training sets contain 10+24 individuals from the UMIST and the VIDTIMIT databases, respectively (randomly selected). The test sets contain the remaining 8+19 individuals from the UMIST and the VIDTIMIT databases, respectively.</p><p>The FERET database <ref type="bibr" target="#b30">[29]</ref> contains 1199 individuals captured into 256×384 images. Each individual appears in 7 pose categories: frontal (fa/0), quarter left (ql/-22.5°), quarter right (qr/22.5°), half-profile left (hl/-67.5°), half-profile right (hr/67.5°), profile left (pl/-90°), and profile right (pr/90°). Among the 7 images of each individual, three (ql/-22.5°, fa/0, and qr/22.5°) are classified as "frontal", two (hl/-67.5° and hr/67.5°) are classified as "half-profile", and the other two (pl/-90° and pr/90°) are classified as "profile". Note that faces facing to the left direction (ql, hl, and pl) are flipped horizontally. From the 1199 individuals, 319 are selected for experiments. Three pairs of training and test sets are constructed from the selected data. Each training set contains 219 individuals (randomly selected) and each test set contains the remaining 100 individuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experiments using VidTIMIT+UMIST dataset</head><p>The training and the test data for these experiments are described in Section 3.1. Note that individuals used for testing are completely different from those used for training. The test data are divided into enrollment samples and probe data. The enrollment samples consist of "frontal" faces while the probe data consist of "half-profile" and "profile" faces. To detect faces, facial landmarks, and head poses from face images, the front-end described in Section 2 is employed.</p><p>Figure <ref type="figure" target="#fig_5">3</ref> and Figure <ref type="figure" target="#fig_6">4</ref> show results of the experiments, presented in the form of the number (in percentage) of the successfully recognized images. The Eigen light-fields method is used as the baseline. When only individual classifiers are considered, TFA demonstrates the best recognition rates, i.e., 94.46 ± 0.71% and 70.95 ± 2.68% for half-profile and profile faces, respectively. TPLDA demonstrates recognition rates of 88.81 ± 2.40% and 48.10 ± 6.79% and PLDA demonstrates recognition rates of 85.67 ± 1.47% and 51.38 ± 11.87% for half-profile and profile faces, respectively. The Eigen light-fields method has become the worst performer. It should be noted, however, that the superiority of TFA doesn't apply to experiments with the FERET database (Section 3.3). TFA has therefore simply better captured statistical properties of the data than other classifiers have for this particular dataset.</p><p>Figure <ref type="figure" target="#fig_5">3</ref> also shows recognition results of half profile faces using fusion of classification models (Equation ( <ref type="formula" target="#formula_6">5</ref>)). As can be seen from the figure, all fusion cases have better performance than the corresponding individual models, thus showing the finding that the fused models are complementary (not redundant). The highest recognition rate is achieved by the combination of the three classification models (95.57 ± 1.36%). The second highest recognition rate is achieved by the combination of TFA and PLDA, which are actually the best two individual models (95.25 ± 1.93%). Compared to recognition using individual classification models, peak performance increases from 94.46 ± 0.71% to 95.57 ± 1.36%.  For recognition of profile faces (Figure <ref type="figure" target="#fig_6">4</ref>), fusion significantly outperforms individual classification models only for the combination of PLDA and TPLDA. When the fusion combines TFA (the best individual classification model) and other classification models, it hardly outperforms the individual models or even degrades the performance. These results therefore highlight the second requirement for a fusion to be effective: The fused classifiers should have similar individual performance (as is the case with PLDA and TPLDA). When there is too much discrepancy between the fused classifiers, the gain produced by the fusion is not enough to compensate the discrepancy between the classifiers. Figure <ref type="figure" target="#fig_6">4</ref> shows that the best fusion case corresponds to the combination of the three classification models. This combination reaches peak performance of 72.13 ± 8.49% which is better than the peak performance of individual models (70.95 ± 2.68%). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Half profile/frontal</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experiments using FERET dataset</head><p>Data for these experiments are described in Section 3.1. Classification models are constructed using the training sets, each of which contains 219 individuals. Each test set contains 100 individuals that are further classified as enrollment samples (frontal faces) and probe data (non-frontal faces). To extract appearance features, faces are segmented from the background using an iterative graph-cuts procedure. The segmented faces are registered to standard templates and placed against a mid-gray background. The registration is performed using a piece-wise linear warp based on 21 manually annotated facial landmarks.</p><p>Figure <ref type="figure" target="#fig_8">5</ref> shows recognition results of half-profile faces using frontal faces as samples. For matching to the mean of samples: S(x p , x i• | θ) = P(x p , average(x i• ) | M i , θ) / P(average(x i• ) | M i , θ) is used to compute matching scores since it produces better results than matching to the nearest sample. TPLDA has become the best performer with a peak recognition rate of 81.50 ± 6.61%. TFA and PLDA become the second and the third best performer, respectively, demonstrating recognition rates of 73.67 ± 10.68% and 68.83 ± 2.47%, respectively. These results are thus different from those obtained from the VidTIMIT+UMIST dataset where TFA becomes the best performer followed by TPLDA and PLDA. Figure <ref type="figure" target="#fig_10">6</ref> shows recognition results of profile faces using frontal faces as samples. Similar to previous results, TPLDA, TFA, and PLDA have become the best, the second best, and the third best performers, respectively. TPLDA achieves a peak recognition rate of 55.50 ± 5.20%. TFA and PLDA achieve peak recognition rates of 54.50 ± 6.93% and 50.17 ± 8.28%, respectively. These results are again different from those obtained from the VidTIMIT+UMIST dataset, where TFA, TPLDA, and PLDA become the best, the second best, and the third best performers, respectively. Four combinations of classifiers are also evaluated in the experiments with the FERET database. Figure <ref type="figure" target="#fig_8">5</ref> shows recognition results of half-profile faces using the fused classifiers. As can be seen from the figure, all fusion cases give better performance than the corresponding individual models. The highest recognition rate is achieved by the combination of the three classification models (86.17 ± 3.82%). Figure <ref type="figure" target="#fig_10">6</ref> shows similar situations for recognition of profile faces. All fusion cases have better peak performance than the corresponding individual models, with the combination of the three models becoming the best performer (63.00 ± 5.66%). These results again highlight the finding that the tested classification models are complementary. It should also be noted that the three individual models have similar performance, explaining why the fusion is effective. Compared to recognition using individual classification models, the fusion increases peak recognition rates from 81.50 ± 6.61% to 86.17 ± 3.82% for recognition of halfprofile faces and from 55.50 ± 5.20% to 63.50 ± 5.66% for recognition of profile faces. From experiments on the VidTIMIT+UMIST dataset as well as on the FERET dataset, it can be concluded that fusion of different classifiers effectively improves face recognition across poses. The combinations of classifiers, however, perform differently on different datasets. It appears that when the fused classifiers differ only slightly in performance, the fused classifiers have better performance than the individual classifiers. To choose the most optimal combination of classifiers for a particular deployment, the fusion can be tested on a validation data before it is employed in the real task. Another possibility is simply fusing the three classification models altogether. It has been observed that fusion of the three models outperform the three individual classifiers most of the time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Experiments on Videos</head><p>Previous experiments compute recognition rates by counting the number of successfully recognized images from test videos. Even though frames of training videos have been collectively used to construct classification models, recognition in these experiments is still performed based on still images. The reported recognition rates thus indicate only the probability of correct recognition, given a single image as input. To actually employ videos in the recognition, identities need to be inferred based on multiple images. In this section, such recognition is performed by fusing matching scores across video frames, which is also known in the literature as the decision level fusion. Two fusion methods are considered: voting and product rule (multiplying matching scores, Equation ( <ref type="formula" target="#formula_7">6</ref>)).</p><p>Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref> show results of the fusion on the VidTIMIT+UMIST dataset. Probabilistic latent variable models are trained to include 42 basis vectors and matching to the nearest sample is used to compute matching scores of individual images. For recognition of half-profile faces (Table <ref type="table" target="#tab_0">1</ref>), fusion across video frames has given a better peak performance than using a single still image (98.77 ± 2.14% vs. 95.57 ± 1.36%, Section 3.2). The best recognition rate is achieved when TFA or combinations of TFA and other classifiers are employed together with the product rule. Note that TFA seems to be dominant whenever it is combined with other classifiers. This can be seen from the performance of the combined classifiers, which is identical to the performance of TFA alone. For recognition of profile faces (Table <ref type="table" target="#tab_1">2</ref>), fusion across video frames has also given a better peak performance than using a single still image (81.48 ± 11.11% vs 72.13 ± 8.49%, Section 3.2). The best performance is achieved by the combination of TFA and PLDA coupled with the product rule.</p><p>Table <ref type="table" target="#tab_2">3</ref> and Table <ref type="table" target="#tab_3">4</ref> show results of the fusion on the FERET dataset. Compared to recognition using a single still image (Section 3. 1977 peak performance: 91.33 ± 3.21% vs. 86.17 ± 3.82% and 64.50 ± 3.54% vs. 63.50 ± 5.66% for half-profile and profile faces, respectively. The best recognition rate is achieved when the product rule is applied to matching scores obtained from the combination of the three classification models. Note that voting is not tested on this dataset since there are only two probe images for each individual. The improved performance given by fusion across multiple frames on the Vid-TIMIT+UMIST and the FERET datasets highlights the advantages of using video over single still image. The multiple observations available in videos provide additional information that can be employed to solve ambiguity in recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>This research evaluates the application of probabilistic latent variable models, namely PLDA, TPLDA, and TFA, as well as fusion of these classifiers, to face recognition across poses. Half-profile and profile faces are used as inputs to the recognition system, where frontal faces are used as enrollment samples. The evaluation is conducted using still images and videos, in particular, the VidTIMIT+UMIST and the FERET datasets are collected for this purpose.</p><p>Results of the experiments have shown that fusion of classifiers (at the decision level, i.e., product rule) generally produces better recognition performance than individual classifiers. This proves that different probabilistic latent variable models learn and capture statistical properties of data that are complementary. There is an important note, though, that fusion seems to produce clear improvement when the fused individual classifiers only slightly differ in performance. The optimal combination of classifiers also seems to vary from dataset to dataset. For the VidTIMIT+UMIST dataset, the peak performance increases from 94.46 ± 0.71% to 95.57 ± 1.36% and from 70.95 ± 2.68% to 72.13 ± 8.49% for recognition of half-profile faces and profile faces, respectively. For the FERET dataset, the peak performance increases from 81.50 ± 6.61% to 86.17 ± 3.82% and from 55.50 ± 5.20% to 63.50 ± 5.66% for recognition of half profile and profile faces, respectively.</p><p>To actually employ videos for face recognition, fusion has also been applied across video frames. Product rule and voting are used as the fusion method at the decision level. Results of experiments have shown that recognition using videos produces better performance than using single still image. For the VidTIMIT+UMIST dataset, the peak performance increases from 95.57 ± 1.36% to 98.77 ± 2.14% and from 72.13 ± 8.49% to 81.48 ± 11.11% for recognition of half-profile faces and profile faces, respectively. For the FERET dataset, the peak performance increases from 86.17 ± 3.82% to 91.33 ± 3.21% and from 63.50 ± 5.66% to 64.50 ± 3.54% for recognition of half-profile faces and profile faces, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>ISSN: 1693-6930 TELKOMNIKA Vol. 15, No. 4, December 2017 : 1969 -1979 1970</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The proposed recognition method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Normalization using piece-wise triangular warp</figDesc><graphic coords="3,154.00,586.11,85.10,81.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ISSN: 1693-6930 TELKOMNIKA Vol. 15, No. 4, December 2017 : 1969 -1979 1972</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>TELKOMNIKA</head><label></label><figDesc>ISSN: 1693-6930  Improved Face Recognition across Poses using Fusion of Probabilistic... (Moh Edi Wibowo) 1973</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Results of Recognition of Half-Profile Faces from the VidTIMIT+UMIST Dataset</figDesc><graphic coords="6,130.10,103.00,220.55,166.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Results of Recognition of Profile Faces from the VidTIMIT+UMIST Dataset</figDesc><graphic coords="6,130.70,356.05,219.95,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>across Poses using Fusion of Probabilistic... (Moh Edi Wibowo) 1975</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Results of Recognition of Half-Profile Faces from the FERET Dataset</figDesc><graphic coords="7,103.65,360.93,388.01,210.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>ISSN: 1693-6930 TELKOMNIKA Vol. 15, No. 4, December 2017 : 1969 -1979 1976</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Results of Recognition of Profile Faces from the FERET Dataset</figDesc><graphic coords="8,131.30,228.45,222.60,166.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>3), fusion across video frames has given better across Poses using Fusion of Probabilistic... (Moh EdiWibowo)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Frame Fusion on the Recognition of Half-Profile Faces from the VidTIMIT+UMIST Dataset</figDesc><table><row><cell>Classification Model</cell><cell>Fusion Method</cell><cell>Recognition Rate (%)</cell></row><row><cell>PLDA</cell><cell>Voting</cell><cell>85.19 ± 3.71</cell></row><row><cell>PLDA</cell><cell>Product Rule</cell><cell>86.42 ± 2.14</cell></row><row><cell>TFA</cell><cell>Voting</cell><cell>97.53 ± 4.28</cell></row><row><cell>TFA</cell><cell>Product Rule</cell><cell>98.77 ± 2.14</cell></row><row><cell>TPLDA</cell><cell>Voting</cell><cell>95.06 ± 2.14</cell></row><row><cell>TPLDA</cell><cell>Product Rule</cell><cell>95.06 ± 2.14</cell></row><row><cell>PLDA + TFA</cell><cell>Voting</cell><cell>98.77 ± 2.14</cell></row><row><cell>PLDA + TFA</cell><cell>Product Rule</cell><cell>98.77 ± 2.14</cell></row><row><cell>PLDA + TPLDA</cell><cell>Voting</cell><cell>96.30 ± 0.00</cell></row><row><cell>PLDA + TPLDA</cell><cell>Product Rule</cell><cell>95.06 ± 2.14</cell></row><row><cell>TFA + TPLDA</cell><cell>Voting</cell><cell>97.53 ± 2.14</cell></row><row><cell>TFA + TPLDA</cell><cell>Product Rule</cell><cell>98.77 ± 2.14</cell></row><row><cell>PLDA + TFA + TPLDA</cell><cell>Voting</cell><cell>98.77 ± 2.14</cell></row><row><cell>PLDA + TFA + TPLDA</cell><cell>Product Rule</cell><cell>97.53 ± 2.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Frame Fusion on the Recognition of Profile Faces from the VidTIMIT+UMIST Dataset</figDesc><table><row><cell>Classification Model</cell><cell>Fusion Method</cell><cell>Recognition Rate (%)</cell></row><row><cell>PLDA</cell><cell>Voting</cell><cell>77.78 ± 9.80</cell></row><row><cell>PLDA</cell><cell>Product Rule</cell><cell>79.01 ± 8.56</cell></row><row><cell>TFA</cell><cell>Voting</cell><cell>55.56 ± 9.80</cell></row><row><cell>TFA</cell><cell>Product Rule</cell><cell>58.02 ± 13.01</cell></row><row><cell>TPLDA</cell><cell>Voting</cell><cell>62.96 ± 11.11</cell></row><row><cell>TPLDA</cell><cell>Product Rule</cell><cell>62.96 ± 9.79</cell></row><row><cell>PLDA + TFA</cell><cell>Voting</cell><cell>79.01 ± 11.31</cell></row><row><cell>PLDA + TFA</cell><cell>Product Rule</cell><cell>81.48 ± 11.11</cell></row><row><cell>PLDA + TPLDA</cell><cell>Voting</cell><cell>59.26 ± 12.83</cell></row><row><cell>PLDA + TPLDA</cell><cell>Product Rule</cell><cell>62.96 ± 12.83</cell></row><row><cell>TFA + TPLDA</cell><cell>Voting</cell><cell>72.84 ± 9.32</cell></row><row><cell>TFA + TPLDA</cell><cell>Product Rule</cell><cell>70.37 ± 9.80</cell></row><row><cell>PLDA + TFA + TPLDA</cell><cell>Voting</cell><cell>75.31 ± 10.69</cell></row><row><cell>PLDA + TFA + TPLDA</cell><cell>Product Rule</cell><cell>79.01 ± 11.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Frame Fusion on the Recognition of Half-Profile Faces from the FERET Dataset</figDesc><table><row><cell>Classification Model</cell><cell>Fusion Method</cell><cell>Recognition Rate (%)</cell></row><row><cell>PLDA</cell><cell>Product Rule</cell><cell>79.00 ± 11.53</cell></row><row><cell>TFA</cell><cell>Product Rule</cell><cell>74.33 ± 6.66</cell></row><row><cell>TPLDA</cell><cell>Product Rule</cell><cell>81.67 ± 4.93</cell></row><row><cell>PLDA + TFA</cell><cell>Product Rule</cell><cell>88.67 ± 3.79</cell></row><row><cell>PLDA + TPLDA</cell><cell>Product Rule</cell><cell>89.00 ± 1.00</cell></row><row><cell>TFA + TPLDA</cell><cell>Product Rule</cell><cell>88.33 ± 4.04</cell></row><row><cell>PLDA + TFA + TPLDA</cell><cell>Product Rule</cell><cell>91.33 ± 3.21</cell></row></table><note><p>TELKOMNIKA Vol. 15, No. 4, December 2017 : 1969 -1979 1978</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Frame Fusion on the Recognition of Profile Faces from the FERET Dataset</figDesc><table><row><cell>Classification Model</cell><cell>Fusion Method</cell><cell>Recognition Rate (%)</cell></row><row><cell>PLDA</cell><cell>Product Rule</cell><cell>50.00 ± 18.38</cell></row><row><cell>TFA</cell><cell>Product Rule</cell><cell>41.50 ± 12.02</cell></row><row><cell>TPLDA</cell><cell>Product Rule</cell><cell>54.50 ± 6.36</cell></row><row><cell>PLDA + TFA</cell><cell>Product Rule</cell><cell>55.50 ± 6.36</cell></row><row><cell>PLDA + TPLDA</cell><cell>Product Rule</cell><cell>60.50 ± 4.95</cell></row><row><cell>TFA + TPLDA</cell><cell>Product Rule</cell><cell>61.50 ± 9.19</cell></row><row><cell>PLDA + TFA + TPLDA</cell><cell>Product Rule</cell><cell>64.50 ± 3.54</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Handbook of Face Recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Study of Automated Face Recognition System for Office Door Access Control Application</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 3rd International Conference on Communication Software and Networks</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="132" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Facial Recognition in Multimodal Biometrics System for Finger Disabled Applicants</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mazlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suliman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Indonesian Journal of Electrical Engineering and Computer Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="638" to="645" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face Recognition at a Distance System for Surveillance Applications</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth IEEE International Conference on Biometrics: Theory Applications and Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face Recognition for Smart Interactions</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Ekenel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1007" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognition of a Face in a Mixed Document</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bouhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Ayachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fakir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oukessou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TELKOMNIKA Indonesian Journal of Electrical Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="301" to="312" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Toward Pose-Invariant 2-D Face Recognition through Point Distribution Models and Facial Symmetry</title>
		<author>
			<persName><forename type="first">D</forename><surname>González-Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alba-Castro</forename><surname>Jl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="413" to="429" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face Recognition under Pose Variations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Franklin Institute</title>
		<imprint>
			<biblScope unit="volume">343</biblScope>
			<biblScope unit="page" from="596" to="613" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Face Recognition Based on Frontal Views Generated from Non-Frontal Images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><surname>Telkomnika</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m"> Improved Face Recognition across Poses using Fusion of Probabilistic</title>
		<imprint>
			<biblScope unit="page">1979</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Locally Linear Regression for Pose-Invariant Face Recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1716" to="1725" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Von Der Malsburg C. Analysis and Synthesis of Pose Variations of Human Faces by a Linear PCMAP Model and Its Application for Pose-Invariant Face Recognition System</title>
		<author>
			<persName><forename type="first">K</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="142" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pose-Invariant Face Recognition with Parametric Linear Subspaces</title>
		<author>
			<persName><forename type="first">K</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Der Malsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="64" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single-View Based Recognition of Faces Rotated in Depth</title>
		<author>
			<persName><forename type="first">T</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Der Malsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Workshop on Automatic Face and Gesture Recognition</title>
		<imprint>
			<biblScope unit="page" from="176" to="181" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On Transforming Statistical Models for Non-Frontal Face Verification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="288" to="302" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Face Recognition Based on Fitting a 3D Morphable Model</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1063" to="1074" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Locally Linear Discriminant Analysis for Multimodally Distributed Classes for Face Recognition with a Single Model Image</title>
		<author>
			<persName><forename type="first">T-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">KPCA plus LDA: A Complete Kernel Fisher Discriminant Framework for Feature Extraction and Recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="230" to="244" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Appearance-Based Face Recognition and Light-Fields</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="449" to="465" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probabilistic Linear Discriminant Analysis for Inferences about Identity</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tied Factor Analysis for Face Recognition across Large Pose Differences</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Felisberti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="970" to="984" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probabilistic Models for Inference about Identity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="144" to="157" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rapid Object Detection using a Boosted Cascade of Simple Features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face Alignment by Explicit Shape Regression</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradual Training of Cascaded Shape Regression for Facial Landmark Localization and Pose Estimation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Wibowo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tjondronegoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Active Shape Models -Their Training and Application</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Maximum Likelihood from Incomplete Data via the EM Algorithm</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rubin</forename><surname>Db</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-Region Probabilistic Histograms for Robust and Scalable Identity Inference</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Biometrics</title>
		<imprint>
			<biblScope unit="page" from="199" to="208" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Face Recognition: From Theory to Applications</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Allinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NATO ASI Series F. Computer and Systems Sciences</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="446" to="456" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The FERET Database and Evaluation Procedure for Face-Recognition Algorithms</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="295" to="306" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
