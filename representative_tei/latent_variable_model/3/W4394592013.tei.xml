<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DGP-LVM: Derivative Gaussian process latent variable models</title>
				<funder>
					<orgName type="full">International Max Planck Research School for Intelligent Systems</orgName>
				</funder>
				<funder ref="#_PPnpGA5">
					<orgName type="full">Cluster of Excellence iFIT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-31">31 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Soham</forename><surname>Mukherjee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Technical University of Dortmund</orgName>
								<address>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Medicine</orgName>
								<orgName type="institution" key="instit1">University Hospital Tübingen</orgName>
								<orgName type="institution" key="instit2">University of Tübingen</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manfred</forename><surname>Claassen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Medicine</orgName>
								<orgName type="institution" key="instit1">University Hospital Tübingen</orgName>
								<orgName type="institution" key="instit2">University of Tübingen</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul-Christian</forename><surname>Bürkner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Technical University of Dortmund</orgName>
								<address>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DGP-LVM: Derivative Gaussian process latent variable models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-31">31 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2404.04074v3[stat.ME]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gaussian processes</term>
					<term>single-cell RNA</term>
					<term>Bayesian inference</term>
					<term>derivative GPs</term>
					<term>latent variables</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop a framework for derivative Gaussian process latent variable models (DGP-LVMs) that can handle multi-dimensional output data using modified derivative covariance functions. The modifications account for complexities in the underlying data generating process such as scaled derivatives, varying information across multiple output dimensions as well as interactions between outputs. Further, our framework provides uncertainty estimates for each latent variable samples using Bayesian inference. Through extensive simulations, we demonstrate that latent variable estimation accuracy can be drastically increased by including derivative information due to our proposed covariance function modifications. The developments are motivated by a concrete biological research problem involving the estimation of the unobserved cellular ordering from single-cell RNA (scRNA) sequencing data for gene expression and its corresponding derivative information known as RNA velocity. Since the RNA velocity is only an estimate of the exact derivative information, the derivative covariance functions need to account for potential scale differences. In a real-world case study, we illustrate the application of DGP-LVMs to such scRNA sequencing data. While motivated by this biological problem, our framework is generally applicable to all kinds of latent variable estimation problems involving derivative information irrespective of the field of study.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gaussian processes (GPs) are a class of statistical models known for their flexible structure and favourable properties to analyse complex data <ref type="bibr" target="#b39">(Williams and Rasmussen, 1995)</ref>. Since their inception, several extensions have been proposed, among which the most relevant ones to this paper are adding derivative information to GPs <ref type="bibr" target="#b32">(Solak et al., 2002;</ref><ref type="bibr" target="#b29">Rasmussen and Williams, 2006)</ref>, building GPs with multiple outputs <ref type="bibr" target="#b9">(Cressie, 1993;</ref><ref type="bibr" target="#b34">Teh et al., 2005;</ref><ref type="bibr" target="#b29">Rasmussen and Williams, 2006)</ref> as well as modelling latent input variables <ref type="bibr" target="#b19">(Lawrence, 2003</ref><ref type="bibr" target="#b20">(Lawrence, , 2005))</ref>. Since differentiation is a linear operation, any variable and its derivative would be linearly related. For the same reason, as a fundamental property of GPs, a derivative of a GP is just another GP with a related covariance function. Together, this results in a single GP model for the outputs and their derivatives with a joint derivative covariance function <ref type="bibr" target="#b32">(Solak et al., 2002)</ref>.</p><p>Derivative GPs have usually been studied and designed to model a single vector-valued output and have not been extended for multiple outputs. Multi-output GPs are most suitable when the response or output of the model contains multiple features, each expressed by its own dimension. One could fit individual GPs for each feature but then risks substantial loss of information in case of interactions between features. Thus a two-fold covariance structure was suggested in <ref type="bibr" target="#b34">(Teh et al., 2005)</ref> allowing GPs to account for this shared information between features. We extend this two-fold covariance structure to derivative GPs. As we will motivate more in a bit, our primary aim is to estimate latent (input) variables from observed input variables measured with error and output variables connected to the inputs via GPs; a challenge leading to what are called latent GPs <ref type="bibr" target="#b19">(Lawrence, 2003)</ref>. When using derivative GPs, such latent inputs are shared between the original outputs and their derivative counterparts, effectively doubling the amount of information available for estimating the latent inputs.</p><p>In real-world data, the derivatives are seldom exactly computed, which adds a major challenge to the modelling endeavour. If derivatives are computed with respect to the observed (non-latent) inputs, they are naturally only an approximation of the derivatives with respect to latent inputs. Conversely, if the derivatives are (implicitly) computed with respect to the latent inputs, the uncertainty of the latter will induce hard-to-quantify uncertainty in the estimated derivatives. One way or another, this lack of exact derivative information poses a serious challenge for derivative GP modelling. Existing approaches are not equipped to deal with the significant scale differences between outputs and derivatives, thus requiring modifications in its covariance functions to ensure valid and efficient latent variable estimation.</p><p>In this paper, we demonstrate a combination of all the above model extensions leading to our DGP-LVM: derivative Gaussian process latent variable model framework. We provide more context about the real-world modelling challenges in Section 1.1 as a basis for our motivation to develop DGP-LVM. The remainder of the paper is structured as follows. We discuss and provide context on related works in Section 2. We introduce our methodology and model development in Section 3 and perform extensive simulation studies in Section 4 that demonstrate the relevance of our contributions. We further illustrate DGP-LVM on a real single-cell RNA sequencing data in Section 5 before discussing our methods' limitations and future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivation</head><p>In developmental biology, to describe temporal biological processes, researchers use stochastic approaches to understand cellular progression, that is, how cells develop and undergo changes in their state throughout various stages over the period of time <ref type="bibr" target="#b23">(Maamar et al., 2007;</ref><ref type="bibr" target="#b22">Losick and Desplan, 2008;</ref><ref type="bibr" target="#b28">Raj and Oudenaarden, 2008)</ref>. Currently, this problem is frequently tackled with single-cell RNA sequencing (scRNA-seq) technologies by analysing messenger-RNA (mRNA) molecule counts as a measure of gene expression <ref type="bibr" target="#b12">(Haque et al., 2017)</ref>. The measured gene expression (also called expression levels) provide the necessary information about the nature of cells at a specific point of time, also known as cell states, as well as their changes over time. However, due to the experimental limitations of the current sequencing methods each cell gets destroyed in the measurement process and can therefore be observed only once. This situation makes it difficult to infer cell state transitions and the overall sequence of cell states of a temporal biological process. To that end, pseudotime ordering is a popular approach to describe such a biological process as a sequence of cell states along a time sequence <ref type="bibr" target="#b37">(Trapnell et al., 2014)</ref>.</p><p>Single-cell gene expression data provides information about cell state snapshots. While conventional pseudotime ordering approaches operate only on cell state snapshots to estimate pseudotime, only recently, directional information about cell state changes (i.e., derivative information) has been accounted for in this task <ref type="bibr" target="#b11">(Gupta et al., 2022)</ref>. Here, we hypothesise and demonstrate empirically later that including directional information on cell state transitions increases the precision in estimating pseudotime. This directional information is available through a quantity known as RNA velocity that is estimated from the difference in unspliced and spliced gene expression levels over latent experimental time (not to be confused with pseudotime) (La <ref type="bibr" target="#b17">Manno et al., 2018;</ref><ref type="bibr" target="#b1">Bergen et al., 2020)</ref>. By construction, this RNA velocity estimates the derivative of spliced gene expression data with respect to time. Concretely, our aim is to enable using the combination of RNA gene expression and RNA velocity in a single probabilistic framework for pseudotime estimation. This combination of RNA gene expression and its corresponding RNA velocity requires a novel statistical model approach.</p><p>In order to model such data, certain requirements must be satisfied. Starting from support for multi-dimensional outputs that allows inclusion of several genes for each cell, the model should account for varying gene-specific information as well as possible biologically induced interactions among genes. Moreover, since RNA velocity is only a derivative estimation of gene expression levels, they are frequently on a significantly different scale than the gene expression levels. Dealing with this scale difference is a challenge that the model must address in order to provide reliable pseudotime estimates. In this paper, we demonstrate that DGP-LVM is able to tackle all of the above challenges and can estimate latent input variables with significantly higher accuracy than other GP models. Thus, we also demonstrate its potential to be applicable to estimating pseudotime through RNA gene expressions and their corresponding RNA velocities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions</head><p>• We develop a probabilistic GP modelling framework for latent (input) variable estimation using derivative information for any multi-dimensional data-generating process. Our model accounts for dimension specific information and interactions between dimensions in a multi-dimensional data scenario which are common in (but not limited to) fields like single-cell biology.</p><p>• We develop a custom derivative structure for Squared Exponential (SE) and Matern class of covariance functions that is able to account for significant scale differences between the outputs and its corresponding derivatives.</p><p>• Through extensive simulations, we demonstrate that our model provides substantially more accurate latent variable estimates than other GP models under realistic scenarios.</p><p>• We showcase the application of our modelling approach on a reduced real-world scRNA-seq data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Gaussian processes, as a class of models, underwent a wide range of extensions over the years giving rise to various forms of GP models. Specifically, three broad extensions relevant to this work are GPs with derivative information, multi-output GPs and GPs for latent variable modelling. Using derivative information for Gaussian processes was introduced in <ref type="bibr" target="#b32">Solak et al. (2002)</ref> who replaced standard covariance functions with their derivative counterparts. This paved the way to modelling data along with its derivatives as a single GP model. Recently, derivative GPs were extended to support multiple inputs and scalable approximations <ref type="bibr" target="#b10">(Eriksson et al., 2018;</ref><ref type="bibr" target="#b27">Padidar et al., 2021)</ref>. In case of multi-output GPs, recent works <ref type="bibr" target="#b25">(Moreno-Muñoz et al., 2018;</ref><ref type="bibr" target="#b16">Joukov and Kulić, 2022)</ref> study GPs with support for multiple outputs that are of varying nature in terms of data types, however multi-output GPs with derivative information have not been studied in detail yet. Latent GPs were introduced by <ref type="bibr" target="#b19">Lawrence (2003</ref><ref type="bibr" target="#b20">Lawrence ( , 2005) )</ref> and have, so far, been predominantly used for dimensionality reduction <ref type="bibr" target="#b35">(Titsias, 2009;</ref><ref type="bibr" target="#b36">Titsias and Lawrence, 2010)</ref>. More recently, for similar applications of dimension-reduction technique, extensions on GP latent variable model for non-Gaussian likelihoods with different types of latent input structures were discussed in <ref type="bibr" target="#b18">Lalchand et al. (2022)</ref>. These works also focus on scalable approximations to latent GPs. In contrast, we focus on estimating latent inputs that probabilistically explains a dependent multi-output variable.</p><p>For the modelling of scRNA-seq data, GPs have been broadly applied in two relevant directions, specifically, for clustering <ref type="bibr" target="#b4">(Buettner and Theis, 2012;</ref><ref type="bibr" target="#b3">Buettner et al., 2015)</ref> and temporal modelling <ref type="bibr">(Hensman et al., 2013b)</ref>. Considering the latter, pseudotime estimation constitutes a major research direction as it is directly related to understanding the true underlying biological processes. It has been shown previously that point estimates of pseudotime are highly prone to infer false cellular ordering, thus suggesting Bayesian inference to provide uncertainty estimates alongside each estimated pseudotime <ref type="bibr" target="#b7">(Campbell and Yau, 2015)</ref>. Further works focus on latent pseudotime estimations <ref type="bibr" target="#b30">(Reid and Wernisch, 2016;</ref><ref type="bibr" target="#b8">Campbell and Yau, 2018)</ref> along with branching structures for trajectory inference <ref type="bibr" target="#b0">(Ahmed et al., 2019)</ref> based on a GP framework. One of the main limitations in these works lie in their restricted use of gene expressions, taking into account expression profile snapshots as the only available information regarding cellular ordering. We provide evidence that including RNA velocity as derivative information holds the power to estimate latent pseudotime with increased precision compared to what previous approaches could achieve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>We develop DGP-LVM, a framework for derivative Gaussian process modelling with the primary goal of estimating latent variables serving as (implicit) inputs to the GP. As the general setup, we consider a pair of variables (y, x) where y is the output (response) variable and x is the input variable (covariate), with individual observations denoted as y i , x i ∈ R, i = {1, ..., N } where N is the number of observations. In addition to y itself, we incorporate the derivative outputs y ′ = δy/δx into the model. The components of DGP-LVM are first discussed individually, before we combine them into a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Derivative Gaussian processes</head><p>A GP is a stochastic process specified by a mean function m = m(x), and a covariance function K = K(x, x T ), where x T indicate transpose of x, such that a finite set of these points will follow a multivariate Gaussian distribution <ref type="bibr" target="#b39">(Williams and Rasmussen, 1995)</ref>. Concretely, we consider GPs f (x) such that f (x) ∼ GP(m, K). Here we consider a constant mean function (similar to an intercept in regression models). If the output variable y is univariate, modelling the relationship of x and y via a (single-output) GP and independent additive noise can be written as</p><formula xml:id="formula_0">y i = f (x i ) + ε i ,<label>(1)</label></formula><p>where ε i is the i th sample of ε ∼ N (0, σ 2 ) assuming equal-variance Gaussian noise. Together, this is equivalent to</p><formula xml:id="formula_1">y i ∼ N (f (x i ), σ 2 ).<label>(2)</label></formula><p>For, i ̸ = j we have Cov(y i , y j ) = K(x i , x j ) and for i = j, we have Cov(y i , y j ) = Var(y i ) = K(x i , x j ) + σ 2 . The above notation will be extended to multi-output GPs in Section 3.2.</p><p>GPs are able to take advantage of derivatives in addition to its corresponding sample data to increase model accuracy. Since differentiation is a linear operator, a derivative of a GP is just another GP <ref type="bibr" target="#b32">(Solak et al., 2002;</ref><ref type="bibr" target="#b29">Rasmussen and Williams, 2006)</ref>. This property of GPs can be utilised to take derivative of a joint covariance structure of both y and y ′ , if the second order derivative of the covariance function exists. The (joint) derivative GP is then given by</p><formula xml:id="formula_2">f (x) f ′ (x) ∼ GP m f m f ′ , K K ′ K ′T K ′′ ,<label>(3)</label></formula><p>where m f and m f ′ are constant mean functions corresponding to GP f and it's derivative f ′ respectively. K ′ is the first derivative of the covariance function K = K(x, x T ) with respect to x and K ′T is the first derivative of the covariance function with respect to x T . K ′′ is the second order partial derivative of K differentiating both with respect to x and x T . In other words, differentiation simply propagates through the covariance function (see Appendix A for mathematical details). The specific properties of such derivative GP models depend on the chosen covariance function. A common choice is the Squared Exponential (SE) covariance function with hyperparameters ρ as length scale and α as the GP marginal standard deviation (SD). The derivative version of the SE covariance function is given by</p><formula xml:id="formula_3">K(x i , x j ) = α 2 exp - (x i -x j ) 2 2ρ 2 , (<label>4</label></formula><formula xml:id="formula_4">) K ′ (x i , x j ) = α 2 (x i -x j ) ρ 2 exp - (x i -x j ) 2 2ρ 2 , (<label>5</label></formula><formula xml:id="formula_5">)</formula><formula xml:id="formula_6">K ′′ (x i , x j ) = α 2 ρ 4 (ρ 2 -(x i -x j ) 2 ) exp - (x i -x j ) 2 2ρ 2 . (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>Derivative covariance functions are obtainable generally for any chosen covariance function whose second order derivative exists. In this paper, we focus on SE and Matern class covariance functions as perhaps the most common choices. We provide further details on the mathematical forms of derivative SE, Matern 3/2 and Matern 5/2 covariance functions in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Customised hyperparameters</head><p>Properly including the derivative observations y ′ requires more than just using a basic derivative covariance function. Due to the properties of differentiation, y ′ can be on a fundamentally different scale than y and thus needs to be treated as such. In addition to having different signals (i.e., different GP components f (x) vs. f ′ (x)), the error SDs for y and y ′ will also be different. Moreover, for real data, the observations y ′ containing derivative information may not be the exact same as the true derivatives δy/δx, but only be proportional to them (see Section 1). This proportionality induces a scale difference between y ′ and what is canonically modelled by a basic derivative covariance function. This creates a major issue for models ignoring scale differences as we demonstrate in our simulations.</p><p>To incorporate these scale considerations into our model, we propose to adjust the covariance function hyperparameters. Again using the SE covariance function as an example, we propose to introduce a second marginal SD parameter α ′ , corresponding to the derivative part of the GP, while α now only concerns the original part of the GP:</p><formula xml:id="formula_8">K(x i , x j ) = α 2 exp - (x i -x j ) 2 2ρ 2 , (<label>7</label></formula><formula xml:id="formula_9">) K ′ (x i , x j ) = αα ′ (x i -x j ) ρ 2 exp - (x i -x j ) 2 2ρ 2 , (<label>8</label></formula><formula xml:id="formula_10">)</formula><formula xml:id="formula_11">K ′′ (x i , x j ) = α ′2 ρ 4 (ρ 2 -(x i -x j ) 2 ) exp - (x i -x j ) 2 2ρ 2 . (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>In other words, since K(x i , x j ) = Cov(y i , y j ) we account for the GP marginal variance through α 2 since we are only concerned with the original part of the GP f . In the case of Cov(y i , y ′ j ) and Cov(y ′ i , y j ), we compute K ′ (x i , x j ) and thus we account for the GP marginal variance through αα ′ where α belongs to f and α ′ belongs to f ′ . Finally, in the case of Cov(y ′ i , y ′ j ), we compute K ′′ (x i , x j ) and are thus accounting for the GP marginal variance through α ′2 since we are only dealing with the derivative part of the GP f ′ . Since, in addition to the product term αα ′ in K ′ , we also estimate α 2 and α ′2 separately through K and K ′′ , respectively, estimating both α and α ′ does not cause identifiability issues.</p><p>Similarly, we define two residual standard deviation parameters σ and σ ′ , accounting for measurement noise in y and y ′ , respectively. The scale of ρ is only dependent on the scale of x, which is constant across outputs and their derivatives, such that ρ does not need to be split up into two parameters. Together, independent of specifically chosen covariance function, the DGP-LVM on y and y ′ with independent, additive Gaussian noise is then specified as</p><formula xml:id="formula_13">y i ∼ N (f (x i ), σ 2 ) and y ′ i ∼ N (f ′ (x i ), σ ′2 ). (<label>10</label></formula><formula xml:id="formula_14">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multidimensional outputs</head><p>Multivariate output GPs (or multi-output GPs) model multiple response variables {y 1 , ..., y D } jointly over D &gt; 1 output dimensions <ref type="bibr" target="#b29">(Rasmussen and Williams, 2006)</ref>  <ref type="bibr" target="#b34">(Teh et al., 2005;</ref><ref type="bibr" target="#b2">Bonilla et al., 2007)</ref>. That is, for each observation i, we obtain a vector of across-dimension correlated GP values as</p><formula xml:id="formula_15">  f1 (x i ) . . . fD (x i )   = L ×   f 1 (x i ) . . . f D (x i )   , (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>where L is the Cholesky factor of C, that is, C = LL T with L being lower-triangular. This way, multi-output GPs combine two dependency structures, one within dimensions (and across observations) as expressed by the univariate GPs through corresponding covariance functions and one across output dimensions (but within observations) as expressed by C (or L).</p><p>This readily generalizes to derivative GPs by applying Equation (11) to the derivative GP values f ′ d (x i ) as well, which results in the across-dimension correlated values f ′ d (x i ). Adding independent Gaussian noise, our derivative multi-output GP model then implies for all d and i:</p><formula xml:id="formula_17">y di ∼ N ( fd (x i ), σ 2 d ) and y ′ di ∼ N ( f ′ d (x i ), σ ′2 d ). (<label>12</label></formula><formula xml:id="formula_18">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Latent variable inputs</head><p>So far, we have considered the input x to be known exactly. However, in practice, we often only have a noisy measurement x of x available. In this context, the true x becomes a latent variable, which needs to be appropriately modelled and subsequently estimated. If we assume that the measurements x are Gaussian with known measurement SD s, we can write for each observation i:</p><formula xml:id="formula_19">xi ∼ N (x i , s 2 ). (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>The implied latent x i is then passed to the GP covariance function, which results in what is known as latent(-input) GPs <ref type="bibr" target="#b19">(Lawrence, 2003</ref><ref type="bibr" target="#b20">(Lawrence, , 2005;;</ref><ref type="bibr" target="#b36">Titsias and Lawrence, 2010)</ref>. Such latentinput GPs are even harder to fit than their non-latent counterparts: Not only does the number of unknowns increase substantially, but also new identification issues arise due to both x and ρ now being unknown (see Section 3.4 for details on how we deal with this).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The full model</head><p>Below, we summarize all the extensions that together make up our proposed DGP-LVM framework. To shorten the notation, let us denote the vector of GP hyperparameters for dimension d as θ d . For our considered covariance functions, θ d includes the length scale ρ d , GP marginal SDs α d and α ′ d as well as the error SDs σ d and σ ′ D . Considering the SE covariance function as an example, full DGP-LVMs are then specified as follows:</p><formula xml:id="formula_21">f d (x) f ′ d (x) ∼ GP m f m f ′ , K d K ′ d K ′T d K ′′ d , K d (x i , x j ) = α 2 d exp - (x i -x j ) 2 2ρ 2 d , K ′ d (x i , x j ) = α d α ′ d (x i -x j ) ρ 2 d exp - (x i -x j ) 2 2ρ 2 d , K ′′ d (x i , x j ) = α ′2 d ρ 4 d (ρ 2 d -(x i -x j ) 2 ) exp - (x i -x j ) 2 2ρ 2 d , y di ∼ N (f d (x i ), σ 2 d ), y ′ di ∼ N (f ′ d (x i ), σ ′2 d ), xi ∼ N (x i , s 2 ), θ d ∼ p(θ d ) = p(ρ d ) p(α d ) p(α ′ d ) p(σ d ) p(σ ′ d ). (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>Following the above specifications, after marginalizing out f and f ′ , the multi-output joint probability density factorizes as</p><formula xml:id="formula_23">p(y, y ′ , x, θ) = D d p(y d | x, θ d ) p(y ′ d | x, θ d ) p(x) p(θ d ). (<label>15</label></formula><formula xml:id="formula_24">)</formula><p>where p(y d | x, θ d ) and p(y ′ d | x, θ d ) denote the respective GP-based likelihoods for a single output dimension. p(x) denotes the prior for the latent x implied by the measurement model Eq.( <ref type="formula" target="#formula_19">13</ref>). More details on the choice of prior distributions are discussed in Section 4.2. Using Bayes' rule, we obtain the joint posterior over x and θ as</p><formula xml:id="formula_25">p(x, θ | y, y ′ ) = p(y, y ′ , x, θ) p(y, y ′ , x, θ) dx dθ . (<label>16</label></formula><formula xml:id="formula_26">)</formula><p>Posterior samples of x and θ (i.e. all the covariance function hyperparameters) are obtained through MCMC sampling via adaptive Hamiltonian Monte Carlo <ref type="bibr" target="#b26">(Neal, 2011;</ref><ref type="bibr" target="#b15">Hoffman and Gelman, 2014)</ref>. We implemented all models in Stan using the RStan interface (Stan Development Team, 2024).</p><p>Implemented as above, DGP-LVMs can be applied to the aforementioned problem of pseudotime estimation from single-cell RNA sequencing data. The scRNA-seq data we consider consists of spliced RNA gene counts and RNA velocity, the time derivative of gene counts. DGP-LVM allows inclusion of both these information into a single model (see Section 3.1). Since the RNA velocity is not an exact derivative of spliced RNA counts, it induces a scale difference that is solved by DGP-LVM as shown in Section 3.1.1. Given that single-cell RNA sequencing data is multi-dimensional, DGP-LVM is designed as a multi-output model (see Section 3.2). The primary aim of DGP-LVM is to estimate the latent inputs as explained in Section 3.3, which perfectly aligns with pseudotime estimation since pseudotime is an unobserved cell ordering, and is hence considered as a latent variable. Moreover, the RNA sequencing data comes with its own cell capture time or experimental time which can be considered as a noisy version of the true pseudotime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Simulation study</head><p>The fundamental issue for validating and comparing models designed to estimate latent variables is the lack of ground truth values for real-world data. Thus, it is crucial to test any latent variable model through extensive simulations where the ground truth is available and controllable. Below, we discuss and provide evidence for the importance of our proposed model innovations. Concretely, we showcase DGP-LVM on multiple simulated data setups that closely represent the complexities of a real scRNA sequencing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulated data</head><p>We consider five primary scenarios to generate simulated data. In our first scenario, we generate data from a multi-output GP with scaled derivative SE covariance function. In our second and third scenarios, we generate data from a multi-output GP but with scaled derivative Matern 3/2 and 5/2 covariance functions, respectively. All of the above scenarios constitute cases where the estimated DGP-LVMs align with the true underlying process. However, datasets generated this way can vary strongly in the amount of signal they contain, thus adding a lot of random variation in the simulation results. To account for this issue, in our fourth scenario, we generate data from a derivative periodic process with the true generating function</p><formula xml:id="formula_27">f ij = α j sin x i ρ j f ′ ij = α ′ j ρ j cos x i ρ j<label>(17)</label></formula><p>and corresponding data simulations as</p><formula xml:id="formula_28">y ij ∼ N (f ij , σ 2 ) and y ′ ij ∼ N (f ′ ij , σ ′2 ). (<label>18</label></formula><formula xml:id="formula_29">)</formula><p>In all of these scenarios, the data is generated with varying hyperparameters and correlated outputs assumptions in play (see Section 3). The hyperparameters of the periodic data generating process fulfil a similar purpose to those of SE, Matern 3/2 and Matern 5/2 derivative GPs. Hence we choose to use the same hyperparameter names for simplicity. The scenario of periodic data (Eq.( <ref type="formula" target="#formula_27">17</ref>)) is important on two counts. First, it allows us to better control the amount of signal contained in each generated dataset. Second, it demonstrates that DGP-LVM can achieve good results even when the underlying generating process in not actually a GP.</p><p>Additionally, in the fifth scenario, we further increase the complexity by adding a quadratic and linear trend to the above periodic and corresponding derivative functions, respectively, as</p><formula xml:id="formula_30">f ij = α j sin x i ρ j + bx 2 i , f ′ ij = α ′ j ρ j cos x i ρ j + 2bx i , (<label>19</label></formula><formula xml:id="formula_31">)</formula><p>where b is a scaler and is chosen in accordance with the acting periodicity parameter ρ. The data generating process then follows Eq. ( <ref type="formula" target="#formula_28">18</ref>). This fifth scenario is designed to test the limitations of modelling non-stationary data with stationary GPs.</p><p>To demonstrate the adversity of scale difference between y and y ′ , we induce a scaling factor of λ = 3 that propagates through the GP marginal SD and error SD (see Section 3.1.1). The GP marginal SD for the output y and the derivative y ′ are related through λ such that α = λα ′ . Similarly for the error SD, σ = λσ ′ . Therefore, we only specify sampling distributions for α ′ ∼ Normal + (3, 0.25 2 ) and σ ′ ∼ Normal + (1, 0.25 2 ). In reality, λ &gt; 3 or λ &lt; 1/3 may very well occur (see Section 5). In the simulations, we chose to avoid more extreme λ values to prevent substantial convergence issues for models without scaling modifications. This allows us to showcase these models' (reduced) performance without confounding this finding with convergence considerations. The ground truth GP length scale is sampled as ρ ∼ Normal + (1, 0.05 2 ). The choice of our sampling distributions, especially for the length scale ρ being an informed prior, enables us to explicitly select a range of values for our hyperparameters for which the simulated GP data would contain sufficient amount of signal. Further, we introduce an uniform between-dimension correlation of 0.5 for all the simulated data scenario as to represent moderate interactions between outputs. Combined with the sampling of true hyperparameters for each output dimension, our simulated data mimics the real-world data scenario where such assumptions are prevalent.</p><p>Lastly, we generate ground truth for x as a sequence of values between {0.5, ..., 9.5} with a total output sample size of N = 20 for y and y ′ each and we choose a value for prior measurement SD of the noisy x as s = 0.3 (see Section 3.3). This resembles the realistic scenario where observed times are already a relatively good measure of latent pseudotime considering the overall input scale. Both simulation studies are generated as multi-output data with three sets of output dimensions namely D = 2, 5 and 10. To keep model estimation times within manageable bounds, each simulated dataset contains only 20 y and correspondingly 20 y ′ sample points. We perform 50 trials for each simulation scenario, that is, generate 50 datasets for the three GP data, the periodic and the periodic with trend scenario, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model setup</head><p>The models within our DGP-LVM framework can vary specifically in four components, namely the inclusion of (1) derivative information, (2) scaled derivatives, (3) varying hyperparameters, as well as (4) correlations across output dimensions. In order to study the individual importance of the four components, we systematically enable/disable each of them and investigate the resulting models' performances. The underlying data generating process contains all of the above components, so any model that only has a subset of components will be misspecified at least to some degree. In our simulations, component combinations were fully crossed where sensible (see Table <ref type="table">1</ref> for an overview). We fit 12 GP models for each selected number of output dimensions D resulting in a total of 36 models fitted per generated dataset for a specific simulation scenario. We only exclude specific, non-sensible combinations. For example, it does not make sense to ask if a GP model, which does not include derivative information, accounts for the scaling of the derivatives. We use a constant mean function (similar to an intercept in regression models) in all our models for both f and f ′ (wherever applicable) throughout the simulation study. Such specifications help with overall location shifts in the data.</p><p>Prior specifications for all the model hyperparameters involved were aligned with the data generating conditions to a reasonable extent to better showcase the model contributions. We specify separate priors for the marginal SDs and error SDs corresponding to f , f ′ and y, y ′ respectively, to account for the scale differences between the original and derivative part of the data. We specify the priors for GP marginal SDs α ∼ Normal + (9, 0.75 2 ) and α ′ ∼ Normal + (3, 0.25 2 ). In case of error SDs we specify σ ∼ Normal + (3, 0.75 2 ) and σ ′ ∼ Normal + (1, 0.25 2 ). We shifted our priors for the original part of the GP f and data y in accordance with our choice of scale difference λ in the simulation scenarios. We use an informative prior on the length scale ρ ∼ Normal + (1, 0.05 2 ) is roughly based on the mean Euclidean distance between the xi (as in Section 3.3) as well as the data generating specifications showed in Section 4.1. As prior for the between-output correlation matrix C, we apply an unimodal LKJ(1) distribution <ref type="bibr" target="#b21">(Lewandowski et al., 2009)</ref> defined over the positive definite symmetric matrices with unit diagonals. This distribution is a common prior choice for correlation matrices. For our constant mean function (intercepts), we used a Normal distribution with data-specific Mean and SD (for both y and y ′ Table <ref type="table">1</ref>: GP models along with their specifications used for simulated scenarios Derivative information Scaled derivatives Varying hyperparameters Correlated outputs</p><formula xml:id="formula_32">✓ ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✓ ✓ ✗ ✓ ✓ ✗ ✓ ✓ ✓ ✓ ✗ ✗ ✓ ✗ ✗ ✓ ✓ ✗ ✓ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗</formula><p>Note: Each table row denotes the assigned modifications to the fitted models. The first row shows the modifications involved in DGP-LVM.</p><p>correspondingly) as prior.</p><p>All models were specified in Stan (Stan Development Team, 2024) and fitted with a single MCMC chain of 3000 iterations in total with 1000 warm-ups. We decided to run only a single chain per model to reduce overall computation times. However, we show that using multiple chains yield similar results down the line (see Appendix: Figures 32 and 33.). All models were fitted on all generated datasets. The full study design is depicted in Figure <ref type="figure" target="#fig_0">1</ref>. The simulation studies were conducted using 50 vCPUs (Intel(R) Xeon(R) Gold 6230R CPU @ 2.10 GHz) with 720 GB memory allowances. The maximum runtime (in hours) for DGP-LVM with D = 10 (for a simulated dataset) were approximately 4.6 for the SE model, 5.9 for the Matern 3/2 model and 8.9 for the Matern 5/2 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Summary methods</head><p>In order to evaluate how well fitted models recover the latent ground truth, we compare posterior samples of the latent input variable x denoted by x post with their respective ground truth values denoted by x true using the root mean squared error (RMSE):</p><formula xml:id="formula_33">RMSE(x post ) = E(x post -x true ) 2<label>(20)</label></formula><p>and the mean absolute error:</p><formula xml:id="formula_34">MAE(x post ) = E(| x post -x true |)<label>(21)</label></formula><p>where the expectations are taken over the posterior (approximated via samples). In case of RMSE, E(x post -x true ) 2 can be decomposed into Var(x post ) and Bias(x post , x true ) 2 , thus measuring Bias-variance trade-off. We compute RMSE and MAE from all fitted models shown in Table <ref type="table">1</ref> for each set of output dimensions (2,5 and 10). We prefer models that provide both low bias indicating posterior mean estimates close to the ground truth as well as lower posterior variance indicating high precision, together resulting in an overall low RMSE. Similarly, we prefer low MAE since it shows the amount of absolute bias present while estimating the latent variable. Overall, RMSE penalizes the models for estimating outlying posteriors while MAE is more lenient in that sense.</p><p>To analyse RMSE and MAE values, we use a multilevel analysis of variance model (ANOVA) fitted with brms <ref type="bibr" target="#b5">(Bürkner, 2017)</ref>, which disentangles the contributions of each model component. Using a multilevel model is important to account for the dependency between results of all models fitted on the same dataset. We model fixed main effects of scaled derivatives, varying hyperparameters, correlated outputs and number of output dimensions. For this purpose, we consider scaled derivatives as a factor variable with three levels corresponding to models that (a) do not include derivative information, (b) models that include derivative information with scaling and (c) models that include derivative information without scaling. Varying hyperparameters are represented by a binary factor variable that denotes varying vs. constant hyperparameters across output dimensions. Similarly, correlated outputs represented by a binary factor variable that indicates if the multiple outputs are assumed to be correlated or not. We additionally model fixed interaction effects between (a) scaled derivatives and varying hyperparameters and (b) scaled derivatives and correlated outputs. Since our simulation study is performed over 2, 5, and 10 output dimensions, we include dimension as factor variable with three levels and allowed it to interact with all previously mentioned (fixed) main and interaction effects. We account for the dependency structure in the RMSE and MAE values, induced by fitting multiple models to the same simulated dataset, by a random intercept over datasets as well as corresponding random slopes of the scaled derivatives, varying hyperparameters, and correlated outputs factors. Further, we account for the dependency in the evaluation metric values for the 20 latent inputs estimated from a single model through a random intercept per fitted model. The results based on RMSE are presented in Section 4.5 while their corresponding MAE results are shown in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model convergence</head><p>We investigate the convergence of our fitted GP models for all the five simulation scenarios mentioned before. To that end, we use standard MCMC sampling diagnostics including stateof-the-art versions of the scale reduction factor R, the bulk effective sample size (Bulk-ESS) and the tail effective sample size (Tail-ESS) <ref type="bibr" target="#b38">(Vehtari et al., 2021)</ref>. The combined check of these measures provide a comprehensive picture of individual parameter model convergence.</p><p>In general, R should be very close to 1 and should ideally not exceed 1.01 <ref type="bibr" target="#b38">(Vehtari et al., 2021)</ref>. In a simulation setup, we can evaluate the goodness of the posterior estimation also independently of convergence, as we have access to ground truth values. Hence, also in light of the relatively short MCMC chains, we decide to apply a more relaxed threshold of 1.1. Bulk-ESS indicates the reliability of measures of central tendency such as the posterior mean or median. Tail-ESS indicates the reliability of the 5% and 95% quantile estimates, which are commonly used to construct credible intervals. Both Bulk-ESS and Tail-ESS should have values greater than 100 times the number of MCMC chains. We computed all the convergence measures with the posterior package <ref type="bibr" target="#b6">(Bürkner et al., 2023)</ref>.  For latent inputs and hyperparameters obtained from the simulated SE data scenario, we show R, Bulk-ESS, and Tail-ESS in Figure <ref type="figure" target="#fig_2">2</ref>. We present the MCMC diagnostics plots for the other simulated data scenarios (see Figures <ref type="figure" target="#fig_6">14</ref><ref type="figure" target="#fig_7">15</ref><ref type="figure" target="#fig_0">16</ref><ref type="figure" target="#fig_0">17</ref>) in Appendix B owing to their similar nature. The R were all satisfactory for majority of the simulation trials, with the exception of a few outlying models per trial. The GPs with derivative SE covariance function per simulation trial had better convergence as compared to the GPs with derivative Matern covariance functions. This was expected due to the increased model complexity of the Matern covariance functions. Moreover, the derivative Matern 3/2 (see Figure <ref type="figure" target="#fig_6">14</ref>) being the boundary of existing derivative covariance functions among the Matern class makes it more complex for the sampler to perform as good as the Matern 5/2 (see Figure <ref type="figure" target="#fig_7">15</ref>) and subsequently the much simpler SE covariance function.</p><p>The Bulk-ESS and Tail-ESS were consistently higher than the suggested threshold for all the cases, thus satisfying the recommended criteria. The convergence results for the periodic and periodic with trend scenarios as shown in Figures <ref type="figure" target="#fig_0">16</ref> and<ref type="figure" target="#fig_0">17</ref> were similar to derivative SE data simulation scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>For all of the simulation scenarios discussed in Section 4.1, we evaluated the effects of including derivative information, accounting for scale differences between y and y ′ , estimating varying hyperparameters across multiple outputs as well as correlated outputs. We summarize these aforementioned conditions as model assumptions and show their effects on the RMSE as our primary model evaluation measure of the posterior estimates of latent x and covariance function hyperparameters with respect to their true simulated values. The corresponding MAE results were qualitatively highly similar and are thus only shown in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Model evaluation: Latent inputs</head><p>In addition to the posterior model evaluation measure estimates obtained from multilevel ANOVA, we also show the prior RMSE that would be expected if we only used the prior measurement model xi ∼ N (x i , s 2 ) to infer x. Consequently, the prior evaluation measure acts as a benchmark to illustrate how much precision we gain through the GP modelling of output data.</p><p>Prior Prior  Our findings for the latent x are presented for different simulation scenarios in Figures <ref type="figure" target="#fig_4">3</ref><ref type="figure" target="#fig_6">4</ref><ref type="figure" target="#fig_7">5</ref>for the simulated GP data scenarios and in Figure <ref type="figure">6</ref> for the periodic data scenario. We see how the inclusion of both derivative information and scaling modifications simultaneously results in an overall substantial decrease in mean RMSE in the simulated SE and periodic data scenarios (Figure <ref type="figure" target="#fig_4">3</ref>(a) and 6(a)), thus indicating a better recovery of the true latent values as compared to models without derivative information. In case of the Matern 3/2 and 5/2 data scenarios, although not as substantial as the SE and periodic case, we see similar effects of adding scaled derivatives. This is due to the more challenging nature of the GPs with derivative Matern covariance functions as seen through their model convergence. Additionally, the evaluation measures for all the scenarios further decrease as we increase the number of output dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No derivative</head><p>Overall, we see a reduction in RMSE of more than 50% compared to the corresponding prior metrics, and a reduction of about 30% compared to the models without derivative information in the SE and periodic data cases, thus clearly outlining the benefits of using DGP-LVMs. Conversely, when models include derivative information without accounting for scale differences, the RMSEs are a lot higher, suggesting that the model performs adversely while estimating latent inputs (see Figures 18-21 in Appendix B). Curiously, the performance of such models is even worse than the models not including derivatives at all, sometimes close to (or even worse than) when just using the prior measurement model alone. Presumably, this is because hyperparameter estimates are strongly biased if forced to be the same for both regular outputs and their derivatives; at least when the ground truth assumes hyperparameters to be different by a factor of 3 (which is not unrealistic). As an implication, we then also obtain strongly biased   With respect to the other varied components, modelling varying hyperparameters and correlated outputs may result in a slight increase in the RMSEs (see (b) and (c) of Figures <ref type="figure" target="#fig_6">4</ref><ref type="figure" target="#fig_7">5</ref><ref type="figure">6</ref>), especially in higher output dimensions. We hypothesize that this is due to the significant increase in the number of estimated parameters, while the amount of data points remained constant in our simulations. Concretely, the number of parameters increase by the number of hyperparameters per dimension (i.e., 5 in our case) times the number of output dimensions D, which is quite substantial already for D = 10 output dimensions.</p><p>We encounter a similar issue when modelling outputs as correlated since the increase in estimated model parameters are even more substantial. For output dimension D, we estimate D(D -1)/2 number of parameters just for between-dimension correlations. Such a significant increase in parameters becomes visible in the results especially for D = 10 for most of the simulated scenarios.  For most of the simulated datasets, the hyperparameters show good recovery as indicated by low RMSE. We do see some extreme RMSE values though, especially for GP length-scale ρ. These extreme cases are explained by the significant increase in the number of estimated parameters, when we consider both varying hyperparameters and correlated outputs without increasing the amount of data.</p><p>Interestingly, we see how the scaling assumption helps identifying the GP marginal SD α ′ and error SD σ ′ for the f ′ and y ′ respectively. Without the assumption, the model simply fails to recover the true SD hyperparameters for the derivative part of the data. Additionally, when </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Special case: Non-stationary data</head><p>In the additional case of periodic data with trend, Figure <ref type="figure" target="#fig_12">11</ref> shows a sharp increase in RMSE values for higher dimensions (when D = 10). We believe this to be a direct consequence of modelling non-stationary data with stationary GPs. With more number of non-stationary output dimensions, more of the stationary GPs fail to model the data appropriately, thus showing poor model performance in terms of recovering the ground truth of the latent x. This special simulation scenario highlights one of the limitations of our current framework, which we further discuss in Section 6.1. Interestingly, for the D = 10, the problem vanishes when modelling correlated outputs. This is due to the fact that added trend behaviour is same across outputs owing to the shared inputs, thus being highly correlated to one another. Due to this, accounting for correlated outputs seem to improve the recovery of true latent inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prior Prior</head><p>No derivative Scaled derivative  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Case study</head><p>We showcase the application of DGP-LVM to real-world scRNA sequencing data by re-analysing cell-cycle data from <ref type="bibr" target="#b24">Mahdessian et al. (2021)</ref>. This dataset comprises of single-cell RNA expression profiles along the cell cycle as well as corresponding RNA velocities as estimates of expression profile derivatives obtained as a pre-processing step of cytopath, a method for simulation based cell trajectory inference <ref type="bibr" target="#b11">(Gupta et al., 2022)</ref>. Briefly, we choose this dataset because it covers single-cell transcriptomic profiles of the cell cycle, i.e. a cyclic process going through four phases depicting substantial variation in gene expressions and velocities.  For the purpose of this case study, we use a reduced data set of spliced RNA gene expression data and its corresponding RNA velocity of 20 cells and 12 genes. In other words, each sample point corresponds to a single cell and each output dimension corresponds to a single gene, with the value being the gene expressions per cell. Thus, for this case study, we have the sample points N = 20 for y and correspondingly y ′ each with output dimensions D = 12. We subsampled the dataset in a stratified fashion so that cells from all four phases are included. We use the experimental time known as "cell hours" in the context of this specific data as the prior x for our latent pseudotime (input) x. Both x and x are real numbers with values ranging between 0 and 1. For our prior measurement SD, we choose s = 0.03, so that it is proportional to our choices in simulation studies in Section 4.1.</p><p>We fit DGP-LVMs with derivative SE and Matern 5/2 covariance functions. For this case study, We specify the priors for GP marginal SDs α ∼ Normal + (13.84, 3.46 2 ) and α ′ ∼ Normal + (1, 0.25 2 ). In case of error SDs we specify σ ∼ Normal + (6.92, 3.46 2 ) and σ ′ ∼ Normal + (0.5, 0.25 2 ). These choices were influenced by the large scaling factor λ informed by the data, as the mean and standard deviations of y are on average 13.84 times larger than those of y ′ across the output dimensions. The prior for ρ was specified as Normal + (0.4, 0.1 2 ) for the SE and Normal + (0.6, 0.1 2 ) for the Matern 5/2 as our GP length scale priors loosely based on the scale of the latent input x (as suggested by the prior x). These priors were chosen to account for the varying functional smoothness induced by the choice of covariance function. The DGP-LVM with Matern 3/2 being the least functionally smooth choice of covariance function from the Matern family didn't converge reasonably with a sensible choice of ρ prior for this specific data and is therefore not presented. As in any real-world latent variable estimation problem, we lack the ground truth to compare the estimated latent values against. Therefore, we study the deviation of the posterior estimates of pseudotime from the cell hours (our prior) by considering the difference or shift in values of the estimated pseudotime from the observed cell hours. The results are shown in Figure <ref type="figure" target="#fig_14">12</ref> with cell hours (prior) on the x-axis and shift (difference of pseudotime and cell hours) on the y-axis. Deviations from the y = 0 line indicate that latent pseudotimes are different from their cell hours (prior) as a result of learning from gene expression data and velocities. For some cells, prior-posterior differences are up to 5% of the total time scale. Further, we see that the posterior uncertainties (error bars in y-direction) are substantially smaller (considering the scale of the y-axis compared to x-axis) than the corresponding prior uncertainties (error bars in x-direction), which also indicates that model learning has taken place. Combined with our findings from the simulation study as strong evidence that DGP-LVM is able to learn and recover the true posterior estimates of the latent pseudotimes, these deviations from the prior are interpreted as model learning in the correct direction closer to the true latent ordering of the cells.</p><p>In Figure <ref type="figure" target="#fig_15">13</ref>, we show the posterior mean along with SD estimates of GP hyperparameters. We see strongly varying length-scales ρ, marginal SDs α and error SDs σ across different genes (output dimensions) for both SE and Matern 5/2 models. This clearly points to the necessity of modelling hyperparameters as varying across genes. We also see substantial scale differences between the GP marginal SDs α and α ′ corresponding to f and f ′ , and consequently gene expression y and velocity y ′ outputs, respectively. Similar, but not as drastic results are seen for the error SDs σ and σ ′ . These results indicate significant scale differences between output RNA gene expression and its derivative RNA velocity. Interestingly, the scale differences go in both directions, such that for some genes α and σ are higher than α ′ and σ ′ . For others, the direction is opposite. While the direction is not important for the DGP-LVM models, it may be highly relevant for understanding the biological processes in which the specific genes are involved. The difference in posterior estimates of the hyperparameters in the different models are heavily influenced by the natural varying functional smoothness corresponding to the choice of different covariance functions.</p><p>That said, this case study is meant only as a simple example for demonstrating the application of DGP-LVMs on real-world data. We would like to caution against any specific biological interpretation of the results at this point. The case study was conducted using Apple M1 chip (2 cores in parallel) with 16 GB memory allowances. The runtime (in minutes) for DGP-LVM were approximately 25.3 for the SE model and 73.11 for the Matern 5/2 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Motivated by a real-world problem in the area of single-cell biology, we developed a class of derivative Gaussian process latent variable models, DGP-LVMs. In the real-world case, we aim at estimating the latent ordering of cells from RNA gene expression levels and its corresponding time derivative RNA velocity. For this purpose, DGP-LVMs not only account for scale differences between the outputs and their derivatives, but also learn from multiple, potentially correlated outputs. The latter is highly important for scRNA sequencing data where the latent cell ordering is informed by many genes, each forming their own output coupled with derivative information.</p><p>In our simulation studies, we extensively validate DGP-LVMs demonstrating strong improvements in estimation accuracy of the latent variables by including derivative information. Our results also clearly show the importance of our proposed covariance function modifications. While we specifically focused on modifying the SE and Matern class of covariance functions, our framework is generally applicable for any choice of covariance function that is twice differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Limitations and Future Research</head><p>This paper is only the first step towards tackling latent variable (input) estimation with derivative Gaussian processes. The current main limitation of DGP-LVMs is their data-scalability as they cannot be easily applied to large amounts of data, such as full sized scRNA sequencing datasets, yet. For a dataset of size N , exact GPs have a complexity of O(N 3 ) in operations and O(N 2 ) in memory. In case of multi-output GPs with correlated outputs and varying hyperparameters, the complexities increase to O(N 3 D 3 ) and O(N 2 D 2 ) respectively, where D is the number of output dimensions <ref type="bibr">(Hensman et al., 2013a)</ref>. Additionally, when performing Bayesian inference via HMC involving a total of T unnormalized log posterior evaluations, the number of operations increases to even O(N 3 D 3 T ). Together, this limits inference for exact GPs on data with large N or D. From a real-world data point of view, scRNA sequencing data frequently has a few thousand cells (sample size N ) with the number of genes (output dimensions D) being in the high hundreds after standard pre-processing steps. In case of DGP-LVM, this issue is even more severe due to adding derivative information, effectively doubling the sample size N . To address the computational limitations of DGP-LVM in terms of data-scalability, future research should consider extending approximate GP approaches (e.g., <ref type="bibr" target="#b31">Riutort-Mayol et al., 2022)</ref> to our DGP-LVM framework.</p><p>Another limitation of our current DGP-LVMs is their stationary assumption based on the choice of the covariance functions we discuss here. This limits their applicability to non-stationary data as evidenced by our simulation study of periodic data with an added non-linear trend. While this is a general limitation of stationary GPs, the limitation currently lies in not having a derivative version of non-stationary covariance functions. An interesting future research would be to develop DGP-LVMs for non-stationary data where the primary focus would be on obtaining derivative versions of non-stationary covariance functions and verifying their performance for latent variable estimation.</p><p>Another aspect for future research is the choice of prior distributions. Here, we focused on informative priors for the GP hyperparameters in both our simulation studies and the real-world case study although they are difficult to come by organically. DGP-LVMs will likely benefit from using stronger priors informed by the application-specific subject matter knowledge, specifically in data-sparse scenarios. This not only applies to priors for the GP hyperparameters, but also to the priors of the latent input variables. Moreover, a joint prior on the covariance function hyperparameters along with latent inputs will likely further improve model convergence. The combination of scalable approximations, improved prior specifications, and additional derivative covariance functions would foster the general applicability of DGP-LVMs, thereby further increasing their ability to accurately estimate latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Derivative covariance functions</head><p>In the following covariance functions, α is GP marginal SD corresponding to output y; α ′ is the GP marginal SD corresponding to derivative output y ′ ; ρ is the GP length scale parameter.               </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Squared Exponential</head><formula xml:id="formula_35">K = α 2 exp - (x i -x j ) 2 2ρ 2 K 01 = δK δx j = αα ′ (x i -x j ) ρ 2 exp - (x i -x j ) 2 2ρ 2 K 10 = δK δx i = αα ′ (x j -x i ) ρ 2 exp - (x i -x j ) 2 2ρ 2 K 11 = δ 2 K δx i δx j = α ′2 ρ 4 (ρ 2 -(x i -x j ) 2 ) exp - (x i -x j ) 2 2ρ 2 2. Matern 3/2 K = α 2 1 + 3(x i -x j ) 2 ρ exp - 3(x i -x j ) 2 ρ K 01 = δK δx j = αα ′ 3(x i -x j ) ρ 2 exp - 3(x i -x j ) 2 ρ K 10 = δK δx i = αα ′ 3(x j -x i ) ρ 2 exp - 3(x i -x j ) 2 ρ K 11 = δ 2 K δx i δx j = α ′2 3 ρ 2 1 - 3(x i -x j ) 2 ρ exp - 3(x i -x j ) 2 ρ 3. Matern 5/2 K = α 2 1 + 5(x i -x j ) 2 ρ + 5(x i -x j ) 2 3ρ 2 exp - 5(x i -x j ) 2 ρ K 01 = δK δx j = αα ′ 5(x i -x j ) 3ρ 2 1 + 5(x i -x j ) 2 ρ exp - 5(x i -x j ) 2 ρ K 10 = δK δx i = αα ′ 5(x j -x i ) 3ρ 2 1 + 5(x i -x j ) 2 ρ exp - 5(x i -x j ) 2 ρ K 11 = δ 2 K δx i δx j = α ′2 5 3ρ 2 1 + 5(x i -x j ) 2 ρ - 5(x i -x j ) 2 ρ 2 exp - 5(x i -x j ) 2 ρ</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: High-level overview of the simulation study design.</figDesc><graphic coords="10,129.83,429.13,335.63,221.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Squared exponential scenario: Convergence measures for (a) latent inputs and (b) GP hyperparameters. The individual points correspond to each fitted models per simulated data. The y-axis for Bulk and Tail ESS plots are log10 transformed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Squared exponential scenario: Main effects of including (a) scaled derivatives and interaction effects of assuming (b) varying hyperparameters and (c) correlated outputs on recovery of latent inputs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Matern 3/2 scenario: Main effects of including (a) scaled derivatives and interaction effects of assuming (b) varying hyperparameters and (c) correlated outputs on recovery of latent inputs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Matern 5/2 scenario: Main effects of including (a) scaled derivatives and interaction effects of assuming (b) varying hyperparameters and (c) correlated outputs on recovery of latent inputs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Periodic scenario: Main effects of including (a) scaled derivatives and interaction effects of assuming (b) varying hyperparameters and (c) correlated outputs on recovery of latent inputs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :Figure 9 :Figure 10 :</head><label>8910</label><figDesc>Figure 8: Matern 3/2 scenario: Hyperparameter RMSEs for (a) full DGP-LVM model and (b) models without scale assumption. The different colour denotes if the hyperparameters correspond to the original or the derivative part of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Periodic with trend scenario: Main effects of including (a) scaled derivatives and interaction effects of assuming (b) varying hyperparameters and (c) correlated outputs on recovery of latent inputs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Difference of latent pseudotime estimates obtained via DGP-LVM with (a) SE and (b) Matern 5/2 covariance functions and prior cell hours. The point ranges horizontally show 95% prior CIs and vertically show 95% posterior CIs. Notice that the posterior CIs are actually much smaller than the prior CIs since y-axis scale is significantly smaller than x-axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Hyperparameters for DGP-LVM with (a) SE and (b) Matern 5/2 covariance functions. The points indicate posterior mean and the point ranges indicate 95% CIs for each hyperparameter per output dimension. The different colours denote correspondence to the output y or its derivative y ′ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 15 :Figure 16 :Figure 17 :</head><label>151617</label><figDesc>Figure 15: Matern 5/2 scenario: Convergence measures for (a) latent inputs and (b) GP hyperparameters. The individual points correspond to each fitted models per simulated data. The y-axis for Bulk and Tail ESS plots are log10 transformed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Squared exponential scenario: main effects of including (a) scaled derivatives and interaction effects of assuming (b) varying hyperparameters and (c) correlated outputs on recovery of latent inputs (full version) using RMSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Matern 3/2 scenario: main effects of including (a) scaled derivatives and interaction effects of assuming (b) varying hyperparameters and (c) correlated outputs on recovery of latent inputs (full version) using RMSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Matern 5/2 scenario: main effects of including (a) scaled derivatives and interaction effects of assuming (b) varying hyperparameters and (c) correlated outputs on recovery of latent inputs (full version) using RMSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 21 :Figure 22 :</head><label>2122</label><figDesc>Figure 21: Periodic scenario: main effects of including (a) scaled derivatives and interaction effects of assuming (b) varying hyperparameters and (c) correlated outputs on recovery of latent inputs (full version) using RMSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: Squared exponential scenario: main effects of including (a) scaled derivatives and interaction effects of assuming (b) varying hyperparameters and (c) correlated outputs on recovery of latent inputs (full version) using MAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 24 :</head><label>24</label><figDesc>Figure 24: Matern 3/2 scenario: main effects of including (a) scaled derivatives and interaction effects of assuming (b) varying hyperparameters and (c) correlated outputs on recovery of latent inputs (full version) using MAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 25 :Figure 26 :</head><label>2526</label><figDesc>Figure 25: Matern 5/2 scenario: main effects of including (a) scaled derivatives and interaction effects of assuming (b) varying hyperparameters and (c) correlated outputs on recovery of latent inputs (full version) using MAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 27 :</head><label>27</label><figDesc>Figure 27: Periodic with trend scenario: main effects of including (a) scaled derivatives and interaction effects of assuming (b) varying hyperparameters and (c) correlated outputs on recovery of latent inputs (full version) using MAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 28 :</head><label>28</label><figDesc>Figure 28: Squared exponential scenario: Hyperparameter RMSEs for scaled derivatives (a) without varying hyperparameters, (b) without correlated outputs and (c) without both varying hyperparameters and correlated outputs. The different colour denotes if the hyperparameters correspond to the original or the derivative part of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 29 :</head><label>29</label><figDesc>Figure 29: Matern 3/2 scenario: Hyperparameter RMSEs for scaled derivatives (a) without varying hyperparameters, (b) without correlated outputs and (c) without both varying hyperparameters and correlated outputs. The different colour denotes if the hyperparameters correspond to the original or the derivative part of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 30 :</head><label>30</label><figDesc>Figure 30: Matern 5/2 scenario: Hyperparameter RMSEs for scaled derivatives (a) without varying hyperparameters, (b) without correlated outputs and (c) without both varying hyperparameters and correlated outputs. The different colour denotes if the hyperparameters correspond to the original or the derivative part of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 31 :Figure 32 :</head><label>3132</label><figDesc>Figure 31: Periodic scenario: Hyperparameter RMSEs for scaled derivatives (a) without varying hyperparameters, (b) without correlated outputs and (c) without both varying hyperparameters and correlated outputs. The different colour denotes if the hyperparameters correspond to the original or the derivative part of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 33 :</head><label>33</label><figDesc>Figure 33: Squared exponential scenario with 4 MCMC chains: main effects of including (a) scaled derivatives and interaction effects of assuming (b) varying hyperparameters and (c) correlated outputs on recovery of latent inputs (full version) using MAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. Extending our univariate notation, the individual output values are now denoted as y di for dimension d and observation i, with corresponding derivative values y ′ di . Multi-output GPs are created by first setting up D independent, univariate Gaussian processes f d (x) each with their own set of hyperparameters, that is, (ρ d , α d , α ′ d , σ d and σ ′ d ) for Matern class of covariance functions with adjusted scales. Subsequently the univariate GPs are related to one another by folding them with a (D-dimensional) across-dimension correlation matrix C</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We acknowledge the <rs type="funder">Cluster of Excellence iFIT</rs> (<rs type="grantNumber">EXC 2180</rs>) "<rs type="programName">Image-Guided and Functionally Instructed Tumor Therapies" for supporting Soham Mukherjee</rs>. We acknowledge the valuable insights provided by members of <rs type="institution">ClaassenLab and BürknerLab</rs>, <rs type="person">Revant Gupta</rs> and <rs type="person">Marcello Zago</rs>. We thank <rs type="person">Debapratim Sil</rs> and <rs type="person">Jayati Chatterjee</rs> for their feedback on the manuscript. The authors thank the <rs type="funder">International Max Planck Research School for Intelligent Systems</rs> (<rs type="affiliation">IMPRS-IS</rs>) for supporting Soham Mukherjee.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_PPnpGA5">
					<idno type="grant-number">EXC 2180</idno>
					<orgName type="program" subtype="full">Image-Guided and Functionally Instructed Tumor Therapies&quot; for supporting Soham Mukherjee</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code availability</head><p>The code for the model development, simulation studies as well as the results can be found here: <ref type="url" target="https://github.com/Soham6298/DGP-LVM">https://github.com/Soham6298/DGP-LVM</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Appendix A: Derivative covariance functions</p><p>Here we show the mathematical details of the general covariance function structure as well as specific derivative forms of SE, Matern 3/2 and Matern 5/2 covariance functions.</p><p>Proof of the derivative covariance function structure Lemma: Let X ∈ X be a random variable and g :</p><p>Let us consider y i and v j such that v j = δy j δx j where we consider a Gaussian Process model with y = f (x) + ϵ. For a GP model Cov(y i , y j ) is completely defined using corresponding inputs x i and x j by a covariance function. We see that</p><p>Using similar reasoning and with v i = δy i δx j and v j = δy j δx j , we find</p><p>Cov(y i , y j ) (By DCT)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Additional simulation results</head><p>Here we show the additional plots from our simulation studies. Specifically, we provide the MCMC convergence diagnostics for Matern 3/2, 5/2 as well as the periodic simulation scenarios.</p><p>We then present the full versions of the model evaluation plots for recovery of true latent inputs x using RMSE and MAE as evaluation metrics for all the simulation scenarios. Further we show additional hyperparameter recovery plots based on enabling/disabling different model assumptions. Finally, we present the case where we use four MCMC chains for a reduced SE data simulation scenario that shows minimal or no effect on model evaluation metrics irrespective of number of MCMC chains.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional MCMC diagnostic plots</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">GrandPrix: scaling up the Bayesian GPLVM for single-cell data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rattray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boukouvalas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="54" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalizing rna velocity to transient cell states through dynamical modeling</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Theis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-task Gaussian Process Prediction</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Computational analysis of cell-to-cell heterogeneity in single-cell RNA-sequencing data reveals hidden subpopulations of cells</title>
		<author>
			<persName><forename type="first">F</forename><surname>Buettner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Casale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Proserpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Scialdone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Marioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Stegle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="160" />
			<date type="published" when="2015">2015</date>
			<publisher>Nature Publishing Group</publisher>
			<pubPlace>Number: 2 Publisher</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A novel approach for resolving differences in single-cell gene expression patterns from zygote to blastocyst</title>
		<author>
			<persName><forename type="first">F</forename><surname>Buettner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Theis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="626" to="632" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">brms: An R Package for Bayesian Multilevel Models Using Stan</title>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Bürkner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">posterior: Tools for working with posterior distributions</title>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Bürkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gabry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>R package version 1.5.0</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bayesian Gaussian Process Latent Variable Models for pseudotime inference in single-cell RNA-seq data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Pages: 026872 Section: New Results</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A descriptive marker gene approach to single-cell pseudotime inference</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="35" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Statistics for Spatial Data</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A C</forename><surname>Cressie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bindel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12283</idno>
		<title level="m">Scaling Gaussian Process Regression with Derivatives</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simulation-based inference of differentiation trajectories from RNA velocity fields</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cerletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oxenius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Claassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Reports Methods</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">100359</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A practical guide to singlecell RNA-sequencing for biomedical research and clinical applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lönnberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Medicine</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">75</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gaussian processes for big data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fusi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<idno>CoRR, abs/1309.6835</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical Bayesian modelling of gene expression time series across irregularly sampled replicates and clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rattray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">252</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The no-u-turn sampler</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast Approximate Multioutput Gaussian Processes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Joukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kulić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="56" to="69" />
			<date type="published" when="2022">2022</date>
			<publisher>IEEE Intelligent Systems</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">RNA velocity of single cells</title>
		<author>
			<persName><forename type="first">La</forename><surname>Manno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Soldatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zeisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hochgerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Petukhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lidschreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kastriti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Lönnerberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Furlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Borm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Van Bruggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sundström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Castelo-Branco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Adameyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Linnarsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kharchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">560</biblScope>
			<biblScope unit="issue">7719</biblScope>
			<biblScope unit="page" from="494" to="498" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalised GPLVM with Stochastic Variational Inference</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lalchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 25th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>The 25th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7841" to="7864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probabilistic non-linear principal component analysis with gaussian process latent variable models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">60</biblScope>
			<biblScope unit="page" from="1783" to="1816" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generating random correlation matrices based on vines and extended onion method</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lewandowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kurowicka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Joe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1989" to="2001" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stochasticity and Cell Fate</title>
		<author>
			<persName><forename type="first">R</forename><surname>Losick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Desplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">320</biblScope>
			<biblScope unit="issue">5872</biblScope>
			<biblScope unit="page" from="65" to="68" />
			<date type="published" when="2008">2008</date>
			<publisher>American Association for the Advancement of Science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Noise in Gene Expression Determines Cell Fate in Bacillus subtilis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dubnau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">317</biblScope>
			<biblScope unit="issue">5837</biblScope>
			<biblScope unit="page" from="526" to="529" />
			<date type="published" when="2007">2007</date>
			<publisher>American Association for the Advancement of Science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatiotemporal dissection of the cell cycle with single-cell proteogenomics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mahdessian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Cesnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gnann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Danielsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stenström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schutten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bäckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Axelsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Carja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uhlén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mardinoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lindskog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ayoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Leonetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pontén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lundberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">590</biblScope>
			<biblScope unit="issue">7847</biblScope>
			<biblScope unit="page" from="649" to="654" />
			<date type="published" when="2021">2021</date>
			<publisher>Nature Publishing Group</publisher>
		</imprint>
	</monogr>
	<note>Number: 7847 Publisher</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Heterogeneous Multi-output Gaussian Process Prediction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Moreno-Muñoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Artés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Álvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MCMC Using Hamiltonian Dynamics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Markov Chain Monte Carlo</title>
		<editor>
			<persName><forename type="first">Steve</forename><surname>Brooks</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Galin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiao-Li</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><surname>Meng</surname></persName>
		</editor>
		<imprint>
			<publisher>Chapman; Hall/CRC</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="116" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scaling Gaussian Processes with Derivative Information Using Variational Inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Padidar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bindel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6442" to="6453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nature, Nurture, or Chance: Stochastic Gene Expression and Its Consequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Oudenaarden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="226" />
			<date type="published" when="2008">2008</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pseudotime estimation: deconfounding single cell time series</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wernisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="2973" to="2980" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Practical Hilbert space approximate Bayesian Gaussian processes for probabilistic programming</title>
		<author>
			<persName><forename type="first">G</forename><surname>Riutort-Mayol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Bürkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Solin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Derivative Observations in Gaussian Process Models of Dynamic Systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Solak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Leithead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m">Stan Modeling Language Users Guide and Reference Manual</title>
		<imprint>
			<publisher>Stan Development Team</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>2.32.0</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<title level="m">Semiparametric Latent Factor Models. Proceedings of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Variational Learning of Inducing Variables in Sparse Gaussian Processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Titsias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Twelth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="567" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bayesian Gaussian Process Latent Variable Model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="844" to="851" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The dynamics and regulators of cell fate decisions are revealed by pseudotemporal ordering of single cells</title>
		<author>
			<persName><forename type="first">C</forename><surname>Trapnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cacchiarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grimsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pokharel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Lennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Livak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Mikkelsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Rinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="381" to="386" />
			<date type="published" when="2014">2014</date>
			<publisher>Nature Publishing Group</publisher>
			<pubPlace>Number: 4 Publisher</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Bürkner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rank-Normalization, Folding, and Localization: An Improved R for Assessing Convergence of MCMC</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="667" to="718" />
		</imprint>
	</monogr>
	<note>with Discussion</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gaussian Processes for Regression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
