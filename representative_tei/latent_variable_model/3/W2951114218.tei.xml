<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Semantic Relationship in Multi-turn Conversations with Hierarchical Latent Variables</title>
				<funder ref="#_QNhRFuD">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_wDvMbJR #_DnbUWp4">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lei</forename><surname>Shen</surname></persName>
							<email>shenlei17z@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
							<email>fengyang@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haolan</forename><surname>Zhan</surname></persName>
							<email>zhanhl@ios.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">State Key Laboratory of Computer Science</orgName>
								<orgName type="department" key="dep2">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Semantic Relationship in Multi-turn Conversations with Hierarchical Latent Variables</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-turn conversations consist of complex semantic structures, and it is still a challenge to generate coherent and diverse responses given previous utterances. It's practical that a conversation takes place under a background, meanwhile, the query and response are usually most related and they are consistent in topic but also different in content. However, little work focuses on such hierarchical relationship among utterances. To address this problem, we propose a Conversational Semantic Relationship RNN (CSRR) model to construct the dependency explicitly. The model contains latent variables in three hierarchies. The discourse-level one captures the global background, the pair-level one stands for the common topic information between query and response, and the utterance-level ones try to represent differences in content. Experimental results show that our model significantly improves the quality of responses in terms of fluency, coherence and diversity compared to baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Inspired by the observation that real-world human conversations are usually multi-turn, some studies have focused on multi-turn conversations and taken context (history utterances in previous turns) into account for response generation. How to model the relationship between the response and context is essential to generate coherent and logical conversations. Currently, the researchers employ some hierarchical architectures to model the relationship. <ref type="bibr">Serban et al. (2016)</ref> use a context RNN to integrate historical information, <ref type="bibr" target="#b15">Tian et al. (2017)</ref> sum up all utterances weighted by the similarity score between an utterance and the query, while <ref type="bibr" target="#b17">Zhang et al. (2018)</ref> apply attention mechanism on history utterances. Besides, <ref type="bibr" target="#b16">Xing et al. (2018)</ref> add a word-level attention to capture finegrained features.</p><p>In practice, we usually need to understand the meaning of utterances and capture their semantic dependency, not just word-level alignments <ref type="bibr" target="#b9">(Luo et al., 2018)</ref>. As shown in Table <ref type="table" target="#tab_0">1</ref>, this short conversation is about speaker A asks the current situation of speaker B. At the beginning, they talk about B's position. Then in the last two utterances, both speakers think about the way for B to come back. A mentions "umbrella", while B wants A to "pick him/her up". What's more, there is no "word-to-word" matching in query and response. Unfortunately, the aforementioned hierarchical architectures do not model the meaning of each utterance explicitly and has to summarize the meaning of utterances on the fly during generating the response, and hence there is no guarantee that the inferred meaning is adequate to the original utterance. To address this problem, variational autoencoders (VAEs) <ref type="bibr" target="#b5">(Kingma and Welling, 2014)</ref> are introduced to learn the meaning of utterances explicitly and a reconstruction loss is employed to make sure the learned meaning is faithful to the corresponding utterance. Besides, more variations are imported into utterance level to help generate more diverse responses.</p><p>A: Where are you? B: I'm stuck in my office with rain. A: Didn't you bring your umbrella? B: No. Please come and pick me up. However, all these frameworks ignore the practical situation that a conversation usually takes place under a background with two speakers communicating interactively and query is the most relevant utterance to the response. Hence we need to pay more attention to the relationship between query and response. To generate a coherent and engaging conversation, query and response should be consistent in topic and have some differences in content, the logical connection between which makes sure the conversation can go on smoothly.</p><p>On these grounds, we propose a novel Conversational Semantic Relationship RNN (CSRR) to explicitly learn the semantic dependency in multiturn conversations. CSRR employs hierarchical latent variables based on VAEs to represent the meaning of utterances and meanwhile learns the relationship between query and response. Specifically, CSRR draws the background of the conversation with a discourse-level latent variable and then models the consistent semantics between query and response, e.g. the topic, with a common latent variable shared by the query and response pair, and finally models the specific meaning of the query and the response with a certain latent variable for each of them to capture the content difference. With these latent variables, we can learn the relationship between utterances hierarchically, especially the logical connection between the query and response. What is the most important, the latent variables are constrained to reconstruct the original utterances according to the hierarchical structure we define, making sure the semantics flow through the latent variables without any loss. Experimental results on two public datasets show that our model outperforms baseline methods in generating high-quality responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>Given n input messages {u t } n-1 t=0 , we consider the last one u n-1 as query and others as context. u n denotes corresponding response.</p><p>The proposed model is shown in Figure <ref type="figure">1</ref>. We add latent variables in three hierarchies to HRED <ref type="bibr">(Serban et al., 2016)</ref>. z c is used to control the whole background in which the conversation takes place, z p is for the consistency of topic between query and response pair, z q and z r try to model the content difference in each of them, respectively. For simplicity of equation description, we use n -1 and n as the substitution of q and r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Context Representation</head><p>Each utterance u t is encoded into a vector v t by a bidirectional GRU (BiGRU), f utt θ :</p><formula xml:id="formula_0">v t = f utt θ (u t )<label>(1)</label></formula><p>Figure <ref type="figure">1</ref>: Graphical model of CSRR. u t is the t-th utterance, h ct encodes context information up to time t.</p><p>For the inter-utterance representation, we follow the way proposed by <ref type="bibr" target="#b11">Park et al. (2018)</ref>, which is calculated as:</p><formula xml:id="formula_1">h ct = MLP θ (z c ), if t = 0 f ctx θ (h c t-1 , v t-1 , z c ), otherwise<label>(2)</label></formula><formula xml:id="formula_2">f ctx θ (•)</formula><p>is the activation function of GRU. z c is the discourse-level latent variable with a standard Gaussian distribution as its prior distribution, that is:</p><formula xml:id="formula_3">p θ (z c ) = N (z|0, I)<label>(3)</label></formula><p>For the inference of z c , we use a BiGRU f c to run over all utterance vectors {v t } n t=0 in training set. ({v t } n-1 t=0 in test set):</p><formula xml:id="formula_4">q φ (z c |v 0 , ..., v n ) = N (z|µ c , σ c I)<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">v c = f c (v 0 , ..., v n ) (5) µ c = MLP φ (v c ) (6) σ c = Softplus(MLP φ (v c ))<label>(7)</label></formula><p>MLP(•) is a feed-forward network, and Softplus function is a smooth approximation to the ReLU function and can be used to ensure positiveness <ref type="bibr" target="#b11">(Park et al., 2018;</ref><ref type="bibr" target="#b13">Serban et al., 2017;</ref><ref type="bibr" target="#b2">Chung et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query-Response Relationship Modeling</head><p>According to VAEs, texts can be generated from latent variables <ref type="bibr" target="#b14">(Shen et al., 2017)</ref>. Motivated by this, we add two kinds of latent variables: pairlevel and also utterance-level ones for query and response.</p><p>As depicted in Figure <ref type="figure">1</ref>, h c n-1 encodes all context information from utterance u 0 to u n-2 . We use z p to model the topic in query and response pair. Under the same topic, there are always some differences in content between query and response, which is represented by z q and z r , respectively. We first define the prior distribution of z p as follows:</p><formula xml:id="formula_6">p θ (z p |u &lt;n-1 , z c ) = N (z|µ n-1 , σ n-1 I) (8) u &lt;n-1 denotes utterances {u i } n-2</formula><p>i=0 , µ n-1 and σ n-1 are calculated as:</p><formula xml:id="formula_7">µ n-1 = MLP θ (h c n-1 , z c ) (9) σ n-1 = Softplus(MLP θ (h c n-1 , z c ))<label>(10)</label></formula><p>Since z q (z n-1 ) and z r (z n ) are also under the control of z p , we define the prior distributions of them as:</p><formula xml:id="formula_8">p θ (z i |u &lt;i , z c , z p ) = N (z|µ i , σ i I)<label>(11)</label></formula><p>Here, i = n -1 or n. The means and the diagonal variances are computed as:</p><formula xml:id="formula_9">µ i = MLP θ (h c i , z c , z p ) (12) σ i = Softplus(MLP θ (h c i , z c , z p ))<label>(13)</label></formula><p>The posterior distributions are:</p><formula xml:id="formula_10">q φ (z p |u ≤n-1 , z c ) = N (z|µ n-1 , σ n-1 I) (14) q φ (z i |u ≤i , z c , z p ) = N (z|µ i , σ i I)<label>(15)</label></formula><p>q φ (•) is a recognition model used to approximate the intractable true posterior distribution. The means and the diagonal variances are defined as:</p><formula xml:id="formula_11">µ n-1 = MLP φ (v n-1 , v n , h c n-1 , z c )<label>(16)</label></formula><formula xml:id="formula_12">σ n-1 = Softplus(MLP φ (v n-1 , v n , h c n-1 , z c ))<label>(17)</label></formula><formula xml:id="formula_13">µ i = MLP φ (v i , h c i , z c , z p ) (<label>18</label></formula><formula xml:id="formula_14">)</formula><formula xml:id="formula_15">σ i = Softplus(MLP φ (v i , h c i , z c , z p ))<label>(19)</label></formula><p>Note that in Equation 16 and 17, both v n-1 and v n are taken into consideration, while Equation <ref type="formula" target="#formula_13">18</ref>and 19 use z p and corresponding v i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>Because of the existence of latent variables in query-response pair, we use decoder f dec θ to generate u n-1 and u n :</p><formula xml:id="formula_16">p θ (u i |u &lt;i ) = f dec θ (u i |h c i , z c , z p , z i ) (20)</formula><p>The training objective is to maximize the following variational lower-bound:</p><formula xml:id="formula_17">log p θ (u n-1 , u n |u 0 , ..., u n-2 ) ≥ E q φ [log p θ (u i |z c , z p , z i , u &lt;i )] -D KL (q φ (z c |u ≤n )||p θ (z c )) -D KL (q φ (z p |u ≤n )||p θ (z p |u &lt;n-1 )) - n i=n-1 D KL (q φ (z i |u ≤i )||p θ (z i |u &lt;i )) (21)</formula><p>Equation 21 consists of two parts: the reconstruction term and KL divergence terms based on three kinds of latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>Datasets: We conduct our experiment on Ubuntu Dialog Corpus <ref type="bibr" target="#b8">(Lowe et al., 2015)</ref> and Cornell Movie Dialog Corpus (Danescu-Niculescu-Mizil and Lee, 2011). As Cornell Movie Dialog does not provide a separate test set, we randomly split the corpus with the ratio 8:1:1. For each dataset, we keep conversations with more than 3 utterances. The number of multi-turn conversations in train/valid/test set is 898142/19560/18920 for Ubuntu Dialog, and 36004/4501/4501 for Cornell Movie Dialog. Hyper-parameters: In our model and all baselines, Gated Recurrent Unit (GRU) <ref type="bibr" target="#b1">(Cho et al., 2014)</ref> is selected as the fundamental cell in encoder and decoder layers, and the hidden dimension is 1,000. We set the word embedding dimension to 500, and all latent variables have a dimension of 100. For optimization, we use Adam (Kingma and Ba, 2015) with gradient clipping. The sentence padding length is set to 15, and the max conversation length is 10. In order to alleviate degeneration problem of variational framework <ref type="bibr" target="#b0">(Bowman et al., 2016)</ref>, we also apply KL annealing <ref type="bibr" target="#b0">(Bowman et al., 2016</ref>) in all models with latent variables. The KL annealing steps are 15,000 for Cornell Movie Dialog and 250,000 for Ubuntu Dialog. Baseline Models: We compare our model with three baselines. They all focus on multi-turn conversations, and the third one is a state-of-theart variational model. 1) Hierarchical recurrent encoder-decoder (HRED) <ref type="bibr">(Serban et al., 2016)</ref>. 2) Variational HRED (VHRED) (Serban et al., 2017) with word drop (w.d) and KL annealing <ref type="bibr" target="#b0">(Bowman et al., 2016)</ref>, the word drop ratio equals to 0.25. 3) Variational Hierarchical Conversation RNN (VHCR) with utterance drop (u.d) <ref type="bibr" target="#b11">(Park et al., 2018)</ref> and KL annealing, the utterance drop ratio equals to 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Design</head><p>Open-domain response generation does not have a standard criterion for automatic evaluation, like BLEU <ref type="bibr" target="#b10">(Papineni et al., 2002)</ref>  herence/relevance and diversity of generated responses. To measure the performance effectively, we use 5 automatic evaluation metrics along with human evaluation.</p><p>Average, Greedy and Extrema: Rather than calculating the token-level or n-gram similarity as the perplexity and BLEU, these three metrics are embedding-based and measure the semantic similarity between the words in the generated response and the ground truth <ref type="bibr" target="#b13">(Serban et al., 2017;</ref><ref type="bibr" target="#b7">Liu et al., 2016)</ref>. We use word2vec embeddings trained on the Google News Corpus 1 in this section. Please refer to Serban et al. ( <ref type="formula">2017</ref>) for more details.</p><p>Dist-1 and Dist-2: Following the work of <ref type="bibr" target="#b6">Li et al. (2016)</ref>, we apply Distinct to report the degree of diversity. Dist-1/2 is defined as the ratio of unique uni/bi-grams over all uni/bi-grams in generated responses.</p><p>Human Evaluation: Since automatic evaluation results may not be fully consistent with human judgements <ref type="bibr" target="#b7">(Liu et al., 2016)</ref>, human evaluation is necessary. Inspired by <ref type="bibr" target="#b9">Luo et al. (2018)</ref>, we use following three criteria. Fluency measures whether the generated responses have grammatical errors. Coherence denotes the semantic consistency and relevance between a response and its context. Informativeness indicates whether the response is meaningful and good at word usage.</p><p>A general reply should have the lowest Informativeness score. Each of these measurement scores ranges from 1 to 5. We randomly sample 100 examples from test set and generate total 400 responses using models mentioned above. All generated responses are scored by 7 annotators, who are postgraduate students and not involved in other parts of the experiment.</p><p>1 <ref type="url" target="https://code.google.com/archive/p/word2vec/">https://code.google.com/archive/p/ word2vec/</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results of Automatic Evaluation</head><p>The left part of Table <ref type="table" target="#tab_1">2</ref> is about automatic evaluation on test set. The proposed CSRR model significantly outperforms other baselines on three embedding-based metrics on both datasets. The improvement of our model indicates our semantic relationship modeling better reflects the structure of real-world conversations, and the responses generated by our models are more relevant to context. As for diversity, CSRR also gets the highest Dist-1 and Dist-2 scores.</p><p>For Ubuntu Dialog dataset, VHRED+w.d is the worst. With the help of discourse-level latent variable and utterance drop, VHCR+u.d leads to better performance. However, HRED is the worst on the Cornell Movie dataset. <ref type="bibr" target="#b11">Park et al. (2018)</ref> empirically explained the difference based on that Cornell Movie Dialog dataset is small in size, but very diverse and complex in content and style, and models like HRED often fail to generate appropriate responses for the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results of Human Evaluation</head><p>The right part of Table <ref type="table" target="#tab_1">2</ref> is about human evaluation results on 400 (100×4) responses. First, it is clear that CSRR model receives the best evaluation on three aspects, which proves the effectiveness of CSRR on generating high quality responses. Second, because of the existence of discourse-level and pair-level latent variables, responses are more coherent. Since these two kinds of variables learn high level semantic information, utterance-level ones serve better on expression diversion, also improve sentence fluency and informativeness.  easy questions, like greeting (Example 4), both HRED and CSRR perform well. In contrast, VHRED+w.d and VHCR+u.d tend to generate general and meaningless responses. For hard questions, like some technical ones (Example 1 to 3), the proposed CSRR obviously outperforms other baselines. Note that VHCR is to show the effectiveness of z c and it can also be considered as the ablation study of CSRR to illustrate the validity of z p . From above cases, we empirically find that with the help of z p , response generated by CSRR are not only relevant and consistent to context, but also informative and meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Case Study and Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>In this work, we propose a Conversational Semantic Relationship RNN model to learn the semantic dependency in multi-turn conversations. We ap-ply hierarchical strategy to obtain context information, and add three-hierarchy latent variables to capture semantic relationship. According to automatic evaluation and human evaluation, our model significantly improves the quality of generated responses, especially in coherence, sentence fluency and language diversity.</p><p>In the future, we will model the semantic relationship in previous turns, and also import reinforcement learning to control the process of topic changes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>An example of the semantic relationship in a multi-turn conversation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>for machine translation. Our model is designed to improve the co-Automatic and human evaluation results on Ubuntu Dialog Corpus and Cornell Movie Dialog Corpus.</figDesc><table><row><cell>Model</cell><cell cols="7">Average Extrema Greedy Dist-1 Dist-2 Coherence Fluency Informativeness</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ubuntu Dialog</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HRED</cell><cell>0.570</cell><cell>0.329</cell><cell>0.415</cell><cell>0.494 0.814</cell><cell>2.96</cell><cell>3.64</cell><cell>2.89</cell></row><row><cell>VHRED+w.d</cell><cell>0.556</cell><cell>0.312</cell><cell>0.405</cell><cell>0.523 0.856</cell><cell>2.52</cell><cell>3.35</cell><cell>3.24</cell></row><row><cell>VHCR+u.d</cell><cell>0.572</cell><cell>0.330</cell><cell>0.416</cell><cell>0.512 0.837</cell><cell>2.42</cell><cell>3.48</cell><cell>2.99</cell></row><row><cell>CSRR</cell><cell>0.612</cell><cell>0.345</cell><cell>0.457</cell><cell>0.561 0.882</cell><cell>3.39</cell><cell>3.91</cell><cell>3.75</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Cornell Movie Dialog</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HRED</cell><cell>0.547</cell><cell>0.370</cell><cell>0.387</cell><cell>0.489 0.801</cell><cell>3.02</cell><cell>3.65</cell><cell>2.85</cell></row><row><cell>VHRED+w.d</cell><cell>0.556</cell><cell>0.365</cell><cell>0.405</cell><cell>0.512 0.850</cell><cell>3.05</cell><cell>3.76</cell><cell>3.24</cell></row><row><cell>VHCR+u.d</cell><cell>0.587</cell><cell>0.378</cell><cell>0.434</cell><cell>0.507 0.837</cell><cell>3.13</cell><cell>3.73</cell><cell>3.06</cell></row><row><cell>CSRR</cell><cell>0.620</cell><cell>0.395</cell><cell>0.462</cell><cell>0.522 0.873</cell><cell>3.43</cell><cell>3.82</cell><cell>3.78</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table 3 shows the examples generated by CSRR model and other baseline models. For some Example 1: Context: how's sleeping beauty? → still sleeping. → kiss him, maybe he'll turn into a frog. HRED: he's a good man. Context: hi wondering whether to use 32 or 64 bit → how much ram does the pc have? more than 2 gb you might → definitely? because, i' ve heard that ubuntu recommends 32 bit and HRED: it's a bit more than a 32 bit version of a 32bit system. VHRED+w.d: i have no idea , but it doesn't matter. VHCR+u.d: i don't know, but i haven't used it much. CSRR: yes, but you can use 64 bit or 32 bit. Example 4: Context: how can i narrow down an issue i'm having? i am running → check out the system logs in var logs ... in particular kern.log, syslog → thank you very much. HRED: no problem. VHRED+w.d: i'm glad to hear that, don't know why. VHCR+u.d: i'm not sure what you mean by ' &lt;unk&gt; '. CSRR: you're welcome. i don't know if that's the case.</figDesc><table><row><cell>VHRED+w.d: what?</cell></row><row><cell>VHCR+u.d: maybe he's dead.</cell></row><row><cell>CSRR: Really? I won't try.</cell></row><row><cell>Example 2:</cell></row><row><cell>Context: tried installing the driver from nvidia, but funny</cell></row><row><cell>thing is i've installed → what is the problem you are hav-</cell></row><row><cell>ing exactly → at this point driver activated but not in use</cell></row><row><cell>HRED: what card do you have?</cell></row><row><cell>VHRED+w.d: are you using the nvidia driver? or from</cell></row><row><cell>the command line?</cell></row><row><cell>VHCR+u.d: i have no idea what you are talking about, i</cell></row><row><cell>just know that</cell></row><row><cell>CSRR: you need to install the nvidia driver from the</cell></row><row><cell>nvidia site.</cell></row><row><cell>Example 3:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Examples generated by CSRR model and other baselines. The first example is from Cornell Movie Dialog, while the bottom three rows are from Ubuntu Dialog.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by <rs type="funder">National Natural Science Foundation of China</rs> (NO. <rs type="grantNumber">61662077</rs>, NO. <rs type="grantNumber">61876174</rs>) and <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">NO. YS2017YFGH001428</rs>). We sincerely thank the anonymous reviewers for their helpful and valuable suggestions.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wDvMbJR">
					<idno type="grant-number">61662077</idno>
				</org>
				<org type="funding" xml:id="_DnbUWp4">
					<idno type="grant-number">61876174</idno>
				</org>
				<org type="funding" xml:id="_QNhRFuD">
					<idno type="grant-number">NO. YS2017YFGH001428</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer Caglar Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares Holger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs</title>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics</title>
		<meeting>the 2nd Workshop on Cognitive Modeling and Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="76" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 2nd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter</title>
		<meeting>the 2016 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An auto-encoder matching model for learning utterance-level semantic dependency in dialogue generation</title>
		<author>
			<persName><forename type="first">Liangchen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="702" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A hierarchical latent structure for variational conversation modeling</title>
		<author>
			<persName><forename type="first">Yookoon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1792" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3776" to="3783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3295" to="3301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A conditional variational framework for dialog generation</title>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="504" to="509" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How to make context more useful? an empirical study on contextaware neural conversational models</title>
		<author>
			<persName><forename type="first">Zhiliang</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent attention network for response generation</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5610" to="5617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Context-sensitive generation of open-domain conversational responses</title>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingfu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2437" to="2447" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
