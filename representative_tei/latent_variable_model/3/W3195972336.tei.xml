<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Scalable Strategy for the Identification of Latent-variable Graphical Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2018-09-05">5 Sep 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniele</forename><surname>Alpago</surname></persName>
							<email>alpagodani@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="department">partment of Information Engineering</orgName>
								<orgName type="institution">University of Padova</orgName>
								<address>
									<settlement>Padova</settlement>
									<country>Italy;</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">partment of Information Engineering</orgName>
								<orgName type="institution">University of Padova</orgName>
								<address>
									<settlement>Padova</settlement>
									<country>Italy;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mattia</forename><surname>Zorzi</surname></persName>
							<email>zorzimat@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="department">partment of Information Engineering</orgName>
								<orgName type="institution">University of Padova</orgName>
								<address>
									<settlement>Padova</settlement>
									<country>Italy;</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">partment of Information Engineering</orgName>
								<orgName type="institution">University of Padova</orgName>
								<address>
									<settlement>Padova</settlement>
									<country>Italy;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Augusto</forename><surname>Ferrante</surname></persName>
							<email>augusto@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="department">partment of Information Engineering</orgName>
								<orgName type="institution">University of Padova</orgName>
								<address>
									<settlement>Padova</settlement>
									<country>Italy;</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Scalable Strategy for the Identification of Latent-variable Graphical Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-09-05">5 Sep 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1809.01608v1[math.OC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Latent-variable graphical models</term>
					<term>Reciprocal processes</term>
					<term>Maximum likelihood</term>
					<term>Maximum entropy</term>
					<term>Regularization</term>
					<term>System identification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose an identification method for latent-variable graphical models associated to autoregressive (AR) Gaussian stationary processes. The identification procedure exploits the approximation of AR processes through stationary reciprocal processes thus benefiting of the numerical advantages of dealing with block-circulant matrices. These advantages become more and more significant as the order of the process gets large. We show how the identification can be cast in a regularized convex program and we present numerical examples that compares the performances of the proposed method with the existing ones.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE ideas behind graphical models have their origins in several scientific areas, such as statistical physics and genetics back at the beginning of the last century. However, only recent developments of such ideas allowed to employ graphical models in identification problems involving high dimensional data <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. In this direction, particularly useful are sparse graphical models, i.e. graphs with few edges that describe the interactions between a large number of variables. Such models have become very popular in the literature in the recent years because, beside giving a concise representation of the phenomenon under scruting, sparsity implies a limited number of model's parameters thus avoiding overfitting in the identification procedure. Although the latter is a desirable property, enforcing sparsity in the identification procedure is not always the best choice, as it may prevent a sufficiently rich description of the underlying phenomenon. Indeed, in many practical situations, the presence of a few common, hidden behaviors between the variables of interest explaining the most part of the interactions between the observed variables can be crucial. The fact that a sparse graphical model is not able to describe the essential features of this kind of phenomena motivates the introduction of the socalled latent-variable graphical models. The latter consist in a two-layer graph where the conditional dependence relations between the observed variables are mainly due to the latent variables (i.e. variables not accessible to observations): each latent-variable (on the top-layer) is connected to the majority of the observed variables (on the bottom-layer), making the latter a sparse subgraph. Since the number of latent-variables is small, the overall graph has a reduced number of edges. In the simplest possible setting, one can associate this kind of models to a Gaussian random vector <ref type="bibr" target="#b7">[8]</ref>. The particular graphical structure translates in a sparse plus low-rank decomposition of its concentration matrix. In <ref type="bibr" target="#b7">[8]</ref> the identification of the sparse and the low-rank part of the concentration matrix has been cast in a regularized maximum-likelihood optimization problem. A dynamic version of this problem, i.e. the identification of latent-variable graphical models for AR Gaussian processes, has been considered in <ref type="bibr" target="#b8">[9]</ref> where the problem has been shown to be strictly connected to a maximum-entropy problem. As showed in <ref type="bibr" target="#b9">[10]</ref>, this identification problem can be effectively solved by an ADMM-type algorithm. The optimization procedure, however, involves the inversion and the eigenvalue decomposition of matrices whose dimension is proportional to the product of the order of the process by the dimension of the process, making the procedure numerically critical when the order of the AR process is high, as it happens, for example, when the AR process is an approximation of an ARMA one. In this paper we consider the problem of identifying latentvariable graphical models for stationary Gaussian reciprocal processes. The latter are periodic stationary processes <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> and they have been proven to be a worthy approximation of Gaussian AR processes, provided that the period N is sufficiently large <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b15">[16]</ref>. We will show that the proposed identification procedure is in fact an approximation of the maximum entropy and maximum likelihood identification paradigms proposed for the classical AR processes. The fact that stationary reciprocal process can be modeled by means of block-circulant matrices represents a big numerical advantage as the inversion and the eigenvalue decomposition of such matrices can be performed robustly <ref type="bibr" target="#b18">[19]</ref> making the proposed procedure attractive also for the identification AR processes of high order and hence for ARMA processes. The paper is organized as follows: In Section II we fix the notation and we recall the fundamental results used in the rest of the paper. In Section III we introduce reciprocal processes and we explain how they are related to AR processes. In Section IV we characterize graphical models associated to reciprocal processes while, in Section V, we propose a convex optimization problem for the identification of such models. Section VI is devoted to the ADMM formulation of the optimization problem and Section VII reports numerical experiments concerning the implementation of the proposed procedure. Finally, in Section VIII we draw the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. NOTATION AND BACKGROUND</head><p>In this paper we will deal both with real matrices and with matrix-valued functions defined on the unit-circle T := {e iθ : θ ∈ [-π, π]}. For such functions we will omit the dependence on θ when it is clear from the context, i.e. we will write F in place of F(e iθ ). The rank of a matrix G is denoted by rank(G) while the (normal) rank of any C p×p -valued analytic function F defined on T, is defined as</p><formula xml:id="formula_0">rank(F) := max θ ∈[-π,π]</formula><p>rank(F(e iθ )).</p><p>(1)</p><p>In the same fashion, the following notations will be used indifferently in the case that G is a C p×p -valued function defined on T or a square constant matrix: G denotes the transpose of G, G * its transpose-conjugate and diag(G) ∈ C p denotes the vector whose entries are the diagonal elements of G. ker(G) indicates the kernel of G. G &gt; 0 and G ≥ 0 denote that G is a positive definite and, respectively, positive semidefinite. tr(G), det(G) and G -1 denote the trace of G, the determinant of G and its inverse, respectively. I p denotes the identity matrix of order p.</p><p>We define the cone</p><formula xml:id="formula_1">S p := {F ∈ H p : Φ -α I p ≥ 0 a.e. on T, for some α &gt; 0},</formula><p>where H p is the space of square integrable coercive functions defined on the unit circle and taking values in the space of p × p Hermitian matrices. For any F ∈ H p we will use equivalently the notations</p><formula xml:id="formula_2">π -π F(e iθ ) dθ 2π , F</formula><p>for the integral of F over [-π, π] with respect to the normalized Lebesgue measure on T. We define also the family of matrix pseudo-polynomials</p><formula xml:id="formula_3">Q p,n := n ∑ k=-n Q k e iθ k , Q -k = Q k ∈ R p×p .</formula><p>For any sub-interval (x 1 , x 2 ) := {x :</p><formula xml:id="formula_4">x 1 &lt; x &lt; x 2 } of an interval (a, b) ⊂ R, we denote with (x 1 , x 2 ) c the complement set of (x 1 , x 2 ) in (a, b). E[•]</formula><p>denotes the expectation operator.</p><p>In this paper we will always consider AR processes of order n and reciprocal processes of period N, i.e. completely specified in a finite interval of length N. All such processes are understood with zero mean throughout the paper. It will be always assumed that N &gt; 2n and that N is an even number. The case with N odd can be dealt in a similar way. We define the vector space C ⊂ R mN×mN of the (real) symmetric, blockcirculant matrices</p><formula xml:id="formula_5">C = circ{C 0 ,C 1 , . . . ,CN 2 -1 ,CN 2 ,C N 2 -1 , . . . ,C 1 }, whose first block-column is composed by the m × m blocks C 0 ,C 1 , . . . ,CN 2 -1 ,CN 2 ,C N 2 -1 , . . . ,C 1 .</formula><p>The space C is endowed with the inner product C, D C := tr(C D). The symbol of the block-circulant matrix C ∈ C is defined as the m × m pseudopolynomial</p><formula xml:id="formula_6">Φ(ζ ) := N-1 ∑ k=0 C k ζ -k , with C k = C N-k for k &gt; N 2 ,<label>(2)</label></formula><p>where ζ := e i 2π N is the N-th root of unity. Proposition 1: Let C be a block-circulant matrix with symbol Φ(ζ ) defined by <ref type="bibr" target="#b1">(2)</ref>. Then</p><formula xml:id="formula_7">C = F * diag Φ(ζ 0 ), Φ(ζ 1 ), • • • , Φ(ζ N-1 ) F,<label>(3)</label></formula><p>where F is the (Fourier) unitary block-matrix</p><formula xml:id="formula_8">F = 1 √ N      ζ -0•0 I ζ -0•1 • • • ζ -0•(N-1) I ζ -1•0 I ζ -1•1 I • • • ζ -1•(N-1) I . . . . . . . . . . . . ζ -(N-1)•0 I ζ -(N-1)•1 I • • • ζ -(N-1)•(N-1) I      .</formula><p>This is a classical result in the scalar case; technical details for the block-circulant case can be found, for instance, in <ref type="bibr">[20, page 6]</ref>. We define the subspace B ⊆ C of symmetric, banded blockcirculant mN × mN matrices of bandwidth n, with N &gt; 2n, containing the matrices of the form</p><formula xml:id="formula_9">B = circ{B 0 , B 1 , • • • , B n , 0, • • • , 0, B n , • • • , B 1 },<label>(4)</label></formula><p>that inherits the inner product defined on C. Note that, according to definition (2), the symbol of a banded matrix B ∈ B is</p><formula xml:id="formula_10">Ψ(ζ ) = n ∑ k=-n B k ζ -k , B -k = B k .</formula><p>The projection operator P B : C → B is defined as</p><formula xml:id="formula_11">P B (C) := circ{C 0 ,C 1 , • • • ,C n , 0, • • • , 0,C n , • • • ,C 1 }.</formula><p>Given Ω = {(i, j) : i, j = 1, . . . , m}, the projection operator P Ω : C → C is defined such that P Ω (C) is a block-circulant matrix whose blocks have support Ω.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RECIPROCAL PROCESSES</head><p>Let {y(k), k = 1, 2, . . . , N}, be an m-dimensional Gaussian stationary stochastic process defined on a finite interval <ref type="bibr">[1, N]</ref>. For k = 1, . . . , N, we have y(k) := [y 1 (k) . . . y m (k)] ∈ R m , therefore the process is completely characterized by the random vector y := [y 1 <ref type="bibr" target="#b0">(1)</ref> . . . y m (1) . . . . . . y 1 (N) . . . y m (N)] ∈ R mN . In <ref type="bibr" target="#b10">[11]</ref> it has been shown that y is a restriction of a widesense stationary periodic process of period N defined on the whole integer line Z if and only if the mN × mN covariance matrix Σ of y is symmetric block-circulant:</p><formula xml:id="formula_12">Σ = circ{Σ 0 , Σ 1 , . . . , Σ N 2 , . . . , Σ 1 },<label>(5)</label></formula><p>where E[y(i)y( j) ] = Σ i-j , i, j = 1, . . . , N, are the covariance lags of the process such that Σ k = Σ N-k for k &gt; N/2. In view of the above equivalence, we will denote with y both the widesense stationary periodic process defined in the whole line Z and its restriction, depending on the context. A particular class of stationary periodic processes is represented by reciprocal processes. Definition 1: y is a reciprocal process of order n on [1, N] if, for all t 1 ,t 2 ∈ [1, N], the random variables of the process in the interval (t 1 ,t 2 ) ⊂ [1, N] are conditionally independent to the random variables in (t 1 ,t 2 ) c , given the 2n boundary values y(t 1 -n + 1), . . . , y(t 1 ), y(t 2 ), . . . , y(t 2 + n -1), where the sums tk and t + k are to be understood modulo N.</p><p>The following result has been proved in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">Theorem 3.3]</ref>: it states that a reciprocal process is completely specified by a block-circulant matrix whose inverse has a banded structure.</p><p>Theorem 1: A non-singular mN × mN-dimensional matrix Σ is the covariance matrix of a periodic reciprocal process of order n if and only if its inverse is a positive definite symmetric block-circulant matrix which is banded of bandwidth n, namely Σ -1 ∈ B. Let Σ0 , . . . , Σn be given estimates of the first n+1 covariance lags Σ 0 , . . . , Σ n of the underlying reciprocal process. In view of Theorem 1, the identification of a reciprocal process can be formulated as the following matrix completion problem.</p><p>Problem 1: Given the n + 1 estimates Σ0 , . . . , Σn , compute a sequence Σ n+1 , . . . , Σ N 2 , in such a way to form a symmetric, positive definite block-circulant matrix</p><formula xml:id="formula_13">Σ = circ{ Σ0 , . . . , Σn , Σ n+1 , . . . , Σ N 2 , . . . , Σ n+1 , Σ n , . . . , Σ 1 }, with Σ -1 ∈ B.</formula><p>It has been shown in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b10">[11]</ref> that a particular solution to Problem 1 is the one which solves the following maximum entropy problem:</p><formula xml:id="formula_14">argmax Σ∈C log det Σ subject to Σ &gt; 0 P B (Σ -Σ) = 0.<label>(6)</label></formula><p>whose dual problem has been proven to be argmin</p><formula xml:id="formula_15">X∈B -log det X + X, Σ C subject to X &gt; 0 (7)</formula><p>where Σ ∈ B is the symmetric, banded block-circulant matrix of bandwidth n, Σ = circ{ Σ0 , Σ1 , . . . , Σn , 0, . . . , 0, Σ n , . . . , Σ 1 }, containing the covariance lags estimated from the data and the optimal value of dual variable X is indeed equal to Σ -1 , i.e. the inverse of the solution of <ref type="bibr" target="#b5">(6)</ref>. Strong duality between ( <ref type="formula" target="#formula_14">6</ref>) and <ref type="bibr" target="#b6">(7)</ref> implies that ( <ref type="formula" target="#formula_14">6</ref>) and ( <ref type="formula">7</ref>) are equivalent. In what follows we assume that Σ &gt; 0 as it is a necessary condition for Problem (6) to be feasible. In the case that Σ is not positive definite, we can consider a positive definite banded block-circulant matrix sufficiently close to Σ which can be obtained by solving a structured covariance estimation problem, see <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>.</p><p>AR approximation: Next we recall how reciprocal processes can be seen as an approximation of autoregressive (AR) processes. More precisely, let y := {y(t) : t ∈ Z} be an mdimensional, AR, full-rank, Gaussian wide-sense stationary process of order n,</p><formula xml:id="formula_16">n ∑ k=0 B k y(t -k) = e(t), e(t) ∼ N(0, I m ), t ∈ Z,<label>(8)</label></formula><p>and let R k := E[y(t)y(t -k) ], k ∈ Z, be its k-th covariance lag. The spectrum of y is the Fourier transform of the sequence R k with k ∈ Z, i.e.</p><formula xml:id="formula_17">Φ(e iθ ) = ∞ ∑ k=-∞ R k e -iθ k , R -k = R k , θ ∈ [-π, π]. (<label>9</label></formula><formula xml:id="formula_18">)</formula><p>Suppose now that T observations y(1), . . . , y(T ) of the process y are available, and let</p><formula xml:id="formula_19">Rk = 1 T T ∑ t=k y(t)y(t -k) , k = 0, 1, . . . , n,<label>(10)</label></formula><p>be estimates of the first n + 1 covariance lags R 0 , . . . , R n . The identification of such a process can be cast to a covariance extension problem. Problem 2: Given n + 1 estimates R0 , . . . , Rn , complete them with a sequence R n+1 , R n+2 , . . . in such a way that the Fourier transform of the extended (infinite) sequence is a power spectral density.</p><p>A particular solution of Problem 2 is the one proposed by J. P. Burg in <ref type="bibr" target="#b23">[23]</ref>: choose R n+1 , R n+2 , . . . maximizing the entropy rate of the process, i.e. that solves the following optimization problem argmax</p><formula xml:id="formula_20">Φ∈S m log det Φ subject to e iθ k Φ = Rk , k = 0, 1, . . . , n.<label>(11)</label></formula><p>The dual of <ref type="bibr" target="#b10">(11)</ref> has been shown to be, see for instance <ref type="bibr" target="#b24">[24]</ref>:</p><formula xml:id="formula_21">argmin Φ -1 ∈Q m,n -log det Φ -1 + Φ -1 , Φ subject to Φ &gt; 0<label>(12)</label></formula><p>where</p><formula xml:id="formula_22">Φ(e iθ ) = n ∑ k=-n Rk e -iθ k , R-k = R k ,<label>(13)</label></formula><p>is the truncated periodogram of the process y. These kind of problems have been extensively studied and generalized in the recent years, see for instance <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>.</p><p>We recall that, for N → ∞, Toeplitz matrices can be approximated arbitrarily well by circulant matrices <ref type="bibr" target="#b32">[32,</ref><ref type="bibr">Lemma 4.2]</ref>; hence, for N → ∞, Problem 1 consists in searching a completion that leads to an infinite positive definite block-Toeplitz covariance matrix, i.e. such that the Fourier transform of the resulting extended sequence is a power spectral density. By Theorem 3.1 in <ref type="bibr" target="#b17">[18]</ref>, for N → ∞, Problem (6) is the classical Burg's maximum entropy problem whose solution is an AR process of order n. In light of this observation, we can understand the reciprocal process associated to the solution of <ref type="bibr" target="#b6">(7)</ref> as an approximation of the AR process solution of the Burg's maximum entropy problem <ref type="bibr" target="#b10">(11)</ref>. In the following sections we will exploit this approximation for the identification of latent-variable AR graphical models.</p><p>The reciprocal approximation just explained has also an interesting interpretation in the frequency domain. Indeed, it corresponds to sampling the spectrum (9) of the AR process y, over the interval [-π, π], with sample period 2π/N, thus obtaining the symbol of the covariance matrix of the corresponding reciprocal process:</p><formula xml:id="formula_23">Φ(ζ ) = N-1 ∑ k=0 Σ k ζ -k , Σ k = Σ N-k for k &gt; N 2 .</formula><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates this relation. According to Proposition 1, the covariance matrix Σ of the reciprocal process y that approximates y writes as</p><formula xml:id="formula_24">Σ = F * circ{Φ(ζ 0 ), Φ(ζ 1 ), . . . , Φ(ζ N-1 )}F,<label>(14)</label></formula><p>hence, its inverse</p><formula xml:id="formula_25">Σ -1 = F * circ{Φ(ζ 0 ) -1 , Φ(ζ 1 ) -1 , . . . , Φ(ζ N-1 ) -1 }F,<label>(15)</label></formula><p>can be robustly computed by inverting the N blocks</p><formula xml:id="formula_26">Φ(ζ 0 ), Φ(ζ 1 ), . . . , Φ(ζ N-1</formula><p>), all of size m × m. As a final remark, we recall that eigevalues and eigenvectors of circulant matrices can be robustly computed as well, thanks to the availability of closed-form formulas, see for instance <ref type="bibr" target="#b32">[32]</ref>. As highlighted by the frequency-domain interpretation, the goodness of the approximation depends on the regularity of the spectrum: the larger is the rate of variation of the spectrum, the larger N has to be chosen in order to get a good approximation of the AR process. The frequency-domain interpretation makes even more explicit the relationship between Burg's maximum entropy problem <ref type="bibr" target="#b10">(11)</ref> and Problem <ref type="bibr" target="#b5">(6)</ref>: provided that the number of samples N is sufficiently large, by sampling the spectrum solution of <ref type="bibr" target="#b10">(11)</ref> we obtain an approximation of the matrix Σ solution of (6); viceversa, the symbol of Σ can be extended over the whole interval [-π, π] in order to approximate the solution of <ref type="bibr" target="#b10">(11)</ref>. Figure <ref type="figure">2</ref> summarizes this bi-directional relationships.</p><formula xml:id="formula_27">θ -π = 0 N -1 π 2π N Φ(e iθ ) Φ(ζ )</formula><p>Φ(e iθ ) &gt; 0 solution of ( <ref type="formula" target="#formula_20">11</ref>)</p><formula xml:id="formula_28">Φ(ζ ) symbol of Σ &gt; 0, Σ ∈ C solution of (6) Φ -1 ∈ Q m,n AR process y Σ -1 ∈ B reciprocal process y sampling extension over [-π, π] sampling extension over [-π, π]</formula><p>Fig. <ref type="figure">2</ref>: Schematic representation of the reciprocal approximation of the AR process in terms of solutions of problems <ref type="bibr" target="#b10">(11)</ref> and <ref type="bibr" target="#b5">(6)</ref>.</p><p>IV. GRAPHICAL MODELS Consider a Gaussian random vector x ∼ N(0, Σ) taking values in R m , where Σ = Σ &gt; 0 so that the concentration matrix K = [k i j ] := Σ -1 is well-defined. If we denote the components of x as x 1 , . . . , x m , for any i = j, we have that x i is conditionally independent from x j given the remaining random variables x k , k = i, j, i.e.</p><formula xml:id="formula_29">x i ⊥ ⊥ x j | {x k } k =i, j ,<label>(16)</label></formula><p>if and only if the element k i j in position (i, j) of the concentration matrix K is equal to zero. Formally,</p><formula xml:id="formula_30">x i ⊥ ⊥ x j | {x k } k =i, j ⇐⇒ k i j = 0. (<label>17</label></formula><formula xml:id="formula_31">)</formula><p>The previous relation allows to construct an undirected graph G = (V, E), with V = {1, . . . , m} and E ⊂ V × V , associated to the random vector x by taking the components x 1 , . . . , x m of x as nodes and such that the edges reflect the conditional dependence relations between the random variables, i.e.</p><formula xml:id="formula_32">(i, j) / ∈ E ⇐⇒ x i ⊥ ⊥ x j | {x k } k =i, j .<label>(18)</label></formula><p>The graph G is called the graphical model associated to</p><p>x and it gives a visual representation of the conditional dependence relations between the components of x. Observe that G is completely characterized by the sparsity pattern of the concentration matrix of the random vector.</p><p>A characterization of conditional independence can be given also in the dynamic setting. In particular, we consider an mdimensional, Gaussian, wide-sense stationary AR process x described by a model like <ref type="bibr" target="#b7">(8)</ref>. For any index set I ⊂ V , define</p><formula xml:id="formula_33">X I := span{x j (t) : j ∈ I, t ∈ Z},</formula><p>as the closure of the set containing all the finite linear combinations of the variables x j (t). For any i = j, the notation</p><formula xml:id="formula_34">X {i} ⊥ ⊥ X { j} | X V \{i, j}</formula><p>generalizes <ref type="bibr" target="#b15">(16)</ref> and it means that for all t 1 ,t 2 , x i (t 1 ) and x j (t 2 ) are conditionally independent given the space linearly generated by {x k (t), k ∈ V \ {i, j},t ∈ Z}. One can prove that</p><formula xml:id="formula_35">X {i} ⊥ ⊥ X { j} | X V \{i, j} ⇐⇒ [Φ(e iθ ) -1 ] i j = 0,<label>(19)</label></formula><p>for any θ ∈ [-π, π], see <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, which is the natural generalization of <ref type="bibr" target="#b16">(17)</ref>. Accordingly, we can construct the undirected graph G = (V, E) representing the conditional dependence relations between the components of the process x by defining the set of edges as follows:</p><formula xml:id="formula_36">(i, j) / ∈ E ⇐⇒ X {i} ⊥ ⊥ X { j} | X V \{i, j} .<label>(20)</label></formula><p>In this framework, the graph G is completely characterized by the sparsity pattern of the inverse power spectral density of the process. Identification of sparse graphical models of reciprocal processes have been studied in <ref type="bibr" target="#b33">[33]</ref>. In many practical situations there is the presence of a few common, latent, behaviors between the variables of interest that are responsible of the most part of the interactions between the observed variables and that cannot be captured by considering only a sparse model structure. This leads to a particular type of graphical models called latent-variable graphical models or sparse plus low-rank graphical models, <ref type="bibr" target="#b7">[8]</ref>. Such models admit a two-layer graphical structure in which the nodes in the upper layer stand for the (few) latentvariables, while the nodes in the bottom layer represent the observed variables.</p><p>Latent-variable graphical models associated to Gaussian random vectors have been considered in <ref type="bibr" target="#b7">[8]</ref> and then generalized in <ref type="bibr" target="#b8">[9]</ref> to AR stochastic processes. The latter, say z := {z(t), t ∈ Z}, is assumed to be of the form z = [y x ] where y is the R m -valued process containing the observed variables while x is the process containing l latent variables. Let Φ y denotes the spectral density of y. Under the assumptions that l m and the dependence relations among the observed variables are mostly through the latent variables, we have the decomposition</p><formula xml:id="formula_37">Φ -1 y = Γ -Λ,<label>(21)</label></formula><p>where Γ &gt; 0 is sparse and its support reflects the conditional dependencies among the observed variables, while Λ ≥ 0 is low-rank and its rank equals the number l of latent-variables.</p><p>We are now ready to extend the previous results for Gaussian reciprocal processes. Let z := [y x ] be a Gaussian, periodic, reciprocal process of order n defined on the interval [1, N], where y plays the role of the m-dimensional observed process and x is the l-dimensional latent process, respectively. The covariance matrix Σ z of z and its inverse can be partitioned as</p><formula xml:id="formula_38">Σ z = Σ y Σ yx Σ yx Σ x , Σ -1 z = S A A R ,<label>(22)</label></formula><p>where Σ y ∈ C and Σ x ∈ C l are the covariance matrices of y and x, respectively. Here, C l denotes the vector space of blockcirculant, symmetric matrices as C, except that the blocks have dimension l × l. Applying the Schur complement, we obtain the relation</p><formula xml:id="formula_39">Σ -1 y = S -L,<label>(23)</label></formula><p>where S &gt; 0 is the concentration matrix of process y conditioned on x, and L ≥ 0 is defined as L := A R -1 A . In order to ensure that, according to Theorem 1, Σ -1 y ∈ B we assume both S and L to be symmetric, block-circulant, banded of bandwidth n, i.e. S = circ{S 0 , S 1 , . . . , S n , 0, . . . , 0, S n , . . . , S 1 },</p><formula xml:id="formula_40">L = circ{L 0 , L 1 , . . . , L n , 0, . . . , 0, L n , . . . , L 1 }. (<label>24</label></formula><formula xml:id="formula_41">)</formula><p>By construction, the matrix L has rank equal to the number of latent variables l, therefore under the assumption that l m, it is a low-rank matrix. If S is a sparse matrix, then we will refer to <ref type="bibr" target="#b23">(23)</ref> as sparse plus low-rank decomposition of Σ -1 y which is the analogue of (21) for reciprocal processes. It remains to show that an appropriate sparsity pattern of S reflects that the dependence relations among observed variables are mostly through the few latent variables. For this purpose, let y i := [y i (1) . . . y i (N)] , i = 1, . . . , m, be the i-th component of the process y and let x j := [x j (1) . . . x j (N)] , j = 1, . . . , l, be the j-th component of the process x. Although the components of the reciprocal processes are defined for any k ∈ Z, by periodicity it is sufficient to impose conditional independence only for k ∈ [1, N]. We assume that the blocks S 0 , S 1 , . . . , S n of S have common support Ω ⊆ {(i, j) : i, j = 1, . . . , m} namely,</p><formula xml:id="formula_42">(S k ) i j = (S k ) ji = 0, k = 0, . . . , n, ∀ (i, j) ∈ Ω c , (<label>25</label></formula><formula xml:id="formula_43">)</formula><p>where Ω is the set of pairs that contains all the (i, i), i = 1, . . . , m. By property <ref type="bibr" target="#b16">(17)</ref>, equation ( <ref type="formula" target="#formula_42">25</ref>) is equivalent to</p><formula xml:id="formula_44">y i (t 1 ) ⊥ ⊥ y j (t 2 ) | {y h (s), h = i, j, s = 1, . . . , N, y i (s 1 ), s 1 = t 1 , y j (s 2 ), s 2 = t 2 , x},<label>(26)</label></formula><p>for any t 1 , t 2 ∈ [1, N] and for any pair (i, j) ∈ Ω c . Proposition 2: Condition (26) is equivalent to</p><formula xml:id="formula_45">y i (t 1 ) ⊥ ⊥ y j (t 2 ) | {y h (s), h = i, j, s = 1, . . . , N, x}<label>(27)</label></formula><p>for any t 1 , t 2 ∈ [1, N] and for any (i, j) ∈ Ω c . Proof: The proof exploits basic results of the theory of Hilbert spaces of second-order random variables, see for instance <ref type="bibr" target="#b34">[34,</ref><ref type="bibr">Chapter 2]</ref>. First of all, let</p><formula xml:id="formula_46">ε := ε i ε j = y i y j -E y i y j y h (s), h = i, j, s = 1, . . . , N, x</formula><p>denotes the error affecting the projection of [y i y j ] onto the subspace generated by {y h (s), h = i, j, s = 1, . . . , N, x}, for any t 1 , t 2 ∈ [1, N] and for any (i, j) ∈ Ω c . It can be shown that ε is a zero-mean, Gaussian, random vector. Accordingly, proving ( <ref type="formula" target="#formula_45">27</ref>) is equivalent to prove that</p><formula xml:id="formula_47">E ε i ε j = 0 ⇐⇒ ε i (t 1 ) ⊥ ⊥ ε j (t 2 )<label>(28)</label></formula><p>for any t 1 , t 2 ∈ [1, N] and for any (i, j) ∈ Ω c , <ref type="bibr" target="#b34">[34]</ref>. Let now Π be a permutation matrix that permutes the rows of z = [y x ] in order to obtain</p><formula xml:id="formula_48">z := Π z =     y i y j y h =i, j x     = z1 z2 ,</formula><p>where y h =i, j is the vector containing the random variables y h (s), h = i, j, s = 1, . . . , N. We partition the covariance matrix Σ z of z as</p><formula xml:id="formula_49">Σ z = Σ z1 Σ z1 z2 Σ z2 z1 Σ z2 ,</formula><p>where Σ z1 and Σ z2 are the covariance matrices of z1 and z2 , respectively. It is well known that its inverse can be partitioned conformably as</p><formula xml:id="formula_50">Σ -1 z = Π Σ -1 z Π = S * * * , where S := Σ z1 -Σ z1 z2 Σ -1 z2 Σ z2 z1 -1<label>(29)</label></formula><p>is a permuted version of matrix S, according to the permutation matrix Π. By construction, the Schur complement formula applied on Σ z gives</p><formula xml:id="formula_51">Σ ε = Σ z1 -Σ z1 z2 Σ -1 z2 Σ z2 z1 = S-1 ,<label>(30)</label></formula><p>that relates the covariance matrix Σ ε of the projection error ε to the covariance matrix Σ z of z. Condition ( <ref type="formula" target="#formula_42">25</ref>) is equivalent to say that S, and therefore S-1 , is block-diagonal. Accordingly, by <ref type="bibr" target="#b30">(30)</ref>, Σ ε is block-diagonal, i.e. ε i and ε j are independent, which is equivalent to <ref type="bibr" target="#b28">(28)</ref> as we wanted to prove. The above result reflects the fact that the random variables {y i (s 1 ), y j (s 2 ), s 1 = t 1 , s 2 = t 2 } do not play any role in the conditioning <ref type="bibr" target="#b26">(26)</ref>. Moreover, the group-sparsity condition <ref type="bibr" target="#b25">(25)</ref> translates in the fact that the conditional dependence relations between the observed variables are mainly due to the few latent variables. Accordingly, condition <ref type="bibr" target="#b27">(27)</ref> represents the reciprocal counterpart of condition <ref type="bibr" target="#b16">(17)</ref>. We conclude that Σ -1 z in (22) together with <ref type="bibr" target="#b25">(25)</ref> define an undirected graph for the Gaussian random vector z which admits a two-layer structure where -The nodes in the upper-layer represent the l variables of the latent-process x 1 , . . . , x l while the nodes in the bottomlayer represent the m variables of the observed process y 1 , . . . , y m . -The edges are given by the entries of the concentration matrix Σ -1 z . In particular, the edge (i, j), between two vectors y i and y j , i = j, is described by</p><formula xml:id="formula_52">[(S 0 ) i j (S 1 ) i j . . . (S n ) i j 0 . . . 0 (S n ) ji . . . (S 1 ) ji ] .</formula><p>Example 1: Consider the case in which N = 2, m = 7, l = 2, and suppose that the graphical model associated to the vector z is the one depicted in Figure <ref type="figure" target="#fig_1">3</ref>. In this case, the concentration matrix of vector z will have the structure <ref type="bibr" target="#b22">(22)</ref> with</p><formula xml:id="formula_53">S = S 0 S 1 S 1 S 0 , S 0 , S 1 ∈ R 7×7 ,</formula><p>and R is a 2 × 2 matrix. The presence of an edge between y 2 and y 4 implies that at least one of the two elements (S 0 ) 24 and (S 1 ) 24 is different from zero. Similar arguments holds for the edge between y 5 and y 6 . Thus, Ω = {(i, i) : i = 1, . . . , 7} ∪ {(2, 4), (5, 6)}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. IDENTIFICATION OF LATENT-VARIABLE RECIPROCAL GRAPHICAL MODELS</head><p>The problem of identifying a latent-variable graphical model associated to a Gaussian random vector has been firstly considered in <ref type="bibr" target="#b7">[8]</ref> where the solution is obtained by solving a regularized maximum likelihood problem. In <ref type="bibr" target="#b8">[9]</ref> the problem has been extended to a dynamic setting, by considering an AR Gaussian process. More precisely, in <ref type="bibr" target="#b8">[9]</ref> a regularized version of Problem ( <ref type="formula" target="#formula_21">12</ref>) that relies on the sparse plus low-rank decomposition of the inverse of the observed spectrum in <ref type="bibr" target="#b21">(21)</ref>, has been considered: argmin</p><formula xml:id="formula_54">Γ, Λ∈Q m,n -log det(Γ -Λ) + Γ -Λ, Φy + γ S φ 1 (Γ) + γ L φ * (Λ) subject to Γ -Λ &gt; 0 Λ ≥ 0.<label>(31)</label></formula><p>Here, γ S , γ L &gt; 0 are the regularization parameters that balance the effects of the two regularizers φ 1 and φ * inducing sparsity and low-rank on Γ and Λ, respectively, while Φy is the truncated periodogram of the observed process y. In this section we propose a procedure for the identification of a latentvariable graphical model associated to an AR Gaussian process that exploits the approximation of an AR process through a reciprocal process in the sense explained in Section III.</p><p>Recalling that a latent-variable graphical model of a reciprocal process is characterized by <ref type="bibr" target="#b23">(23)</ref>, the system identification problem can be stated as follows.</p><p>Problem 3: Consider an m-dimensional AR process y and let R0 , . . . , Rn be the estimates of the first n + 1 covariance lags of y computed as in <ref type="bibr" target="#b9">(10)</ref>. Set Σ 0 := R0 , . . . ,</p><formula xml:id="formula_55">Σ n := Rn . Compute the blocks Σ n+1 , . . . , Σ N 2 of the block-circulant covariance ma- trix Σ y = circ{Σ 0 , Σ 1 , . . . , Σ N 2 -1 , Σ N 2 , Σ N 2 -1 , . . . , Σ 1 } such that Σ -1 y = S -L,</formula><p>where S &gt; 0 and L ≥ 0 are as in <ref type="bibr" target="#b24">(24)</ref> with S 0 , . . . , S n having the smallest possible common support Ω, as in <ref type="bibr" target="#b25">(25)</ref>, and the rank of L is as small as possible.</p><p>We stress the fact that only samples of the observed processes are available. Clearly, the matrix Σ y solving Problem 3 is the covariance of the reciprocal process y approximating the observed process y. Since we are going to identify a model for a reciprocal process, we can exploit the maximum entropy dual problem <ref type="bibr" target="#b6">(7)</ref> recalled in Section III. It is worth noting that the support Ω is not known in advance, thus it has to be estimated from the data. In order to do that, inspired by <ref type="bibr" target="#b3">[4]</ref>, we consider the following regularizer</p><formula xml:id="formula_56">h ∞ (S) = ∑ k&gt;h max |(S 0 ) hk |, 2 max j=1,...,n |(S j ) hk |, 2 max j=1,...,n |(S j ) kh | .</formula><p>The latter is a generalization of the ∞ -norm used to induce group-sparsity on vectors, and it is used to enforce on S the group sparsity in <ref type="bibr" target="#b25">(25)</ref>. The trace (as a tractable proxy of the nuclear norm) is used instead for inducing low-rankness in L. Therefore, the paradigm for the estimation of the sparse plus low-rank decomposition of the concentration matrix Σ -1 y now directly follows from <ref type="bibr" target="#b6">(7)</ref> by setting X = S -L, with L ≥ 0, and by adding the regularizers just introduced:</p><formula xml:id="formula_57">argmin S,L∈B -log det(S -L) + Σy , S -L C + λ S h ∞ (S) + λ L tr(L) subject to S -L &gt; 0, L ≥ 0 (32)</formula><p>where λ L , λ S &gt; 0 are the two regularization parameters and Σy = circ{ R0 , R1 , . . . , Rn , 0, . . . , 0, R n , . . . , R 1 } is the symmetric, banded block-circulant matrix of bandwidth n, containing the covariance lags estimated from the observations. As a further motivation, observe that Problem (32) is precisely the reciprocal counterpart of Problem <ref type="bibr" target="#b31">(31)</ref> considered in <ref type="bibr" target="#b8">[9]</ref>. By replacing S with X := S -L, it becomes argmin X,L∈B -log det(X) + tr( Σy X) + λ S h ∞ (X + L) + λ L tr(L) subject to X &gt; 0, L ≥ 0.</p><p>(33) We address the previous constrained optimization problem using the Lagrange multipliers theory. In doing that we add a new dummy variable Y</p><formula xml:id="formula_58">argmin X∈C Y,L∈B -log det(X) + tr( Σy X) + λ S h ∞ (Y) + λ L tr(L) subject to X &gt; 0, L ≥ 0 Y = X + L. (<label>34</label></formula><formula xml:id="formula_59">)</formula><p>The Lagrangian function for this problem is</p><formula xml:id="formula_60">L(X, Y, L, V, Z) = -log det(X) + Σy , X C + λ S h ∞ (Y) + λ L tr(L) -V, L C + Z, X + L -Y C<label>(35)</label></formula><p>where, V ∈ B, because L ∈ B, and V ≥ 0, while Z ∈ C. After simple computations we have</p><formula xml:id="formula_61">L(X, Y, L, V, Z) = -log det(X) + Σy + Z, X C + λ L I mN -V + Z, L C + λ S h ∞ (Y) -Z, Y C .</formula><p>The dual objective function is the infimum over X, Y and L of the Lagrangian. The unique term on L that depends on Y is λ S h ∞ (Y) -Z, Y C . The latter is bounded below if and only if diag(Z j ) = 0, j = 0, . . . , n,</p><formula xml:id="formula_62">2|(Z 0 ) kh | + n ∑ j=1 |(Z j ) kh | + |(Z j ) hk | ≤ λ S N , k &gt; h,<label>(36)</label></formula><p>in which case the infimum is zero. Accordingly,</p><formula xml:id="formula_64">inf Y L =          -log det(X) + Σy + Z, X C + λ L I mN -V + Z, L C if (36), (37) hold, -∞ otherwise.</formula><p>The only term that depends on L is λ L I mN -V + Z, L C . Recalling that L, V ∈ B, by using the linearity of the projection operator P B , we have that</p><formula xml:id="formula_65">λ L I mN -V + Z, L C = λ L I mN -V + P B (Z), L C<label>(38)</label></formula><p>which is linear in L, and therefore it is bounded below if and only if</p><formula xml:id="formula_66">λ L I mN -V + P B (Z) = 0. (<label>39</label></formula><formula xml:id="formula_67">)</formula><p>In this case, the minimum of (38) is zero. Accordingly, inf <ref type="formula" target="#formula_62">36</ref>), ( <ref type="formula" target="#formula_63">37</ref>), (39) hold, -∞ otherwise.</p><formula xml:id="formula_68">Y,L L =          -log det(X) + Σy + Z, X C if (</formula><p>If <ref type="bibr" target="#b36">(36)</ref>, ( <ref type="formula" target="#formula_63">37</ref>), (39) hold, it remains to minimize the strictly convex function</p><formula xml:id="formula_69">L(X) := inf Y,L L = -log det(X) + Σy + Z, X C</formula><p>over the cone of the symmetric, positive definite, banded block-circulant matrices. Observe that, for any Z ∈ C, any Σy ∈ B, and for any sequence X k &gt; 0 converging to a singular matrix, lim</p><formula xml:id="formula_70">k→∞ L(X k ) = ∞.</formula><p>Accordingly, we can assume that the solution lies in the interior of the cone so that a necessary and sufficient condition for X o to be a minimum point for L is that its first Gateaux derivative computed at X = X o is equal to zero in every direction δ X, namely </p><formula xml:id="formula_71">δ L(X o ; δ X) = tr -X -1 o + Σy + Z δ X = 0, ∀ δ X ∈ C.<label>(</label></formula><formula xml:id="formula_72">= ( Σy + Z) -1 . Hence, inf Y,L,X L =          log det( Σy + Z) + mN, if<label>(36)</label></formula><p>, ( <ref type="formula" target="#formula_63">37</ref>), (39), (41) hold, -∞ otherwise.</p><p>Therefore, the dual problem of (32) is argmin V∈B, Z∈C -log det( Σy + Z) -mN subject to V ≥ 0, (36), ( <ref type="formula" target="#formula_63">37</ref>), (39), (41).</p><p>(42)</p><p>Notice that we can remove the variable V. Indeed, recalling that V ≥ 0, the constraint (39) becomes λ L I mN + P B (Z) = V ≥ 0. Accordingly, the dual problem takes the form argmin Z∈C -log det( Σy + Z) -mN subject to (36), ( <ref type="formula" target="#formula_63">37</ref>), (41)</p><formula xml:id="formula_73">λ L I mN + P B (Z) ≥ 0. (<label>43</label></formula><formula xml:id="formula_74">)</formula><p>Proposition 3: Under the assumption that Σy ∈ B and Σy &gt; 0, Problem (43) admits a unique solution.</p><p>Proof: Define f (Z) := log det( Σy + Z). Let <ref type="formula" target="#formula_63">37</ref>), (41) and λ L I mN + P B (Z) ≥ 0 hold} be the set of constraints of Problem (43). First of all, notice that constraints <ref type="bibr" target="#b36">(36)</ref> and <ref type="bibr" target="#b37">(37)</ref> ensure that Q is a bounded subset of C. Indeed, the entries of any Z ∈ Q are bounded by λ S /N so that Z C &lt; ∞ for any Z ∈ Q. Let now (Z (k) ) k∈N be a generic sequence of elements of Q converging to some Z ∈ C, such that Σy + Z ≥ 0 is singular. Then</p><formula xml:id="formula_75">Q := {Z ∈ C | (36), (</formula><formula xml:id="formula_76">lim k→∞ -log det( Σy + Z (k) ) = +∞,</formula><p>and therefore Z (k) is not an infimizing sequence. Hence, we can restrict the research of the minimum to the closed subset of Q defined by</p><formula xml:id="formula_77">Q := {Z ∈ C | Σy + Z ≥ εI mN ,<label>(36)</label></formula><p>, <ref type="bibr" target="#b37">(37)</ref> and λ L I mN + P B (Z) ≥ 0 hold} with ε &gt; 0 small enough. By what we have shown till now, the function f is continuous on the compact set Q and therefore it admits at least one minimum point. Since f is strictly convex, the minimum is also unique. Proposition 4: Under the assumption that Σy ∈ B and Σy &gt; 0, Problem (33) admits a solution (X o , L o ) and X o is unique.</p><p>Proof: Notice that Problem ( <ref type="formula">33</ref>) is a strictly feasible convex optimization problem (for instance, pick X = I mN and L = 0). Accordingly, Slater's condition holds, hence strong duality holds between <ref type="bibr" target="#b33">(33)</ref> and its dual. The strong duality between problems ( <ref type="formula">33</ref>) and ( <ref type="formula" target="#formula_73">43</ref>) and the existence of a unique optimum Z o for the dual problem (43), imply that there exists a unique X o ∈ B so that X o = Σy + Z o -1 which solves the primal problem <ref type="bibr" target="#b33">(33)</ref>. It remains to show that there exists an L o ∈ B that solves the optimization problem</p><formula xml:id="formula_78">argmin L∈B λ S h ∞ (X o + L) + λ L tr(L) subject to L ≥ 0. (<label>44</label></formula><formula xml:id="formula_79">)</formula><p>Notice that, the objective function in (44) is continuous. Since L = 0 is a feasible point, the problem is equivalent to find L ∈ B that minimizes λ S h ∞ (X o + L) + λ L tr(L) over the set</p><formula xml:id="formula_80">K := L ∈ B L ≥ 0, λ S h ∞ (X o + L) + λ L tr(L) ≤ λ S h ∞ (X o ) .</formula><p>It is easy to see that K is a closed and bounded (and thus compact) subset of B. Hence, by Weierstrass' Theorem, Problem (44) admits a solution L o . At this point we can conclude that the primal problem (33) admits a solution (X o , L o ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Interpretations</head><p>In the remaining of this section we will show how Problem (32) can be interpreted either as a regularized maximumlikelihood problem or as a dual of a maximum entropy problem.</p><p>Maximum likelihood interpretation: The reciprocal approximation of AR processes illustrated in Section III allows to interpret Problem (32) as a regularized (conditional) maximum likelihood problem. Indeed, in the following we will show that the fitting function in <ref type="bibr" target="#b32">(32)</ref>, i.e.</p><p>log det(S -L) + tr Σy (S -L) ,</p><p>is the approximation of the (conditional) negative loglikelihood of the AR process (8) that should be understood in the sense explained in Section III. Following <ref type="bibr" target="#b35">[35]</ref>, consider the observed AR process y whose spectrum is denoted by Φ y , and suppose that T observations y(1), . . . , y(T ) of the process are available. The conditional likelihood of the process y is defined as the likelihood function associated to the conditional distribution of y(n+1), y(n+2), . . . , y(n+T ) given y(1), . . . , y(n). Let</p><formula xml:id="formula_82">T n := Toepl{ R0 , R1 , • • • , Rn }</formula><p>be the block-Toeplitz matrix having in the first rows the estimates of the first n + 1 covariance lags of the process R0 , R1 , . . . , Rn computed as in <ref type="bibr" target="#b9">(10)</ref>. For T large enough, the conditional negative log-likelihood function of the AR process can be well approximated by</p><formula xml:id="formula_83">(B) := -(T -n) log det B 0 + T -n 2 tr(B T n B )</formula><p>where</p><formula xml:id="formula_84">B := [B 0 B 1 • • • B n ]</formula><p>is the (n + 1)m-dimensional vector containing the coefficients of the process. Applying Jensen's formula, it turns out that</p><formula xml:id="formula_85">log det B 0 = 1 2 log det Φ y (e iθ ),</formula><p>moreover, if Φy (e iθ ) is the truncated periodogram of the AR process in <ref type="bibr" target="#b12">(13)</ref>, it is easy to see that</p><formula xml:id="formula_86">Φy (e iθ ) e iθ k = R-k = R k .</formula><p>Accordingly, the approximated conditional negative loglikelihood can be rewritten as</p><formula xml:id="formula_87">(B) = T -n 2 log det Φ y (e iθ ) + tr Φy (e iθ ) Φ y (e iθ ) -1 .</formula><p>(46) A natural way to approximate (46) is to approximate the integral with a finite sum, i.e. to discretize the interval [-π, π]. This is precisely the frequency interpretation of the reciprocal approximation explained in Section III that consists in sampling the spectrum of the process to obtain the corresponding symbol (see Figure <ref type="figure" target="#fig_0">1</ref>). In fact, considering as sample frequency ∆θ = 2π/N, the Backward Euler approximation leads to the discrete approximation</p><formula xml:id="formula_88">(B) T -n 2 ∆θ 2π N-1 ∑ k=0 log det Φ y (e iθ k )+tr Φy (e iθ k )Φ y (e iθ k ) -1 .</formula><p>where θ k = k ∆θ -π. The conditional log-likelihood can now be rewritten straightforward in terms of symbols as</p><formula xml:id="formula_89">(B) T -n 2N N-1 ∑ k=0 log det Φ y (ζ k ) + tr N-1 ∑ k=0 Φy (ζ k )Φ y (ζ k ) -1 .</formula><p>Observe now that Φ y (ζ ) is precisely the symbol of the blockcirculant covariance matrix Σ y of the reciprocal process y approximating the process y and Φy (ζ ) is the symbol of the block-circulant matrix Σy in Problem <ref type="bibr" target="#b32">(32)</ref>. Accordingly, form Proposition 1, it follows that</p><formula xml:id="formula_90">(B) T -n 2N -log det Σ -1 y + tr Σy Σ -1 y .</formula><p>Since Σ -1 y = S -L, this is precisely (up to a scaling factor) equal to (45).</p><p>Maximum entropy interpretation: We will show that Problem <ref type="bibr" target="#b32">(32)</ref> can be interpreted a regularized version of the dual of a maximum entropy problem, see <ref type="bibr" target="#b36">[36]</ref> for a general overview of these problems. Consider the regularized solution (S o , L o ) of ( <ref type="formula">32</ref>) and let Ω be the support of S o , i.e. S o satisfies <ref type="bibr" target="#b25">(25)</ref>. Since L o ∈ B is so that L o ≥ 0 and rank L o = lN mN, there exists G = circ{G 0 , G 1 , . . . , G n , 0, . . . , 0} such that G k ∈ R m×l and L o = G G. Accordingly, we can consider a modified version of Problem <ref type="bibr" target="#b32">(32)</ref> where the regularizers are replaced by the corresponding hard-constraints</p><formula xml:id="formula_91">S ∈ V Ω and L ∈ V G , where V Ω := {S ∈ C : P Ω c (S) = 0} and V G := {G (I N ⊗ H)G : H ∈ R l×l , H = H } is such that V G ⊆ B. Thus, the resulting problem is argmin S,L∈B -log det(S -L) + Σy , S -L C subject to S -L &gt; 0, L ≥ 0, S ∈ V Ω , L ∈ V G .<label>(47)</label></formula><p>Proposition 5: The primal of Problem (47) is argmax</p><formula xml:id="formula_92">Σ y ∈C log det Σ y subject to P Ω P B (Σ y -Σy ) = 0, E * G(Σ y -Σy )G E ≥ 0,<label>(48)</label></formula><p>where</p><formula xml:id="formula_93">E * := 1 √ N [I l 0 • • • 0]. Proof: We derive the dual of Problem (48). Observing that E = F * 1 where 1 := 1 √ N [I l I l • • • I l ]</formula><p>, the Lagrangian of Problem (48) writes as</p><formula xml:id="formula_94">L(Σ y , W, H) = log det Σ y + P Ω∪B ( Σy -Σ y ), W C + 1 F G(Σ y -Σy )G F * 1, H C ,</formula><p>where W ∈ C, H ∈ R l×l is a positive semidefinite symmetric matrix, and P Ω∪B (S) = P Ω P B (S). The last term of the Lagrangian can be rewritten as</p><formula xml:id="formula_95">tr F(Σ y -Σy )F * FG F * 1H1 FGF * = tr F(Σ y -Σy )F * FG F * (I N ⊗ H) FGF * = tr (Σ y -Σy ) G (I N ⊗ H) G ,</formula><p>where we have exploited the fact that F(Σ y -Σy )F * and FGF * are block-diagonal matrices and the fact that</p><formula xml:id="formula_96">F * (I N ⊗ H)F = I N ⊗ H. Accordingly, L(Σ y , W, H) = log det Σ y + Σy -Σ y , P Ω∪B (W) C + Σ y -Σy , G (I N ⊗ H) G C = log det Σ y + Σy -Σ y , S -L C ,</formula><p>where S := P Ω∪B (W) belongs to V Ω and L := G (I N ⊗ H) G ≥ 0 belongs to V G ⊆ B, i.e. they satisfy all the constraints in (47). Similar arguments as the ones used to prove formula (40), allow us to assert that a necessary and sufficient condition for Σ o to be a minimum point for L is that its first Gateaux</p><formula xml:id="formula_97">derivative computed at Σ y = Σ o is equal to zero in every direction δ Σ, namely δ L(Σ o ; δ Σ) = tr Σ -1 o -S + L δ Σ = 0, ∀ δ Σ ∈ C.</formula><p>By assumption we have that S -L &gt; 0 thus, the substitution of the optimum Σ o = (S -L) -1 in the Lagrangian L leads precisely to the objective function in (47). Some observations on the two constraints of (48) are in order. The first constraint P Ω P B (Σ y -Σy ) = 0 fixes the entries corresponding to the indexes in Ω of the first n + 1 lags of the reciprocal process. Concerning the second constraint, let Ψ(ζ ) and Φ y (ζ ) be the symbols of G and Σ y , respectively. By Proposition 1, we have that</p><formula xml:id="formula_98">E * G Σ y G E = 1 N N-1 ∑ k=0 Ψ(ζ k ) Φ y (ζ k ) Ψ(ζ k ) * ,<label>(49)</label></formula><p>which is the covariance of the output of the m×l filter</p><formula xml:id="formula_99">Ψ(ζ ) = ∑ n k=0 G k ζ -k</formula><p>fed with the reciprocal process y. Accordingly, the second constraint in (48) states that the covariance matrix of the process at the output of the filter is lower-bounded by E * G Σy G E. We conclude that Problem (48) can be seen as the reciprocal counterpart of the maximum entropy problem <ref type="bibr" target="#b8">[9]</ref>, argmax</p><formula xml:id="formula_100">Φ y ∈S m log det Φ y subject to e iθ k Φ y -Rk pq = 0, k=0<label>,1,...,n (p,q)</label></formula><formula xml:id="formula_101">∈Ω Ψ (Φ y -Φy ) Ψ * ≥ 0,<label>(50)</label></formula><p>where Ψ(e iθ ) = ∑ n k=0 G k e -iθ k . Indeed, the second constraint in (50) can be approximated with the backward Euler approximation with sample frequency ∆θ = 2π/N obtaining (49).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ALTERNATING DIRECTION METHOD OF MULTIPLIERS</head><p>The solution of Problem (43) requires the joint enforcement of the constraints (36), <ref type="bibr" target="#b37">(37)</ref> and λ L I mN + P B (Z) ≥ 0, which may be a difficult task. In this section we will use the alternating direction methods of multipliers (ADMM) <ref type="bibr" target="#b37">[37]</ref> to solve Problem (43) by showing that the constraints can be separated and each one can be enforced in an alternating way. First of all observe that, by defining the variable P := λ L I mN + P B (Z), Problem (43) rewrites as argmin Z,P∈C -log det( Σy + Z) -mN subject to <ref type="bibr" target="#b36">(36)</ref>, <ref type="bibr" target="#b37">(37)</ref> P = λ L I mN + P B (Z)</p><formula xml:id="formula_102">P ≥ 0. (<label>51</label></formula><formula xml:id="formula_103">)</formula><p>where we have omitted the domain of the objective function Σy +Z &gt; 0 since it will be checked in the stepsize-choice stage of the algorithm. The augmented Lagrangian for the problem is</p><formula xml:id="formula_104">L ρ (Z, P, M) = -log det Σy + Z -M, P -λ L I mN -P B (Z) C + ρ 2 P -λ L I mN -P B (Z) 2 C</formula><p>where ρ &gt; 0 is the penalty term and M ∈ C is the Lagrange multiplier associated to the equality constraint on P. Accordingly, the ADMM updates are the following:</p><p>1) The Z-minimization step</p><formula xml:id="formula_105">Z k+1 = argmin Z∈C L ρ (Z, P k , M k ) subject to Z ∈ Z.<label>(52)</label></formula><p>2) The P-minimization step</p><formula xml:id="formula_106">P k+1 = argmin P∈C L ρ (Z k+1 , P, M k ) subject to P ≥ 0.<label>(53)</label></formula><p>3) Dual variable update</p><formula xml:id="formula_107">M k+1 = M k -ρ P k+1 -λ L I mN -P B (Z k+1 ) . (<label>54</label></formula><formula xml:id="formula_108">)</formula><p>where Z := {Z ∈ C : (36), ( <ref type="formula" target="#formula_63">37</ref>)} and we have considered a constant value of ρ in order simplify the notation. We will discuss later how to update ρ to get a faster convergence. Updates 1) and 2) are not in an implementable format. The Z-update step (52) is equivalent to the minimization of</p><formula xml:id="formula_109">I(Z) := -log det( Σy + Z) + ρ 2 P B (Z) 2 C + M k -ρ (P k -λ L I mN ), P B (Z) C</formula><p>, over the set Z, which has no closed-form solution, as noticed in <ref type="bibr" target="#b3">[4]</ref> where the solution is approximated by a projective-gradient step. Following the same lines, the new Z-update step starts from a known feasible point Z 0 = Z and continue the iterations following the update rule</p><formula xml:id="formula_110">Z k+1 = P Z Z k -t k ∇I(Z k )<label>(55)</label></formula><p>where</p><formula xml:id="formula_111">∇I(Z k ) = -( Σy +Z k ) -1 +P B (M k )+ρ P B Z k -P k + λ L I mN</formula><p>is the gradient of the cost-function I computed in Z k , t k is the stepsize founded by the Armijo condition, and P Z is the projection operator onto the constraints space Z.</p><p>The optimization problem involved in the P-update step (53) is equivalent to minimize the functional</p><formula xml:id="formula_112">J(P) := ρ 2 P 2 C -P, M k + ρ λ L I mN + P B (Z k+1 )</formula><p>C over all P ≥ 0. Since J is a quadratic functional of P the minimization of J over the whole vector space C admits the closed form solution</p><formula xml:id="formula_113">P o = 1 ρ M k + λ L I mN + P B (Z k+1 )</formula><p>but it is not a positive semidefinite matrix in general. Accordingly, in order to find our solution, we have to find the positive semidefinite block-circulant matrix that better approximates P o in the norm induced by the scalar product on C (i.e. the Frobenius norm on C). Recall the following well-known result. Lemma 1: Let A ∈ C n×n be an Hermitian matrix whose eigenvalue decomposition is given by A = U * ΛU, with</p><formula xml:id="formula_114">U * U = UU * = I and Λ = diag{λ 1 , . . . , λ n }.</formula><p>Then, the positive semidefinite matrix that better approximates A in the Frobenius norm is the projection of A onto the cone of positive semidefinite matrices P + , namely</p><formula xml:id="formula_115">P P + (A) := argmin X≥0 X -A F = U * diag{γ o 1 , . . . , γ o n }U,</formula><p>where</p><formula xml:id="formula_116">γ o i = λ ii , if λ ii ≥ 0, 0, if λ ii &lt; 0.</formula><p>The following proposition ensures that the projection of a symmetric, block-circulant matrix onto the cone of positive semi-definite matrices is still block-circulant. Proposition 6: Let C be a symmetric, block-circulant matrix</p><formula xml:id="formula_117">C = F * diag C(ζ 0 ), C(ζ 1 ), . . . , C(ζ N-1 ) F,</formula><formula xml:id="formula_118">and let C(ζ k ) = V k Λ k V * k with V * k V k = V k V * k = I m and Λ k = diag{λ k1 , .</formula><p>. . , λ km }, being the eigen-decomposition of the (Hermitian) block C(ζ k ), for k = 0, . . . , N -1. Then the eigen-decomposition of C can be written as</p><formula xml:id="formula_119">C = W * ΛW, W = V * F,</formula><p>where V = diag{V 0 , . . . ,V N-1 } and Λ = diag{Λ 0 , . . . , Λ N-1 }. Then,</p><formula xml:id="formula_120">P C + (C) := argmin X≥0 X -C C = W * diag{Γ 0 , . . . , Γ N-1 } W</formula><p>where Γ k = diag{γ k1 , . . . , γ km } and</p><formula xml:id="formula_121">γ ki = λ ki , if λ ki ≥ 0, 0, if λ ki &lt; 0.</formula><p>for k = 0, . . . , N -1.</p><p>Proof: The result follows from applying Lemma 1 with U = W. Of course,</p><formula xml:id="formula_122">P C + (C) = F * diag{V 0 Γ 0 V * 0 , . . . ,V N-1 Γ N-1 V * N-1</formula><p>}F is a block-circulant matrix because it is block-diagonalized by the Fourier-block matrix.</p><p>According to Proposition 6, the positive semidefinite blockcirculant matrix that better approximates P o in the C norm is the projection of P o onto the cone of the symmetric, positive semidefinite, block-circulant matrices C + , that is</p><formula xml:id="formula_123">P k+1 = P C + (P o ) = P C + 1 ρ M k + λ L I mN + P B (Z k+1 ) . (<label>56</label></formula><formula xml:id="formula_124">)</formula><p>We conclude that the ADMM algorithm for the estimation of the sparse and the low-rank component of the inverse of the covariance matrix of the reciprocal process consists in the following updates</p><formula xml:id="formula_125">Z k+1 = P Z Z k -t k ∇I(Z k ) , P k+1 = P C + 1 ρ k M k + λ L I mN + P B (Z k+1 ) , M k+1 = M k -ρ k P k+1 -λ L I mN -P B (Z k+1 ) .<label>(57)</label></formula><p>A typical update for ρ is ρ k+1 = αρ k , with α &gt; 1 being a certain growth coefficient that needs to be properly tuned.</p><p>Notice that the matrices involved in (57) are all symmetric and block-circulant. Accordingly, as explained in Section III, the introduction of the reciprocal approximation allows to obtain a robust identification procedure even in the case when n is large. Indeed, relations ( <ref type="formula" target="#formula_24">14</ref>) and ( <ref type="formula" target="#formula_25">15</ref>), allow to compute inverse matrices and eigenvalues in a robust way. Moreover, it is worth noting from <ref type="bibr" target="#b14">(15)</ref>, that the dimensions of the matrices whose eigenvalues must be computed in the optimization procedure, depend only on m, hence the identification algorithm we are proposing scales with respect to n gaining robustness in the results even if the order of the AR process is large. Following <ref type="bibr" target="#b37">[37]</ref>, the basic stopping criterium for the algorithm is based on the primal and dual residuals of the optimality conditions that respectively measure the satisfaction of the inequality constraint P ≥ 0 and the distance between two successive iterates of the variable P. More precisely, the primal residual at iteration k + 1 is defined as</p><formula xml:id="formula_126">r k+1 := P k+1 -λ L I mN -P B (Z k+1 ),</formula><p>while the dual residual turns out to be</p><formula xml:id="formula_127">s k+1 := P B c (M k ) -ρ k P k+1 -P B (P k ) .</formula><p>It is reasonable that the primal and dual residual must be small, that is r k C ≤ ε p and s k C ≤ ε d , where ε p &gt; 0 and ε d &gt; 0 are feasibility tolerances for the primal and dual feasibility conditions. The latter are defined as</p><formula xml:id="formula_128">ε p := mN ε abs + ε rel max λ L √ mN, Z k C , P k C , ε d := mN ε abs + ε rel M k C .</formula><p>Here, ε abs and ε rel are predefined absolute and relative tolerances for the problem. Accordingly, the algorithm converges if all the conditions</p><formula xml:id="formula_129">r k C ≤ ε p , s k C ≤ ε d , ρ k = ρ max<label>(58)</label></formula><p>hold true, where ρ max &gt; 0 is the maximum value allowed for the penalty parameter ρ k , selected by the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. NUMERICAL EXAMPLES</head><p>In this section we compare the performances of our method to which we will refer to as approximated algorithm with the one proposed in <ref type="bibr" target="#b9">[10]</ref> for the solution of Problem <ref type="bibr" target="#b31">(31)</ref>, which will be referred to as exact algorithm. In particular we will show how the two algorithms behave considering both the case in which the observed process has low dimension and the case in which we have an high dimensional observed process.</p><p>Low-dimensional case: Synthetic data ere generated from the AR latent-variable model of order n = 8,</p><formula xml:id="formula_130">y(t) = n ∑ k=1 A k y(t -k) + η(t),<label>(59)</label></formula><p>with m = 20 observed variables and l = 1 latent variables. Here, η(t) is white Gaussian noise with variance E[η(t) η(t)] = 21.14 and T = 1000 samples have been used to compute the estimated covariance lags Rk , k = 0, . . . , n. Figure <ref type="figure" target="#fig_2">4</ref> (center) reports the sparsity pattern of the underlying model, randomly generated so that the non-zero elements represents the 5% of the total elements. For the approxi- The values of the regularization parameters that identify the grids have been selected so that the estimated models capture a range of features as complete as possible: from a very sparse model with a relatively high rank, to a quasi-full model with the lowest rank possible. Figure <ref type="figure">5</ref> shows the supports and the ranks estimated by the approximated algorithm corresponding to the different values of λ S and λ L . For both methods the value of α that gives the better performances, i.e. that guarantees the minimum gap between ε p / ε d and the primal/dual residual at the final iteration, respectively, has been selected. Accordingly, we have chosen α = 1.007 for the approximated algorithm while α = 1.002 has been chosen for the exact algorithm. Let s(h) and ε(h) denote the vectors containing the dual residual and its feasibility tolerance for the model h = 1, . . . , 25 respectively. Figure <ref type="figure">6</ref> displays the (logarithm of the) averages</p><formula xml:id="formula_131">Ωa , la = 1 Ω, l = 1 Ωe , le = 1</formula><formula xml:id="formula_132">µ s = 1 25 25 ∑ h=1 s(h), µ ε = 1 25 25 ∑ h=1 ε(h),</formula><p>obtained by our method with α = 1.007 (left) and by the exact method for α = 1.002 (right). For both algorithms, the primal residual always satisfies the condition in the stopping criterium (58) therefore there is no need to displaying it. We observe that the exact algorithm does not converge for any value of α we have considered. Indeed, the plot in Figure <ref type="figure">6</ref>  µ s stays significantly above the threshold µ ε . The optimal values of the regularization parameters have then been selected by cross-validation, using a test data set of 500 samples. Figure <ref type="figure" target="#fig_2">4</ref> compares the optimal sparsity pattern provided by the approximated algorithm Ωa (left), corresponding to λ S = 95 and λ L = 5.4, and the optimal sparsity pattern estimated by the exact algorithm Ωe (right) corresponding to γ S = 2.6 and γ L = 2.95, together with the estimates of the number of latent variables, la and le , respectively. Notice that both algorithms estimates the correct number of latent variables but only the approximated one produces an estimate of the sparsity pattern comparable with the true one. Let Φe and Φa be the estimates of the spectra of the true observed process Φ y obtained by the solutions of problems <ref type="bibr" target="#b31">(31)</ref> and <ref type="bibr" target="#b32">(32)</ref>, respectively. According to High-dimensional case: We consider now an AR latentvariable model in have m variables n = 24 and the variance of the noise is E[η(t) η(t)] = 87.1. The number of samples used to estimate the covariance lags R k is T = 15000. The number of conditionally dependent pairs in the true model is 158 so that the cardinality of the true support is |Ω| = 396. Table <ref type="table">8</ref> compares the performances of our approximated algorithm with the exact algorithm proposed in <ref type="bibr" target="#b9">[10]</ref> for different values of the sparsity regularization parameters λ S and γ S , that have been chosen in order to have approximatively the same variety on the results. The notation |Ω -Ω| indicates the error on the sparsity pattern in terms of number of misclassified entries. Both algorithms estimate the correct number of latent  variables, but the approximated algorithm gives a result very close to the true one (highlighted in red in Figure <ref type="figure" target="#fig_5">8</ref>) while for the exact algorithm, even if the cardinality of the true support has been correctly estimated, the error in the reconstruction of the sparsity pattern is quite high. This is due to the fact that the higher is the order of the process n, the less accurate is the computation of eigenvalues and inverse matrices by the exact algorithm. Figure <ref type="figure" target="#fig_5">8</ref> shows that such an issue is avoided in the approximated version, thanks to the availability of closed-form formulas for the computation of the eigenvalues of block-circulant matrices. Moreover, we see that the run time of the exact algorithm is about twice the run time of the approximated one. This confirm the fact that the approximated algorithm scales with the order n of the AR process we are approximating as suggested in Section V. This kind of scenario agrees with what we have discussed in Section V: high-order AR process are quite challenging instances for the exact procedure proposed in <ref type="bibr" target="#b9">[10]</ref>; in this cases, the reciprocal approximation leads to remarkable benefits in the performances of the identification procedure.</p><p>VIII. CONCLUSIONS In this paper an identification paradigm for latent-variable graphical models associated to reciprocal processes has been presented. It has been shown that the proposed paradigm is theoretically strongly sustained, being an approximation of the corresponding problem for AR processes both in a maximum likelihood and in a maximum entropy sense. The performances of the proposed method have been compared with the approach proposed in <ref type="bibr" target="#b9">[10]</ref> where no approximation is introduced. The numerical examples have shown that for highorder AR processes reciprocal approximation gives substantial improvements in terms of robustness and scalability of the identification procedure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Spectrum Φ(e iθ ) and its sampled version Φ(ζ ) with N = 12 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Example of a latent-variable graphical model: x 1 , x 2 are the latent-variables and y 1 , y 2 , . . . , y 7 are the manifest variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Sparsity pattern estimated by the approximated algorithm with α = 1.007, λ S = 95, λ L = 5.4 (left), true sparsity pattern (center), sparsity pattern estimated by the exact algorithm with α = 1.002, γ S = 2.6, γ L = 2.95 (right). The red squares indicate the conditional dependent pairs while the white squares indicates the conditional independent pairs. la , l and le denote the number of latent variables. mated algorithm we have considered N = 30 samples of the spectrum. In both the ADMM implementations we have set ε abs = 10 -5 and ε rel = 10 -4 while ρ max = 10 4 . In order to tune the update of the penalty term ρ in the ADMM, we have ran both the algorithms for different values of the growth coefficient α ∈ [1.001, 1.1]. More precisely, for each value of α, a 5 × 5 grid of candidate estimated models has been produced, corresponding to five linearly spaced values of the regularization parameters λ S ∈ [60, 130] and λ L ∈ [3, 7.8] for the approximated algorithm, and five linearly spaced values of γ S ∈ [1.42, 2.6] and γ L ∈ [2.425, 2.95] for the exact algorithm.The values of the regularization parameters that identify the grids have been selected so that the estimated models capture a range of features as complete as possible: from a very sparse model with a relatively high rank, to a quasi-full model with the lowest rank possible. Figure5shows the supports and the ranks estimated by the approximated algorithm corresponding to the different values of λ S and λ L . For both methods the value of α that gives the better performances, i.e. that guarantees the minimum gap between ε p / ε d and the primal/dual residual at the final iteration, respectively, has been selected. Accordingly, we have chosen α = 1.007 for the approximated algorithm while α = 1.002 has been chosen for the exact algorithm. Let s(h) and ε(h) denote the vectors containing the dual residual and its feasibility tolerance for the model h = 1, . . . , 25 respectively. Figure6displays the (logarithm of the) averages</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>||||Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Fig. 5: Supports and ranks estimated by the approximated algorithm for λ S ∈ [60, 130] and λ L ∈ [3, 7.8]. The growth coefficient is set α = 1.007.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 , 2 F Φ y 2 F, 2 F Φ y 2 F,Fig. 7 :</head><label>222227</label><figDesc>Fig. 7: Relative errors in the estimated spectra: approximated algorithm (left), exact algorithm (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Summary of the performances of the two algorithm for λ S = 100, 146.25, 350 and γ S = 0.7, 0.826, 1.7. The values of the low-rank regularization parameters are λ L = 8.6875 for the approximated algorithm (left) and γ L = 2.3 for the exact algorithm (right). These results have been obtained on a 2014 1.4GHz MacBook Air.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Graphical Models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lauritzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Oxford university press</publisher>
			<pubPlace>Oxford, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graphical interaction models for multivariate time series</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dahlhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Metrika</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="172" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Arma identification of graphical models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Avventi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Lindquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wahlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1167" to="1178" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Topology selection in graphical models of autoregressive processes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Songsiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2671" to="2705" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Maximum-likelihood estimation of autoregressive models with conditional independence constraints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Songsiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2009-04">April 2009</date>
			<biblScope unit="page" from="1701" to="1704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Conditional independence graphs for multivariate autoregressive models by convex optimization: Efficient algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dumitrescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giurcaneanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maximum entropy expectation-maximization algorithm for fitting latent-variable graphical models to multivariate time series</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dumitrescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giurcaneanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Latent variable graphical model selection via convex optimization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1935" to="1967" />
			<date type="published" when="2012-08">08 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AR identification of latent-variable graphical models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zorzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sepulchre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="2327" to="2340" />
			<date type="published" when="2016-09">Sept 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse plus lowrank autoregressive identification in neuroimaging time series</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ligeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zorzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sepulchre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 54th IEEE Conference on Decision and Control (CDC)</title>
		<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
			<biblScope unit="page" from="3965" to="3970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A maximum entropy solution of the covariance extension problem for reciprocal processes</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Picci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1999" to="2012" />
			<date type="published" when="2011-09">Sept 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Characterization of stationary discrete-time gaussian reciprocal processes over a finite interval</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferrante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="334" to="355" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Regular and reciprocal multivariate stationary gaussian reciprocal processes over z are necessarily markov</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Syst. Est. Control</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="134" to="154" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling and estimation of discrete-time gaussian reciprocal processes</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frezza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Krener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1013" to="1023" />
			<date type="published" when="1990-09">Sep 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multidimensional rational covariance extension with applications to spectral estimation and image compression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ringh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lindquist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1950" to="1982" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The circulant rational covariance extension problem: The complete solution</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Lindquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Picci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="2848" to="2861" />
			<date type="published" when="2013-11">Nov 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reciprocal realization and modeling of textured images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chiuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Picci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th IEEE Conference on Decision and Control</title>
		<meeting>the 44th IEEE Conference on Decision and Control</meeting>
		<imprint>
			<date type="published" when="2005-12">Dec 2005</date>
			<biblScope unit="page" from="6059" to="6064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An efficient algorithm for maximum entropy extension of block-circulant covariance matrices</title>
		<author>
			<persName><forename type="first">F</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Picci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">439</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2309" to="2329" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A fast solver for the circulant rational covariance extension problem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ringh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Control Conference (ECC)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><surname>European</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="727" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On the identification of sparse plus low-rank graphical models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Alpago</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<pubPlace>Padova, Italy</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Padova, Dept. of Information Engineering</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A maximum entropy enhancement for a family of high-resolution spectral estimators</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zorzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="318" to="329" />
			<date type="published" when="2012-02">Feb 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the estimation of structured covariance matrices</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zorzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferrante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2145" to="2151" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Maximum entropy spectral analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Burg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">interpretation of the dual problem of the THREE-like approaches</title>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="87" to="92" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A new family of high-resolution multivariate spectral estimators</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zorzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="892" to="904" />
			<date type="published" when="2014-04">April 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multidimensional rational covariance extension with applications to spectral estimation and image compression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ringh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lindquist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1950" to="1982" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kullback-leibler approximation of spectral density functions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lindquist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="2910" to="2917" />
			<date type="published" when="2003">12 2003</date>
		</imprint>
	</monogr>
	<note>Information Theory</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Time and spectral domain relative entropy: A new approach to multivariate spectral estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Masiero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="2561" to="2575" />
			<date type="published" when="2012-10">Oct 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relative entropy and the multivariable multidimensional moment problem</title>
		<author>
			<persName><forename type="first">T</forename><surname>Georgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1052" to="1066" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A new approach to spectral estimation: A tunable high-resolution spectral estimator</title>
		<author>
			<persName><forename type="first">C</forename><surname>Byrnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lindquist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3189" to="3205" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Identifiability and wellposedness of shaping filter parametrizations: A global analysis approach</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Byrnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Enqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lindquist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="59" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Toeplitz and circulant matrices: A review</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Communications and Information Theory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="155" to="239" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Identification of sparse reciprocal graphical models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Alpago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zorzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferrante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="659" to="664" />
			<date type="published" when="2018-10">Oct 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Linear Stochastic Systems: a Geometric Approach to Modeling, Estimation and Identification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Lindquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Picci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graphical models of autoregressive processes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Songsiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Convex optimization in signal processing and communications</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="89" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the geometry of maximum entropy problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferrante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="439" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
