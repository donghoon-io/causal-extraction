<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Advancing Semi-Supervised Task Oriented Dialog Systems by JSA Learning of Discrete Latent Variable Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yucheng</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Liu</surname></persName>
							<email>liuhong21@mails.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Huang</surname></persName>
							<email>huangyi@chinamobile.com</email>
						</author>
						<author>
							<persName><forename type="first">Junlan</forename><surname>Feng</surname></persName>
							<email>fengjunlan@chinamobile.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Speech Processing and Machine Intelligence</orgName>
								<orgName type="institution">Heriot-Watt University</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Lab Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">China Mobile Research Institute Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Advancing Semi-Supervised Task Oriented Dialog Systems by JSA Learning of Discrete Latent Variable Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Developing semi-supervised task-oriented dialog (TOD) systems by leveraging unlabeled dialog data has attracted increasing interests. For semi-supervised learning of latent state TOD models, variational learning is often used, but suffers from the annoying high-variance of the gradients propagated through discrete latent variables and the drawback of indirectly optimizing the target log-likelihood. Recently, an alternative algorithm, called joint stochastic approximation (JSA), has emerged for learning discrete latent variable models with impressive performances. In this paper, we propose to apply JSA to semi-supervised learning of the latent state TOD models, which is referred to as JSA-TOD. To our knowledge, JSA-TOD represents the first work in developing JSA based semi-supervised learning of discrete latent variable conditional models for such long sequential generation problems like in TOD systems. Extensive experiments show that JSA-TOD significantly outperforms its variational learning counterpart. Remarkably, semi-supervised JSA-TOD using 20% labels performs close to the full-supervised baseline on MultiWOZ2.1.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Task-oriented dialog (TOD) systems are designed to help users to achieve their goals through multiple turns of natural language interaction. The system needs to parse user utterances, track dialog states, query a task-related database (DB), decide actions and generate responses, and to do these iteratively across turns. The information flow in a task-oriented dialog is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Recent studies recast such information flow in a TOD system as conditional generation of tokens and base on pretrained language models (PLMs) such as GPT2 <ref type="bibr" target="#b27">(Radford et al., 2019)</ref> and T5 <ref type="bibr" target="#b28">(Raffel et al., 2020)</ref> as the model backbone. Fine-tuning a PLM over annotated dialog datasets such as Multi-WOZ <ref type="bibr" target="#b2">(Budzianowski et al., 2018)</ref> via supervised learning has shown promising results <ref type="bibr" target="#b9">(Hosseini-Asl et al., 2020;</ref><ref type="bibr" target="#b26">Peng et al., 2020;</ref><ref type="bibr" target="#b36">Yang et al., 2021;</ref><ref type="bibr" target="#b21">Liu et al., 2022)</ref>, but requires manually labeled dialog states and system acts (if used).</p><p>Notably, there are often easily-available unlabeled dialog data such as in customer-service logs and online forums. This has motivated the development of semi-supervised leaning (SSL) for TOD systems, which aims to leverage both labeled and unlabeled dialog data. A broad class of SSL methods builds a latent variable model (LVM) of observations and labels and blends unsupervised and supervised learning. Unsupervised learning with a LVM usually maximizes the marginal log-likelihood, which is often intractable to compute. Variational learning <ref type="bibr" target="#b14">(Kingma and Welling, 2014)</ref> introduces an auxiliary inference model and, instead, maximizes the evidence lower bound (ELBO) of the marginal log-likelihood. This approach of variational learning of LVMs has been studied for semi-supervised TOD systems such as in <ref type="bibr" target="#b11">Jin et al. (2018)</ref>; <ref type="bibr">Zhang et al. (2020b)</ref>; <ref type="bibr" target="#b20">Liu et al. (2021)</ref>; <ref type="bibr" target="#b18">Li et al. (2021)</ref>. Particularly, discrete latent variables are mostly used, since dialog states and system acts are often modeled as taking discrete values.</p><p>However, for variational learning of discrete latent variable models, the Monte-Carlo gradient estimator for the inference model parameter is known to have high-variance. Most previous studies use the Gumbel-Softmax trick <ref type="bibr" target="#b10">(Jang et al., 2017)</ref> or the Straight-Through trick <ref type="bibr" target="#b1">(Bengio et al., 2013)</ref> empirically, which in fact are biased estimators. Another drawback of variational learning is that it indirectly optimizes the lower bound of the target marginal log-likelihood, which leaves an uncontrolled gap between the target and the bound, depending on the expressiveness of the inference model.</p><p>Recently, an alternative algorithm, called joint stochastic approximation (JSA) <ref type="bibr" target="#b35">(Xu and Ou, 2016;</ref><ref type="bibr" target="#b25">Ou and Song, 2020)</ref>, has emerged for learning discrete latent variable models with impressive performances. JSA directly optimizes the marginal likelihood and completely avoids gradient propagation through discrete latent variables. In this paper, we propose to apply JSA to semi-supervised learning of the latent state TOD models, which is referred to as JSA-TOD. We develop recursive turnlevel Metropolis Independence Sampling (MIS) to enable the successful application of JSA, which needs posterior sampling of the latent states from the whole dialog session. To our knowledge, JSA-TOD represents the first work in developing JSA based semi-supervised learning of discrete latent variable conditional models for such long sequential generation problems like in TOD systems.</p><p>Extensive experiments show that JSA-TOD significantly outperforms its variational learning counterpart in semi-supervised learning. Remarkably, semi-supervised JSA-TOD using 20% labels performs close to the supervised-only baseline using 100% labels on MultiWOZ2.1. The code and data are released at <ref type="url" target="https://github.com/cycrab/JSA-">https://github.com/cycrab/JSA-</ref>TOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semi-Supervised TOD Systems</head><p>There are increasing interests in developing SSL methods for TOD systems, which aims to leverage both labeled and unlabeled data. Roughly speaking, there are two broad classes of SSL methods -the pretraining-and-finetuning approach and the latent variable modeling approach. With the development of pretrained language models such as GPT2 <ref type="bibr" target="#b27">(Radford et al., 2019)</ref> and T5 <ref type="bibr" target="#b28">(Raffel et al., 2020)</ref>, the pretraining-and-finetuning approach based on backbones of PLMs has shown excellent performance for TOD systems <ref type="bibr" target="#b9">(Hosseini-Asl et al., 2020;</ref><ref type="bibr" target="#b36">Yang et al., 2021;</ref><ref type="bibr" target="#b16">Lee, 2021)</ref>.</p><p>Discrete latent variable models have been used for semi-supervised TOD systems <ref type="bibr" target="#b11">(Jin et al., 2018;</ref><ref type="bibr">Zhang et al., 2020b)</ref> 1 , initially based on LSTM architectures. Recently, discrete latent variable models based on PLMs have been studied in <ref type="bibr" target="#b20">Liu et al. (2021)</ref>, combining the strengths of PLMs and LVMs for semi-supervised TOD systems. However, previous studies all resort to variational methods for learning latent variable models, which suffers from the high-variance of the gradients propagated through discrete latent variables and the drawback of indirectly optimizing the target log-likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Joint Stochastic Approximation for</head><p>Learning Latent Variable Models</p><p>Traditionally, variational methods minimize the "exclusive Kullback-Leibler (KL) divergence" KL[p||q] ≜ q log q p , where p and q are shorthands for the true posterior (of the latent variable given the observation) and its approximation (also called the inference model) respectively, in learning a latent variable model. Recently, the JSA algorithm has been developed <ref type="bibr" target="#b35">(Xu and Ou, 2016;</ref><ref type="bibr" target="#b25">Ou and Song, 2020)</ref>, which proposes to minimize the "inclusive KL" KL[p||q] ≜ p log p q , which has good statistical properties that makes it more appropriate for certain inference and learning problems, particularly for those using discrete latent variables. Similar idea has been studied in a concurrent and independent work <ref type="bibr" target="#b23">(Naesseth et al., 2020)</ref>. More investigations and extensions along this direction have been examined <ref type="bibr" target="#b12">(Kim et al., 2020</ref><ref type="bibr" target="#b13">(Kim et al., , 2022))</ref>.</p><p>In <ref type="bibr" target="#b32">Song and Ou (2020)</ref>, JSA is applied to semisupervised sequence-to-sequence learning, which consistently outperforms variational learning on two semantic parsing benchmark datasets. However, both generative model and inference model in <ref type="bibr" target="#b32">(Song and Ou, 2020)</ref> are LSTM-based and much simpler than the ones in this work; its model complexity is similar to a single turn in a TOD system. Another difference is that this paper represents the first application of JSA in its conditional sequential version, since the latent state TOD model is a conditional sequential generative model. 1 There are other previous studies of using discrete latent variable models in TOD systems, for example, <ref type="bibr" target="#b34">Wen et al. (2017)</ref>; <ref type="bibr" target="#b39">Zhao et al. (2019)</ref>; <ref type="bibr" target="#b0">Bao et al. (2020)</ref>. But most of them are mainly designed to improve response generation and diversity, instead of towards semi-supervised learning. See <ref type="bibr">Zhang et al. (2020b)</ref>; <ref type="bibr" target="#b20">Liu et al. (2021)</ref> for more review of related work in latent variable models for dialogs.</p><p>3 Preliminary: Joint Stochastic Approximation (JSA)</p><p>Stochastic approximation (SA) refers to an important family of iterative stochastic optimization algorithms for stochastically solving a root finding problem, which has the form of expectations being equal to zeros <ref type="bibr" target="#b30">(Robbins and Monro, 1951)</ref>. Within the SA framework, the joint stochastic approximation (JSA) algorithm is recently developed <ref type="bibr" target="#b35">(Xu and Ou, 2016;</ref><ref type="bibr" target="#b25">Ou and Song, 2020)</ref> for learning a broad class of latent variable models, particularly for learning models with discrete latent variables. Interestingly, JSA amounts to coupling an SA version of Expectation-Maximization (SAEM) <ref type="bibr" target="#b4">(Delyon et al., 1999;</ref><ref type="bibr" target="#b15">Kuhn and Lavielle, 2004</ref>) with an adaptive Markov Chain Monte Carlo (MCMC) procedure. Based on JSA, the annoying difficulty of propagating gradients through discrete latent variables and the drawback of indirectly optimizing the target log-likelihood can be gracefully addressed. Consider a latent variable generative model p θ (z, x) for observation x and latent variable z, with parameter θ. Like in variational methods, JSA also jointly trains the target model p θ (z, x) together with an auxiliary amortized inference model q ϕ (z|x). The difference is that JSA directly maximizes w.r.t. θ the marginal log-likelihood and simultaneously minimizes w.r.t. ϕ the inclusive KL divergence KL(p θ (z|x)||q ϕ (z|x)) between the posterior and the inference model, pooled over the training dataset:</p><formula xml:id="formula_0">           min θ 1 n n i=1 log p θ (x (i) ) min ϕ 1 n n i=1 KL[p θ (z (i) |x (i) )||q ϕ (z (i) |x (i) )]</formula><p>(1) where the training dataset consists of n independent and identically distributed (IID) data-points</p><formula xml:id="formula_1">x (1) , • • • , x (n) .</formula><p>The optimization problem Eq. ( <ref type="formula">1</ref>) can be solved by setting the gradients to zeros and applying the SA algorithm to find the root for the resulting simultaneous equations, which has the exact form of expectations equal to zeros:</p><formula xml:id="formula_2">1 n n i=1 E p θ (z (i) |x (i) ) ∇ θ log p θ (x (i) , z (i) ) = 0 1 n n i=1 E p θ (z (i) |x (i) ) ∇ ϕ log q ϕ (z (i) | x (i) ) = 0 (2)</formula><p>The resulting JSA algorithm, as summarized in</p><formula xml:id="formula_3">Algorithm 1 The JSA algorithm repeat Monte Carlo sampling: Draw κ over 1, • • • , n, pick the data-point x (κ)</formula><p>along with the cached z(κ) , and use MIS to draw z (κ) ; Parameter updating: Update θ by ascending: ∇ θ log p θ (z (κ) , x (κ) ); Update ϕ by ascending: ∇ ϕ log q ϕ (z (κ) |x (κ) ); until convergence Algorithm 1, iterates Monte Carlo sampling and parameter updating. In each iteration, we draw a training observation x (κ) and then sample z (κ)  through Metropolis Independence Sampling (MIS), with p θ (z (κ) |x (κ) ) as the target distribution and q ϕ (z|x (κ) ) as the proposal:</p><p>1</p><formula xml:id="formula_4">) Propose z ∼ q ϕ (z|x (κ) ); 2) Accept z (κ) = z with probability min 1, w(z) w(z (κ) ) where w(z) = p θ (z|x (κ) ) q ϕ (z|x (κ) ) ∝ p θ (z,x (κ) ) q ϕ (z|x (κ) )</formula><p>is the usual importance ratio between the target and the proposal distribution and z(κ) denotes the cached latent state for observation x (κ) .</p><p>The JSA algorithm can be intuitively understood as a stochastic extension of the well-known EM algorithm <ref type="bibr" target="#b5">(Dempster et al., 1977)</ref>. Since the latent variable z (κ) is unknown for data-point x (κ) , the Monte Carlo sampling step in JSA fills the missing value for z (κ) through sampling p θ (z (κ) |x (κ) ), which is analogous to the E-step in EM. Then in the parameter updating step, z (κ) is treated as if being known, and used to optimize over θ and ϕ by performing gradient ascent using ∇ θ log p θ (z (κ) , x (κ) ) and ∇ ϕ log q ϕ (z (κ) |x (κ) ) respectively. This is analogous to the M-step in EM, but with the proposal q ϕ being adapted as well. In summary, we could refer to the underlying mechanism of JSA as Propose, Accept/Reject, and Optimize (or, for short, the PARO mechanism), which establishes JSA as a simple, solid and effective approach to learning discrete latent variable models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Definition of Discrete Latent Variables in TOD systems</head><p>In a TOD system, let u t denote the user utterance, b t the dialog state, db t the DB result, a t the system act and r t the delexicalized response, respectively, at turn t. In this work, all these variables are converted to token sequences, like in DAMD <ref type="bibr">(Zhang et al., 2020a)</ref>. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the workflow for a TOD system is, for each dialog turn t, to generate b t , a t and r t , given u t and dialog history</p><formula xml:id="formula_5">u 1 , r 1 , • • • , u t-1 , r t-1 .</formula><p>The database result db t is deterministically obtained by querying database using the predicted b t , and thus could be omitted in the following probabilistic modeling of a TOD system for simplicity. Let h t = {b t , a t } denote the concatenation of dialog state and system act. Specifically, dialog state b t and system act a t are represented by sequences of labels, for example, [train] day monday [hotel] pricerange cheap and [train] [inf orm] choice departure [request] destination, respectively. Notably, h t 's are observed in labeled dialogs, but they become latent variables in unlabeled dialogs in training and need to be generated in testing. With this definition of h t 's, latent variable models can be developed for TOD systems, which will be described shortly in the next subsection.</p><p>Remarkably, the above definition of latent variables as sequences of labels in this paper is similar to <ref type="bibr">Zhang et al. (2020b)</ref>; <ref type="bibr" target="#b20">Liu et al. (2021)</ref>. An important feature of such latent variables is that they are sensible and interpretable, which correspond to meaningful annotations according to the task knowledge. It is only in unlabeled dialogs that they become unobservable. This is different in nature from some other previous studies of using latent variables in TOD models <ref type="bibr" target="#b34">(Wen et al., 2017;</ref><ref type="bibr" target="#b39">Zhao et al., 2019;</ref><ref type="bibr" target="#b0">Bao et al., 2020)</ref>, where the latent variables are just assumed to be K-way categorical variables and learned in a purely data driven way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A Probabilistic Latent State TOD Model</head><p>With the above introduction of latent variables and motivated by recent studies <ref type="bibr">(Zhang et al., 2020b;</ref><ref type="bibr" target="#b21">Liu et al., 2022)</ref>, the workflow of a TOD system could be described by a conditional sequential generative model with latent variables h t 's as follows for T turns, with parameter θ:</p><formula xml:id="formula_6">p θ (h 1:T , r 1:T |u 1:T ) = T t=1 p θ (h t , r t |u 1 , h 1 , r 1 , • • • , u t-1 , h t-1 , r t-1 , u t ) (3) = T t=1 p θ (h t , r t |h t-1 , r t-1 , u t )(by Markov assumption) (4)</formula><p>Here Eq. (3) and Eq. ( <ref type="formula">4</ref>) could be collectively referred to as latent state TOD models, being non-Markov and Markov respectively. Eq. ( <ref type="formula">3</ref> In contrast, Eq. ( <ref type="formula">4</ref>) makes the Markov assumption that the conditional generation of current h t and r t (when given u t ) depends on the dialog history only through h t-1 and r t-1 at the immediately preceding turn. Markov models have been employed in LSTM-based TOD systems such as in <ref type="bibr" target="#b17">Lei et al. (2018)</ref>; <ref type="bibr">Zhang et al. (2020a)</ref>; <ref type="bibr">Zhang et al. (2020b)</ref>. A recent study in <ref type="bibr" target="#b21">Liu et al. (2022)</ref> revisits Markovian generative architectures (MGAs) for PLM backbones (GPT2 and T5) and shows their efficiency advantages in memory, computation and learning over non-Markov models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Instantiation and Supervised Learning</head><p>In our experiments, we mainly consider MGA based latent state TOD systems <ref type="bibr" target="#b21">(Liu et al., 2022)</ref>, which are illustrated in Figure <ref type="figure" target="#fig_1">2</ref> as directed probabilistic graphical models. The conditional distribution p θ (h t , r t |h t-1 , r t-1 , u t ) is instantiated as</p><formula xml:id="formula_7">p θ (b t , a t , r t |b t-1 , r t-1 , u t )<label>(5)</label></formula><p>which is realized based on a GPT2 backbone in our experiments. The concatenation b t-1 ⊕ r t-1 ⊕ u t is used as the conditioning input, and the output b t ⊕ a t ⊕ r t is generated token-by-token in an autoregressive manner, where ⊕ denotes the concatenation of token sequences.</p><p>In order to perform unsupervised learning over unlabeled dialogs (to be detailed below), we introduce an inference model q ϕ (h 1:T |u 1:T , r 1:T ) as follows to approximate the true posterior p θ (h 1:T |u 1:T , r 1:T ):</p><formula xml:id="formula_8">q ϕ (h 1:T |u 1:T , r 1:T ) = T t=1 q ϕ (h t |h t-1 , r t-1 , u t , r t ) (6) The conditional q ϕ (h t |h t-1 , r t-1 , u t , r t ) is instan- tiated as q ϕ (b t , a t |b t-1 , r t-1 , u t , r t )<label>(7)</label></formula><p>which is realized based on a GPT2 backbone as well in our experiments.</p><p>When labeled dialog data are available, the supervised training of the latent state generative model p θ in and inference model q ϕ can be decomposed into turn-level teacher-forcing, since the latent states h t 's are known (labeled) for all turns and the model likelihoods decomposes over turns, as shown in Eq. ( <ref type="formula">4</ref>) and Eq. ( <ref type="formula">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">JSA Learning over Unlabeled Dialogs</head><p>Suppose that we have n unlabeled dialogs (u</p><formula xml:id="formula_9">(i) 1:T i , r (i) 1:T i )|i = 1, • • • , n , i.e.</formula><p>, user utterances and system responses are available for each dialog, but without any annotations of the latent states.</p><p>The training instances are indexed by the superscripts, and T i denote the number of turns in the i-th training instance. The unsupervised learning of the latent state TOD model over such unlabeled data can be realized by applying the JSA algorithm, and more specifically its conditional version, to maximize the conditional marginal log-likelihood log p θ (r 1:T |u 1:T ).</p><p>The objective functions in JSA learning can be developed as follows, similar to Eq. ( <ref type="formula">1</ref>):</p><formula xml:id="formula_10">                 min θ 1 n n i=1 log p θ (r (i) 1:T i |u (i) 1:T i ) min ϕ 1 n n i=1 KL[p θ (h (i) 1:T i |u (i) 1:T i , r (i) 1:T i ) ||q ϕ (h (i) 1:T i |u (i) 1:T i , r (i) 1:T i )]</formula><p>where we substitute observation x by r 1:T and latent variable z by h 1:T , all conditioned on u 1:T .</p><p>Basically, JSA learning iterates Monte Carlo sampling and parameter updating, as outlined in Algorithm 1. In each iteration, we randomly pick a training instance (u 1:T , r 1:T ) along with the cached latent state h1:T , and we need to draw a posterior sample h 1:T ∼ p θ (h 1:T |u 1:T , r 1:T ). Remarkably, it can be shown in Appendix A that for the posterior p θ (h 1:t |u 1:t , r 1:t ) induced from the joint distribution in Eq. ( <ref type="formula">4</ref>), the following recursion holds:</p><formula xml:id="formula_11">p θ (h 1:t |u 1:t , r 1:t ) ∝p θ (h 1:t-1 |u 1:t-1 , r 1:t-1 )p θ (h t , r t |h t-1 , r t-1 , u t )<label>(8)</label></formula><p>Based on such recursion, we can develop a recursive turn-level MIS sampler, as shown in Algorithm 2, which recursively runs MIS sampler turn-by-turn and finally obtains a valid posterior sample for the whole dialog session, i.e., h 1:T ∼ p θ (h 1:T |u 1:T , r 1:T ). Suppose that we have obtained a sample for the previous t-1 turns, i.e., h 1:t-1 ∼ p θ (h 1:t-1 |u 1:t-1 , r 1:t-1 ). Then, we perform MIS sampling as follows, with p θ (h 1:t |u 1:t , r 1:t ) as the target distribution and</p><formula xml:id="formula_12">p θ (h 1:t-1 |u 1:t-1 , r 1:t-1 )q ϕ (h t |h t-1 , r t-1 , u t , r t )</formula><p>(9) as the proposal distribution:</p><p>1) Propose h ′ t ∼ q ϕ (h t |h t-1 , r t-1 , u t , r t ). Thus, (h 1:t-1 , h ′ t ) is a valid sample proposed from the proposal distribution as shown in Eq. ( <ref type="formula">9</ref>);</p><p>2) Simulate ξ ∼ U nif orm[0, 1] and let</p><formula xml:id="formula_13">h t =      h ′ t , if ξ ≤ min 1, w(h 1:t-1 , h ′ t ) w(h 1:t-1 , ht ) ht , otherwise (10)</formula><p>where the importance ratio between the target and the proposal distribution</p><formula xml:id="formula_14">w(h 1:t-1 , h t ) = p θ (h 1:t |u 1:t , r 1:t ) p θ (h 1:t-1 |u 1:t-1 , r 1:t-1 )q ϕ (h t |h t-1 , r t-1 , u t , r t ) ∝ p θ (h t , r t |h t-1 , r t-1 , u t ) q ϕ (h t |h t-1 , r t-1 , u t , r t )<label>(11)</label></formula><p>After we obtain the sampled latent state h 1:T from Algorithm 2, we perform parameter updating, as outlined in Algorithm 1. The sampled latent state h 1:T is treated as if being known, and we can calculate the gradients of log p θ (h 1:T , r 1:T |u 1:T ) and log q ϕ (h 1:T |u 1:T , r 1:T ) w.r.t. θ and ϕ according to Eq. ( <ref type="formula">4</ref>) and Eq. ( <ref type="formula">6</ref>) respectively, as if we calculate gradients in supervised training. Thanks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Recursive turn-level MIS sampler</head><p>Input: A T -turn dialog (u 1:T , r 1:T ) with cached latent state h1:T , generative model p θ in Eq. ( <ref type="formula">4</ref>), inference model q ϕ in Eq. ( <ref type="formula">6</ref>). for t = 1 to T do Propose h ′ t ∼ q ϕ (h t |h t-1 , r t-1 , u t , r t ); Accept h ′ t as h t , or reject h ′ t and keep ht as h t , according to Eq. ( <ref type="formula">10</ref>); end for Return: h 1:T , as a posterior sample from p θ (h 1:T |u 1:T , r 1:T ) and used as the new cached latent state.</p><p>to the PARO mechanism of JSA, we have such a conceptual simplicity for learning seemly complex conditional sequential latent variable model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Semi-Supervised TOD Systems via JSA</head><p>Now we have introduced the method of building latent state TOD systems (Eq. ( <ref type="formula">4</ref>) and Eq. ( <ref type="formula">6</ref>)) with JSA learning (Algorithm 1 and Algorithm 2), which is referred to JSA-TOD. Semi-supervised learning over a mix of labeled and unlabeled data could be readily realized in JSA-TOD by maximizing the weighted sum of log p θ (h 1:T , r 1:T |u 1:T ) (the conditional joint loglikelihood) over labeled data and log p θ (r 1:T |u 1:T ) (the conditional marginal log-likelihood) over unlabeled data.</p><p>The semi-supervised training procedure of JSA-TOD is summarized in Algorithm 3. Specifically, we first conduct supervised pre-training of both the generative model p θ and the inference model q ϕ on labeled data in JSA-TOD. Then we randomly draw supervised and unsupervised mini-batches from labeled and unlabeled data. For labeled dialogs, the latent states h t 's are given (labeled). For unlabeled dialogs, we apply the recursive turn-level MIS sampler (Algorithm 2) to sample the latent states h t 's<ref type="foot" target="#foot_0">foot_0</ref> and treat them as if being given. The gradients calculation and parameter updating are then the same for labeled and unlabeled dialogs. Such simplicity in application is an appealing property of JSA, apart from its superior performance, as we show later in experiments.</p><p>Algorithm 3 Semi-supervised training in JSA-TOD Input: A mix of labeled and unlabeled dialogs.</p><p>Run supervised pre-training of θ and ϕ on labeled dialogs; repeat Draw a dialog (u 1:T , r 1:T ); if (u 1:T , r 1:T ) is not labeled then Generate h 1:T by applying the recursive turn-level MIS sampler (Algorithm 2); end if </p><formula xml:id="formula_15">J θ = 0, J ϕ = 0; for i = 1, • • • , T do J θ + = log p θ (h t , r t |h t-1 , r t-1 , u t ); J ϕ + = log q ϕ (h t |h t-1 , r t-1 , u t ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment settings</head><p>Experiments are conducted on MultiWOZ2.1 <ref type="bibr" target="#b7">(Eric et al., 2020)</ref>, which is an English multi-domain dialogue dataset of human-human conversations, collected in a Wizard-of-Oz setup with 10.4k dialogs over 7 domains. The dataset was officially randomly split into a train, test and development set, which consist of 8434, 1000 and 1000 dialog samples, respectively. The dialogs in the dataset are all labeled with dialog states and system acts at every turn. Compared to MultiWOZ2.0, Multi-WOZ2.1 removed some noisy state values. Following <ref type="bibr" target="#b21">(Liu et al., 2022)</ref>, some inappropriate state values and spelling errors are further corrected. Dialog responses are delexicalized to reduce surface language variability. We implement domain-adaptive pre-processing like in DAMD <ref type="bibr">(Zhang et al., 2020a)</ref>. More implementation details for our experiments are available in Appendix B.</p><p>For evaluation in MultiWOZ2.1, there are mainly four metrics for corpus based evaluation <ref type="bibr" target="#b22">(Mehri et al., 2019)</ref>. Inform Rate measures how often the entities provided by the system are correct; Success Rate refers to how often the system is able to answer all the requested attributes by user; BLEU Score is used to measure the fluency of the generated responses by analyzing the amount of n-gram overlap between the real responses and the gener- ated responses; Combined Score is computed as (BLEU + 0.5 * (Inform + Success)). To avoid any inconsistencies in evaluation, we use the evaluation scripts in <ref type="bibr" target="#b24">Nekvinda and Dušek (2021)</ref>, which are now also the standardized scripts adopted in the MultiWOZ website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results</head><p>In the semi-supervised experiments, we randomly draw some proportions (5%, 10%, 15% and 20%) of the labeled dialogs from the MultiWOZ2.1 training set, with the rest dialogs in the training set treated as unlabeled, and conduct semi-supervised experiments. Specifically, the number of dialogs kept as labeled under these proportions are 1686, 1265, 843, and 421, respectively, while the rest dialogs are used as unlabeled (i.e., the original labels of dialog states and system acts at all turns are removed for those dialogs in the training set).</p><p>The main results are shown in Table <ref type="table" target="#tab_1">1</ref>. For model instantiations, we use the GPT2 based Markov generative model and inference model, as introduced in <ref type="bibr" target="#b21">Liu et al. (2022)</ref>. It has been shown in <ref type="bibr" target="#b21">Liu et al. (2022)</ref> that using Markovian generative architecture achieves better results than non-Markov models in the low-resource setting for both supervised-only learning and semi-supervised variational learning, which makes it a strong baseline to compare. We first train the generative model and inference model on only the labeled data, which is referred to as "Supervised-only" (Sup-only for short). Then, we perform semi-supervised training on both labeled and unlabeled data. Using the variational method in <ref type="bibr" target="#b20">(Liu et al., 2021</ref><ref type="bibr" target="#b21">(Liu et al., , 2022))</ref>, we get the baseline results of "Variational", where the Straight-Through trick is used to propagate the gradients through discrete latent variables. Using the JSA method proposed in Algorithm 3, we get the results of "JSA". We conduct the experiments with 3 random seeds and report the mean and standard deviation in Table <ref type="table" target="#tab_1">1</ref>.</p><p>From Table <ref type="table" target="#tab_1">1</ref>, we can see that both the Variational and the JSA methods outperform the Supervised-only method substantially across all label proportions. This clearly demonstrate the advantage of semi-supervised TOD systems. Remarkably, semi-supervised JSA-TOD using 20% labels performs close to the supervised-only baseline using 100% labels on MultiWOZ2.1.</p><p>When comparing the two semi-supervised methods, JSA performs better than Variational significantly across almost all label proportions in terms of all four metrics (Inform Rate, Success Rate, BLEU, and Combined Score). Exceptionally, in the case of 5% labels, the Inform Rate of JSA is worse than that of Variational, the Success Rates are close; Nevertheless, the Combined Score of JSA is significantly better. Presumably, this is because we use the Combined Scores to monitor the training, apply early stopping and select the model with the best Combined Score on the validation set. Such model selection put more priority on the overall performance in terms of Combined Scores.</p><p>Further, the results in Table <ref type="table" target="#tab_1">1</ref> are pooled over all label proportions and all random seeds, and the matched-pairs significance tests <ref type="bibr" target="#b8">(Gillick and Cox, 1989)</ref> are conducted to compare JSA and Variational for Inform, Success and BLEU respectively. The p-values are 9.27 × 10 -2 , 2.576 × 10 -14 , and 2.939 × 10 -39 respectively, which show that JSA significantly outperforms Variational.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation study and analysis</head><p>Notably, the JSA and the variational methods in our experiments use the same model instantiations for p θ and q ϕ . The only difference lies in the learning methods they used. In the following, we provide ablation study to illustrate the superiority of JSA over variational in learning latent state TOD models.</p><p>The importance of Metropolis Independence Sampling in JSA. In JSA, we need to use Monte Carlo sampling, particularly the Metropolis Independence Sampling (MIS) to decide whether or not to update the cached latent states h t 's. A naive method is to always accept the labels proposed by the inference model, which is somewhat like selftraining <ref type="bibr" target="#b31">(Rosenberg et al., 2005)</ref>. Another simple method is to run session-level MIS, with the whole Eq. ( <ref type="formula">4</ref>) as the target distribution and the whole Eq. ( <ref type="formula">6</ref>) as the proposal distribution. The whole h 1:T is proposed via ancestral sampling and then get accepted/rejected. The results from one run with each different method are shown in Table <ref type="table" target="#tab_2">2</ref>. Both MIS based methods significantly improves the results, which clearly reveals the importance of using MIS in JSA. By the accept/reject mechanism, we accept latent states which have higher importance ratios and exploit them to update both generative model and inference model, and at the same time, we also explore the state space by randomly accepting latent states which have lower importance ratios. Exploitation and exploration of the latent states seems to be well balanced in JSA, which may explain its good performance. Our proposed recursive turn-level MIS in Algorithm 2 clearly outperforms the session-level MIS, since it samples in a much lower dimensional state space.</p><p>The latent state prediction performance of inference model. In both variational and JSA learning, the inference model q ϕ , which is introduced to approximate the true posterior, plays an important role. The latent states inferred from q ϕ are used, either directly as in variational learning or after accepted/rejected as in JSA learning, to optimize the generative model p θ . We measure the quality of the latent states predicted from q ϕ by label precision/recall/F1, compared to oracle b t and a t (excluding db t ) . We compare different q ϕ obtained from three training methods -Supervisedonly, Variational, and JSA. Note that at the end of running any particular training method, we obtain not only p θ but also q ϕ . The performances of p θ over the test set are shown in Table <ref type="table" target="#tab_1">1</ref>. The testing performances of q ϕ obtained from one run of each different method are shown in Table <ref type="table" target="#tab_3">3</ref>. It can be seen that semi-supervised variational learning does not improve the prediction ability of the inference model, compared to the inference model trained only on the labeled data. In contrast, the prediction performance of the inference model is increased significantly by semi-supervised JSA learning, which is in line with the superior results of for building TOD systems for low-resource scenarios <ref type="bibr" target="#b26">(Peng et al., 2020;</ref><ref type="bibr" target="#b33">Su et al., 2021)</ref>. In this section, we show that JSA learning can be used to further improve over such pretrained models. We use four dialog corpora -MSRE2E <ref type="bibr" target="#b19">(Li et al., 2018</ref><ref type="bibr">), Frames (El Asri et al., 2017)</ref>, TaskMaster <ref type="bibr" target="#b3">(Byrne et al., 2019)</ref> and SchemaGuided <ref type="bibr" target="#b29">(Rastogi et al., 2020)</ref>, which consist of 16545 dialogs with human annotations on belief states and dialog acts, and we follow the preprocessing in <ref type="bibr" target="#b33">(Su et al., 2021)</ref>.</p><p>Generative model and inference model, initialized from GPT2, are pretrained separately on those four corpora, the same as that in supervised-pretraining. Then, we conduct semi-supervised training with only 3% labels in MultiWOZ2.1 (i.e., 240 labeled dialogs with the rest being unlabeled). The results in Table <ref type="table" target="#tab_4">4</ref> show that semi-supervisded JSA on top of pretrained models obtains the best result. This is an encouraging result from using 3% labels, which is close to the naive supervised-only method using 20% labels as shown in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>This paper represents a progress towards building semi-supervised TOD systems by learning latent state TOD models. Traditionally, variational learning is often used; notably, the recently emerged JSA method has been shown to surpass variational learning, particularly in learning of discrete latent variable models. This paper represents the first application of JSA in its conditional sequential version, particularly for such long sequential generational problems like in TOD systems. Extensive experiments clearly show the superiority of JSA-TOD over its variational learning counterpart, not only in benchmark metrics for semi-supervised TOD systems but also from the latent state prediction performances and the variances of the gradients of the inference model. Since discrete latent variable models are widely used in many natural language procession tasks, we hope the results presented in this paper will encourage the community to further explore the applications of JSA and improve upon current approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The information flow in a task-oriented dialog. Square brackets denote special tokens in GPT2.</figDesc><graphic coords="1,310.24,212.60,207.35,79.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The probabilistic graphical model of Markov latent state generative model (a) and inference model (b) for TOD systems. u t and r t are user utterance and system response respectively. The latent variables h t = {b t , a t } are the concatenation of dialog state and system act, which specifically are represented by token sequences in our experiments.</figDesc><graphic coords="4,70.87,70.87,218.27,88.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) represents non-Markov latent state models, which, with different further instantiations, are used in recent PLM-based TOD systems such as in Hosseini-Asl et al. (2020); Yang et al. (2021); Liu et al. (2021).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>r t ); end for Update θ by ascending: ∇ θ J θ ; Update ϕ by ascending: ∇ ϕ J ϕ ; until convergence return θ and ϕ</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Main results on MultiWOZ2.1 for comparison between supervised-only, variational, and JSA methods. Results are reported as the mean and standard deviation from 3 runs with different random seeds.</figDesc><table><row><cell>Proportion</cell><cell>Method</cell><cell>Inform</cell><cell>Success</cell><cell>BLEU</cell><cell>Combined</cell></row><row><cell>100%</cell><cell>Sup-only</cell><cell cols="4">84.50±0.29 72.77±0.50 18.96±0.36 97.59±0.54</cell></row><row><cell></cell><cell>Sup-only</cell><cell cols="4">75.70±1.87 61.07±2.21 16.66±0.29 85.05±2.16</cell></row><row><cell>20%</cell><cell cols="5">Variational 81.83±1.55 67.67±0.50 17.88±0.95 92.63±0.30</cell></row><row><cell></cell><cell>JSA</cell><cell cols="4">83.25± 0.65 71.40±1.20 18.72±0.07 96.04±0.85</cell></row><row><cell></cell><cell>Sup-only</cell><cell cols="4">80.00±0.43 55.57±1.22 16.20±0.24 79.00±1.51</cell></row><row><cell>15%</cell><cell cols="5">Variational 80.85±0.65 67.67±0.88 17.68±0.29 91.86±0.19</cell></row><row><cell></cell><cell>JSA</cell><cell cols="4">83.23±0.53 71.97±1.27 18.59±0.19 95.47±0.73</cell></row><row><cell></cell><cell>Sup-only</cell><cell cols="4">67.57±0.39 50.03±1.09 15.31±0.28 74.11±0.59</cell></row><row><cell>10%</cell><cell cols="5">Variational 80.67±1.33 66.97±1.23 17.34±0.56 91.15±1.80</cell></row><row><cell></cell><cell>JSA</cell><cell cols="4">81.97±0.79 70.40±0.99 18.09±0.38 94.27±1.23</cell></row><row><cell></cell><cell>Sup-only</cell><cell cols="4">49.73±2.45 33.67±1.79 14.07±0.13 55.77±1.94</cell></row><row><cell>5%</cell><cell cols="5">Variational 74.17±0.53 59.93±0.34 16.06±0.69 83.11±1.04</cell></row><row><cell></cell><cell>JSA</cell><cell cols="4">72.37±1.19 59.73±0.92 18.57±0.54 84.62±0.43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation results for using different methods to update latent states h t 's (label proportion: 10%, random seed:11)</figDesc><table><row><cell>Method</cell><cell cols="4">Inform Success BLEU Combined</cell></row><row><cell>Without MIS</cell><cell>71.10</cell><cell>59.80</cell><cell>18.71</cell><cell>84.16</cell></row><row><cell>Session-level MIS</cell><cell>78.70</cell><cell>65.50</cell><cell>17.20</cell><cell>89.30</cell></row><row><cell cols="2">Recursive turn-level MIS 82.80</cell><cell>71.80</cell><cell>18.56</cell><cell>95.86</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison of inference models from different methods, measured by latent state prediction precision/recall/F1 over the test set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>MultiWOZ2.1 testing results for different methods (label proportion: 3%). "+Pretrained model" means that the method is initialized from the pretrained models over four external dialog corpra.</figDesc><table><row><cell>Method</cell><cell cols="4">Inform Success BLEU Combined</cell></row><row><cell>Supervised-only</cell><cell>38.70</cell><cell>25.60</cell><cell>16.42</cell><cell>48.57</cell></row><row><cell>+ Pretrained model</cell><cell>57.70</cell><cell>39.30</cell><cell>14.15</cell><cell>62.65</cell></row><row><cell cols="2">Semi-supervised JSA 55.50</cell><cell>44.80</cell><cell>16.93</cell><cell>67.08</cell></row><row><cell>+ Pretrained model</cell><cell>73.00</cell><cell>58.60</cell><cell>18.61</cell><cell>84.41</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Sampling is empirically implemented via greedy decoding in our experiments.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>This research was funded by <rs type="institution">Joint Institute of Tsinghua University -China Mobile Communications Group Co., Ltd., Beijing, China</rs>.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>JSA's generative model as shown in Table <ref type="table">1</ref>.</p><p>The variance of the gradients from inference model. The gradients for the inference model parameters in variational learning are known to have high-variance, due to gradient propagation through discrete latent variables, while JSA avoids such drawback. From one run of semi-supervised learning under 10% labels, we plot the gradient norms for the inference model parameters, from using the variational and the JSA methods respectively, which are shown in Figure <ref type="figure">3</ref>. For clarity of comparison, we normalize the sum of the gradient norms over all iterations to be one. It can be clearly seen from Figure <ref type="figure">3</ref> that the gradients during variational training are more noisy than those in JSA tranining. Specifically, the variances of the time-series of the gradient norms in Figure <ref type="figure">3</ref> are 3.097 × 10 -6 and 1.527 × 10 -6 for the variational and the JSA methods respectively.</p><p>Pretrained models on external dialog corpora can be further improved by JSA learning for semi-supervised TOD systems. Pretraining and LVM based learning are two broad classes of semisupervised methods. Recently, pretraining on external dialog copora has also shown to be promising p θ (r</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>We implement the models with Huggingface Transformers repository of version 4.8.2. We initialize both the generative model and the inference model with DistilGPT-2, a distilled version of GPT2. For all of supervised pre-training, variational learning and JSA learning, we use the AdamW optimizer and a linear scheduler with 20% warm up steps and maximum learning rate 10 -4 . The minibatch base size is set to be 8 with gradient accumulation steps of 4. The 3 random seeds for the results in Table <ref type="table">1</ref> are 9, 10 and 11. The total epochs for supervised pre-training are 50, and those for both variational learning and JSA learning are 40. We monitor the performance on the validation set and apply early stopping (stop when the current best model is not exceeded by models in the following 4 epochs). We select the best model on the validation set, then evaluate it on test set. All our experiments are performed on a single 32GB Tesla-V100 GPU.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PLATO: Pre-trained dialogue generation model with discrete latent variable</title>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiwoz -a largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling</title>
		<author>
			<persName><forename type="first">Paweł</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-Hsiang</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iñigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ultes</forename><surname>Stefan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Ramadan Osman, and Milica Gašić</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Taskmaster-1: Toward a realistic and diverse dialog dataset</title>
		<author>
			<persName><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chinnadhurai</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Duckworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyu-Young</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Cedilnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4516" to="4525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convergence of a stochastic approximation version of the EM algorithm</title>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Delyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lavielle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Moulines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="94" to="128" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">Arthur</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Frames: A corpus for adding memory to goal-oriented dialogue systems</title>
		<author>
			<persName><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremie</forename><surname>Zumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emery</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiwoz 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines</title>
		<author>
			<persName><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shachi</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanchit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adarsh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuj</forename><forename type="middle">Kumar</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Some statistical issues in the comparison of speech recognition algorithms</title>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="532" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00796</idno>
		<title level="m">A simple language model for task-oriented dialogue</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Explicit state tracking with semisupervision for neural dialogue generation</title>
		<author>
			<persName><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongshen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangsong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive strategy for resetting a non-stationary markov chain during learning via joint stochastic approximation</title>
		<author>
			<persName><forename type="first">Hyunsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Symposium on Advances in Approximate Bayesian Inference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Markov-chain monte carlo score estimators for variational inference with score climbing</title>
		<author>
			<persName><forename type="first">Kyurae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jisu</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coupling a stochastic approximation version of EM with an MCMC procedure</title>
		<author>
			<persName><forename type="first">Estelle</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lavielle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ESAIM: Probability and Statistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="115" to="131" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving end-to-end task-oriented dialog system with a simple auxiliary task</title>
		<author>
			<persName><forename type="first">Yohan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1296" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures</title>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">56th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Xiangnan He, and Dawei Yin</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised variational reasoning for medical dialogue generation</title>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="544" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11125</idno>
		<title level="m">Microsoft dialogue challenge: Building end-to-end task-completion dialogue systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Variational latentstate GPT for semi-supervised task-oriented dialog systems</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenru</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junlan</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04314</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Revisiting Markovian generative architectures for efficient task-oriented dialog systems</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junlan</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06452</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structured fusion networks for dialog</title>
		<author>
			<persName><forename type="first">Shikib</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tejas</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 20th Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Markovian score climbing: Variational inference with KL(p||q)</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Naesseth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Lindsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="15499" to="15510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shades of bleu, flavours of success: The case of multiwoz</title>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Nekvinda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)</title>
		<meeting>the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="34" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint stochastic approximation and its application to learning discrete latent variable models</title>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfu</forename><surname>Song</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="929" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Soloist: Building task bots at scale with transfer learning and machine teaching</title>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinchao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Shahin Shayandeh, Lars Liden, and Jianfeng Gao</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxue</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Sunkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Khaitan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8689" to="8696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A stochastic approximation method. The annals of mathematical statistics</title>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sutton</forename><surname>Monro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Semi-supervised self-training of object detection models</title>
		<author>
			<persName><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Schneiderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>WACV/MOTION</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised seq2seq joint-stochastic-approximation autoencoders with applications to semantic parsing</title>
		<author>
			<persName><forename type="first">Yunfu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
		<idno type="DOI">10.1109/LSP.2019.2953999</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="31" to="35" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Multi-task pre-training for plug-and-play task-oriented dialogue system</title>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arshit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-An</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR, abs/2109.14739</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Latent intention dialogue models</title>
		<author>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint stochastic approximation learning of Helmholtz machines</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop Track</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">UBAR: Towards fully end-to-end task-oriented dialog system with GPT-2</title>
		<author>
			<persName><forename type="first">Yunyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A probabilistic end-to-end task-oriented dialog model with latent belief states towards semisupervised learning</title>
		<author>
			<persName><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junlan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of the Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">2020a. Taskoriented dialog systems that consider multiple appropriate responses under the same context</title>
		<author>
			<persName><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking action spaces for reinforcement learning in end-to-end dialog agents with latent variable models</title>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaige</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
