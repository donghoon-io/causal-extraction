<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimized realization of Bayesian networks in reduced normal form using latent variable model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-03-17">17 March 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">M</forename><surname>E T H O D O L O G I E S A N D A P P L I C A T I O N</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Giovanni</forename><forename type="middle">Di</forename><surname>Gennaro</surname></persName>
							<email>giovanni.digennaro@unicampania.it</email>
							<idno type="ORCID">0000-0001-9757-1712</idno>
							<affiliation key="aff1">
								<orgName type="department">Dipartimento di Ingegneria</orgName>
								<orgName type="institution">Università degli Studi della Campania &quot;Luigi Vanvitelli&quot;</orgName>
								<address>
									<addrLine>via Roma 29</addrLine>
									<settlement>Aversa</settlement>
									<region>CE</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amedeo</forename><surname>Buonanno</surname></persName>
							<email>amedeo.buonanno@enea.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Energy Technologies and Renewable Energy Sources</orgName>
								<orgName type="institution">ENEA</orgName>
								<address>
									<addrLine>P. Enrico Fermi 1</addrLine>
									<settlement>Portici</settlement>
									<region>NA</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dipartimento di Ingegneria</orgName>
								<orgName type="institution">Università degli Studi della Campania &quot;Luigi Vanvitelli&quot;</orgName>
								<address>
									<addrLine>via Roma 29</addrLine>
									<settlement>Aversa</settlement>
									<region>CE</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francesco</forename><forename type="middle">A N</forename><surname>Palmieri</surname></persName>
							<email>francesco.palmieri@unicampania.it</email>
							<affiliation key="aff1">
								<orgName type="department">Dipartimento di Ingegneria</orgName>
								<orgName type="institution">Università degli Studi della Campania &quot;Luigi Vanvitelli&quot;</orgName>
								<address>
									<addrLine>via Roma 29</addrLine>
									<settlement>Aversa</settlement>
									<region>CE</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">B</forename><surname>Giovanni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Di</forename><surname>Gennaro</surname></persName>
						</author>
						<title level="a" type="main">Optimized realization of Bayesian networks in reduced normal form using latent variable model</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-17">17 March 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s00500-021-05642-3</idno>
					<note type="submission">Accepted: 29 January 2021 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bayesian networks</term>
					<term>Belief propagation</term>
					<term>Factor graphs</term>
					<term>Latent variable</term>
					<term>Optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bayesian networks in their Factor Graph Reduced Normal Form are a powerful paradigm for implementing inference graphs. Unfortunately, the computational and memory costs of these networks may be considerable even for relatively small networks, and this is one of the main reasons why these structures have often been underused in practice. In this work, through a detailed algorithmic and structural analysis, various solutions for cost reduction are proposed. Moreover, an online version of the classic batch learning algorithm is also analysed, showing very similar results in an unsupervised context but with much better performance; which may be essential if multi-level structures are to be built. The solutions proposed, together with the possible online learning algorithm, are included in a C++ library that is quite efficient, especially if compared to the direct use of the well-known sum-product and Maximum Likelihood algorithms. The results obtained are discussed with particular reference to a Latent Variable Model structure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Factor Graph (FG) representation, and in particular the so-called Normal Form (FGn) <ref type="bibr" target="#b8">(Forney 2001;</ref><ref type="bibr" target="#b10">Loeliger 2004)</ref>, is a very appealing formulation to visualize and manipulate Bayesian graphs; representing their relative joint probability by assigning variables to arcs and functions (or factors) to nodes.</p><p>Furthermore, in the Factor Graph in Reduced Normal Form (FGrn), through the use of replicator units (or equal constraints), the graph is reduced to an architecture in which each variable is connected to two factors at most <ref type="bibr" target="#b13">(Palmieri 2016)</ref>; with belief messages that flow bidirectionally into the network. This paradigm has demonstrated its extensive modularity and flexibility in managing variables of different types and cardinalities <ref type="bibr" target="#b15">(Palmieri and Buonanno 2015)</ref>, and can also be used to build multi-layer network <ref type="bibr" target="#b14">(Palmieri and Buonanno 2014;</ref><ref type="bibr">Buonanno and Palmieri 2015b, c)</ref>. For this reason, in a previous work a Simulink<ref type="foot" target="#foot_0">foot_0</ref> library for the rapid prototyping of an FGrn network was already implemented <ref type="bibr">(Buonanno and Palmieri 2015a)</ref>, but because of the limitations imposed by Simulink, it is not particularly suitable for treating large amounts of data and/or complex architectures with many variables.</p><p>Truthfully, despite the large presence of such arguments in the literature <ref type="bibr" target="#b9">(Koller and Friedman 2009;</ref><ref type="bibr" target="#b0">Barber 2012;</ref><ref type="bibr" target="#b12">Murphy 2012)</ref>, this type of structure always suffers from high computational and memory costs, due to the lack of attention given to the specific algorithmic implementation. Therefore, this work aims to improve complexity efficiency in both inference and learning (overcoming software limitations), and all the solutions obtained have been included in a C++ library (<ref type="url" target="https://github.com/mlunicampania/FGrnLib">https://github.com/mlunicampania/FGrnLib</ref>). The various problems, related to the propagation and learning of probabilities within the FGrn paradigm, are addressed by focusing on the implementation of the Latent Variable Model (LVM) <ref type="bibr" target="#b1">(Bishop 1999;</ref><ref type="bibr" target="#b12">Murphy 2012)</ref>, also called Autoclass <ref type="bibr" target="#b5">(Cheeseman and Stutz 1996)</ref>. LVMs can be used in a large number of applications and can be seen as a basic building block for more complex architectures. After a brief introduction of the FGrn paradigm in Sect. 2 and the LVM in Sect. 3, necessary to provide the fundamental elements for subsequent discussions, the C++ library project is described in Sect. 4. In Sect. 5 a detailed analysis of the computational complexity of the various algorithmic elements is presented for each bulding block. In Sect. 6 some simulation results that verify how the proposeed algorithms produce indisputable advantages are presented. Finally, in Sect. 7, an incremental learning algorithm is introduced by modifying the ML recursions, that not only presents a significant decrease in terms of memory costs (allowing learning even in the presence of huge datasets), but shows in some cases better performance in avoiding local minima traps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Factor graphs in reduced normal form</head><p>A FGrn requires only the combination of the elements shown in Figure <ref type="figure" target="#fig_0">1:</ref> (a) The Variable V , which can take a single discrete value v ∈ V = {v 1 , ..., v |V| }, is represented as an oriented edge with two discrete messages,<ref type="foot" target="#foot_1">foot_1</ref> proportional (∝) to the discrete distributions, that travel in both directions.</p><p>Depending on the direction assigned to the variable, the two messages are respectively called forward f V (v) and backward b V (v), and can be also represented as |V|dimensional column vectors: f V and b V . Note that the marginal distribution p V <ref type="bibr">(v)</ref>, which is proportional to the posterior given the observations anywhere else in the network, is proportional to the product</p><formula xml:id="formula_0">p V (v) ∝ f V (v)b V (v),</formula><p>or in vector form</p><formula xml:id="formula_1">p V ∝ f V b V ,<label>(1)</label></formula><p>where denotes the Hadamard (element-by-element) product. (b) The Replicator Block (Diverter) represents the equality constraint of the connected variables. The constraint of equality between the variables is obtained by making sure that each replica can carry different forward and backward messages. A replicator acts like a bus with messages combined and diverted towards the connected branches.</p><p>The combination rule (product rule) is such that outgoing messages are the product of all the incoming ones, except for the one that belongs to the same variable </p><formula xml:id="formula_2">b V (i) (v) ∝ m j=1 j =i f V ( j) (v) M k=m+1 b V (k) (v), i = 1 : m f V (i) (v) ∝ m j=1 f V ( j) (v) M k=m+1 k =i b V (k) (v), i = m + 1 : M, or in vector form b V (i) ∝ m j=1 j =i f V ( j) M k=m+1 b V (k) , i = 1 : m f V (i) ∝ m j=1 f V ( j) M k=m+1 k =i b V (k) , i = m + 1 : M (2) (c)</formula><formula xml:id="formula_3">P(Y |V ) = Pr{Y = y j |V = v i } i=1:|V| j=1:|Y| = θ i j i=1:|V| j=1:|Y| ,</formula><p>or more explicitly</p><formula xml:id="formula_4">P(Y |V ) = ⎡ ⎢ ⎢ ⎢ ⎣ P (Y = y 1 |V = v 1 ) • • • P Y = y |Y| |V = v 1 P (Y = y 1 |V = v 2 ) • • • P Y = y |Y| |V = v 2 . . . . . . . . . P Y = y 1 |V = v |V| • • • P Y = y |Y| |V = v |V| ⎤ ⎥ ⎥ ⎥ ⎦ = ⎡ ⎢ ⎢ ⎢ ⎣ θ 11 • • • θ 1|Y| θ 21 • • • θ 2|Y| . . . . . . . . . θ |V|1 • • • θ |V||Y| ⎤ ⎥ ⎥ ⎥ ⎦ .</formula><p>Outgoing messages are</p><formula xml:id="formula_5">f Y (y i ) ∝ |V| j=1 θ i j f V (v j ) and b V v j ∝ |Y| i=1 θ i j b Y (y i ),</formula><p>or in vector form</p><formula xml:id="formula_6">f Y ∝ P(Y |V ) T f V and b V ∝ P(Y |V )b Y . (3) (d)</formula><p>The Source block defines an independent |V|-dimensional source variable V with its prior distribution π V . Therefore, the outgoing message is</p><formula xml:id="formula_7">f V (v i ) = π V (v i ), i = 1 : |V|,</formula><p>or in vector form</p><formula xml:id="formula_8">f V = π V .</formula><p>It should be emphasized that the rules presented are a rigorous translation of the total probability theorem and Bayes' rule.</p><p>Note that the only parameters that need to be learned during the training phase are the matrices inside the SISO blocks and the priors inside the Sources. Although different variations are possible <ref type="bibr" target="#b9">(Koller and Friedman 2009;</ref><ref type="bibr" target="#b0">Barber 2012;</ref><ref type="bibr" target="#b13">Palmieri 2016)</ref>, training algorithms are derived mainly as maximization of the likelihood on the observed variables that can be anywhere in the network. Furthermore, within the FGrn paradigm, learning takes place locally; that is, the parameters inside the SISO blocks and the Sources can be learned using only the backward and forward messages to that particular element. This also means that parameter learning in this representation can be addressed in a unified way because we can use a single rule to train any SISO or Source block in the system, simultaneously and independently of its position. Therefore, learning is done iterating over three simple steps:</p><p>1. present the observations to the network in the form of distributions; they can be anywhere in the network as backward or forward messages;</p><p>2. propagate the messages in the whole network, in accordance with the mathematical rules just described; 3. perform the update of SISO blocks and Sources using incoming messages.</p><p>The prior of a source is learned simply by calculating the new marginal probability (using Equation <ref type="formula" target="#formula_1">1</ref>), due to the changes of the backward message to the Source. On the other hand, learning the matrices inside the SISO blocks is more complex. According to our experience <ref type="bibr" target="#b13">(Palmieri 2016)</ref>, the best algorithm for learning the conditional probability matrices inside the SISO blocks is the Maximum Likelihood (ML) algorithm, which uses the following update</p><formula xml:id="formula_9">θ (1) lm ←- θ (0) lm N n=1 f V [n] (l) N n=1 f V [n] (l)b Y [n] (m) f T V [n] θ (0) b Y [n]</formula><p>.</p><p>(4)</p><p>Equation 4 represents the heart of the ML algorithm and usually requires multiple cycles in order to achieve convergence. However, changing the matrices also changes the propagated messages. For this reason, the whole learning process (starting from the presentation of the evidence) also needs to be performed several times; namely for a fixed number of epochs (an hyperparameter).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Latent variable models</head><p>Factor Graphs and in particular FGrn can be used to represent a joint probability distribution by appropriately using the dependencies/independence between the variables. In many cases, however, the probabilistic description of an event can be further simplified using a set of unobserved variables, which represent the unknown "causes" generating the event.</p><p>In these models the variables are typically divided into Visible and Hidden: the first relating to the inputs observed during the inference, the latter belonging to the internal representation. Obviously, the main advantage of this type of model lies in the fact that it has fewer parameters than the complete model, producing a sort of compressed representation of the observed data (which are therefore easier to manage and analyse).</p><p>The simplest and most complete model that includes both visible and hidden variables is the bipartite graph of Fig. <ref type="figure">2</ref>, named Latent Variable Model (LVM) <ref type="bibr" target="#b12">(Murphy 2012;</ref><ref type="bibr" target="#b1">Bishop 1999)</ref> or Autoclass <ref type="bibr" target="#b5">(Cheeseman and Stutz 1996)</ref>; any other hidden variables model can ultimately be reduced to such a model. Within the figure, it is possible to distinguish H latent variables, S 1 , . . . , S H , and N observed variables, Y 1 , . . . , Y N ; where typically N H . It should be noted that although the given nomenclature seems to subdivide the variables according to their position, 123 Fig. <ref type="figure">2</ref> The general structure for Latent Variable Models Fig. <ref type="figure">3</ref> The general structure for the Latent Variable Models in Reduced Normal Form where the variables below are known while those above need to be estimated, the bidirectional structure of the network remains quite general, including cases in which some of the above variables may be known and some of the bottom variables need to be estimated.</p><p>The FGrn of a Bayesian network represented by the general LVM structure of Fig. <ref type="figure">2</ref> is shown in Fig. <ref type="figure">3</ref>. The system is intrinsically represented as a generative model; that is, choosing to direct the variables downwards and positioning the marginally independent sources S 1 , ..., S H at the top. Moreover, it should be noted that the supposed independence of all known variables given the hidden variables (provided by the latent variable model) allows to greatly simplify the analysis of the total joint probability, which can now be represented by the factorization</p><formula xml:id="formula_10">P (Y 1 , . . . , Y N , S 1 , . . . , S H ) = P (Y 1 |S 1 , . . . , S H ) • • • P (Y N |S 1 , . . . , S H ) P (S 1 ) • • • P (S H ) .</formula><p>As said, the structure allows to manage totally heterogeneous variables, but it is good to clarify that (being a representation of a Bayesian network) the variables must be presented as probability vectors; that is, through vectors containing the "degree of similarity" of the variable to each element of its discrete alphabet. The source variables, which have prior distributions π S 1 , ... π S H , are mapped to the product space P, of dimen-</p><formula xml:id="formula_11">sions |P| = |S 1 | × • • • × |S H |,</formula><p>via the fixed row-stochastic matrices (shaded blocks in Fig. <ref type="figure">3</ref>)</p><formula xml:id="formula_12">P (S 1 S 2 . . . S H ) (1) |S 1 = |S 1 | H i=1 |S i | I |S1| ⊗ 1 T |S2| ⊗ • • • ⊗ 1 T |S H | , . . . P (S 1 S 2 . . . S H ) (H ) |S H = |S H | H i=1 |S i | 1 T |S1| ⊗ • • • ⊗ 1 T |S H -1 | ⊗ I |S H | , (<label>5</label></formula><formula xml:id="formula_13">)</formula><p>where ⊗ denotes the Kronecker product, 1 K is a Kdimensional column vector with all ones, and I K is the K × K identity matrix <ref type="bibr" target="#b13">(Palmieri 2016)</ref>.</p><p>The conditional probability matrix is such that each variable contributes to the product space with its value, and it is uniform on the components that compete to the other source variables. This is the FGrn counterpart of the Junction Tree reduction procedure because it is equivalent to "marry the parents" in Bayesian Graphs <ref type="bibr" target="#b9">(Koller and Friedman 2009)</ref>, but here there are explicit branches for the product space variable. For this reason, although the messages traveling bidirectionally and the initial Bayesian network present many loops (Fig. <ref type="figure">2</ref>), the FGrn architecture will not show any convergence problem because the LVM has been reduced to a tree.</p><p>Finally, the j-th SISO block at the bottom of Fig. <ref type="figure">3</ref>, with j = 1, ..., N , represents the conditional probability matrices P(Y j |S 1 S 2 ...S H ), which than will have dimensions |P| × |Y j |.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The LVM with one hidden variable</head><p>When H &gt; 1 we have a Many-To-Many LVM model, which was already discussed in a previous work <ref type="bibr" target="#b15">(Palmieri and Buonanno 2015)</ref>; calling it Discrete Independent Component Analysis (DICA) because it uses the same generative model of the Independent Component Analysis (ICA), but on discrete variables.</p><p>Vice versa, when H = 1, we obtain a One-To-Many LVM model, in which there is just one latent factor (parent) that conditions Y 1 , . . . , Y N (children) and where obviously fixed matrices (previously represented by the shaded blocks) are no longer necessary. Although the general paradigm has the great advantage of allowing the presence of Sources of dif- ferent types, for simplicity in the tests performed we have preferred to focus exclusively on this more manageable architecture (Fig. <ref type="figure" target="#fig_1">4</ref>). The figure shows the most general case (used in the final tests), in which a Class Variable L is also added to the bottom variables. This configuration is used in the case of supervised learning, allowing (after learning) to perform various tasks, including:</p><p>(a) Pattern classification, achievable by injecting the observations as delta distributions on the backwards of Y 1 , ..., Y N and leaving the backward on L uniform. The classification will be returned to L through its forward message f L . (b) Pattern completion, where only some of the observed variables Y 1 , ..., Y N are available in inference (and then injected into the network through delta distributions on their backwards) while the others are unknown (and therefore represented by uniform backward distributions). Also L may be unknown (uniform distribution), partially known (generic distribution), or known (delta distribution). The forward distributions at the unknown variables complete the pattern, and at L provide the best inference in case it is not perfectly known. The posterior on L is obtained by multiplying the two messages on it (Equation 1); avoidable step if L is not known at the beginning (because in this case the backward is uniform). (c) Prototype inspection, obtainable by injecting only a delta distribution at L on the jth label. The forward distributions f Y 1 , ..., f Y N will represent the separable prototype distribution of that class.</p><p>Another way to use the previous network is to make the class variable coincide with the hidden variable S = L, forcing the corresponding SISO block matrix to be diagonal. This constraint will create a so-called "Naive Bayes Classifier" <ref type="bibr" target="#b0">(Barber 2012)</ref>, further simplifying the factorization into</p><formula xml:id="formula_14">P (Y 1 , ..., Y N , L) = P (Y 1 |L) • • • P (Y N |L) P(L). (<label>6</label></formula><formula xml:id="formula_15">)</formula><p>In this case, usually all the variables are observed during training, and the typical use in inference is to obtain L from observed Y 1 , ..., Y N . Note that the case related to unsupervised learning can be obtained from the general model presented simply by eliminating the variable L. In this case, after learning, the elements of the alphabet S = {s 1 , ..., s |S| } of the hidden variable S represent "Bayesian clusters" of the data, which follows the prior distribution π S (learned blindly). The network can be used in inference both for the pattern completion, in the case where (as seen previously) only some of the underlying variables are known and we try to estimate the others through the corresponding forward messages, and to create a so-called embedding representation, in which the backward message becomes a different representation of the underlying known variables. In the latter case, in order to understand the representation that the network has created, we can look at the j-th centroid of the bayesian clusters injecting as f S a delta distribution j = [0 . . . 1 . . . 0] T , where the 1 is at the j-th position. The set of forward distributions f Y 1 , ..., f Y N generated by the network will represent the marginal distributions around the centroid of the jth cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Design of FGrnLib</head><p>There are several software packages, known in the literature, that can be used to design and/or simulate Bayesian networks (an updated list can be found in <ref type="bibr" target="#b11">Murphy (2014)</ref>). Unfortunately, many of them are in closed packages and/or run only on private servers, preventing proper performance analysis. Others either have limitations on the number of variables and the size of the network, or do not use the FG architecture. Therefore, the main purpose of this work (ie the reduction of complexity) has merged with the design of an optimized library, called FGrnLib, for the realization of a Bayesian network through the use of the FGrn model; that is open and contains an efficient implementation of the elements in Fig. <ref type="figure" target="#fig_0">1</ref>. The FGrnLib library has been written in C++, following the classic object-oriented paradigm, and it has been adapted for parallel computing (on multiprocessor systems with shared memory) through the use of the OpenMP application interface. The various algorithmic operations have been implemented to limit as much as possible the computational complexity without significantly affecting memory requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data structures</head><p>Before starting the analysis of individual operations, it is necessary to focus on the structure of the main classes (Fig. <ref type="figure" target="#fig_2">5</ref>) and their individual roles. These classes correspond to the main elements presented in Fig. <ref type="figure" target="#fig_0">1</ref> (and mathematically described in Sect. 2), but also have subtle design choices that need to be clarified (especially in reference to what was previously done in <ref type="bibr">Buonanno and Palmieri (2015a)</ref>). The main classes are:</p><p>-The Link class, which represents a single discrete variable of the model. This class contains the two forward and backward messages for the variable, and is designed to ensure that each message at each time step is represented by a single vector. This means that in every instant of time every single link takes on only two messages (in the two different directions), providing better control of information traveling on the network but preventing the possibility of learning through the simultaneous presentation of all the evidence. -The Diverter class, which imposes the constraint of equality among the variables. This class has been created to be as general as possible, in the sense that it can automatically adapt the parameters of the net to the number of variables. For this reason, it includes not only the process of replication of variables but also the creation and control of product space matrices (Equation <ref type="formula" target="#formula_12">5</ref>). For space complexity, these (sparse and row-stochastic) matrices are stored in column vectors, whose elements represent the index of the active column in that particular row of the matrix</p><formula xml:id="formula_16">⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ 1 0 0 0 1 0 0 0 1 1 0 0 . . . ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ -→ ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ 0 1 2 0 . . . ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦</formula><p>.</p><p>. -The SISOBlock class, which represents the probability of the output variable Y given an input variable V , contains the row-stochastic conditional probability matrix P(Y |V ). Due to the fact that the Link class allows only two vector at a time, the SISO blocks are realized to permit the storage and retrieval of all messages that reached the blocks during the batch learning phase; avoiding transmitting the evidences all at the same time but reusing the single Link memory units for each step. -The Source class, which represents the independent variables inside the model, with their prior probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data flow management</head><p>Since the FGrn paradigm allows to work only through local information, the flow of messages within the network can be parallelized. However, parallelizing the message flow in the network imposes essential changes to the Diverter class.</p><p>In fact, the multiplication of the messages internally to the Diverter can take place only after all the messages of the variables relating thereto have been updated. Being the only responsible for the combination of messages coming from different directions, the Diverter must necessarily act as a "barrier" (with the meaning that this term assumes in parallel programming); solving the synchronization problems simply by not updating the output values until it receives the activation signal from a supervisor. Although within the FGrnLib a Supervisor class has been defined to more easily manage some predefined network, the meaning of a supervisor is quite general here because it refers to any class that possesses all the references to the single elements of the network realized.</p><p>In Fig. <ref type="figure" target="#fig_3">6</ref> it is shown how the supervisor handles the message scheduling, relatively to both the inference mode and the batch learning mode. Note that, in order for messages that travel in parallel to be propagated anywhere on the network, a number of steps equal to the diameter of the graph is required <ref type="bibr" target="#b16">(Pearl 1988)</ref>.</p><p>Recalling therefore that, in graph theory, the diameter of the network is the longest possible path (in terms of number of crossed arcs) among the smallest existing paths able to connect each vertex inside the graph, it is easy to understand that a simple one-layer LVM network (like in Fig. <ref type="figure">3</ref> or in Fig. <ref type="figure" target="#fig_1">4a</ref>) will only need three propagation steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Inference</head><p>The inference phase, depicted in Fig. <ref type="figure" target="#fig_3">6a</ref>, is relatively simple: the various messages proceed in parallel until they reach the Diverter. All that is necessary is to block the messages at the Diverter, to prevent the start of the multiplication process before all the messages have become available. The supervisor must therefore perform only two phases: parallelize the input variables and then activate the Diverter. It should be noted that typically all messages are initialized to uniform vectors, and updated according to the rules described in Sect. 2.</p><p>For what has been said, in the first step of Fig. <ref type="figure" target="#fig_3">6a</ref> the supervisor performs the parallelization of the input variables Y 1 , . . . , Y N , placing in the backward messages appropriate distributions for the known (or partially known) variables and uniform distributions for the unknown ones. Hence, in the second step, the messages start to be propagated in the network (using the second in Equation <ref type="formula">3</ref>). At this point, when all the variables have performed the second step, the supervisor must activate the Diverter to allow the propagation of the messages to continue. The last message propagation step allows to obtain the desired output values (using the first in Equation <ref type="formula">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Batch learning</head><p>Every single epoch of the batch learning phase (Fig. <ref type="figure" target="#fig_3">6b</ref>) is not so different from what we have just seen for the inference phase, since the supervisor basically performs only two more operations. The first one, which only applies at the beginning of the whole learning phase (that is, it is not reiterated at each epoch), consists in activating the batch learning mode inside the SISO blocks and the Sources. This step enables storage within the SISO blocks, which consequently memorize the incoming vectors (from both sides) as soon as they are updated, and the Sources, which will begin to add together all incoming backward messages within a temporary vector. It should be noted that, in the case of multi-layer structures, the supervisor will also have to worry about preventing the SISO blocks of the next level from propagating incoming messages, thus transforming the latter into "barriers" as well. In fact, in order to get the classic layer-by-layer approach, the information should not propagate above the Diverter until the underlying batch learning phase is complete, but only if the top layer does not provide Sources. So as not to complicate the Diverter too much, forcing it to know the connected elements, the SISO blocks also provide a pause mode, which if enabled prevents forward propagation of messages.</p><p>After activation of the batch learning mode the messages propagate within the network through the same previous steps, being blocked again to the Diverter. However, as we can see in Fig. <ref type="figure" target="#fig_3">6b</ref>, the descending phase will not include the production of the final messages, which will only be stored in the SISO blocks in order to avoid performing the related mathematical operations.</p><p>The second different operation, performed by the supervisor at the end of each epoch, consists in the activation of the actual learning procedure, which will execute the ML algorithm using the stored vectors (up to a sufficient convergence or at most for a fixed number of times) and change the prior of the Source. After this last phase, the procedure can be repeated for another epoch, presenting the evidence to the network again. At the end of the learning, the last message from the supervisor will also modify the operating modes of the SISO blocks and the Sources, thus making them ready for the subsequent inference phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Complexity and efficient algorithms</head><p>Having to pay particular attention to the computational and memory costs, in the creation of the library we worked on the details of each individual element. This, together with the probabilistic nature of the Bayesian networks, has led to the preliminary definition of particular basic data structures that it is perhaps necessary to analyse quickly. In fact, the library also defines classes that represent probability vectors and row-stochastic matrices, to facilitate the interpretation and definition of the variables and to easily manage all the algebraic operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Probability vector</head><p>A probability vector is a vector with elements in [0, 1] that sum to one. Although the network does not use only probability vectors, the execution of every operation that uses them must then provide normalization. Every normalization consists of d -1 sums and d divisions, so the computational cost will be O(d)</p><formula xml:id="formula_17">⎡ ⎢ ⎢ ⎢ ⎣ υ 1 υ 2 . . . υ d ⎤ ⎥ ⎥ ⎥ ⎦ norm = ⎡ ⎢ ⎢ ⎢ ⎣ ζ 1 ζ 2 . . . ζ d ⎤ ⎥ ⎥ ⎥ ⎦ -→ ζ i = υ i d j=1 υ j .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Row-stochastic matrix multiplication</head><p>In a row-stochastic matrix P each row is a probability vector.</p><p>It is important to observe that the premultiplication or postmultiplication of a row-stochastic matrix for a vector (with the appropriate dimensions) Fig. <ref type="figure">7</ref> A diverter with M = 4 (one input and three output variables)</p><formula xml:id="formula_18">ξ 1 . . . ξ l ⎡ ⎢ ⎣ p 11 • • • p 1d . . . . . . . . . p l1 • • • p ld ⎤ ⎥ ⎦ = ζ 1 . . . ζ l -→ ζ j = l i=1 p i j ξ i ⎡ ⎢ ⎣ p 11 • • • p 1d . . . . . . . . . p l1 • • • p ld ⎤ ⎥ ⎦ ⎡ ⎢ ⎣ ξ 1 . . . ξ d ⎤ ⎥ ⎦ = ⎡ ⎢ ⎣ ζ 1 . . . ζ l ⎤ ⎥ ⎦ -→ ζ i = d j=1 p i j ξ i</formula><p>will consist of ld multiplications and (l -1)d or l(d -1) sums respectively, producing the same computational cost O(ld).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Diverter</head><p>From a computational point of view, the most critical structure in the implementation of a Bayesian network using FGrn is represented by the Diverter, where, obviously, the greatest criticality is in the efficient implementation of the internal multiplication process (Equation <ref type="formula">2</ref>). In fact, the possible presence of zeros, in different positions of the single vectors, obliges to perform the calculation of the outgoing vectors individually. In the general case of Fig. <ref type="figure">3</ref>, indicating the total value of all the variables connected to the diverter with M = H + N and assuming that the input vectors have dimensions equal to d, bare application of the multiplication rule would require M(M -2)d multiplications. Regardless of the size of the input vectors, the computational cost is therefore polynomial and equal to O(M 2 ) vectorial multiplications.</p><p>Considering as an example the Diverter of Fig. <ref type="figure">7</ref> (with M = 4), the simple application of the product rule in the production of the outgoing messages (b</p><formula xml:id="formula_19">V (0) , f V (1) , f V (2) , f V (3) )</formula><p>would require eight vectorial multiplications:</p><formula xml:id="formula_20">b V (0) = b V (1) b V (2) b V (3) ; f V (1) = f V (0) b V (2) b V (3) ; f V (2) = f V (0) b V (1) b V (3) ; f V (3) = f V (0) b V (1) b V (2) .</formula><p>This process can be performed more efficiently by defining an order among the variables connected to the Diverter and by performing a double cascade process in which each variable is responsible only for passing the correct value to For each output message two contributions are used: one derived from the left part of the computational graph and the other one derived from the right part the neighboring variable. In this way, the variables at the ends of the chain will perform no multiplication while each variable inside the chain will perform only three multiplications, relative to the passage of the two temporary vectors along the chain and to the output of the outgoing message. With reference to the example of Fig. <ref type="figure">7</ref> we have the data flow represented in the Fig. <ref type="figure" target="#fig_4">8</ref>. In other words, the proposed solution exploits the presence of the same multiplication groups through a round-trip process. This reduces the computational complexity from quadratic to linear, O(M), finally requiring only 3(M -2) vector multiplications. Although there is obviously an increase in the memory required, due to temporary vectors along the chain, it remains linear with M, and has been further optimized (requiring only M -1 temporary vectors altogether) by choosing to reuse the same vectors (a i = a i ) when changing direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Unknown variables</head><p>A very attractive property of using a probabilistic paradigm, which makes it preferable in certain contexts, is represented by its ability to manage unknown inputs. Even in the case of maximum uncertainty, that is when nothing is known about the particular variable in that particular observation set, a process of inference or learning can still be performed by making the corresponding message of that variable a uniformly distributed probability vector</p><formula xml:id="formula_21">bY i = ⎡ ⎢ ⎢ ⎣ 1 |Y i | . . . 1 |Y i | ⎤ ⎥ ⎥ ⎦ .</formula><p>In this particular circumstance, the message propagation process can be optimized by avoiding the multiplication of backward vectors with the matrices inside the SISO blocks, noting that</p><formula xml:id="formula_22">b S (i) = P (Y i |S) bY i = ⎡ ⎢ ⎣ P (Y i = ξ 1 |S = σ 1 ) • • • P Y i = ξ |Y i | |S = σ 1 . . . . . . . . . P Y i = ξ 1 |S = σ |S| • • • P Y i = ξ |Y i | |S = σ |S| ⎤ ⎥ ⎦ ⎡ ⎢ ⎢ ⎣ 1 |Y i | . . . 1 |Y i | ⎤ ⎥ ⎥ ⎦ = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ |Y i | j=1 θ 1 j bY i ξ j . . . |Y i | j=1 θ |S| j bY i ξ j ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ 1 |Y i | |Y i | j=1 θ 1 j . . . 1 |Y i | |Y i | j=1 θ |S| j ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ = ⎡ ⎢ ⎢ ⎣ 1 |Y i | . . . 1 |Y i | ⎤ ⎥ ⎥ ⎦ .</formula><p>By not propagating the unknown variable (setting b S (i) as an expanded/reduced version of bY i ), for every single unknown variable present in input during the inferential and the learning process we can save |S| vector multiplications, improving overall network performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Efficient ML implementation</head><p>Regarding the learning phase, particular attention has been given to the realization of an efficient implementation of the ML algorithm. First of all, since the matrix inside the SISO blocks is set to be row-stochastic by construction, it has been noted that the first divisor in the Equation 4 becomes unnecessary. For this reason, the equation can be rewritten as follows</p><formula xml:id="formula_23">θ (1) lm ←-θ (0) lm N n=1 f V [n] (l)b Y [n] (m) f T V [n] θ (0) b Y [n]</formula><p>,</p><p>or in vector form</p><formula xml:id="formula_24">θ (1) ←-θ (0) N n=1 f V [n] b T Y [n] f T V [n] θ (0) b Y [n]</formula><p>.</p><p>123</p><p>Furthermore, it can be observed that the value obtained through the vector multiplications f</p><formula xml:id="formula_25">T V [n] θ (0) b Y [n]</formula><p>is actually equal to the sum of all elements of the matrix θ</p><formula xml:id="formula_26">0 f V [n] b T Y [n]</formula><p>. This assertion is provable by observing that</p><formula xml:id="formula_27">θ 0 f V [n] b T Y [n] = ⎡ ⎢ ⎢ ⎣ θ (0) 11 • • • θ (0) 1|Y| . . . . . . . . . θ (0) |V|1 • • • θ (0) |V||Y| ⎤ ⎥ ⎥ ⎦ ⎡ ⎢ ⎣ φ 1 . . . φ |V| ⎤ ⎥ ⎦ β 1 • • • β |Y| = ⎡ ⎢ ⎢ ⎣ θ (0) 11 • • • θ (0) 1|Y| . . . . . . . . . θ (0) |V|1 • • • θ (0) |V||Y| ⎤ ⎥ ⎥ ⎦ ⎡ ⎢ ⎣ φ 1 β 1 • • • φ 1 β |Y| . . . . . . . . . φ |V| β 1 • • • φ |V| β |Y| ⎤ ⎥ ⎦ = ⎡ ⎢ ⎢ ⎣ θ (0) 11 φ 1 β 1 • • • θ (0) 1|Y| φ 1 β |Y| . . . . . . . . . θ (0) |V|1 φ |V| β 1 • • • θ (0) |V||Y| φ |V| β |Y| ⎤ ⎥ ⎥ ⎦</formula><p>whose sum of all the elements can than be written in the form</p><formula xml:id="formula_28">|V| l=1 |Y| m=1 θ (0) f V [n] b T Y [n] = |V| l=1 φ l |Y| m=1 θ (0) lm β m ,</formula><p>which precisely is equal to</p><formula xml:id="formula_29">f T V [n] θ (0) b Y [n] = φ 1 • • • φ |V| ⎡ ⎢ ⎢ ⎣ θ (0) 11 • • • θ (0) 1|Y| . . . . . . . . . θ (0) |V|1 • • • θ (0) |V||Y| ⎤ ⎥ ⎥ ⎦ ⎡ ⎢ ⎢ ⎣ β 1 . . . β |Y| ⎤ ⎥ ⎥ ⎦ = φ 1 • • • φ |V| ⎡ ⎢ ⎢ ⎣ |Y| m=1 θ (0) 1m β m . . . |Y| m=1 θ (0) |V|m β m ⎤ ⎥ ⎥ ⎦ = |V| l=1 φ l |Y| m=1 θ (0) lm β m .</formula><p>This suggests that moving the Hadamard product inside the summation</p><formula xml:id="formula_30">θ (1) ←- N n=1 `(0) f V [n] b T Y [n] f T V [n] θ (0) b Y [n]</formula><p>and calculating the sum of all the elements of the matrix θ 0</p><formula xml:id="formula_31">f V [n] b T Y [n]</formula><p>at the same time of their generation, computational complexity of the algorithm can overall be reduced from N (3|Y| + 1)|V| to 2N |V||Y| multiplications, which is rather significant if considering that this reduction is relative to each algorithm recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Performance on LVM</head><p>To evaluate the computational advantages obtained by the proposed improvements let us consider the more general situation of Fig. <ref type="figure">3</ref>, in which the LVM (single-layer) model </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Cost of the inference phase</head><p>In the inference mode, at the beginning of the process, the backward messages of the N output variables will be postmultiplied by the probabilistic matrix inside the SISO blocks; thus producing O(N |P||Y|) operations before being sent to the Diverter. As seen previously, O((H + N )|P|) operations will be performed inside the Diverter, related to the multiplication of all the H + N incoming messages to it. Finally, the messages must be propagated again to the SISO blocks, which will be pre-multiplied by the matrices still producing O(N |P||Y|) operations, and thus making the total computational cost equal to O((N |Y| + H )|P|). At this point the forward messages of the Y 1 , . . . , Y N output variables will be available for analysis. Table <ref type="table" target="#tab_1">1</ref> summarizes the differences in computational terms determined by the introduced optimizations. To provide a practical example, suppose to perform an inferential process on an LVM network with 10 binary output variables and a single hidden variable, with an embedding space |S| of 10. It will be noted that while the direct algorithm will require 1500 multiplications, the optimized one will require only 670.</p><p>Moreover, the additional memory required by the optimized algorithm is still linear with the size of the inputs, since it depends only on the temporary vectors inside the Diverter. In fact, in the previous example, the significant computational advantage obtained will correspond to an increase in memory equal to only 10 vectors of size 10. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Cost of the batch learning phase</head><p>For what concerns the batch learning session, assume to have L different examples given in input through the backward messages of the output variables. As already mentioned, when the process begins backward messages entering the network will be saved inside the SISO blocks, requesting an amount of memory of O(L|Y|) individually. After storing the vector, the process will continue by multiplying it and the probability matrix; sending the result to the Diverter.</p><p>In the propagation phase the computational costs will not change with respect to the inference phase, but it must be remembered that the messages that will return to the SISO blocks will be stored again; individually producing an additional memory cost equal to O(L|P|). The total cost of additional memory required is thus</p><formula xml:id="formula_32">O(L N (|Y|+|P|)), while the computational cost is still O(L(N |Y| + H )|P|).</formula><p>Once this first phase has been completed, the ML algorithm will be executed at most K times (with K fixed a priori), trying to make conditional probability matrices converge, and the whole process is then repeated T times. Thus, the total computational cost of the batch learning session is equal to</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O(T L(K N|Y| + H )|P|).</head><p>The comparison between the computational costs of the direct and the optimized case is shown more clearly in Table <ref type="table" target="#tab_2">2</ref>.</p><p>Furthermore, it is easy to state that the repetition of the process for T epochs, as well as the various calls of the ML algorithm, do not imply the need for any additional memory units with respect to the non-optimized case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Incremental algorithm</head><p>The ML algorithm has many undoubted advantages, being very stable and generally converging in a few steps, but it obviously has the disadvantage of being batch (i.e. able to perform learning using only the entire training set). In order to obtain a lighter implementation we have changed the previous structure by requiring that at each epoch of the learning phase only one ML cycle (i.e. K = 1) is included. In other words, the algorithm has been made incremental, making it unnecessary to store backward messages within SISO blocks; and thus eliminating the need for the previous storage space equal to L(|P| + |Y|) for each SISO block.</p><p>Despite the great advantage in terms of both memory and computational costs, this type of approach has surprisingly proved to be as robust as the previous one, being less likely to provide overfitting of the data. Referring to a One-To-Many LVM structure (Fig. <ref type="figure" target="#fig_1">4b</ref>), in which the only hidden variable has an embedding space |S| of 20, various tests were performed on different datasets.</p><p>Table <ref type="table" target="#tab_3">3</ref> present the classification success rates, both on the training and on the test set, of three databases from the UCI repository: Wisconsin Breast Cancer Dataset <ref type="bibr" target="#b6">(Dheeru and Taniskidou 2017)</ref>, Mammographic Mass Data Set <ref type="bibr" target="#b7">(Elter et al. 2007)</ref> and Contraceptive Method Choice Data Set <ref type="bibr" target="#b6">(Dheeru and Taniskidou 2017)</ref>. The values presented are obtained by making sure that the latent variable is not learned, and therefore represent the learning ability of a single layer; being a good index to represent layer-by-layer learning of more complex networks. Note that, in this particular situation, the incremental algorithm does not give results that deviate much from the values obtained with the batch learning, providing in some cases even better results. It should also be noted that the results do not improve according to the adaptability of the paradigm to the specific case, since they are better even when the realized one-layer LVM network is probably not suitable for capturing the underlying implication scheme (as seen in the case of Contraceptive Method database).</p><p>Finally, the results prove even more interesting if we consider that in both cases (batch and incremental) they are obtained using the same number of epochs (in particular equal to 20). In fact, an incremental algorithm should typi- cally employ many more steps to achieve the performance of a batch algorithm, whereas in the cases under examination the change seems to bring only advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion and conclusions</head><p>In this work, an in-depth analysis of the individual elements necessary to create a Bayesian network through the FGrn paradigm was conducted, showing how it is possible to reduce memory and computational costs during implementation. The analysis led to the creation of a C++ library able to provide excellent results from a computational point of view, transforming polynomial costs into linear (respectively to the number of variables involved). The incremental use of the ML algorithm has finally demonstrated how it is further possible to reduce both the computational and memory costs of the learning phase in an unsupervised context. All these algorithmic choices are the basis for extending the FGrn paradigm to higher-scale problems. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 FGrn components: a a Variable with the associated forward and backward messages; b a Diverter representing the replication of M variables; c a SISO block with the conditional distribution matrix of the connected variables; d a Source with the prior distribution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 A</head><label>4</label><figDesc>Fig. 4 A N -tuple with the Latent Variable S and a Class Variable L drawn as a Bayesian Network, b Factor Graph in Reduced Normal Form</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 The basic class diagram for FGrnLib, that shows the dependencies between the classes</figDesc><graphic coords="6,53.65,56.09,487.69,156.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 Messages a inference phase; b batch learning phase</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8</head><label>8</label><figDesc>Fig.8Details of the efficient implementation of the products inside the Diverter, with in red the input messages and in blue the output messages. For each output message two contributions are used: one derived from the left part of the computational graph and the other one derived from the right part</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Computational cost differences for the inference phase between the classical and the optimized algorithm , ..., Y N . Since the variables Y 1 , ..., Y N can have different dimensions (never less than 2), before starting it is important to underline that we will assume that all the observed variables have the same dimension |Y|. This statement does not compromise in any way the goodness of the results obtained, for we can certainly decide to choose |Y| as the highest value possible being interested only in an upperbound description of computational complexity (in terms of big-O notation). For the same reason, we will avoid considering the case (more advantageous) in which some variables are not know. Note that, the previous problem does not arise in the case of input variables to the Diverter because they already have the same dimension |P| by construction.</figDesc><table><row><cell>Computational cost</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Computational cost differences for the batch learning phase between the classical and the optimized algorithm</figDesc><table><row><cell></cell><cell>Computational cost</cell></row><row><cell>Direct</cell><cell>O(T L(N (K |Y| + H + N ) + H 2 )|P|)</cell></row><row><cell>Optimized</cell><cell>O(T L(K N|Y| + H )|P|)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Comparison Breast Cancer = 10 useful variables, 699 instances, 16 missing values; Mammographic Mass = 6 variables, 961 instances, 162 missing values; Contraceptive Method = 10 variables, 1473 instances, 0 missing values</figDesc><table><row><cell>of the classification accuracy on three</cell><cell></cell><cell>Training set</cell><cell></cell><cell>Test set</cell><cell></cell></row><row><cell>different datasets between the</cell><cell></cell><cell>Batch (%)</cell><cell>Incremental (%)</cell><cell>Batch (%)</cell><cell>Incremental (%)</cell></row><row><cell>incremental and the batch algorithm</cell><cell>Breast cancer</cell><cell>96.6</cell><cell>96.2</cell><cell>95.48</cell><cell>97.99</cell></row><row><cell></cell><cell>Mammographic mass</cell><cell>86.62</cell><cell>85.88</cell><cell>79.5</cell><cell>78.88</cell></row><row><cell></cell><cell>Contraceptive method</cell><cell>58.1</cell><cell>58.9</cell><cell>49.89</cell><cell>50.52</cell></row><row><cell></cell><cell>Datasets composition:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Funding Open access funding provided by Università degli Studi della Campania Luigi Vanvitelli within the CRUI-CARE Agreement. This work has been partially sponsored by PON03PE-00185-1-2 MAR.TE., with Consorzio Nazionale Interuniversitario per le Telecomunicazioni (CNIT) -Italy.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Simulink is a MATLAB-based graphical programming developed by MathWorks (more information is available at https://www. mathworks.com/products/simulink.html).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Since the variable can only assume discrete values within the vocabulary, the two messages will be vectors of dimensions equal to the size of the vocabulary.</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of data and material</head><p>The only data used in the paper refer to the three public databases from the UCI repository: Wisconsin Breast Cancer Dataset <ref type="bibr" target="#b6">(Dheeru and Taniskidou 2017)</ref>, Mammographic Mass Data Set <ref type="bibr" target="#b7">(Elter et al. 2007</ref>) and Contraceptive Method Choice Data Set <ref type="bibr" target="#b6">(Dheeru and Taniskidou 2017)</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compliance with ethical standards</head><p>Conflicts of interest Author Giovanni Di Gennaro declares that he has no conflict of interest. Author Amedeo Buonanno declares that he has no conflict of interest. Author Francesco A.N. Palmieri declares that he has no conflict of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code availability</head><p>As stated in the paper, the C ++ library created is made available at <ref type="url" target="https://github.com/mlunicampania/FGrnLib">https://github.com/mlunicampania/FGrnLib</ref>.</p><p>Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ref type="url" target="http://creativecommons.org/licenses/by/4.0/">http://creativecomm ons.org/licenses/by/4.0/</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bayesian reasoning and machine learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning in graphical models</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="371" to="403" />
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note>Latent variable models</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simulink implementation of belief propagation in normal factor graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buonanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Palmieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural networks: computational and theoretical issues</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Towards building deep networks with bayesian factor graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buonanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Palmieri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04492</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Two-dimensional multi-layer factor graphs in reduced normal form</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buonanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Palmieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international joint conference on neural networks, IJCNN2015</title>
		<meeting>the international joint conference on neural networks, IJCNN2015<address><addrLine>Killarney, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-12">2015. July 12-17, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Cheeseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stutz</surname></persName>
		</author>
		<ptr target="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.7048" />
		<title level="m">Bayesian classification (AutoClass): theory and results</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Uci machine learning repository</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dheeru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Taniskidou</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The prediction of breast cancer biopsy outcomes using two cad approaches that both emphasize an intelligible decision process</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schulz-Wendtland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wittenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med Phys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4164" to="4172" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Codes on graphs: normal realizations</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Forney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Inf Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="520" to="548" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An introduction to factor graphs</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Loeliger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process Mag</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="41" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Software packages for graphical models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="http://www.cs.ubc.ca/~murphyk/Software/bnsoft.html" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective (adaptive computation and machine learning series)</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comparison of algorithms for learning hidden variables in Bayesian factor graphs in reduced normal form</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Palmieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw Learn Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2242" to="2255" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Belief propagation and learning in convolution multi-layer factor graph</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buonanno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the the 4th international workshop on cognitive information processing</title>
		<meeting>the the 4th international workshop on cognitive information processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discrete independent component analysis (dica) with belief propagation</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buonanno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE machine learning for signal processing conference</title>
		<meeting>IEEE machine learning for signal processing conference<address><addrLine>Boston, MA, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1920">2015. 2015. Sept. 17-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">San Francisco Publisher&apos;s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
		</imprint>
	</monogr>
	<note>Probabilistic reasoning in intelligent systems: networks of plausible inference</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
