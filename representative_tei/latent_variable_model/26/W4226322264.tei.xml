<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Triple-level Model Inferred Collaborative Network Architecture for Video Deraining</title>
				<funder ref="#_TCKUveV">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
				<funder ref="#_E5C9Qkv">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-11-08">8 Nov 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pan</forename><surname>Mu</surname></persName>
							<email>panmu@zjut.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yaohua</forename><surname>Liu</surname></persName>
							<email>liuyaohua918@163.com</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Risheng</forename><surname>Liu</surname></persName>
							<email>rsliu@dlut.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xin</forename><surname>Fan</surname></persName>
							<email>xin.fan@dlut.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University of Technology</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematical Sciences</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<region>China</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">DUT-RU International School of Information Science &amp; Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">DUT-RU International School of Information Science &amp; Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Triple-level Model Inferred Collaborative Network Architecture for Video Deraining</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-11-08">8 Nov 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2111.04459v1[eess.IV]</idno>
					<note type="submission">received April 19, 2005; revised August 26, 2015.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T18:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video deraining</term>
					<term>model optimization</term>
					<term>collaborative structure</term>
					<term>neural architecture search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video deraining is an important issue for outdoor vision systems and has been investigated extensively. However, designing optimal architectures by the aggregating model formation and data distribution is a challenging task for video deraining. In this paper, we develop a model-guided triple-level optimization framework to deduce network architecture with cooperating optimization and auto-searching mechanism, named Triple-level Model Inferred Cooperating Searching (TMICS), for dealing with various video rain circumstances. In particular, to mitigate the problem that existing methods cannot cover various rain streaks distribution, we first design a hyper-parameter optimization model about task variable and hyper-parameter. Based on the proposed optimization model, we design a collaborative structure for video deraining. This structure includes Dominant Network Architecture (DNA) and Companionate Network Architecture (CNA) that is cooperated by introducing an Attention-based Averaging Scheme (AAS). To better explore inter-frame information from videos, we introduce a macroscopic structure searching scheme that searches from Optical Flow Module (OFM) and Temporal Grouping Module (TGM) to help restore latent frame. In addition, we apply the differentiable neural architecture searching from a compact candidate set of taskspecific operations to discover desirable rain streaks removal architectures automatically. Extensive experiments on various datasets demonstrate that our model shows significant improvements in fidelity and temporal consistency over the state-of-theart works. Source code is available at <ref type="url" target="https://github.com/vis-optgroup/TMICS">https://github.com/vis-optgroup/TMICS</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W ITH the flourishing development of computer tech- nology, outdoor vision system plays a critical role in many real-world applications. For instance, the advent of low-cost technology in the field of video capture systems has made it easier for various organizations to adopt surveillance technology. However, bad weather harms perceptual Fig. <ref type="figure">1</ref>. Visual comparison with different video deraining methods on realworld rainy frames. Comparing with JORDER <ref type="bibr" target="#b0">[1]</ref>, J4RNet <ref type="bibr" target="#b1">[2]</ref>, FastDeRain <ref type="bibr" target="#b2">[3]</ref> and SPANet <ref type="bibr" target="#b3">[4]</ref>, our developed method (i.e., TMICS) performs the best visual quality under these various types of complex rain streaks obviously. performance and degrades video quality. This degradation could prejudice outdoor multimedia systems and influence the visibility of real-world images captured by camera drones. Thus, developing efficient rain streaks removal method is imperative for a wide range of computer vision tasks, such as video surveillance, intelligent vehicles, object detection, tracking, and remote sensing monitoring, etc.</p><p>An early study for video deraining introduced temporal median filters <ref type="bibr" target="#b4">[5]</ref> to deal with each pixel. In <ref type="bibr" target="#b5">[6]</ref>, by developing two separate models that capture the dynamics and the photometry of rain through its physical properties, a comprehensive model for the visual appearance of rain was proposed. Subsequently, a large number of research efforts have been dedicated to remove rain streaks from different rain scenes. The existing methods mainly include two categories: frequency domain-based methods and time domainbased schemes.</p><p>Frequency domain-based methods rely on the characteristic in frequency space. For example, in <ref type="bibr" target="#b6">[7]</ref>, by applying physical and statistical characteristics in the frequency domain, researchers derived a simple rain model to determine the general properties of a single streak. However, as for estimating rain streaks, frequency domain-based methods tend to roughly, and often show errors for complicated rain streaks.</p><p>To further improve the performance of video derain, some schemes focus on utilizing temporal dynamics information of consecutive frames to remove rain streaks from rain videos <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Specifically, by dividing rain streaks into two types (i.e., sparse and dense scenes), frequency domain-based matrix decomposition <ref type="bibr" target="#b9">[10]</ref> methodology was applied for removing rain streaks. Based on the discriminative characteristics of rain streaks, tensor-based video derain methods were developed <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b10">[11]</ref>. These schemes are not effective for heavy rain due to lacking of data information. Recently, data-driven based deep learning methodologies are popularly used in rain streak removal applications, such as CNN-based reconstruction network <ref type="bibr" target="#b1">[2]</ref>, recurrent dual-level flow network <ref type="bibr" target="#b12">[13]</ref> and self-learned network <ref type="bibr" target="#b13">[14]</ref>, etc. However, the performance of CNN-based methods depends on welldesigned architectures and subtle adjustments that are hard to be constructed.</p><p>To mitigate the above issues, we develop a model-guided auto-searching method for removing different video rain streaks. Specifically, based on the formulated triple-level video deraining optimization framework, a collaborative learning scheme is deduced via introducing cooperating optimization and network architecture searching mechanism, named as Triple-level Model Inferred Cooperating Searching (TMICS). The pipeline is shown in Figure <ref type="figure" target="#fig_1">2</ref>. Firstly, different from existing video deraining works that only concern motion case, we introduce a macroscopic structure searching scheme from Optical Flow-estimation Module (OFM) and Temporal Grouping Module (TGM) for inter-frames information extraction. Secondly, the existing learning-based video de-raining methods rely on training data and cannot cover rain streaks distribution information that differs significantly from training data. To mitigate this issue, we design a hyper-parameter optimization model about task variable and hyper-parameter. For the task variable propagation, we develop a collaborative structure, i.e., Dominant Network Architecture (DNA) and Companionate Network Architecture (CNA), to deduce the cooperating optimization procedure. In addition, we introduce an Attention-based Averaging Scheme (AAS) to effectively fuse features from collaborative structures by the guidance of hyper-parameter. Thirdly, the performance of CNN-based methods depends on well-designed architectures and subtle adjustments that are hard to be constructed. To deal with this problem, based on the above hyper-parameter optimization model, we apply the microscopic network architectures searching <ref type="bibr" target="#b14">[15]</ref> from a compact task-specific search space to discover desirable video deraining architectures. Finally, the ablation study demonstrates the effectiveness of the developed modules. Extensive evaluations illustrate that the proposed framework performs favorably against state-of-the-art video deraining methods. Figure <ref type="figure">1</ref> demonstrates the superiority of our proposed approach over existing methods in a challenging video sequence. The contributions are summarized as follows:</p><p>• We develop a triple-level model-driven framework with macroscopic structure searching and microscopic collaborative architecture searching schemes for video deraining. The designed model not only optimizes task variables (or network parameters), but also optimizes hyper-parameters and architecture weights. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Single Image Derain: Traditional single image deraining methods usually apply inherent physical features to characterize rain streaks. For example, in <ref type="bibr" target="#b15">[16]</ref>, sparse coding was applied to divide rain streaks from high frequency layer. Prior-based strategies explore prior knowledge to recover clear image from the rainy one, such as morphological component analysis <ref type="bibr" target="#b16">[17]</ref>, low-rank assumptions <ref type="bibr" target="#b17">[18]</ref>, guided filter <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, dictionary learning <ref type="bibr" target="#b20">[21]</ref>, Gaussian mixture model <ref type="bibr" target="#b21">[22]</ref> and joint convolutional analysis and synthesis sparse representation <ref type="bibr" target="#b22">[23]</ref>, etc. In recent years, deep learning-based methods govern rain streaks removal literature, such as shallow CNN-based schemes <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, dilated convolution <ref type="bibr" target="#b0">[1]</ref>, dense blocks <ref type="bibr" target="#b25">[26]</ref> and self-supervised method <ref type="bibr" target="#b26">[27]</ref>. Besides, modeldriven methodologies have also witnessed the rapid progress of deep learning in image deraining field <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Additionally, by introducing directional gradient operator of arbitrary direction, an efficient and robust constraints-based model was proposed in <ref type="bibr" target="#b32">[33]</ref>.</p><p>Video Derain: Different from single image rain streaks removal, video deraining can additionally make use of the temporal correlation and dynamics to explore the intrinsic properties. An early attempt method for video deraining was developed in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b33">[34]</ref> that utilized a space-time correlation model to analyze the visual effects of rain streaks. Thereafter, a variety of methods are proposed for video deraining. One ? ? ?</p><p>• • • ? ? ?</p><p>• • • ? ? ?</p><p>• • • ? ? ?</p><p>• • • ? ? ?</p><p>• • • ? ? ?</p><p>• • • ? ? ?  branch of these methods is to explore the inherent rain streaks priors and general background signals, such as morphological component analysis <ref type="bibr" target="#b34">[35]</ref>, spatio-temporal correlation of patch groups <ref type="bibr" target="#b35">[36]</ref>, directional prior of rain streaks <ref type="bibr" target="#b8">[9]</ref>, Gaussian mixture model <ref type="bibr" target="#b36">[37]</ref>, low-rank regularization <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b10">[11]</ref>, matrix decomposition <ref type="bibr" target="#b9">[10]</ref> and tensor model <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Deep learning based schemes have been investigated in rain streaks removal application <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. For example, in <ref type="bibr" target="#b1">[2]</ref>, a CNN based reconstruction network was developed for video rain streaks removal by integrating rain degradation classification, spatial texture appearances based rain removal.</p><formula xml:id="formula_0">• • • • • • ResBlock Layer 1-1 Layer 1-L • • • Layer 1-1 Layer 1-L TGM OFM</formula><p>In <ref type="bibr" target="#b12">[13]</ref>, a recurrent network was designed for synthesizing visually authentic rain videos to predict the rain-related variables and perform an inverse scheme to estimate rain-free frame.</p><p>Rain Model: In general, video rain model can be formulated as I t = B t + R t where t = 1, • • • , T means the timesteps of video frames, I t ∈ R m×n , B m×n t and R m×n t denote the captured image with rain streaks, rain-free background and rain streaks respectively. In <ref type="bibr" target="#b9">[10]</ref>, a reconstructed rain model was developed by separating rain streaks into two types, i.e. sparse and dense rain streaks. This model can be written as</p><formula xml:id="formula_1">I t = B t + F t + R s t + R d t ,</formula><p>where F t , R s t and R d t represent the intensity fluctuations caused by foregrounds, sparse and dense rain streaks respectively. In <ref type="bibr" target="#b12">[13]</ref>, rain accumulation and accumulation flow were considered in the following form</p><formula xml:id="formula_2">I t = β t B t + R t + (1 -β t )A t + U t ,</formula><p>where A t is the global atmospheric light, β t means the atmospheric transmission which is correlated with the scene depth and U t denotes the rain accumulation flow layer.</p><p>Neural Architecture Search: Neural Architecture Search (NAS) <ref type="bibr" target="#b40">[41]</ref> aims to discover high-performance task-specific architectures to replace heavy manual design automatically. Early search strategies for NAS apply evolutionary algorithm <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> and reinforcement learning <ref type="bibr" target="#b43">[44]</ref>, which achieve remarkable performance along with much inefficient computation, spent on architecture evaluations at the same time. In light of these computation issues, various of gradientbased differentiable approaches have been proposed.</p><p>By performing gradient-descent form for the bi-level search scheme, a continuous relaxation method is developed <ref type="bibr" target="#b14">[15]</ref> to optimize both the model weights and architecture parameters. Several differentiable architecture search algorithms also show comparable performance on low-level vision tasks. As for image restoration (e.g., image deraining), HiNAS <ref type="bibr" target="#b44">[45]</ref> searches for both inner cell architectures and outer layer widths to be memory and computation efficient. CLEARER <ref type="bibr" target="#b45">[46]</ref> combines multi-scale search space with task-flexible modules for image denoising and deraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR APPROACH</head><p>In this section, we first construct a triple-level optimization model to jointly optimize network architectures, task variables and hyper-parameters in Section III-A. Subsequently, we provide a detailed procedure to deduce the optimization model in Section III-B. Finally, the training details are illustrated in Section III-C. The overall procedure is presented in Algorithm 1 and the detailed pipeline is shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Formulation for Video Deraining</head><p>In this work, we take into account more complicated rain synthesis model and exploring the temporal information of rainy videos. With this goal, we first re-construct rainy video frame as the following model</p><formula xml:id="formula_3">I t = λ•(B t +R t )+(1-λ)•(B t +K⊗R t ), t = 1, • • • , T,<label>(1)</label></formula><p>where I t ∈ R m×n , B t ∈ R m×n and R t ∈ R m×n . Here • and ⊗ represent the element-wise multiplication and convolution operation respectively. K denotes the constructed model aiming to generate auxiliary rain streaks. λ ∈ R m×n means the weight between different rain streaks types. Indeed, R t is usually considered as independent and identically distributed sample.</p><p>Note that the main aim of video deraining is to find a clear background frame, i.e., B t . Under the above model ( <ref type="formula" target="#formula_3">1</ref>), the introduced hyper-parameter λ aims to help us find a better task variable. Thus, a hyper-parameter optimization model (see <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>) is introduced to jointly optimize task variable B t and hyper-parameter λ that can be formulated as the following scheme</p><formula xml:id="formula_4">min λ L D (λ, B t (λ); K, I t ) s.t., B t (λ) ∈ arg min L C (B t (λ); I t ) , I t = F {I p } T p=1 ,<label>(2)</label></formula><p>where L D and L C denote the loss function in the upperlevel and lower-level respectively, F(•) represents the temporal alignment module, and T is the input frame number. Indeed, module F(•) is designed for introducing frame information which is usually used through different forms in video derain methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>As for optimizing B t in Eq. ( <ref type="formula" target="#formula_4">2</ref>), it can be written as an averaging scheme based on <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Thus, for any fixed λ, we have the following averaging-based scheme</p><formula xml:id="formula_5">B t ← λ•arg min Bt L D (λ, B t ; K, I t ) ND(ω * D ;K,Dtr(It,Igt)) +(1-λ)•arg min Bt L C (B t ; I t ) NC(ω * C ;Dtr(It,Igt)) ,<label>(3)</label></formula><p>where N D and N C respectively denote the constructed Dominant Network Architecture (DNA) and Companionate Network Architecture (CNA). ω * D and ω * C represent the learned optimal network parameters for N D and N C respectively. I t and I gt represent the input rainy image of the current frame and the corresponding ground truth respectively. As for λ-subproblem, a single numerical parameter cannot well cooperate the constructed dominant and companionate network. Thus, with the above obtained B t , we introduce an Attention-based Averaging Scheme (AAS) to learn λ-subproblem.</p><p>Moreover, designing network by experience in video deraining requires substantial efforts. How to trade-off between automated search and experience-driven is a crucial task for discovering structure F and task-specific architectures (i.e., N := {N D , N C }). Motivated by the success of NAS in highlevel vision field <ref type="bibr" target="#b14">[15]</ref>, under the bi-level optimization scheme in Eq. ( <ref type="formula" target="#formula_4">2</ref>), we construct a triple-level optimization model to search network architectures for video rain streaks removal.</p><p>The above model can be reformulated as the following form</p><formula xml:id="formula_6">min α,β L arc (β, N D (ω * D (α)), N C (ω * C (α)); D val ) , s.t.,      ω * D (α) ∈ arg min ωD L D (λ, N D (ω D (α); K, D tr (I t , I gt ))), ω * C (α) ∈ arg min ωC L C (N C (ω C (α); D tr (I t , I gt ))), I t = F β; {I t } T t=1 ,<label>(4)</label></formula><p>where α and β are the architecture relaxation weights, D tr and D val denote training and validation datasets. Indeed, Eq. ( <ref type="formula" target="#formula_4">2</ref>) and Eq. ( <ref type="formula" target="#formula_6">4</ref>) imply a triple-level optimization model with network parameters ω = (ω D , ω C ) (the first level), architecture parameter α, β (the second level) and hyper-parameter λ (the third level). Then, we provide detailed procedure for finding the optimal solution of the above optimization formulation in the following section III-B. We summarize some necessary definitions in Table <ref type="table" target="#tab_2">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimization Procedure</head><p>With the developed optimization models in Eqs. ( <ref type="formula" target="#formula_4">2</ref>)-( <ref type="formula" target="#formula_5">3</ref>), this part introduces microscopic architecture searching for N = {N D , N C } and macroscopic structure searching for frame alignment module F. Specifically, we construct a collaborative rain streaks removal structure, i.e., N D and N C , to jointly characterize different rain streaks distribution. Then, with the above structures, we design an attention-based module to simulate λ.</p><p>1) Optimizing Hyper-parameter: As for λ, we design an attention mechanism to obtain an adaptive map that can directly learn inter-channel information from global context and contribute to our performance. Indeed, the attentionbased module auto-cooperates different video rain streaks distribution and helps improve the generalization capability of the network. In detail, given two restored frames, we perform two shared 3 × 3 convolutions to extract features and then introduce the global average pooling to encode weights of each spatial channels. Soft-max operator is adopted to generate λ for each estimated frame.</p><p>2) Macroscopic Structure Search for Frame Alignment Scheme F: Choosing suitable alignment modules to estimate spatial-temporal information from task-specific data distribution is important for video deraining. In this work, we introduce a macroscopic structure search mechanism to select a suitable alignment module. In other words, we choose either TGM or OFM for frame alignment module F by NAS. Indeed, we can formulate this search strategy using relaxation weights,</p><formula xml:id="formula_7">F β; {I t } T t=1 = β 1 F OFM ({I t } T t=1 ) + β 2 F TGM ({I t } T t=1 )</formula><p>, where i β i = 1. We leverage these hybrid aligned features to feed N in the search phase. The detailed structure is shown in Figure <ref type="figure">3</ref>.</p><p>For the motion estimation (i.e., OFM), by applying recurrent all-pairs field transforms as stated in <ref type="bibr" target="#b52">[53]</ref>, we introduce an optical flow prediction to obtain the spatial-temporal information from a sequence video frames {I t } t=1,...,T . With this pre-trained network, it produces a series of aligned frames for subsequent spatial-temporal information extraction, F OFM {I t } T t=1 = I t + u, where u means the updated flow.</p><p>For TGM, the input sequence is divided into two groups <ref type="bibr" target="#b53">[54]</ref>. Specifically, given a consecutive rainy video frame sequence, we select 2M + 1 neighboring frames to calculate the reference frame I t . The two groups are</p><formula xml:id="formula_8">G 1 := {I t-2M , • • • , I t-2 , I t , I t+2 , • • • , I t+2M }, G 2 := {I t-2M +1 , • • • , I t-1 , I t , I t+1 , • • • , I t+2M -1 }.</formula><p>Note that each group represents a certain type of frame rate. Then, the residual block and fusion block with shared weights are applied to extract and fuse spatial-temporal information within the above two groups,</p><formula xml:id="formula_9">F TGM {I p } T p=1 = Concat(N Res (G 1 ), N Res (G 2 ))</formula><p>, where N Res is the residual block as stated in Figure <ref type="figure">3</ref>.</p><p>3) Constructing Auxiliary Rain Streaks: After the above calculation, we roughly estimate rain streaks by using a simple residual network structure N Res which can be written as R t = N Res (I t ). We apply the estimated rain streaks R t to encode the physical structural factors underlying rain streaks which can be written as</p><formula xml:id="formula_10">R t ← N i=1 W i * (K i ⊗ R t ) + R t .</formula><p>This model enables the capability of characterizing a wide range of rain streaks, such as small/large, blur rain streaks etc. Here K i represents the constructed filter, W i denotes the corresponding weight, and * means multiply operation. In this model, we apply two groups of filters to simulate rain types. Consequently, the re-constructed frames Ĩt can be</p><formula xml:id="formula_11">Ĩt ← M(K; I t ) := I t + R t .</formula><p>As real-world rainy scene usually contains many rain streaks cases, thus it is troublesome to cover intricate rain streaks distribution (such as heavy or combined rain streaks) with simple rain model. The constructed model provides us an opportunity to enable the capability of characterizing a wide range of rain streaks, such as small/large, blur rain streaks etc.</p><p>4) Microscopic Architecture Search for N : Generally, designing an efficient task-specific neural network structure (i.e., N ) requires substantial architecture engineering. In other words, it is significant but difficult to decide which module and how many convolutions, dilation, residual blocks are applied in a network. Focus on these key points, we apply the differentiable architecture search strategy <ref type="bibr" target="#b14">[15]</ref> from a discrete set of candidate operation cells to discover the task-specific DNA and CNA network simultaneously. These cells consist of an ordered sequence of L nodes to automatically discover desirable rain streaks removal architectures, and the pipeline is shown in Figure <ref type="figure" target="#fig_6">4</ref>. For each intermediate, it relaxes the discrete architecture parameters into a continuous distribution to perform a differentiable search.</p><p>Task-oriented Search Space. As proper search space is cru- Estimating frame content with I t ← F β; {I p } T p=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Obtaining complex rain streaks by Ĩt ← M(K; I t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Learning microscopic architecture N through: 5:</p><p>while not converged do   Output ω * D , ω * C , β, α and λ. 12: end while cial for finding the architecture backbone, we introduce a series of basic operators as the compact candidate search space, including residual blocks, dense blocks, dilated convolutions and attention mechanisms, which have been corroborated their effectiveness for rain removal tasks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>. The pre-defined operators are listed in the following:</p><p>• Residual: 3 × 3 and 5 × 5 residual blocks;</p><p>• Dense: 3 × 3 and 5 × 5 dense blocks;</p><p>• Attention: spatial attention and channel attention;</p><p>• Dilated: 3 × 3 and 5 × 5 dilated convolution with DF=2, where DF denotes the dilated rate. Note that each convolution is followed by a batch normalization layer and a ReLU activation layer, and all operators include three layers of convolutions. We take l-th layer as an example and calculate the choice of a particular operation by relaxing networks with a soft-max operation. Let x l be the input of the l-th layer and   l ∈ {1, • • • , L}, then we have</p><formula xml:id="formula_12">• • • • • • • • • • • • Layer Layer Layer • • •. • • •</formula><formula xml:id="formula_13">x l+1 = N l i=1 α i l f i l (x l ),<label>(5)</label></formula><p>where f i l denotes the operation realized by the i-th operation at l-th layer and α i l means the corresponding weight. Here α i l ∈ [0, 1] and i α i l = 1. The Final Derived Architectures. Subsequently, we can leverage the differentiable search to perform Eq. ( <ref type="formula" target="#formula_6">4</ref>). More specific, we set four layers of intermediates to construct the basic blocks (i.e., cells). Then we cascade four cells to establish the concrete architectures (i.e., DNA and CNA) at the searching and training phase. More searching and training configurations are shown in the following Section III-C. It is worth emphasizing that the relaxed weights are shared by both networks to simplify the searching procedure. Derived from the searching phase, we can obtain the final architectures for light and heavy rain scenarios respectively. Specifically, as for the micro search, the cell for light rain consists of 3×3 residual block, spatial attention, 3 × 3 dilated convolutions and channel attention orderly. Moreover, the cell for heavy rain includes 5 × 5 residual block, spatial attention, channel attention and 3 × 3 residual block, which indicates cells for heavy rain have the wider respective fields and residual information to capture long-range rain streaks reasonably. The attention mechanisms and dilated convolutions are also effective and verified in previous works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b0">[1]</ref>. As for temporal alignment module F, TGM has been searched for heavy rain and OFM has been searched for light rain. That may be caused by the serious warping error of OFM for heavy rain.</p><p>In summary, designing an appropriate deep network architecture for each task and data set is tedious and timeconsuming. The neural architecture search strategy attempts to find suitable architecture automatically and provides a tradeoff between automated search and experience-driven. Indeed, the designed microscopic automatically searched architectures from a task-specific search space to discover desirable video deraining architectures. Further, the macroscopic structure searching scheme combines optical flow-estimation and temporal grouping module to jointly concern motion and rain streaks occlusion circumstance. Besides, the designed collaborative scheme can preserve detail and structure of video when estimating various rain streaks distribution. Thus, the derived model are efficient compared with some existing handcrafted networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>In this part, we report more details about the loss functions, searching configurations and training configurations.</p><p>Loss Functions. As for L D and L C described in Section III, we apply the negative SSIM loss (i.e., L SSIM , see <ref type="bibr" target="#b30">[31]</ref>) and 1 loss between the restored video frames x (i.e., x = N D (ω D (α))) and their ground truths y as</p><formula xml:id="formula_14">L D = - 1 M M i=1 L SSIM (x, y) + ρ x -y 1 , (<label>6</label></formula><formula xml:id="formula_15">)</formula><p>where ρ is a parameter. The same loss function is applied for</p><formula xml:id="formula_16">L C with x = N C (ω C (ω)).</formula><p>As for L arc , it is constructed as</p><formula xml:id="formula_17">L arc = L D + ηL reg , (<label>7</label></formula><formula xml:id="formula_18">)</formula><p>where η is a positive parameter and L reg is the regularization about architecture parameter  Actually, the above L reg will impose the distribution of α l to ignore the weakest candidates gradually for discovering the most suitable operation of i-th layer effectively. Search Configurations. Considering the different distribution of rain streaks, we choose the RainSynLight25 and RainSynComplex25 as the datasets to search the diverse optimal architectures respectively. During the searching phase, we only sample twenty groups of rainy frames to search 80 epochs with batch size of 4. Furthermore we define the train and validation loss using Eq. ( <ref type="formula" target="#formula_17">7</ref>) with ρ = 0.75 and η = 0.01. The SGD optimizer is employed for the search phase. The learning rate of searching architecture is 1e -4 . On the other hand, the initial learning rate of updating parameters is 3e -4 and decays to 10 -6 with cosine annealing strategy. In order to provide a warm start for network parameters, we only update network weights before the first 30 epochs.</p><formula xml:id="formula_19">L reg = - L l=1 N i=1 α i l log(α i l ).</formula><p>Training Configurations. Derived from the searching phase, we perform two-stage training strategy. First, we train the cooperate architecture (i.e., DNA and CNA) end-to-end with 1 loss and negative SSIM loss (i.e., Eq. ( <ref type="formula" target="#formula_14">6</ref>) with ρ = 0.75). We exploit the cosine annealing strategy to decay the initial learning rate 3 × 10 -4 to 10 -6 . Employing the Adam optimizer and cropping the patches of size 240 × 240 randomly, we train the whole networks for 160 epochs. Data augmentations, including the horizontal and vertical flipping are implemented. After obtaining the pre-trained part of above networks, we train the fused attention mechanism leveraging a small learning rate of 1e -6 for 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>To evaluate the proposed method, we first conduct kinds of detailed ablation studies to analyze the effectiveness of frame aligned modules, auto-searching architectures, and rain streaks generating module. Subsequently, a series of qualitative and quantitative assessments are performed comparing with existing state-of-the-art approaches. Experimental results demonstrate the effectiveness and superiority of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ablation Experiments</head><p>Datasets. We compare the proposed method with state-ofthe-arts on RainSynLight25, RainSynComplex25 and NTU-Rain datasets. The ablation experiments are conducted on LasVR and hybrid SynComplex&amp;Light datasets. The detailed information (e.g., training / test numbers and rain streak types) are summarized in Table. II.</p><p>Metrics. We use two most widely used numerical metrics, i.e., Peak Signal-to-Noise Ratio (PSNR) and Structure Similarity Index (SSIM) to evaluate the performance of different methods. Following previous works, results are evaluated on luminance channel. To further measure the performance of different methods, we also analyze the perceptual quality based  on Visual Information Fidelity (VIF) <ref type="bibr" target="#b56">[57]</ref>, Feature SIMilarity (FSIM) <ref type="bibr" target="#b57">[58]</ref>, Natural Image Quality Evaluator (NIQE) <ref type="bibr" target="#b58">[59]</ref>, Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b59">[60]</ref> and tLPIPS <ref type="bibr" target="#b60">[61]</ref>. Higher PSNR, SSIM values imply a better pixelwise accuracy, and lower NIQE, LPIPS and tLPIPS values represent better perceptual quality. Furthermore, higher VIF and FSIM also indicate more visual-pleasant results. To evaluate the best frame number, we conduct an experiment on RainSynCom-plex25 and LasVR datasets about three neighborhood frame settings (i.e., 3, 5 and 7). The corresponding experimental results are listed in Table <ref type="table" target="#tab_4">III</ref>. Observed that unsuitable frames damage the performance in both RainSynComplex25 and LasVR datasets. Thus, in following experiments, we utilize five continuous observations to estimate one rain-free frame.</p><p>Impact of the Developed Triple-level Model. To demonstrate the effectiveness of the propose triple-level model, we plotted one group of representative results from RainSyn-Complex25 in Figure <ref type="figure">5</ref>. Obviously, the result of without DNA still has some residual rain streaks. The result without CNA coupling with rain streaks generation module obtains promising visual results and remove the dominant rain streaks. Our proposed AAS (i.e., with CNA + DNA) generates the most vivid background and achieve rain-free performance. This is because the developed AAS scheme helps preserve both structure and details when removing more rain streaks.</p><p>We then explore the hyper-parameter λ for AAS by comparing different settings and the experimental results are shown in Figure <ref type="figure" target="#fig_3">6</ref>. Specifically, by choosing a fixed λ (i.e., λ = 0, 0.5, 0.7), the performance still has space to be improved. Fortunately, the proposed attention mechanism can provide adaptive λ and incorporate networks leading to a better result. Influence of Different Settings. To validate the effectiveness of each module in our developed method, we make a comprehensive ablation study about five different settings, named Optical Flow Module (OFM), Temporal Grouping Module (TGM), Manually Designed Architecture (MDA), Auto-Searching Architecture (ASA), and Generating Auxiliary Rain Streaks (GARS). The corresponding numerical results are summarized in Table <ref type="table" target="#tab_5">IV</ref>.</p><p>Impact of Networks Architecture Search Setting: To investigate the proposed ASA for collaborative networks, we compare this strategy with manually designed network (i.e., MDA). Specifically, for the manually designed structure, we stack 3×3 residual block, dense block, dilated convolution and spatial attention orderly from searching space to design the basic blocks and manually designed network. The experimental results are listed in Table <ref type="table" target="#tab_5">IV</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with State-of-the-Art</head><p>Comparing on Synthesized Datasets. We compare the developed method with some state-of-the-art approaches on video deraining, including both single-image deraining methods (i.e., JOint Rain DEtection and Removal network (JORDER <ref type="bibr" target="#b0">[1]</ref>), Density-aware Single-Image Deraining using Multi-stream Dense Network (DID-MDN <ref type="bibr" target="#b25">[26]</ref>), SPatial Attentive single-image deraining network (SPANet <ref type="bibr" target="#b3">[4]</ref>)) and video deraining methods (i.e., Multi-Scale Convolutional Sparse Coding (MS-CSC <ref type="bibr" target="#b11">[12]</ref>), FastDeRain <ref type="bibr" target="#b2">[3]</ref>, Joint Recurrent Rain Removal and Reconstruction Network (J4R-Net <ref type="bibr" target="#b1">[2]</ref>), Super-Pixel Alignment and Compensation CNN (SpacCNN <ref type="bibr" target="#b54">[55]</ref>), DualFlow <ref type="bibr" target="#b12">[13]</ref>, CLEARER <ref type="bibr" target="#b45">[46]</ref> and Self-Learned Deraining Network (SLDNet <ref type="bibr" target="#b13">[14]</ref>)). The performance of video rain streaks removal is evaluated on three rainy video benchmarks: RainSynLight25, RainSynComplex25 and NTURain, which involve diversified kinds of rain streaks including direction, scale, density and intensity.</p><p>As for the quantitative comparison, we calculated two widely used metrics (i.e., PSNR and SSIM), and listed experimental results in Table <ref type="table" target="#tab_6">V</ref>. It can be seen that our approach shows significant superiority to previous methods on three datasets. Compared with recently proposed DualFlow, our method attains more than 0.85dB, 1.77dB and 1.33dB in PSNR on RainSynLight25, RainSynComplex25 and NTU-Rain datasets respectively. The result of single DNA (i.e., TMICS S) also obtains promising performances. This corroborates the flexibility and universality of our proposed method when dealing with various video situation with different rain streaks types. Except for PSNR and SSIM metrics that measure performance by pixel-wise accuracy, we further calculate NIQE, LPIPS and tLPIPS to evaluate the quantified perceptual quality. We compare our method with four methods which have relative high PSNR and SSIM scores (i.e., JORDER, FastDerain, J4R-Net and SpacCNN), and the comparison results on challenging RainSynComplex25 are listed in Table <ref type="table" target="#tab_6">VI</ref>.</p><p>As the lower NIQE, LPIPS and tLPIPS values denote better perceptual quality, our method achieves the best perceptual performance on different evaluating metrics.</p><p>As for the qualitative performance, Figure <ref type="figure" target="#fig_8">7</ref> and Figure <ref type="figure" target="#fig_4">8</ref> show the visual comparisons between our scheme and four best methods with relative high PSNR and SSIM scores (i.e., FastDerain, J4R-Net, JORDER and SpacCNN). Observed that in Figure <ref type="figure" target="#fig_8">7</ref>, the proposed method performs the best while other methods (i.e., FastDeRain, JORDER, J4RNet, and SpacCNN) still have rain streaks. Besides, Figure <ref type="figure" target="#fig_4">8</ref> depicts the visual performance on NTURain dataset. Observed that FastDerain, J4R-Net, and JORDER still have rain streaks and mistake the background details as rain streaks. SpacCNN generates a much blurred background. Overall, our method provides more effective performance with less remaining rain streaks, abundant details, and less blurs.</p><p>Comparing on Real-world Video Datasets. To further illustrate the performance of our method, we chose three difficult real-world rainy videos with various rain circumstances against a series of competitive methods, including MS-CSC, FastDerain, JORDER, SPANet, J4R-Net and SpacCNN. These rainy video sequences are collected from different scenes, i.e., YouTube 1 , movie clips, and Mixkit 2 . As shown in Figure <ref type="figure" target="#fig_5">9</ref>, previous methods tend to leave distinct rain streaks and mistake background details as rain streaks. Fortunately, our developed method shows good capability that preserves background details as well as removes more rain streaks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This work developed a model-guided auto-searching method by the formulated triple-level video deraining optimization framework for removing different video rain streaks. We first introduced a macroscopic structure searching scheme for inter-frames information extraction. Then, we designed a cooperating optimization model about task variables and hyper-parameter based on the re-constructed comprehensive model. For the task variable propagation, we designed two collaborative structures, i.e., DNA and CNA. Subsequently, we introduced an attention-based averaging scheme to effectively fuse features from collaborative structures. To obtain stateof-the-art neural network structures (i.e., DNA and CNA), we applied the microscopic network architectures searching from a compact task-specific search space to discover desirable video deraining architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Schematic of proposed framework TMICS for video deraining. (I): A rough framework based on the designed model in Eq. (2). (II): The detailed procedure of TMICS with deep network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Algorithm 1</head><label>31</label><figDesc>Fig. 3. A diagram of the architecture to illustrate OFM and TGM. Five consecutive frames are used to derain the middle frame. "Concat" means the concatenation operation. Algorithm 1 Procedure for solving the model in Eq. (4) Require: Input necessary parameters. Ensure: x * . 1: while not converged do 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>6 :</head><label>6</label><figDesc>Updating ω * D (α), ω * D (α) with weighted shared α by ω * D (α) ∈ arg min ωD L D (λ, N D (ω D (α); K, D tr (I t , I gt ))), ω * C (α) ∈ arg min ωC L C (N C (ω L (α); D tr (I t , I gt ))).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>8 :</head><label>8</label><figDesc>Updating architecture α and β by gradient descent:β ∈ arg min β L arc (β, N D (ω * D (α)), N C (ω * C (α)); D val ). α ∈ arg min α L arc (β, N D (ω * D (α)), N C (ω * C (α)); D val ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>9 :</head><label>9</label><figDesc>With the obtained ω * D and ω * C , updating λ by 10: λ ∈ arg min L D (λ, N D (ω * D ), N D (ω * C ); K, D tr ).11:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. A diagram of the auto-searching architecture to illustrate the optimization procedure of one cell. (a) The unknown units in initial cell. (b) The searching procedure on candidate units with continuous relaxation. (c) The final obtained cell through the learned relaxation weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. Ablation study about the triple level models. PSNR and SSIM scores of a video sequence are plotted in the top row with three different settings, i.e., w / o CNA, w / o DNA, w / DNA + CNA. The bottom row shows their visual performances.</figDesc><graphic coords="7,307.66,323.41,153.23,85.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Video deraining performance comparison on two videos from RainSynLight25 (the top row) and RainSynComplex25 (the bottom row) respectively.</figDesc><graphic coords="8,55.26,334.41,98.70,100.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>FastDerainFig. 8 .</head><label>8</label><figDesc>Fig. 8. Video deraining results on two types of videos (i.e., synthetic and real frames in the top and bottom row respectively) from NTURain dataset.</figDesc><graphic coords="9,55.26,178.71,98.70,134.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>-(a) and (b). Observed that model (b) with auto-searching strategy outperforms manually designed model (i.e., model (a)) in terms of PSNR and SSIM scores. The superiority of the proposed ASA module demonstrates that the network searching architecture can effectively exploit basic operators in search space. Impact of GARS Module: We also compare the effectiveness of GARS in Table IV-(d) and (e). The collaborative networks with GRS obtains significant improvements based on the auxiliary information of structural factors. This is mainly because the generated auxiliary rain streaks enable the capability of network to characterize a wide range of video circumstances. Frames Estimation Modules: To explore the proposed alternative strategy, we introduce three baseline models, i.e. (b) only with OFM, (c) only with TGM and (d) with alternative OFM or TGM. As can be seen in Table IV, model (d) outperforms (b) and (c) both on SynHybrid and LasVR datasets. This experiment illustrate the effectiveness of the developed alternative module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Video deraining performance comparison on three real-world rainy videos. The remaining rain streaks are marked with red boxes. The background details are marked with a yellow arrow.</figDesc><graphic coords="10,54.94,436.40,125.94,70.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>To mitigate the problem that existing methods cannot cover various rain streaks distribution, we design a hyperparameter optimization model which has the ability to estimate various rain streaks. We design a collaborative structure, i.e., DNA and CNA, via AAS to preserve details and structure. This structure helps improve the generalization capability of network.</figDesc><table /><note><p><p>• We apply automatically microscopic search strategy from task-specific search space to discover desirable network. To find a suitable video frame estimation module, we design a macroscopic structure searching scheme via combining OFM and temporal TGM. The above search strategies provide a trade-off between automated search and experience-driven.</p>• Extensive experiments demonstrate the effectiveness of our proposed method and show comparable results against state-of-the-art video deraining methods on different video rainy benchmarks.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>• • • • • • Input Frames Frame Alignment Module Macroscopic Structure Search Microscopic Architecture Search 3×3 3 Dense Block 5×5 4 Dense Block 5 Dilated Block 6 Dilated Block Searching Space Attention Module Attention Module 7</head><label></label><figDesc></figDesc><table><row><cell>5×5</cell><cell>5×5</cell><cell>5×5</cell><cell>3×3</cell><cell>3×3</cell><cell>3×3</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell>2</cell></row><row><cell cols="3">Residual Block</cell><cell cols="3">Residual Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Multiplication</cell><cell>Conv 3×3</cell><cell>OFM: Optical Flow Module</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Addition</cell><cell>AvgPool</cell><cell>TGM: Temporal Grouping Module</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Spatial Attention</cell><cell>8 Channel Attention</cell><cell>Softmax operator</cell><cell>Element-wise multiplication</cell><cell>Search cells</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I SUMMARY</head><label>I</label><figDesc>OF SOME IMPORTANT NOTATIONS.</figDesc><table><row><cell>Notation</cell><cell></cell><cell>Description</cell><cell></cell><cell>Notation</cell><cell>Description</cell><cell>Notation</cell><cell>Description</cell></row><row><cell>•</cell><cell cols="3">Element-wise multiplication</cell><cell>I t</cell><cell>Input frame</cell><cell>F</cell><cell>Frame alignment scheme</cell></row><row><cell>⊗</cell><cell></cell><cell cols="2">Convolution operation</cell><cell>B t</cell><cell>Background frame</cell><cell>F OFM</cell><cell>Optical flow-estimation module</cell></row><row><cell>*</cell><cell></cell><cell>Multiply</cell><cell></cell><cell>R t</cell><cell>Rain streaks</cell><cell>F TGM</cell><cell>Temporal grouping module</cell></row><row><cell>K</cell><cell></cell><cell cols="2">Constructed filter</cell><cell>α</cell><cell cols="2">Architecture weights N := (N D , N C )</cell><cell>Task-specific architectures</cell></row><row><cell>λ</cell><cell></cell><cell cols="2">Hyper-parameter</cell><cell>β</cell><cell>Architecture weights</cell><cell>N Res</cell><cell>Residual Block</cell></row><row><cell>M</cell><cell cols="3">Generating complex input frame</cell><cell>ω</cell><cell>Network parameters</cell></row><row><cell cols="2">(a)Unkonwn Cell</cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell></row><row><cell cols="2">(b) Candidate</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Units</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(c) Obtained</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cell</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II SUMMARY</head><label>II</label><figDesc>OF THE TRAINING / TEST IMAGE NUMBER AND RAIN STREAKS TYPES OF FOUR DIFFERENT DATASETS USED IN THIS WORK.</figDesc><table><row><cell>Dataset</cell><cell>Image Num. training / test</cell><cell>Rain Types</cell></row><row><cell cols="2">RainSynLight25 1900 / 775</cell><cell>sythetic rain streaks [2]</cell></row><row><cell cols="2">RainSynComplex25 1900 / 775</cell><cell>sythetic rain streaks [2]</cell></row><row><cell>NTURain</cell><cell cols="2">2400 / 1682 sythetic and real rain streaks [55]</cell></row><row><cell>LasVR</cell><cell>-/ 200</cell><cell>sythetic rain streaks [56]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>(PSNR AND SSIM) OF AN ABLATION STUDY ABOUT DIFFERENT FRAMES ON RAINSYNCOMPLEX25 AND LASVR DATA SETS. RainSynComplex25 25.69 / 0.8276 28.90 / 0.8743 23.91 / 0.7731 LasVR 32.36 / 0.9063 33.41 / 0.9102 32.16 / 0.9070</figDesc><table><row><cell>Frames</cell><cell>3</cell><cell>5</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>(PSNR AND SSIM) OF ESTIMATING DIFFERENT STRATEGIES, INCLUDING MDA (MANUALLY DESIGNED ARCHITECTURE), ASA (AUTO-SEARCHING ARCHITECTURE), GARS (GENERATING AUXILIARY RAIN STREAKS), ON SYNHYBRID AND LASVR DATA SETS. LasVR 31.3549 32.6170 32.6902 33.1070 33.4157 0.9042 0.9074 0.9088 0.9120 0.9144</figDesc><table><row><cell>Settings</cell><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell><cell>(e)</cell></row><row><cell>OFM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TGM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MDA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GARS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SynHybrid</cell><cell cols="5">30.35 32.7369 32.8993 33.9986 34.1054 0.9058 0.9347 0.9376 0.9411 0.9424</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V AVERAGED</head><label>V</label><figDesc>PSNR AND SSIM RESULTS AMONG DIFFERENT RAIN STREAKS REMOVAL METHODS ON THREE SYNTHESIZED VIDEO DATASETS. TMICS S DENOTES THE RESULT FROM SINGLE DNA. RED AND BLUE COLORS ARE USED TO INDICATE TOP 1 st AND 2 nd RANK, RESPECTIVELY. .7312 31.57 / 0.9508 23.78 / 0.8140 30.37 / 0.9235 27.46 / 0.8844 32.96 / 0.9434 RainSynComplex25 16.57 / 0.5833 26.92 / 0.8011 17.51 / 0.5888 20.20 / 0.6335 18.25 / 0.5824 24.13 / 0.7163 NTURain 25.63 / 0.7600 30.54 / 0.9255 25.67 / 0.8819 32.69 / 0.9451 30.58 / 0.9413 30.73 / 0.9407 NTURain 33.11 / 0.9475 29.02 / 0.9158 36.05 / 0.9676 34.89 / 0.9540 36.64 / 0.9702 37.38 / 0.9704</figDesc><table><row><cell>Methods</cell><cell>MS-CSC</cell><cell>FastDeRain</cell><cell>DID-MDN</cell><cell>JORDER</cell><cell>SPANet</cell><cell>J4R-Net</cell></row><row><cell cols="2">RainSynLight25 24.43 / 0Methods SpacCNN</cell><cell>CLEARER</cell><cell>DualFlow</cell><cell>SLDNet</cell><cell>TMICS S</cell><cell>TMICS</cell></row><row><cell>RainSynLight25</cell><cell cols="3">32.78 / 0.9239 27.08 / 0.8858 35.80 / 0.9622</cell><cell>-/ -</cell><cell cols="2">36.10 / 0.9674 36.65 / 0.9689</cell></row><row><cell cols="4">RainSynComplex25 21.21 / 0.5854 17.51 / 0.7108 27.72 / 0.8239</cell><cell>-/ -</cell><cell cols="2">28.90 / 0.8743 29.49 / 0.8933</cell></row><row><cell>FastDerain</cell><cell>J4R-Net</cell><cell></cell><cell>JORDER</cell><cell>SpacCNN</cell><cell></cell><cell>TMICS (Ours)</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported by the <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2020YFB1313503</rs>), the <rs type="funder">National Natural Science Foundation of China</rs> (Nos. <rs type="grantNumber">61922019</rs>) and the <rs type="funder">Fundamental Research Funds for the Central Universities</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TCKUveV">
					<idno type="grant-number">2020YFB1313503</idno>
				</org>
				<org type="funding" xml:id="_E5C9Qkv">
					<idno type="grant-number">61922019</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep joint rain detection and removal from a single image</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="1685" to="1694" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Erase or fill? deep joint recurrent rain removal and reconstruction in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="3233" to="3242" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fastderain: A novel video rain streak removal method using directional gradient priors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2089" to="2102" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatial attentive single-image deraining with a high quality real rain dataset</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">279</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simulation of rain in videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Starik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Texture Workshop, ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="406" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detection and removal of rain from videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="I" to="I" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatio-temporal frequency analysis for removing rain and snow from videos</title>
		<author>
			<persName><forename type="first">P</forename><surname>Barnum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Photometric Analysis for Computer Vision</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A rain pixel recovery algorithm for videos with highly dynamic scenes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1097" to="1104" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A novel tensor-based video rain streaks removal approach via utilizing discriminatively intrinsic priors</title>
		<author>
			<persName><forename type="first">T.-X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4057" to="4066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video desnowing and deraining based on matrix decomposition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4210" to="4219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video deraining via nonlocal low-rank regularization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematical Modelling</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="896" to="913" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video rain streak removal by multiscale convolutional sparse coding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="6644" to="6653" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Frame-consistent recurrent video deraining with dual-level flow</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1661" to="1670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-learning video rain streak removal: When cyclic consistency meets temporal correspondence</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1720" to="1729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic single-image-based rain streaks removal via image decomposition</title>
		<author>
			<persName><forename type="first">L.-W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1742" to="1755" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single-frame-based rain removal via image decomposition</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICASSP</title>
		<imprint>
			<biblScope unit="page" from="1453" to="1456" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transformed low-rank model for line pattern noise removal</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1726" to="1734" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Single-image-based rain and snow removal using multi-guided filter</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="258" to="265" />
		</imprint>
	</monogr>
	<note>in NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single image rain and snow removal via guided l0 smoothing filter</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="2697" to="2712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Removing rain from a single image via discriminative sparse coding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICCV</title>
		<imprint>
			<biblScope unit="page" from="3397" to="3405" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rain streak removal using layer priors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2736" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint convolutional analysis and synthesis sparse representation for single image layer separation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1708" to="1716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Removing rain from single images via a deep detail network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3855" to="3863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Progressive image deraining networks: a better and simpler baseline</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3937" to="3946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Density-aware single image de-raining using a multi-stream dense network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="695" to="704" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards scale-free rain streak removal via self-supervised fractal band learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Structural residual learning for single image rain removal</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Knowledge-Based Systems</publisher>
			<biblScope unit="page">106595</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning bilevel layer priors for single image rain streaks removal</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="307" to="311" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Flexible bilevel image layer modeling for robust deraining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICME</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single image deraining: From model-based to data-driven and beyond</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Investigating customization strategies and convergence behaviors of task-specific admm</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="8278" to="8292" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Single image rain removal boosting via directional gradient</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICME</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vision and rain</title>
		<author>
			<persName><forename type="first">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="27" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-learning-based rain streak removal for image/video</title>
		<author>
			<persName><forename type="first">L.-W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCAS</title>
		<imprint>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2012">2012</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A generalized low-rank appearance model for spatio-temporally correlated rain streaks</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1968" to="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Should we encode rain streaks in video as deterministic or stochastic?</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="2516" to="2525" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Video deraining and desnowing using temporal correlation and low-rank matrix completion</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2658" to="2670" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">High-quality frame recurrent video de-raining with multi-contextual adversarial network</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing, Communications, and Applications</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recurrent multi-frame deraining: Combining physics guidance and adversarial learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Memory-efficient hierarchical neural architecture search for image denoising</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3657" to="3666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Clearer: Multi-scale neural architecture search for image restoration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A generic descent aggregation framework for gradient-based bi-level optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07976</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11517</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Towards gradient-based bilevel optimization with non-convex followers and beyond</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<editor>NeruIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A generic first-order algorithmic framework for bi-level programming beyond lower-level singleton</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6305" to="6315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A first order method for solving convex bilevel optimization problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sabach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shtern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="640" to="660" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Investigating task-driven latent feasibility for nonconvex image modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7629" to="7640" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Video super-resolution with temporal group attention</title>
		<author>
			<persName><forename type="first">T</forename><surname>Isobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Slabaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8008" to="8017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Robust video content alignment and compensation for rain removal in a cnn framework</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="6286" to="6295" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Removing rain in videos: a large-scale database and a two-stream convlstm approach</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICME</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="664" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Image information and visual quality</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fsim: A feature similarity index for image quality assessment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="2378" to="2386" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="586" to="595" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Temporally coherent gans for video super-resolution (tecogan)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09393</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
