<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text Independent Speaker Recognition System using GMM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Bagul</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">M.E. (Digital System)</orgName>
								<address>
									<settlement>Vpcoe Baramati</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Prof</roleName><forename type="first">R</forename><forename type="middle">K</forename><surname>Shastri</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">E&amp;T.C Dept</orgName>
								<address>
									<postBox>H.O.D</postBox>
									<settlement>Vpcoe Baramati</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Text Independent Speaker Recognition System using GMM</title>
					</analytic>
					<monogr>
						<idno type="ISSN">2250-3153</idno>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T18:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speaker Recognition</term>
					<term>feature extraction</term>
					<term>statistical model</term>
					<term>Gaussian mixture model</term>
					<term>Mel Frequency Cepstral Coefficients</term>
					<term>Estimation and Maximization</term>
					<term>likelihood</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The idea of the AUDIO SIGNAL PROCESSING (Speaker Recognition [4]  Project) is to implement a recognizer using Matlab which can identify a person by processing his/her voice. The Matlab functions and scripts were all well documented and parameterized in order to be able to use them in the future. The basic goal of our project is to recognize and classify the speeches of different persons. This classification is mainly based on extracting several key features like Mel Frequency Cepstral Coefficients (MFCC [2]) from the speech signals of those persons by using the process of feature extraction using MATLAB. The above features may consists of pitch, amplitude, frequency etc. It can be achieved by using tools like MATLAB. Using a statistical model like Gaussian mixture model (GMM [6]) and features extracted from those speech signals we build a unique identity for each person who enrolled for speaker recognition <ref type="bibr" target="#b3">[4]</ref>. Estimation and Maximization algorithm is used, An elegant and powerful method for finding the maximum likelihood solution for a model with latent variables, to test the later speeches against the database of all speakers who enrolled in the database.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>his project encompasses the implementation of a speaker recognition <ref type="bibr" target="#b3">[4]</ref> program in Matlab. Speaker recognition <ref type="bibr" target="#b3">[4]</ref> systems can be characterized as text-dependent or textindependent. The system we have developed is the latter, textindependent, meaning the system can identify the speaker regardless of what is being said. The program will contain two functionalities: A training mode, a recognition mode. The training mode will allow the user to record voice and make a feature model of that voice. The recognition mode will use the information that the user has provided in the training mode and attempt to isolate and identify the speaker. Most of us are aware of the fact that voices of different individuals do not sound alike. This important property of speech-of being speaker dependent-is what enables us to recognize a friend over a telephone. Speech is usable for identification <ref type="bibr" target="#b0">[1]</ref> because it is a product of the speaker"s individual anatomy and linguistic background. In more specific, the speech signal produced by a given individual is affected by both the organic characteristics of the speaker (in terms of vocal tract geometry <ref type="bibr" target="#b2">[3]</ref>) and learned differences due to ethnic or social factors. To consider the above concept as a basic, we have tried to establish an "Speaker Recognition <ref type="bibr" target="#b3">[4]</ref> System" by using the simulation software Matlab Speaker recognition <ref type="bibr" target="#b3">[4]</ref> can be classified into identification and verification. Speaker identification is the process of determining which registered speaker provides a given utterance. Speaker verification, on the other hand, is the process of accepting or rejecting the identity claim of a speaker. The system that we will describe is classified as text-independent speaker identification system since its task is to identify the person who speaks regardless of what is saying.</p><p>In this paper, we will discuss only the text independent but speaker dependent Speaker Recognition <ref type="bibr" target="#b3">[4]</ref> system. All technologies of speaker recognition <ref type="bibr" target="#b3">[4]</ref>, identification and verification, text-independent and text dependent, each has its own advantages and disadvantages and may require different treatments and techniques. The choice of which technology to use is application-specific. At the highest level, all speaker recognition <ref type="bibr" target="#b3">[4]</ref> systems contain two main modules: feature extraction and feature matching. Feature extraction is the process that extracts a small amount of data from the voice signal that can later be used to represent each speaker. Feature matching involves the actual procedure to identify the unknown speaker by comparing extracted features from his/her voice input with the ones from a set of known speakers.</p><p>A wide range of possibilities exist for parametrically representing the speech signal for the speaker recognition <ref type="bibr" target="#b3">[4]</ref> task, such as Linear Prediction Coding (LPC), Mel-Frequency Cepstrum Coefficients (MFCC <ref type="bibr" target="#b1">[2]</ref>).LPC analyzes the speech signal by estimating the formants, removing their effects from the speech signal, and estimating the intensity and frequency of the remaining buzz. The process of removing the formants is called inverse filtering, and the remaining signal is called the residue. Another popular speech feature representation is known as RASTA-PLP, an acronym for Relative Spectral Transform -Perceptual Linear Prediction. PLP was originally proposed by Hynek Hermansky as a way of warping spectra to minimize the differences between speakers while preserving the important speech information [Herm90]. RASTA is a separate technique that applies a band-pass filter to the energy in each frequency subband in order to smooth over short-term noise variations and to remove any constant offset resulting from static spectral coloration in the speech channel e.g. from a telephone line.</p><p>MFCC"s are based on the known variation of the human ear"s critical bandwidths with frequency, filters spaced linearly at low frequencies and logarithmically at high frequencies have been used to capture the phonetically important characteristics of speech. This is expressed in the mel-frequency scale, which is T www.ijsrp.org linear frequency spacing below 1000 Hz and a logarithmic spacing above 1000 Hz . MFCC <ref type="bibr" target="#b1">[2]</ref> is perhaps the best known and most popular. Here is just overview of our approach to this project, first we extracted features from the speech signal and then we give them to the statistical model, here we use GMM <ref type="bibr" target="#b5">[6]</ref> as statistical model to create a unique voice print for each identity. . After creation of all voice prints for all identities we check the data base of these voice prints against another voice print which was created by GMM <ref type="bibr" target="#b5">[6]</ref> using testing data. In this project, the GMM <ref type="bibr" target="#b5">[6]</ref> approach will be used, due to ease of implementation and high accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Mel Frequency Cepstral Coefficients(MFCC"S):</head><p>MFCC"s are coefficients that represent audio, based on perception. It is derived from the Fourier Transform or the Discrete Cosine Transform of the audio clip. The basic difference between the FFT/DCT and the MFCC <ref type="bibr" target="#b1">[2]</ref> is that in the MFCC <ref type="bibr" target="#b1">[2]</ref>, the frequency bands are positioned logarithmically (on the mel scale) which approximates the human auditory system's response more closely than the linearly spaced frequency bands of FFT or DCT. This allows for better processing of data, for example, in audio compression. The main purpose of the MFCC <ref type="bibr" target="#b1">[2]</ref> processor is to mimic the behaviour of the human ears.</p><p>The MFCC <ref type="bibr" target="#b1">[2]</ref> process is subdivided into five phases or blocks. In the frame blocking section, the speech waveform is more or less divided into frames of approximately 30 milliseconds. The windowing block minimizes the discontinuities of the signal by tapering the beginning and end of each frame to zero. The FFT block converts each frame from the time domain to the frequency domain.</p><p>In the Mel frequency wrapping block, the signal is plotted against the Mel-spectrum to mimic human hearing. Studies have shown that human hearing does not follow the linear scale but rather the Mel-spectrum scale which is a linear spacing below 1000 Hz and logarithmic scaling above 1000 Hz. In the final step, the Mel-spectrum plot is converted back to the time domain by using the following equation:</p><formula xml:id="formula_0">ğ‘€ğ‘’ğ‘™ ğ‘“ = 2595 * ğ‘™ğ‘œğ‘”10 1 + ğ‘“ 700 â€¦ â€¦ â€¦ (1)</formula><p>Fig. <ref type="figure" target="#fig_1">2</ref>: MFCC BLOCK DIAGRAM</p><p>The resultant matrices are referred to as the Mel-Frequency Cepstrum Coefficients <ref type="bibr" target="#b1">[2]</ref>. This spectrum provides a fairly simple but unique representation of the spectral properties of the voice signal which is the key for representing and recognizing the voice characteristics of the speaker. A speaker voice patterns may exhibit a substantial degree of variance: identical sentences, uttered by the same speaker but at different times, result in a similar, yet different sequence of MFCC <ref type="bibr" target="#b1">[2]</ref> matrices. The purpose of speaker modelling is to build a model that can cope with speaker variation in feature space and to create a fairly unique representation of the speaker's characteristics.  In order to produce a set of acoustic vectors, the original vector of sampled values is framed into overlapping blocks. Each block will contain N samples with adjacent frames being separated by M samples where M &lt; N. The first overlap occurs at N-M samples. Since speech signals are quasi stationary between 5msec and 100msec, N will be chosen so that each block is within this length in time. In order to calculate N, the sampling rate needs to be determined. N will also be chosen to be a power of 2 in order to make use of the Fast Fourier Transform in a subsequent stage. M will be chosen to yield a minimum of 50% overlap to ensure that all sampled values are accounted for within at least two blocks. Each block will be windowed to minimize www.ijsrp.org spectral distortion and discontinuities. A Hamming window will be used. The Fast Fourier Transform will then be applied to each windowed block as the beginning of the Mel-Cepstral Transform. After this stage, the spectral coefficients of each block are generated. The Mel Frequency Transform will then be applied to each spectrum to convert the scale to a mel scale. The following approximate transform can be used(refer equation 1). Finally, the Discrete Cosine Transform will be applied to each Mel Spectrum to convert the values back to real values in the time domain. After creating speaker model we need to identify speaker based on some features such as MFCC <ref type="bibr" target="#b1">[2]</ref> as mentioned above. The features of each user are matched against unknown user. And the speaker with best score is declared to be the claimed speaker. After extracting features we need to create a speaker model using some statistical model like GMM <ref type="bibr" target="#b5">[6]</ref> statistical model. Finite mixture models and their typical parameter estimation methods can approximate a wide variety of pdf's and are thus attractive solutions for cases where single function forms, such as a single normal distribution, fail. However, from a practical point of view it is often sound to form the mixture using one predefined distribution type, a basic distribution. Generally the basic distribution function can be of any type, but the multivariate normal distribution, the Gaussian distribution, is undoubtedly one of the most well-known and useful distributions in statistics, playing a predominant role in many areas of applications . For example, in multivariate analysis most of the existing inference procedures have been developed under the assumption of normality and in linear model problems the error vector is often assumed to be normally distributed. In addition to appearing in these areas, the multivariate normal distribution also appears in multiple comparisons, in the studies of dependence of random variables, and in many other related fields. Thus, if there exists no prior knowledge of a pdf of phenomenon, only a general model can be used and the Gaussian distribution is a good candidate due to the enormous research effort in the past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multivariate normal distribution</head><p>A non-singular multivariate normal distribution of a D dimensional random variable X x can be defined as</p><formula xml:id="formula_1">X~N(x,Î¼,âˆ‘)=(1/(2Ï€)^D/2 âˆ‘ ^1/2 )exp [~1 2 (ğ‘¥~ğœ‡)^ğ‘‡ (ğ‘¥~ğœ‡) -1 ]</formula><p>â€¦â€¦â€¦..(2) Where Î¼ is the mean vector and Î£ the covariance matrix of the normally distributed random variable X. In Figure <ref type="figure">5</ref> an example of 2-dimensional Gaussian pdf is shown. Multivariate Gaussian pdf's belong to the class of elliptically contoured distributions, which is evident in Fig. <ref type="figure">5</ref> where the equi probability surfaces of the Gaussian are centered hyper ellipsoids .The Gaussian distribution in Eq. 1 can be used to describe a pdf of a real valued random vector (x 2 RD). A similar form can be derived for complex random vectors (x 2 CD) as</p><formula xml:id="formula_2">ğ‘ ğ¶ ğ‘¥; ğœƒ, âˆ‘ = 1 ğœ‹ ğ· âˆ‘ exp [-(ğ‘¥ -ğœƒ) * âˆ‘ * 1(ğ‘¥ -ğœƒ)] â€¦â€¦.(<label>3</label></formula><formula xml:id="formula_3">)</formula><p>Where * denotes adjoint matrix (transpose and complex conjugate).</p><p>Fig. <ref type="figure">5</ref>: A two dimensional gaussian pdf and contour plots 2.3 Finite mixture model Despite the fact that multivariate Gaussian pdf's have been successfully used to represent features and discriminate between different classes in many practical problems the assumption of single component leads to strict requirements for the phenomenon characteristics: a single basic class which smoothly varies around the class mean. The smooth behavior is not typically the most significant problem but the assumption of unimodality. For multimodality distributed features the unimodality assumption may cause an intolerable error to the estimated pdf and consequently into the discrimination between classes. The error is not caused only by the limited representation power but it may also lead to completely wrong interpretations (e.g. a class represented by two Gaussian distributions and another class between them). In object recognition <ref type="bibr" target="#b3">[4]</ref> this can be the case for such a simple thing as a human eye, which is actually an object category instead of a class since visual presence of the eye may include open eyes, closed eyes, Caucasian eyes, Asian eyes, eyes with glasses, and so on. For a multimodal random variable, whose values are generated by one of several randomly occurring independent sources instead of a single source, a finite mixture model can be used to approximate the true pdf. If the Gaussian form is sufficient for single sources, then a Gaussian mixture model (GMM <ref type="bibr" target="#b5">[6]</ref>) can be used in the approximation. It should be noted that this does not necessarily need be the case but GMM <ref type="bibr" target="#b5">[6]</ref> can also approximate many other types of distributions.</p><p>The GMM <ref type="bibr" target="#b5">[6]</ref> probability density function can be defined as a weighted sum of Gaussians ğ’‘ ğ’™; ğœ½ = âˆ‘ ğ… ğ’„ğ‘µ ğ’™;ğœ½ ğ’„,âˆ‘ ğ’„ ğ‘ª ğ’„=ğŸ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¦â€¦(3)</head><p>Where c is the weight of c th component. The weight can be interpreted as a priori probability that a value of the random variable is generated by the c th source, and thus, 0â‰¤Î± c â‰¤1 and</p><formula xml:id="formula_4">âˆ‘ ğ›¼ ğ‘ =1 ğ¶ ğ‘=1 â€¦â€¦â€¦â€¦â€¦â€¦.<label>(4)</label></formula><p>Now, a Gaussian mixture model probability density function is completely defined by a parameter list <ref type="bibr" target="#b5">[6]</ref> ğœƒ = {ğ›¼1, ğœƒ1, âˆ‘ 1 ,â€¦â€¦â€¦â€¦ , ğ›¼ ğ¶, ğœƒ ğ¶, âˆ‘ ğ¶ }â€¦â€¦â€¦â€¦..( <ref type="formula">5</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Estimation Maximization:</head><p>An elegant and powerful method for finding the maximum likelihood solution for a model with latent variables. Total data log-likelihood:</p><p>L=ln p(D|Ï€, Âµ, C)â€¦â€¦â€¦â€¦â€¦â€¦... <ref type="bibr" target="#b5">(6)</ref> Setting the derivatives of L with respect to the means Î¼k to zero, we obtain:</p><formula xml:id="formula_5">ğœ‡ ğ¾ = 1 ğ‘ ğ¾ âˆ‘ ğ›¾(ğ‘ ğ‘›ğ‘˜ )ğ‘¥ ğ‘› ğ‘ ğ‘›=1 â€¦â€¦â€¦â€¦..<label>(7)</label></formula><p>N K =Effective number of points assigned to the component k</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ESTIMATION-MAXIMISATION FOR GMM S (ALGORITHM)</head><p>Given a Gaussian mixture model, the goal is to maximize the likelihood with respect to the parameters.</p><p>1. Initialize the means Î¼ k , covariances C k and mixing coefficients Ï€ k , and evaluate the initial value of the log likelihood.</p><p>2. E step: Evaluate the responsibilities ğ›¾ (Znk) using the current parameter values 3. M step: Re-estimate the parameters Î¼ K new , C k new , Î  k new and Using the current responsibilities.</p><p>4. Evaluate the log likelihood and check for convergence of either the parameters or the log likelihood. If the convergence criterion is not satisfied return to step 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Log likely hood Calculation:</head><p>Another quantity that plays an important role is the conditional probability of z given x </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¦â€¦â€¦â€¦(14)</head><p>IV. RESULTS</p><p>The implementation of this project is done in MATLAB and the results can be seen in a GUI. The GUI takes the filename of the speaker as input and gives the name of the speaker as output. The GUI basically contains a button named "train" when this button is pressed the data is trained and stored in the excel sheets. And it also contains a text field in which the input file name is given. After giving the input file name we ISSN 2250-3153 www.ijsrp.org have to press "Enter" then the result will be displayed in the same text box. The snapshots of the GUI when providing inputs and when results are displayed are shown below. Over the last decade, the GMM <ref type="bibr" target="#b5">[6]</ref> has become established as the standard classifier for text-independent speaker recognition <ref type="bibr" target="#b3">[4]</ref>. It operates on atomic levels of speech and can be effective with very small amounts of speaker specific training data. The primary focus of this work was on a task domain for a real application, such as voice mail labelling and retrieval. The Gaussian Mixture speaker model was specifically evaluated for identification tasks using short duration utterances from unconstrained conversational speech, possibly transmitted over noisy telephone channels. Gaussian mixture models were motivated for modelling speaker identity based on two interpretations. The component Gaussians were first shown to represent characteristic spectral shapes (vocal tract configurations) from the phonetic sounds which comprise a person"s voice. By modelling the underlying acoustic classes, the speaker model is better able to model the short term variations of a person"s voice, allowing high identification performance for short utterances. The Gaussian mixture speaker model was also interpreted as a non-parametric, multivariate pdf model, capable of modelling feature distributions. The experimental evaluation examined several aspects of using Gaussian mixture speaker models for text independent speaker identification. Some observations and conclusions are An identification performance of Gaussian mixture speaker model is insensitive to the method of model initialization. Variance limiting is important in training to avoid model singularities. There appears to be a minimum model order needed to adequately model speakers and achieve good identification performance.</p><p>The Gaussian mixture speaker model maintains high identification performance with increasing population size. These results indicate that Gaussian mixture models provide a robust speaker representation for the difficult task of speaker recognition <ref type="bibr" target="#b3">[4]</ref> using corrupted, unconstrained speech. The models are computationally inexpensive and easily implemented on a real time platform <ref type="bibr" target="#b5">[6]</ref>. Furthermore their probabilistic frame-work allows direct integration with speech recognition <ref type="bibr" target="#b3">[4]</ref> systems and incorporation of newly developed speech robustness techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Block Diagram Of Project</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1. 2</head><label>2</label><figDesc>Feature Extraction Module: Input: Digital speech signal (vector of sampled values)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Sample Speech Signal Output: A set of acoustic vectors</figDesc><graphic coords="2,358.04,447.25,155.24,110.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: basic structure of speaker identification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6:GUI Snapshot</figDesc><graphic coords="5,59.73,237.89,178.65,97.50" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speaker Recognition : A Tutorial</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cambell</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of The IEEE</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1437" to="1462" />
			<date type="published" when="1997-09">Sept. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m">SPEAKER IDENTIFICATION USING MEL FREQUENCY CEPSTRA COEFFICIENTS &quot;565 Md. Rashidul Hasan, Mustafa Jamil, Md. Golam Rabbani Md. Saifur Rahman, 3rd International Conference on Electrical &amp; Computer Engineering ICECE</title>
		<meeting><address><addrLine>Dhaka Bangladesh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-12-30">2004,28-30 December 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fundamentals of speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juang</forename><forename type="middle">B H</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Stuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
		<title level="m">A Mixture of Gaussians Front End for Speech Recognition</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>Proceedings Eurospeech</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the application of the Bayesian approach to real forensic\Condition with GMM -based systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ortega-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Lucena-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Speaker Odyssey-The Speaker Recognition</title>
		<meeting><address><addrLine>Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-06">2001. June 2001</date>
			<biblScope unit="page" from="135" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust text-independent speaker identification using Gaussian mixture speaker models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
