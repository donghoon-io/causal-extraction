<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent Aspect Mining via Exploring Sparsity and Intrinsic Information *</title>
				<funder ref="#_Pn5ANqh">
					<orgName type="full">Research Grant Council of the Hong Kong Special Administrative Region, China</orgName>
				</funder>
				<funder>
					<orgName type="full">Interface Technologies</orgName>
				</funder>
				<funder ref="#_tYwWAcd">
					<orgName type="full">CUHK MoE-Microsoft Key Laboratory of Human-centric Computing</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yinqing</forename><surname>Xu</surname></persName>
							<email>yqxu@se.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<country key="HK">Hong Kong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianyi</forename><surname>Lin</surname></persName>
							<email>tylin@se.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<country key="HK">Hong Kong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
							<email>wlam@se.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<country key="HK">Hong Kong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zirui</forename><surname>Zhou</surname></persName>
							<email>zrzhou@se.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<country key="HK">Hong Kong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
							<email>hcheng@se.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<country key="HK">Hong Kong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anthony Man-Cho</forename><surname>So</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<country key="HK">Hong Kong</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Latent Aspect Mining via Exploring Sparsity and Intrinsic Information *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2661829.2662062</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T18:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Search and Retrieval]: Text Mining Topic Model</term>
					<term>Sparse Coding</term>
					<term>Aspect Mining Figure 1: A Sample Hotel Review</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate latent aspect mining problem that aims at automatically discovering aspect information from a collection of review texts in a domain in an unsupervised manner. One goal is to discover a set of aspects which are previously unknown for the domain, and predict the user's ratings on each aspect for each review. Another goal is to detect key terms for each aspect. Existing works on predicting aspect ratings fail to handle the aspect sparsity problem in the review texts leading to unreliable prediction. We propose a new generative model to tackle the latent aspect mining problem in an unsupervised manner. By considering the user and item side information of review texts, we introduce two latent variables, namely, user intrinsic aspect interest and item intrinsic aspect quality facilitating better modeling of aspect generation leading to improvement on the accuracy and reliability of predicted aspect ratings. Furthermore, we provide an analytical investigation on the Maximum A Posterior (MAP) optimization problem used in our proposed model and develop a new block coordinate gradient descent algorithm to efficiently solve the optimization with closedform updating formulas. We also study its convergence analysis. Experimental results on the two real-world product review corpora demonstrate that our proposed model outperforms existing state-of-the-art models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>There has been much research effort on extracting and mining information from review texts, such as sentiment analysis <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b16">16]</ref>, opinion summarization and identification <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b11">11]</ref>. But most of these models just target for the general sentiment analysis of review texts. In order to provide users more effective detailed insights of different reviews, it is necessary to detect more fine-grained information of the items. To address this task, aspect-based sentiment analysis has been conducted <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b14">14]</ref> and it led to useful opinion summarization. Aspects are the common attributes or components of an item in a particular domain as exemplified by the aspects such as "Room", "Service" and "Location" of the hotel. On the E-commerce web sites such as Amazon and eBay, users usually write a piece of review text and provide an overall rating for the reviewed item. However, we do not know user's ratings on each aspect, i.e. aspect ratings.</p><p>The latent aspect mining task investigated in this paper takes as input a collection of review texts in a particular domain together with a numerical overall rating of each review. The goal is to discover a set of aspects and predict ratings on each aspect for each review in an unsupervised manner. Also, the key terms for each aspect are detected. Note that the aspects are previously unknown and only the number of aspects is required. Figure <ref type="figure">1</ref> illustrates a sample hotel review. Such kind of review consists of some text content and an overall rating (e.g. 2-star). Suppose that a collection of such reviews and ratings information is available, and the number of aspects is provided. The aim of latent aspect mining is to discover the aspects including "Room", "Value", "Location", etc. and predict user's ratings on each aspect for each review, e.g. 1-star for the Room aspect and 2-star for the Value aspect. Some key terms such as "standard", "twin" for the Room aspect, "remote", "accessible" for the Location aspect, etc. can also be detected. Recently, Wang et al. <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b25">25]</ref> have proposed a model called Latent Aspect Rating Analysis Model (LARAM) that can tackle the latent aspect mining problem. They adopted the classical topic model Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b1">[1]</ref> to model the generation of words in online reviews, and determine the aspect rating based on a rating regression component.</p><p>One limitation of probabilistic topic models such as LDAbased models is that they are ineffective when dealing with aspect sparsity in texts <ref type="bibr" target="#b31">[31]</ref>. Aspect sparsity refers to the observation that the text content of most reviews only covers some aspects, rather than mentioning all aspects. In fact, it is quite common that real-world reviews exhibit aspect sparsity issue. For example, let us consider the hotel domain with a set of aspects such as Value, Room, Location, Cleanliness, Food, Service, etc. For a particular review, a user typically comments on some aspects and not necessarily all the aspects. Another example refers to a particular hotel which is famous for its delicious food. It is more likely that a typical review of this hotel contains comments on its the Food aspect while some other aspects such as Value and Room. are not mentioned especially for short reviews. The main obstacle for traditional probabilistic topic models such as LDA in LARAM mentioned above to handle aspect sparsity is that topic or aspect proportions are modeled as normalized distributions, namely, the sum of each aspect proportion should be one, so applying a sparsity inducing l1-regularizer as in lasso <ref type="bibr" target="#b20">[20]</ref> is not helpful. As a result, some of the aspect ratings (e.g ratings on the Room aspect and the Value aspect in the example above) predicted by the probabilistic topic model may not be reliable.</p><p>Recently, non-probabilistic sparse coding techniques, such as the Sparse Topical Coding (STC) model proposed by Zhu et al. <ref type="bibr" target="#b31">[31]</ref>, can tackle the above sparsity issue. STC does not require the aspect proportion be the normalized distribution, so it is able to employ a theoretically sound l1-regularizer to control the aspect sparsity. However, one limitation is that it cannot be directly applied to tackle the latent aspect mining problem since we need to consider more latent variables such as aspect ratings. Incorporating these additional variables into the model may prohibit a closed-form updating formula, compromising computational efficiency especially for large data sets. Another issue of the STC model is that there is no convergence analysis reported for the block coordinate descent algorithm commonly used in Maximum A Posterior (MAP) estimation adopted by the model.</p><p>Another observation is that in practical situations, we can easily collect side information of the reviews such as user and item information. For example, it is easy to obtain all the reviews written by a particular user, or all the reviews associated with the same item. Such user and item information can be exploited to improve the latent aspect mining problem. Existing models for this problem do not explore such information.</p><p>We investigate the latent aspect mining problem. The input data of this problem consists of a collection of reviews in a particular domain with a numerical overall rating associated with each review. One goal is to discover a set of aspects which are previously unknown for the domain, and predict the aspect ratings for each review. Another goal is to detect key terms for each aspect. We propose a new generative model that can tackle the latent aspect mining problem in an unsupervised manner. It is capable of alleviating the aspect sparsity issue when predicting aspect ratings. Our proposed model, known as Sparse Aspect Coding Model (SACM), is a new model employing l1-regularizer to control the sparsity on the aspect proportions. In addition, we consider user and item side information of review texts. Such information can facilitate better modeling of aspect generation leading to improvement on the accuracy and reliability of predicted aspect ratings. Specifically, we introduce two notions, namely, user intrinsic aspect interest and item intrinsic aspect quality, which are modeled as latent variables in our model. User intrinsic aspect interest captures the intrinsic interest for each aspect of a particular user. Item intrinsic aspect quality represents the intrinsic quality for each aspect of a particular item. In addition to aspect rating prediction, our proposed model is able to detect key terms for each aspect.</p><p>We make use of MAP technique to find the solution. Instead of directly applying block coordinate descent algorithms as in STC, we first conduct analytical investigation on the MAP optimization problem and develop a new algorithm called block coordinate gradient descent algorithm with a closed-form formula to iteratively update the solution. We also study its convergence analysis. This new algorithm allows our model to process the text data efficiently.</p><p>Experimental results on two different real-world product review corpora demonstrate that our proposed model outperforms existing state-of-the-art models.</p><p>Our contributions in this paper can be summarized as follows:</p><p>• We propose a new model for tackling the latent aspect mining problem in an unsupervised manner. This model is capable of alleviating the aspect sparsity issue via a new modeling and derivation extended from the sparse topical coding method.</p><p>• We incorporate user and item side information of review texts via the design of two notions, namely, user intrinsic aspect interest and item intrinsic aspect quality which are modeled as latent variables.</p><p>• We conduct an analytical investigation on the MAP optimization problems used in our proposed model and propose a new block coordinate gradient descent algorithm to solve the MAP optimization with closedform updating formulas. We also study its convergence analysis.</p><p>• We demonstrate the efficacy of user intrinsic aspect interest and item intrinsic aspect quality discovered from the model for supporting user and item characterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>There have been much efforts on sentiment analysis for online reviews. One category is to determine whether a review is positive or negative <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b3">3]</ref>. Another category is to classify online reviews into multi-point sentiment scale <ref type="bibr" target="#b16">[16]</ref>. However, all the above models above just conduct overall sentiment analysis and do not explore fine-grained aspects.</p><p>There have been some works on extracting aspect terms from review texts. Titov et al. <ref type="bibr" target="#b22">[22]</ref> proposed a model called MG-LDA to automatically extract the ratable aspects. Mukherjee et al. <ref type="bibr" target="#b15">[15]</ref> applied the user provided seed words of a few aspect categories to jointly extract and cluster aspect terms by a semi-supervised model. Chen et al. <ref type="bibr" target="#b2">[2]</ref> exploited the prior domain knowledge to generate coherent aspects. Some research efforts have been conducted on aspect-level sentimental opinion mining. Mei et al. <ref type="bibr" target="#b13">[13]</ref> introduced sentiment in discovering the facets and also positive/negative opinions. Later, Titov and McDonald <ref type="bibr" target="#b21">[21]</ref> extended their multi-grain topic model to extract aspect-specific topics. Lin et al. <ref type="bibr" target="#b10">[10]</ref> proposed a sentiment/topic joint model called JST to extract the aspect and its corresponding sentiment polarity. However, it is still not informative enough to identify the sentiment orientation or predict ratings on each topical aspect of a particular item, especially for large review corpora. In <ref type="bibr" target="#b26">[26]</ref>, Wang et al. aimed at inferring the user's ratings and also relative weights on each aspect based on the review text and overall ratings. To tackle this problem, they have proposed two models. One model, known as Latent Rating Regression (LRR), models the overall rating by applying two-fold linear regression model for aspect rating and aspect weight, based on the user specified seed terms for each aspect. Their second model, known as Latent Aspect Rating Analysis Model (LARAM), is a unified generative model and it does not need to predefine the aspect seed terms. Nevertheless, the above models based on probabilistic topic models fail to handle the aspect sparsity issue.</p><p>The sparsity-enhanced models have been widely used in different applications. Yang et al. <ref type="bibr" target="#b29">[29]</ref> extended the popular spatial pyramid matching model and proposed a linear SP-M kernel based on SIFT sparse codes. Shashanka et al. <ref type="bibr" target="#b19">[19]</ref> applied an entropic prior in Maximum A Posterior estimation to enforce sparsity based on the Probabilistic Latent Semantic Analysis. Zhu et al. <ref type="bibr" target="#b31">[31]</ref> improved the traditional probabilistic models by incorporating the sparse coding idea to discover sparse latent representations for each document. Later, Zhu et al. <ref type="bibr" target="#b30">[30]</ref> presented another model called Conditional Topical Coding which is enhanced by incorporating rich language features in text.</p><p>Recently, there have been some works on considering the user and item side information to conduct sentiment analysis for online reviews. Li et al. <ref type="bibr">[9]</ref> explored the reviewer and product information to predict the overall rating of each review. Wang et al. <ref type="bibr" target="#b27">[27]</ref> proposed a supervised topic model to label the prediction for each review with consideration of user and item information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROBLEM DEFINITION</head><p>We provide the problem definition for the latent aspect mining problem investigated in this paper. The input of the latent aspect mining problem consists of a collection of review texts in a particular domain. For each review text, it is also associated with a numerical overall rating. One goal is to discover the set of previously unknown aspects for the domain and predict the ratings on each aspect for each review. It only requires to specify the total number of aspects. Another goal is to detect key terms for each aspect.</p><p>Reviews are written by users to share opinions about their reviewed product items. For a particular domain, the input review corpus is represented as D = {d1, d2, ..., d |D| }. We use U = {1, ..., U } and H = {1, ..., H} to denote the user collection and item collection. Typically, we assume that the review d ∈ D is written by the user u d ∈ U for the item h d ∈ H. Also, the overall rating, denoted by Y d ∈ R+, is given by the user to express his overall satisfaction for the reviewed item. Normally, this rating value is a numerical integer value and it commonly ranges from 1 to 5 star.</p><p>An aspect represents the common attributes or components of the product item in a particular domain. For example, "Service", "Room" aspects for the hotel domain, "Flavor", "Location" for the restaurant domain, etc.. Let K be the total number of aspects in a particular domain. We use A = {1, 2, ..., K} to denote the set of aspects that has been commented in the review corpus. Each aspect is denoted by k ∈ A. An aspect rating is the user's fine-grained rating on each aspect of the reviewed item, e.g. "3-star Service" and "5-star Room" for a hotel. For the review d ∈ D, the aspect ratings are represented by a K-dimensional vector</p><formula xml:id="formula_0">Y A d ∈ R K + .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">OVERVIEW AND BACKGROUND OF OUR MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview of Our Model</head><p>We propose a new generative model that can tackle the latent aspect mining problem in an unsupervised manner. This model is capable of alleviating the aspect sparsity issue when predicting the aspect ratings. The aspect sparsity issue has been discussed in Section 1. Our proposed model, known as SACM, controls the sparsity of aspect proportions by means of l1-regularizer, and generates the aspect ratings by considering user and item intrinsic information. We introduce two notions, namely, user intrinsic aspect interest and item intrinsic aspect quality, which are modeled as latent variables in our model. User intrinsic aspect interest denotes the intrinsic interest for each aspect of a particular user. This notion is different from the notion of aspect weight defined in <ref type="bibr" target="#b26">[26]</ref>. Specifically, aspect weight represents the user's emphasis placed on each aspect when the user decides the overall rating. It varies with different review texts. User intrinsic aspect interest is not item dependent while aspect weight is item dependent. For example, consider a foodie user who has a great interest on the Food aspect in the hotel domain. This user's reviews will mainly comment on this aspect no matter for which hotel. Likewise, if a user has no interest on the Food aspect, his/her reviews do not likely mention this aspect. Item intrinsic aspect quality represents the intrinsic quality for each aspect of a particular item, and it is not user dependent. For example, for a five star hotel, the intrinsic quality for most of its aspects will be superior than that of lower star hotels except for the price.</p><p>One characteristic of our proposed SACM is that the sparsity of aspect proportion in a particular review can be handled more effectively via the modeling of user intrinsic aspect interest and item intrinsic aspect quality. In general, it can be observed that if a particular aspect is not mentioned in a review, it is essentially due to two main reasons. The first reason is that the user has no interest on that aspect. The second reason is that the concerned aspect of a particular item is not so distinctive that such aspect is normally ignored when a user writes the review for that particular item. Another characteristic of SACM is that the aspect rating is modeled by a Gaussian distribution with the mean related to item intrinsic aspect quality, and the variance related to user intrinsic aspect interest. It can be observed that the aspect rating of a particular item from a large number of users should attain an average value determined by the item intrinsic aspect quality, and the variance of such aspect rating is related to the user intrinsic aspect</p><formula xml:id="formula_1">n ∈ Id d ∈ D θ d s dn W dn β k k = 1 : K Figure 2: Sparse Topical Coding Model interest.</formula><p>For example, in the hotel domain, when a foodie user, who is sensitive to the Food aspect, has written reviews for a number of different hotels, his ratings on the Food aspect of each hotel should exhibit some variations.</p><p>The degree of sensitivity depends on his/her intrinsic interest. On the contrary, a user, who never cares about food, would exhibit much less variations in aspect ratings on the Food aspect in this user's reviews. Thus, the variance of aspect rating is related to user intrinsic aspect interest.</p><p>In addition to aspect rating prediction, our proposed model can also detect the key terms for each aspect. The learned (aspect) dictionary within our proposed model contains terms that are associated with each aspect together with the association strength.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sparse Topical Coding Background</head><p>Recently, Zhu et al. <ref type="bibr" target="#b31">[31]</ref> have proposed a Sparse Topical Coding (STC) model for discovering the hidden topic representations from a collection of documents. Unlike traditional topic models, it can directly control the sparsity of the inferred representations by sparsity-inducing regularizers such as l1 regularizer. Figure <ref type="figure">2</ref> depicts the graphical model of STC. Each circle represents a variable. The shaded circles represent the observed variables and non-shaded circles are the hidden variables to be inferred. The inner rectangle plate denotes the replication for the word in each document, and the outer rectangle plate is the replication for a document. The arrows capture the dependency among the variables. STC models the observed text words in each document by latent variables including word code s, document code θ and the dictionary β.</p><p>For a particular document d ∈ D, the document code</p><formula xml:id="formula_2">θ d ∈ R K</formula><p>+ is a K-dimensional vector, where the component θ dk represents the document's association strength regarding the topic k. For example, the sample document code (0.5, 2.3, 1.4, 3.4, 0.0) indicates that this document mainly focuses on the second, third and fourth topic but hardly mention the first and fifth topic. Different from the topic distribution in traditional probabilistic topic models, the sum of each component of the document code does not require to be one, so l1 regularizer can be applied to enforce sparsity for the document code, i.e. some components of the document code equal zero. Similarly, the word code s dn is also a Kdimensional vector, and the kth component s dnk captures the association strength on the topic k for the word n in the document d. The sum of the component in the word code does not need to be one as well. Hence, this word code for each word is also different from the topic assignment. In traditional probabilistic topic models, topic assignment just assigns each word to one of the predefined topics while word code can let each word belong to multiple topics with varying degrees. β ∈ R K×N + is a dictionary<ref type="foot" target="#foot_0">foot_0</ref> with K bases and the vocabulary size of N . It is a global matrix and document independent. Each row β k• represents an topical basis with a unigram distribution over the vocabulary V . In other words, β k• belongs to a (N -1)-simplex. Essentially, the document d is projected to a semantic space spanned by the topical bases in the dictionary β.</p><p>Assume that V = {1, ..., N } is the vocabulary with N words. We model each document d ∈ D as a vector (w d1 , ..., w dn d ) T , where n d = |I d |. I d is the index set of the appearing words in the document d and w dn , where n ∈ I d , denotes the number of occurrences, namely the word count, of the word n in the document d. The basic generative process for the words in the document d ∈ D is as follows: we first sample the document code from the prior p(θ d ), and sample the word code s dn from p(s dn |θ d ) for each observed word n, where n is the word index in vocabulary. Finally, we sample the observed word count w dn from a distribution with s T dn β•n as the mean, where β•n represents the n-th column of β. The joint distribution is defined as:</p><formula xml:id="formula_3">p(θ d , s d , {w dn }n∈I d |β) = p(θ d ) n∈I d p(s dn |θ d )p(w dn |s dn , β) (1)</formula><p>Normally, the word count in each document is assumed to be sampled from a Poisson distribution. For the sparsity of θ d and s dn , the document code is induced by the Laplace prior, and the word code is drawn from the supergaussian. The specific formulations are shown in Section 5.1.</p><p>STC employs the Maximum A Posterior (MAP) estimation method to infer these set of hidden variables. We represent the collection of document code and word code as Θ and S, respectively, i.e. Θ = {θ d } d∈D , S = {s dn } d∈D,n∈I d . The hidden variable set can be represented as Ω = {Θ, S, β}. The observed data is the text words {w dn } d∈D,n∈I d . The goal is to infer hidden variable set Ω conditioned on the observed data. The MAP objective function of STC can be formulated as follows:</p><formula xml:id="formula_4">ΩMAP = arg max Ω p(Ω|{w dn } d∈D,n∈I d )<label>(2)</label></formula><p>The block coordinate descent algorithm is usually employed to solve the objective function above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">OUR PROPOSED MODEL -SACM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model Description</head><p>As mentioned in Section 4, our proposed model, known as Sparse Aspect Coding Model (SACM), incorporates two latent variables, namely, user intrinsic aspect interest tu and item intrinsic aspect quality q h when modeling the observed review text and overall rating. User intrinsic aspect interest tu for the user u ∈ U represents this user's intrinsic interest for each aspect. Item intrinsic aspect quality q h denotes the intrinsic quality of the item h ∈ H for each aspect, which is user independent. More description for these two notions can be found in Section 4. The generative process is as follows: One would first choose the subset of all aspects for giving comments and decide the text proportion for describing each aspect based on the user intrinsic aspect interest tu and item intrinsic aspect quality q h . Then, some terms including opinionated words would be selected to form the review content. The details of the generation process of a the IR community. To avoid confusion, we call it "aspect dictionary" instead of "dictionary" in this paper.</p><p>word will be described below. Next, the sentimental orientation for each aspect characterized by the aspect rating is determined. Finally, the observed overall rating given by this user will be based on the weighted sum of aspect ratings.</p><p>The graphical model of SACM is depicted in Figure <ref type="figure" target="#fig_0">3</ref>. The outer rectangle plate represents the replication for a review. The inner rectangle plate captures each word in each review. There are two components in this model. The first component shown on the lower left is related to the review text content component including θ d , s dn and w dn . The second component shown on the upper right is related to the rating mining component.</p><p>We first describe the review text content component which uses a variant of STC mentioned in Section 4.2 to generate the observed words. For a particular review d ∈ D written by the user u d ∈ U for the item h d ∈ H, the document code θ d is modeled as the Hadamard product between the user intrinsic aspect interest tu d and the item intrinsic aspect quality q h d instead of Laplace prior. Precisely, the kth element of the document code θ dk represents the association strength on the aspect k. Also, the more the word occurrence over the kth aspect, the higher the value of θ dk is. Specifically, the dominated aspect proportions in a review mainly depend on the corresponding tu d and q h d . For instance, in the hotel domain, a user who likes delicious food will have high t u d k where the aspect k is the Food aspect. This user likely provides opinions on food in detail in his/her reviews leading to a high value of θ dk . Additionally, a hotel possessing distinctive environment, i.e. q hk is high where k is the Environment aspect, is likely to draw attention from users by its environment. Thus, it tends to attract some comments on this aspect. As a result, the corresponding θ dk also has a high value. The above examples show us that both tu d and q h d contributes to θ d . Based on the above motivation, we use Eq. (3) below to generate the aspect proportion, which is modeled by the document code θ d for review d,</p><formula xml:id="formula_5">θ d = tu d • q h d<label>(3)</label></formula><p>where the operator • is the Hadamard product, which is defined as the entry-wise product between the vector tu d and the vector q h d . It is reasonable that the user intrinsic aspect interest tu, u ∈ U is drawn from the Laplace prior, i.e. p(tu) ∝ exp(-λ tu 1). Specifically, a user usually will not be interested in all possible aspects of a particular item. Then, we use the STC model to generate the observed review text. After obtaining the document code θ d , we sample the word code s dn from p(s dn |θ d ) for each observed word n, where n is the word index in vocabulary, and sample the observed word count w dn from a distribution with s T dn β•n as the mean, where β•n represents the n-th column of β. Unlike the multinomial distribution adopted in traditional probabilistic topic models, for the sparsity of word code, s dn is drawn from the supergaussian as shown below. The l1-norm within them tends to find sparse codes.</p><formula xml:id="formula_6">p(s dn |θ d ) ∝ exp(-γ s dn -θ d 2 2 -ρ s dn 1)<label>(4)</label></formula><p>Then, the word count in each document is sampled from the Poisson distribution p(w dn |s dn , β) = P oiss(w dn ; s T dn β•n). In the rating mining component, we define the aspect weight represents the user's relative weight placed on each aspect when the user decides the overall rating for a partic- </p><formula xml:id="formula_7">h ∈ H u ∈ U d ∈ D t u t u d θ d s dn W dn η d q h d Y A d α Y d c 2 q h n ∈ Id β k k ∈ A</formula><formula xml:id="formula_8">η dk = exp(θ dk ) j exp(θ dj )<label>(5)</label></formula><p>For a review d written by the user u d for the item h d , we assume that the k-th element of the aspect rating Y A dk is drawn from a Gaussian distribution. The mean and variance are assumed to be q h d k and α 2 t 2 u d k respectively where α is a positive scaler.</p><formula xml:id="formula_9">Y A dk ∼ N (q h d k , α 2 t 2 u d k )<label>(6)</label></formula><p>Consequently, the ratings on the kth aspect Y A dk from all reviews for a particular item h d should attain the average value determined by the intrinsic aspect quality q h d of this item h d . For a particular user u, the variance for his/her aspect ratings should be related to this user's intrinsic aspect interest tu. For example, in the hotel domain, a foodie person is likely to write more about the Food aspect in the reviews, and this user would be more sensitive about the variation of the Food aspect in different hotels. Thus, he would give ratings on the Food aspect with higher variance. Another example is that a thrifty person would be more sensitive to the Price aspect and tends to provide a wider range of ratings for the Price aspect for different hotels. But for other aspects, this user does not care much and the ratings on them would exhibit much less variance.</p><p>Finally, as the generative process mentioned above, we assume that the overall rating Y d of the review d is drawn from a Gaussian distribution. The weighted sum of aspect ratings η T d Y A d is the mean and c 2 is a fixed variance, i.e.</p><formula xml:id="formula_10">Y d ∼ N (η T d Y A d , c 2</formula><p>). Since the user intrinsic aspect interest is modeled by a Laplace prior, we employ the Maximum A Posterior (MAP) to estimate all the latent variables in this model. Let T and Q be the collection of user intrinsic aspect interest and item intrinsic aspect quality respectively, i.e. T = {tu}u∈U , Q = {q h } h∈H , and we represent the collection of word codes and aspect ratings as S = {s dn } d∈D,n∈I d and Y = {Y A d } d∈D , respectively. Our goal is to infer the latent variable set Ω where Ω = {Y, S, T, Q, β, α}. The objective function is the negative logarithm of the posterior p(Ω|{w dn , Y d } d∈D,n∈I d ). Combining (3) to <ref type="bibr" target="#b6">(6)</ref>, and the review text content component, the optimization problem based on MAP estimation is given as follows:</p><p>min</p><formula xml:id="formula_11">Ω u λ tu 1 + d n∈I d (γ s dn -θ d 2 2 + ρ s dn 1) - d n∈I d [(w dn log(s T dn β•n)) -s T dn β•n] + d 1 2c 2 (Y d - k η dk Y A dk ) 2 + d k [log(αt u d k ) + 1 2α 2 t 2 u d k (Y A dk -q h d k ) 2 ] s.t. tu ≥ 0, q h ≥ 0, s dn ≥ 0, η dk = exp(θ dk ) j exp(θ dj ) θ d = tu d • q h d , β k ∈ S (N-1) , α &gt; 0, ∀d, n ∈ I d , ∀k<label>(7)</label></formula><p>where S (N-1) represents the (N -1)-simplex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Aspect Rating Prediction and Term Detection</head><p>We utilize the dictionary β to detect the key terms for each aspect. For a particular aspect k, each row β k• represents the association strength of each term for the aspect k. We can rank the terms based on their association strength and treat the top terms as the representative key terms for the aspect k. In contrast with the STC model, our proposed model incorporates the overall ratings associated with each review text as input, so our learned aspect dictionary can be more informative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Optimization Technique-Block Coordinate Gradient Descent</head><p>We investigate the optimization technique for finding the solution for MAP estimation in <ref type="bibr" target="#b7">(7)</ref>. Note the such problem could be written in the following form,</p><formula xml:id="formula_12">min f (Y, S, T, Q, β, α) + λ T 1 + ρ S 1 s.t. T ≥ 0, Q ≥ 0, S ≥ 0, α &gt; 0, β k ∈ S (N-1) , ∀k<label>(8)</label></formula><p>where <ref type="figure">f (Y,</ref><ref type="figure">S,</ref><ref type="figure">T,</ref><ref type="figure">Q,</ref><ref type="figure">β,</ref><ref type="figure">α</ref>) denotes the function that unifies all the other terms in the objective of problem <ref type="bibr" target="#b7">(7)</ref>.</p><formula xml:id="formula_13">T 1 = u tu 1, S 1 = d n∈I d s dn 1 and</formula><p>A popular approach for solving optimization problem of form ( <ref type="formula" target="#formula_12">8</ref>) is the block coordinate descent (BCD) method. At each iteration of BCD, a single block (subset) of the whole set of variables is chosen to be optimized while fixing the remaining variables, as used in STC <ref type="bibr" target="#b31">[31]</ref>. However, our model is more complex such that for each subproblem of BCD, we are unable to find a closed-form solution. In other words, in our model, solving each subproblem of BCD would be of high computational cost. To remedy this issue, we introduce the block coordinate gradient descent (BCGD) method. Like BCD, BCGD is also an iterative algorithm starting with a specified initial point x 0 = (Y 0 , S 0 , T 0 , Q 0 , β 0 , α 0 ). At each iteration of BCGD, it first chooses an block B to be updated (in (8), B ∈ {Y, S, T, Q, β, α}). Then it calculates a descent direction at the current point x = (Y, S, T, Q, β, α) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>for h = 1 to |H| do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>Optimize over q h : solve the gradient ∇q h f to obtain d(x; q h ). Choose a step size αq h to set q new h = q h + αq h d(x; q h ) and update x</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>Optimize over β: solve the gradient ∇ β f to obtain d(x; β). Choose a step size α β to set β new = β + α β d(x; β) after being projected to a simplex and update x</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>Optimize over α: set the gradient ∇αf to zero, we up-</p><formula xml:id="formula_14">date α by α = d k ( Y A dk -q h d k t u d k ) 2 1</formula><p>|D||K| and then update x 17: until certain convergence criterion is met with respect to the block B, denoted by d(x; B). After the descent direction d(x; B) is obtained, we update the variable by x new = x + αBd(x; B). Here αB is the step size that could be determined by various searching rule, e.g. Armijo rule. Now the remaining question of BCGD is how to calculate the descent direction d(x; B). Mathematically, it is given as follows:</p><formula xml:id="formula_15">d(x; B) := arg min ∇f (x) T d + 1 2 d 2 2 + r(x + d) s.t. d + x ∈ F, dj = 0, ∀j ∈ B.<label>(9)</label></formula><p>where r(x) = λ T 1 + ρ S 1 and F denotes the whole feasible region in <ref type="bibr" target="#b8">(8)</ref>. Though it seems complicated, the following proposition ensures that for problem <ref type="bibr" target="#b8">(8)</ref>, the descent direction d(x; B) admits closed-form solutions for B ∈ {Y, S, T, Q, α}.</p><p>Proposition 1. Suppose v and x are given vectors in R n , then the optimal solution of the following optimization problem</p><formula xml:id="formula_16">min v T d + 1 2 d 2 2 + µ x + d 1 s.t. dj + xj ≥ 0, j = 1, . . . , n. (<label>10</label></formula><formula xml:id="formula_17">)</formula><p>is given by</p><formula xml:id="formula_18">d * j = max{-xj, -vj -µ}, j = 1, . . . , n.<label>(11)</label></formula><p>Let ∇Bf denote the partial derivative of function f with respect to block B. Then it is obvious that the descent directions d(x; T), d(x; S) are obtained by solving optimization problem of form <ref type="bibr" target="#b10">(10)</ref> with v = ∇Tf , µ = λ and v = ∇Sf , µ = ρ, respectively. Moreover, the optimization for descent directions d(x; Y), d(x; Q) and d(x; α) are all of form <ref type="bibr" target="#b10">(10)</ref> with v = ∇Y f, v = ∇Qf, v = ∇αf respectively and µ = 0. Thus by Proposition 1, the updating scheme of block B ∈ {Y, S, T, Q, α} are all with simple implementation and of low computational cost.</p><p>For the aspect dictionary block β, since its feasible region is a simplex, we could not hope for a closed-form of its update. Instead, we apply the projected gradient descent method for solving <ref type="bibr">(9)</ref> and use a linear algorithm <ref type="bibr" target="#b5">[5]</ref> to perform the projection to the simplex.</p><p>Thus, we summarize our Block Coordinate Gradient Descent for solving <ref type="bibr" target="#b7">(7)</ref> in Algorithm 1. Our goal is to solve each latent variables including Y, S, T, Q, β, and α separately assuming that the other variables are fixed in an alternate manner.</p><p>Moreover, the convergence of BCGD has been extensively studied in the optimization community. Specifically, for optimization problems with the property that all non-smooth parts are of a block-separable structure, such as <ref type="bibr" target="#b8">(8)</ref>, both the objective value and the iterates generated by Algorithm 1 are guaranteed to converge to a critical point. We summarize the result in the following theorem and its proof could be found in <ref type="bibr" target="#b23">[23]</ref>.</p><p>Theorem 1. Suppose {x k } is the sequence generated by Algorithm 1, and the step sizes are chosen by the Armijo rule bounded away from 0. Then the value of the objective function is nonincreasing and every cluster point of {x k } is a stationary point of Problem (8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data Sets</head><p>We carry out some experiments on two review corpora. One is the beer review corpus from a beer-rating web site BeerAdvocate 2 , which has been used in <ref type="bibr" target="#b12">[12]</ref>. Another is the hotel review corpus crawled from TripAdvisor 3 , and originally used in <ref type="bibr" target="#b26">[26]</ref> and <ref type="bibr" target="#b25">[25]</ref>. In the beer corpus, for each review, in addition to review texts, ratings are given on 4 aspects including Appearance, Aroma, Palate, and Taste. Furthermore, there is an overall rating for each review. All ratings range from 1 to 5 stars. In the hotel corpus, users are allowed to rate hotels on 7 predefined aspects in each review: Value, Room, Location, Cleanliness, Check In/Front Desk, Service, and Business Service, as well as an overall rating. All ratings range from 1 to 5 stars. In some reviews, there are several aspects not being rated and they are represented by "-1" instead of 1 to 5 stars. We call such kind of aspect rating as a non-existent aspect rating, and its corresponding aspect is non-existent aspect. Very often, a review text may contain only some and not necessarily all aspects. This issue is known as aspect sparsity as mentioned in Section 1. Some previous works such as <ref type="bibr" target="#b25">[25]</ref> filter out such kind of reviews in their experiments since their models cannot handle aspect sparsity. In contrast, we retain such kind of reviews without removing them and form two data sets in our experiments. They are called "Beer" and "Hotel" data set. Table <ref type="table" target="#tab_1">1</ref> depicts the statistics of these data sets. The Sparse Ratio is defined as the fraction of non-existent aspect 2 <ref type="url" target="http://beeradvocate.com/">http://beeradvocate.com/</ref> 3 <ref type="url" target="http://www.tripadvisor.com/">http://www.tripadvisor.com/</ref> ratings.</p><formula xml:id="formula_19">SparseRatio = d g d D × K<label>(12)</label></formula><p>where g d denotes the number of non-existent aspect ratings in the review d. D and K are the number of reviews and the number of predefined aspects, respectively. By controlling the weight of l1 regularizer, our model can also be applicable for data sets without non-existent aspect ratings. Therefore, in order to further investigate the efficacy of our model, we also prepare two additional data sets deriving from the beer and hotel corpora without aspect sparsity by removing reviews containing non-existent aspect ratings similar to some previous works such as <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b25">25]</ref>. These additional data sets are called "Beer-nonsparse" and "Hotel-nonsparse" as depicted in Table <ref type="table" target="#tab_1">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Setup</head><p>We perform pre-processing on these data sets including : (1) removing the punctuations, stop words from a standard stop word list as in <ref type="bibr" target="#b8">[8]</ref>, and the terms whose count frequency is less than 5. (2) converting the words into lower cases. (3) stemming each word to its root form using Porter Stemmer <ref type="bibr" target="#b18">[18]</ref>.</p><p>We carry out the experiment on predicting the aspect ratings for each review to conduct quantitative evaluation. The numerical aspect ratings can be used as the ground-truth for the task of aspect rating prediction. Note that if a certain aspect rating exists but its corresponding aspect has not been mentioned in the review text content, then the rating cannot been regarded as a valid ground-truth information for the evaluation of aspect rating prediction. To ensure the validity of the ground-truth aspect ratings, we employ the Aspect Segmentation algorithm in <ref type="bibr" target="#b26">[26]</ref> to segment each review. Aspect ratings which are not supported by the text content segments will be treated as non-existent aspect ratings. Besides, in order to align with the predefined aspects, we use a set of seed words for each aspect (e.g. "friend" and "concierg" for the Service aspect) in the beer and hotel domain as a prior to guide the text content component in our model, which has been conducted similarly in <ref type="bibr" target="#b25">[25]</ref>.</p><p>We initialize each word code s by the prior seed words, and uniformly initialize θ, t, q and β. The aspect ratings Y A d are initialized by its corresponding overall rating Y d . For the parameter setting, we manually set α = 1.0, c = 0.1, ρ = 5e -4 , and search for the most appropriate λ and γ both in the range of [0.1, 1.0]. The number of aspects for the Beer and Beer-nonsparse data sets is fixed as 4 while we fixed the number of aspects to 7 for the Hotel and Hotel-nonsparse data sets.</p><p>Our quantitative experiments are conducted in three trials. In the first trial, all the models will be evaluated on the "Beer" and "Hotel" data sets. The Beer data set is much larger than the Hotel data set. In the second trial, we evaluate all the models on the "Beer-nonsparse" and "Hotelnonsparse" data sets. Note that our proposed model can be easily configured to generate numerical aspect ratings without non-existent aspect rating by means of controlling the weight of l1-regularizer. In the third trial, we examine the performance of non-existent aspect identification for our model. Finally, we also perform some qualitative experiment on user and item characterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Metrics</head><p>Similar to previous works such as <ref type="bibr" target="#b25">[25]</ref>, we make use of several metrics to measure the performance of our proposed model and all the comparing methods. Specifically, we use three groups of metrics to conduct quantitative evaluation. The first group of metrics evaluate the performance on all the reviews based on the aspect, including: (1) Mean Square Error (MSE) between the predicted aspect ratings and the ground-truth aspect ratings. It can evaluate the prediction accuracy. For the data set involving the aspect sparsity, we need to adjust the MSE metric as follows. Suppose that we successfully predict the non-existent aspect rating meaning that both the predicted aspect rating and the ground-truth aspect rating are "non-existent", MSE will be zero. On the other hand, if a model fails to detect "non-existent" aspect rating, the MSE will be penalized by our specified constant. In our experiment, this constant is 1.0. Note that if there is no non-existent aspect rating in the review data set, then this MSE is exactly the same as the standard MSE.</p><p>(2) Pearson correlation of all the reviews (ρa). For an individual review, this metric can evaluate the performance on preserving the relative order of aspect ratings; (3) Percentage of failing to detect the best and worst aspect within reviews (M isa); (4) nDCG of aspect ranking in all the reviews (nDCGa). For each review, we regard the groundtruth aspect ratings as the graded relevance to calculate the nDCG. Each of the first group of metrics is calculated by the average value over all the reviews. The second group of metrics evaluate the performance based on items, including:</p><p>(1) Pearson correlation across all the items (ρ h ) measures the performance on maintaining the ranking order of aspect ratings for all items. Based on all the reviews commenting on each item, we calculate the average predicted aspect ratings and the ground-truth aspect ratings for each item to calculate ρ h ; (2) Mean Average Precision (M AP h @10) measures the ranking accuracy for items. If each aspect is a query, after ranking by the ground-truth aspect ratings, we regard the top 10% of the items as the relevant answers. M AP h @10 evaluates whether we are able to preserve their top ranking positions if using the predicted aspect rating to rank them. Each of the second group of metrics is calculated by the average value over all the items. For Beer and Hotel data set, when we calculate metrics except MSE, each non-existent predicted or ground-truth aspect ratings will be replaced by the mean determined by the existing aspect ratings in the same review.</p><p>The third group of metrics are used to evaluate the performance on the non-existent aspect identification, including:</p><p>(1) Precision is the fraction of predicted non-existent aspects that are predicted correctly. (2) Recall is the fraction of non-existent aspects having being predicted correctly. (3) F1 score is the harmonic mean of precision and recall.</p><p>Note that for M SE and M isa, the lower the value is, the better the performance is. For the remaining metrics, the higher the value is, the better the performance is. Table <ref type="table">2</ref>: Aspect rating prediction performance on the data sets with aspect sparsity. For M SE and M isa, the lower the value is, the better the performance is. For other metrics, the higher the value is, the better the performance is. Table <ref type="table">3</ref>: Non-existent aspect identification performance on the data sets with aspect sparsity. For all the metrics, the higher the value is, the better the performance is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Aspect Rating Prediction</head><p>We conduct the aspect rating prediction of our model SACM comparing with LRR <ref type="bibr" target="#b26">[26]</ref> and LARAM <ref type="bibr" target="#b25">[25]</ref>, which are two state-of-the-art models to do latent aspect mining problem. Since LRR model needs to apply topic models to identify aspects, in order to conduct fair comparision, sL-DA <ref type="bibr" target="#b4">[4]</ref> model which is able to consider the overall rating is employed to identify aspects for each review in LRR.</p><p>The aspect rating prediction performance of different models on the data sets with aspect sparsity is illustrated in Table <ref type="table">2</ref>, where we highlight the best performance for each metric. In general, for both Beer and Hotel data sets, our proposed model SACM outperforms two comparing methods in all measures. In the first group of metrics, M SE denotes that SACM can achieve better prediction accuracy. ρa, M isa and nDCGa are the aspect-based ranking metrics. The results show that SACM is able to better preserve the relative order of the aspect ratings within a review. In other words, our model can better answer the questions such as "What is this user's favourite aspect?" and "Does this user prefer the Service than the Room of this hotel?". In addition, ρa is relatively low for all the methods because our predicted aspect ratings are real values while the ground truth aspect ratings are all integers, leading to an over-penalty for the ρa metric. Instead, nDCGa is able to alleviate this bias and handles the integer tie cases well. In the second group of metrics, ρ h and M AP h @10 indicate that the performance of LRR and LARAM on the ranking of items is inferior in comparision with that of SACM.</p><p>Table <ref type="table">4</ref> depicts the result of the second trial experiment on "Beer-nonsparse" and "Hotel-nonsparse" data sets. It can be observed that our proposed model still outperforms the LRR and the LARAM in all measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Non-existent Aspect Identification</head><p>For the data sets "Beer" and "Hotel" with aspect sparsity, our model SACM is capable of identifying the non-existent aspect indicated by the user intrinsic aspect interest t. To evaluate the identification performance, we compare SACM with two different methods: Sparse Topical Coding (STC) and its supervised version MedSTC. These existing models only conduct non-existent aspect identification but cannot predict aspect ratings. STC can take a collection of reviews as input and identify the non-existent aspects of each review by the low association strength in the corresponding document code θ. MedSTC improves the STC model by taking advantage of the overall rating associated with each review, and identifies the non-existent aspects similar with STC. Note that we assume if the association strength or user intrinsic aspect interest value of a certain aspect is less than 0.005, then we regard the aspect as a non-existent aspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Beer-nonsparse</head><p>Hotel-nonsparse Table <ref type="table">4</ref>: Aspect rating prediction performance on the data sets without aspect sparstiy. For M SE and M isa, the lower the value is, the better the performance is. For other metrics, the higher the value is, the better the performance is.</p><p>With the same parameters setting mentioned in Section 6.2, we show the non-existent aspect identification performance measured by precision, recall and F1 score in Table <ref type="table">3</ref>. We can observe that SACM shows superior performance on nonexistent aspect identification than all the comparing methods, which is benefit from considering the user and item information into the modeling of aspect ratings and review texts. STC achieves a poor performance because of ignoring the valuable overall rating associated with each review. Besides, it can be observed that all the methods perform better on the Beer data set than that on the Hotel data set. It is mainly due to the reason that the fraction of non-existent aspect ratings (i.e. Sparse Ratio) of the Beer data set is less than that of Hotel data set implying that the users in BeerAdvocate are more willing to share detailed experience with others.  The detected key terms of some aspects in the Hotel data set Table <ref type="table" target="#tab_6">5</ref> shows the detected key term lists of some aspect-  s, including Room, Cleanliness, Check In/Front Desk (i.e. CI/FD), Service, and Business service (i.e. BS), for the Hotel data set by SACM. It can be observed that each key term list can express the basic idea of its corresponding aspect. For example, "single", "door", "room", "standard", and "window" appear quite common in the reviews providing comments on the Room aspect. These terms are quite indicative to the Room aspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Room</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">USER AND ITEM CHARACTERIZATION</head><p>As we discussed before, the output user intrinsic aspect interest tu and item intrinsic aspect quality q h in the SACM can be used to characterize different types of users and items. Specifically, we apply the k-means clustering on the user intrinsic aspect interest tu for all the users u ∈ U on the Beer data set and perform the same procedures for item intrinsic aspect quality q h , h ∈ H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">User Characterization</head><p>For the clustering of user intrinsic aspect interest, we specify the number of cluster as 6, and the average normalized user intrinsic aspect interest of each cluster is depicted in Figure <ref type="figure" target="#fig_2">4</ref>. There are six types of users. Users in type (b) and type (c) represent the groups who have no obvious preference for different aspects of beers. When writing the reviews, these types of users always write detailed experience for each aspect. But other four types of users, namely, (a), (d), (e) and (f) have their own taste. For example, users in type (d) appear to be hardly interested in the Appearance of beer. When this type of user wants to buy a bottle of beer, he/she would not care about the appearance of the beers no matter how beautiful the appearance design is. On the other hand, from the perspective of beer merchants, they can provide personalized beer sales strategy for different types of users based on the result of user characterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Item Characterization</head><p>For the clustering of item intrinsic aspect quality, we specify the number of cluster as 5, and we show the average item intrinsic aspect quality of each cluster in Figure <ref type="figure" target="#fig_3">5</ref>. It can be observed that the top item group possesses relatively better average aspect quality on each aspect than that of lower item group. We name them "5-star" item to "1-star" item. Users can make use of the result of item characterization to know the difference between different items at the aspect level, and choose the most appropriate item based on their own aspect interest. For example, based on Figure <ref type="figure" target="#fig_3">5</ref>, when a user in type (a), who has no interest in the Taste aspect of beer, wants to buy a bottle of beer, he/she is able to see the main advantage of "5-star" beer due to its Taste, and the quality of other aspects has little difference with "4star" beer. Hence, he would buy the "4-star" beer for saving money. On the other hand, merchants can know the reasons why the items they sold are inferior than other items. For example, based on Figure <ref type="figure" target="#fig_3">5</ref>, a "4-star" beer seller can find that its main weakness is the Taste aspect in contrast with "5-star" beers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION</head><p>We propose a generative model to tackle the latent aspect mining problem. Our proposed model SACM can handle the aspect sparsity when predict the aspect ratings from a review text corpus. SACM applies l1-regularizer to control the sparsity on the aspect proportion and also takes user and item intrinsic information into consideration. Moreover, we conduct the analytical investigation for the Maximum A Posterior (MAP) problem used in our proposed model and develop a new block coordinate gradient descent algorithm to effectively find the solution with closed-form updating formulas. Our experimental results on two real-world review corpora demonstrate that our proposed model SACM outperforms the state-of-the-art models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Our Proposed Model -SACM ular review. For the review d, we assume that aspect weightη d ∈ R K++ is generated by the document code θ d , which denotes the aspect strength in each aspect. After normalization, we have each element of η d as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Result of user characterization. "Appearance" is denoted by "app."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Result of item characterization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Algorithm for Our Proposed Model SACM Input: A collection of reviews D = {d 1 , d 2 , ..., d |D| }. For each review d ∈ D, the overall ratings Y d , the corresponding user u, and the item h. Output: Y A d , s dn , tu, q h , β, α, ∀d, u, h , α 0 ), n ∈ I d , ∀d, u, h . Optimize over s dn : solve the gradient ∇s dn f to obtain d(x; s dn ). Choose a step size αs dn to set s new dn = s dn + αs dn d(x; s dn ) and update x Optimize over tu: solve the gradient ∇t u f to obtain d(x; tu). Choose a step size αt u to set t new</figDesc><table><row><cell cols="3">1: Initialize x 0 = (Y A d h , β 0 2: repeat 0 , s 0 dn , t 0 u , q 0</cell></row><row><cell>3: 4:</cell><cell cols="2">for d = 1 to |D| do Optimize over Y A d : solve the gradient ∇ Y A d d(x; Y A d ). Choose a step size α Y A d to set Y A d new = Y A f to obtain d +</cell></row><row><cell></cell><cell>α Y A d</cell><cell>d(x; Y A d ) and update x</cell></row><row><cell>5: 6:</cell><cell cols="2">for n = 1 to |I d | do</cell></row><row><cell>7:</cell><cell cols="2">end for</cell></row><row><cell>8:</cell><cell cols="2">end for</cell></row><row><cell>9:</cell><cell cols="2">for u = 1 to |U | do</cell></row><row><cell>10:</cell><cell></cell></row><row><cell></cell><cell></cell><cell>u</cell><cell>= tu +</cell></row><row><cell></cell><cell cols="2">αt u d(x; tu) and update x</cell></row><row><cell>11:</cell><cell cols="2">end for</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of data sets</figDesc><table><row><cell>Data Set</cell><cell cols="4">#Item #User #Review Sparse Ratio</cell></row><row><cell>Beer</cell><cell>6,469</cell><cell>14,993</cell><cell>302,399</cell><cell>0.273</cell></row><row><cell>Hotel</cell><cell>1,850</cell><cell>79,189</cell><cell>91,224</cell><cell>0.442</cell></row><row><cell>Beer-nonsparse</cell><cell>3,743</cell><cell>7,781</cell><cell>81,787</cell><cell>0.0</cell></row><row><cell>Hotel-nonsparse</cell><cell>1,850</cell><cell>52,882</cell><cell>58,513</cell><cell>0.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that β is called a dictionary in the sparse coding area historically. It is different from the concept of dictionary in</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>The work described in this paper is substantially supported by grants from the <rs type="funder">Research Grant Council of the Hong Kong Special Administrative Region, China</rs> (Project Code: <rs type="grantNumber">CUHK413510</rs>) and the <rs type="grantName">Direct Grant</rs> of the Faculty of Engineering, CUHK (Project Code: <rs type="grantNumber">4055034</rs>). This work is also affiliated with the <rs type="funder">CUHK MoE-Microsoft Key Laboratory of Human-centric Computing</rs> and <rs type="funder">Interface Technologies</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Pn5ANqh">
					<idno type="grant-number">CUHK413510</idno>
					<orgName type="grant-name">Direct Grant</orgName>
				</org>
				<org type="funding" xml:id="_tYwWAcd">
					<idno type="grant-number">4055034</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting domain knowledge in aspect extraction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mining the peanut gallery: Opinion extraction and semantic classification of product reviews</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Pennock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised topic models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient projections onto the l 1-ball for learning in high dimensions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="272" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mining opinion features in customer reviews</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="755" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Disclda: Discriminative learning for dimensionality reduction and classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="897" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating reviewer and product information for review rating prediction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1820" to="1825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint sentiment/topic model for sentiment analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Opinion integration through semi-supervised topic modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning attitudes and attributes from multi-aspect reviews</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1020" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Topic sentiment mixture: modeling facets and opinions in weblogs</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wondra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The flda model for aspect-based opinion mining: addressing the cold start problem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on WWW</title>
		<meeting>the international conference on WWW</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aspect extraction through semi-supervised modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="339" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An algorithm for suffix stripping. Program: electronic library and information systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse overcomplete latent variable decomposition of counts data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shashanka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1313" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A joint model of text and aspect ratings for sentiment summarization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="308" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling online reviews with multi-grain topic models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A coordinate gradient descent method for nonsmooth separable minimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="387" to="423" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Latent aspect rating analysis without aspect keyword supervision</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Latent aspect rating analysis on review text data: a rating regression approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="783" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Supervised topic model with consideration of user and item</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Walk and learn: a two-stage approach for opinion words and opinion targets co-extraction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="95" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conditional topical coding: an efficient topic model conditioned on rich features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="475" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sparse topical coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="831" to="838" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
