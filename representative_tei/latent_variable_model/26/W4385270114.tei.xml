<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Long-Term Time-Series Forecasting: Feature, Pattern, and Distribution</title>
				<funder ref="#_HWEcf9C">
					<orgName type="full">National Key R&amp;D Progamm of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-01-05">5 Jan 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinjiang</forename><surname>Lu</surname></persName>
							<email>luxinjiang@baidu.com</email>
						</author>
						<author>
							<persName><forename type="first">Haoyi</forename><surname>Xiong</surname></persName>
							<email>xionghaoyi@baidu.com</email>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Longyuan Power</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiantao</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Jin</surname></persName>
							<email>jinbo@dlut.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Dejing</forename><surname>Dou</surname></persName>
							<email>dejingdou@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Baidu</forename><surname>Research</surname></persName>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Group Corp. Ltd</orgName>
								<orgName type="institution" key="instit2">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Long-Term Time-Series Forecasting: Feature, Pattern, and Distribution</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-01-05">5 Jan 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2301.02068v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T18:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Long-term time-series forecasting</term>
					<term>Transformer</term>
					<term>Normalizing Flow</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Long-term time-series forecasting (LTTF) has become a pressing demand in many applications, such as wind power supply planning. Transformer models have been adopted to deliver high prediction capacity because of the high computational self-attention mechanism. Though one could lower the complexity of Transformers by inducing the sparsity in point-wise self-attentions for LTTF, the limited information utilization prohibits the model from exploring the complex dependencies comprehensively. To this end, we propose an efficient Transformerbased model, named Conformer, which differentiates itself from existing methods for LTTF in three aspects: (i) an encoderdecoder architecture incorporating a linear complexity without sacrificing information utilization is proposed on top of slidingwindow attention and Stationary and Instant Recurrent Network (SIRN); (ii) a module derived from the normalizing flow is devised to further improve the information utilization by inferring the outputs with the latent variables in SIRN directly; (iii) the inter-series correlation and temporal dynamics in time-series data are modeled explicitly to fuel the downstream self-attention mechanism. Extensive experiments on seven real-world datasets demonstrate that Conformer outperforms the state-of-the-art methods on LTTF and generates reliable prediction results with uncertainty quantification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Time-series data evolve over time, which can result in perplexing time evolution patterns over the short-and longterm. The time evolution nature of time-series data is of great interest to many downstream tasks including time-series classification, outlier detection, and time-series forecasting. Among these tasks, time-series forecasting (TF) has attracted many researchers and practitioners in a wide range of application domains, such as transportation and urban planning <ref type="bibr" target="#b0">[1]</ref>, energy and smart grid management <ref type="bibr" target="#b1">[2]</ref>, as well as weather <ref type="bibr" target="#b2">[3]</ref> and disease propagation analysis <ref type="bibr" target="#b3">[4]</ref>.</p><p>In many real-world application scenarios, given a substantial amount of time-series data recorded, there is a necessity to make a decision in advance, such that, with long-term prediction, the benefits can be maximized while the potential risks can be avoided. Therefore, in this work, we study the problem of forecasting time series that looks far into the future, namely long-term time-series forecasting (LTTF).</p><p>While tons of TF methods <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref> have been proposed with statistical learners, the use of domain knowledge however seems indispensable to model the temporal dependencies for TF but also limits the potential in applications. Recently, deep models <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b12">[13]</ref> have been proposed for TF, which can be categorized into two types: the RNN-based and the Transformer-based models. RNN-based methods capture and utilize long-and short-term temporal dependencies to make the prediction, but fail to deliver good performance in longterm time-series forecasting tasks. Transformer-based models have achieved promising performance in extracting temporal patterns for LTTF because of the usage of self-attention mechanisms. However, such "full" attention mechanisms bring quadratic computation complexity for TF tasks, which thus becomes the main bottleneck for Transformer-based models to solve the long-term time-series forecasting task.</p><p>Several works have been devoted to improving the computation efficiency of self-attention mechanisms and lowering the complexity of handling a length-L sequence to (O(L log L) or O(L √ L)), such as Logtrans <ref type="bibr" target="#b13">[14]</ref>, Reformer <ref type="bibr" target="#b11">[12]</ref>, Informer <ref type="bibr" target="#b14">[15]</ref> and Autoformer <ref type="bibr" target="#b12">[13]</ref>. In the NLP field, some pioneering works have been proposed to reduce the complexity of self-attention to linear (O(L)), including Longformer <ref type="bibr" target="#b15">[16]</ref> and BigBird <ref type="bibr" target="#b16">[17]</ref>. However, these deep models with a linear complexity might limit the information utilization and strain the performance of LTTF. Lowering the computational complexity to O(L) without sacrificing information utilization is a big challenge for LTTF.</p><p>In addition to the complexity, as the input length climbs up, the intricate time-series could exhibit obscure and confusing temporal patterns, which may lead to unstable prediction for self-attention-based models. Moreover, multivariate longterm time-series often embody multiple temporal patterns at different temporal resolutions, e.g., seconds, minutes, hours, or days. On the other hand, the intricate and prevailing multidimensional characteristics of the time-series data exhibit multi-faceted complex correlations among different series. Therefore, how to make the prediction for LTTF more stable and disaggregate multiscale dynamics and multivariate dependencies in time-series data are two more challenges.</p><p>To this end, our work devotes to the above three challenges and proposes a novel model based on Transformer for LTTF, namely Conformer. In particular, Conformer first explicitly ex-plores the inter-series correlations and temporal dependencies with Fast Fourier Transform (FFT) plus multiscale dynamics extraction. Then, to address the LTTF problem in a sequenceto-sequence manner with linear computational complexity, an encoder-decoder architecture is employed on top of the sliding-window self-attention mechanism and the proposed stationary and instant recurrent network (namely, SIRN). More specifically, the sliding-window attention allows each point to attend to its local neighbors for reference, such that the selfattention dedicated to a length-L time-series requires the O(L) complexity. Besides, to explore global signals in time-series data without violating the linear complexity, we renovate the cycle structure of the recurrent neural network (RNN) and distill stationary and instant patterns in long-term time-series with the series decomposition model in a recurrent way.</p><p>Moreover, to relieve the fluctuation effect caused by the aleatoric uncertainty <ref type="bibr" target="#b17">[18]</ref> of time series data and improve the prediction reliability for LTTF, we further put efforts to model the underlying distribution of time-series data. To be specific, we devise a normalizing flow block to absorb latent states yielded in the SIRN model and generate the distribution of future series directly. More specifically, we leverage the outcome latent state of the encoder, as well as the latent state of the decoder, as input to initiate the normalizing flow. Afterward, the latent state of the decoder can be cascaded to infer the distribution of the target series. Along this line, the information utilization for LTTF can be further enhanced and the time-series forecasting can be implemented in a generative fashion, which is more noise-resistant.</p><p>Extensive experiments on seven real-world datasets validate that Conformer outperforms the state-of-the-art (SOTA) baselines with satisfactory margins. To sum up, our contributions can be highlighted as follows:</p><p>• We reduce the complexity of self-attention to O(L) without sacrificing prediction capacity with the help of windowed attention and the renovated recurrent network. • We design a normalizing flow block to infer target series from hidden states directly, which can further improve the prediction and equip the output with uncertainty awareness. • Extensive experiments on five benchmark datasets and two collected datasets validate the superior long-term timeseries forecasting performance of Conformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Methods for Time-Series Forecasting</head><p>Many statistical methods have achieved big success in timeseries forecasting (TF). For instance, ARIMA <ref type="bibr" target="#b4">[5]</ref> is flexible to subsume multiple types of time-series but the limited scalability strains its further applications. Vector Autoregression (VAR) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> makes significant progress in multivariate TF by discovering dependencies between high-dimensional variables. Besides, there exist other traditional methods for the TF problem, such as SVR <ref type="bibr" target="#b7">[8]</ref>, SVM <ref type="bibr" target="#b18">[19]</ref>, etc., which also play important roles in different fields.</p><p>Another line of studies focuses on deep learning methods for TF, including RNN-and CNN-based models. For example, LSTM <ref type="bibr" target="#b19">[20]</ref> and GRU <ref type="bibr" target="#b20">[21]</ref> show their strengths in extracting the long-and short-term dependencies, LSTNet <ref type="bibr" target="#b0">[1]</ref> combines the CNN and RNN to capture temporal dependencies in the time-series data, DeepAR <ref type="bibr" target="#b8">[9]</ref> utilizes the autoregressive model, as well as the RNN, to model the distribution of future time-series. There are also some works focusing on CNN models <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b24">[25]</ref>, which can capture inner patterns of the time-series data through convolution.</p><p>The Transformer <ref type="bibr" target="#b25">[26]</ref> has shown its great superiority in NLP problems because of its effective self-attention mechanism, and it has been extended to many different fields successfully. There are many attempts to apply the Transformer to TF tasks, and the main idea lies in aiming to break the bottleneck of efficiency by focusing on the sparsity of the self-attention mechanism. The LogSparse Transformer <ref type="bibr" target="#b13">[14]</ref> allows each point to attend to itself and its previous points with exponential step size, Reformer <ref type="bibr" target="#b11">[12]</ref> explores the hashing self-attention, Informer <ref type="bibr" target="#b14">[15]</ref> utilizes probability estimation to reduce the time and memory complexities, Autoformer <ref type="bibr" target="#b12">[13]</ref> studies the auto-correlation mechanism in place of self-attention. All the above models reduce the complexity of self-attention to O(L log L). The Sparse Transformer <ref type="bibr" target="#b26">[27]</ref> reduces the complexity to O(L √ L) with attention matrix factorization. The very recent Longformer <ref type="bibr" target="#b15">[16]</ref> and BigBird <ref type="bibr" target="#b16">[17]</ref> adopt a number of attention patterns and can further reduce the complexity to O(L). However, the above reduction of complexity is often at the expense of sacrificing information utilization and the self-attention mechanism might not be reliable when temporal patterns are intricate in the LTTF task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generative Models</head><p>There are works attempting to learn the distribution of future time-series data. Gaussian mixture model (GMM) <ref type="bibr" target="#b27">[28]</ref> can learn the complex probability distribution with the EM algorithm, but it fails to suit dynamic scenarios. Wu et al. <ref type="bibr" target="#b28">[29]</ref> proposed a generative model for TF by using the dynamic Gaussian mixture. <ref type="bibr" target="#b29">[30]</ref> devises an end-to-end model to make coherent and probabilistic forecasts by generating the distribution of parameters. In addition, the authors of <ref type="bibr" target="#b31">[31]</ref> proposed an autoregressive model to learn the distribution of the data and make the probabilistic prediction.</p><p>The variational inference was proposed for generative modeling and introduced latent variables to explain the observed data <ref type="bibr" target="#b32">[32]</ref>, which provides more flexibility in the inference. Both GAN <ref type="bibr" target="#b33">[33]</ref> and VAE <ref type="bibr" target="#b34">[34]</ref> show their impressive performances in distribution inference, but the cumbersome training process plus the limited generalization to new data hinder them for wider applications. Normalizing Flows (NFs) are a family of generative models, an NF is the transformation of a simple distribution that results in a more complex distribution. NF models have been applied in many fields successfully to learn intractable distribution, including image generation, noise modeling, video generation, audio generation, etc. Conformer employs the NF as an inner block for LTTF to absorb latent states in the encoder-decoder architecture, which differentiates itself from prior works. Fig. <ref type="figure">1</ref>: The framework overview of Conformer. In particular, the encoder extracts local patterns with sliding-window multi-head attention (MHA) and explores long-term trends and instant patterns with the proposed SIRN module. The decoder then receives long sequence inputs with the target elements being padded into zeros, measures the weighted composition of multi-faceted temporal patterns, and generates the prediction for target elements. At last, the normalizing flow block absorbs latent states yielded in the encoder-decoder architecture and predicts target elements with a chain of invertible transformations directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM STATEMENT</head><p>We introduce the problem definition in this section. Given a length-L time-series</p><formula xml:id="formula_0">X = {x 1 , x 2 , • • • x L | x i ∈ R dx }</formula><p>where x i is not limited to the univariate case (i.e., d x ≥ 1), the time series forecasting problem takes a length-L x time-series</p><formula xml:id="formula_1">X = {x m+1 , • • • , x m+Lx } as input to predict the future length-L y time series Y = {x n+1 , • • • , x n+Ly } (n = m + L x and m = 1, • • • , L -L y ).</formula><p>For the sake of clarity, we denote Y = {y n+1 , • • • , y n+Ly | y j ∈ X }. Long-term time-series forecasting is to predict the future time-series with larger L y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head><p>The framework overview of Conformer is shown in Fig. <ref type="figure">1</ref>. Conformer mainly consists of three parts: the input representation block, encoder-decoder architecture, and normalizing flow block. First, the input representation block preprocesses and embeds the input time series accordingly. Then, the encoderdecoder architecture explores the local temporal patterns with windowed attention from time-series representations and examines long-term intricate dynamics from both stationary and instant perspectives with the help of recurrent network and time-series decomposition. Moreover, to improve information utilization, the normalizing flow block leverages latent states in the recurrent network and generates target series from the latent states directly. The technical details of these three components will be introduced in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Input Representation</head><p>The time series data exhibits intricate patterns since multifaceted underlying signals are often complex and varying. Given a length-L time series X ,</p><formula xml:id="formula_2">X = {x 1 , x 2 , • • • , x L |x i ∈ R dx } (d x ≥ 1)</formula><p>, we investigate the underlying multi-faceted relatedness in X from two perspectives, i.e., the "vertical" feature perspective, and the "horizontal" temporal perspective.</p><p>1) Multivariate Correlation: Complex relatedness among different variables in a multivariate time series hinders the effectiveness of distinguishing and harnessing important signals for future series prediction. On the one hand, the impacts of different variables on forecasting future series differ. For instance, the heatmaps in Fig. <ref type="figure" target="#fig_1">2</ref>    different variables exhibit distinct relatedness to the target variable, which can also vary over time. On the other hand, the well-leveraged dependencies among variables can benefit time-series forecasting. Fast Fourier Transform (FFT) <ref type="bibr" target="#b35">[35]</ref> has been proven to be effective in discovering the correlations for time series data <ref type="bibr" target="#b36">[36]</ref>- <ref type="bibr" target="#b38">[38]</ref>. Inspired by this, we adopt FFT to represent implicit multivariate correlations of a length-L time series by exploring the auto-correlation as follows:</p><formula xml:id="formula_3">MR X X = f -1 (f (X )f * (X )) ,<label>(1)</label></formula><p>where f and f -1 denote FFT and inverse FFT, respectively. The asterisk represents a conjugate operation. Besides, we employ Softmax to highlight informative variables accordingly:</p><formula xml:id="formula_4">W R = Softmax(MR X X ) .<label>(2)</label></formula><p>2) Multiscale Dynamics: Temporal patterns are helpful in solving the long-term time-series forecasting problem <ref type="bibr" target="#b39">[39]</ref>. We further examine the temporal patterns by means of multiscale representation. Specifically, a time series can present distinct temporal patterns at different temporal resolutions. In other words, more attention should be paid to informative dynamics extracted at certain temporal resolutions.</p><p>To implement the temporal pattern extraction at different scales, we first devise a temporal resolution set S {second, minute, hour, day, week, month, year} for X . Then the sampled time-series set Γ S = {Γ S1 , • • • , Γ S K } is obtained, where K denotes the number of temporal resolutions and Γ S k is the sequence of sampled timestamps at corresponding temporal resolution S k . Afterward, each series in Γ S is embedded into a latent space with d×L dimensionality, such that different series in Γ S are additive:</p><formula xml:id="formula_5">ΓS = E(Γ S ) = {E(Γ S1 ), • • • , E(Γ S K )} = { ΓS1 , • • • , ΓS K } ,<label>(3)</label></formula><p>where E denotes an embedding operation and ΓS k ∈ R d×L represents the embedded series at a certain temporal resolution S k . Then the multiscale temporal patterns can be modeled as:</p><formula xml:id="formula_6">ΓS = W S Concat( ΓS ) + (b S ) = K k=1 W S k ( ΓS k ) + (b S ) ,<label>(4)</label></formula><p>where W S ∈ R L×L×K and b S ∈ R d×L are trainable weights and bias, respectively. The prime symbol denotes the matrix transpose. Besides, W S k ∈ R L×L denotes the k-th sliced matrix of W S .</p><p>3) Fusing Multivariate and Temporal Dependencies: Moreover, to make different variables in multivariate time series more distinguishable w.r.t. their importance for future series, we further apply the convolution to take temporal dependencies into account, which is defined as follows:</p><formula xml:id="formula_7">X v = W v (W R X + X ) + b v ,<label>(5)</label></formula><p>where denotes the convolution operation, and W v ∈ R dx×d and b v ∈ R d×L denote weights and bias, respectively. Finally, by combining the above multivariate correlations and multiscale dynamics with Eqs. ( <ref type="formula" target="#formula_4">2</ref>) and ( <ref type="formula" target="#formula_7">5</ref>), the outcome time-series representation can be obtained as follows:</p><formula xml:id="formula_8">X in = X v + ΓS .<label>(6)</label></formula><p>B. Encoder-Decoder Architecture Our proposed Conformer adopts the encoder-decoder architecture for long-term time-series forecasting.</p><p>1) Attention Mechanism: The standard attention mechanism <ref type="bibr" target="#b25">[26]</ref> takes a three-tuple (query, key, value) as input and employs the scaled dot product and Softmax to calculate the weights against the value as:</p><formula xml:id="formula_9">Attn(Q, K, V ) = Softmax( QK T √ d k )V , where Q ∈ R L×d k , K ∈ R L×d k ,</formula><p>and V ∈ R L×dv represent query, key and value, respectively.</p><p>Moreover, the multi-head attention (MHA) <ref type="bibr" target="#b25">[26]</ref> employs projections for the original query, key, and value N times, and the i-th projected query, key, and value can be obtained by</p><formula xml:id="formula_10">Q i = QW Q i , K i = KW K i , and V i = V W V i , where W Q i ∈ R d k ×d k /N , W K i ∈ R d k ×d K /N</formula><p>, and W V i ∈ R dv×dv/N . Afterward, the attention can be applied to these queries, keys, and values in parallel, and the outcome is further concatenated and projected as follows:</p><formula xml:id="formula_11">ha i =Attn(Q i , K i , V i ), i = 1, 2, • • • , N MHA(Q, K, V ) = Concat(ha 1 , ha 2 , • • • , ha N )W o .<label>(7)</label></formula><p>Sliding-Window Attention. Duplicated messages exist across different heads in full self-attention <ref type="bibr" target="#b40">[40]</ref>. A time series often shows a strong locality of reference, thus a great deal of information about a point can be derived from its neighbors. Hence, the full attention message might be too redundant for future series prediction. Given the importance of locality for TF, the sliding-window attention (with fixed window size w) allows each point attends to its 1  2 w neighbors on each side. Thus, the time complexity of this pattern is O(w × L), which scales linearly with input length. Therefore, we adopt this windowed attention to realize self-attention.</p><p>2) Stationary and Instant Recurrent Network: Although the windowed attention can reduce the complexity to O(L), the information utilization could be sacrificed for LTTF due to point-wise sparse connections. RNNs have achieved big successes in many sequential data applications <ref type="bibr" target="#b41">[41]</ref>- <ref type="bibr" target="#b44">[44]</ref> attributed to their capabilities of capturing dynamics in sequences via cycles in the network of nodes. To enhance information utilization without increasing time and memory complexities, we, therefore, renovate the recurrent network accordingly. In particular, we not only distill the stationary (trend) and instant (seasonal) temporal patterns from input series but also integrate the distilled long-term patterns, as well as the aforementioned local temporal patterns, into the time-series representation. The architecture of the proposed Stationary and Instant Recurrent Network (SIRN) is demonstrated in Fig. <ref type="figure" target="#fig_3">3a</ref>.</p><p>Specifically, we feed the input representation to the first RNN block (followed by a Softmax) to initialize the global representation and add it to the local representation, as well as the original input representation, as follows:</p><formula xml:id="formula_12">X in =SoftMax(RNN(X in )) × X in + MHA W (X in ) + X in ,<label>(8)</label></formula><p>where MHA W (•) denotes the sliding-window attention. Note that the RNN block (followed by Softmax) in the first term of Eq. ( <ref type="formula" target="#formula_12">8</ref>) aims to capture the global temporal dependency, which can supplement the local dependency captured by the windowed attention. Though intricate and diverse, the complex temporal patterns in different time-series data can be roughly divided into (coarse-grained) stationary trends and (fine-grained) instant patterns. Along this line, we employ the series decomposition introduced in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b45">[45]</ref> to distill stationary and instant patterns by capturing trend and seasonal parts of the time-series data. Similar to <ref type="bibr" target="#b12">[13]</ref>, we adopt the moving average to capture longterm trends and the residual of the original series subtracting the moving average as seasonal patterns:</p><formula xml:id="formula_13">X t = AvgPool(Padding(X in )), X s = X in -X t ,<label>(9)</label></formula><p>where X t , X s ∈ R L×dx denote the trend and seasonal parts of X in , respectively. Then, we use a convolution layer to embed the seasonal pattern. And, we feed the embedded representation, coupled with the local representation, to another decomposition block for distilling more seasonal patterns. This distillation process can be implemented in a recurrent way:   <ref type="formula" target="#formula_19">15</ref>) and ( <ref type="formula" target="#formula_20">16</ref>), the latent state of decoder is adopted to generate the target variable.</p><formula xml:id="formula_14">X (l) t , X (l) s = Decomp(Conv(X (l-1) s ) + MHA W (X in )), l = 1, • • • , η ,<label>(10)</label></formula><p>where Decomp denotes Eq. ( <ref type="formula" target="#formula_13">9</ref>), X (0) s = X s and X (0) t = X t . On the other hand, the trend parts generated by different decompositions are merged and fed to the second RNN block. Finally, the distilled multi-faceted temporal dynamics are fused to generate the outcome representation:</p><formula xml:id="formula_15">X out = W(X (η) s + RNN( η l=0 X (l) t )).<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Time Series Prediction with Normalizing Flow</head><p>The aforementioned SIRN framework adopts RNN to extract global signals. In addition, the hidden states yielded by RNN are beneficial for understanding the distribution of timeseries data. Specifically, we design a normalizing-flow block to learn the distribution of hidden states to increase the reliability of prediction.</p><p>1) Background of Normalizing Flow: A time series X = {x 1 , • • • , x L } can be reconstructed by maximizing the marginal log-likelihood: log p(X ) = L i=1 log p(x i ). Due to the intractability of such log-likelihood, a parametric inference model over the latent variables z, i.e., q(z|x), was introduced. Then, one can optimize the variational lower bound on the marginal log-likelihood of each observation x as follows:</p><formula xml:id="formula_16">log p(x) E q(z|x) [log p(x, z) -log q(z, x)] = log p(x) -D KL (q(z|x) || p(z|x)) =L(x; θ) ,<label>(12)</label></formula><p>where D KL (•) denotes the Kullback-Leibler divergence. When the dimensionality of z climbs up, the diagonal posterior distribution is often adopted, which is, however, not flexible enough to match the complex true posterior distributions <ref type="bibr" target="#b46">[46]</ref>. To solve this, the Normalizing Flow <ref type="bibr" target="#b47">[47]</ref> was proposed to build flexible posterior distributions.</p><p>Basically, one can start off with an initial random variable z 0 (with a simple distribution, coupled with a known density function), and then apply a chain of invertible transformations f t , such that the outcome z T has a more flexible distribution:</p><formula xml:id="formula_17">z 0 q(z 0 |x), z t = f t (z t-1 ), t = 1, • • • , T .<label>(13)</label></formula><p>Besides, as long as the Jacobian determinant det dzt dzt-1 is available, the transformation can take the following definition:</p><formula xml:id="formula_18">f t (z t-1 ) = z t-1 + u g(w T z t-1 + b) ,<label>(14)</label></formula><p>where u, w and b are parameters, and g(•) denotes a nonlinear function.</p><p>2) Normalizing Flow for LTTF: The proposed architecture of normalizing flow in Conformer is shown in Fig. <ref type="figure" target="#fig_3">3b</ref>.</p><p>Let h denote the hidden state yielded by the first RNN block in SIRN. Then, draw a random variable from a Gaussian distribution, i.e., N (0, I), and the distribution of the hidden state in the encoder can be obtained as:</p><formula xml:id="formula_19">z e = FCN (e) µ (h e ) + FCN (e) σ (h e ) • ,<label>(15)</label></formula><p>where FCN (e) µ and FCN (e) σ are two fully connected networks, h e denotes the hidden state in encoder. Afterward, we take the latent representation z e and the decoder latent state h d as input to initiate the normalizing flow:</p><formula xml:id="formula_20">z 0 = FCN (d) µ (h d ) + FCN (d) σ (h d ) • z e . (<label>16</label></formula><formula xml:id="formula_21">)</formula><p>Now that the normalizing flow can be iterated as follows:</p><formula xml:id="formula_22">z t = FCN (t) µ (h d , z t-1 ) + FCN (t) σ (h d , z t-1 ) • z t-1 , t = 1, • • • , T .<label>(17)</label></formula><p>Here, we utilize the decoder latent state to cascade the message, such that the future series can be generated directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Function</head><p>In order to coordinate with the other parts of Conformer, the commonly used log-likelihood is substituted for the MSE (mean squared error) loss function for learning the normalizing flow framework. In particular, the random variable sampled from the outcome distribution, i.e., z t , is deemed as the point estimation of the target series. Then, we adopt MSE loss functions on prediction w.r.t. the target series for both encoderdecoder architecture and normalizing flow framework. Finally, the loss function is defined as follows:</p><formula xml:id="formula_23">L = λ • MSE(Y out , Y) + (1 -λ) • MSE(Z out , Y) (18)</formula><p>where Y out and Z out denote the output of decoder and normalizing flow, respectively, and λ is a trade-off hyperparameter balancing the relative contributions of encoderdecoder and normalizing flow.</p><p>V. EXPERIMENTS A. Experiment Settings 1) Datasets: We conduct experiments on seven datasets including five benchmark datasets and two collected datasets. Table <ref type="table" target="#tab_1">I</ref> describes some basic statistics of these datasets.</p><p>ECL<ref type="foot" target="#foot_0">foot_0</ref> was collected in 15-minute intervals from 2011 to 2014. We select the records from 2012 to 2014 since many zero values exist in 2011 <ref type="bibr" target="#b0">[1]</ref>. The processed dataset contains the hourly electricity consumption of 321 clients. We use 'MT 321' as the target, and the train/val/test is 12/2/2 months.</p><p>Weather<ref type="foot" target="#foot_1">foot_1</ref> was recorded in 10-minute intervals from 07/2020 to 07/2021. There exist 21 meteorological indicators, e.g., the amount of rain, humidity, etc. We choose temperature as the target, and the train/val/test is 10/1/1 months.</p><p>Exchange <ref type="bibr" target="#b0">[1]</ref> records the daily exchange rates of eight countries from 1990 to 2016. We use the exchange rates of Singapore as the target, The train/val/test is 16/2/2 years.</p><p>ETT <ref type="bibr" target="#b14">[15]</ref> records the electricity transformer temperature. Every data point consists of six power load features and the target value is "oil temperture". This dataset is separated into {ETTh1, ETTh2} and {ETTm1, ETTm2} for 1-hour-level and 15-minute-level observations, respectively. We use ETTh1 and ETTm1 as our datasets. The train/val/test are 12/2/2 and 12/1/1 months for ETTh1 and ETTm1, respectively.</p><p>Wind (Wind Power) <ref type="foot" target="#foot_2">3</ref> records the generated wind power of a wind farm in 15-minute intervals from 01/2020 to 07/2021. The train/val/test is 12/1/1 months.</p><p>AirDelay was collected from the "On-Time" database in the TranStas data library <ref type="foot" target="#foot_3">4</ref> . We extracted the flights arrived at the airports in Texas and examined arrival delays in the first month of the year 2022, and the canceled flights were removed. Note that the time interval of this dataset is varying. This dataset was split into train/val/test as 7:1:2.</p><p>2) Baselines: We compare Conformer with 9 baselines, i.e., 5 Transformer methods (Autoformer, Informer, Reformer, Longformer, and LogTrans), 2 RNN methods (GRU and LSTNet), and 2 other deep methods (TS2Vec and N-Beats).</p><p>• GRU <ref type="bibr" target="#b20">[21]</ref>: GRU employs the gating mechanism such that each recurrent unit adaptively captures temporal signals in the series. In this work, we adopt a 2-layer GRU. All baselines employ the one-step prediction strategy. For the RNN-based methods, the number of hidden states is chosen from {16, 24, 32, 64}. For the Transformer-based methods, the number of heads of the self-attention is 8 and the dimensionality is set as 512 for all attention mechanisms in the experiments. Moreover, the sampling factor of the selfattention is set to 1 for both Informer and Autoformer, other settings are the same as suggested by <ref type="bibr" target="#b12">[13]</ref>. All Transformerbased baselines (except Autoformer) use the same embedding  <ref type="bibr" target="#b15">[16]</ref> Autoformer <ref type="bibr" target="#b12">[13]</ref> Informer <ref type="bibr" target="#b14">[15]</ref> Reformer <ref type="bibr" target="#b11">[12]</ref> LSTNet <ref type="bibr" target="#b0">[1]</ref> GRU <ref type="bibr">[</ref> method applied to the Informer. As suggested by <ref type="bibr" target="#b12">[13]</ref>, we omit the position embedding and keep the value embedding and timestamp embedding for Autoformer.</p><p>3) Implementation Details: Conformer 5 includes a 2-layer encoder and a 1-layer decoder, as well as a 2-layer normalizing flow block. The window size of the sliding-window attention is 2, and λ in Eq. ( <ref type="formula">18</ref>) is set to 0.8. We use an Adam optimizer, and the initial learning rate is 1 × 10 -4 . The batch-size is 32 and the training process employs early stopping within 10 epochs. In addition, we use MAE (mean absolute error) and MSE (mean squared error) as the evaluation metrics.</p><p>An input-L x -predict-L y window is applied to roll the train, validation and test sets with stride one time step, respectively. This setting is adopted for all datasets. The input length L x is 96 and the predict length L y is chosen from {48, 96, 192, 384, 768} on all datasets. The averaged results in 5 runs are reported. All models are implemented in PyTorch and trained/tested on a Linux machine with one A100 40GB GPU.</p><p>All of the RNN blocks in Conformer are implemented with GRU. Under the multivariate LTTF setting, we adopt 1-layer GRU and 2-layer GRU for encoder and decoder, respectively. Under the univariate LTTF setting, both the encoder and decoder adopt 1-layer GRU. 5 The source code of Conformer is available at <ref type="url" target="https://github.com/PaddlePaddle/PaddleSpatial/tree/main/research/Conformer">https://github.com/ PaddlePaddle/PaddleSpatial/tree/main/research/Conformer</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Prediction Results of Multivariate LTTF</head><p>We compare Conformer to other baselines in terms of MSE and MAE under the multivariate time-series forecasting setting, and the results are reported in Table <ref type="table" target="#tab_2">II</ref>   <ref type="bibr" target="#b15">[16]</ref> Autoformer <ref type="bibr" target="#b12">[13]</ref> Informer <ref type="bibr" target="#b14">[15]</ref> Reformer <ref type="bibr" target="#b11">[12]</ref> LSTNet <ref type="bibr" target="#b0">[1]</ref> GRU <ref type="bibr">[</ref>   <ref type="bibr" target="#b12">[13]</ref> Informer <ref type="bibr" target="#b14">[15]</ref> Reformer <ref type="bibr" target="#b11">[12]</ref> LogTrans <ref type="bibr" target="#b13">[14]</ref> LSTNet <ref type="bibr" target="#b0">[1]</ref> GRU <ref type="bibr" target="#b20">[21]</ref> TS2VEC <ref type="bibr">[</ref> Conformer consistently delivers good performance, which suggests the promising generalization ability. In addition, for the dataset with irregular time intervals (e.g., AirDelay), Conformer still achieves the best performance consistently, while the improvements are less significant. This suggests that the temporal patterns in less-structured time-series data are more challenging for deep models to capture.</p><p>Forecasting with Time-Determined Lengths. We further evaluate the performance of multivariate LTTF when the input and output lengths are configured as time-determined intervals, e.g., 1 day. In particular, in this experiment, the input length L x is set to 1 day and the output length L y is chosen from {1 day (1D), 1 week (1W), 2 weeks (2W), 1 month (1M)}. We inspect forecasting performances of different methods on ETTh1 and ETTm1 datasets. The results are reported in Table III. As depicted, Conformer still achieves the best (or competitive) performance, which suggests the high capacity of Conformer in perceiving long-term signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Comparisons Under Univariate LTTF</head><p>Table <ref type="table" target="#tab_7">IV</ref> reports prediction performances of different methods under the univariate LTTF setting. Conformer achieves the best (or competitive) MSE and MAE scores under various predict-length settings. In particular, satisfactory prediction improvements can be observed on Exchange, ECL and Weather datasets. For instance, compared to the second best results, Conformer achieves 45.8% (0.3889→0.2107) MSE reduction under predict-192 on Exchange dataset, and 15.9%      In addition, under the univariate LTTF setting, we find that RNN-based methods achieve competitive prediction results on Weather and Wind datasets, which can validate the advantages of RNN in extracting temporal dynamics of the time-series data with low entropy and regular patterns.</p><formula xml:id="formula_24">X in -R-Γ def = W v X + b v MSE 0.</formula><formula xml:id="formula_25">X in -X def = W v W R X + b v +</formula><formula xml:id="formula_26">X in -X -Γ def = W v W R X + b v MSE 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>We conduct the ablation study under multivariate TF setting <ref type="foot" target="#foot_4">6</ref> .</p><p>1) Multivariate Correlation and Multiscale Dynamics: We compare Conformer with its tailored variants w.r.t. the multivariate correlation and multiscale dynamics, and report their prediction performances on ECL and ETTm1 datasets. From Table <ref type="table" target="#tab_9">V</ref>, we can obtain several insightful clues on how to embed the input series for LTTF. 1) X in v. X in -R : Multivariate correlation contributes less when the dimensions of series is higher (#dims. of ECL data is much larger than ETTm1 data) or the predict-length is prolonged. 2) X in v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X in</head><p>-X -Γ : Temporal dependency is more important for the series with lower dimensions, and for high dimensional time-series, the effectiveness of temporal dependency can be replaced by the inter-series correlation when L y climbs up. 3) X in -R v. X in -X and X in -R-Γ v. X in -X -Γ : Multiscale dynamics delivers better performance when being guided by the raw series, which holds regardless of #dims. of time-series. Besides, multivariate correlation contributes more than the raw data for low dimensional time-series. 4)</p><formula xml:id="formula_27">X in -R v. X in -R-Γ and X in -X v. X in</formula><p>-X -Γ : Multiscale dynamics could harm the performance for LTTF when being equipped with the multivariate correlation if the raw time-series is absent.</p><p>2) Stationary and Instant Recurrent Network (SIRN): For the ablation study of the proposed SIRN, we compare Conformer to its different variants by tailoring the encoderdecoder architecture on the Wind dataset, which can be found in Table <ref type="table" target="#tab_13">VI</ref>. Specifically, we replace the sliding-window attention and the RNNs with other self-attention mechanisms to verify the effectiveness of SIRN. From Table <ref type="table" target="#tab_13">VI</ref>, we can see that SIRN achieves best performance under different settings, which validate the effectiveness of information utilization of combining the local and global patterns.</p><p>3) Normalizing Flow: To verify the effectiveness of normalizing flow block in Conformer for LTTF task, we compare    <ref type="formula" target="#formula_20">16</ref>)).</p><p>• Conformer -N F : We implement a tailored Conformer by removing the normalizing flow framework.</p><p>The prediction results on Wind dataset are reported in Table VII. We can observe that: 1) The contribution of normalizing flow is indispensable for LTTF regardless of forecast setting and predict length, and 2) the way that we adapt the normalizing flow to the LTTF task is effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Parameter Sensitivity Analysis</head><p>We report parameter sensitivity analysis in Fig. <ref type="figure" target="#fig_5">4</ref>, which is conducted on the Wind dataset. To be specific, we inspect four hyper parameters including the input length L x , the window size w of sliding-window attention, the trade-off parameter λ and the number of transformations in Normalizing Flow. Generally, we can observe that the performance of Conformer is quite stable most of the time w.r.t. the varying of different hyper-parameters. In particular, as shown in Fig. <ref type="figure" target="#fig_5">4a</ref>, long-term time-series forecasting setting (e.g., L y = 384) seems to be more capable of handling longer input, though the volatility of performance is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Computational Efficiency Analysis</head><p>We conduct execution time consumption and memory usage comparisons between Conformer (with sliding-window attention) and other attention mechanisms. We replace the standard self-attention mechanism in Transformer with different variants and carry out the prediction with the corresponding method for 10 3 times (taking the sequences in different time spans as inputs), then the averaged running time per forecast is reported. For the memory cost comparisons, the maximum memory usage is recorded. The time consumption and memory usage of different attentions are demonstrated in Fig. <ref type="figure" target="#fig_6">5</ref>. Conformer performs with better efficiency in both short-and long-term time-series forecasting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Model Analysis</head><p>1) Fusing Inter-Series and Across-Time Dependencies: As introduced in Section IV-A, the series data is embedded and fused by taking the multivariate correlation and multiscale dynamics into account. To further assess the effectiveness of input representation module in Conformer, we realize different ways of fusing multivariate correlation and multiscale dynamics below (let W Γ = Softmax( ΓS )):</p><p>• Method 1:</p><formula xml:id="formula_28">X in = W v (W Γ W R X + X ) + b v • Method 2: X in = W v (W R X + W Γ X ) + b v • Method 3: X in = W v (W R X + W Γ X + X ) + b v • Method 4: X in = [W v (W R X + X ) + b v ]W Γ</formula><p>The results are reported in Table <ref type="table" target="#tab_15">VIII</ref>. We can see that how to fuse the multivariate correlation and temporal dependency is important for the LTTF task. This impact weighs more for low dimensional time-series data since the self-attention mechanism in encoder-decoder architecture can better explore intricate dependencies when the dimensionality grows.</p><p>2) Uncertainty-Aware Forecasting: The outcome variance of the Normalizing Flow block can suggest the fluctuation range of the forecasting results. We randomly select a case in ETTm1 dataset under the multivariate setting and demonstrate the forecasting results with uncertainty quantification for different output lengths in Fig. <ref type="figure" target="#fig_8">6</ref>. We can see that Conformer tends to make a conservative forecast and the uncertainty quantification can cover the extreme ground truth values if the NF block can be weighted more.</p><p>3) How Far The Message Should Be Cascaded in Normalizing Flow: We inspect how the normalizing flow works for LTTF by varying the number of transformations on two    cases in ECL and ETTm1 datasets, respectively, in Fig. <ref type="figure" target="#fig_13">7</ref>. We can see that the further the latent variable being transformed the better the outcome series performs. Therefore, the power of normalizing flow in Conformer for LTTF should be explored more dedicatedly.</p><p>4) How to Feed Hidden States to The Normalizing Flow Block in Conformer: As shown in Fig. <ref type="figure">1</ref>, in both encoder and decoder, the first outcome hidden state of the last SIRN layer is fed to the normalizing flow. To assess the effect of feeding hidden states to normalizing flow, we implement Conformer by combining the outcome hidden states in the first/last SIRN layer of the encoder/decoder, which results in Conformer (h   where k denotes the last SIRN layer. We report the prediction results in Table <ref type="table" target="#tab_16">IX</ref>. As can be seen, the impact of feeding different hidden states to normalizing flow is generally marginal though, the low dimensional time-series forecasting is more sensitive to the way of absorbing hidden states for normalizing flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Multivariate Time-series Forecasting Showcase</head><p>We additionally plot the prediction and the ground truth of the target value. The qualitative comparisons between Conformer and other baselines on ETTm1 dataset are demonstrated in Fig. <ref type="figure" target="#fig_14">8</ref>. We can see that, our model obviously achieves the best performance among different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Discussion</head><p>Windowed Attention: Conformer v. Swin Transformer. The windowed attention mechanism is applied in many applications thanks to its linear complexity, such that the powerful self-attention can be scaled up to large data. The very recent Swin Transformer <ref type="bibr" target="#b50">[50]</ref> and its variant <ref type="bibr" target="#b51">[51]</ref> adopt the windowed attention and devise a shifted window attention to implement a general purpose backbone for computer vision tasks. Basically, both Conformer and Swin Transformer exploit the selfattention within neighbored/partitioned windows regarding the   computational efficiency. Besides the locality, connectivity is another merit one can not neglect. To achieve connectivity, a shifted window mechanism is proposed for Swin Transformer, while we propose SIRN for Conformer so as to absorb longrange dependencies in the time-series data.</p><p>Comparisons of Computational Complexity. The windowed attention contributes most to the complexity reduction of Conformer. Hence, we take different SOTA attention mechanisms as competitors to conduct the computational complexity analysis in Section V-F. The computational costs of other components in Conformer are not elaborated, which will be provided in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we proposed a transformer-based model, namely Conformer, to address the long-term time-series forecasting (LTTF) problem. Specifically, Conformer first embeds the input time series with the multivariate correlation modeling and multiscale dynamics extraction to fuel the downstream self-attention mechanism. Then, to reduce the computation complexity of self-attention and fully distill the series-level temporal dependencies without sacrificing information utilization for LTTF, sliding-window attention, as well as a proposed stationary and instant recurrent network (SIRN), are equipped to the Conformer. Moreover, a normalizing flow framework is employed to further absorb the latent states in the SIRN, such that the underlying distribution can be learned and the target series can be directly reconstructed in a generative way. Extensive empirical studies on six real-world datasets validate that Conformer achieves state-of-the-art performance on longterm time-series forecasting under multivariate and univariate prediction settings. In addition, with the help of normalizing flow, Conformer can generate the prediction results with uncertainty quantification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Exchange rate. (b) Wind power.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Different variables of time-series data evolve at varying rhythms and dynamics. The details of these datasets can be found in Section V-A1.</figDesc><graphic coords="3,433.46,242.42,123.01,105.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Stationary and instant recurrent network (SIRN). (b) Normalizing flow framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The architecture of SIRN and the normalizing flow framework. (a) The first RNN block embeds the global information of input time-series and the second RNN block represents the aggregated trend information extracted by the decomposition block. The decomposition procedure following the initial decomposition can be repeated multiple times. The latent state yielded by the first RNN will be utilized in the normalizing flow framework. (b) After initiating the flow of transformations with Eqs. (15) and (16), the latent state of decoder is adopted to generate the target variable.</figDesc><graphic coords="5,66.36,50.54,163.86,125.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Input length. (b) Window size w. (c) Trade-off parameter λ. (d) Number of transformations in Normalizing Flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Parameter sensitivity analysis of Conformer.</figDesc><graphic coords="10,177.51,197.20,114.76,88.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Computation efficiency analysis. The input length is set as 96 and all the experiments are conducted on the Wind dataset under the multivariate forecasting setting. the original Conformer with its several variants. In particular, we tailor the normalizing flow block in Conformer by realizing a generative forecast method with the help of Gaussian probabilistic model as follows: • Conformer ze -N F : The outcome distribution z t (yielded by normalizing flow) is replaced by z e (obtained by Eq. (15)). • Conformer z d -N F : The outcome distribution z t (yielded by normalizing flow) is replaced by z d . In particular, we replace h e with h d in Eq. (15) and generate z d accordingly. • Conformer ze+z d -N F : The outcome z t (yielded by normalizing flow) is replaced by z 0 (obtained by Eq. (16)).• Conformer -N F : We implement a tailored Conformer by removing the normalizing flow framework.The prediction results on Wind dataset are reported in Table VII. We can observe that: 1) The contribution of normalizing flow is indispensable for LTTF regardless of forecast setting and predict length, and 2) the way that we adapt the normalizing flow to the LTTF task is effective.</figDesc><graphic coords="10,53.22,197.97,114.76,87.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) Predict Length = 96. (b) Predict Length = 192. (c) Predict Length = 384. (d) Predict Length = 768.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: With the help of Normalizing Flow, Conformer can generate the prediction results with uncertainty quantification for LTTF. Four illustrative cases are demonstrated on the ETTm1 dataset under the multivariate setting. λ denotes the contributions of the encoder-decoder, that is, 1 -λ represents the impacts of the normalizing flow block.</figDesc><graphic coords="11,185.30,344.27,111.04,93.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Uncertainty-aware LTTF with varying #transforms. To evaluate the performance of normalizing flow more clearly, we omit the contribution of SIRN by setting λ = 0 in Eq. (18). (a)-(d) and (e)-(h) demonstrate two cases in ECL and ETTm1 datasets, respectively.</figDesc><graphic coords="12,313.66,295.19,106.41,67.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Prediction cases on the ETTm1 dataset under the input-96-predict-192 setting.</figDesc><graphic coords="12,68.47,381.11,106.41,67.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>illustrate rhythms of different variables in various time-series datasets, it is clear that</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Statistical descriptions of the time-series datasets. LSTNet combines the convolution and recurrent networks to extract short-term dependencies among variables and long-term trends in the time series. Note that, to simplify the parameter tuning, the highway and skip connection mechanisms are omitted. • N-Beats [48]: N-Beats was proposed to address time-series forecasting via a deep model on top of the backward and forward residual links and a very deep stack of fullyconnected layers. We implement N-Beats for multivariate LTTF with suggested settings. • Reformer [12]: Reformer uses locality-sensitive hashing (LSH) attention and reversible residual layers to reduce the computation complexity. We implement Reformer by setting the bucket length and the number of rounds for LSH attention as 24 and 4, respectively. • Longformer [16]: Longformer combines the windowed attention with a task motivated global attention to scale up linearly as the sequence length grows. • LogTrans [14]: LogTrans breaks the memory bottleneck of Transformer for LTTF via producing queries and keys with the help of causal convolutional self-attention. The number of the LogTransformer blocks is set to 2 and the sub len of the sparse-attention is set to 1. • Informer [15]: Informer proposes the ProbSparse slefattention to reduce time and memory complexities, and handles the long-term sequence with self-attention distilling operation and generative style decoder. • Autoformer [13]: Autoformer renovates the series decomposition with the help of auto-correlation mechanism, and put the series decomposition as a basic inner block of the deep model. • TS2Vec [49]: TS2Vec is a universal framework for learning representations of time series. It performs contrastive learning in a hierarchical way over augmented context views, which leads to the robust contextual representation for each timestamp. We implement TS2Vec for univariate LTTF with the suggested settings.</figDesc><table><row><cell cols="2">Datasets # Dims.</cell><cell>Time Span</cell><cell cols="3"># Points Target Variable Interval</cell></row><row><cell>ECL</cell><cell cols="3">321 01/2012 -12/2014 26304</cell><cell>MT 321</cell><cell>1 hour</cell></row><row><cell>Weather</cell><cell>21</cell><cell cols="4">01/2020 -06/2021 36761 Temperature 10 mins</cell></row><row><cell>Exchange</cell><cell>8</cell><cell cols="2">01/1990 -12/2016 7588</cell><cell>Country8</cell><cell>1 day</cell></row><row><cell>ETTh1</cell><cell>7</cell><cell cols="2">07/2016 -07/2018 17420</cell><cell>OT</cell><cell>1 hour</cell></row><row><cell>ETTm1</cell><cell>7</cell><cell cols="2">07/2016 -07/2018 69680</cell><cell>OT</cell><cell>15 mins</cell></row><row><cell>Wind</cell><cell>7</cell><cell cols="4">01/2020 -05/2021 45550 Wind Power 15 mins</cell></row><row><cell>AirDelay</cell><cell>6</cell><cell cols="2">01/01 -01/31, 2022 54451</cell><cell>ArrDelay</cell><cell>-</cell></row><row><cell cols="2">• LSTNet [1]:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Comparisons of multivariate LTTF results (the best and 2nd best scores are boldfaced and underlined, resp.).</figDesc><table><row><cell>Model</cell><cell>Conformer Longformer</cell><cell>Transformer-based</cell><cell>RNN-based</cell><cell>Others</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Multivariate LTTF with time-determined (boldface and underline for the best and 2nd best scores).</figDesc><table><row><cell>Model</cell><cell>Conformer Longformer</cell><cell>Transformer-based</cell><cell>RNN-based</cell><cell>Others</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>Comparisons of univariate LTTF results (the best and 2nd best scores are boldfaced and underlined, resp.).</figDesc><table><row><cell>Model</cell><cell>Conformer Autoformer</cell><cell>Transformer-based</cell><cell>RNN-based</cell><cell>Others</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE V :</head><label>V</label><figDesc>Ablation study of the input representation.</figDesc><table><row><cell></cell><cell></cell><cell>Dataset</cell><cell></cell><cell></cell><cell>ECL</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ETTm1</cell></row><row><cell></cell><cell cols="2">Predict Length</cell><cell>48</cell><cell>96</cell><cell>192</cell><cell>384</cell><cell>768</cell><cell>96</cell><cell>192</cell><cell>384</cell><cell>768</cell></row><row><cell cols="3">X in = X v + ΓS (refer to Eq. (6))</cell><cell cols="8">MSE 0.1921 0.2124 0.2378 0.2643 0.3396 0.6954 0.7856 0.9298 0.9835 MAE 0.3034 0.3193 0.3456 0.3620 0.4092 0.5901 0.6387 0.6988 0.7193</cell></row><row><cell></cell><cell>X in -Γ</cell><cell>def = X v</cell><cell cols="8">MSE 0.1995 0.2614 0.2787 0.2932 0.3406 0.8342 0.9878 1.0936 1.1578 MAE 0.3659 0.3766 0.3791 0.4083 0.6623 0.7353 0.7786 0.8066</cell></row><row><cell>X in -R</cell><cell cols="2">def = W v X + b v + ΓS</cell><cell cols="8">MSE 0.1896 0.2178 0.2421 0.2674 0.3425 0.7216 0.8313 0.9429 0.9794 MAE 0.3002 0.3257 0.3512 0.3654 0.4109 0.6107 0.6593 0.7031 0.7152</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE VI :</head><label>VI</label><figDesc>Ablation study of the Stationary and Instant Recurrent Network (on Wind dataset). Conformer (with Auto-Corr [13]) 1.0253 0.7109 1.2878 0.8191 1.4263 0.8742 2.7366 1.2251 3.2173 1.3381 3.4182 1.3816 Conformer (with Prob-Attn [15]) 1.0182 0.7069 1.2817 0.8144 1.4246 0.8734 2.7557 1.2229 3.2231 1.3425 3.4423 1.3801 Conformer (with LSH-Attn [12]) 1.0223 0.7086 1.2778 0.8136 1.4209 0.8730 2.7454 1.2249 3.1930 1.3405 3.4140 1.3793 Conformer (with Log-Attn [14]) 1.0393 0.7157 1.2866 0.8165 1.4272 0.8755 2.7449 1.2365 3.2116 1.3476 3.4148 1.3831 Conformer (with Full-Attn [26]) 1.0165 0.7070 1.2756 0.8117 1.4195 0.8715 2.7356 1.2229 3.1964 1.3477 3.4165 1.3809</figDesc><table><row><cell>Setting</cell><cell>Multivariate Time-Series Forecasting</cell><cell>Univariate Time-Series Forecasting</cell></row><row><cell>Predict Length</cell><cell cols="2">48 MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE 96 192 48 96 192</cell></row><row><cell>Conformer (with full SIRN)</cell><cell cols="2">0.9479 0.6539 1.1725 0.7641 1.3291 0.8464 2.6124 1.1886 3.1175 1.3198 3.3957 1.3623</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE VII :</head><label>VII</label><figDesc>Ablation Study of Normalizing Flow for LTTF on the Wind dataset.</figDesc><table><row><cell>Setting</cell><cell>Multivariate Time-series Forecasting</cell><cell>Univariate Time-series Forecasting</cell></row><row><cell>Predict Length</cell><cell cols="2">48 MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE 96 192 48 96 192</cell></row><row><cell>Conformer</cell><cell cols="2">0.9479 0.6539 1.1725 0.7641 1.3291 0.8464 2.6124 1.1886 3.1175 1.3198 3.3957 1.3623</cell></row><row><cell>Conformer Conformer ze ze+z d -N F -N F Conformer z d -N F</cell><cell cols="2">1.0082 0.7015 1.2488 0.8017 1.4095 0.8686 2.7961 1.2492 3.3128 1.3807 3.4604 1.4155 0.9866 0.6953 1.2163 0.7960 1.3632 0.8599 2.7514 1.2363 3.2797 1.3814 3.4432 1.4176 0.9956 0.6949 1.2167 0.7954 1.3682 0.8473 2.7977 1.2651 3.3767 1.4256 3.5208 1.4302</cell></row><row><cell cols="3">Conformer -N F 0.9796 0.6927 1.2184 0.8015 1.3455 0.8517 2.7974 1.2614 3.5117 1.4469 3.4421 1.4159</cell></row><row><cell cols="2">(0.4352→0.3659) on ECL dataset and 7.4% (0.0914→0.0846)</cell><cell></cell></row><row><cell cols="2">on Weather dataset under predict-384 and predict-48, respec-</cell><cell></cell></row><row><cell cols="2">tively. Moreover, Conformer still achieves the best scores on</cell><cell></cell></row><row><cell cols="2">the AirDelay dataset, which further demonstrates the effective-</cell><cell></cell></row><row><cell cols="2">ness of Conformer in extracting complex temporal patterns.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE VIII :</head><label>VIII</label><figDesc>Comparisons of fusing inter-series correlation and time dependency for LTTF.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell>ECL</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Exchange</cell><cell></cell></row><row><cell>Predict Length</cell><cell>48</cell><cell>96</cell><cell>192</cell><cell>384</cell><cell>768</cell><cell>48</cell><cell>96</cell><cell>192</cell><cell>384</cell></row><row><cell>Conformer</cell><cell cols="9">MSE 0.1921 0.2124 0.2378 0.2643 0.3396 0.0764 0.1193 0.2900 0.4730 MAE 0.3034 0.3193 0.3456 0.3620 0.4092 0.2093 0.2607 0.4187 0.5369</cell></row><row><cell>Conformer (Method 1)</cell><cell cols="9">MSE 0.2003 0.2713 0.2826 0.2898 0.3441 0.1839 0.2938 0.4347 1.0596 MAE 0.3117 0.3784 0.3790 0.3775 0.4150 0.3371 0.4313 0.5190 0.8072</cell></row><row><cell>Conformer (Method 2)</cell><cell cols="9">MSE 0.1965 0.2354 0.2632 0.2987 0.3437 0.1593 0.3433 0.4321 0.5486 MAE 0.3007 0.3323 0.3587 0.3821 0.4191 0.3144 0.4530 0.5197 0.6007</cell></row><row><cell>Conformer (Method 3)</cell><cell cols="9">MSE 0.1997 0.2791 0.2771 0.3061 0.3433 0.2443 0.3310 0.4089 0.9034 MAE 0.3117 0.3854 0.3744 0.3881 0.4135 0.3795 0.4597 0.4965 0.7444</cell></row><row><cell>Conformer (Method 4)</cell><cell cols="9">MSE 0.2010 0.2735 0.2749 0.3078 0.3391 0.1135 0.1534 0.2344 0.5701 MAE 0.3135 0.3770 0.3710 0.3899 0.4076 0.2597 0.3055 0.3805 0.5978</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE IX :</head><label>IX</label><figDesc>Comparisons of feeding hidden states to the normalizing flow block. The best scores are in boldface and the 2nd best scores are in underlines.</figDesc><table><row><cell cols="2">Dataset</cell><cell></cell><cell></cell><cell>ECL</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Exchange</cell></row><row><cell cols="2">Predict Length</cell><cell>48</cell><cell>96</cell><cell>192</cell><cell>384</cell><cell>768</cell><cell>48</cell><cell>96</cell><cell>192</cell><cell>384</cell></row><row><cell cols="2">Conformer</cell><cell cols="9">MSE 0.1921 0.2124 0.2378 0.2643 0.3396 0.0764 0.1193 0.2900 0.4730 MAE 0.3034 0.3193 0.3456 0.3620 0.4092 0.2093 0.2607 0.4187 0.5369</cell></row><row><cell>Conformer (h</cell><cell>(e) k , h (d) k )</cell><cell cols="9">MSE 0.1901 0.2300 0.2814 0.3057 0.3387 0.1150 0.1506 0.2787 0.5593 MAE 0.3010 0.3322 0.3776 0.3920 0.4168 0.2643 0.3013 0.4108 0.5872</cell></row><row><cell>Conformer (h</cell><cell>(e) 1 , h (d) k )</cell><cell cols="9">MSE 0.2004 0.2283 0.2554 0.2896 0.3398 0.1156 0.1476 0.2577 0.6053 MAE 0.3112 0.3321 0.3588 0.3782 0.4121 0.2655 0.2983 0.3911 0.6086</cell></row><row><cell>Conformer (h</cell><cell>(e) 1 , h (d) 1 )</cell><cell cols="9">MSE 0.1984 0.2265 0.2507 0.2751 0.3565 0.1181 0.1669 0.2498 0.5300 MAE 0.3083 0.3304 0.3543 0.3732 0.4226 0.2676 0.3157 0.3925 0.5609</cell></row><row><cell>Conformer (h</cell><cell>(e)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>k , h (d) 1 ) MSE 0.1904 0.2202 0.2512 0.2799 0.3547 0.1155 0.1497 0.2846 0.5203 MAE 0.3018 0.3260 0.3586 0.3796 0.4259 0.2654 0.3004 0.4140 0.5549</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://archive.ics.uci.edu/ml/ datasets/ElectricityLoadDiagrams20112014</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.bgc-jena.mpg.de/wetter/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We collect this dataset and publish it at https://github.com/PaddlePaddle/ PaddleSpatial/tree/main/paddlespatial/datasets/WindPower.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://www.transtats.bts.gov. The processed dataset is available at https://github.com/PaddlePaddle/PaddleSpatial/tree/main/paddlespatial/ datasets/AirDelay.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>Hereinafter, all the experiments are carried out under the multivariate TF setting by default.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>We thank <rs type="institution">China Longyuan Power Group Corp. Ltd.</rs> for supporting this work. Besides, this work was supported in part by <rs type="funder">National Key R&amp;D Progamm of China</rs> (No. <rs type="grantNumber">2021ZD0110303</rs>).</p></div>
			</div>
			<div type="funding">
<div> *   <p>This work was done when the first author was an intern at <rs type="affiliation">Baidu Research</rs> under the supervision of the second author.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HWEcf9C">
					<idno type="grant-number">2021ZD0110303</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling long-and short-term temporal patterns with deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A review of wind speed and wind power forecasting with deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Energy</title>
		<imprint>
			<biblScope unit="volume">304</biblScope>
			<biblScope unit="page">117766</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint air quality and weather prediction based on multi-adversarial spatiotemporal networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 35th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Funnel: automatic mining of spatially coevolving epidemics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sakurai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Van Panhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stock price prediction using the arima model</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ariyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Adewumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Ayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 UKSim-AMSS 16th International Conference on Computer Modelling and Simulation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="106" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep autoregressive networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1242" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Estimating structured vector autoregressive models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Melnyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="830" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Financial time series forecasting using support vector machines</title>
		<author>
			<persName><forename type="first">K.-J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="307" to="319" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepar: Probabilistic forecasting with autoregressive recurrent networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Flunkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Januschowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1181" to="1191" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1907.00235</idno>
		<ptr target="http://arxiv.org/abs/1907.00235" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Informer: Beyond efficient transformer for long sequence time-series forecasting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Longformer: The longdocument transformer</title>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno>abs/2004.05150</idno>
		<ptr target="https://arxiv.org/abs/2004.05150" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ontañón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2007.14062" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2007">2007.14062. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gawlikowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R N</forename><surname>Tassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Humt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kruspe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roscher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03342</idno>
		<title level="m">A survey of uncertainty in deep neural networks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Support vector machine with adaptive parameters in financial time series forecasting</title>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E H</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1506" to="1518" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Gate-variants of gated recurrent unit (gru) neural networks,&quot; in 2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS)</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Salem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1597" to="1600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Conditional time series forecasting with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borovykh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bohte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Oosterlee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04691</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Time-series modeling with undecimated fully convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mittelman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.00317</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dilated convolutional neural networks for time series forecasting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borovykh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bohte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Oosterlee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Finance</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Forthcoming</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multivariate time series forecasting with dilated residual convolutional neural networks for urban air quality prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Benhaddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ouarzazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arabian Journal for Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3423" to="3442" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>abs/1706.03762</idno>
		<ptr target="http://arxiv.org/abs/1706.03762" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.10509" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1904">1904.10509, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gaussian mixture models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of biometrics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">741</biblScope>
			<biblScope unit="page" from="659" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Dynamic gaussian mixture based deep generative model for robust forecasting on sparse multivariate time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Davidson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02164</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end learning of coherent probabilistic forecasts for hierarchical time series</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Rangapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Benidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mercado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Januschowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Research</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ser. Machine Learning</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v139/rangapuram21a.html" />
		<imprint>
			<date type="published" when="2021-07-24">18-24 Jul 2021</date>
			<biblScope unit="page" from="8832" to="8843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Seward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno>abs/2101.12072</idno>
		<ptr target="https://arxiv.org/abs/2101.12072" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Normalizing flows: An introduction and review of current methods</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brubaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The fast fourier transform</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Nussbaumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fast Fourier Transform and Convolution Algorithms</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="80" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">What is the fast fourier transform?</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Cochran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Cooley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Favin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Helms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Kaenel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Maling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Rader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1664" to="1674" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Identifying seasonality in time series by applying fast fourier transform</title>
		<author>
			<persName><forename type="first">H</forename><surname>Musbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Hawary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Electrical Power and Energy Conference (EPEC)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Ljung</surname></persName>
		</author>
		<title level="m">Time series analysis: forecasting and control</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Latent lstm allocation: Joint clustering and non-linear dynamic modeling of sequence data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Revealing the dark secrets of BERT</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1908.08593" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1908">1908.08593, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A day-ahead pv power forecasting method based on lstm-rnn model and time correlation modification under partial daily pattern prediction framework</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy Conversion and Management</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="page">112766</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Prediction of time series gene expression and structural analysis of gene regulatory networks using recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fiorentino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Milanetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gosti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Tartaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">141</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Stl: A seasonal-trend decomposition procedure based on loess</title>
		<author>
			<persName><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Irma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of official statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="73" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Temporal latent auto-encoder: A method for probabilistic multivariate time series forecasting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Quanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="9117" to="9125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4743" to="4751" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">N-beats: Neural basis expansion analysis for interpretable time series forecasting</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carpov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ts2vec: Towards universal representation of time series</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="8980" to="8987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Swin transformer v2: Scaling up capacity and resolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
