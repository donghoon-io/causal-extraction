<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mutually-Regularized Dual Collaborative Variational Auto-encoder for Recommendation Systems</title>
				<funder>
					<orgName type="full">Tencent</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-11-21">21 Nov 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yaochen</forename><surname>Zhu</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
							<email>zzchen@whu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University Wuhan</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Media Lab</orgName>
								<address>
									<settlement>Tencent Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University Wuhan</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mutually-Regularized Dual Collaborative Variational Auto-encoder for Recommendation Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-11-21">21 Nov 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3485447.3512110</idno>
					<idno type="arXiv">arXiv:2211.11662v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T18:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommender systems</term>
					<term>Multi-VAE</term>
					<term>cold-start items</term>
					<term>generative models</term>
					<term>variational inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, user-oriented auto-encoders (UAEs) have been widely used in recommender systems to learn semantic representations of users based on their historical ratings. However, since latent item variables are not modeled in UAE, it is difficult to utilize the widely available item content information when ratings are sparse. In addition, whenever new items arrive, we need to wait for collecting rating data for these items and retrain the UAE from scratch, which is inefficient in practice. Aiming to address the above two problems simultaneously, we propose a mutually-regularized dual collaborative variational auto-encoder (MD-CVAE) for recommendation. First, by replacing randomly initialized last layer weights of the vanilla UAE with stacked latent item embeddings, MD-CVAE integrates two heterogeneous information sources, i.e., item content and user ratings, into the same principled variational framework where the weights of UAE are regularized by item content such that convergence to a non-optima due to data sparsity can be avoided. In addition, the regularization is mutual in that user ratings can also help the dual item content module learn more recommendationoriented item content embeddings. Finally, we propose a symmetric inference strategy for MD-CVAE where the first layer weights of the UAE encoder are tied to the latent item embeddings of the UAE decoder. Through this strategy, no retraining is required to recommend newly introduced items. Empirical studies show the effectiveness of MD-CVAE in both normal and cold-start scenarios.</p><p>Codes are available at <ref type="url" target="https://github.com/yaochenzhu/MD-CVAE">https://github.com/yaochenzhu/MD-CVAE</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Personalized recommendation plays a pivotal role in modern web services. Collaborative filtering (CF), which recommends new items by exploiting similarity patterns in users' historical interactions, has become a fundamental component of existing recommender systems <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref>. However, the performance of CF degenerates significantly when collected historical ratings are sparse. In addition, it cannot recommend items that have yet received any ratings from users. Consequently, hybrid methods that utilize item content as auxiliary information to address the sparsity and cold-start problems have gained more attention among researchers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Item content can be naturally incorporated into traditional matrix factorization (MF)-based CF methods <ref type="bibr" target="#b2">[3]</ref>. These methods model users' preference to items by the inner product between the corresponding user, item embeddings, where item content information can be directly used to constrain the factorized item collaborative embeddings to prevent the model from stuck into a bad solution due to the sparsity of ratings <ref type="bibr" target="#b15">[16]</ref>. However, since these factorizationbased methods can only capture the linear similarity between users and items, their model capacity is severely restricted for the recommendation demand of large-scale modern online platforms.</p><p>Recently, auto-encoders (AEs) have been widely adopted in CF <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34]</ref> due to their ability to learn compact user representations from sparse inputs and to encode non-linear user similarities based on their historical interactions. As a Bayesian version of AE, variational auto-encoder (VAE) demonstrates superiority because it explicitly models the uncertainty in user latent variable as the embedding variance and is more robust to rating noise <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref>. However, since these AEs are user-oriented, i.e., they encode users' historical interactions into user latent embeddings and from them directly reconstruct ratings for recommendations, latent item factors are unintentionally eliminated from the model (see Fig. <ref type="figure" target="#fig_0">1</ref>). Although such a characteristic is efficient to fold-in (i.e., infer the latent user variable and reconstruct the ratings for) new users whose click histories are not present in the training data <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref> (Folding-in new users for MF-based methods, in comparison, requires laborious iterative inference process), the direct way to incorporate item content information is obscured. This severely degenerates the recommendation performance for items with sparse ratings. Faced with this dilemma, several pioneering works have explored methods to utilize the widely available item content information in UAEs. An exemplar method is the collective VAE (CoVAE) <ref type="bibr" target="#b3">[4]</ref>, where item feature co-occurrences are used to train UAE together with user ratings (which are essentially item co-purchases). Although improvement has been observed compared with the vanilla UAEs, since user ratings and item feature co-occurrences are two heterogeneous information sources, such a treatment lacks some theoretical interpretability. In addition, since item features are only used as extra training samples, the collaborative and content modules of CoVAE are loosely coupled, and to make out-of-matrix predictions, i.e., to recommend newly introduced items, is still infeasible. To address the above challenges, we propose a Bayesian generative model called mutually-regularized dual collaborative variational auto-encoder (MD-CVAE) for recommendations. By replacing randomly initialized last layer weights of the vanilla UAE with stacked latent item embeddings, MD-CVAE tightly couples UAE and an item content VAE into a unified variational framework where item content information can be introduced to regularize the weights of UAE, preventing it from converging to a non-optima when historical ratings are sparse. In turn, the collaborative information in user ratings constrains the item content VAE to learn more recommendation-oriented item representations. Furthermore, we propose a symmetric inference strategy for MD-CVAE that ties the first layer weights of UAE encoder to the latent item embeddings of UAE decoder. Through this mechanism, MD-CVAE can directly recommend new items without a time-consuming model retraining process, where latency can be substantially reduced. The main contribution of this paper can be summarized as:</p><p>â€¢ By viewing UAE weights from an item embedding perspective, we propose a hierarchical Bayesian generative framework, MD-CVAE, that seamlessly unifies UAE with an item content representation learning module to address the sparsity and cold-start problems of vanilla UAEs. â€¢ The MD-CVAE is tightly coupled in that item content embeddings regularize the weights of UAE to prevent bad model solutions due to rating sparsity, while user ratings in turn help the item content embedding module to learn more recommendation-oriented content representations. â€¢ A symmetric inference strategy is proposed for MD-CVAE to address the out-of-matrix recommendation problem where existing UAE-based hybrid recommender systems fail. Experiments on real-world datasets show that MD-CVAE outperforms various state-of-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM FORMULATION</head><p>In this paper, we consider recommendations with implicit feedback <ref type="bibr" target="#b10">[11]</ref>. The user-item interactions are represented by an ğ¼ by ğ½ binary rating matrix R where each row r ğ‘‡ ğ‘– denotes whether user ğ‘– has interacted with each of the existing ğ½ items. The item content is denoted by a ğ½ by ğ‘† matrix X where each row x ğ‘‡ ğ‘— is the extracted content feature for item ğ‘—<ref type="foot" target="#foot_0">foot_0</ref> . Given partial observations of R and item content X, the primary target of MD-CVAE is to predict the remaining ratings in R with the support of item content X such that new relevant items can be automatically recommended even if interactions for some items are extremely sparse. Moreover, if ğ½ â€² new items arrive with features X â€² (ğ½ â€² by ğ‘†) and no user interactions, another goal of MD-CVAE is to immediately recommend these cold-start items without a laborious model retraining process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY 3.1 Generative Process</head><p>With a proper definition of the problems and goals, we are ready to introduce the mutually-regularized dual collaborative variational auto-encoder (MD-CVAE). The PGM of MD-CVAE is illustrated in Fig. <ref type="figure">2</ref>. In MD-CVAE, users and items are represented in two lowdimensional latent spaces of dimension ğ¾ ğ‘¢ and ğ¾ ğ‘£ , respectively. As with most probabilistic recommendation methods, the latent variable u for user ğ‘– is drawn from a Gaussian distribution as:</p><formula xml:id="formula_0">u âˆ¼ N (0, ğœ† -1 ğ‘¢ I ğ¾ ğ‘¢ ).<label>(1)</label></formula><p>Different from the vanilla UAE (i.e., Multi-VAE in <ref type="bibr" target="#b16">[17]</ref>) where the latent item factors are eliminated from consideration, we explicitly introduce z ğ‘ to embed the item collaborative information for item ğ‘—, and draw it from another Gaussian distribution as:</p><formula xml:id="formula_1">z ğ‘ âˆ¼ N (0, ğœ† -1 ğ‘£ I ğ¾ ğ‘£ ).<label>(2)</label></formula><p>With the latent item collaborative embedding z ğ‘ explicitly introduced into the probabilistic graph, the widely available item content can be used to incorporate auxiliary information into the system. Specifically, MD-CVAE draws the latent item content variable from a standard normal distribution as follows:</p><formula xml:id="formula_2">z ğ‘¡ âˆ¼ N (0, I ğ¾ ğ‘£ ).<label>(3)</label></formula><p>Importantly, to tightly couples the item collaborative and content variables, as with the collaborative variational auto-encoder (CVAE)</p><formula xml:id="formula_3">H inf b H inf b H inf b X Z t R H gen t H gen t U R W W b b inf H gen b H gen b H gen b V V Î» u I J X UAE H inf t V V Sym. Normal s,T s Figure 2:</formula><p>The probabilistic graphical model (PGM) of MD-CVAE. Solid lines denote the generative process whereas dashed lines denote the inference process.</p><p>[16], we set the latent item variable v to be composed of both item collaborative and content latent variables as follows:</p><formula xml:id="formula_4">v = z ğ‘ + z ğ‘¡ .<label>(4)</label></formula><p>Given z ğ‘¡ , then, the latent item variable v follows the conditional distribution N (z ğ‘¡ , ğœ† -1 ğ‘£ I ğ¾ ğ‘£ ), which is the key to introduce mutual regularization between v and z ğ‘¡ in the MAP objective. We define the horizontally stacked latent item variables v for all ğ½ existing items as</p><formula xml:id="formula_5">V ğ‘  = [v ğ‘‡ 1 , v ğ‘‡ 2 , â€¢ â€¢ â€¢ , v ğ‘‡ ğ½ ]</formula><p>and the corresponding random matrix as ğ‘‰ ğ‘  (The superscript ğ‘  is short for stacked). To generate the rating vector r, we first embed the user latent variable u into a latent representation with an ğ‘ -1-layer multi-layer perceptron MLP ğ‘¢,ğ‘”ğ‘’ğ‘› : R ğ¾ ğ‘¢ â†’ R ğ¾ ğ‘£ with trainable weights ğœ½ ğ‘Ÿ as h ğ‘”ğ‘’ğ‘› ğ‘ (u) = MLP ğ‘¢,ğ‘”ğ‘’ğ‘› (u). Then, following Multi-VAE <ref type="bibr" target="#b16">[17]</ref>, we draw the rating vector r from a multinomial distribution ğ‘ ğœ½ ğ‘Ÿ (r | u, V ğ‘  ) as follows:</p><formula xml:id="formula_6">r âˆ¼ ğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘– softmax V ğ‘  â€¢ h ğ‘”ğ‘’ğ‘› ğ‘ (u) , #ğ¼ğ‘›ğ‘¡ ,<label>(5)</label></formula><p>where #ğ¼ğ‘›ğ‘¡ is the number of interacted items for user ğ‘–. Eq. (5) defines an ğ‘ -layer MLP on top of MLP ğ‘¢,ğ‘”ğ‘’ğ‘› where the stacked item latent variable V ğ‘  can be viewed as its last layer weights. Softmax is used as the activation to make the outputs valid multinomial parameters where the probability mass is summed to one. This MLP is similar to the decoder of the vanilla UAE, except that the last layer weights are replaced with latent item embeddings where item content information can be incorporated as Eq. ( <ref type="formula" target="#formula_4">4</ref>). Through this mechanism, MD-CVAE generalizes the generative process of vanilla UAEs such that item side information can be tightly coupled with their decoder's last layer weights for hybrid recommendations.</p><p>To emphasize that V ğ‘  also serves the last layer weights of the UAE decoder, we rewrite</p><formula xml:id="formula_7">ğ‘ ğœ½ ğ‘Ÿ (r | u, V ğ‘  ) as ğ‘ ğœ½ ğ‘Ÿ ,V ğ‘  (r | u)</formula><p>, where the union of ğœ½ ğ‘Ÿ , V ğ‘  represents the set of all decoder weights. Moreover, compared to CVAE that uses a linear MF as the collaborative backbone, MD-CVAE inherits the computational efficiency of UAEs to fold-in new users whose interactive history is not included in the training set, where the whole ratings of a user can be generated from the user latent variables via ğ‘ ğœ½ ğ‘Ÿ ,V ğ‘  (r | u) by one forward pass.</p><p>The item content x is generated from z ğ‘¡ through ğ‘ (x | z ğ‘¡ ) parameterized by another dual item MLP. Specifically, if x is real-valued, it can be generated from a normal distribution N (MLP ğ‘–,ğ‘”ğ‘’ğ‘› (z ğ‘¡ ), ğœ† -1 ğ‘¥ I ğ‘† ) where the output of the MLP denotes its mean and ğœ† ğ‘¥ is the precision; or if x is binary, we squash the output by sigmoid function and draw x from the corresponding Bernoulli distribution. Accordingly, we denote the random matrices for horizontally stacked z ğ‘¡ and x as ğ‘ ğ‘  ğ‘¡ and ğ‘‹ ğ‘  , respectively. With the above generative process defined, MD-CVAE is represented by the joint distribution of observable and hidden variables for user ğ‘– and all ğ½ existing items as follows:</p><formula xml:id="formula_8">ğ‘ ğœ½ (ğ‘…, ğ‘‹ ğ‘  , ğ‘ˆ , ğ‘‰ ğ‘  , ğ‘ ğ‘  ğ‘¡ ) =ğ‘ ğœ½ ğ‘Ÿ ,ğ‘‰ ğ‘  (ğ‘… | ğ‘ˆ ) â€¢ ğ‘ ğœ½ ğ‘¥ (ğ‘‹ ğ‘  | ğ‘ ğ‘  ğ‘¡ )â€¢ ğ‘ (ğ‘‰ ğ‘  | ğ‘ ğ‘  ğ‘¡ ) â€¢ ğ‘ (ğ‘ ğ‘  ğ‘¡ ) â€¢ ğ‘ (ğ‘ˆ ),<label>(6)</label></formula><p>where ğœ½ ğ‘¥ denotes trainable weights of the generative neural network for the item content and</p><formula xml:id="formula_9">ğœ½ = {ğœ½ ğ‘Ÿ , ğœ½ ğ‘¥ }. Moreover, ğ‘ ğœ½ ğ‘¥ (ğ‘‹ ğ‘  | ğ‘ ğ‘  ğ‘¡ ) and ğ‘ (ğ‘‰ ğ‘  | ğ‘ ğ‘  ğ‘¡ ) can be factorized into the product of per item distributions Î  ğ‘— ğ‘ ğœ½ ğ‘¥ (ğ‘‹ | ğ‘ ğ‘¡ ), Î  ğ‘— ğ‘ (ğ‘‰ | ğ‘ ğ‘¡ )</formula><p>, respectively, due to the assumption of marginal independence among ğ½ items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Normal Inference: Vanilla MD-CVAE</head><p>Given Eq. ( <ref type="formula" target="#formula_8">6</ref>), however, the non-linearity of generative process precludes us from calculating the posterior distribution of the latent variables analytically, as computing the model evidence log ğ‘ ğœ½ (ğ‘…, ğ‘‹ ğ‘  ) requires integrating the latent space of ğ‘ˆ , ğ‘‰ ğ‘  , ğ‘ ğ‘  ğ‘¡ , which is intractable. Therefore, we resort to variational inference <ref type="bibr" target="#b0">[1]</ref>, where we introduce variational distribution ğ‘ ğ“ (ğ‘ˆ , ğ‘‰ ğ‘  , ğ‘ ğ‘  ğ‘¡ | ğ‘…, ğ‘‹ ğ‘  ) parameterized by deep neural networks with trainable parameters ğ“ and in ğ‘ ğ“ find the distribution closest to the true but intractable posterior measured by KL divergence as an approximation. According to the conditional independence implied by the PGM of MD-CVAE, the joint posterior of all hidden variables can be factorized into the product of three compact parts as follows:</p><formula xml:id="formula_10">ğ‘ ğ“ (ğ‘ˆ , ğ‘‰ ğ‘  , ğ‘ ğ‘  ğ‘¡ | ğ‘…, ğ‘‹ ğ‘  ) = ğ‘ ğ“ (ğ‘ˆ | ğ‘…) â€¢ ğ‘ ğ“ (ğ‘ ğ‘  ğ‘¡ | ğ‘‹ ğ‘  ) â€¢ ğ‘(ğ‘‰ ğ‘  | ğ‘ ğ‘  ğ‘¡ ),<label>(7)</label></formula><p>where ğ‘(ğ‘‰ ğ‘  | ğ‘ ğ‘  ğ‘¡ ) is assumed to be a conditional Gaussian distribution with ğ‘ ğ‘  ğ‘¡ as the mean and a pre-defined fixed value as the variance. According to the variational approximation theory <ref type="bibr" target="#b11">[12]</ref>, minimizing the KL divergence is equivalent to maximization of the evidence lower bound (ELBO) as follows:</p><formula xml:id="formula_11">L =E ğ‘ ğ“ [log ğ‘ ğœ½ (ğ‘…, ğ‘‹ ğ‘  , ğ‘ˆ , ğ‘‰ ğ‘  , ğ‘ ğ‘  ğ‘¡ ) -log ğ‘ ğ“ (ğ‘ˆ , ğ‘‰ ğ‘  , ğ‘ ğ‘  ğ‘¡ | ğ‘…, ğ‘‹ ğ‘  )] =E ğ‘ ğ“ [log ğ‘ ğœ½ (ğ‘… | ğ‘ˆ ) + log ğ‘ (ğ‘‰ ğ‘  | ğ‘ ğ‘  ğ‘¡ ) + log ğ‘ ğœ½ (ğ‘‹ ğ‘  | ğ‘ ğ‘  ğ‘¡ )] -KL ğ‘ ğ“ (ğ‘ ğ‘  ğ‘¡ | ğ‘‹ ğ‘  )âˆ¥ğ‘ (ğ‘ ğ‘  ğ‘¡ ) -KL ğ‘ ğ“ (ğ‘ˆ | ğ‘…)âˆ¥ğ‘ (ğ‘ˆ ) + C,<label>(8)</label></formula><p>where the term C includes the entropy of ğ‘(ğ‘‰ ğ‘  | ğ‘ ğ‘  ğ‘¡ ), which is constant due to its fixed variance. The two parameterized terms in the factorized posterior, i.e., ğ‘ ğ“ (ğ‘ˆ | ğ‘…), ğ‘ ğ“ (ğ‘ ğ‘  ğ‘¡ | ğ‘‹ ğ‘  ), correspond to the encoder of the UAE and the encoder of the dual item VAE in MD-CVAE, respectively. The maximum of Eq. ( <ref type="formula" target="#formula_11">8</ref>) is achieved if and only if the variational distribution exactly matches true posterior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Symmetric Inference: MDsym-CVAE</head><p>The generative process of MD-CVAE allows us to incorporate widely available item content information to generate ratings from user latent variables for recommendations. Given the historical ratings r for user ğ‘–, however, exploiting item content to support the inference of u that governs the generation of r is still infeasible, which may lead to large uncertainty in the estimation if observed ratings r for the user is sparse. In this section, we propose a symmetric inference strategy for MD-CVAE to solve this problem.</p><p>We demonstrate that, like the last layer weights of a UAE decoder, the first layer weights of a UAE encoder can also be interpreted as latent item embeddings. The reason is that, given that the inputs to UAEs are binary rating vectors denoting the implicit feedback, the first dense layer of the UAE encoder, which represents matrixvector multiplication between its weights and a binary rating vector, can be decomposed into two basic operations, i.e., embedding and element-wise sum as follows (bias omitted for simplicity):</p><formula xml:id="formula_12">h ğ‘–ğ‘›ğ‘“ ğ‘ = W ğ‘–ğ‘›ğ‘“ ğ‘ â€¢ r = âˆ‘ï¸ ğ‘— I(ğ‘Ÿ ğ‘— = 1) â€¢ w ğ‘–ğ‘›ğ‘“ ğ‘,ğ‘— ,<label>(9)</label></formula><p>where I is the indicator function and w ğ‘–ğ‘›ğ‘“ ğ‘,ğ‘— is the ğ‘—th column of weight matrix W ğ‘–ğ‘›ğ‘“ ğ‘ (see Fig. <ref type="figure" target="#fig_1">3</ref>). Therefore, a one-to-one relationship can be established between the ğ‘—th column of W ğ‘–ğ‘›ğ‘“ ğ‘ and the ğ‘—th item, which allows us to view W ğ‘–ğ‘›ğ‘“ ğ‘ as vertically stacked embeddings of the ğ½ existing items. Consequently, item content embeddings z ğ‘¡,ğ‘— can be fused with w ğ‘–ğ‘›ğ‘“ ğ‘,ğ‘— the same way as Eq. ( <ref type="formula" target="#formula_4">4</ref>) to support the inference of u. However, a better strategy is to reuse v ğ‘— in the UAE decoder as the weights w ğ‘–ğ‘›ğ‘“ ğ‘,ğ‘— by setting W ğ‘–ğ‘›ğ‘“ ğ‘ = V ğ‘ ,ğ‘‡ , where ğ‘‡ stands for matrix transposition, such that extra structure regularization is imposed on the UAE part of the MD-CVAE to avoid overfitting on sparse ratings. Since through this strategy, the UAE encoder in MD-CVAE is a symmetric version of the UAE decoder (by the first and the last layer), we use MDsym-CVAE to distinguish it from the vanilla MD-CVAE. Compared to MD-CVAE, MDsym-CVAE has another advantage of direct recommending cold-start items without a laborious model retraining process, which will be thoroughly discussed in the out-of-matrix prediction section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Maximum A Posteriori Estimate</head><p>Given Eq. ( <ref type="formula" target="#formula_11">8</ref>), MD-CVAE can be trained end-to-end. However, a joint training may lead to the model's converging to an undesirable sub-optima where it relies solely on one information source for recommendations. Therefore, we adopt an EM-like optimization strategy as CVAE. First, for user ğ‘– we fix stacked item content random vectors, i.e., ğ‘ ğ‘  ğ‘¡ , to áºğ‘  ğ‘¡ , and maximize the following objective:</p><formula xml:id="formula_13">L ğ‘€ğ´ğ‘ƒ ğ‘_step = E ğ‘ ğ“ (ğ‘ˆ |ğ‘…) log ğ‘ ğœ½ (ğ‘… | ğ‘ˆ ) - ğœ† ğ‘£ 2 â€¢ âˆ¥ğ‘‰ ğ‘  -áºğ‘  ğ‘¡ âˆ¥ 2 ğ¹ -KL ğ‘ ğ“ (ğ‘ˆ | ğ‘…)âˆ¥ğ‘ (ğ‘ˆ ) - ğœ† ğ‘Š 2 â€¢ âˆ‘ï¸ ğ‘™ âˆ¥W (ğ‘™) ğ‘ âˆ¥ 2 ğ¹ ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_14">W (ğ‘™)</formula><p>ğ‘ is the ğ‘™th layer weights of the UAE part of MD-CVAE (which are also assumed to be Gaussian), ğœ† ğ‘Š is the precision, and ğ¹ is the Frobenius norm. Eq. <ref type="bibr" target="#b9">(10)</ref> trains the UAE with an extra item content constraint to its last layer weights ğ‘‰ ğ‘  . Then, for item ğ‘—, we fix the ğ‘—th row of the updated weights ğ‘‰ ğ‘  to V and optimize the following objective of the dual item content VAE as follows:</p><formula xml:id="formula_15">L ğ‘€ğ´ğ‘ƒ ğ‘¡ _step = E ğ‘ ğ“ (ğ‘ ğ‘¡ |ğ‘‹ ) log ğ‘ ğœ½ (ğ‘‹ | ğ‘ ğ‘¡ ) - ğœ† ğ‘£ 2 â€¢ âˆ¥ V -ğ‘ ğ‘¡ âˆ¥ 2 2 -KL ğ‘ ğœ™ (ğ‘ ğ‘¡ | ğ‘‹ )âˆ¥ğ‘ (ğ‘ ğ‘¡ ) - ğœ† ğ‘Š 2 â€¢ âˆ‘ï¸ ğ‘™ âˆ¥W (ğ‘™) ğ‘¡ âˆ¥ 2 ğ¹ ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_16">W (ğ‘™)</formula><p>ğ‘¡ is the ğ‘™th layer weights of the dual item content VAE of MD-CVAE. For both L ğ‘€ğ´ğ‘ƒ ğ‘¡ _step and L ğ‘€ğ´ğ‘ƒ ğ‘_step the objective is composed of three parts: (1) The expected log-likelihood term, which encourages the latent user, item variables to best explain the observed user ratings and item content. <ref type="bibr" target="#b1">(2)</ref> the MSE between the UAE weights and latent item content embeddings term, which tightly couples the user and item VAE where user ratings and item content mutually constrain each other to learn better representations for recommendations. (3) The KL with prior and weight decay terms, which impose regularization to the network weights to alleviate over-fitting. Liang et al. <ref type="bibr" target="#b16">[17]</ref> have shown that the KL term in the collaborative part could be too strong, which over-constrains the user collaborative embeddings. As a solution, they introduced a scalar ğ›½ to control the weight of the KL term in Eq. <ref type="bibr" target="#b9">(10)</ref>. Under such a setting, the model learns to encode as much information of r in u as it can at the initial training stages while gradually regularizing u by forcing it close to the prior as the training proceeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Reparameterization Trick</head><p>We use reparameterization trick to make the optimization of Eqs. <ref type="bibr" target="#b9">(10)</ref> and <ref type="bibr" target="#b10">(11)</ref> amenable <ref type="bibr" target="#b13">[14]</ref>. For the two Gaussian latent variables z ğ‘” âˆˆ {u, z ğ‘¡ }, we calculate their mean and logarithm of standard deviation via the corresponding encoder network and take samples z (ğ‘™) ğ‘” via the following differentiable bi-variate transformation:</p><formula xml:id="formula_17">z (ğ‘™) ğ‘” [ğ z ğ‘” , ğˆ z ğ‘” ], ğ (ğ‘™) = ğ z ğ‘” + ğ (ğ‘™) âŠ™ ğˆ z ğ‘” ,<label>(12)</label></formula><p>where ğ (ğ‘™) âˆ¼ N (0, I ğ¾ {ğ‘¢,ğ‘£} ). The samples are then forwarded to the corresponding decoder network to reconstruct the inputs. When backward propagated, it was shown in <ref type="bibr" target="#b13">[14]</ref> that the sampling gradient is an unbiased estimator of the gradient of L ğ‘€ğ´ğ‘ƒ {ğ‘¡,ğ‘ }_step w.r.t. the network parameters. Moreover, previous work has shown that the variance of reparameterization trick is small enough such one sample for each data point suffices for convergence <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">In-Matrix Prediction</head><p>After the training of MD-CVAE, when conducting in-matrix predictions for a user, i.e., all candidate items have already been rated by at least one other user in the collected data, the historical interactions of that user can be used to calculate the mean of her latent embedding ğ u via the UAE encoder. Then, the multinomial probabilities of all hold-out items can be obtained via the UAE decoder, which are ranked to fetch ğ‘€ most relevant ones for recommendations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Out-of-Matrix Prediction</head><p>The out-of-matrix prediction (i.e. there exist items in the candidate set that have yet received any ratings from the users) could be categorized into the offline case and the online case, with differences between the two cases illustrated in Fig. <ref type="figure" target="#fig_2">4</ref> for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.7.1</head><p>The Offline Case. In the offline case, the never-before-visited items are mixed with normal items in the offline model training phase. Two problems could occur if such items exist for vanilla UAEs. First, in the rating generation process, since these items get no positive feedback, all embedded user latent variables h ğ‘”ğ‘’ğ‘› ğ‘ (u) are discouraged from being close to the last layer weights associated with these items in Eq. ( <ref type="formula" target="#formula_6">5</ref>). Therefore, these items would hardly get any recommendations, and users will be stuck to the same old items. In addition, in the model inference process, according to Eq. ( <ref type="formula" target="#formula_12">9</ref>), once initialized, the columns of the first encoder layer weights that correspond to cold-start items will never get updated. Therefore, if new users who have interacted with these items are folded-in for predictions, the inferred user latent variable ğ u would depend on randomly initialized weights, which would lead to unexpected behavior of the system. In contrast, the vanilla MD-CVAE addresses the first problem by constraining the last layer weights of UAE with item content embeddings, where recommendations of coldstart items can be made by utilizing similarity patterns between item content. Furthermore, MDsym-CVAE addresses the second problem by setting the first layer weights of UAE as the transpose of its content-regularized last layer weights, where the embeddings of fold-in users who have interacted with cold-start items will still contain content information of these items for recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.7.2</head><p>The Online Case. In the online case, ğ½ â€² new items arrive when the model has already been deployed after training on rating data of existing ğ½ items. Meanwhile, some users may have interacted with new items. When this occurs, UAE and vanilla MD-CVAE must wait for collecting users' interactions with these items and be retrained from scratch such that ğ½ â€² new items can be properly recommended (A more direct reason is the dimensional mismatch between the network inputs/outputs and its first/last layer weights when ğ½ â€² new items arrive). For MDsym-CVAE, since weights of these two layers If we denote the first and the last layer weights of the UAE part of MDsym-CVAE trained on existing ğ½ items as V ğ‘  ğ‘œğ‘™ğ‘‘ âˆˆ R ğ½ Ã—ğ¾ ğ‘£ and the horizontally stacked item content embeddings for ğ½ â€² new items as</p><formula xml:id="formula_18">V ğ‘  ğ‘›ğ‘’ğ‘¤ = [ğ ğ‘‡ z ğ‘¡ ,ğ½ +1 , ğ ğ‘‡ z ğ‘¡ ,ğ½ +2 , â€¢ â€¢ â€¢ , ğ ğ‘‡ z ğ‘¡ ,ğ½ +ğ½ â€² ] âˆˆ R ğ½ â€² Ã—ğ¾ ğ‘£</formula><p>, the new weights (or transposed weights) V ğ‘  ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿ âˆˆ R (ğ½ +ğ½ â€² )Ã—ğ¾ ğ‘£ are calculated as:</p><formula xml:id="formula_19">V ğ‘  ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿ = [V ğ‘  ğ‘œğ‘™ğ‘‘ âˆ¥ V ğ‘  ğ‘›ğ‘’ğ‘¤ ],<label>(13)</label></formula><p>where âˆ¥ denotes the operation of horizontal concatenation. V ğ‘  ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿ can then be used as the surrogate for V ğ‘  in Eqs. ( <ref type="formula" target="#formula_6">5</ref>) and ( <ref type="formula" target="#formula_12">9</ref>) to infer user latent variable u and generate user rating vector r. The remaining part of the network and the recommendation procedure of MD-CVAE remain unaltered. Through this strategy, predictions for cold-start items can be made as if they were normal items included in the training set, which reduces the frequency of model retraining and alleviates computational overhead. This is especially favorable in industrial applications where low latency matters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL ANALYSIS 4.1 Datasets</head><p>We demonstrate the experimental results on three datasets. The first dataset, citeulike-a, is a widely used hybrid recommendation benchmark collected by <ref type="bibr" target="#b25">[26]</ref> for research article recommendations, where the title and abstract of each article are used as the item content. The second dataset, movielen-10m <ref type="bibr" target="#b7">[8]</ref>, however, is rarely used to evaluate hybrid recommenders due to lack of item features. We collect the movie plots from IMDB and process the raw texts as <ref type="bibr" target="#b15">[16]</ref>. We keep movies with available plots and select a subset of users accordingly to form the movielen-sub dataset. This dataset is included to evaluate MD-CVAE when collaborative information is comparatively sufficient. The last dataset is the Amazon toys &amp; games dataset <ref type="bibr" target="#b8">[9]</ref>, which is larger in scale and sparser. We concatenate all reviews an item receives as the item features. For all three datasets, we obtain an 8,000-dimensional feature for each item from the item textual features as <ref type="bibr" target="#b15">[16]</ref>. In preprocessing, we randomly split the users by the ratio 8:1:1 for training, validation, testing, respectively. For each user, 20% of interacted items are held out for evaluation. Table <ref type="table" target="#tab_0">1</ref> summarizes the detail of the datasets after preprocessing. Fig. <ref type="figure" target="#fig_3">5</ref> illustrates the distributions of interaction density for different items. The distribution of interaction density for all three datasets clearly demonstrates a long-tail characteristic, which hinders good recommendations for items with sparse ratings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>Two metrics are used to evaluate the model performance in this paper: Recall@ğ‘€ and truncated normalized discounted cumulative gain (NDCG@ğ‘€). For a user ğ‘–, we first obtain the predicted rank of hold-out items by sorting their predicted multinomial probabilities.</p><p>If we denote the item at rank ğ‘Ÿ by ğ‘— (ğ‘Ÿ ) and the set of hold-out items for user ğ‘– by J ğ‘– , Recall@ğ‘€ is then calculated as:</p><formula xml:id="formula_20">Recall @ğ‘€ (ğ‘–) = ğ‘€ ğ‘Ÿ =1 I [ ğ‘— (ğ‘Ÿ ) âˆˆ J ğ‘– ] min (ğ‘€, |J ğ‘– |) ,<label>(14)</label></formula><p>where the denominator is the minimum of ğ‘€ and the number of hold-out items. Recall@ğ‘€ has a maximum of 1, achieved when all relevant items are ranked among top ğ‘€ positions. Truncated discounted cumulative gain (DCG@ğ‘€) is computed as</p><formula xml:id="formula_21">DCG @ğ‘€ (ğ‘–) = ğ‘€ âˆ‘ï¸ ğ‘Ÿ =1 2 I[ ğ‘— (ğ‘Ÿ ) âˆˆ J ğ‘– ] -1 log(ğ‘Ÿ + 1) ,<label>(15)</label></formula><p>which, instead of uniformly weighting all positions, introduces a logarithm discount function over the ranks where larger weights are applied to recommended items that appear at higher ranks. NDCG@ğ‘€ is calculated by normalizing the DCG@ğ‘€ to [0, 1] by the ideal DCG@ğ‘€ where all relevant items are ranked at the top.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Since the datasets that we include vary both in scale and scope, we select the architecture and hyperparameters of MD-CVAE on validation users through grid search. For the UAE part, we search networks with {0,1,2} hidden layer with hidden size {50, 100, 150, 200, 250} and keep the dual item VAE compatible with UAE, i.e., the dimension of latent item embedding equals to the hidden size of the last UAE layer. In addition, ğœ† ğ‘£ is an important hyperparameter that controls the strength of mutual regularization in MD-CVAE, and for each architecture, we search ğœ† ğ‘£ from {0.1, 1, 2, 5, 10}. We first layerwise pretrain the dual item content VAE and then iteratively train the UAE (ğ‘_step) and the dual item VAE (ğ‘¡_step) for 100 epochs. Adam <ref type="bibr" target="#b12">[13]</ref> is used as the optimizer with batches of 500 users. We randomly split the datasets into ten train/val/test splits. For each split, we keep the model with the best averaged value of Recall@20, Recall@40, and NDCG@100 on validation users. The metrics on test users averaged over ten splits are reported as model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Baselines</head><p>In this section, we compare MD-CVAE and MDsym-CVAE with state-of-the-art collaborative and hybrid recommendation baselines. Generally, for factorization-based methods, we search the optimal hyperparameters on validation users with Bayesian parameter search (BPS) <ref type="bibr" target="#b4">[5]</ref>, whereas for Multi-VAE-based baselines, we determine the optimal model structure with grid search.</p><p>â€¢ FM (factorization machine) is a widely employed algorithm for hybrid recommendation that is robust to sparse inputs <ref type="bibr" target="#b6">[7]</ref>. We use BPS as suggested in <ref type="bibr" target="#b4">[5]</ref> to find the optimal hyperparameters and loss function on validation users. â€¢ CTR <ref type="bibr" target="#b25">[26]</ref> learns the topics of item content via latent Dirichlet allocation (LDA) <ref type="bibr" target="#b1">[2]</ref> and couples it with probabilistic matrix factorization (PMF) <ref type="bibr" target="#b19">[20]</ref> for collaborative filtering. We find the optimal hyperparameters ğ‘, ğ‘, ğœ† ğ‘¢ , ğœ† ğ‘£ by BPS.</p><p>â€¢ CDL <ref type="bibr" target="#b26">[27]</ref> replaces the LDA in CTR with a stacked Bayesian denoising auto-encoder (SDAE) <ref type="bibr" target="#b24">[25]</ref> to learn the item content embeddings in an end-to-end manner. We set the mask rate of SDAE to 0.3 and search its architecture same as MD-CVAE. â€¢ CVAE <ref type="bibr" target="#b15">[16]</ref> further improves over CDL by utilizing a VAE in place of the Bayesian SDAE, where a self-adaptive Gaussian noise is introduced to corrupt the latent item embeddings instead of corrupting the input features with zero masks. Grid search is used to find the optimal hyperparameters. â€¢ Multi-VAE <ref type="bibr" target="#b16">[17]</ref> breaks the linear collaborative modeling ability of PMF by designing a VAE with multinomial likelihood to model user ratings such that user collaborative information in ratings can be captured for recommendations. â€¢ CoVAE <ref type="bibr" target="#b3">[4]</ref> utilizes the non-linear Multi-VAE as the collaborative backbone and incorporates item feature information by treating feature co-occurrences as pseudo training samples to collectively train Multi-VAE with item features. â€¢ CondVAE <ref type="bibr" target="#b20">[21]</ref> builds a user conditional VAE where user features are used as the condition. We extend the original CondVAE by replacing the categorical user features with averaged item features built from the interacted items, which we find have a better performance on all three datasets. â€¢ DICER <ref type="bibr" target="#b32">[33]</ref> is an item-oriented auto-encoder (IAE)-based recommender system where the item content information is utilized to learn disentangled item embeddings from their user ratings to achieve more robust recommendations. â€¢ DAVE <ref type="bibr" target="#b30">[31]</ref> hybrids an IAE with an adversarial UAE through neural collaborative filtering (NCF) <ref type="bibr" target="#b9">[10]</ref>. For a fair comparison, item content information is incorporated into the IAE part of DAVE by concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.1</head><p>In-matrix Case. We first evaluate the models for in-matrix predictions. The results are summarized in Table <ref type="table" target="#tab_1">2</ref>. Table <ref type="table" target="#tab_1">2</ref> comprises of three parts. The middle part shows the FM and three tightly-coupled hybrid baselines with linear collaborative modules. Generally, the performance improves with the increase of the representation learning ability of the item content embedding network. CVAE, which uses VAE to encode item content information, performs consistently better than CDL and CTR. However, simple methods such as FM can out-perform some deep learning baselines when their parameters are systematically searched, which is consistent with the findings in <ref type="bibr" target="#b4">[5]</ref>. The bottom part shows the vanilla UAE and hybrid baselines that employ UAE as the collaborative backbone.</p><p>Since UAE can capture non-linear similarity among users, their performance improves over linear hybrid methods when datasets are comparatively dense. However, on the sparser toys &amp; games dataset, they are out-performed by the shallow models. CoVAE does not always out-perform Multi-VAE, which indicates that the co-occurrence of item features does not necessarily coincide with the user co-purchases. Finally, DAVE performs almost consistently better than other UAE-based baselines, with an extra adversarial loss added to both user and item auto-encoders as regularization. However, the above UAE-based hybrid baselines are looselycoupled, where user collaborative information cannot in turn guide the item content module to learn more recommendation-oriented content embeddings. Tightly coupling an item content VAE with UAE by introducing mutual regularization between UAE weights and item content embeddings, MD-CVAE out-performs the baselines on all three datasets. Specifically, we observe that MDsym-CVAE performs slightly worse than MD-CVAE on denser datasets (i.e., citeulike-a and movielen-sub). The reason could be that the symmetric structure in MDsym-CVAE trades some modeling capacity for the ability to recommend cold-start items. However, the better performance of MDsym-CVAE on toys &amp; games dataset demonstrates that limiting the model capacity is not necessarily a disadvantage, as the extra structure regularization is conducive to preventing the model from overfitting when ratings are sparse. 4.4.2 Out-of-matrix Case. Although CTR, CDL, CVAE are limited in their collaborative modeling ability, they are strong in out-ofmatrix prediction where the UAE-based methods fail, because for CTR, CDL, CVAE, the inferred content embeddings of new items can be used in place of the latent item variables to calculate their innerproduct with user latent variables for recommendations <ref type="bibr" target="#b25">[26]</ref>. In this section, we compare the out-of-matrix performance of MDsym-CVAE with the three strong tightly-coupled baselines. In the experiment, we first fix the number of cold-start items, i.e., 1,600 for citeulike-a (10%), 160 for movielen-sub (2%), 1,200 (10%) for toys &amp; games, and evaluate the models separately on normal and cold-start We then choose the optimal ğœ† ğ‘£ and compare the results of MDsym-CVAE with the three strong baselines. Finally, we explore the influence of the number of cold-start items on recommendation performance for all tightly-coupled models. Fig. <ref type="figure" target="#fig_4">6</ref> shows the performance of MDsym-CVAE on normal and cold-start items with varied ğœ† ğ‘£ . In Fig. <ref type="figure" target="#fig_4">6</ref> we can find that generally, the recommendation performance for both normal and cold-start items improves first and then deteriorates with the increase of ğœ† ğ‘£ . Specifically, when ğœ† ğ‘£ exceeds the in-matrix optimal value, which is denoted by red stars in Fig. <ref type="figure" target="#fig_4">6</ref>, both metrics drop monotonically for normal items, while they increase and then decrease for coldstart items. It is reasonable, since a larger ğœ† ğ‘£ imposes a stronger constraint on UAE weights, which is beneficial for recommending cold-start items as their content embeddings are expected to resemble more to their corresponding weights if they would have appeared in the training set, while it may hurt predictions for normal items as it limits the model capacity when rating information is sufficient. We choose the optimal ğœ† ğ‘£ for MDsym-CVAE where the performance on normal items drops no more than 80% to keep the balance between predicting normal and cold-start items, which leads to ğœ† ğ‘£ = 50, 50, 10 for the three datasets, respectively. The same criterion is used to select the optimal ğœ† ğ‘£ for the three PMF-based tightly-coupled hybrid baselines. The performance comparisons are summarized in Table <ref type="table" target="#tab_2">3</ref>. From Table <ref type="table" target="#tab_2">3</ref> we can find that MDsym-CVAE performs consistently better on cold-start items compared to the strongest baseline CVAE, with significantly better performance on normal items. This further demonstrates the superior cold-start item recommendation ability of the proposed MDsym-CVAE.  Finally, we vary the number of cold-start items and evaluate the baselines and MDsym-CVAE. The results are illustrated in Fig. <ref type="figure" target="#fig_6">7</ref>. From Fig. <ref type="figure" target="#fig_6">7</ref> we can find that the recommendation performance on cold-start items drops as their number increases as expected. However, the decrease is slower for citeulike-a dataset. The reason could be that item features in citeulike-a dataset are extracted from the article title and abstract, which are directly related to the item content, whereas for movielen-sub and toys &amp; games datasets, the item features are extracted from noisier crawled plots and reviews. Therefore, better cold-start recommendation performance of MDsym-CVAE can be achieved if the extracted item content features are less noisy and more recommendation-oriented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we have proposed MD-CVAE to address two problems of vanilla UAEs in recommendations: sparsity and cold-start items. MD-CVAE seamlessly integrates latent item embeddings with UAE, allowing the incorporation of item content information to support recommendations in user-oriented models. Moreover, MD-CVAE can be easily generalized to multi-class classification tasks where new classes constantly appear after model training and deployment, such as automatic image censoring, etc., with a little adaptation. Therefore, we speculate that MD-CVAE could have a broader impact on tasks other than recommendations discussed in this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of UAE-based recommenders and two key problems. Q1. How to utilize item content information where no item latent variable is explicitly modeled? Q2. How to enable UAE to recommend cold-start items like i6?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The zoomed-in view of the first layer of UAE. Like the last layer of UAE decoder, the first layer of UAE encoder can also be viewed as latent item embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Two cases of out-of-matrix prediction: (left) coldstart items already exist when the model is trained offline; (right) new items arrive in an online manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The truncated item rating density distribution for citeulike-a, movielen-sub, and Amazon toys &amp; games datasets. The red curves illustrate the estimated probability density functions and the light-blue dashed lines shows the percentiles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Model performance with varied ğœ† ğ‘£ on normal and cold-start items. â˜… denotes the optimal ğœ† ğ‘£ in in-matrix case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The influence of the number of cold-start items on the recommendation performance for these items.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets used in the paper stacked item embeddings of existing ğ½ items and are constrained to be close to corresponding item content embeddings, we can use item content embeddings inferred by the dual item content VAE as surrogates to the missing weights for ğ½ â€² new items and conduct recommendations as if they were normal items.</figDesc><table><row><cell>dataset</cell><cell cols="3">#users #items %density #features</cell></row><row><cell>citeulike-a</cell><cell>5,551 16,980</cell><cell>0.217%</cell><cell>8,000</cell></row><row><cell cols="2">movielen-sub 10,881 7,701</cell><cell>0.922%</cell><cell>8,000</cell></row><row><cell cols="2">toys &amp; games 14,706 11,722</cell><cell>0.072%</cell><cell>8,000</cell></row><row><cell>are viewed as</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparisons between MD-CVAE and various baselines on citeulike-a, movielen-sub, and amazon datasets. The best method is highlighted in bold, whereas the best method in each part is marked with underline.</figDesc><table><row><cell></cell><cell></cell><cell>citeulike-a</cell><cell></cell><cell></cell><cell>movielen-sub</cell><cell></cell><cell></cell><cell>toys &amp; games</cell><cell></cell></row><row><cell></cell><cell cols="9">Recall@20 Recall@40 NDCG@100 Recall@20 Recall@40 NDCG@100 Recall@20 Recall@40 NDCG@100</cell></row><row><cell>MD-CVAE</cell><cell>0.303</cell><cell>0.377</cell><cell>0.301</cell><cell>0.353</cell><cell>0.452</cell><cell>0.381</cell><cell>0.141</cell><cell>0.188</cell><cell>0.102</cell></row><row><cell>MDsym-CVAE</cell><cell>0.295</cell><cell>0.374</cell><cell>0.297</cell><cell>0.347</cell><cell>0.449</cell><cell>0.377</cell><cell>0.147</cell><cell>0.191</cell><cell>0.106</cell></row><row><cell>FM</cell><cell>0.231</cell><cell>0.312</cell><cell>0.238</cell><cell>0.324</cell><cell>0.421</cell><cell>0.357</cell><cell>0.088</cell><cell>0.121</cell><cell>0.062</cell></row><row><cell>CTR</cell><cell>0.169</cell><cell>0.250</cell><cell>0.190</cell><cell>0.285</cell><cell>0.398</cell><cell>0.312</cell><cell>0.124</cell><cell>0.179</cell><cell>0.089</cell></row><row><cell>CDL</cell><cell>0.209</cell><cell>0.295</cell><cell>0.226</cell><cell>0.311</cell><cell>0.405</cell><cell>0.339</cell><cell>0.133</cell><cell>0.181</cell><cell>0.092</cell></row><row><cell>CVAE</cell><cell>0.236</cell><cell>0.334</cell><cell>0.247</cell><cell>0.304</cell><cell>0.422</cell><cell>0.355</cell><cell>0.139</cell><cell>0.188</cell><cell>0.094</cell></row><row><cell>Multi-VAE</cell><cell>0.269</cell><cell>0.346</cell><cell>0.274</cell><cell>0.326</cell><cell>0.423</cell><cell>0.357</cell><cell>0.114</cell><cell>0.157</cell><cell>0.082</cell></row><row><cell>CoVAE</cell><cell>0.247</cell><cell>0.338</cell><cell>0.260</cell><cell>0.338</cell><cell>0.436</cell><cell>0.367</cell><cell>0.120</cell><cell>0.174</cell><cell>0.085</cell></row><row><cell>CondVAE</cell><cell>0.274</cell><cell>0.359</cell><cell>0.275</cell><cell>0.341</cell><cell>0.437</cell><cell>0.365</cell><cell>0.132</cell><cell>0.180</cell><cell>0.094</cell></row><row><cell>DICER</cell><cell>0.279</cell><cell>0.363</cell><cell>0.277</cell><cell>0.329</cell><cell>0.428</cell><cell>0.359</cell><cell>0.127</cell><cell>0.172</cell><cell>0.092</cell></row><row><cell>DAVE</cell><cell>0.281</cell><cell>0.362</cell><cell>0.283</cell><cell>0.340</cell><cell>0.432</cell><cell>0.371</cell><cell>0.125</cell><cell>0.177</cell><cell>0.086</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparisons between MDsym-CVAE and tightly coupled baselines (NI: Normal items; CI: Cold-start items).</figDesc><table><row><cell></cell><cell>(a) citeulike-a</cell><cell></cell></row><row><cell cols="3">Recall@20 (NI / CI) NDCG@100 (NI / CI)</cell></row><row><cell>MDsym-CVAE</cell><cell>0.290 / 0.251</cell><cell>0.286 / 0.249</cell></row><row><cell>CTR</cell><cell>0.169 / 0.209</cell><cell>0.189 / 0.207</cell></row><row><cell>CDL</cell><cell>0.206 / 0.218</cell><cell>0.203 / 0.214</cell></row><row><cell>CVAE</cell><cell>0.238 / 0.235</cell><cell>0.236 / 0.232</cell></row><row><cell></cell><cell>(b) movielen-sub</cell><cell></cell></row><row><cell cols="3">Recall@20 (NI / CI) NDCG@100 (NI / CI)</cell></row><row><cell>MDsym-CVAE</cell><cell>0.351 / 0.309</cell><cell>0.369 / 0.275</cell></row><row><cell>CTR</cell><cell>0.284 / 0.164</cell><cell>0.310 / 0.195</cell></row><row><cell>CDL</cell><cell>0.301 / 0.196</cell><cell>0.338 / 0.227</cell></row><row><cell>CVAE</cell><cell>0.318 / 0.279</cell><cell>0.342 / 0.240</cell></row><row><cell></cell><cell>(c) toys &amp; games</cell><cell></cell></row><row><cell cols="3">Recall@20 (NI / CI) NDCG@100 (NI / CI)</cell></row><row><cell>MDsym-CVAE</cell><cell>0.113 / 0.099</cell><cell>0.084 / 0.082</cell></row><row><cell>CTR</cell><cell>0.089 / 0.084</cell><cell>0.072 / 0.069</cell></row><row><cell>CDL</cell><cell>0.096 / 0.090</cell><cell>0.077 / 0.079</cell></row><row><cell>CVAE</cell><cell>0.110 / 0.094</cell><cell>0.081 / 0.080</cell></row><row><cell>items w.r.t varied ğœ† ğ‘£ .</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Subscript ğ‘– and ğ‘— would be omitted when no ambiguity exists. Capital non-boldface symbols such as ğ‘…, ğ‘‹ are used to denote the corresponding random vectors of r, x, where exceptions such as ğ¼ , ğ½ are easily identified from contexts. ğ‘‹ ğ‘  , ğ‘‰ ğ‘  are used to denote the random matrices for stacked x, v. Capital boldface symbols such as R, X, V are used to denote matrices.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>This research was supported in part by <rs type="funder">Tencent</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName><forename type="first">Alp</forename><surname>David M Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>David M Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">the Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hybrid recommender systems: Survey and experiments</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User Modeling and User-adapted Interaction</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="331" to="370" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A collective variational autoencoder for top-N recommendation with side information</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 3rd Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Are we really making much progress? A worrying analysis of recent neural recommendation approaches</title>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Ferrari Dacrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="101" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A hybrid collaborative filtering model with deep structure for recommender systems</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhonghuo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxia</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangxi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DeepFM: A factorization-machine based neural network for CTR prediction</title>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1725" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The movielens datasets: History and context</title>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Interactive Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web</title>
		<meeting>the 25th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth IEEE International Conference on Data Mining</title>
		<meeting>the Eighth IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Advances in collaborative filtering</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender Systems Handbook</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="77" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collaborative variational autoencoder for recommender systems</title>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>She</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="305" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Variational autoencoders for collaborative filtering</title>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
		<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep critiquing for VAEbased recommender systems</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hojin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ga</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1269" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning disentangled representations for recommendation</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Neural Information Processing Systems</title>
		<meeting>the 33rd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5711" to="5722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A novel top-N recommendation approach based on conditional variational auto-encoder</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongjun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Pacific-Asia Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="357" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">AutoRec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName><forename type="first">Suvash</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">RecVAE: A new variational autoencoder for top-N recommendations with implicit feedback</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Shenbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Alekseev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Tutubalina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Malykh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><forename type="middle">I</forename><surname>Nikolenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="528" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hybrid recommender system based on autoencoders</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romaric</forename><surname>Gaudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ©rÃ©mie</forename><surname>Mary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">LÃ©on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Collaborative topic modeling for recommending scientific articles</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Collaborative deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A hybrid conditional variational autoencoder model for personalised top-N recommendation</title>
		<author>
			<persName><forename type="first">Yaxiong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval</title>
		<meeting>the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A multimodal variational encoder-decoder framework for micro-video popularity prediction</title>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaosi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2542" to="2548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-modal variational auto-encoder for content-based micro-video background music recommendation</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dual adversarial variational embedding for robust recommendation</title>
		<author>
			<persName><forename type="first">Qiaomin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Jing Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Content-collaborative disentanglement representation learning for enhanced recommendation</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Caverlee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM Conference on Recommender Systems</title>
		<meeting>the 14th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02088</idno>
		<title level="m">Deep causal reasoning for recommendations</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
