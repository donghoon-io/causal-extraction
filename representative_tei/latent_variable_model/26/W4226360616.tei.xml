<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Go with the Flows: Mixtures of Normalizing Flows for Point Cloud Generation and Reconstruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-11-29">29 Nov 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Janis</forename><surname>Postels</surname></persName>
							<email>jpostels@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ETH Zurich</orgName>
								<orgName type="institution" key="instit2">University of Bologna</orgName>
								<orgName type="institution" key="instit3">Luc Van Gool ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eth</forename><surname>Zurich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ETH Zurich</orgName>
								<orgName type="institution" key="instit2">University of Bologna</orgName>
								<orgName type="institution" key="instit3">Luc Van Gool ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mengya</forename><surname>Liu</surname></persName>
							<email>mengya.liu@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ETH Zurich</orgName>
								<orgName type="institution" key="instit2">University of Bologna</orgName>
								<orgName type="institution" key="instit3">Luc Van Gool ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Riccardo</forename><surname>Spezialetti</surname></persName>
							<email>riccardo.spezialetti@unibo.it</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ETH Zurich</orgName>
								<orgName type="institution" key="instit2">University of Bologna</orgName>
								<orgName type="institution" key="instit3">Luc Van Gool ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Federico</forename><forename type="middle">Tombari</forename><surname>Google</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ETH Zurich</orgName>
								<orgName type="institution" key="instit2">University of Bologna</orgName>
								<orgName type="institution" key="instit3">Luc Van Gool ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">T</forename><forename type="middle">U</forename><surname>Munich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ETH Zurich</orgName>
								<orgName type="institution" key="instit2">University of Bologna</orgName>
								<orgName type="institution" key="instit3">Luc Van Gool ETH Zurich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Go with the Flows: Mixtures of Normalizing Flows for Point Cloud Generation and Reconstruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-11-29">29 Nov 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.03135v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T18:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth</head><p>Single NF [32] NF Mixture (ours) Ground truth Single NF [32] NF Mixture (ours) Figure 1. Our method (Ours) uses mixtures of Normalizing Flows (NFs) to generate point clouds overcoming limitations of single normalizing flows [53, 32]. Each mixture component (indicated by coloring) specializes in a distinct subregion in an unsupervised fashion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Nowadays point clouds, as the output of many modern 3D scanning devices, e.g. LiDARs and RGB-D cameras, * equal contribution denote an increasingly popular data format for 3D shapes. Thus, a generative model that can sample shapes represented as point clouds is valuable for several 3D computer vision down-stream tasks such as shape completion, synthesis and up-sampling. Although Variational Autoencoder (VAE) <ref type="bibr" target="#b30">[31]</ref>, Generative Adversarial Network (GAN) <ref type="bibr" target="#b17">[18]</ref> and Normalizing Flows <ref type="bibr" target="#b12">[13]</ref> (NFs) have shown impressive results on various applications <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b2">3]</ref>, point clouds remain challenging for these methods due to their lack of a regular underlying grid structure compared to images.</p><p>Prior work handled the irregular structure of point clouds by generating shapes with a fixed number of points using GANs or auto-regressive models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b14">15]</ref>. Recently, another family of generative models, NFs, has gained attention due to its appealing properties <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b31">32]</ref>. While they naturally allow trading off runtime with resolution by adapting the number of generated points at inference time, their invertibility also allows training them by directly minimizing the negative log-likelihood of the data leading to improved training stability over GANs <ref type="bibr" target="#b0">[1]</ref>.</p><p>While the application of continuous <ref type="bibr" target="#b52">[53]</ref> and discrete <ref type="bibr" target="#b31">[32]</ref> NFs yielded state-of-the-art performance on shape generation and reconstruction benchmarks, it also has innate limitations. Transforming a standard Gaussian using an invertible map into a complex geometry accurately (e.g. with holes or multiple modes) requires squeezing/expanding space infinitely strong <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11]</ref>. Therefore, one must resort to very deep NFs <ref type="bibr" target="#b10">[11]</ref>. Our work generalizes NFs by introducing additional discrete latent variables. Thus, shapes are composed as a product of experts using multiple NFs. Each NF specializes in a subregion in an unsupervised fashion -see fig. <ref type="figure">1</ref> where the point color indicates the NF. Importantly, this alleviates the problem of mismatching abstract geometric properties of the source distribution and the target distribution by learning to sew together the final object using several invertible maps. Besides performance gains, this further yields interesting clustering that potentially enables broad applications such as unsupervised part segmentation, semantic correspondence, etc.</p><p>We demonstrate that mixtures of NFs introduced in section 3 generalize and exceed single-flow-based models at similar size, while reducing the inference runtime. This increased representational strength manifests itself in superior generation and reconstruction and in improved details on the generated/reconstructed point clouds resulting from individual NFs specializing in subregions of the 3D shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Mixtures of Normalizing Flows</head><p>NFs <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> are a class of generative models that allow efficient likelihood evaluation using invertible transformations. Recently, they have fueled a variety of applications <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b51">52]</ref>. Despite their popularity, there have been surprisingly few works on mixtures of NFs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref>, all focused on toy data problems. <ref type="bibr" target="#b13">[14]</ref> separates the space into disjoint subsets using piece-wise linear activation functions to let each flow specialize on one subset. However, the discontinuity arising from partitioning leads to training difficulties <ref type="bibr" target="#b10">[11]</ref>. In turn, <ref type="bibr" target="#b10">[11]</ref> relaxes the invertibility constraint of NFs by introducing additional continuous latent variables yielding improved performance on MNIST and CIFAR10. <ref type="bibr" target="#b16">[17]</ref> trains a mixture of NFs using bosting, where each flow learns the residual likelihood. Our work refrains from applying <ref type="bibr" target="#b10">[11]</ref> or <ref type="bibr" target="#b16">[17]</ref> since the continuous nature of the latent variables <ref type="bibr" target="#b10">[11]</ref> and the iterative training procedure <ref type="bibr" target="#b16">[17]</ref> do not allow obtaining well separated clusters. The mixture of NFs trained in <ref type="bibr" target="#b38">[39]</ref> is closest to ours. However, they operate on toy data and the latent variables of their VAE only encompass the mixture weights whereas our VAE's continuous latent variables encode 3D shapes on which we condition the mixture of NFs. Lastly, <ref type="bibr" target="#b25">[26]</ref> uses a Gaussian mixture model as the base distribution for a NF and applies this scheme to semi-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Generative Models for Point Clouds</head><p>Due to the unorganized structure of point clouds, pioneering generative models treat point clouds as a set of 3D points organized into a N × 3 matrix, where N is fixed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48]</ref>. For example, Gadelha et al. <ref type="bibr" target="#b15">[16]</ref> combine a multi-resolution encoder-decoder to form a VAE <ref type="bibr" target="#b30">[31]</ref> for point cloud generation. Achlioptas et al. <ref type="bibr" target="#b0">[1]</ref> explore the use of GANs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref> to generate point clouds. However, generating a point cloud with a fixed number of points limits its flexibility. This issue has been partially mitigated with the introduction of plane-folding decoders <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b53">54]</ref>, which learn to deform 2D points sampled from a grid into a set of 3D points allowing to generate shapes with an arbitrary number of points. However, the above methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b44">45]</ref> rely on heuristic set distances such as the Chamfer distance (CD) and the Earth Mover's distance (EMD), which both lead to several drawbacks <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b5">6]</ref>. While the CD favors point clouds that are concentrated in the mode of the marginal point distribution, the EMD is often computed by approximations and thus can lead to biased gradients.</p><p>Alternatively, a point cloud can be viewed as a point/3D distribution. PointGrow <ref type="bibr" target="#b44">[45]</ref> models this distribution autoregressively. ShapeGF <ref type="bibr" target="#b5">[6]</ref> applies an energy-based framework to model a shape by learning the gradient field of its log-density. However, they do not learn the lowdimensional shape embeddings leading to poor performance compared to related work. PointFlow <ref type="bibr" target="#b52">[53]</ref> employs two continuous NFs <ref type="bibr" target="#b18">[19]</ref> to model both shape distribution and point distributions. <ref type="bibr" target="#b43">[44]</ref> generates the weights of the continuous NF using a hypernetwork paired with a spherical log-normal base distribution achieving similar results as PointFlow <ref type="bibr" target="#b52">[53]</ref>. But these works are computationally expensive due to differential equations <ref type="bibr" target="#b7">[8]</ref>. Discrete Point Flow (DPF) <ref type="bibr" target="#b31">[32]</ref> uses discrete affine coupling layers resulting in a significant speed-up of the method. Moreover, other works develop conditional NFs to improve the representation performance. Pumarola et al. <ref type="bibr" target="#b39">[40]</ref> proposes a novel conditioning scheme for NFs to address 3D reconstruction and rendering from point clouds. SoftFlow <ref type="bibr" target="#b26">[27]</ref> conditions a single NF on the noise magnitude used during training. More recently, <ref type="bibr" target="#b34">[35]</ref> uses a diffusion probabilistic process to model 3D point clouds.</p><p>Concurrently, <ref type="bibr" target="#b27">[28]</ref> uses multiple continuous NFs for point clouds inheriting long training times from PointFlow <ref type="bibr" target="#b52">[53]</ref>. Note that they do not train a mixture of NFs, but rather an unrelaxed version using hard assignments. Finally, <ref type="bibr" target="#b36">[37]</ref> also proposes multiple invertible maps for point clouds. However, they focus on reconstruction -without learning a generative model -and rely on a set of handcrafted optimization objectives instead of maximizing log-likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Mixtures of Normalizing Flows for Point Clouds</head><p>This section, initially, revisit VAEs <ref type="bibr" target="#b30">[31]</ref>, which are used to approximate the distribution of point clouds, and NFs. Then, we introduce our main contribution -mixtures of NFs -for modeling and learning from point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background</head><p>Normalizing Flows <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> are explicit generative models that transform a simple base distribution p(y) (e.g. Gaussian N (0, 1)) into a complex data distribution p(x) (e.g. point cloud) using a series of n invertible transformations f = f n-1 • ... • f 0 with f : y → x. Hereby, the f i are designed to allow efficient evaluation of the log-determinant of their Jacobian. Thus, using a change of variable, we can train NFs by directly minimizing the negative log-likelihood of the data -log(p f (x)) under the model as follows</p><formula xml:id="formula_0">L(θ) = E x∼p(x) [-log(p f (x))]</formula><p>(1)</p><formula xml:id="formula_1">= -E x∼p(x) log(p(y)) - n-1 i=0 log det |J f -1 i (x)|</formula><p>where θ denotes the parameters of the invertible maps and y = f -1 (x). A common choice for the invertible transformation is the coupling layer <ref type="bibr" target="#b12">[13]</ref>. Given an input z ∈ R d a coupling layer c splits the dimensions of y into two sets L ⊂ {1, . . . , d} and K ⊂ {1, . . . , d} with L ∪ K = ∅. It then applies the identity mapping to one set c(y L ) = y L . The other set of features is scaled by s = s(y L ) and translated by t = t(y L ) such that c(y</p><formula xml:id="formula_2">K ) = s(y L ) y K + t(y L )</formula><p>where s(y L ) and t(y L ) are typically (non-invertible) multilayer perceptrons (MLPs). The log-determinant of the Jacobian of this transformation is equivalent to summing the scaling factors log (det |J c (y)|) = k∈K s k (y L ). Further, conditioning a NF on a variable z is commonly achieved <ref type="bibr" target="#b1">[2]</ref> by introducing conditional scaling and translation in each coupling layer -t(y; z) and s(y; z) -and/or parameterizing the mean and the diagonal covariance matrix of the base distribution as functions of z, i.e. p(y|z) = N (y; µ(z), Σ(z)).</p><p>Here, we follow <ref type="bibr" target="#b31">[32]</ref> and apply both mechanisms.</p><p>Variational Autoencoders <ref type="bibr" target="#b30">[31]</ref> are latent variable models that approximate a data distribution p(x) by minimizing the negative Evidence Lower BOund (ELBO)</p><formula xml:id="formula_3">-ELBO(θ, ψ, φ) = E z∼p(z|x) [-log(p θ (x|z))] + D KL (p φ (z|x)||p ψ (z)) = L D + L P rior<label>(2)</label></formula><p>Here, D KL is the Kullback-Leibler divergence, p φ (z|x) denotes an encoder parameterizing the approximate posterior distribution, p ψ (z) is a prior distribution of z and p θ (x|z) is a decoder model parameterizing the distribution of x conditioned on z. While the prior distribution is often fixed (e.g. standard Gaussian N (0, 1)), this work chooses the more flexible approach of learning its parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Method</head><p>We model a distribution p(X) of 3D point clouds X ∈ R N ×3 where each point cloud itself represents a distribution p(x) over points x ∈ R 3 in 3D space. We model p(X) using a VAE wherein each point distribution p(x) is represented by a mixture of NFs. Subsequently, in this context z ∈ R d refers to a d-dimensional latent representation of an entire point cloud. The overall architecture is depicted in fig. <ref type="figure" target="#fig_0">2</ref>. Mixtures of NFs for point distributions. Prior work estimated point distributions using either a single continuous <ref type="bibr" target="#b53">[54]</ref> or discrete <ref type="bibr" target="#b31">[32]</ref> NF conditioned on the latent shape representation z. Despite strong performance, using a single (conditional) NF to transform a standard Gaussian N (0, 1) into complicated geometries has fundamental limitations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11]</ref>, since it requires infinite bi-Lipschitz constants in the limit of arbitrary precision <ref type="bibr" target="#b10">[11]</ref>. Further explanations and a toy example are in the supplement. To achieve such bi-Lipschitz constants, it is necessary to use large/deep NFs which limit their practical relevance. Fig. <ref type="figure">9</ref> provides further intuition regarding the advantages of mixtures of NFs in form of a 2D toy example. To bypass this shortcoming, one could either use a prior distribution with a similar geometry as the target distribution or a mixture of NFs. The former requires dynamically adapting the prior distribution to the geometry of the target distribution, leading to a chicken-and-egg problem. Therefore, we choose to model point clouds as a mixture of NFs, i.e. using several independent invertible maps. Thus, complicated geometries are composed of separate simpler geometries. Formally, we model the conditional point distribution p θ (x|z) in eq. ( <ref type="formula" target="#formula_3">2</ref>) using a mixture of m conditional NFs</p><formula xml:id="formula_4">p(x|z) = m i=0 w i (z)N (f -1 θi (x); µ(z), Σ(z)) det |J f -1 θ i (x; z)|<label>(3</label></formula><p>) where f θi denotes the i-th NF with its parameters θ i , N (f -1 θi (x); µ(z), Σ(z)) is the likelihood under the shared prior distribution and w i (z) ≥ 0 are the mixture weights denoting the probability that a point of a point cloud is generated by i-th NF. They are a function of the latent variable z and normalized to m i=0 w i (z) = 1 ∀z. It is important to condition the mixture weights on the latent shape representation z, as using identical mixture weights for modelling all shapes is too restrictive. Consequently, the first part of eq. ( <ref type="formula" target="#formula_3">2</ref>) becomes</p><formula xml:id="formula_5">L D = E z∼p φ -log m i=0 w i (z)N (f -1 θi (x); µ(z), Σ(z)) * det |J f -1 θ i (x; z)|<label>(4)</label></formula><p>This objective leads to specialization of individual NFs. This is proved in the supplement.</p><p>Latent shape representation z. We model the conditional distribution p φ (z|X) given a point cloud X as a normal distribution N (z; µ(X), Σ(X)). The mean µ(X) and the diagonal covariance matrix Σ(X) are parameterized by a per-  mutation invariant version of PointNet <ref type="bibr" target="#b40">[41]</ref>.</p><p>Learned prior distribution p ψ (z). Ideally, the prior over latent representations of point clouds matches the marginal distribution of latent representations of real point clouds after the training, as this enables generating realistic point clouds at inference time. Therefore, we use a learned prior distribution <ref type="bibr" target="#b8">[9]</ref> parameterized by a discrete NF g ψ based on coupling layers with parameters ψ. During generative modelling of point clouds we use an unconditional prior distribution. However, on single-view reconstruction (SVR) we condition the prior distribution on a latent representation of the image/view which is produced by a ResNet18 <ref type="bibr" target="#b21">[22]</ref>. This is achieved using a conditional NF (section 3.1). Combining this with our parameterization of the approximate posterior distribution yields the following prior loss:</p><formula xml:id="formula_6">L P rior = -H (p φ ) - E z∼p φ (z|X) [log (p ψ (z))] = - d 2 log (2π) - 1 2 d i=1 log (Σ(X))<label>(5)</label></formula><p>-</p><formula xml:id="formula_7">E z∼p φ log(N (g -1 ψ (z))) + log(det |J g -1 ψ (z)|)</formula><p>Optimizing mixtures of NFs. Point clouds typically only cover the surface of a 3D shape, thus are two-dimensional. We follow the common practice of adding Gaussian noise (µ = 0, σ = 0.02) to point clouds during training <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27]</ref>. This stabilizes the training as transforming a 3D Gaussian into a 2D distribution yields a pathological training objective <ref type="bibr" target="#b4">[5]</ref>. Moreover, when training a mixture of NFs it is necessary to initially enforce a uniform prior on the mixture weights w i (z) which encourages each NF to spread its probability mass over the entire shape. Otherwise, the mixture of NFs can get stuck in suboptimal solutions where the regions that a NF is responsible for are highly disjoint or the model learns to only make use of one NF. We implement this as a hard prior where we fix w i (z) = w 0 during a warm-up period. Empirically, we found 5 epochs to be sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the performance of our model on three tasks: point cloud generation (section 4.1), autoencoding (section 4.2), and SVR (section 4.3). Further, we show that mixtures of NFs learn to specialize in a semantically meaningful and consistent manner (section 4.4) and yield increasing benefit when decreasing the model size (section 4.5). Dataset. We conduct all the experiments using the normalized version of ShapeNet dataset <ref type="bibr" target="#b6">[7]</ref> provided by <ref type="bibr" target="#b31">[32]</ref>. For more details, please refer to the supplementary material. Evaluation Metrics. Following the evaluation proposed by <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b14">15]</ref>, we measure the quality of reconstructed shapes, in section 4.2 and section 4.3, in terms of Chamfer Distance (CD) and Earth Mover's Distance (EMD). However, these metrics were demonstrated to have severe limitations due to their sensitivity to outliers <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b0">1]</ref>. For this reason, we include the more robust F1-score <ref type="bibr" target="#b32">[33]</ref> (F1) that measures the percentage of points that are correctly reconstructed <ref type="bibr" target="#b46">[47]</ref>, i.e. the euclidean distance between each pre-dicted point and the ground truth under a certain threshold τ . Following prior work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b31">32]</ref>, we evaluate generative modelling performance in section 4.1 by comparing generated and reference point clouds with the following metrics:</p><p>i) The Jensen-Shannon divergence (JSD) measures the similarity between two marginal point distributions obtained by taking the union of all generated (or reference) point clouds, and discretizing them to a voxel grid.</p><p>ii) Coverage (COV) measures the fraction of test point clouds that are matched to at least one generated point cloud. Matching pairs are based on either CD, EMD or F1.</p><p>iii) The Minimum matching distance (MMD) is the average distance of test point clouds to their nearest neighbor in the generated set according to CD, EMD or F1. Note that, unlike CD/EMD, F1 is larger for more similar point clouds.</p><p>iv) 1-nearest neighbour accuracy (1-NNA) is the leaveone-out accuracy of the 1-NN classifier in identifying generated and reference point clouds. The nearest neighbor is computed using either CD, EMD or F1. For more details we refer the reader to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b52">53]</ref>. While JSD and COV measure the diversity of the generated samples, MMD and 1-NNA aim at quantifying their perceptual quality. Prior work only uses CD and EMD for computing MMD. However, we observe that matches of MMD based on CD and EMD are highly dissimilar (see section 4.1). Consequently, MMD is unlikely to reflect the quality of high frequency components of generated point clouds. Therefore, we also include F1 into our evaluation and, further, evaluate generative modeling using the FID score <ref type="bibr" target="#b23">[24]</ref> which quantifies both, perceptual quality and diversity. While more advanced measures that disentangle perceptual quality and diversity <ref type="bibr" target="#b35">[36]</ref> exist, here we rely on the more well-known FID score which has been shown to correlate well with human perceptual scores on images. We compute the FID score on the features extracted by a DGCNN <ref type="bibr" target="#b50">[51]</ref> pretrained on ShapeNet provided by <ref type="bibr" target="#b45">[46]</ref>.</p><p>Experimental setup. We use the same configuration of the encoder and prior as in DPF <ref type="bibr" target="#b31">[32]</ref>. As in <ref type="bibr" target="#b31">[32]</ref> we set the size D of the latent shape representation z to 128 for generation and 512 for both autoencoding and SVR. The decoder is a mixture of M NFs with each flow containing N conditional affine coupling layers. These coupling layers compute scaling and translation, as discussed in section 3.1, using two fully connected layers with H hidden neurons, batch normalization and Swish activation functions. We modulate scaling and translation using an encoding of z according to FiLM <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b31">32]</ref>. To ensure comparability with DPF <ref type="bibr" target="#b31">[32]</ref> we enforce our method to have similar parameter count. Therefore, we reduce N and H in each of the M flows. In all experiments we follow <ref type="bibr" target="#b31">[32]</ref> and set N = 63 and H = 64 for the single-flow model. A detailed description of the selection of H and N for mixtures of NFs can be found in the supplementary materials. We further refer to the sup-plement for an ablation study on the number M of mixture components. This ablation study found that M = 4 performs well across all categories. Consequently, use M = 4 in our experiments. Baselines. On autoencoding and generation, we compare our method with existing models for point clouds including recent flow-based methods such as DPF <ref type="bibr" target="#b31">[32]</ref> and PointFlow <ref type="bibr" target="#b52">[53]</ref> as well as other popular works such as AtlasNet <ref type="bibr" target="#b19">[20]</ref> and latent-GAN <ref type="bibr" target="#b0">[1]</ref>. We retrained DPF <ref type="bibr" target="#b31">[32]</ref> using the official implementation published by the authors due to the lack of pre-trained models. Results of other works are either obtained from <ref type="bibr" target="#b31">[32]</ref> or using a pretrained model provided by the corresponding authors. On SVR, we compare our results against the most similar work in the literature, i.e. methods that reconstruct a shape from an image in form of a point cloud. This includes: AtlasNet <ref type="bibr" target="#b19">[20]</ref>, DCG <ref type="bibr" target="#b48">[49]</ref>, Pixel2Mesh <ref type="bibr" target="#b49">[50]</ref> and DPF <ref type="bibr" target="#b31">[32]</ref>. Oracle. Similar to DPF <ref type="bibr" target="#b31">[32]</ref>, we provide an "oracle" to quantify an upper bound on the performance of our model. In the evaluation of generative modeling the oracle compares a set point clouds obtained from the test set with one obtained from the training set. During the evaluation of autoencoding and SVR, the oracle provides a point cloud obtained by sampling from the ground truth. Optimization. Details can be found in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Generative Modeling</head><p>Experimental setup. We evaluate how well mixtures of NFs fit the distribution of point clouds. In line with prior work, we train a mixture of NFs on the three categories car, chair and airplane. We compute the evaluation metrics mentioned in section 4 between the test set and a set of generated point clouds of equal size. Each point cloud comprises 2048 points. We repeat the evaluation 10 times and report the average results. Results including standard deviation can be found in the supplementary. Results. Table <ref type="table" target="#tab_0">1</ref> shows the results of this experiment. Mixtures of NFs obtain the best results regarding JSD, COV-CD/EMD and 1-NNA-EMD. Latent-GAN-CD/EMD <ref type="bibr" target="#b0">[1]</ref> shows strong performance on MMD-CD/EMD while performing poorly on EMD/CD as expected since it is optimized using CD/EMD. Compared with other flow-based models, for which we also evaluate the FID score, mixtures of NFs yield better performance than DPF <ref type="bibr" target="#b31">[32]</ref> across most metrics and clearly outperform PointFlow <ref type="bibr" target="#b52">[53]</ref>. We also compare our approach with DPF based on the F1-score (τ = 10 -4 ) where we outperform DPF 7 out of 9 times. Fig. <ref type="figure" target="#fig_2">4</ref> shows qualitative examples. Most importantly, we observe that each component of our mixture of NFs specializes in a distinct subregion of the shape. Interestingly, this specialization generalizes across different shapes. Analysis of MMD. The role of the metrics MMD-CD/EMD is to quantify the perceptual quality of the generated point clouds. However, a qualitative examination reveals that matches between generated point clouds and test samples are dissimilar (see supplement). More, an ablation study in section 4.5 regarding the performance of models with fewer parameters reveals that the quality of the reconstructed point clouds clearly degrades with decreasing model size. However, in the supplement we further demonstrate that MMD-CD/EMD remain largely unchanged for smaller models despite an obvious degradation in the perceptual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Autoencoding</head><p>Experimental setup. We evaluate the autoencoding using mixtures of NFs (4 components) jointly on all categories of ShapeNet <ref type="bibr" target="#b6">[7]</ref>. We report CD, EMD and F1-score (τ = 10 -4 ) by following prior work in comparing test samples with 2048 points with their reconstructions of equal size. Results. In table 2 mixtures of NFs obtain the highest F1score and second best CD/EMD. Moreover, we outperform latent-GAN <ref type="bibr" target="#b0">[1]</ref> and PointFlow <ref type="bibr" target="#b52">[53]</ref> across all metrics. At-lasNet <ref type="bibr" target="#b19">[20]</ref> trained with CD as criteria performs the best on CD. As for EMD, we are slightly worse than DPF <ref type="bibr" target="#b31">[32]</ref>. We argue that this is expected since EMD favors evenly distributed point clouds <ref type="bibr" target="#b31">[32]</ref> which is simpler to achieve using a single NF. Conversely, we report a lower CD than DPF <ref type="bibr" target="#b31">[32]</ref>. CD prioritizes regions <ref type="bibr" target="#b31">[32]</ref>, this reflects the ability of our model to better capture the local geometry of the shape, as qualitatively shown in fig. <ref type="figure">1</ref>. By zooming into a specific part of the shape, we can see how our model precisely reconstructs fine-grained geometric details, conversely DPF <ref type="bibr" target="#b31">[32]</ref> tends to get a smoother and noisier shape, this confirming our expectations as explained in the methodology section. For example, looking at the tail of the airplane and the legs of the chair, we can see how our method is able Input Ground truth DPF <ref type="bibr" target="#b31">[32]</ref> Ours 4 Flows Figure <ref type="figure">5</ref>. Qualitative comparison of SVRs methods on the ShapeNet <ref type="bibr" target="#b6">[7]</ref> test set. On the left, we show the RGB view used as input. We also report the results for DPF <ref type="bibr" target="#b31">[32]</ref>.</p><p>to reconstruct them completely and clearly, while DPF <ref type="bibr" target="#b31">[32]</ref> fails at reconstructing high-frequency regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Single-view Reconstruction</head><p>Experimental setup. We evaluate the ability of mixtures of NFs to reconstruct point clouds from a single RGB image. At test time we sample from the prior conditioned on the encoding of RGB test images and subsequently sample from the decoder conditioned on this sample. We report CD, EMD and F1-score. Unlike in our experiments on autoencoding section 4.2, here we choose a threshold τ = 10 -3 to ensure comparability with prior work <ref type="bibr" target="#b31">[32]</ref>.</p><p>Results. Quantitative results can be found in table <ref type="table" target="#tab_2">3</ref>. Generally, we observe that mixtures of NFs yield at least second best performance across all metrics. AtlasNet <ref type="bibr" target="#b19">[20]</ref> outperforms our method for CD, which is expected since it explicitly optimizes this metric. Interestingly, mixtures of NFs demonstrate the best performance in F1-score, which is regarded as a more faithful metric for perceptual quality <ref type="bibr" target="#b32">[33]</ref>.</p><p>In fig. <ref type="figure">5</ref> we show qualitative examples of SVR using DPF <ref type="bibr" target="#b31">[32]</ref> and mixtures of NFs. We observe that mixtures of NFs yield sharper reconstructions with particular improvements on complicated geometries, e.g. the lamp. Also for SVR we observe that in mixtures of NFs each flow learns to be responsible for one part of the shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Rotation Invariant Latent Variables</head><p>This section qualitatively evaluates whether our mixtures of NFs can learn to specialize to semantically meaningful regions of the object, which is an interesting property that could aid applications such as point cloud registration. Experimental setup. We train a mixture of 4 NFs following the setup used for autoencoding on the airplane category of ShapeNet <ref type="bibr" target="#b6">[7]</ref>. We also augment the training data with random 3D rotations. We qualitatively examine whether the mixture assignments are rotation invariant. If mixtures of NFs are able to specialize in a semantically meaningful way, we expect approximate rotational invariance.  Results. In fig. <ref type="figure" target="#fig_3">6</ref> we illustrate qualitative results for this experiment. Once the model is trained, we randomly rotate the input shapes before passing them through the model.</p><formula xml:id="formula_8">JSD ↓ MMD COV ↑ 1-NNA ↓ FID ↓ Category Method CD ↓ EMD ↓ F1 ↑ CD EMD F1 CD EMD F1 l-GAN-</formula><p>We observe that different components of the mixture model learn to specialize in reconstructing different semantic parts of the airplanes (e.g. yellow → wings, green → center of the airplane). However, learning the distribution of randomly rotated 3D point clouds is a much harder task and this is reflected in slightly less detailed models reconstructed by our method, as can be seen in fig. <ref type="figure" target="#fig_3">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Decreasing Number of Parameters</head><p>Based on prior work on NFs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11]</ref> and our reasoning in section 3.2, mixtures of NFs yield increasing benefits in the regime of smaller decoder sizes. This experiment aims at verifying this intuition by comparing the reconstruction performance of DPF <ref type="bibr" target="#b31">[32]</ref> with mixtures of NFs for decoders with decreasing number of parameters. We are particularly interested in the reconstruction performance since it directly measures the representational strength of the underlying model. We refer to the supplement for generation metrics associated with this experiment. Experimental setup. We train DPF and mixtures of NFs using our generative modeling setup on the airplane category (see section 4) varying the size of the decoder. For DPF we use a decoder NF with 63 (original size), 24, 12 and 6 coupling layers and unchanged H = 64. We compare the reconstruction performance regarding the F1-score against a mixture of four NFs. H and N of the mixture are chosen such that it contains slightly fewer parameters. The detailed choice of H and N can be found in the supplement. Results. In fig. <ref type="figure" target="#fig_4">7</ref> (a) we observe that the relative improvement in terms of reconstruction performance increases for smaller decoder. While the mixture of four NFs achieves a relative improvement over DPF <ref type="bibr" target="#b31">[32]</ref> of 3.11% in the original size, decreasing the number of coupling layers to 6 more than doubles the relative improvement up to 7.65%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Inference Runtime Comparison</head><p>Finally, we plot the inference runtime of mixtures of NFs against the number of components at constant param- eter count. Specifically, we measure the average time per generated point during sampling. fig. <ref type="figure" target="#fig_4">7</ref> (b) shows the relative inference runtime improvement depending on the size of the mixture. Since sampling of each point requires only a smaller NF the average runtime decreases with an increasing number of mixture components. However, since the runtime of a NF mainly depends on the number of coupling layers, we do not observe a linear behaviour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed mixtures of NFs for modeling 3D point clouds which outperform models based on a single NF <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b31">32]</ref> on generation, autoencoding and SVR (see section 4.2 &amp; section 4.3). This resonates with the theoretical insight that single-flow-based models struggle on complicated geometries <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11]</ref>. We showed that mixtures of NFs can bypass these shortcomings by learning to compose a shape as a product of experts. While our consistent improvements are smaller in the overparameterized regime, we demonstrated that the relative gain over single-flowbased models increases for smaller models. This indicates that mixtures of NFs indeed denote a useful inductive bias for point clouds.</p><p>Furthermore, we observe that mixtures of NFs exhibit other interesting properties. The specialization of mixture components generalizes across different shapes (e.g. the same flow is always responsible for the wings in fig. <ref type="figure" target="#fig_3">6</ref>) and can be made rotational invariant by adding random rotations at training time. This implies that mixtures of NFs gain a deeper understanding of the underlying shape. However, currently these invariant clusters lead to worse quality of the reconstructed point clouds which denotes a promising future research direction. Moreover, interpolating subregions individually leads to unrealistic shapes (see supplement). In future research we plan to explore ways to allow realistic interpolation of subregions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material of "Go with the Flows: Mixtures of Normalizing Flows for Point Cloud Generation and Reconstruction"</head><p>Subsequently, we present further experimental results and details regarding mixtures of NFs for point clouds. Therefore, section 6 investigates the impact of varying the number of components in a mixture of NFs. In section 7 we give a detailed description of the training, optimization and architecture used in our experiments. Further, section 10 demonstrate additional results on generative modeling. In section 11 we show qualitative results of interpolating between latent representations of shapes, followed by the exploration of dense sampling with sparse input (see in section 12. section 9 presents an analytic toy example regarding the benefits of applying a mixture of NFs to point clouds as opposed to single-flow-based models. Lastly, we show more qualitative examples regarding generation, autoencoding and SVR in section 13, section 14 and section 15 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Ablation Study</head><p>We perform an ablation study regarding the number components n in a mixture of NFs. Therefore, we train mixtures of NFs using a varying number components n on the categories airplane (n ∈ [1, 2, 4, 6, 8, 10]), chair (n ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8]</ref>), car (n ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8]</ref>) and report reconstruction performance in terms of the CD, EMD and F1-score (see table <ref type="table" target="#tab_3">4</ref>). We observe that any number &gt; 1 leads to a clear improvement over a single-flow-based model. In our main experiments we choose n = 4 as it performs well across all categories. However, we also note that there appears to be no strong preference regarding the number of components. We interpret this as evidence that geometries on ShapeNet are not complex enough to benefit from a very large n. </p><formula xml:id="formula_9">Method #flows CD ↓ EMD ↓ F1 ↑, τ = 10</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Training Details</head><p>Architecture. We follow <ref type="bibr" target="#b31">[32]</ref> and implement the encoder in all our experiments as a PointNet <ref type="bibr" target="#b40">[41]</ref>. Our PointNet encoder consists of 5 layers with feature sizes of the layers set to 3, 64, 128, 256, 512. Subsequently, we perform max-pooling along the dimension of the points. The resulting representation is fed through a MLP comprised of two fully-connected layers. The first has a dimensionality of 512, the second has a dimensionality of L = 128 (generative modeling) or L = 512 (autoencoding and SVR).</p><p>All our NFs use coupling layers as their fundamental building blocks which translate and scale alternatingly odd/even dimensions where translation/scaling factors are computed as functions of even/odd dimensions. Scaling and translation factors are computed using two separate models applied to the masked input: linear layer (input dimension: d; output dimension: D), 1-d Batchnorm, Swish activation function and a final linear layer (input dimension: D; output dimension: d). In our conditional coupling layers the input is initially transformed by two separate models of the form: linear layer (input dimension: k; output dimension: K), 1d Batchnorm, Swish activation function and a final linear layer (input dimension: K; output dimension: K). We then apply FiLM conditioning <ref type="bibr" target="#b37">[38]</ref> where the condition is computed by a model of the form: linear layer (input dimension: L; output dimension: K), 1-d Batchnorm, Swish activation function and a final linear layer (input dimension: K; output dimension: K). Subsequently, scaling and translation are each fed through a ReLU activation function followed by a linear layer (input dimension: K; output dimension: k).</p><p>Our learned prior is implemented as a normalizing flow consisting of 14 coupling layers. In these coupling layers we set K = 128 and k = D (D: dimensionality of the latent space/bottleneck) since it needs to be of the same dimensionality as the latent space for reasons of invertibility (generative modeling: d = 128, autoencoding/SVR d = 512).</p><p>When training on SVR we implement the prior model as a conditional NF. The condition is computed using an image encoder which is implemented as a ResNet18 <ref type="bibr" target="#b22">[23]</ref>.</p><p>Setting K and number of coupling in mixtures of NFs. In order to ensure comparability between models based on a single NF and mixtures of NFs, we reduce the size of each NF in mixtures of NFs such it has slightly less parameters than a given reference model using a single NF. In particular, assume a single-flow-based model comprised of N coupling layers using a hidden dimensionality K. For a mixture of m NFs we compute the number of coupling layers N of each component in the mixture as N = N √ m . Subsequently, we determine the hidden dimensionality K of each component of the mixture by reducing K until the total number of parameters of the mixture model is smaller than the one of the single-flow-based model.</p><p>Optimization. We train all our models on Nvidia Titan RTX using ADAM <ref type="bibr" target="#b28">[29]</ref> for 1450 (generation), 1050 (autoencoding) or 36 (SVR) epochs using a batch size of 36. We start each training with a learning rate of 2.56•10 -4 and divide it by four at certain epochs (generation: 800, 1200, 1400; autoencoding: 400, 800, 1000; SVR: <ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35)</ref>.</p><p>Dataset. In order to provide a fair comparison with prior work, we conduct all the experiments using the ShapeNet dataset <ref type="bibr" target="#b6">[7]</ref> provided by <ref type="bibr" target="#b31">[32]</ref>. In our autoencoding experiments we use the ShapeNetCore.v2, which contains 55k point clouds subdivided into 55 classes. As for point cloud generation, we follow <ref type="bibr" target="#b52">[53]</ref> and focus on three categories of the ShapeNet <ref type="bibr" target="#b6">[7]</ref> dataset: airplanes, cars, and chairs. Finally, for single-view reconstruction we adopt the dataset from <ref type="bibr" target="#b9">[10]</ref>, which contains renders of shapes from the 13 classes of ShapeNetCore.v1. For each shape 24 images at a resolution of 137 × 137 are rendered from random viewpoints. The ground truth point clouds are obtained sampling from the original meshes. We randomly split per class in dataset into 70/10/20 proportion distributing to train/validation/test set for generation and autoencoding, for single-view reconstruction, we use the same train/test split from <ref type="bibr" target="#b9">[10]</ref>. All experiments regarding generation and autoencoding are conducted on the normalized dataset provided by <ref type="bibr" target="#b31">[32]</ref>. Similarly, for single-view reconstruction the models are trained on normalized data. However, we scale the data into a unit radius sphere during evaluation to ensure comparability with related work.</p><p>Visualization of F1-score We highlight the advantage of the F1-score as a convincing metric for perceptual quality by visualizing heatmaps of its components, precision and recall. fig. <ref type="figure">8</ref> provides two examples from which we can see that F1-score clearly focuses on high-frequency regions of the objects. We observe that for DPF the contributions to the F1-score primarily come from the high-frequency regions of an object, while in the case of mixtures of NFs the contributions are more evenly spread over the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Specialization of Individual NFs</head><p>This is a consequence of Jensen's inequality. For w i ≥ 0 with i w i = 1 and p i ≥ 0, we know that log( i w i p i ) ≥ i w i log(p i ) =⇒ -log( i w i p i ) ≤i w i log(p i ), equal for p i = p j ∀i, j. Thus a solution using unequal probabilities p i is preferred over an equal counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Toy Example on the Advantages of Mixtures of Normalizing Flows</head><p>This section presents an analytic toy example demonstrating the advantages of using mixtures of NFs. Therefore, consider the one-dimensional distributions in fig. <ref type="figure">9</ref>:</p><formula xml:id="formula_10">p Y (Y ) = 0.5 if x &lt; 1 and x ≥ -1 0 else (6) p X (X) =      1 if (x ≥ 1 and x &lt; 2)</formula><p>or (x ≥ -2 and x &lt; -1) 0 else <ref type="bibr" target="#b6">(7)</ref> We wish to find an invertible transformation f : Y → X such that the change of variable formula</p><formula xml:id="formula_11">p Y (Y ) = p X (f (Y )) df (Y ) dY<label>(8)</label></formula><p>is satisfied. In this simple example we can directly write down the solution, namely</p><formula xml:id="formula_12">f (Y ) = 2 • (Y -1) if x ≤ 1 2 • (Y + 1) if x &gt; 1 .<label>(9)</label></formula><p>Interestingly this function contains a discontinuity at 0. This discontinuity also implies an infinite bi-Lipschitz constant of the optimal solution as can be seen from the definition of the bi-Lipschitz constant K of a function g:</p><formula xml:id="formula_13">1 K |x 2 -x 1 | ≤ |g(x 2 ) -g(x 1 )| ≤ K |x 2 -x 1 | ∀x 1 , x 2<label>(10</label></formula><p>) In the vicinity of the origin K has to approach infinity in order to fulfill above inequality. Attempting to learn such a discontinuous function using a neural network, which is only a universal function approximator for continuous functions, is difficult. However, we can bypass the discontinuity in this solution by utilizing two invertible maps, f 1 = 4 • (Y -1.5) and f 2 = 4 • (Y + 1.5), and composing them as a mixture. Thus, we are introducing an additional continuous random variable w that identifies the invertible map responsible for a particular point y ∈ Y . This describes the underlying idea of applying mixtures of NFs to 3D data. By introducing additional latent variables we can empower our continuous model to avoid appoximating discontinuous behaviour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Further Results on Generative Modelling</head><p>Point Cloud Matches Using MMD-CD and MMd-EMD  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Results on Generative including Standard Deviation</head><p>We report quantitative metrics on generative modeling including their standard deviation in table <ref type="table" target="#tab_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Generative Modelling with Reduced Model Size</head><p>We report quantitative (see table <ref type="table" target="#tab_5">6</ref>) and qualitative (see fig. <ref type="figure" target="#fig_9">11</ref>) results on generative modeling using decoder models of reduced parameter counts for a single-flow-based model and a mixture of 4 NFs. We reduce the parameter count by decreasing the number of coupling layers used by the normalizing flow <ref type="bibr" target="#b23">(24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6)</ref>. Note that the original model used in our main experiments contains 63 coupling layers. We observe that for 24 &amp; 12 coupling layer the quantitative metrics on generative modeling remain largely unchanged while the quality of the generated samples (see fig. <ref type="figure" target="#fig_9">11</ref>) and reconstruction performance (see main paper) clearly degrade. We only see a clear quantitative degrada- tion in generative modeling performance when limiting the model to 6 coupling layers. We conclude that commonly used metrics for generative modeling struggle to represent perceptual quality of generated point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Interpolating Latent Representations</head><p>In fig. <ref type="figure" target="#fig_11">12</ref> and fig. <ref type="figure" target="#fig_1">13</ref>, we show qualitative examples of interpolating between latent representations learned by our models trained on airplane, car and chair. We sample two point clouds (the left-most and the right-most in the fig. <ref type="figure" target="#fig_11">12</ref>) and map them on their latent representations using our encoder. Subsequently, we linearly interpolate between these   latent representation and reconstruct the result using our decoder model which is based on a mixture of 4 NFs (see in fig. <ref type="figure" target="#fig_11">12</ref>). Interestingly, our model also allows interpolating individual parts of one shape (see fig. <ref type="figure" target="#fig_1">13</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Qualitative Results with Sparse Input</head><p>Here, we investigate whether mixtures of NFs can upsample sparse point clouds. In fig. <ref type="figure" target="#fig_2">14 we</ref> show qualitative examples. Following DPF <ref type="bibr" target="#b31">[32]</ref>, a sparse point cloud of 512 is upsampled to 32768 points. In line with previous results, mixtures of NFs yield better results in high-frequency regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.">Additional Qualitative Results on Generation</head><p>We show additional qualitative examples of generated point clouds using mixtures of NFs in fig. <ref type="figure">15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.">Additional Qualitative Results on Autoencoding</head><p>We show additional qualitative examples on autoencoding of point clouds using mixtures of NFs in fig. <ref type="figure" target="#fig_3">16</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.">Additional Qualitative Results on SVR</head><p>We show additional qualitative examples on SVR of point clouds using mixtures of NFs in fig. <ref type="figure" target="#fig_4">17</ref>    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Model architecture. Black arrows indicate the training process, while red arrows represent the sampling of a point cloud from our model. During training, PointNet encodes a point cloud X to infer a posterior distribution p φ (z|X) which can be seen as a distribution over shapes. A prior distribution parameterized by a NF g -1 ψ is trained by maximizing the log-likelihood of z. When training single-view reconstruction (SVR), we condition g -1ψ on an image encoding. The decoder is parameterized as a mixture of NFs conditioned on z where each flow f -1 θ i learns to specialize in a subregion of the shape. Our model is optimized end-to-end by minimizing L = Lprior + LD.</figDesc><graphic coords="4,349.45,63.31,226.75,170.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Toy example. Fitting ground truth distribution (1.) using a NF with 4/2 (2. &amp; 3. / 4. &amp; 5.) coupling layers. We use Mixtures of NFs (2 NFs) (3. &amp; 5.) -color indicates the NF -and a single NF (2. &amp; 4.). Hyperparameters of the coupling layers chosen such that the single NF has more parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Qualitative results of generation. From above to below, we show samples generated with mixtures of NFs using models trained on the airplane, chair and car categories of ShapeNet<ref type="bibr" target="#b6">[7]</ref>.</figDesc><graphic coords="6,13.54,150.01,163.66,122.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Qualitative examples of reconstructions on the ShapeNet [7] test set when training the autoencoding model with random rotations. The first two rows show the reconstructions obtained when applying random rotations to the same input point cloud. The third and fourth row show different test shapes and their reconstructions.</figDesc><graphic coords="7,266.24,579.94,142.18,106.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. (a) Average reconstruction performance (F1-score) on the chair category of ShapeNet for DPF (blue) and a mixture of 4 NFs (orange) vs. the parameter count. The relative improvement increases for small decoder. (b) Relative inference runtime of mixtures of NFs compared with DPF at similar parameter count vs. the mixture size (#Flows). More components reduce the inference runtime since each point is sampled from a smaller network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig</head><label></label><figDesc>Fig. ?? depicts matches used by the MMD metric based on CD and EMD. We observe that matched point clouds are highly dissimilar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Figure 8. Visualization on precision and recall of the F1-score (TOP: DPF, BOTTOM: mixtures of 4 NFs).Points are considered to be more precise when the color is lighter in precision heatmap, meanwhile, points in the recall heatmap are lighter when the ground truth is well reconstructed by the model.</figDesc><graphic coords="14,27.53,102.33,238.34,178.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Failure cases of matches for ground truth shapes when computing MMD based on CD (a) and EMD (b). For each ground truth shape within the test set depicted (top row), searching for the most similar generated shape according to the CD and EMD distance yields clearly different matched shapes (bottom row). This supports the claim that MMD-CD/EMD do not clearly reflect the perceptual quality of generated point clouds.</figDesc><graphic coords="14,256.96,340.38,163.06,122.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Qualitative results of generating point clouds using a decoder with 1 NF with 24 coupling layers (TOP) and 4 NFs with equivalent parameter count (BOTTOM).</figDesc><graphic coords="16,20.54,250.47,178.74,134.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Qualitative examples of interpolation between two point clouds.</figDesc><graphic coords="17,-3.62,492.96,178.74,134.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 .Figure 14 .</head><label>1314</label><figDesc>Figure 13. Qualitative examples of interpolating individual components of the mixture of NFs.</figDesc><graphic coords="18,-3.62,507.06,178.74,134.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 .Figure 16 .Figure 17 .</head><label>151617</label><figDesc>Figure 15. Additional qualitative examples of generated point clouds using mixtures of NFs.</figDesc><graphic coords="20,-3.62,485.79,178.74,134.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>CD [1]</cell><cell>2.76</cell><cell>5.69</cell><cell>5.16</cell><cell>-</cell><cell>39.5 17.1</cell><cell>-</cell><cell cols="2">72.9 92.1</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">l-GAN-EMD [1]</cell><cell>1.77</cell><cell>6.05</cell><cell>4.15</cell><cell>-</cell><cell>39.7 40.4</cell><cell>-</cell><cell cols="2">75.7 73.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Airplane</cell><cell cols="2">PointFlow [53]</cell><cell>1.42</cell><cell>6.05</cell><cell>4.32</cell><cell>-</cell><cell>44.7 48.4</cell><cell>-</cell><cell cols="2">70.9 68.4</cell><cell>-</cell><cell>0.68</cell></row><row><cell></cell><cell cols="2">DPF [32]</cell><cell>1.14</cell><cell>6.03</cell><cell>4.27</cell><cell cols="6">50.84 46.4 48.2 42.7 70.3 67.5 72.7</cell><cell>0.16</cell></row><row><cell></cell><cell cols="2">Ours 4 Flows</cell><cell>1.03</cell><cell>6.06</cell><cell>4.26</cell><cell cols="6">50.11 46.5 48.4 42.3 70.1 66.9 71.7</cell><cell>0.15</cell></row><row><cell></cell><cell></cell><cell>Oracle</cell><cell>0.50</cell><cell>5.97</cell><cell>3.98</cell><cell cols="6">75.48 51.4 52.7 94.3 49.8 48.2 50.2</cell><cell>0.07</cell></row><row><cell></cell><cell cols="2">l-GAN-CD [1]</cell><cell>3.65</cell><cell>16.66</cell><cell>7.91</cell><cell>-</cell><cell>42.3 17.1</cell><cell>-</cell><cell cols="2">68.5 96.5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">l-GAN-EMD [1]</cell><cell>1.27</cell><cell>16.78</cell><cell>5.75</cell><cell>-</cell><cell>44.3 43.8</cell><cell>-</cell><cell cols="2">66.6 67.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Chair</cell><cell cols="2">PointFlow [53]</cell><cell>1.51</cell><cell>17.15</cell><cell>6.20</cell><cell>-</cell><cell>43.3 46.5</cell><cell>-</cell><cell cols="2">67.0 70.4</cell><cell>-</cell><cell>0.29</cell></row><row><cell></cell><cell cols="2">DPF [32]</cell><cell>1.37</cell><cell>17.24</cell><cell>6.13</cell><cell cols="6">19.63 45.1 46.0 34.7 64.8 68.2 67.7</cell><cell>0.26</cell></row><row><cell></cell><cell cols="2">Ours 4 Flows</cell><cell>1.45</cell><cell>17.30</cell><cell>6.11</cell><cell cols="6">21.08 45.2 46.5 39.2 65.3 65.6 62.2</cell><cell>0.26</cell></row><row><cell></cell><cell></cell><cell>Oracle</cell><cell>0.49</cell><cell>16.39</cell><cell>5.71</cell><cell cols="6">49.14 52.8 53.4 99.8 49.7 49.6 49.5</cell><cell>0.08</cell></row><row><cell></cell><cell cols="2">l-GAN-CD [1]</cell><cell>2.65</cell><cell>8.83</cell><cell>5.36</cell><cell>-</cell><cell>41.3 15.9</cell><cell>-</cell><cell cols="2">62.6 92.7</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">l-GAN-EMD [1]</cell><cell>1.31</cell><cell>9.00</cell><cell>4.40</cell><cell>-</cell><cell>38.3 32.9</cell><cell>-</cell><cell cols="2">65.2 63.2</cell><cell>-</cell><cell>-</cell></row><row><cell>Car</cell><cell cols="2">PointFlow [53]</cell><cell>0.59</cell><cell>9.53</cell><cell>4.71</cell><cell>-</cell><cell>42.3 35.8</cell><cell>-</cell><cell cols="2">70.1 74.2</cell><cell>-</cell><cell>0.20</cell></row><row><cell></cell><cell cols="2">DPF [32]</cell><cell>0.57</cell><cell>9.67</cell><cell>4.60</cell><cell cols="6">18.11 40.8 43.7 37.7 71.3 66.0 69.1</cell><cell>0.11</cell></row><row><cell></cell><cell cols="2">Ours 4 Flows</cell><cell>0.55</cell><cell>9.50</cell><cell>4.62</cell><cell cols="6">18.20 41.4 43.8 37.8 69.0 64.8 66.1</cell><cell>0.10</cell></row><row><cell></cell><cell></cell><cell>Oracle</cell><cell>0.37</cell><cell>9.24</cell><cell>4.56</cell><cell cols="6">35.03 52.8 52.7 99.5 50.9 50.5 49.1</cell><cell>0.05</cell></row><row><cell></cell><cell cols="2">l-GAN-CD [1]</cell><cell>3.02</cell><cell>10.39</cell><cell>6.14</cell><cell>-</cell><cell>32.6 32.3</cell><cell>-</cell><cell cols="2">70.7 94.3</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">l-GAN-EMD [1]</cell><cell>1.45</cell><cell>10.61</cell><cell>4.77</cell><cell>-</cell><cell>40.8 39.0</cell><cell>-</cell><cell>69.2</cell><cell>68</cell><cell>-</cell><cell>-</cell></row><row><cell>Average</cell><cell cols="2">PointFlow [53]</cell><cell>1.17</cell><cell>10.91</cell><cell>5.08</cell><cell>-</cell><cell>43.4 43.6</cell><cell>-</cell><cell>69.3</cell><cell>71</cell><cell>-</cell><cell>0.39</cell></row><row><cell></cell><cell cols="2">DPF [32]</cell><cell>1.03</cell><cell>10.98</cell><cell>5.00</cell><cell cols="6">29.5 43.9 46.0 37.7 68.8 67.2 69.1</cell><cell>0.18</cell></row><row><cell></cell><cell cols="2">Ours 4 Flows</cell><cell>1.01</cell><cell>10.95</cell><cell>5.00</cell><cell cols="6">29.80 44.4 46.2 39.8 68.1 65.8 66.7</cell><cell>0.17</cell></row><row><cell></cell><cell></cell><cell>Oracle</cell><cell>0.45</cell><cell>10.53</cell><cell>4.75</cell><cell cols="6">53.22 52.3 52.9 97.9 50.1 49.4 49.6</cell><cell>0.07</cell></row><row><cell></cell><cell></cell><cell>7.07</cell><cell>7.70</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">l-GAN-EMD [1] 9.18</cell><cell>5.30</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">AtlasNet [20]</cell><cell>5.66</cell><cell>5.81</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">PointFlow [53]</cell><cell>7.54</cell><cell>5.18</cell><cell>32.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DPF [32]</cell><cell>6.92</cell><cell>4.66</cell><cell>34.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Ours 4 Flows</cell><cell>6.88</cell><cell>4.80</cell><cell>34.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Oracle</cell><cell></cell><cell>3.10</cell><cell>3.13</cell><cell>76.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p>Generative modeling. JSD, MMD-EMD and MMD-F1 (τ = 10 -4 ) are multiplied by 10 2 , MMD-CD is multiplied by 10 4 Method CD ↓ EMD ↓ F1 ↑, τ = 10 -4</p>l-GAN-CD</p><ref type="bibr" target="#b0">[1]</ref> </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note><p><p><p>Autoencoding. Comparison with related work on the full ShapeNet dataset</p><ref type="bibr" target="#b6">[7]</ref></p>. CD is multiplied by 10 4 , EMD by 10 2 .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Single-view Reconstruction. Comparison with related work on 13 categories of ShapeNet<ref type="bibr" target="#b6">[7]</ref>. CD is multiplied by 10 3 , EMD is multiplied by 10 2 .</figDesc><table><row><cell>Method</cell><cell cols="3">CD ↓ EMD ↓ F1 ↑, τ = 10 -3</cell></row><row><cell>AtlasNet [20]</cell><cell>5.34</cell><cell>12.54</cell><cell>52.2</cell></row><row><cell>DCG [49]</cell><cell>6.35</cell><cell>18.94</cell><cell>45.7</cell></row><row><cell cols="2">Pixel2Mesh [50] 5.91</cell><cell>13.80</cell><cell>-</cell></row><row><cell>DPF [32]</cell><cell>5.80</cell><cell>11.17</cell><cell>52.0</cell></row><row><cell>Ours 4 Flows</cell><cell>5.66</cell><cell>11.18</cell><cell>52.3</cell></row><row><cell>Oracle</cell><cell>1.10</cell><cell>5.70</cell><cell>84.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Auto-encoding. Ablation study on different number of NFs. CD is multiplied by 10 4 , EMD is multiplied by 10 2 .</figDesc><table><row><cell>-4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Generative modeling. Comparison with related work. JSD and MMD-EMD, MMD-F1 (τ = 10 -4) are multiplied by 10 2 , MMD-CD is multiplied by 10 4 Nr. coupling layers Nr. of Flows CD EMD CD EMD CD EMD 24 1 1.11 17.24 6.09 45.7 47.7 66.6 66.4 24 4 1.09 17.49 6.28 45.8 43.5 67.2 73.2 12 1 1.16 17.25 6.14 44.8 47.0 66.4 71.7 12 4 1.10 17.48 6.16 45.0 45.5 70.0 72.6 6 1 1.32 19.47 6.26 39.8 46.1 82.1 79.5 6 4 1.32 19.47 6.47 39.8 42.0 82.1 81.2</figDesc><table><row><cell>JSD ↓</cell><cell>MMD ↓</cell><cell>COV ↑</cell><cell>1-NNA ↓</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Quantitative evaluation of generative modeling using a smaller decoder with (24, 12 &amp; 6 coupling layers).</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3d point clouds</title>
		<author>
			<persName><forename type="first">Panos</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018. 1, 2, 4, 5, 6, 7, 15</date>
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Lynton Ardizzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Lüth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ullrich</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><surname>Köthe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02392</idno>
		<title level="m">Guided image generation with conditional invertible neural networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variational transformer networks for layout generation</title>
		<author>
			<persName><forename type="first">Diego</forename><forename type="middle">Martin</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janis</forename><surname>Postels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-chart generative surface modeling</title>
		<author>
			<persName><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Kezurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Avineri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Flows for simultaneous manifold learning and density estimation</title>
		<author>
			<persName><forename type="first">Johann</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Cranmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="442" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning gradient fields for shape generation</title>
		<author>
			<persName><forename type="first">Ruojin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadar</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.06520</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Angel X Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015">2015. 4, 6, 7, 8, 13</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Prafulla Dhariwal, John Schulman, Ilya Sutskever, and Pieter Abbeel</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName><forename type="first">Danfei</forename><surname>Christopher B Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relaxing bijectivity constraints with continuously indexed normalising flows</title>
		<author>
			<persName><forename type="first">Rob</forename><surname>Cornish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Caterini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Deligiannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008">2020. 2, 3, 8</date>
			<biblScope unit="page" from="2133" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Density estimation using real nvp</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07714</idno>
		<title level="m">A rad approach to deep mixture models</title>
		<imprint>
			<date type="published" when="2008">2019. 2, 3, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004">2017. 1, 2, 4</date>
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiresolution tree networks for 3d point cloud processing</title>
		<author>
			<persName><forename type="first">Matheus</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient boosted normalizing flows</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Giaquinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<title level="m">Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ffjord: Free-form continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky Tq</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A papier-mâché approach to learning 3d surface generation</title>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008">2018. 2, 5, 6, 7, 8</date>
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5769" to="5779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004">June 2016. 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6629" to="6640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with normalizing flows</title>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Polina</forename><surname>Kirichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4615" to="4630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Softflow: Probabilistic framework for normalizing flow on manifolds</title>
		<author>
			<persName><forename type="first">Hyeongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeonseung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joun</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Yeop Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Soo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Takumi</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuniaki</forename><surname>Uehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.02346</idno>
		<title level="m">Chartpointflow for topology-aware 3d point cloud generation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic gradient descent</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR: International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Durk P Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4743" to="4751" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discrete point flow networks for efficient point cloud generation</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edmond</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020">2020. 1, 2, 3, 4, 5, 6, 7, 8, 12, 13</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tanks and temples: Benchmarking large-scale scene reconstruction</title>
		<author>
			<persName><forename type="first">Arno</forename><surname>Knapitsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Srflow: Learning the super-resolution space with normalizing flow</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="715" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Diffusion probabilistic models for 3d point cloud generation</title>
		<author>
			<persName><forename type="first">Shitong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2837" to="2845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reliable fidelity and diversity metrics for generative models</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Ferjad Naeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<idno>PMLR, 2020. 5</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="7176" to="7185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural parts: Learning expressive 3d shape abstractions with invertible neural networks</title>
		<author>
			<persName><forename type="first">Despoina</forename><surname>Paschalidou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<idno type="arXiv">arXiv:2009.00585</idno>
		<title level="m">Guilherme GP Pires and Mário AT Figueiredo. Variational mixture of normalizing flows</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">C-flow: Conditional generative flow models for images and 3d point clouds</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">3d point cloud generative adversarial network based on tree structured graph convolutions</title>
		<author>
			<persName><forename type="first">Dong</forename><forename type="middle">Wook</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Woo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junseok</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Przemysław</forename><surname>Spurek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Zięba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacek</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Trzciński</surname></persName>
		</author>
		<author>
			<persName><surname>Hyperflow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08710</idno>
		<title level="m">Representing 3d objects as surfaces</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pointgrow: Autoregressively learned point cloud generation with self-attention</title>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Sarma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Unsupervised point cloud reconstruction for classific feature learning</title>
		<author>
			<persName><forename type="first">An</forename><surname>Tao</surname></persName>
		</author>
		<ptr target="https://github.com/AnTao97/UnsupervisedPointCloudReconstruction" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">What do single-view 3d reconstruction networks learn?</title>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">René</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3405" to="3414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning localized generative models for 3d point clouds via graph convolution</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Fracastoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Magli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep cascade generation on point sets</title>
		<author>
			<persName><forename type="first">Kaiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2008">2019. 5, 8</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<author>
			<persName><forename type="first">Nanyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Deflow: Learning complex image degradations from unpaired data with conditional flows</title>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05796</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pointflow: 3d point cloud generation with continuous normalizing flows</title>
		<author>
			<persName><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019. 1, 2, 4, 5, 6, 7, 8, 13</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Adversarial autoencoders for compact representations of 3d point clouds. Computer Vision and Image Understanding</title>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Zamorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Zięba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Klukowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafał</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Stokowiec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Trzciński</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
