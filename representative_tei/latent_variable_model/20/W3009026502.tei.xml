<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Comparing the modeling powers of RNN and HMM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Achille</forename><surname>Salaün</surname></persName>
							<email>achille.salaun@nokia.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Samovar</orgName>
								<orgName type="department" key="dep2">Institut Polytechnique de Paris (Evry</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Telecom SudParis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Nokia Bell Labs Nozay</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yohan</forename><surname>Petetin</surname></persName>
							<email>yohan.petetin@telecom-sudparis.eu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Samovar</orgName>
								<orgName type="department" key="dep2">Institut Polytechnique de Paris (Evry</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Telecom SudParis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">François</forename><surname>Desbouvries</surname></persName>
							<email>francois.desbouvries@telecom-sudparis.eu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Samovar</orgName>
								<orgName type="department" key="dep2">Institut Polytechnique de Paris (Evry</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Telecom SudParis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Comparing the modeling powers of RNN and HMM</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T18:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Recurrent neural networks, Hidden Markov Models</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent Neural Networks (RNN) and Hidden Markov Models (HMM) are popular models for processing sequential data and have found many applications such as speech recognition, time series prediction or machine translation. Although both models have been extended in several ways (eg. Long Short Term Memory and Gated Recurrent Unit architectures, Variational RNN, partially observed Markov models. . . ), their theoretical understanding remains partially open. In this context, our approach consists in classifying both models from an information geometry point of view. More precisely, both models can be used for modeling the distribution of a sequence of random observations from a set of latent variables; however, in RNN, the latent variable is deterministically deduced from the current observation and the previous latent variable, while, in HMM, the set of (random) latent variables is a Markov chain. In this paper, we first embed these two generative models into a generative unified model (GUM). We next consider the subclass of GUM models which yield a stationary Gaussian observations probability distribution function (pdf). Such pdf are characterized by their covariance sequence; we show that the GUM model can produce any stationary Gaussian distribution with geometrical covariance structure. We finally discuss about the modeling power of the HMM and RNN submodels, via their associated observations pdf: some observations pdf can be modeled by a RNN, but not by an HMM, and vice versa; some can be produced by both structures, up to a re-parameterization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Let us consider the general time series prediction problem, which consists in predicting random future observations {X t+1 , • • • , X t+j } = X t+1:j from a realization of the past ones, {x 0 ,</p><formula xml:id="formula_0">• • • , x t } = x 0:t ∈ R t+1 .</formula><p>This problem has many applications such as speech recognition <ref type="bibr" target="#b5">[6]</ref>  <ref type="bibr" target="#b11">[12]</ref>, time series prediction <ref type="bibr" target="#b7">[8]</ref>, or machine translation <ref type="bibr" target="#b2">[3]</ref>. Various tools have been proposed to address this problem <ref type="bibr" target="#b13">[14]</ref>. In particular, a solution to such a prediction problem is brought by generative models. Such models aim at modeling the observation sequence with a probability distribution p θ (x 0:t ), where θ describes the set of parameters of the corresponding generative model. Once the model is known (i.e. θ has been estimated), the prediction of the future observations is computed from p θ (x t+1:j |x 0:t ).</p><p>In this paper, we focus on two popular generative models : Recurrent Neural Networks (RNN) on the one hand <ref type="bibr" target="#b9">[10]</ref>  <ref type="bibr" target="#b6">[7]</ref>, and Hidden Markov Models (HMM) <ref type="bibr" target="#b11">[12]</ref>  <ref type="bibr" target="#b4">[5]</ref> on the other hand.</p><p>Both models build a distribution p θ (x 0:T ) via some latent and possibly random variables H 0:T :</p><p>• In the RNN, each latent variable is deduced deterministically from the past one and from the previous observation; • By contrast, the distribution of the sequence of observations produced by an HMM is a marginal of a joint distribution which now involves random hidden variables.</p><p>In the common vision, these hidden variables are discrete <ref type="bibr" target="#b11">[12]</ref>; however, the general definition of such a model (see e.g. <ref type="bibr" target="#b4">[5]</ref>) also encompasses the case where these variables are continuous. And indeed, such (continuous states) HMM have been widely used in many engineering applications such as econometric or tracking problems <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Due to their proximity with RNN, we will only focus here on continuous states HMM.</p><p>So RNN and HMM share similarities since they both involve latent variables, but they differ from the way that those variables are built. Consequently, a natural question is to compare both models in terms of modeling power. More precisely, we wonder what kind of distributions p θ (x 0:T ) can be modeled by each model, and what are the consequences induced by the different construction of the latent variables. As we shall see, there is no trivial inclusion of RNN into HMM or the converse but the modeling power of HMM is wider than that of RNN, even if there exists some distributions which can be only reached by the RNN structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. GENERATIVE MODELS</head><p>In this section we first recall the principle of RNN and of HMM. We next show that both models can be seen as particular cases of a common probabilistic model called Generative Unified Model (GUM). Finally, we discuss the assumptions underlying our comparative study of these generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. RNN</head><p>RNN are particular neural networks which take into account the temporal structure of the data and are described by a set of parameters θ = (θ 0 , θ 1 , θ 2 ). The distribution of the observations is obtained by managing hidden units h t ∈ R that are sequentially and deterministically computed through a given activation function f (.) and additional parameters θ 1 = (W hh , W xh , k):</p><formula xml:id="formula_1">h t = f θ1 (h t-1 , x t ) = f (W hh h t-1 + W xh x t + k). (1)</formula><p>Next, the distribution of the observations is directly deduced from the hidden units,</p><formula xml:id="formula_2">p θ (x 0:T ) = p θ (x 0 ) T t=1 p θ (x t |x 0:t-1 ) = p θ0 (x 0 ) T t=1 p θ2 (x t |h t-1 ),</formula><p>(2) where p θ0 (x 0 ) and p θ2 (x t |h t-1 ) are given parametrized distributions. Since by construction the likelihood p θ (x 0:T ) is computable, the parameters θ which define (2) can be estimated by applying a gradient ascent method. In particular, the popular backpropagation algorithm <ref type="bibr" target="#b10">[11]</ref> provides a solution to compute this gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. HMM</head><p>Continuous states HMM models are graphical statistical models which have found many applications in signal processing, in particular in contexts where the objective is to estimate a sequence of hidden states from a sequence of observations (e.g., estimating the kinematics of a target from noisy radar measurements). However, such models can also be used to model a sequence of observations. Here, the distribution p θ (x 0:T ) is the marginal of the joint distribution of the latent and observed variables (H 0:T , X 0:T ),</p><formula xml:id="formula_3">p θ (h 0:T , x 0:T ) = p θ0 (h 0 ) T t=1 p θ1 (h t |h t-1 ) T t=0 p θ2 (x t |h t ). (3)</formula><p>This factorization describes the fact that H 0:T is a Markov chain characterized by an initial distribution p θ0 (h 0 ) and a transition distribution p θ1 (h t |h t-1 ) and that given the latent variables h 0:t , the observations X 0:T are independent, and X t only depends on the hidden state at the same instant t via the likelihood p θ2 (x t |h t ). Here, the computation of the predictive likelihood p θ (x t+1 |x 0:t ) relies on the Bayes filter and its associated approximations <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. GUM</head><p>As we have seen, both HMM and RNN rely on a sequence of hidden variables to model a distribution p θ (x 0:t ). Moreover, by translating the temporal indexes of the hidden units of the RNN, i.e. by setting h t = h t-1 , we observe that both models share a different but close structure in terms of conditional dependencies. Indeed, in both cases, the pair {H t , X t } t≥0 is Markovian. In the HMM case, H t is a random variable and its distribution is deduced from p θ1 (h t |h t-1 ),</p><formula xml:id="formula_4">p θ (h t , x t |h t-1 , x t-1 ) = p θ1 (h t |h t-1 )p θ2 (x t |h t );<label>(4)</label></formula><p>while in the RNN, h t is deterministic given h t-1 and x t-1 ,</p><formula xml:id="formula_5">p θ (h t , x t |h t-1 , x t-1 ) = δ f θ 1 (ht-1,xt-1) (h t )×p θ2 (x t |h t ),<label>(5)</label></formula><p>where δ a (x) stands for the Dirac delta function at point a.</p><p>Finally, the observation x t is generated from the hidden state h t whatever the considered model. From ( <ref type="formula" target="#formula_4">4</ref>) and ( <ref type="formula" target="#formula_5">5</ref>), both models can be seen as particular cases of a GUM parametrized by θ, in which the pair {H T , X T } is Markovian and the associated transition distribution reads</p><formula xml:id="formula_6">p θ (h t , x t |h t-1 , x t-1 ) = p θ1 (h t |h t-1 , x t-1 )p θ2 (x t |h t ). (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>The graphical structures of the three models are summarized in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Scope of the study</head><p>As we have explained above, RNN and HMM can now be seen as two particular cases of the GUM and only differ from the distribution of the latent variable H t , given h t-1 and x t-1 . We now exploit this unified framework to compare the modeling power of each model by characterizing and comparing the distributions p θ (x 0:T ) and p θ (x 0:T ) in function of θ and of θ. In order to address such a theoretical comparison between RNN and HMM with feasible computations, we focus on the case where the objective is to model a sequence of observations X 0:T in which each observation X t follows a known Gaussian distribution p(x t ) which does not depend on t. Thus, the models can now be directly compared via the joint distribution p θ (x 0:T ) in the GUM. We assume that</p><formula xml:id="formula_8">p θ0 (h 0 ) = N (h 0 ; m 0 ; η),<label>(7)</label></formula><formula xml:id="formula_9">p θ1 (h t |h t-1 , x t-1 ) = N (h t ; ah t-1 + cx t-1 ; α), (8) p θ2 (x t |h t ) = N (x t ; bh t ; β).<label>(9)</label></formula><p>Note that the linear characteristic of the model (equations ( <ref type="formula">8</ref>) and ( <ref type="formula" target="#formula_9">9</ref>)) ensures that the distributions p(x t ) of each observation X t is Gaussian (and indeed that p(x 0:t ) is Gaussian too). Let us now comment on these assumptions. Setting c = 0 we get linear and Gaussian HMM, which actually are ubiquitous in many applications such as navigation and tracking (see e.g. <ref type="bibr" target="#b8">[9]</ref>); however setting α = 0 we get RNN with linear activation functions, whereas activation functions are generally nonlinear and are a key of the modeling power of that model. Note however that we address a fair comparison in the sense that non linear activation function / transition distribution can be used in practice in both models. Our comparison study is next led in three steps. First, in the linear and Gaussian GUM framework, we identify the class of models parametrized by θ = (a, b, c, η, α, β) which satisfy p(x t ) = N (x t ; 0; 1) for all t; we thus obtain a class of joint Gaussian distributions p θ (x 0:t ) which only differ by their associated covariance matrix. We next study the modeling power of this family of distributions and we show that the linear and Gaussian GUM can model any multivariate stationary Gaussian distribution in which, for all τ , the covariance function has the form cov(X t , X t+τ ) = A τ -1 B, for appropriate constants A and B. Finally, we discuss the modeling power of the RNN (2) and of the HMM (3) w.r.t. the GUM, by drawing a cartography of the three models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODELING POWER OF THE GUM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Structure of the covariance sequence</head><p>From now on, we study the modeling power of the GUM with assumptions ( <ref type="formula" target="#formula_8">7</ref>)-( <ref type="formula" target="#formula_9">9</ref>) and m 0 = 0. By using the Markovianity of (H t , X t ), we build p θ (h 0:T , x 0:T ), and next p θ (x 0:T ) by marginalizing out the hidden states h 0:T . Due to the linear and Gaussian assumption (see section II-D), it is easy to check that p θ (x 0:T ) is Gaussian, and is thus described by a mean</p><formula xml:id="formula_10">H t-1 H t X t-1 X t (a) HMM H t-1 H t X t-1 X t (b) RNN H t-1 H t X t-1 X t (c) GUM</formula><p>Fig. <ref type="figure" target="#fig_0">1</ref>: Conditional dependencies in RNN, HMM, and GUM. Dashed arrows represent deterministic dependencies. Plain ones are probabilistic dependencies. The HMM and the RNN are particular instances of the GUM. In the first case, H t is conditionally independent of X t-1 ; in the second one, H t is no longer random given H t-1 and X t-1 .</p><p>vector and a covariance matrix. In this paper, we focus on the dependency structure induced by the three models. For that reason, we assume without loss of generality that p(x t ) does not depend on t, and indeed without loss of generality that</p><formula xml:id="formula_11">( ) : ∀t ∈ N, p θ (x t ) = N (x t ; 0; 1)</formula><p>(In practice, choosing standard marginals can be seen as a renormalization of the data; releasing ( ) would introduce translations and dilatations in the computations). The fact that varX t = 1 for all t implies that:</p><formula xml:id="formula_12">β = 1 -b 2 η (10) α = (1 -a 2 -2abc)η -c 2<label>(11)</label></formula><p>The first equation is obtained from the computation of p(x 0 ); the second one comes from the computation of p(x 1 ). As α and β are functions of a, b, c, η, any linear Gaussian GUM under constraint ( ) is fully described by these last four parameters. Nonetheless, they cannot be chosen freely since α and β have to be positive. Finally, for all T ∈ N * , p(x 0:T ) = N (x 0:T ; 0 T ; Σ T ), where the covariance matrix Σ T has ones on its diagonal and is defined elsewhere by the covariances:</p><formula xml:id="formula_13">∀t ∈ N, ∀τ ∈ N * , cov(X t , X t+τ ) = (a + bc) τ -1 (ab 2 η + bc)<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Positivity constraints on the covariance parameters</head><p>As we have just seen, for any linear and Gaussian GUM under constraint ( ), cov(X t , X t+τ ) is geometrical ie. cov(X t , X t+τ ) = A τ -1 B for some A and B. Conversely, for any A and B, the Toeplitz symmetric matrix R T (A, B) with first row [1, B, AB, ..., A T -2 B] is not necessarily a covariance matrix, since at this point we do not know whether R T (A, B) is indeed positive semi-definite. We thus characterize the (A, B) domain for which R T (A, B) is a covariance matrix for all T ∈ N * . We have the following result (the proof is omitted due to lack of space but relies on the Carathéodory-Toeplitz theorem <ref type="bibr" target="#b0">[1]</ref>): </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. GUM EQUIVALENCE CLASSES</head><p>At this point we know that the observations distribution of any linear and Gaussian GUM under constraint ( ) is Gaussian stationary with geometrical covariance structure A τ -1 B, with (A, B) ∈ S. Conversely, we now wonder whether any such pdf can be modeled by some GUM. In other words, we study the inverse mapping of:</p><formula xml:id="formula_14">φ : (a, b, c, η) → (A = a + bc, B = ab 2 η + bc)<label>(13)</label></formula><p>One can easily show that φ is surjective, i.e. for any (A, B) ∈ S, there exists at least one GUM providing p A,B (x 0:t ) and we can characterize this GUM. However it is not injective since two different GUM can model a same observation distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Modeling power of RNN and HMM</head><p>We now study whether some distribution p A,B (x 0:T ) for (A, B) ∈ S, can be produced either by an RNN, or by an HMM, or both, or neither of them.</p><p>• HMM: In the HMM case, c = 0, which implies A = a and B = ab 2 η. Taking into account this constraint yields the exact set of HMM models φ -1 (A, B) described by |B| ≤ |A| and AB ≥ 0. • RNN: In the standard RNN (2) with constraint ( ), α = 0 and c 2 = η. This second constraint comes from the way we usually initialize the RNN which is opposite to that of the GUM (the standard RNN starts by modeling the initial distribution of x 0 from which is computed the distribution of h0 via the deterministic transition). These contraints yield to the exact set of standard RNN models φ -1 (A, B) described by (B = A(2A 2 -1) and -1 ≤ A ≤ 1)∪(A = B and -1 ≤ A ≤ 1) ∪ (A ∈ R, B = 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discussion</head><p>Fig. <ref type="figure" target="#fig_1">2</ref> shows whether a distribution can be modeled by an RNN (orange), an HMM (blue), a GUM (light blue) or none of them. For example, there is at least one instance of GUM modeling the distribution described by</p><formula xml:id="formula_15">(A = 1 2 , B =<label>1 4</label></formula><p>). Among all the possible instances of GUM reaching that distribution, some are actually HMM but none are RNN. In that example, the set of parameters (a = 1 3 , b = 1 2 , c = 1 3 , η = 1) provides a proper solution (among others). As we see from the figure, any pdf in S can be obtained by at least one GUM; furthermore some can be produced by an HMM, but not by a RNN, and vice versa; finally some points can be reached by both.</p><p>Note that the expression of cov(X t , X t+τ ) given in ( <ref type="formula" target="#formula_13">12</ref>) displays a behavior of the GUM already known in HMM and RNN; unless |a + bc| = 1, the current output is geometrically uncorrelated from the past outputs; in the case where |a+bc| = 1, (10) and ( <ref type="formula" target="#formula_12">11</ref>) yield α = 0 and βc 2 = 0. In other words, to have long term dependencies, determinism through time is required. This phenomenon has already been observed in <ref type="bibr" target="#b3">[4]</ref>.</p><p>Let us now comment on the dimensionality of the models, which is related to the associated set of points in Figure <ref type="figure" target="#fig_1">2</ref>. The RNN model is parameterized only by two parameters, η and the product bc (this product bc comes from the initialization constraint c 2 = η), whence the curve. By contrast the HMM is parameterized by three parameters a, b and η, whence the two triangles. Such distributions can be modeled by a GUM. The blue (resp. orange) areas (resp. curves) coincide with the value of A and B which can be taken by the HMM (resp. the RNN). This results shows that the modeling power of the GUM is larger than that of the HMM, which is larger than that of the RNN. However, let us note that in the context of this study, the RNN is finally defined by only 2 free parameters, the HMM relies on 3 parameters and the GUM on 4 parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUDING REMARKS</head><p>In this paper we compared two popular models for processing time series, the HMM and the RNN. First, we have encompassed both models in a common probabilistic model: both models can be seen as a particular instance of a GUM (whatever the activation function for the RNN, or the transition and likelihood distributions for the HMM). In order to address an exact comparison of the expressivity power of both models, we have focussed on the linear and Gaussian case. We have thus shown that the linear and Gaussian GUM can model a large class of stationary multivariate Gaussian distributions with geometrical covariance sequence. We also showed that none of the RNN or HMM sets is included into the other one, but that the modeling power of each model could easily be extended to the GUM framework, at the price of an augmentation of the number of parameters which characterize the model. However by considering deterministic transitions this price augmentation can be overcome. This highlights a more general trade-off between expressivity and practicability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 1 :</head><label>1</label><figDesc>Let R T (A, B) be a Toeplitz symmetric matrix with first row [1, B, AB, ..., A T -2 B]. R T (A, B) is a covariance matrix for all T ∈ N * if and only if (A, B) belongs to the parallelogram P defined by A ∈ [-1, 1] and A-1 2 ≤ B ≤ A+1 2 ; or to the line D defined by B = 0. We set S def = P ∪ D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Modeling powers of RNN, HMM and GUM with regards to A and B. The parallelogram (light blue) coincides with all the multivariate centered Gaussian distributions with a covariance matrix which satisfy Cov(X t , X t+τ ) = A τ -1 B.Such distributions can be modeled by a GUM. The blue (resp. orange) areas (resp. curves) coincide with the value of A and B which can be taken by the HMM (resp. the RNN). This results shows that the modeling power of the GUM is larger than that of the HMM, which is larger than that of the RNN. However, let us note that in the context of this study, the RNN is finally defined by only 2 free parameters, the HMM relies on 3 parameters and the GUM on 4 parameters.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The classical moment problem and some related questions in analysis</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Akhiezer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kemmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965">1965</date>
			<publisher>Oliver &amp; Boyd Edinburgh</publisher>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A tutorial on particle filters for online nonlinear / non-Gaussian Bayesian tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arulampalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maskell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Clapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="174" to="188" />
			<date type="published" when="2002-02">February 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diffusion of credit in markovian models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="553" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inference in Hidden Markov Models</title>
		<author>
			<persName><forename type="first">O</forename><surname>Cappé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Moulines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rydén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Springer Series in Statistics</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gonina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4774" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent neural networks and robust time series prediction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="240" to="254" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Forecasting, structural time series models and the Kalman filter</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Harvey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural networks and physical systems with emergent collective computational abilities</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="2554" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Beyond the Kalman Filter: Particle Filters for Tracking Applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ristic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arulampalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gordon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Artec House</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Shumway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Stoffer</surname></persName>
		</author>
		<title level="m">Time Series Analysis and Its Applications (Springer Texts in Statistics)</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
