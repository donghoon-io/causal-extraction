<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rhetorically Controlled Encoder-Decoder for Modern Chinese Poetry Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhiqiang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition Center</orgName>
								<orgName type="institution">Tencent Inc</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zuohui</forename><surname>Fu</surname></persName>
							<email>zuohui.fu@rutgers.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Cao</surname></persName>
							<email>jcao@cs.utah.edu</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Pattern Recognition Center</orgName>
								<orgName type="institution">University of Utah</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<region>Tencent Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yik-Cheung</forename><surname>Tam</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition Center</orgName>
								<orgName type="institution">Tencent Inc</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Niu</surname></persName>
							<email>niucheng@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition Center</orgName>
								<orgName type="institution">Tencent Inc</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition Center</orgName>
								<orgName type="institution">Tencent Inc</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rhetorically Controlled Encoder-Decoder for Modern Chinese Poetry Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T18:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Rhetoric is a vital element in modern poetry, and plays an essential role in improving its aesthetics. However, to date, it has not been considered in research on automatic poetry generation. In this paper, we propose a rhetorically controlled encoder-decoder for modern Chinese poetry generation. Our model relies on a continuous latent variable as a rhetoric controller to capture various rhetorical patterns in an encoder, and then incorporates rhetoricbased mixtures while generating modern Chinese poetry. For metaphor and personification, an automated evaluation shows that our model outperforms state-of-the-art baselines by a substantial margin, while a human evaluation shows that our model generates better poems than baseline methods in terms of fluency, coherence, meaningfulness, and rhetorical aesthetics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern Chinese poetry, originating from 1900 CE, is one of the most important literary formats in Chinese culture and indeed has had a profound influence on the development of modern Chinese culture. Rhetoric is a vital element in modern poetry, and plays an important role in enhancing its aesthetics. Incorporating intentional rhetorical embellishments is essential to achieving the desired stylistic aspects of impassioned modern Chinese poetry. In particular, the use of metaphor and personification, both frequently used forms of rhetoric, are able to enrich the emotional impact of a poem. Specifically, a metaphor is a figure of speech that describes one concept in terms of another one. Within this paper, the term "metaphor" is considered in the sense of a general figure of speech 比喻 (bi yu), encompassing both metaphor in its narrower sense and similes. Personification is a figure of speech in which a thing, an idea or an animal is given human attributes, i.e., nonhuman objects are portrayed in such a way that we feel they have the ability to act like human beings. For example, 她笑起来像花儿一样 ('She smiles like lovely flowers' ) with its connection between smiling and flowers highlights extraordinary beauty and pureness in describing the verb 'smile'. 夜空中的星星眨着眼睛 ('Stars in the night sky squinting' ) serves as an example of personification, as stars are personified and described as squinting, which is normally considered an act of humans, but here is invoked to more vividly describe twinkling stars.</p><p>As is well known, rhetoric encompasses a variety of forms, including metaphor, personification, exaggeration, and parallelism. For our work, we collected more than 8,000 Chinese poems and over 50,000 Chinese song lyrics. Based on the statistics given in Table <ref type="table" target="#tab_0">1</ref>, we observe that metaphor and personification are the most frequently used rhetorical styles in modern Chinese poetry and lyrics (see Section 4.1 for details about this data). Hence, we will mainly focus on the generation of metaphor and personification in this work. As an example, an excerpt from the modern Chinese poem 独自 (Alone) is given in Figure <ref type="figure" target="#fig_0">1</ref>, where the fourth sentence (highlighted in blue) invokes a metaphorical simile, while the second one (highlighted in red) contains a personification.</p><p>In recent years, neural generation models have become widespread in natural language processing (NLP), e.g., for response generation in dialogue <ref type="bibr" target="#b10">(Le et al., 2018)</ref>, answer or question generation in question answering, and headline generation in news systems. At the same time, poetry generation is of growing interest and has attained high levels of quality for classical Chinese poetry. Previously, Chinese poem composing research mainly focused on traditional Chinese poems. In light of the mostly short sentences and the metrical constraints of traditional Chinese poems, the majority of research attention focused on term selection to improve the thematic consistency <ref type="bibr" target="#b19">(Wang et al., 2016)</ref>.</p><p>In contrast, modern Chinese poetry is more flexible and rich in rhetoric. Unlike sentimentcontrolled or topic-based text generation methods <ref type="bibr" target="#b2">(Ghazvininejad et al., 2016)</ref>, which have been widely used in poetry generation, existing research has largely disregarded the importance of rhetoric in poetry generation. Yet, to emulate humanwritten modern Chinese poems, it appears necessary to consider not only the topics but also the form of expression, especially with regard to rhetoric. In this paper, we propose a novel rhetorically controlled encoder-decoder framework inspired by the above sentiment-controlled and topic-based text generation methods, which can effectively generate poetry with metaphor and personification.</p><p>Overall, the contributions of the paper are as follows:</p><p>• We present the first work to generate modern Chinese poetry while controlling for the use of metaphor and personification, which play an essential role in enhancing the aesthetics of poetry.</p><p>• We propose a novel metaphor and personification generation model with a rhetorically controlled encoder-decoder.</p><p>• We conduct extensive experiments showing that our model outperforms the state-of-theart both in automated and human evaluations.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Poetry Generation</head><p>Poetry generation is a challenging task in NLP.</p><p>Traditional methods <ref type="bibr" target="#b0">(Gervás, 2001;</ref><ref type="bibr" target="#b13">Manurung, 2004;</ref><ref type="bibr" target="#b5">Greene et al., 2010;</ref><ref type="bibr" target="#b6">He et al., 2012)</ref> relied on grammar templates and custom semantic diagrams. In recent years, deep learning-driven methods have shown significant success in poetry generation, and topic-based poetry generation systems have been introduced <ref type="bibr" target="#b3">(Ghazvininejad et al., 2017</ref><ref type="bibr" target="#b1">(Ghazvininejad et al., , 2018;;</ref><ref type="bibr">Yi et al., 2018b)</ref>. In particular, <ref type="bibr" target="#b25">Zhang and Lapata (2014)</ref> propose to generate Chinese quatrains with Recurrent Neural Networks (RNNs), while <ref type="bibr" target="#b19">Wang et al. (2016)</ref> obtain improved results by relying on a planning model for Chinese poetry generation.</p><p>Recently, Memory Networks <ref type="bibr" target="#b16">(Sukhbaatar et al., 2015)</ref> and Neural Turing Machines <ref type="bibr" target="#b4">(Graves et al., 2014)</ref> have proven successful at certain tasks. The most relevant work for poetry generation is that of <ref type="bibr" target="#b24">Zhang et al. (2017)</ref>, which stores hundreds of human-authored poems in a static external memory to improve the generated quatrains and achieve a style transfer. The above models rely on an external memory to hold training data (i.e., external poems and articles). In contrast, <ref type="bibr">Yi et al. (2018a)</ref> dynamically invoke a memory component by saving the writing history into memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Stylistic Language Generation</head><p>The ability to produce diverse sentences in different styles under the same topics is an important characteristic of human writing. Some works have explored style control mechanisms for text generation tasks. For example, <ref type="bibr" target="#b27">Zhou and Wang (2018)</ref> use naturally labeled emojis for large-scale emotional response generation in dialogue. <ref type="bibr" target="#b7">Ke et al. (2018)</ref> and <ref type="bibr" target="#b18">Wang et al. (2018)</ref> propose a sentence controlling function to generate interrogative, imperative, or declarative responses in dialogue. For the task of poetry generation, <ref type="bibr" target="#b20">Yang et al. (2018)</ref> introduce an unsupervised style labeling to generate stylistic poetry, based on mutual information. Inspired by the above works, we regard rhetoric in poetry as a specific style and adopt a Conditional Variational Autoencoder (CVAE) model to generate rhetoric-aware poems.</p><p>CVAEs <ref type="bibr" target="#b15">(Sohn et al., 2015;</ref><ref type="bibr" target="#b9">Larsen et al., 2016)</ref> extend the traditional VAE model (Kingma and Welling, 2014) with an additional conditioned label to guide the generation process. Whereas VAEs essentially directly store latent attributes as probability distributions, CVAEs model latent variables conditioned on random variables. Recent research in dialogue generation shows that language generated by VAE models benefit from a significantly greater diversity in comparison with traditional Seq2Seq models. Recently, CVAEs and adversarial training have been explored for the task of generating classical Chinese poems <ref type="bibr" target="#b12">(Li et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this paper, our goal is to leverage metaphor and personification (known as rhetoric modes) in modern Chinese poetry generation using a dedicated rhetoric control mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Before presenting our model, we first formalize our generation task. The inputs are poetry topics specified by K user-provided keywords {w k } K k=1 . The desired output is a poem consisting of n lines {L i } n i=1 . Since we adopt a sequence-to-sequence framework and generate a poem line by line, the task can be cast as a text generation one, requiring the repeated generation of an i-th line that is coherent in meaning and related to the topics, given the previous i -1 lines L 1:i-1 and the topic keywords w 1:K . In order to control the rhetoric modes, the rhetoric label r may be provided either as an input from the user, or from an automatic prediction based on the context. Hence, the task of poetry line generation can be formalized as follows:</p><formula xml:id="formula_0">L * i = arg max L P (L | L 1:i-1 , w 1:K , r i )<label>(1)</label></formula><p>As mentioned above, incorporating rhetoric into poetic sentences requires controlling for the rhetoric mode and memorizing contextual topic information. To this end, we first propose two conditional variational autoencoder models to effectively control when to generate rhetoric sentences, and which rhetoric mode to use. The first model is a Manual Control CVAE model (MCCVAE). It receives the user's input signal as a rhetoric label r to generate the current sentence in the poem, and is designed for user-controllable poetry generation tasks. The second model is the Automatic Control CVAE (ACCVAE), which automatically predicts when to apply appropriate forms of rhetoric and generates the current sentence based on contextual information.</p><p>Subsequently, to memorize pertinent topic information and generate more coherent rhetorical sentences, we propose a topic memory component to store contextual topic information. At the same time, we propose a rhetorically controlled decoder to generate appropriate rhetorical sentences. This is a mechanism to learn the latent rhetorical distribution given a context and a word, and then perform a rhetorically controlled term selection during the decoding stage. Our proposed framework will later be presented in more detail in Figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Seq2seq Baseline</head><p>Our model is based on the sequence-to-sequence (Seq2Seq) framework, which has been widely used in text generation. The encoder transforms the current input text X = {x 1 , x 2 , ..., x J } into a hidden representation H = {h 1 , h 2 , ..., h J }, as follows:</p><formula xml:id="formula_1">h j = LSTM(e(x j ), h j-1 ), (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where LSTM is a Long Short-Term Memory Network, and e(x j ) denotes the embedding of the word x j .</p><p>The decoder first updates the hidden state S = {s 1 , s 2 , .., s T }, and then generates the next sequence Y = {y 1 , y 2 , ..., y T } as follows:</p><formula xml:id="formula_3">s t = LSTM(e(y t-1 ), s t-1 )) P (y t | y t-1 , s t ) = softmax(W s t ),<label>(3)</label></formula><p>where this second LSTM does not share parameters with the encoder's network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Proposed Models</head><p>In the following, we will describe our models for rhetorically controlled generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Manual Control (MC) CVAE</head><p>We introduce a Conditional Variational Autoencoder (CVAE) for the task of poetry generation. Mathematically, the CVAE is trained by maximizing a variational lower bound on the conditional likelihood of Y given c, in accordance with </p><formula xml:id="formula_4">p(Y | c) = p(Y | z, c) p(z | c) dz,<label>(4)</label></formula><formula xml:id="formula_5">-L(θ D ; θ P ; θ R ; Y, c) = L KL + L decoderCE = KL(q R (z | Y, c) || p P (z | c)) -E q R (z|Y,c) (log p D (Y | z, c))<label>(5)</label></formula><p>Here, θ D , θ P , θ R are the parameters of the decoder, prior network, and recognition network, respectively. Intuitively, the second term maximizes the sentence generation probability after sampling from the recognition network, while the first term minimizes the distance between prior and recognition network. Usually, we assume that both the prior and the recognition networks are multivariate Gaussian distributions, and their mean and log variance are estimated through multilayer perceptrons (MLP) as follows:</p><formula xml:id="formula_6">µ, σ 2 = MLP posterior (LSTM(Y ), c) µ , σ 2 = MLP prior (c)<label>(6)</label></formula><p>A single layer of the LSTM is used to encode the current lines, and obtain the h X component of c.</p><p>The same LSTM structure is also used to encode the next line Y in the training stage. By using Eq. ( <ref type="formula" target="#formula_6">6</ref>), we calculate the KL divergence between these distributions to optimize Eq. ( <ref type="formula" target="#formula_5">5</ref>). Following the practice in <ref type="bibr" target="#b26">Zhao et al. (2017)</ref>, a reparameterization technique is used when sampling from the recognition and the prior network during training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Automatic Control(AC) CVAE</head><p>In the ACCVAE model, we first predict the rhetorical mode of the next sentence using an MLP that is designed as follows:</p><formula xml:id="formula_7">p(r|h X ) = softmax(MLP predictor (h X )) r = arg max p(r | h X )<label>(7)</label></formula><p>In this case, the conditional variable c is also [h X ; e(r)], where h X is taken as the last hidden state of the encoder LSTM. The loss function is then defined as:</p><formula xml:id="formula_8">L = L KL + L decoderCE + L predictorCE<label>(8)</label></formula><p>In this paper, a two-layer MLP is used for Eq. ( <ref type="formula" target="#formula_7">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Topic Memory Component</head><p>As shown above, LSTMs are used to encode the lines of the poem. Considering the fact that Memory Networks <ref type="bibr" target="#b16">(Sukhbaatar et al., 2015)</ref> have demonstrated great power in capturing long temporal dependencies, we incorporate a memory component for the decoding stage. By equipping it with a larger memory capacity, the memory is able to retain temporally distant information in the writing history, and provide a RAM-like mechanism to support model execution. In our poetry generation model, we rely on a special topic memory component to memorize both the topic and the generation history, which are of great help in generating appropriate rhetorical and semantically consistent sentences. As illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, our topic memory is M ∈ R K ×d h , where each row of the matrices is a memory slot with slot size d h and the number of slots is K . Before generating the i-th line L i , topic words w k from the user and the input text are written into the topic memory in advance, which remains unchanged during the generation of a sentence. Memory Reading. We introduce an Addressing Function as α = A(M, q), which calculates the probabilities of each slot of the memory being selected and invoked. Specifically, we define:</p><formula xml:id="formula_9">z k = b T σ(M k , q) α k = softmax(z k ),<label>(9)</label></formula><p>where σ defines a non-linear layer, q is the query vector, b is the parameter, M is the memory to be addressed, M k is the k-th slot of M , and α k is the k-th element in vector α. For the topic memory component, the input q should be [s t-1 ; c; z], so the topic memory is read as follow:</p><formula xml:id="formula_10">α = A r (M, [s t-1 ; c; z]) o t = K k=1 α k M k , (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>where α is the reading probability vector, s t-1 represents the decoder hidden state, and o t is the memory output at the t-th step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Rhetorically Controlled Decoder</head><p>A general Seq2seq model may tend to emit generic and meaningless sentences. In order to create poems with more meaningful and diverse rhetoric, we propose a rhetorically controlled decoder. It assumes that each word in a poem sentence has a latent type designating it as a content word or as a rhetorical word. The decoder then calculates a word type distribution over the latent types given the context, and computes type-specific generation distributions over the entire vocabulary. The final probability of generating a word is a mixture of type-specific generation distributions, where the coefficients are type probabilities. The final generation distribution P(y t | s t , o t , z, c) from the sampled word is defined as</p><formula xml:id="formula_12">P(y t | s t , o t , z, c) = P(y t | τ t = content, s t , o t , z, c) P(τ t = content | s t , z, c) +P(y t | τ t = rhetoric, s t , z, c) P(τ t = rhetoric | s t , z, c),<label>(11)</label></formula><p>where τ t denotes the word type at time step t. This specifies that the final generation probability is a mixture of the type-specific generation probability P(y t | τ t , s t , z, c), weighted by the probability of the type distribution P(τ t | s t , z, c). We refer to this decoder as a rhetorically controlled decoder. The probability distribution over word types is given by</p><formula xml:id="formula_13">P(τ t | s t , z, c) = softmax(W 0 [s t ; z; c] + b 0 ),</formula><p>where s t is the hidden state of the decoder at time step t, W ∈ R k×d with the dimension d. The word type distribution predictor can be trained in decoder training stage together. The type-specific generation distribution is given by</p><formula xml:id="formula_14">P(y t | τ t = content, s t , o t , z, c) = softmax(W content [s t ; o t ; z; c] + b content ) (12) P(y t | τ t = rhetoric, s t , z, c) = softmax(W rhetoric [s t ; z; c] + b rhetoric ), (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>where W content , W rhetoric ∈ R |V |×d , and |V | is the size of the entire vocabulary. Note that the type-specific generation distribution is parameterized by these matrices, indicating that the distribution for each word type has its own parameters. Instead of using a single distribution, our rhetorically controlled decoder enriches the model by applying multiple type-specific generation distributions, which enables the model to convey more information about the potential word to be generated. Also note that the generation distribution is over the same vocabulary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Overall Loss Function</head><p>The CVAE and Seq2seq model with the rhetorically controlled decoder should be trained jointly. Therefore, the overall loss L is a linear combination of the KL term L KL , the classification loss of the rhetoric predictor cross entropy (CE) L predictorCE , the generation loss of the rhetorical controlled decoder cross entropy L decoderCE , and the word type classifier (word type distribution predictor) cross entropy L word classifier :</p><formula xml:id="formula_16">L = L KL + L decoderCE + L word classifier + γL predictorCE (14)</formula><p>The technique of KL cost annealing can address the optimization challenges of vanishing latent variables in this encoder-decoder architecture. γ is set to 0 if the Manual Control CVAE is used, and 1 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Setups</head><p>We conduct all experiments on two datasets 1 . One is a modern Chinese poetry dataset, while the other is a modern Chinese lyrics dataset. We collected the modern Chinese poetry dataset from an online poetry website 2 and crawled about 100,000 Chinese song lyrics from a small set of online music websites. The sentence rhetoric label is required for our model training. To this end, we built a classifier to predict the rhetoric label automatically. We sampled about 15,000 sentences from the original poetry dataset and annotated the data manually with three categories, i.e., metaphor, personification, and other. This dataset was divided into a training set, validation set, and test set. Three classifiers, including LSTM, Bi-LSTM, and Bi-LSTM with a self-attention model, were trained on this dataset. The Bi-LSTM with self-attention classifier <ref type="bibr" target="#b21">(Yang et al., 2016)</ref> outperforms the other models and achieves the best accuracy of 0.83 on the 1 <ref type="url" target="https://github.com/Lucien-qiang/Rhetoric-Generator">https://github.com/Lucien-qiang/Rhetoric-Generator</ref> 2 <ref type="url" target="http://www.shigeku.com/">http://www.shigeku.com/</ref> test set. In this classifier, the sizes of word embedding, hidden state and the attention size are set to 128, 256, 30 respectively, and a two-layer LSTM is used. The results for different classes are given in Table <ref type="table" target="#tab_1">2</ref>. Additionally, we select a large number of poem sentences with metaphor and personification to collect the corresponding rhetorical words. Based on statistics of word counts and part of speech, we obtained over 500 popular words associated with metaphor and personification as rhetorical words. Our statistical results show that these words cover a wide range of metaphorical and anthropomorphic features.</p><p>Meanwhile, in our entire model, the sizes of word embedding, rhetoric label embedding, hidden state are set to 128, 128, 128 respectively. The dimensionality of the latent variable is 256 and a single-layer decoder is used. The word embedding is initialized with word2vec vectors pre-trained on the whole corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Models for Comparisons</head><p>We also compare our model against previous stateof-the-art poetry generation models:</p><p>• Seq2Seq: A sequence-to-sequence generation model, as has been successfully applied to text generation and neural machine translation <ref type="bibr" target="#b17">(Vinyals and Le, 2015)</ref>.</p><p>• HRED: A hierarchical encoder-decoder model for text generation <ref type="bibr" target="#b14">(Serban et al., 2016)</ref>, which employs a hierarchical RNN to model the sentences at both the sentence level and the context level.</p><p>• WM: A recent Working Memory model for poetry generation <ref type="bibr">(Yi et al., 2018b)</ref>.</p><p>• CVAE: A standard CVAE model without the specific decoder. We adopt the same architecture as that introduced in Zhao et al. (2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Design</head><p>In order to obtain objective and realistic evaluation results, we rely on a combination of both machine evaluation and human evaluation. Automated Evaluation. To measure the effectiveness of the models automatically, we adopt several metrics widely used in existing studies. BLEU scores<ref type="foot" target="#foot_0">foot_0</ref> and Perplexity are used to quantify how well the models fit the data. The Rhetoric-F1 score is used to measure the rhetorically controlled accuracy of the generated poem sentences. Specifically, if the rhetoric label of the generated sentence is consistent with the ground truth, the generated result is right, and wrong otherwise. The rhetoric label of each poem sentence is predicted by our rhetoric classifier mentioned above (see 4.1 for details about this classifier). Distinct-1/Distinct-2 <ref type="bibr" target="#b11">(Li et al., 2016)</ref> is used to evaluate the diversity of the generated poems.</p><p>Human Evaluation. Following previous work <ref type="bibr">(Yi et al., 2018b)</ref>, we consider four criteria for human evaluation:</p><p>• Fluency: Whether the generated poem is grammatically correct and fluent.</p><p>• Coherence: Whether the generated poem is coherent with the topics and contexts.</p><p>• Meaningfulness: Whether the generated poem contains meaningful information.</p><p>• Rhetorical Aesthetics: Whether the generated rhetorical poem has some poetic and artistic beauty.</p><p>Each criterion is scored on a 5-point scale ranging from 1 to 5. To build a test set for human evaluation, we randomly select 200 sets of topic words to generate poems with the models. We invite 10 experts<ref type="foot" target="#foot_1">foot_1</ref> to provide scores according to the above criteria and the average score for each criterion is computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation Results</head><p>The results of the automated evaluation are given in Table <ref type="table" target="#tab_2">3</ref>. Our MC model obtains a higher BLEU score and lower perplexity than other baselines on the poetry dataset, which suggests that the model is on a par with other models in generating grammatical sentences. Note that our AC model obtains higher Distinct-1 and Distinct-2 scores because it tends to generate more diverse and informative results.</p><p>In terms of the rhetoric generation accuracy, our model outperforms all the baselines and achieves  the best Rhetoric-F1 score of 0.67 on the poetry dataset, which suggests that our model can control the rhetoric generation substantially more effectively. The other baselines have low scores because they do not possess any direct way to control for rhetoric. Instead, they attempt to learn it automatically from the data, but do not succeed at this particularly well. Table <ref type="table" target="#tab_3">4</ref> provides the results of the human evaluation. We observe that on both datasets, our method achieves the best results in terms of the Meaningfulness and Rhetorical Aesthetics metrics. Additionally, we find that the WM model has higher scores in the Coherence metric over the two datasets, indicating that the memory component has an important effect on the coherence and relevance of the topics. The CVAE model obtains the best results in terms of the Fluency metric, which shows that this model can generate more fluent sentences, but it lacks coherence and meaningfulness. Overall, our model generates poems better than other baselines in terms of fluency, coherence, meaningfulness, and rhetorical aesthetics. In particular, these results show that a rhetorically controlled encoder-decoder can generate reasonable metaphor and personification in poems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study</head><p>Table <ref type="table" target="#tab_5">5</ref> presents example poems generated by our model. These also clearly show that our model can control the rhetoric-specific generation. In Case 1, our model is able to follow the topics 恋爱;脸面 (love, face) and the metaphor label when generating the sentence, e.g., 你的眼神像心灵的花朵 一样绽放 (Your eyes blossom like flowers in my heart). In Case 2, our model obtaining the personification signal is able to generate a personification word 走来 (walk ).</p><p>As an additional case study, we also randomly select a set of topic words {青 春 Youth, 爱 情 Love, 岁月 Years} and present three five-line poems generated by Seq2Seq, WM, and our model, respectively, with the same topics and automatically controlled rhetoric. All the poems generated by the different models according to the same topic words are presented in Figures <ref type="figure">3</ref> and<ref type="figure" target="#fig_2">4</ref>. The poem generated by our model is more diverse and aesthetically pleasing with its use of metaphor and personification, while the two other poems focus more on the topical relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future work</head><p>In this paper, we propose a rhetorically controlled encoder-decoder for modern Chinese poetry generation. Our model utilizes a continuous latent variable to capture various rhetorical patterns that govern the expected rhetorical modes and introduces rhetoric-based mixtures for generation. Experiments show that our model outperforms state-of-the-art approaches and that our model can effectively generate poetry with convincing metaphor and personification.</p><p>In the future, we will investigate the possibility of incorporating additional forms of rhetoric, such as parallelism and exaggeration, to further enhance the model and generate more diverse poems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A modern Chinese poetry with metaphor and personification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 3: The results of the Seq2Seq and WM model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative evaluation of the phenomena of metaphor and personification in modern Chinese poems and lyrics.</figDesc><table><row><cell cols="2">Dataset Docs</cell><cell cols="3">Lines Metaphor Personification</cell></row><row><cell>Poetry</cell><cell cols="2">8,744 137,105</cell><cell>31.4%</cell><cell>18.5%</cell></row><row><cell cols="3">Lyrics 53,150 1,036,425</cell><cell>23.8%</cell><cell>13.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of the rhetoric classifier on the test sets.</figDesc><table><row><cell>Model</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>Metaphor</cell><cell>0.93</cell><cell>0.92</cell><cell>0.92</cell></row><row><cell>Personification</cell><cell>0.69</cell><cell>0.62</cell><cell>0.65</cell></row><row><cell>Other</cell><cell>0.76</cell><cell>0.82</cell><cell>0.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of machine evaluation. PPL represents perplexity.</figDesc><table><row><cell cols="2">Dataset Model</cell><cell>BLEU(%)</cell><cell>PPL</cell><cell cols="5">Precision Recall Rhetoric-F1 Distinct-1 Distinct-2</cell></row><row><cell></cell><cell>Seq2seq</cell><cell>0.38</cell><cell>124.55</cell><cell>0.49</cell><cell>0.45</cell><cell>0.47</cell><cell>0.0315</cell><cell>0.0866</cell></row><row><cell></cell><cell>HRED</cell><cell>0.41</cell><cell>119.74</cell><cell>0.51</cell><cell>0.50</cell><cell>0.50</cell><cell>0.0347</cell><cell>0.0924</cell></row><row><cell>Poetry</cell><cell>CVAE</cell><cell>0.44</cell><cell>108.72</cell><cell>0.62</cell><cell>0.61</cell><cell>0.61</cell><cell>0.0579</cell><cell>0.1775</cell></row><row><cell></cell><cell>WM</cell><cell>0.42</cell><cell>115.39</cell><cell>0.57</cell><cell>0.60</cell><cell>0.58</cell><cell>0.0498</cell><cell>0.1243</cell></row><row><cell></cell><cell>AC model (ours)</cell><cell>0.43</cell><cell>112.28</cell><cell>0.64</cell><cell>0.65</cell><cell>0.64</cell><cell>0.0607</cell><cell>0.1854</cell></row><row><cell></cell><cell>MC model (ours)</cell><cell>0.47</cell><cell>95.65</cell><cell>0.68</cell><cell>0.67</cell><cell>0.67</cell><cell>0.0595</cell><cell>0.1747</cell></row><row><cell></cell><cell>Seq2seq</cell><cell>0.52</cell><cell>257.06</cell><cell>0.37</cell><cell>0.34</cell><cell>0.35</cell><cell>0.0149</cell><cell>0.0574</cell></row><row><cell></cell><cell>HRED</cell><cell>0.54</cell><cell>201.85</cell><cell>0.37</cell><cell>0.35</cell><cell>0.36</cell><cell>0.0193</cell><cell>0.0602</cell></row><row><cell>Lyrics</cell><cell>CVAE</cell><cell>0.59</cell><cell>147.45</cell><cell>0.40</cell><cell>0.41</cell><cell>0.41</cell><cell>0.0231</cell><cell>0.0655</cell></row><row><cell></cell><cell>WM</cell><cell>0.55</cell><cell>183.67</cell><cell>0.37</cell><cell>0.40</cell><cell>0.38</cell><cell>0.0216</cell><cell>0.0628</cell></row><row><cell></cell><cell>AC model (ours)</cell><cell>0.58</cell><cell>159.78</cell><cell>0.41</cell><cell>0.41</cell><cell>0.41</cell><cell>0.0325</cell><cell>0.0817</cell></row><row><cell></cell><cell>MC model (ours)</cell><cell>0.57</cell><cell>170.46</cell><cell>0.45</cell><cell>0.49</cell><cell>0.47</cell><cell>0.0273</cell><cell>0.0739</cell></row><row><cell></cell><cell>Poetry</cell><cell cols="2">Lyrics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">F C M RA F C M RA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Seq2Seq</cell><cell cols="3">2.7 2.4 2.8 2.3 3.0 2.4 2.9 2.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HRED</cell><cell cols="3">2.8 2.9 2.7 2.5 2.9 2.7 3.0 2.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CVAE</cell><cell cols="3">3.2 2.7 3.0 3.1 3.3 2.6 2.9 2.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WM</cell><cell cols="3">3.1 3.4 3.1 3.0 3.1 3.1 2.8 2.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">AC model (ours) 3.0 3.4 3.2 3.5 3.3 3.0 3.1 3.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The results of human evaluation. F means Fluency. C stands for Coherence. M represents Meaningfulness while RA represents Rhetorical Aesthetics.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The result of the rhetoric control.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>The BLEU score is calculated with the standard multibleu.perl script.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>The experts are Chinese literature students or members of a poetry association.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An expert system for the composition of formal spanish poetry</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Gervás</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications and Innovations in Intelligent Systems VIII</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="19" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural poetry translation</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating topical poetry</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1183" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hafez: an interactive poetry generation system</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Priyadarshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, System Demonstrations</title>
		<meeting>ACL 2017, System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic analysis of rhythmic poetry with applications to generation and translation</title>
		<author>
			<persName><forename type="first">Erica</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tugba</forename><surname>Bodrumlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 conference on empirical methods in natural language processing</title>
		<meeting>the 2010 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="524" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating chinese classical poems with statistical machine translation models</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generating informative responses with controlled sentence function</title>
		<author>
			<persName><forename type="first">Pei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1499" to="1508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno>1050:10</idno>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName><forename type="first">Anders</forename><surname>Boesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lindbo</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Søren</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Thin Nguyen, and Svetha Venkatesh</title>
		<author>
			<persName><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1515" to="1525" />
		</imprint>
	</monogr>
	<note>Variational memory encoderdecoder</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter</title>
		<meeting>the 2016 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating classical chinese poems via conditional variational autoencoder and adversarial training</title>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3890" to="3900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An evolutionary algorithm approach to poetry generation</title>
		<author>
			<persName><forename type="first">Hisar</forename><surname>Manurung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<title level="m">A neural conversational model</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to ask questions in opendomain conversational systems with typed decoders</title>
		<author>
			<persName><forename type="first">Yansen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2193" to="2203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with planning based neural network</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1051" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stylistic chinese poetry generation via unsupervised style disentanglement</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3960" to="3969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with a salient-clue mechanism</title>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning</title>
		<meeting>the 22nd Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="241" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with a working memory model</title>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4553" to="4559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Flexible and creative chinese poetry generation using neural memory</title>
		<author>
			<persName><forename type="first">Jiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1364" to="1373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mojitalk: Generating emotional responses at scale</title>
		<author>
			<persName><forename type="first">Xianda</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1128" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
