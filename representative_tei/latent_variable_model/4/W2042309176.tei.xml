<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint classification of actions and object state changes with a latent variable discriminative model</title>
				<funder ref="#_XEXF2ky">
					<orgName type="full">UK Engineering and Physical Sciences Research Council</orgName>
				</funder>
				<funder ref="#_fmf374d">
					<orgName type="full">Royal Academy of Engineering</orgName>
				</funder>
				<funder ref="#_2v8YE2U">
					<orgName type="full">European Commission (TOMSY</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Efstathios</forename><surname>Vafeias</surname></persName>
							<email>e.vafeias@sms.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Efstathios Vafeias is with the School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<postCode>EH8 9QT</postCode>
									<settlement>Edinburgh</settlement>
									<country>UK, email</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Subramanian</forename><surname>Ramamoorthy</surname></persName>
							<email>s.ramamoorthy@ed.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<postCode>EH8 9QT</postCode>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint classification of actions and object state changes with a latent variable discriminative model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ICRA.2014.6907570</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a technique to classify human actions that involve object manipulation. Our focus is to accurately distinguish between actions that are related in that the object's state changes define the essential differences. Our algorithm uses a latent variable conditional random field that allows for the modelling of spatio-temporal relationships between the human motion and the corresponding object state changes. Our approach involves a factored representation that better allows for the description of causal effects in the way human action causes object state changes. The utility of incorporating such structure in our model is that it enables more accurate classification of activities that could enable robots to reason about interaction, and to learn using a high level vocabulary that captures phenomena of interest. We present experiments involving the recognition of human actions, where we show that our factored representation achieves superior performance in comparison to alternate flat representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Among the many perceptual modalities available to a modern robotic system, vision is perhaps the richest in terms of the variety of information it captures about the external world and about the agents within that world. This very richness also makes interpretation highly ambiguous and often brittle. One reason why a robot might try to interpret the visual feed is to identify actions and activities being performed by agents in the environment. Given the aspirations of the robotics community to introduce robots into human environments, robots should be competent in interacting with people and recognizing human actions.</p><p>Activity recognition is typically conceptualised as a classification problem, extracting an underlying context from variability in motion, shape and other confounding factors. The focus of this paper is on a method to improve this process of extracting activity categories, by jointly analysing the actions of the human user and of the object that is the target of the activity. The goal of such a method is to enable the identification of activity categories that are behaviourally meaningful, hence useful in representing and learning of higher level interactions.</p><p>Early computer vision methods for action classification were primarily concerned with variability in motion, e.g., of body pose. While these approaches have enjoyed successes in applications that require distinctions between sequences of poses, they have often succeeded by ignoring interactions with the environment that change the context of the action. In this paper, our focus is on trying to model this notion of context as well. Unlike methods that are based, say, on features of single frames within the activity, we model spatiotemporal interactions with objects that define the classification outcome of an action. This makes our method suitable for the understanding of activities which are best defined by the joint evolution of the state of an object in the environment and the changes in body poses that cause that state to change.</p><p>We present a technique that learns to classify such interactions, from video data, and performs better than alternate baselines due to its use of the joint information in the recognition process. We base our work on object information, without explicitly identifying the object, showing that spatiotemporal relationships are sufficient to improve the performance of activity recognition. We believe this is better suited to the needs of incremental and lifelong learning because the notion of tracking the motion of a not-yet-fully-modelled object conceptually precedes the more sophisticated task of identifying and making inferences about the detailed properties of the object in question.</p><p>In order to explain the concept intuitively before jumping into detail, consider the fact that two actions -picking up and slightly pushing an object -can appear highly aliased and difficult to disambiguate unless one also considers what happened to the object: did it leave the hand and roll away or did it move with the hand away from the surface of a table ? The disambiguating signal is neither the pose of the hand nor the identity of the object. Instead, it is the joint hand-object movement. Incorporating such structure into our models is key to learn 'symbolic' relational concepts.</p><p>In this paper, we build upon previous work on action classification based on state of the art statistical learning techniques. Given our intention to work with sequential data, we adopt a discriminative sequential classifier, Conditional Random Fields(CRF) <ref type="bibr" target="#b0">[1]</ref>. Our model is a variation of the hidden state CRF <ref type="bibr" target="#b1">[2]</ref>, which allows us to consider the objectaction spatio-temporal dependencies upon action classifications. The method is experimentally evaluated in Section IV, where we show that mutually modelling actions and object movements can significantly boost the recognition performance, when these actions involve objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The idea that objects and actions are intertwined and highly dependant was originally proposed by the psychologist James J. Gibson, who coined the term affordance <ref type="bibr" target="#b2">[3]</ref>. Affordance refers to a quality of an object or the environment that allows an agent to perform an action. In the field of robotics, the idea of affordances has been explored from various viewpoints. One popular approach is to apply known motor actions to either segment objects or learn about the affordance possibilities <ref type="bibr" target="#b3">[4]</ref> <ref type="bibr" target="#b4">[5]</ref>. Other approaches consider the role of objects in imitation learning and use them to support motor experiences <ref type="bibr" target="#b5">[6]</ref>. Kruger et. al. <ref type="bibr" target="#b6">[7]</ref> introduced a framework that describes how object state changes can help to form action primitives. In other cases human demonstration is used to visually infer object affordances <ref type="bibr" target="#b7">[8]</ref>. Furthermore, Aksoy et. al. <ref type="bibr" target="#b8">[9]</ref> represent relations between objects to interpret human actions in the context of learning by demonstration.</p><p>There is a large corpus of work in human activity recognition in computer vision and pattern recognition illustrating its utility as a test case for sequential classifiers. Different forms of input to recognition methods include still images, video streams and video with 3D data. Approaches such as those presented in <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b10">[11]</ref> combine pose estimation and object information for single frame inference, to learn the mutual context of actions and objects. These methods have been tested on databases where objects provide enough information, but almost no temporal dependencies are required to classify the image, e.g. holding a racket and taking the serving pose is very likely to be assigned with playing tennis. In our work, we are interested in recognizing more subtle actions that differ at a lower level, where object recognition itself is not enough to generally characterize the activity. The same action can be performed with multiple objects, thus we focus especially on temporal dependencies. Other methods <ref type="bibr" target="#b11">[12]</ref> to classify activities from videos create visual vocabularies of features that capture spatio-temporal statistics, and then feed them into well established classifiers like SVMs or AdaBoost.</p><p>Kjellstrom et al. <ref type="bibr" target="#b7">[8]</ref> use a Factorial CRF to simultaneously classify human actions and object affordances from videos. The difference from our work is that they use object detection that assumes known object instances and accurate hand pose segmentation. This is a valid setting for imitation learning, yet difficult to achieve in other activity recognition scenarios, especially when we do not yet have detailed object labels. While the FCRF and HCRF methods used in this paper are similar in the way they split object and action information, Kjellstrom et al. <ref type="bibr" target="#b7">[8]</ref> consider factorization separately and predict object-action combinations, however we use a joint factorization of object and action hidden states to classify them. Additionally we use hidden states to explore structure from raw input, rather than manually annotating, sequences which contributes to make the supervision part of the algorithm less tedious.</p><p>Unlike the previous work <ref type="bibr" target="#b12">[13]</ref>[11] on classification without an explicit notion of time scale, we also want to model long term temporal relationships of the actions. In this paper, we describe a classifier that accounts for contextual information, and we show that the state of the object can greatly improve the classification of subtle actions. A key feature of many actions, is that they consist of segments of simpler homogeneous motions. Our approach exploits this observation by splitting the trajectory into smaller segments without losing valid information from the motion. Indeed, a key attribute that distinguishes our work from related prior work is that by working at a larger temporal scale, we capture structure in the activities somewhat more efficiently and in a way that is better suited to our applications of interest, such as human-robot coordinated actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>We use a hidden state CRF, which is a discriminative probabilistic model that defines a conditional distribution over sequence labels given the observations. In the remainder of this section we present our training and classification methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The model: Hidden State CRF</head><p>Hidden state Conditional Random Fields (HCRF) are random fields that include latent variables in their structure, shown to perform well in a wide range of problems. The CRF model was first introduced by Lafferty et al. <ref type="bibr" target="#b0">[1]</ref> and has been popular in natural language processing, although it has been increasingly gaining traction in computer vision. Quattoni <ref type="bibr" target="#b1">[2]</ref> extended the CRF framework to incorporate hidden variables in order to capture the spatial relationships of object parts in images, while Wang et. al. <ref type="bibr" target="#b13">[14]</ref> used a HCRF for gestural recognition.</p><p>Like any undirected graphical model, we represent variables as nodes(V) and edges(E) accounting for variable correlation, forming a graph G = (V, E). The graph is factorized and represented as a conditional probability distribution. One of the major advantages of the HCRF is that the latent variables can be dependent on arbitrary features of the observation sequence, giving them the ability to capture long-term temporal, spatial or contextual dependencies. The hidden variables at each time t have the potential to select arbitrary subsets of observations, giving them the ability to depend on observations from any frame of the sequence. Selecting subsets of the full observation set is commonly used in image processing as a way to express spatial relationships. We choose to separate object-related features and body based features connected to different latent variables at each time step t. We create nodes that depend on the f obj t object observations, and nodes that depend on f skel t , the skeleton tracking observations. The observation sequence is represented as T }] a chain of pair nodes(object -skeleton) and their states from their corresponding sets. The normalizing partition function is given by Z(y|x; θ) = y∈Y,s e Ψ(y,s,x;θ)</p><formula xml:id="formula_0">X = [x 1 , x 2 , ..., x T ],</formula><p>The model is factorized through the potential function Ψ(y, s; θ) ∈ . The potential function is parametrized by θ, and its purpose is to measure the compatibility between the observation sequence X, the hidden state configuration, and a label y. The parameter vector θ has 3 components θ = [θ v , θ e , θ y ]. Each of the three vector components is used to model a different factor of the graph. The first component θ v models the dependencies between the raw features<ref type="foot" target="#foot_0">foot_0</ref> f skel t , f obj t and the hidden states</p><formula xml:id="formula_1">h i ∈ H s , h i ∈ H o . The length of the vector θ v is (d s × |H s |) + (d o × |H o |).</formula><p>The component θ e models the connectivity between the hidden states, which, for a fully connected graph like the one we use, has length for In the above definition of Ψ(y, s, x; θ), the function ϕ is the inner product of the features at time step j and the θ v parameters of the corresponding hidden state. The θ y [y, h j ] term stands for the weight of connection between the latent state and the class y, whereas θ e [y, h j , h ] measures the dependency of state h j to state h for the given class y.</p><formula xml:id="formula_2">(|Y |×|H s |×|H s |)+(|Y |×|H o |×|H o |) + 2 × (|Y |×|H s |×|H o |).</formula><p>The above graphical model is an abstract way to think of learning from sequential data, and may be combined with a variety of features depending on the application. It can be expanded to work with a variety of human-object, human-environment or even human-human interactions. In this paper we will focus on a specific setup that we use in our experiments. In this setup, we wish to classify the type of activity involving a person manipulating objects in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>h s h s h s h s h o h o h o h o time time</head><p>Object tracking information </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation</head><p>At each given time step we track a set of features that correspond to the body posture of the person and the object that is being manipulated. We use the skeleton tracking <ref type="bibr" target="#b14">[15]</ref> that is provided with the OpenNi SDK<ref type="foot" target="#foot_1">foot_1</ref> , the tracker provides 18 joint positions, j i = {x i , y i , z i }. The object detection routine is performed at initialization of the algorithm. Assuming that all objects are supported on a plane we try to find the largest plane that fits our point cloud. Following the plane fitting process we identify Euclidean clusters of points whose projection falls into the convex hull of the plane points. Each cluster is treated as a potential object, and only objects that fall within close distance to the user's hands are considered for tracking. For object pose tracking, we use an of-the-shelf Monte Carlo algorithm, created by Ryohei Ueda and implemented as part of the Point Cloud Library <ref type="bibr" target="#b15">[16]</ref>. To calculate the likelihood of the poses it uses weighted metrics based on point cloud data and RGB color values from the Fig. <ref type="figure">2</ref>: Joint positions are transformed to a new coordinate system with x-axis aligned to the mid point of the hips and the left hip, y-axis defined by the mid point of hips and the shoulder center, while z-axis is the normal defined by x-y plane.</p><p>video stream. Object pose information at each time step t is represented as a 6D vector containing position and rotation vectors, o t = {x, y, z, roll, pitch, yaw}.</p><p>While the combination of depth and intensity images can provide a rich set of features, our strategy is to classify the actions with a minimal set. This decision allows us to stress the importance of learning the structure of interaction between object and body motion. The temporal relationships between the state distribution of human actions and the object's spatial changes affect the classification of a sequence. We represent a sequence of length T as X = [x 1 , x 2 , ..., x T ], and each observation at time t is composed of f skel t , f obj t , which are the features extracted from the skeleton tracking and features from object tracking respectively.</p><p>1) Pose features: To create the pose features, we use the 3D body joint locations to build a compact representation of postures. The Kinect sensor offers a real-time estimation of joint positions in the scene. To create our representation we use 6 joints, L/R shoulder, L/R elbow and L/R hand. We transform the positions to a skeleton centric coordinate system and we take the center of hips as the center of the coordinate system, Figure <ref type="figure">2</ref> shows the axes of the new reference frame. By choosing a frame transformation aligned to the direction the person is facing, the skeleton configuration becomes independent of the viewpoint. The Cartesian coordinates of each joint are transformed into spherical coordinates and the radius is omitted, each joint is represented by a tuple j i t = (ϕ i t , θ i t ). The resulting viewpoint invariant feature vector has length of 6×2 = 12.</p><p>2) Hand features: While pose features capture the static joint configuration we also want to include dynamic information about the motion performed at each time step. In order to incorporate motion patterns to our feature set, we compute the joint velocities of the L/R hand.</p><p>3) Object features: The object tracker provides information about the trajectory of an object's center of mass. We use the trajectory to compute the object velocity and the distances from the head joint, L/R hand joint positions. The object information is a 6D vector consisting of the 3 distances and the object's velocity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Managing trajectories</head><p>A typical trajectory has a length of 60-250 frames depending on the class. Our classification model is a dynamic template model, meaning that learned parameters are copied and reused in each step with different inputs. The label of the sequence is estimated by summing the potentials of each frame belonging into a specific class. Long sequences tend to result in error accumulation over time, which has an adverse effect on classification. To alleviate this problem, trajectories are split using a heuristic procedure that detects similar moving patterns and merge them into a single block. We take advantage of the fact that joint speed profiles consist of an accelerating motion segment, a maximum speed segment, and a decelerating segment. Our heuristic sums the squared speed of all joints and then finds the peaks and valleys in the new signal as in Fig. <ref type="figure" target="#fig_2">3</ref>, subject to some constraints, e.g. minimum thresholds for peaks and valleys and a minimum length of 5 frames for a split to be valid. The mean velocity of each joint is the new feature set for a segment. Merging similar frames into a single observation variable has the advantage of creating shorter sequences and thus shorter latent variable chains. While this particular trajectory segmentation method is a simple heuristic, it does manage to capture motion changes and to segment homogenous parts of the motion. Thus, it exploits the full power of our model which relies on capturing temporal correlations between varying states. Alternate trajectory segmentation methods could be used as drop in replacements without altering our overall arguments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Inference and Learning of Parameters θ *</head><p>Given a set of parameter values θ * and a new sequence of observations X, the label y * can be computed as</p><formula xml:id="formula_3">y * = argmax y∈Y P (y|x, θ * )</formula><p>Learning in conditional random fields is treated as an optimization problem, where a θ is estimated through the maximization of the objective function(Eq. 1). The likelihood term of the objective function, logP (y i |x i ; θ), is calculated by loopy belief propagation.</p><formula xml:id="formula_4">L(θ) = N i=1 logP (y i |x i ; θ) - ||θ|| 2 2σ 2 (1) θ * = argmax θ L(θ) (2)</formula><p>where N is the total number of training sequences in the dataset D = {x i , y i }, i = 1, ..., N , and θ are target parameters. The second term, -||θ|| 2 2σ 2 , is the L 2 regularization penalty that we use to avoid overfitting. In a simple CRF, with no hidden states, the likelihood function L(θ) is convex taking the form of an exponential of the potential function. However, in the case of hidden state CRF, we need to marginalize over the hidden states and thus create a summation of exponentials which makes our objective function non-convex. To optimize this function we use gradient ascent with various starting points to avoid local maxima. The optimization algorithm we chose is the L-BFGS <ref type="bibr" target="#b16">[17]</ref> which is shown to perform well with a large number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>To evaluate how our system deals with actions that result in object state changes, we have created a new dataset 3 of people using various objects. Our dataset consists of actions with substantial motion similarities, making it hard to naively distinguish between them without knowing the effect on the objects of the environment. Our baseline comparison is against two different implementations of the HCRF <ref type="bibr" target="#b13">[14]</ref>. While HMM models are ubiquitous tool for modelling sequential data, applying them on high-dimensional real-valued observations is not a trivial task and all freely available methods, that we know of, do not deal with high dimensionality. For this reason, we are unable to compare against alternatives such as HMM variants. We report on experiments with the following models:</p><p>• B model: a simple HCRF model with a single chain of latent variables trained on action features only. • B-O model: a HCRF model with a single chain of latent variables and trained on action and object features that are modelled through a single observation variable. • Full Model: Our model as explained in section III. These models have been selected to bring out the importance of object interactions in activity and behaviour understanding. HCRF models perform reasonably in classifying sequential data, however we show that altering the basic model to explicitly consider the interaction boosts the overall recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>Our overall goal is to model human activity that involves object manipulation. In order to do that we had to create a dataset that is suited to our needs. We recorded 918 sequences from 12 different people. We selected 5 different actions that have similar appearance and statistics if seen out of context (i.e., without the object information). The action 3 Our dataset will be made available at: <ref type="url" target="http://wcms.inf.ed.ac.uk/ipab/autonomy/">http://wcms.inf.ed.ac.uk/ipab/autonomy/</ref> Fig. <ref type="figure">4</ref>: Images from the action sequences, from top to bottom, images show action: drink, push, stack, read set we recorded is based on the categories A={drink, push, pickup, stack objects, read}, as in Fig. <ref type="figure">4</ref>. The actions were not thoroughly scripted, as each person was given simple oral instructions about each action and was not given an explicit instruction regarding preferred or typical trajectories to follow. This is how we expect regular people to behave in everyday base. On each repetition, users freely choose how to perform the action, which hand to use, what the starting position of the object should be, speed of execution, etc. Most subjects did not repeat the same motion, so the majority of the recorded sequences have a wide range of motion variations. The actions were recorded with a Kinect camera at a rate of 30Hz, and the time length of each sequence were from 50 frames to 250 frames depending on the action class.</p><p>All the sequences were recorded in our lab in a fairly generic setting (Fig. <ref type="figure">4</ref>). Users stood in front of a table on which the objects of interest were placed, at distance of 2 ∼ 2.5 meters away from the Kinect camera. The difficulties in recognizing motions in the dataset are primarily related to motion similarity and occlusions. Occlusions seriously affect the performance of the skeleton tracking algorithm, which is really designed to work best in an occlusion free environment. In order to strengthen our hypothesis and test our model, we chose highly similar (i.e., aliased) motions to be part of the dataset. For example, reaching to pick an object produces a similar body posture sequence as pushing an object on the table. Similarity in motion can be found in picking and stacking objects, the reaching part of the motion and the withdrawing of the person are very similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Models and implementation details</head><p>The full model is expected to learn the spatio-temporal action -object state transitions and be able to outperform its simple counterpart where no information fusion is performed. To optimize performance, we search for parameter configurations, keeping the one with the best score on the cross validation set. The free model parameters that need to be determined are the number of hidden states in the sets, H s , H o , the observation window length, the standard deviation (σ) of the L 2 regularization factor and the starting values of θ for the gradient search. For hidden states, we experimented with a number of different state sets, varying from 3 to 8 for the object latent variables and from 4 to 12 for the latent variables that depend on skeleton nodes. Based on the average sequence length we experimented with window sizes of ω = 0, 1, 2. After determining the best pair of hidden state numbers, we set them to a constant value and then tuned the L 2 standard deviation parameter. The σ of the regularization term was set to σ = 1 k where k=-3,...,3.</p><p>To investigate how information fusion affects the classification performance we implemented two alternative hidden state CRF models as comparison methods. Both models contain a single layer of latent variables, meaning we use one latent variable per time-step to form single a chain for each sequence. The first model is trained only with body motion information (noted as B model), the object features are neglected, while in the second HCRF (noted as B-O model) the object features and body motion are modelled through a single observation variable X T . Our aim is to show that we can gain accuracy compared to a model that doesn't consider object context, but also showing that modelling the action and the object information through different observation and latent variables can further improve the performance of the model. Both models have the same free parameters, number of hidden states, regularization factor and the length of the observation window. Parameters are tuned via the same grid search technique as mentioned before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head><p>To evaluate the performance of the different models we report the F 1 score which is a measure of accuracy that considers both the precision and recall. The F 1 score is defined as F 1 = 2 × precicion×recall precicion+recall . In figure <ref type="figure" target="#fig_3">5</ref>, we summarize the F 1 score of our approach for each class with 3 different observation window lengths( ω = 0, 1, 2). For each window parameter we report the best configuration of hidden states and regularization term, which differs for each model. The bar graph shows performance is correlated to the temporal relationships between the hidden states and the observations. Figure <ref type="figure" target="#fig_4">6</ref> shows the confusion matrix of our best results on this dataset with average F 1 score for all the classes of 92.96%. From the confusion matrix, we can see that the lowest classification rate corresponds to the pick class. Picking is a sub-event occurring in every action of the dataset, so in noisy sequences or sequences which have been mistreated during the trajectory splitting, this can cause misclassification. Intuitively when we consider spatio-temporal process we think of them in terms of past state, present state, and future state and the change of state, encoding the past-present-future information in each hidden state and not only through latent state transitions creates a more powerful representation of the action.</p><p>The full model appears to perform well on our current dataset, but in order to show the importance of object information, it is crucial to compare it to similar models that discard the object information or do not implicitly model it. In Figure <ref type="figure" target="#fig_5">7</ref> we show the performance of the three models in each class and in Figure <ref type="figure" target="#fig_6">8</ref> we report the mean F 1 score of each model with the corresponding standard deviation. Between the simple B model and the full model, there is a significant increase in performance by 13.84%. In Fig. <ref type="figure" target="#fig_5">7</ref>, we see that B-Single performs better than our model on the drinking action and matching our models performance on the stacking action. The drinking action is particularly distinctive, so that even the simpler baseline method is already able to capture it and our method provides no added advantage here. The case of the stacking action is more interesting in that the real reason for B-single doing well is a favourable class bias. The stacking action has a similar profile to the pickup action for the whole length of the sequence, while at the start of the sequence it shares profiles with read and push. This class bias pushes the score for stack to equalize the performance of our model. Adding the object information to the training set for the B-O model increases the average F 1 score from 79.11% to 88.83%, and overall there is a lower variance in the accuracy between classes. Comparing the B-O model with our full implementation, we observe a notable increase in the average F 1 score from 88.83% to 92.96% while halving the standard deviation between class accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>The main contribution of our paper is a novel method for activity modelling. Specifically, we present an algorithm that improves upon the state of the art in recognition of actions, which is a key ingredient of HRI where a robot needs to understand the goals and context of human action based on overt behaviour as seen from changes in body pose. Our  main observation is that categorization is vastly improved when we jointly model the changes in body pose and the state of the object that is being acted upon, i.e., the effects on the environment of the person's movement. We do this in the setting of a state of the art statistical learning algorithm for discriminative classification, the HCRF. Our experiments demonstrate that thinking of actions in terms of motion and outcome yields significant overall improvement. We view this work as a first step in understanding how to devise the ability to decode human activity at finer levels, which lead to improved human -robot interactions and learning by demonstration capabilities. Our future plan is to continue this investigation to understand how different time scales, the much longer as well as the subtle but shorter, can be captured in a similarly factored way.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The θ y vector corresponds to the links between the hidden states and the label node y, and is of (|Y |×|H s |+|Y |×|H o |). We define Ψ(y, s, x; θ) as a summation along the chain Ψ(y, s, x; θ) = T j=1 ϕ(f s j , θ v [s j|s ]) + T j θ y [y, s j|s ] + T k=1 ϕ(f o k , θ v [s k|o ]) + T k=1 θ y [y, s k|o ] + T j=2 ( k=s,o θ e [y, s j|k , s j-1|k ]) + θ e [y, s j|o , s j|k ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: A segmented sequence is shown at the bottom of the figure. White nodes represent latent variables of the model, blue lines represent object-related factors, while red lines are used to represent action-related factors. This allows our model to explicitly distinguish between action and the outcome of an action on the manipulted object. In Conditional Random Fields, the latent variable models the dependence between each state and the entire observation sequence in order to deal with the variable length of observations each state is dependant on a window of observations, [x t-ω , x t+ω ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Top: Joint velocities at each timestep. Bottom: Shows the normalized sum of squared velocoties. The green triangles note the split positions. This figure is best viewed in colour.</figDesc><graphic coords="5,351.45,50.08,168.30,126.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Full model with different observation windows ω = 0, 1, 2, reporting the F 1 score for each class. Average F 1 scores for ω = 0, 1, 2 are 81.35%, 87.17% and 92.96% respectively.</figDesc><graphic coords="7,81.73,50.08,189.34,141.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Confusion matrix of our test results with average F 1 score 92.96%. Parameters: h o = 4, h s = 7, ω = 2.</figDesc><graphic coords="7,363.60,50.08,144.00,108.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: F 1 scores of each class for different models. B single layer: Only body motion features are trained. B-O single layer: Both body motion and object features are modeled under the same observation variable. Full Model:The full model as presented in Section III.</figDesc><graphic coords="8,81.73,50.08,189.34,139.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Fig.8: F 1 scores for each model, class mean and its standard deviation. Our full model achieves the best result while maintaining a very low variance between class accuracy. Our implementation shows a more robust approach on how one can jointly classify actions that result to object state changes.</figDesc><graphic coords="8,105.36,273.91,142.09,125.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and each observation x t is an object-action pattern represented by 2 feature vectors, f skel</figDesc><table><row><cell>t 6 . The model has two sets of latent variables, s obj = ∈ 18 , f obj ∈ t</cell></row><row><cell>[s o 1 , s o 2 , ..., s o</cell></row></table><note><p><p><p><p><p>T ], and s skel = [s s 1 , s s 2 , ..., s s T ], each node having zero and first order dependencies. The connectivity of the model is represented in Figure</p>1</p>. Each pair of nodes is assigned to a different set of hidden states according to its type.</p>Latent  </p>variables referring to skeleton s s i are assigned hidden state values from the set h s i ∈ H s , and object variables s o i , are assigned from a different set h o i ∈ H o . The conditional probability distribution of label class is expressed as p(y|x; θ) = s p(y, s|x; θ) = 1 Z(y|x; θ) s e Ψ(y,s,x;θ) We denote as s = [{s o 1 , s s 1 }, {s o 2 , s s 2 }, ..., {s o T , s s</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In cases were the observation window is ω &gt; 0, then ft = [f t-w , f t+w ], so as to include all the raw observations of the time window ω.y Skeleton Tracking InformationObject information Action information</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The OpenNI framework is an open source SDK, more info: http://www.openni.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>We thank <rs type="person">Aris Valtazanos</rs> for helpful discussion and assistance with aspects of presentation. This work has taken place in the <rs type="institution">Robust Autonomy and Decisions (RAD) group, School of Informatics, University of Edinburgh</rs>. The RAD Group is supported in part by grants from the <rs type="funder">UK Engineering and Physical Sciences Research Council</rs> (<rs type="grantNumber">EP/H012338/1</rs>), the <rs type="funder">European Commission (TOMSY</rs> Grant <rs type="grantNumber">270436</rs>, <rs type="grantNumber">FP7-ICT-2009.2.1 Call 6</rs>) and a <rs type="funder">Royal Academy of Engineering</rs> <rs type="grantName">Ingenious grant</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_XEXF2ky">
					<idno type="grant-number">EP/H012338/1</idno>
				</org>
				<org type="funding" xml:id="_2v8YE2U">
					<idno type="grant-number">270436</idno>
				</org>
				<org type="funding" xml:id="_fmf374d">
					<idno type="grant-number">FP7-ICT-2009.2.1 Call 6</idno>
					<orgName type="grant-name">Ingenious grant</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hidden conditional random fields</title>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1848" to="1852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The ecological approach to the visual perception of pictures</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Leonardo</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="227" to="235" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning about objects through action-initial steps towards artificial cognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fitzpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Metta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Natale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sandini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. ICRA&apos;03. IEEE International Conference on</title>
		<meeting>ICRA&apos;03. IEEE International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3140" to="3145" />
		</imprint>
	</monogr>
	<note>Robotics and Automation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning object affordances: From sensory-motor coordination to imitation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bernardino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Santos-Victor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics, IEEE Transactions on</title>
		<imprint>
			<date>Feb</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual learning by imitation with motor representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Santos-Victor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<imprint>
			<date>June</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="438" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning actions from observations</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics &amp; Automation Magazine</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="30" to="43" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual object-action recognition: Inferring object affordances from human demonstration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kjellström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="90" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning the semantics of object-action relations by observation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abramov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dörr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dellen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wörgötter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1229" to="1249" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recognizing human actions from still images with latent poses</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2030" to="2037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling mutual context of object and human pose in human-object interaction activities</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recognizing realistic actions from videos in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="1996" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Observing human-object interactions: Using spatial and functional compatibility for recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date>Oct</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1775" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hidden conditional random fields for gesture recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Demirdjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1521" to="1527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<idno type="DOI">10.1145/2398356.2398381</idno>
		<ptr target="http://doi.acm.org/10.1145/2398356.2398381" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="124" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3D is here: Point Cloud Library (PCL)</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cousins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">May 9-13 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Algorithm 778: L-bfgs-b: Fortran subroutines for large-scale bound-constrained optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<idno type="DOI">10.1145/279232.279236</idno>
		<ptr target="http://doi.acm.org/10.1145/279232.279236" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="550" to="560" />
			<date type="published" when="1997-12">Dec. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
