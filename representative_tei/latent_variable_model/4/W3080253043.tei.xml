<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks Author</title>
				<funder ref="#_tHvUNzn #_8n8vhCF #_D94N4cn">
					<orgName type="full">Australian Research Council</orgName>
					<orgName type="abbreviated">ARC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Zonghan, Pan, Shirui, Long, Guodong, Jiang, Jing, Chang</roleName><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Technology</orgName>
								<orgName type="institution" key="instit2">Monash University</orgName>
								<orgName type="institution" key="instit3">University of Technology</orgName>
								<orgName type="institution" key="instit4">University of Technology</orgName>
								<orgName type="institution" key="instit5">Monash University</orgName>
								<orgName type="institution" key="instit6">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Griffith</forename><surname>Research Online</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Technology</orgName>
								<orgName type="institution" key="instit2">Monash University</orgName>
								<orgName type="institution" key="instit3">University of Technology</orgName>
								<orgName type="institution" key="instit4">University of Technology</orgName>
								<orgName type="institution" key="instit5">Monash University</orgName>
								<orgName type="institution" key="instit6">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
							<email>zonghan.wu-3@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Technology</orgName>
								<orgName type="institution" key="instit2">Monash University</orgName>
								<orgName type="institution" key="instit3">University of Technology</orgName>
								<orgName type="institution" key="instit4">University of Technology</orgName>
								<orgName type="institution" key="instit5">Monash University</orgName>
								<orgName type="institution" key="instit6">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
							<email>shirui.pan@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Technology</orgName>
								<orgName type="institution" key="instit2">Monash University</orgName>
								<orgName type="institution" key="instit3">University of Technology</orgName>
								<orgName type="institution" key="instit4">University of Technology</orgName>
								<orgName type="institution" key="instit5">Monash University</orgName>
								<orgName type="institution" key="instit6">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
							<email>guodong.long@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Technology</orgName>
								<orgName type="institution" key="instit2">Monash University</orgName>
								<orgName type="institution" key="instit3">University of Technology</orgName>
								<orgName type="institution" key="instit4">University of Technology</orgName>
								<orgName type="institution" key="instit5">Monash University</orgName>
								<orgName type="institution" key="instit6">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
							<email>jing.jiang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Technology</orgName>
								<orgName type="institution" key="instit2">Monash University</orgName>
								<orgName type="institution" key="instit3">University of Technology</orgName>
								<orgName type="institution" key="instit4">University of Technology</orgName>
								<orgName type="institution" key="instit5">Monash University</orgName>
								<orgName type="institution" key="instit6">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
							<email>xiaojun.chang@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Technology</orgName>
								<orgName type="institution" key="instit2">Monash University</orgName>
								<orgName type="institution" key="instit3">University of Technology</orgName>
								<orgName type="institution" key="instit4">University of Technology</orgName>
								<orgName type="institution" key="instit5">Monash University</orgName>
								<orgName type="institution" key="instit6">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
							<email>chengqi.zhang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Technology</orgName>
								<orgName type="institution" key="instit2">Monash University</orgName>
								<orgName type="institution" key="instit3">University of Technology</orgName>
								<orgName type="institution" key="instit4">University of Technology</orgName>
								<orgName type="institution" key="instit5">Monash University</orgName>
								<orgName type="institution" key="instit6">University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks Author</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394486.3403118</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph neural networks</term>
					<term>graph structure learning</term>
					<term>multivariate time series forecasting</term>
					<term>spatial-temporal graphs</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields including economics, finance, and traffic. A basic assumption behind multivariate time series forecasting is that its variables depend on one another but, upon looking closely, it is fair to say that existing methods fail to fully exploit latent spatial dependencies between pairs of variables. In recent years, meanwhile, graph neural networks (GNNs) have shown high capability in handling relational dependencies. GNNs require well-defined graph structures for information propagation which means they cannot be applied directly for multivariate time series where the dependencies are not known in advance. In this paper, we propose a general graph neural network framework designed specifically for multivariate time series data. Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework. Experimental results show that our proposed model outperforms the state-of-the-art baseline methods on 3 of 4 benchmark datasets and achieves on-par performance with other approaches on two traffic datasets which provide extra structural information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>â€¢ Computing methodologies â†’ Neural networks; Artificial intelligence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Modern societies have benefited from a wide range of sensors to record changes in temperature, price, traffic speed, electricity usage, and many other forms of data. Recorded time series from different sensors can form multivariate time series data and can be interlinked. For example, the rise in daily temperature may cause an increase in electricity usage. To capture systematic trends over a group of dynamically changing variables, the problem of multivariate time series forecasting has been studied for at least sixty years. It has seen tremendous applications in the domains of economics, finance, bioinformatics, and traffic.</p><p>Multivariate time series forecasting methods inherently assume interdependencies among variables. In other words, each variable depends not only on its historical values but also on other variables. However, existing methods do not exploit latent interdependencies among variables efficiently and effectively. Statistical methods, such as vector auto-regressive model (VAR) and Gaussian process model (GP), assume a linear dependency among variables. The model complexity of statistical methods grows quadratically with the number of variables. They face the problem of overfitting with a large number of variables. Recently developed deep-learning-based methods, including LSTNet <ref type="bibr" target="#b11">[12]</ref> and TPA-LSTM <ref type="bibr" target="#b18">[19]</ref>, are powerful to capture non-linear patterns. LSTNet encodes short-term local information into low dimensional vectors using 1D convolutional neural networks and decodes the vectors through a recurrent neural network. TPA-LSTM processes the inputs by a recurrent neural network and employs a convolutional neural network to calculate the attention score across multiple steps. LSTNet and TPA-LSTM do not model the pair-wise dependencies among variables explicitly, which weakens model interpretability.</p><p>Graphs are a special form of data which describes the relationships between different entities. Recently, graph neural networks have achieved great success in handling graph data due to their permutation-invariance, local connectivity, and compositionality. By propagating information through structures, graph neural networks allow each node in a graph to be aware of its neighborhood context. Multivariate time series forecasting can be viewed naturally from a graph perspective. Variables from multivariate time series can be considered as nodes in a graph, and they are interlinked through their hidden dependency relationships. It follows that modeling multivariate time series data using graph neural networks can be a promising way to preserve their temporal trajectory while exploiting the interdependency among time series.</p><p>The most suitable type of graph neural networks for multivariate time series is spatial-temporal graph neural networks. Spatialtemporal graph neural networks take multivariate time series and an external graph structure as inputs, and they aim to predict future values or labels of multivariate time series. Spatial-temporal graph neural networks have achieved significant improvements compared to methods that do not utilize structural information. However, these approaches still fall short for modeling multivariate time series due to the following challenges:</p><p>â€¢ Challenge 1: Unknown Graph Structure. Existing GNN approaches rely heavily on a pre-defined graph structure in order to perform time series forecasting. In most cases, multivariate time series does not have an explicit graph structure.</p><p>The relationships among variables has to be discovered from data rather than being provided as ground truth knowledge. â€¢ Challenge 2: Graph Learning &amp; GNN Learning. Even though a graph structure is available, most GNN approaches focus only on message passing (GNN Learning) and overlook the fact that the graph structure is not optimal and should be updated during training. The question then is how to simultaneously learn the graph structure and the GNN for time series in an end-to-end framework.</p><p>In this paper, we propose a novel approach to overcome these challenges. As demonstrated by Figure <ref type="figure">1</ref>, our framework consists of three core components -the graph learning layer, the graph convolution module, and the temporal convolution module. For Challenge 1, we propose a novel graph learning layer, which extracts a sparse graph adjacency matrix adaptively based on data. Furthermore, we develop a graph convolution module to address the spatial dependencies among variables, given the adjacency matrix computed by the graph learning layer. This is designed specifically for directed graphs and avoids the over-smoothing problem that frequently occurs in graph convolutional networks. Finally, we propose a temporal convolution module to capture temporal patterns by modified 1D convolutions. It can both discover temporal patterns with multiple frequencies and process very long sequences.</p><p>As all parameters are learnable through gradient descent, the proposed framework is able to model multivariate time series data and learn the internal graph structure simultaneously in an end-toend manner (for Challenge 2). To reduce the difficulty of solving a highly non-convex optimization problem and to reduce memory occupation in processing large graphs, we propose a learning algorithm that uses a curriculum learning strategy to find a better local optimum and splits multivariate time series into subgroups during training. The advantages here are that our proposed framework is generally applicable to both small and large graphs, short and long time series, with and without externally defined graph structures. In summary, our main contributions are as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Structure Learning</head><p>Time series (data points)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Convolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Convolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forecasting Results</head><p>Figure <ref type="figure">1</ref>: A concept map of our proposed framework. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUNDS 2.1 Multivariate Time Series Forecasting</head><p>Time series forecasting has been studied for a long time. The majority of existing methods follow a statistical approach. The autoregressive integrated moving average (ARIMA) <ref type="bibr" target="#b0">[1]</ref> generalizes a family of a linear model, including auto-regressive (AR), moving average (MA), and auto-regressive moving average (ARMA). The vector auto-regressive model (VAR) extends the AR model to capture the linear interdependencies among multiple time series. Similarly, the vector auto-regressive moving average model (VARMA) is proposed as a multivariate version of the ARMA model. Gaussian process (GP), as a Bayesian approach, models the distribution of a multivariate variable over functions. GP can be applied naturally to model multivariate time series data <ref type="bibr" target="#b4">[5]</ref>. Although statistical models are widely used in time series forecasting due to their simplicity and interpretability, they make strong assumptions with respect to a stationary process and they do not scale well to multivariate time series data. Deep-learning-based approaches are free from stationary assumptions and they are effective methods to capture non-linearity. Lai et al. <ref type="bibr" target="#b11">[12]</ref> and Shih et al. <ref type="bibr" target="#b18">[19]</ref> are the first two deep-learning-based models designed for multivariate time series forecasting. They employ convolutional neural networks to capture local dependencies among variables and recurrent neural networks to preserve long-term temporal dependencies. Convolutional neural networks encapsulate interactions among variables into a global hidden state. Therefore, they cannot fully exploit latent dependencies between pairs of variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks</head><p>Graph neural networks have enjoyed great success in handling spatial dependencies among entities in a network. Graph neural networks assume that the state of a node depends on the states of its neighbors. To capture this type of spatial dependency, various kinds of graph neural networks have been developed through message passing <ref type="bibr" target="#b6">[7]</ref>, information propagation <ref type="bibr" target="#b10">[11]</ref>, and graph convolution <ref type="bibr" target="#b9">[10]</ref>. Sharing similar roles, they essentially capture a node's highlevel representation by passing information from a node's neighbors to the node itself. Most recently, we have seen the emergence of a type of graph neural networks known as spatial-temporal graph neural networks. This form of neural networks is proposed initially to solve the problem of traffic prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref> and skeleton-based action recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref>. The inputs to spatialtemporal graph neural networks are multivariate time series with an external graph structure which describes the relationships among variables in multivariate time series. For spatial-temporal graph neural networks, spatial dependencies among nodes are captured by graph convolutions, while temporal dependencies among historical states are preserved by recurrent neural networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref> or 1D convolutions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. Although existing spatial-temporal graph neural networks have achieved significant improvements compared to methods without using a graph structure, they are incapable of handling pure multivariate time series data effectively due to the absence of a pre-defined graph and lack of a general framework. From a graph-based perspective, we consider variables in multivariate time series as nodes in graphs. We describe the relationships among nodes using the graph adjacency matrix. The graph adjacency matrix is not given by the multivariate time series data in most cases and will be learned by our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FRAMEWORK OF MTGNN 4.1 Model Architecture</head><p>We first elaborate on the general framework of our model. As illustrated in Figure <ref type="figure">2</ref>, MTGNN on the highest level consists of a graph learning layer, ğ‘š graph convolution modules, ğ‘š temporal convolution modules, and an output module. To discover hidden associations among nodes, a graph learning layer computes a graph adjacency matrix, which is later used as an input to all graph convolution modules. Graph convolution modules are interleaved with temporal convolution modules to capture spatial and temporal dependencies respectively. Figure <ref type="figure">3</ref> gives a demonstration of how a temporal convolution module and a graph convolution module collaborate with each other. To avoid the problem of gradient vanishing, residual connections are added from the inputs of a temporal convolution module to the outputs of a graph convolution module. Skip connections are added after each temporal convolution module. To get the final outputs, the output module projects the hidden features to the desired output dimension. In more detail, the core components of our model are illustrated in the following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Learning Layer</head><p>The graph learning layer learns a graph adjacency matrix adaptively to capture the hidden relationships among time series data. To construct a graph, existing studies measure the similarity between pairs of nodes by a distance metric, such as dot product and Euclidean distance <ref type="bibr" target="#b12">[13]</ref>. This leads inevitably to the problem of high time and space complexity with ğ‘‚ (ğ‘ 2 ). It means the computation and memory cost grows quadratically with the increase of graph size. This restricts the model's capability of handling larger graphs. To address this limitation, we adopt a sampling approach, which only calculates pair-wise relationships among a subset of nodes. This cuts off the bottleneck of computation and memory in each minibatch. More details will be provided in Section 4.6.</p><p>Another problem is that existing distance metrics are often symmetric or bi-directional. In multivariate time series forecasting, we expect that the change of a node's condition causes the change of another node's condition such as traffic flow. Therefore the learned relation is supposed to be uni-directional. Our proposed graph learning layer is specifically designed to extract uni-directional relationships, illustrated as follows:</p><formula xml:id="formula_0">M 1 = ğ‘¡ğ‘ğ‘›â„(ğ›¼E 1 Î˜ 1 )<label>(1)</label></formula><formula xml:id="formula_1">M 2 = ğ‘¡ğ‘ğ‘›â„(ğ›¼E 2 Î˜ 2 )<label>(2)</label></formula><formula xml:id="formula_2">A = ğ‘…ğ‘’ğ¿ğ‘ˆ (ğ‘¡ğ‘ğ‘›â„(ğ›¼ (M 1 M ğ‘‡ 2 -M 2 M ğ‘‡ 1 )))<label>(3)</label></formula><formula xml:id="formula_3">ğ‘“ ğ‘œğ‘Ÿ ğ‘– = 1, 2, â€¢ â€¢ â€¢ , ğ‘<label>(4)</label></formula><formula xml:id="formula_4">idx = ğ‘ğ‘Ÿğ‘”ğ‘¡ğ‘œğ‘ğ‘˜ (A[ğ‘–, :]) (5) A[ğ‘–, -idx] = 0,<label>(6)</label></formula><p>where  largest values of a vector. The asymmetric property of our proposed graph adjacency matrix is achieved by Equation <ref type="formula" target="#formula_2">3</ref>. The subtraction term and the ReLU activation function regularize the adjacency matrix so that if ğ´ ğ‘£ğ‘¢ is positive, its diagonal counterpart ğ´ ğ‘¢ğ‘£ will be zero. Equation <ref type="formula">5</ref>-6 is a strategy to make the adjacency matrix sparse while reducing the computation cost of the following graph convolution. For each node, we select its top-k closest nodes as its neighbors. While retaining the weights for connected nodes, we set the weights of non-connected nodes as zero.</p><p>Incorporate External Data. The inputs to the graph learning layer are not limited to node embeddings. In case that external knowledge about the attributes of each node is given, we can also set</p><formula xml:id="formula_5">E 1 = E 2 = Z,</formula><p>where Z is a static node feature matrix. Some works have considered capturing dynamic spatial dependencies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>. In other words, they dynamically adjust the weight of two connected nodes based on temporal inputs. However, assuming dynamic spatial dependencies makes the model extremely hard to converge when we need to learn the graph structure at the same time. The advantage of our approach is that we can learn stable and interpretable node relationships over the period of the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mix-hop Propagation Layer</head><p>Mix-hop Propagation Layer  Once the model is trained in an on-line learning version, our graph adjacency matrix is also adaptable to change as new training data updates the model parameters.</p><formula xml:id="formula_6">+ A A T (a) GC module MLP 1 ğ» (#) ğ» %&amp; A MLP K ğ» (') ğ» %&amp; MLP 0 ğ» (() ğ» %&amp; A A â€¦ + ğ» *+,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Graph Convolution Module</head><p>The graph convolution module aims to fuse a node's information with its neighbors' information to handle spatial dependencies in a graph. The graph convolution module consists of two mixhop propagation layers to process inflow and outflow information passed through each node separately. The net inflow information is obtained by adding the outputs of the two mix-hop propagation layers. Figure <ref type="figure" target="#fig_2">4</ref> shows the architecture of the graph convolution module and the mix-hop propagation layer.</p><p>Mix-hop Propagation Layer. Given a graph adjacency matrix, we propose the mix-hop propagation layer to handle information flow over spatially dependent nodes. The proposed mix-hop propagation layer consists of two steps -the information propagation step and the information selection step. We first give the mathematical form of these two steps and then illustrate our motivations. The information propagation step is defined as follows:</p><formula xml:id="formula_7">H (ğ‘˜) = ğ›½H ğ‘–ğ‘› + (1 -ğ›½) ÃƒH (ğ‘˜-1) , (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>where ğ›½ is a hyper parameter, which controls the ratio of retaining the root node's original states. The information selection step is defined as follows</p><formula xml:id="formula_9">H ğ‘œğ‘¢ğ‘¡ = ğ¾ ğ‘–=0 H (ğ‘˜) W (ğ‘˜) , (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>where ğ¾ is the depth of propagation, H ğ‘–ğ‘› represents the input hidden states outputted by the previous layer, H ğ‘œğ‘¢ğ‘¡ represents the output hidden states of the current layer, H (0) = H ğ‘–ğ‘› , Ãƒ = D-1 (A + I), and Dğ‘–ğ‘– = 1 + ğ‘— A ğ‘– ğ‘— . In Figure <ref type="figure" target="#fig_2">4b</ref>, we demonstrate the information propagation step and information selection step in the proposed mix-hop propagation layer. It first propagates information horizontally and selects information vertically.</p><p>The information propagation step propagates node information along with the given graph structure recursively. A severe limitation of graph convolutional networks is that node hidden states converge to a single point as the number of graph convolution layers goes to infinity. This is because the graph convolutional network with many layers reaches the random walk's limit distribution regardless of the initial node states. To address this problem, motivated by Klicpera et al. <ref type="bibr" target="#b10">[11]</ref>, we retain a proportion of nodes' original states during the propagation process so that the propagated node states can both preserve locality and explore a deep neighborhood. However, if we only apply Equation <ref type="formula" target="#formula_7">7</ref>, some node information will be lost. Under the extreme circumstance that no spatial dependencies exist, aggregating neighborhood information simply adds useless noises to each node. Therefore, the information selection step is introduced to filter out important information produced at each hop. According to Equation 8, the parameter matrix W (ğ‘˜) functions as a feature selector. When the given graph structure does not entail spatial dependencies, Equation 8 is still able to preserve the original nodeself information by adjusting W (ğ‘˜) to 0 for all ğ‘˜ &gt; 0.</p><p>Connection to existing works. The idea of mix-hop has been explored by <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b1">[2]</ref>. Kapoor et al. <ref type="bibr" target="#b8">[9]</ref> concatenate information from different hops. Chen et al. <ref type="bibr" target="#b1">[2]</ref> propose an attention mechanism to weight information among different hops. They both apply GCN for information propagation. However, as GCN faces the oversmoothing problem, information from higher hops may not or negatively contribute to the overall performance. To avoid this, our approach keeps a balance between local and neighborhood information. Furthermore, Kapoor et al. <ref type="bibr" target="#b8">[9]</ref> show that their proposed model with two mix-hop layers has the capability to represent the delta difference between two consecutive hops. Our approach can achieve the same effect with only one mix-hop propagation layer. Suppose ğ¾ = 2, W (0) = 0, W (1) = -1, and W (2) = 1, then</p><formula xml:id="formula_11">H ğ‘œğ‘¢ğ‘¡ = Î”(H (2) , H (1) ) = H 2 -H 1 .<label>(9)</label></formula><p>From this perspective, using summation is more efficient to represent all linear interactions of different hops compared with the concatenation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Temporal Convolution Module</head><p>The temporal convolution module applies a set of standard dilated 1D convolution filters to extract high-level temporal features. This module consists of two dilated inception layers. One dilated inception layer is followed by a tangent hyperbolic activation function  and works as a filter. The other layer is followed by a sigmoid activation function and functions as a gate to control the amount of information that the filter can pass to the next module. Figure <ref type="figure" target="#fig_4">5</ref> shows the architecture of the temporal convolution module and the dilated inception layer. Dilated Inception Layer. The temporal convolution module captures sequential patterns of time series data through 1D convolutional filters. To come up with a temporal convolution module that is able to both discover temporal patterns with various ranges and handle very long sequences, we propose the dilated inception layer which combines two widely applied strategies from convolutional neural networks, i.e., using filters with multiple sizes <ref type="bibr" target="#b19">[20]</ref> and applying dilated convolution <ref type="bibr" target="#b23">[24]</ref>.</p><p>First, choosing the right kernel size is a challenging problem for convolutional networks. The filter size can be too large to represent short-term signal patterns subtly, or too small to discover long-term signal patterns sufficiently. In image processing, a widely employed strategy is called inception, which concatenates the outputs of 2D convolution filters with three different kernel sizes, 1 Ã— 1, 3 Ã— 3, and 5 Ã— 5. Moving from 2D images to 1D time series, the set of 1 Ã— 1, 1 Ã— 3, and 1 Ã— 5 filter sizes do not suit the nature of temporal signals.</p><p>As temporal signals tend to have several inherent periods such as 7, 12, 24, 28, and 60, a stack of inception layers with filter size 1 Ã— 1, 1 Ã— 3, and 1 Ã— 5 cannot well encompass those periods. Alternatively, we propose a temporal inception layer consisting of four filter sizes, viz. 1 Ã— 2, 1 Ã— 3, 1 Ã— 6, and 1 Ã— 7. The aforementioned periods can all be covered by the combination of these filter sizes. For example, to represent the period 12, a model can pass the inputs through a 1 Ã— 7 filter from the first temporal inception layer followed by a 1 Ã— 6 filter from the second temporal inception layer.</p><p>Second, the receptive field size of a convolutional network grows in a linear progression with the depth of the network and the kernel size of the filter. Consider a convolutional network with ğ‘š 1ğ· convolution layers of kernel size ğ‘, the receptive field size of the convolutional network is,</p><formula xml:id="formula_12">ğ‘… = ğ‘š(ğ‘ -1) + 1. (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>To process very long sequences, it requires either a very deep network or very large filters. We adopt dilated convolution to reduce model complexity. Dilated convolution operates a standard convolution filter on down-sampled inputs with a certain frequency. For example, where the dilation factor is 2, it applies standard convolution on inputs sampled every two steps. Following <ref type="bibr" target="#b13">[14]</ref>, we let the dilation factor for each layer increase exponentially at a rate of ğ‘ (ğ‘ &gt; 1). Suppose the initial dilation factor is 1, the receptive field size of a ğ‘š layer dilated convolutional network with kernel size ğ‘ is ğ‘… = 1 + (ğ‘ -1)(ğ‘ ğ‘š -1)/(ğ‘ -1). <ref type="bibr" target="#b10">(11)</ref> This indicates that the receptive field size of the network also grows exponentially with an increase in the number of hidden layers at the rate of ğ‘. Therefore, using this dilation strategy can capture much longer sequences than proceeding without it. Formally, combining inception and dilation, we propose the dilated inception layer, demonstrated by Figure <ref type="figure" target="#fig_4">5b</ref>. Given a 1D sequence input z âˆˆ R ğ‘‡ and filters consisting of 6 , and f 1Ã—7 âˆˆ R 7 , our dilated inception layer takes the form,</p><formula xml:id="formula_14">f 1Ã—2 âˆˆ R 2 , f 1Ã—3 âˆˆ R 3 , f 1Ã—6 âˆˆ R</formula><formula xml:id="formula_15">z = ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡ (z â˜… f 1Ã—2 , z â˜… f 1Ã—3 , z â˜… f 1Ã—6 , z â˜… f 1Ã—7 ),<label>(12)</label></formula><p>where the outputs of the four filters are truncated to the same length according to the largest filter and concatenated across the channel dimension, and the dilated convolution denoted by zâ˜…f 1Ã—ğ‘˜ is defined as</p><formula xml:id="formula_16">z â˜… f 1Ã—ğ‘˜ (ğ‘¡) = ğ‘˜-1 ğ‘ =0 f 1Ã—ğ‘˜ (ğ‘ )z(ğ‘¡ -ğ‘‘ Ã— ğ‘ ), (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>where ğ‘‘ is the dilation factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Skip Connection Layer &amp; Output Module</head><p>Skip connection layers are essentially 1 Ã— ğ¿ ğ‘– standard convolutions where ğ¿ ğ‘– is the sequence length of the inputs to the ğ‘– ğ‘¡â„ skip connection layer. It standardizes information that jumps to the output module to have the same sequence length 1. The output module consists of two 1 Ã— 1 standard convolution layers, transforming the channel dimension of the inputs to the desired output dimension.</p><p>In case we want to predict a certain future step only, the desired output dimension is 1. When we want to predict ğ‘„ consecutive steps, the desired output dimension is ğ‘„.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Proposed Learning Algorithm</head><p>We for i in 1:m do compute the stochastic gradient of Î˜ according to ğ¿.</p><p>13:</p><p>update model parameters Î˜ according to their gradients and the learning rate ğ›¾ . ğ‘–ğ‘¡ğ‘’ğ‘Ÿ = ğ‘–ğ‘¡ğ‘’ğ‘Ÿ + 1. 16: until convergence it is computationally expensive, the adjacency matrix can be precomputed in parallel before making predictions.</p><p>The second consideration of our proposed algorithm is to facilitate our model stabilize in a better local optimum. In the task of multi-step forecasting, we observe that long-term predictions often achieve greater improvements than those in the short-term in terms of model performance. We believe the reason is that our model predicts multi-steps altogether, and long-term predictions produce a much higher loss than short-term predictions. As a result, to minimize the overall loss, the model focuses more on improving the accuracy of long-term predictions. To address this issue we propose a curriculum learning strategy for the multi-step forecasting task. The algorithm starts with solving the easiest problem, predicting the next one-step only. It is very advantageous for the model to find a good starting point. With the increase in iteration numbers, we increase the prediction length of the model gradually so that the model can learn the hard task step by step. Covering all this, our algorithm is given in Algorithm 1. Further complexity analysis of our model can be found in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL STUDIES</head><p>We validate MTGNN on two tasks -both single-step and multistep forecasting. First, we compare the performance of MTGNN with other multivariate time series models on four benchmark datasets for multivariate time series forecasting, where the aim is to predict a single future step. Furthermore, to show how well MTGNN performs, compared with other spatial-temporal graph neural networks which, in contrast, use pre-defined graph structural information, we evaluate MTGNN on two benchmark datasets for spatial-temporal graph neural networks, where the aim is to predict multiple future steps. Further results on parameter study can be found in Appendix A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setting</head><p>In Table <ref type="table" target="#tab_4">1</ref>, we summarize statistics of benchmark datasets. More details about the datasets is given in Appendix A.2. We use five evaluation metrics, including Mean Absolute Error (MAE), Root Mean </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline Methods for Comparision</head><p>MTGNN and MTGNN+sampling are our models to be evaluated. MTGNN is our proposed model. MTGNN+sampling is our proposed model trained on a sampled subset of a graph in each iteration.</p><p>Baseline methods are summarized in the following:</p><p>5.2.1 Single-step forecasting.</p><p>â€¢ AR: An auto-regressive model.</p><p>â€¢ VAR-MLP: A hybrid model of the multilayer perception (MLP) and auto-regressive model (VAR) <ref type="bibr" target="#b24">[25]</ref>. â€¢ GP: A Gaussian Process time series model <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>â€¢ RNN-GRU: A recurrent neural network with fully connected GRU hidden units. â€¢ LSTNet: A deep neural network, which combines convolutional neural networks and recurrent neural networks <ref type="bibr" target="#b11">[12]</ref>. â€¢ TPA-LSTM: An attention-recurrent neural network <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Multi-step forecasting.</head><p>â€¢ DCRNN: A diffusion convolutional recurrent neural network, which combines diffusion graph convolutions with recurrent neural networks <ref type="bibr" target="#b12">[13]</ref>. â€¢ STGCN: A spatial-temporal graph convolutional network, which incorporates graph convolutions with 1D convolutions <ref type="bibr" target="#b22">[23]</ref>. â€¢ Graph WaveNet: A spatial-temporal graph convolutional network, which integrates diffusion graph convolutions with 1D dilated convolutions <ref type="bibr" target="#b20">[21]</ref>. â€¢ ST-MetaNet: A sequence-to-sequence architecture, which employs meta networks to generate parameters <ref type="bibr" target="#b14">[15]</ref>. â€¢ GMAN: A graph multi-attention network with spatial and temporal attentions <ref type="bibr" target="#b25">[26]</ref>. â€¢ MRA-BGCN: A multi-range attentive bicomponent GCN <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Main Results</head><p>Table <ref type="table" target="#tab_5">2</ref> and Table <ref type="table" target="#tab_6">3</ref> provide the main experimental results of MT-GNN and MTGNN+sampling. We observe that MTGNN achieves state-of-the-art results on most of the tasks, and the performance of MTGNN only degrades marginal when it samples sub-graphs for training. In the following, we discuss experimental results of single-step and multi-step forecasting respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Single-step forecasting.</head><p>In this experiment, we compare MT-GNN with other multivariate time series models. Table <ref type="table" target="#tab_5">2</ref> shows the experimental results for the single-step forecasting task. In general, our MTGNN achieves state-of-the-art results over almost all horizons on Solar-Energy, Traffic, and Electricity data. In particular, on Traffic data, the improvement of MTGNN in terms of RSE is significant. MTGNN lowers down RSE by 7.24%, 3.88%, 4.83% over the horizons of 3, 12, 24 on the traffic data. The main reason why MTGNN improves the results of traffic data evidently is that the nature of traffic data is better suited for our model assumption about the spatial-temporal dependencies. Obviously, the future traffic occupancy rate of a road not only depends on its past but also on its connected roads' occupancy rates. MTGNN fails to make improvements on the exchange-rate data, possibly due to the smaller graph size and fewer training examples of exchange-rate data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Multi-step forecasting.</head><p>In this experiment, we compare MT-GNN with other spatial-temporal graph neural network models. Table <ref type="table" target="#tab_6">3</ref> shows the experimental results for the task of multi-step forecasting. The significance of MTGNN lies in that it achieves on-par performance with state-of-the-art spatial-temporal graph neural networks without using a pre-defined graph, while DCRNN, STGCN, and MRA-BGCN fully rely on pre-defined graphs. Graph Wavenet proposes a self-adaptive adjacency matrix, but it needs to combine with a pre-defined graph in order to achieve optimal performance. ST-MetaNet employs attention mechanisms to adjust the edge weights of a pre-defined graph. GMAN leverages node2vec algorithm to preserve node structural information while performing attention mechanisms. When a graph is not defined, these methods cannot model multivariate times series data efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>We conduct an ablation study on the METR-LA data to validate the effectiveness of key components that contribute to the improved outcomes of our proposed model. We name MTGNN without different components as follows:</p><p>â€¢ w/o GC: MTGNN without the graph convolution module. We replace the graph convolution module with a linear layer. â€¢ w/o Mix-hop: MTGNN without the information selection step in the mix-hop propagation layer. We pass the outputs of the information propagation step to the next module directly. â€¢ w/o Inception: MTGNN without inception in the dilated inception layer. While keeping the same number of output channels, we use a single 1 Ã— 7 filter only. â€¢ w/o CL: MTGNN without curriculum learning. We train MTGNN without gradually increasing the prediction length. We repeat each experiment 10 times with 50 epochs per repetition and report the average of MAE, RMSE, MAPE with a standard deviation over 10 runs on the validation set in Table <ref type="table" target="#tab_7">4</ref>. The introduction of graph convolution modules significantly improves the results as it enables information flow among isolated but interdependent nodes. The effect of mix-hop is evident as well: it validates that the use of mix-hop is helpful for selecting useful information at each information propagation step in the mix-hop propagation layer. The effect of inception is significant in terms of RMSE, but marginal in terms of MAE. This is because using a single 1 Ã— 7 filter has half more parameters than using a combination of 1 Ã— 2, 1 Ã— 3, 1 Ã— 5, 1 Ã— 7 filters under the condition that the number of output channels for the dilated inception layer remains the same. Lastly, our curriculum learning strategy proves to be effective. It enables our model to converge quickly to an optimum that fits for the easiest task, and fine-tune parameters step by step as the level of learning difficulty increases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Study of the Graph Learning Layer</head><p>To validate the effectiveness of our proposed graph learning layer, we conduct a study which experiments with different ways of constructing a graph adjacency matrix. Table <ref type="table" target="#tab_8">5</ref> shows different forms of A with experimental results tested on the validation set of the METR-LA data averaged on 10 runs. Predefined-A is constructed by road network distance <ref type="bibr" target="#b12">[13]</ref>. Global-A assumes the adjacency matrix is a parameter matrix, which contains ğ‘ 2 parameters. Motivated by <ref type="bibr" target="#b20">[21]</ref>, Undirected-A and Directed-A are computed by the similarity scores of node embeddings. Motivated by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>, Dynamic-A assumes the spatial dependency at each time step is dependent on its node inputs. Uni-directed-A is our proposed method. According to Table <ref type="table" target="#tab_8">5</ref>, our proposed uni-directed-A achieves the lowest mean MAE, RMSE, and MAPE. It improves over predefined-A, undirected-A, and dynamic-A significantly. Our uni-directed-A improves over undirected-A and directed-A marginally in terms of MAE and MAPE but proves to be more robust due to a lower RMSE. We further investigate the learned graph adjacency matrix via a case study. In Figure <ref type="figure" target="#fig_8">6a</ref>, we plot the raw time series of node 55 and its pre-defined top-3 neighbors. In Figure <ref type="figure" target="#fig_8">6b</ref>, we chart the raw time series of node 55 and its learned top-3 neighbors. Figure <ref type="figure" target="#fig_8">6c</ref> shows the geo-location of these nodes, with green nodes representing the   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we introduce a novel framework for multivariate time series forecasting. To the best of our knowledge, we are the first to address the multivariate time series forecasting problem via a graph-based deep learning approach. We propose an effective method to exploit the inherent dependency relationships among multiple time series. Our method demonstrates superb performance in a variety of multivariate time series forecasting tasks and opens a new door to use GNNs to handle diverse non-structural data.</p><p>Following <ref type="bibr" target="#b12">[13]</ref>, we split these two datasets into a training set (70%), validation set (20%), and test set (10%) in chronological order. The input sequence length is 12, and the target sequence contains the next 12 future steps. The time of the day is used as an auxiliary feature for the inputs. For the selected baseline methods, the pairwise road network distances are used as the pre-defined graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Experimental Setup</head><p>We repeat the experiment 10 times and report the average value of evaluation metrics. The model is trained by the Adam optimizer with gradient clip 5. The learning rate is 0.001. The l2 regularization penalty is 0.0001. Dropout with 0.3 is applied after each temporal convolution module. Layernorm is applied after each graph convolution module. The depth of the mix-hop propagation layer is set to 2. The retain ratio from the mix-hop propagation layer is set to 0.05. The saturation rate of the activation function from the graph learning layer is set to 3. The dimension of node embeddings is 40.</p><p>Other hyper-parameters are reported according to different tasks.</p><p>A.3.1 Single-step forecasting. We use 5 graph convolution modules and 5 temporal convolution modules with the dilation exponential factor 2. The starting 1 Ã— 1 convolution has 1 input channel and 16 output channels. The graph convolution module and the temporal convolution modules both have 16 output channels. The skip connection layers all have 32 output channels. The first layer of the output module has 64 output channels and the second layer of the output module has 1 output channel. The number of training epochs is 30. For Traffic, Solar-Energy, and Electricity, the number of neighbors for each node is 20. For Exchange-Rate, the number of neighbors for each node is 8. The batch size is set to 4. For the MTGNN+sampling model, we split the nodes of a graph into three partitions randomly with a batch size of 16. Following <ref type="bibr" target="#b11">[12]</ref>, we use RSE and CORR as evaluation metrics.</p><p>A.3.2 Multi-step forecasting. We use 3 graph convolution modules and 3 temporal convolution modules with the dilation exponential factor 1. The starting 1 Ã— 1 convolution has 2 input channels and q q q q q q 2.76 2.78 2.80 2.82 2 4 6</p><p>x MAE Group q METR-LA (a) Number of layers q q q q q q 2.75 (e) Retain ratio q q q q q q 2.76  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Parameter Study</head><p>We conduct a parameter study on eight core hyper-parameters which influence the model complexity of MTGNN. We list these hyper-parameters as follows: Number of layers, the number of temporal convolution modules, ranges from 1 to 6. Number of filters, the number of output channels for temporal convolution modules and graph convolution modules, ranges from 4 to 128. Number of neighbors, the parameter ğ‘˜ in Equation <ref type="formula">5</ref>, ranges from 10 to 60. Saturation rate, the parameter ğ›¼ in Equation 1, 2, and 3, ranges from 0.5 to 5. Retain ratio of mix-hop propagation layer, the parameter ğ›½ in Equation <ref type="formula" target="#formula_7">7</ref>, ranges from 0 to 0.8. Depth of mix-hop propagation layer, the parameter ğ¾ in Equation <ref type="formula" target="#formula_9">8</ref>, ranges from 1 to 6.</p><p>We repeat each experiment 10 times with 50 epochs each time and report the average of MAE with a standard deviation over 10 runs on the validation set. We change the parameter under investigation and fix other parameters in each experiment. Figure <ref type="figure" target="#fig_10">7</ref> shows the experimental results of our parameter study. As shown in Figure <ref type="figure" target="#fig_10">7a</ref> and Figure <ref type="figure" target="#fig_10">7b</ref>, increasing the number of layers and filters enhances our model's expressive capacity, while reducing the MAE loss. Figure <ref type="figure" target="#fig_10">7c</ref> shows that a small number of neighbors gives better results. It is possibly because a node may only depend on a limited number of other nodes, and increasing its neighborhood merely introduces noises to the model. The model performance is not sensitive to the saturation rate, as shown in Figure <ref type="figure" target="#fig_10">7d</ref>. However, a large saturation rate can impose values of the adjacency matrix produced by the graph learning layer approach to 0 or 1. As shown in Figure <ref type="figure" target="#fig_10">7e</ref>, a high retain ratio degrades the model performance significantly. We think it is because by default the propagation depth of the mix-hop propagation layer is set to 2, and as a result, keeping a high proportion of root information constrains a node from exploring its neighborhood. Figure <ref type="figure" target="#fig_10">7f</ref> shows that it is enough to propagate node information with 2 or 3 steps. With the increase of the depth of propagation, the proposed mix-hop propagation layer does not suffer from the over-smoothing problem incurred by information aggregation. With the depth of propagation equal to 6, it has the lowest mean MAE with a larger variation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure2: The framework of MTGNN. A 1 Ã— 1 standard convolution first projects the inputs into a latent space. Afterward, temporal convolution modules and graph convolution modules are interleaved with each other to capture temporal and spatial dependencies respectively. The hyper-parameter, dilation factor ğ‘‘, which controls the receptive field size of a temporal convolution module, is increased at an exponential rate of ğ‘. The graph learning layer learns the hidden graph adjacency matrix, which is used by graph convolution modules. Residual connections and skip connections are added to the model to avoid the problem of gradient vanishing. The output module projects hidden features to the desired dimension to get the final results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b) Mix-hop propagation layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Graph convolution and mix-hop propagation layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The temporal convolution and dilated inception layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>ğ‘“ ( X [:, :, ğ‘–ğ‘‘ (ğ‘‰ ğ‘– ), :]; Î˜) 11: compute ğ¿ = ğ‘™ğ‘œğ‘ ğ‘  ( Å¶ [:, : ğ‘Ÿ, :], Y [:, : ğ‘Ÿ, ğ‘–ğ‘‘ (ğ‘‰ ğ‘– ) ]) 12:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Time series of node 55 and its top-3 neighbors given by the pre-defined A. Node locations of node 55 and its neighbors marked on Google Maps. Yellow nodes represent node 55's top-3 neighbors given by the pre-defined A. Green nodes represent node 55's top-3 neighbors given by the learned A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Case study</figDesc><graphic coords="10,60.43,190.05,226.97,86.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Parameter Study</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>â€¢</head><label></label><figDesc>To the best of our knowledge, this is the first study on multivariate time series data generally from a graph-based perspective with graph neural networks. â€¢ We propose a novel graph learning module to learn hidden spatial dependencies among variables. Our method opens a new door for GNN models to handle data without explicit graph structure.</figDesc><table /><note><p><p>â€¢ We present a joint framework for modeling multivariate time series data and learning graph structures. Our framework is more generic than any existing spatial-temporal graph neural network as it can handle multivariate time series with or without a pre-defined graph structure.</p>â€¢ Experimental results show that our method outperforms the state-of-the-art methods on 3 of 4 benchmark datasets and achieves on-par performance with other GNNs on two traffic datasets which provide extra structural information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>In this paper, we focus on the task of multivariate time series forecasting. Let z ğ‘¡ âˆˆ R ğ‘ denote the value of a multivariate variable of dimension ğ‘ at time step t, where ğ‘§ ğ‘¡ [ğ‘–] âˆˆ ğ‘… denote the value of the ğ‘– ğ‘¡â„ variable at time step t. Given a sequence of historical ğ‘ƒ time steps of observations on a multivariate variable,X = {z ğ‘¡ 1 , z ğ‘¡ 2 , â€¢ â€¢ â€¢ , z ğ‘¡ ğ‘ƒ }, our goal is to predict the ğ‘„-step-away value of Y = {z ğ‘¡ ğ‘ƒ +ğ‘„ }, or a sequence of future values Y = {z ğ‘¡ ğ‘ƒ +1 , z ğ‘¡ ğ‘ƒ +2 , â€¢ â€¢ â€¢ , z ğ‘¡ ğ‘ƒ +ğ‘„ }.More generally, the input signals can be coupled with other auxiliary features such as time of the day, day of the week, and day of the season. Concatenating the input signals with auxiliary features, we assume the inputs instead are X = {S ğ‘¡ 1 , S ğ‘¡ 2 , â€¢ â€¢ â€¢ , S ğ‘¡ ğ‘ƒ } where S ğ‘¡ ğ‘– âˆˆ R ğ‘ Ã—ğ· , ğ· is the feature dimension, the first column of S ğ‘¡ ğ‘– equals to z ğ‘¡ ğ‘– , and the rest are auxiliary features. We aim to build a mapping ğ‘“ (â€¢) from X to Y by minimizing the absolute loss with ğ‘™2 regularization.Graphs describe the relationships among entities in a network. We give a formal definition of graph-related concepts below.Definition 3.1 (Graph).A graph is formulated as ğº = (ğ‘‰ , ğ¸) where ğ‘‰ is the set of nodes, and ğ¸ is the set of edges. We use ğ‘ to denote the number of nodes in a graph. Let ğ‘£ âˆˆ ğ‘‰ to denote a node and ğ‘’ = (ğ‘£, ğ‘¢) âˆˆ ğ¸ to denote an edge pointing from ğ‘¢ to ğ‘£. The neighborhood of a node ğ‘£ is defined as ğ‘ (ğ‘£) = {ğ‘¢ âˆˆ ğ‘‰ |(ğ‘£, ğ‘¢) âˆˆ ğ¸}. The adjacency matrix is a mathematical representation of a graph, denoted as A âˆˆ ğ‘… ğ‘ Ã—ğ‘ with ğ´ ğ‘– ğ‘— = ğ‘ &gt; 0 if (ğ‘£ ğ‘– , ğ‘£ ğ‘— ) âˆˆ ğ¸ and ğ´ ğ‘– ğ‘— = 0 if (ğ‘£ ğ‘– , ğ‘£ ğ‘— ) âˆ‰ ğ¸.</figDesc><table><row><cell>Definition 3.2 (Node Neighborhood). Definition 3.3 (Adjacency Matrix).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The dataset ğ‘‚, node set ğ‘‰ , the initialized MTGNN model ğ‘“ ( â€¢) with Î˜, learning rate ğ›¾ , batch size ğ‘, step size ğ‘ , split size ğ‘š (default=1). 2: set ğ‘–ğ‘¡ğ‘’ğ‘Ÿ = 1, ğ‘Ÿ = 1 3: repeat ğ‘… ğ‘Ã—ğ‘‡ Ã—ğ‘ Ã—ğ· , Y âˆˆ ğ‘… ğ‘Ã—ğ‘‡ â€² Ã—ğ‘ ) from ğ‘‚.</figDesc><table><row><cell cols="2">4: sample a batch ( X âˆˆ 5: random split the node set ğ‘‰ into ğ‘š groups, âˆª ğ‘š ğ‘–=1 ğ‘‰ ğ‘– = ğ‘‰ .</cell></row><row><cell>7:</cell><cell>ğ‘Ÿ = ğ‘Ÿ + 1</cell></row><row><cell>8:</cell><cell>end if</cell></row><row><cell>9:</cell><cell></cell></row><row><cell>propose a learning algorithm to enhance our model's capability</cell><cell></cell></row><row><cell>of handling large graphs and stabilizing in a better local optimum.</cell><cell></cell></row><row><cell>Training on a graph often requires storing all node intermediate</cell><cell></cell></row><row><cell>states into memory. If a graph is large, it will face the problem of</cell><cell></cell></row><row><cell>memory overflow. Most relevant to us, Chiang et al. [4] propose a</cell><cell></cell></row><row><cell>sub-graph training algorithm to tackle the memory bottleneck. They</cell><cell></cell></row><row><cell>apply a graph clustering algorithm to partition a graph into sub-</cell><cell></cell></row><row><cell>graphs and train a graph convolutional network on the partitioned</cell><cell></cell></row><row><cell>sub-graphs. In our problem, it is not practical to cluster nodes based</cell><cell></cell></row><row><cell>on their topological information because our model learns the latent</cell><cell></cell></row><row><cell>graph structure at the same time. Alternatively, in each iteration, we</cell><cell></cell></row><row><cell>randomly split the nodes into several groups and let the algorithm</cell><cell></cell></row><row><cell>learn a sub-graph structure based on the sampled nodes. This gives</cell><cell></cell></row><row><cell>each node the full possibilities of being assigned with another node</cell><cell></cell></row><row><cell>in one group so that the similarity score between these two nodes</cell><cell></cell></row><row><cell>can be computed and updated. As a side benefit, if we split the nodes</cell><cell></cell></row></table><note><p><p>into ğ‘  groups, we can reduce the time and space complexity of our graph learning layer from ğ‘‚ (ğ‘ 2 ) to (ğ‘ /ğ‘ ) 2 in each iteration. After training, as all node embeddings are well-trained, a global graph can be constructed to fully utilize spatial relationships. Although Algorithm 1 The learning algorithm of MTGNN. 1: Input: 6:</p>if ğ‘–ğ‘¡ğ‘’ğ‘Ÿ %ğ‘  == 0 and ğ‘Ÿ &lt;= ğ‘‡ â€² then</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Datasets</cell><cell cols="5"># Samples # Nodes Sample Rate Input Length Output Length</cell></row><row><cell>traffic</cell><cell>17,544</cell><cell>862</cell><cell>1 hour</cell><cell>168</cell><cell>1</cell></row><row><cell>solar-energy</cell><cell>52,560</cell><cell>137</cell><cell>10 minutes</cell><cell>168</cell><cell>1</cell></row><row><cell>electricity</cell><cell>26,304</cell><cell>321</cell><cell>1 hour</cell><cell>168</cell><cell>1</cell></row><row><cell>exchange-rate</cell><cell>7,588</cell><cell>8</cell><cell>1 day</cell><cell>168</cell><cell>1</cell></row><row><cell>metr-la</cell><cell>34272</cell><cell>207</cell><cell>5 minutes</cell><cell>12</cell><cell>12</cell></row><row><cell>pems-bay</cell><cell>52116</cell><cell>325</cell><cell>5 minutes</cell><cell>12</cell><cell>12</cell></row><row><cell cols="6">Squared Error (RMSE), Mean Absolute Percentage Error (MAPE),</cell></row><row><cell cols="6">Root Relative Squared Error (RRSE), and Empirical Correlation Co-</cell></row><row><cell cols="6">efficient (CORR). For RMSE, MAE, MAPE, and RRSE, lower values</cell></row><row><cell cols="6">are better. For CORR, higher values are better. Other experimental</cell></row><row><cell cols="4">setups are given in Appendix A.3.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Baseline comparison under single-step forecasting for multivariate time series methods.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell cols="2">Solar-Energy</cell><cell></cell><cell></cell><cell>Traffic</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Electricity</cell><cell></cell><cell></cell><cell cols="2">Exchange-Rate</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Horizon</cell><cell></cell><cell></cell><cell cols="2">Horizon</cell><cell></cell><cell></cell><cell></cell><cell>Horizon</cell><cell></cell><cell></cell><cell></cell><cell>Horizon</cell><cell></cell></row><row><cell>Methods</cell><cell>Metrics</cell><cell>3</cell><cell>6</cell><cell>12</cell><cell>24</cell><cell>3</cell><cell>6</cell><cell>12</cell><cell>24</cell><cell>3</cell><cell>6</cell><cell>12</cell><cell>24</cell><cell>3</cell><cell>6</cell><cell>12</cell><cell>24</cell></row><row><cell>AR</cell><cell>RSE</cell><cell cols="7">0.2435 0.3790 0.5911 0.8699 0.5991 0.6218 0.6252</cell><cell>0.63</cell><cell cols="8">0.0995 0.1035 0.1050 0.1054 0.0228 0.0279 0.0353 0.0445</cell></row><row><cell></cell><cell>CORR</cell><cell cols="16">0.9710 0.9263 0.8107 0.5314 0.7752 0.7568 0.7544 0.7519 0.8845 0.8632 0.8591 0.8595 0.9734 0.9656 0.9526 0.9357</cell></row><row><cell>VARMLP</cell><cell>RSE</cell><cell cols="16">0.1922 0.2679 0.4244 0.6841 0.5582 0.6579 0.6023 0.6146 0.1393 0.1620 0.1557 0.1274 0.0265 0.0394 0.0407 0.0578</cell></row><row><cell></cell><cell>CORR</cell><cell cols="16">0.9829 0.9655 0.9058 0.7149 0.8245 0.7695 0.7929 0.7891 0.8708 0.8389 0.8192 0.8679 0.8609 0.8725 0.8280 0.7675</cell></row><row><cell>GP</cell><cell>RSE</cell><cell cols="16">0.2259 0.3286 0.5200 0.7973 0.6082 0.6772 0.6406 0.5995 0.1500 0.1907 0.1621 0.1273 0.0239 0.0272 0.0394 0.0580</cell></row><row><cell></cell><cell>CORR</cell><cell cols="16">0.9751 0.9448 0.8518 0.5971 0.7831 0.7406 0.7671 0.7909 0.8670 0.8334 0.8394 0.8818 0.8713 0.8193 0.8484 0.8278</cell></row><row><cell>RNN-GRU</cell><cell>RSE</cell><cell cols="16">0.1932 0.2628 0.4163 0.4852 0.5358 0.5522 0.5562 0.5633 0.1102 0.1144 0.1183 0.1295 0.0192 0.0264 0.0408 0.0626</cell></row><row><cell></cell><cell>CORR</cell><cell cols="16">0.9823 0.9675 0.9150 0.8823 0.8511 0.8405 0.8345 0.8300 0.8597 0.8623 0.8472 0.8651 0.9786 0.9712 0.9531 0.9223</cell></row><row><cell>LSTNet-skip</cell><cell>RSE</cell><cell cols="16">0.1843 0.2559 0.3254 0.4643 0.4777 0.4893 0.4950 0.4973 0.0864 0.0931 0.1007 0.1007 0.0226 0.0280 0.0356 0.0449</cell></row><row><cell></cell><cell>CORR</cell><cell cols="16">0.9843 0.9690 0.9467 0.8870 0.8721 0.8690 0.8614 0.8588 0.9283 0.9135 0.9077 0.9119 0.9735 0.9658 0.9511 0.9354</cell></row><row><cell>TPA-LSTM</cell><cell>RSE</cell><cell cols="16">0.1803 0.2347 0.3234 0.4389 0.4487 0.4658 0.4641 0.4765 0.0823 0.0916 0.0964 0.1006 0.0174 0.0241 0.0341 0.0444</cell></row><row><cell></cell><cell>CORR</cell><cell cols="16">0.9850 0.9742 0.9487 0.9081 0.8812 0.8717 0.8717 0.8629 0.9439 0.9337 0.9250 0.9133 0.9790 0.9709 0.9564 0.9381</cell></row><row><cell>MTGNN</cell><cell>RSE</cell><cell cols="16">0.1778 0.2348 0.3109 0.4270 0.4162 0.4754 0.4461 0.4535 0.0745 0.0878 0.0916 0.0953 0.0194 0.0259 0.0349 0.0456</cell></row><row><cell></cell><cell>CORR</cell><cell cols="16">0.9852 0.9726 0.9509 0.9031 0.8963 0.8667 0.8794 0.8810 0.9474 0.9316 0.9278 0.9234 0.9786 0.9708 0.9551 0.9372</cell></row><row><cell cols="2">MTGNN+sampling RSE</cell><cell cols="16">0.1875 0.2521 0.3347 0.4386 0.4170 0.4435 0.4469 0.4537 0.0762 0.0862 0.0938 0.0976 0.0212 0.0271 0.0350 0.0454</cell></row><row><cell></cell><cell>CORR</cell><cell cols="16">0.9834 0.9687 0.9440 0.8990 0.8960 0.8815 0.8793 0.8758 0.9467 0.9354 0.9261 0.9219 0.9788 0.9704 0.9574 0.9382</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Baseline comparison under multi-step forecasting for spatial-temporal graph neural networks.</figDesc><table><row><cell></cell><cell></cell><cell>Horizon 3</cell><cell>Horizon 6</cell><cell cols="2">Horizon 12</cell></row><row><cell></cell><cell cols="4">MAE RMSE MAPE MAE RMSE MAPE MAE RMSE</cell><cell>MAPE</cell></row><row><cell>METR-LA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DCRNN</cell><cell>2.77</cell><cell>5.38 7.30% 3.15</cell><cell>6.45 8.80% 3.60</cell><cell cols="2">7.60 10.50%</cell></row><row><cell>STGCN</cell><cell>2.88</cell><cell>5.74 7.62% 3.47</cell><cell>7.24 9.57% 4.59</cell><cell cols="2">9.40 12.70%</cell></row><row><cell>Graph WaveNet</cell><cell>2.69</cell><cell>5.15 6.90% 3.07</cell><cell>6.22 8.37% 3.53</cell><cell cols="2">7.37 10.01%</cell></row><row><cell>ST-MetaNet</cell><cell>2.69</cell><cell>5.17 6.91% 3.10</cell><cell>6.28 8.57% 3.59</cell><cell cols="2">7.52 10.63%</cell></row><row><cell>MRA-BGCN</cell><cell>2.67</cell><cell>5.12 6.80% 3.06</cell><cell>6.17 8.30% 3.49</cell><cell cols="2">7.30 10.00%</cell></row><row><cell>GMAN</cell><cell>2.77</cell><cell>5.48 7.25% 3.07</cell><cell>6.34 8.35% 3.40</cell><cell cols="2">7.21 9.72%</cell></row><row><cell>MTGNN</cell><cell>2.69</cell><cell>5.18 6.86% 3.05</cell><cell>6.17 8.19% 3.49</cell><cell>7.23</cell><cell>9.87%</cell></row><row><cell cols="2">MTGNN+sampling 2.76</cell><cell>5.34 5.18% 3.11</cell><cell>6.32 8.47% 3.54</cell><cell cols="2">7.38 10.05%</cell></row><row><cell>PEMS-BAY</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DCRNN</cell><cell>1.38</cell><cell>2.95 2.90% 1.74</cell><cell>3.97 3.90% 2.07</cell><cell>4.74</cell><cell>4.90%</cell></row><row><cell>STGCN</cell><cell>1.36</cell><cell>2.96 2.90% 1.81</cell><cell>4.27 4.17% 2.49</cell><cell>5.69</cell><cell>5.79%</cell></row><row><cell>Graph WaveNet</cell><cell>1.30</cell><cell>2.74 2.73% 1.63</cell><cell>3.70 3.67% 1.95</cell><cell>4.52</cell><cell>4.63%</cell></row><row><cell>ST-MetaNet</cell><cell>1.36</cell><cell>2.90 2.82% 1.76</cell><cell>4.02 4.00% 2.20</cell><cell>5.06</cell><cell>5.45%</cell></row><row><cell>MRA-BGCN</cell><cell>1.29</cell><cell>2.72 2.90% 1.61</cell><cell>3.67 3.80% 1.91</cell><cell>4.46</cell><cell>4.60%</cell></row><row><cell>GMAN</cell><cell>1.34</cell><cell>2.82 2.81% 1.62</cell><cell>3.72 3.63% 1.86</cell><cell cols="2">4.32 4.31%</cell></row><row><cell>MTGNN</cell><cell>1.32</cell><cell>2.79 2.77% 1.65</cell><cell>3.74 3.69% 1.94</cell><cell>4.49</cell><cell>4.53%</cell></row><row><cell cols="2">MTGNN+sampling 1.34</cell><cell>2.83 2.83% 1.67</cell><cell>3.79 3.78% 1.95</cell><cell>4.49</cell><cell>4.62%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation study.</figDesc><table><row><cell cols="2">Methods MTGNN</cell><cell>w/o GC</cell><cell>w/o Mix-hop</cell><cell>w/o Inception w/o CL</cell></row><row><cell>MAE</cell><cell cols="4">2.7715Â±0.0119 2.8953Â±0.0054 2.7975Â±0.0089 2.7772Â±0.0100 2.7828Â±0.0105</cell></row><row><cell>RMSE</cell><cell cols="4">5.8070Â±0.0512 6.1276Â±0.0339 5.8549Â±0.0474 5.8251Â±0.0429 5.8248Â±0.0366</cell></row><row><cell>MAPE</cell><cell cols="4">0.0778Â±0.0009 0.0831Â±0.0009 0.0779Â±0.0009 0.0778Â±0.0010 0.0784Â±0.0009</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparison of different graph learning methods. We observe that the central node's pre-defined top-3 neighbors are much closer to the node itself on the map. As a result, their time series are more correlated simultaneously, as shown by the red circles in Figure6a. On the contrary, the central node's learned top-3 neighbors distribute further away from it but still lie on the same road it follows. According to Figure6b, time series of the learned top-3 neighbors are more capable of indicating extreme traffic conditions of the central node in advance.</figDesc><table><row><cell>Methods</cell><cell>Equation</cell><cell>MAE</cell><cell>RMSE</cell><cell>MAPE</cell></row><row><cell>Pre-defined-A</cell><cell>-</cell><cell>2.9017Â±0.0078</cell><cell>6.1288Â±0.0345</cell><cell>0.0836Â±0.0009</cell></row><row><cell>Global-A</cell><cell>A = ğ‘…ğ‘’ğ¿ğ‘ˆ (W)</cell><cell>2.8457Â±0.0107</cell><cell>5.9900Â±0.0390</cell><cell>0.0805Â±0.0009</cell></row><row><cell>Undirected-A</cell><cell>A = ğ‘…ğ‘’ğ¿ğ‘ˆ (ğ‘¡ğ‘ğ‘›â„(ğ›¼ (M1M ğ‘‡ 1 )))</cell><cell>2.7736Â±0.0185</cell><cell>5.8411Â±0.0523</cell><cell>0.0783Â±0.0012</cell></row><row><cell>Directed-A</cell><cell>A = ğ‘…ğ‘’ğ¿ğ‘ˆ (ğ‘¡ğ‘ğ‘›â„(ğ›¼ (M1M ğ‘‡ 2 )))</cell><cell>2.7758Â±0.0088</cell><cell>5.8217Â±0.0451</cell><cell>0.0783Â±0.0006</cell></row><row><cell>Dynamic-A</cell><cell cols="2">Ağ‘¡ = ğ‘†ğ‘œ ğ‘“ ğ‘¡ğ‘€ğ‘ğ‘¥ (ğ‘¡ğ‘ğ‘›â„(Xğ‘¡ W1)ğ‘¡ğ‘ğ‘›â„(W ğ‘‡ 2 X ğ‘‡ ğ‘¡ )) 2.8124Â±0.0102</cell><cell>5.9189Â±0.0281</cell><cell>0.0794Â±0.0008</cell></row><row><cell cols="2">Uni-directed-A (ours) A = ğ‘…ğ‘’ğ¿ğ‘ˆ (ğ‘¡ğ‘ğ‘›â„(ğ›¼ (M1M ğ‘‡ 2 -M2M 1 )))</cell><cell cols="3">2.7715Â±0.0119 5.8070Â±0.0512 0.0778Â±0.0009</cell></row><row><cell cols="5">central node's learned top-3 neighbors and yellow nodes represent-</cell></row><row><cell cols="4">ing the central node's pre-defined top-3 neighbors.</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the <rs type="funder">Australian Research Council (ARC)</rs> under Grant <rs type="grantNumber">LP160100630</rs>, <rs type="grantNumber">LP180100654</rs> and <rs type="grantNumber">DE190100626</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tHvUNzn">
					<idno type="grant-number">LP160100630</idno>
				</org>
				<org type="funding" xml:id="_8n8vhCF">
					<idno type="grant-number">LP180100654</idno>
				</org>
				<org type="funding" xml:id="_D94N4cn">
					<idno type="grant-number">DE190100626</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX: REPRODUCIBILITY</head><p>In this section, we provide the details of our implementation for reproducibility. Our source codes 1 are publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Complexity Analysis</head><p>We analyze the time complexity of the main components of the proposed model MTGNN, which is summarized in Table <ref type="table">6</ref>. The time complexity of the graph learning layer is</p><p>where ğ‘ denotes the number of nodes, ğ‘  1 represents the dimension of node input feature vectors, and ğ‘  2 represents the dimension of node hidden feature vectors. Treating ğ‘  1 and ğ‘  2 as constants, the time complexity of the graph learning layer becomes ğ‘‚ (ğ‘ 2 ). It is attributed to the pairwise computation of node hidden feature vectors. The graph convolution module incurs ğ‘‚ (ğ¾ (ğ‘€ğ‘‘ 1 + ğ‘ğ‘‘ 1 ğ‘‘ 2 ) time complexity, where ğ¾ is the propagation depth, ğ‘ is the number of nodes, ğ‘‘ 1 denotes the input dimension of node states, ğ‘‘ 2 denotes the output dimension of node states. Regarding ğ¾, ğ‘‘ 1 and ğ‘‘ 2 as constants, the time complexity of the graph convolution module turns to ğ‘‚ (ğ‘€). This result comes from the fact that in the information propagation step, each node receives information its neighbors and the sum of the number of neighbors of each node exactly equals the number of edges. The time complexity of the temporal convolution module equals to ğ‘‚ (ğ‘ğ‘™ğ‘ ğ‘– ğ‘ ğ‘œ /ğ‘‘), where ğ‘™ is the input sequence length, ğ‘ ğ‘– is the number of input channels, ğ‘ ğ‘œ is the number of output channels, and ğ‘‘ is the dilation factor. The time complexity of the temporal convolution module mainly depends on ğ‘ Ã— ğ‘™, which is the size of the input feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Components Time Complexity</head><p>Graph Learning Layer </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><surname>George Ep Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gwilym</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">C</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greta</forename><forename type="middle">M</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName><surname>Ljung</surname></persName>
		</author>
		<title level="m">Time series analysis: forecasting and control</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DAGCN: Dual Attention Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCNN</title>
		<meeting>of IJCNN</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-Range Attentive Bicomponent Graph Convolutional Network for Traffic Forecasting</title>
		<author>
			<persName><forename type="first">Weiqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD</title>
		<meeting>of KDD</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bayesian time series learning with Gaussian processes</title>
		<author>
			<persName><forename type="first">Roger</forename><surname>Frigola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bayesian time series learning with Gaussian processes</title>
		<author>
			<persName><forename type="first">Roger</forename><surname>Frigola-Alcalde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention based spatial-temporal graph convolutional networks for traffic flow forecasting</title>
		<author>
			<persName><forename type="first">Shengnan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youfang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaiyu</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="922" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing</title>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>GÃ¼nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling long-and short-term temporal patterns with deep neural networks</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</title>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Urban Traffic Prediction from Spatio-Temporal Data Using Deep Meta Learning</title>
		<author>
			<persName><forename type="first">Zheyi</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="1720" to="1730" />
			<date type="published" when="2019">2019</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gaussian processes for time-series modelling</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Ebden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Reece</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neale</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzanne</forename><surname>Aigrain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos. Trans. R. Soc. A</title>
		<imprint>
			<biblScope unit="volume">371</biblScope>
			<date type="published" when="1984">2013. 1984. 2013. 20110550</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName><forename type="first">Youngjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">MichaÃ«l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="362" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporal pattern attention for multivariate time series forecasting</title>
		<author>
			<persName><forename type="first">Shun-Yao</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan-Keng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="1421" to="1441" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph WaveNet for Deep Spatial-Temporal Graph Modeling</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3482" to="3489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoteng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3634" to="3640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Time series forecasting using a hybrid ARIMA and neural network model</title>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="159" to="175" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GMAN: A Graph Multi-Attention Network for Traffic Prediction</title>
		<author>
			<persName><forename type="first">Chuanpan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
