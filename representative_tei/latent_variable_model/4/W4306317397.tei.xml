<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Diverse Chemical Reactions for Single-step Retrosynthesis via Discrete Latent Variables</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-08-10">10 Aug 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Huarui</forename><surname>He</surname></persName>
							<email>huaruihe@mail.ustc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
							<email>jiewangx@ustc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yunfei</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Wu</surname></persName>
							<email>fengwu@ustc.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CAS Key Laboratory of Technology</orgName>
								<orgName type="institution" key="instit2">GIPAS</orgName>
								<orgName type="institution" key="instit3">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">CAS Key Laboratory of Technology</orgName>
								<orgName type="institution" key="instit2">GIPAS</orgName>
								<orgName type="institution" key="instit3">University of Science and Technology of China</orgName>
								<orgName type="institution" key="instit4">Hefei Comprehensive National Science Center</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">CAS Key Laboratory of Technology</orgName>
								<orgName type="institution" key="instit2">GIPAS</orgName>
								<orgName type="institution" key="instit3">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">CAS Key Laboratory of Technology</orgName>
								<orgName type="institution" key="instit2">GIPAS</orgName>
								<orgName type="institution" key="instit3">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Diverse Chemical Reactions for Single-step Retrosynthesis via Discrete Latent Variables</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-08-10">10 Aug 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2208.05482v1[q-bio.QM]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>â€¢ Applied computing â†’ Computational biology</term>
					<term>Chemistry</term>
					<term>â€¢ Computing methodologies â†’ Natural language processing</term>
					<term>Latent variable models Retrosynthesis, Variational Autoencoder, Transformer, Graph Neural Network, Discrete Latent Variable</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single-step retrosynthesis is the cornerstone of retrosynthesis planning, which is a crucial task for computer-aided drug discovery. The goal of single-step retrosynthesis is to identify the possible reactants that lead to the synthesis of the target product in one reaction. By representing organic molecules as canonical strings, existing sequence-based retrosynthetic methods treat the productto-reactant retrosynthesis as a sequence-to-sequence translation problem. However, most of them struggle to identify diverse chemical reactions for a desired product due to the deterministic inference, which contradicts the fact that many compounds can be synthesized through various reaction types with different sets of reactants.</p><p>In this work, we aim to increase reaction diversity and generate various reactants using discrete latent variables. We propose a novel sequence-based approach, namely RetroDCVAE, which incorporates conditional variational autoencoders into single-step retrosynthesis and associates discrete latent variables with the generation process. Specifically, RetroDCVAE uses the Gumbel-Softmax distribution to approximate the categorical distribution over potential reactions and generates multiple sets of reactants with the variational decoder. Experiments demonstrate that RetroDCVAE outperforms state-of-the-art baselines on both benchmark dataset and homemade dataset. Both quantitative and qualitative results show that RetroDCVAE can model the multi-modal distribution over reaction types and produce diverse reactant candidates.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As a fundamental problem in organic chemistry, retrosynthesis planning refers to the technique to iteratively deconstruct a compound into intermediates or simpler precursors, until a set of commercially available reactants is reached. Since we can solve retrosynthesis planning by searching backwards and recursively applying singlestep retrosynthesis to unavailable molecules, this work focuses on single-step retrosynthesis whose goal is to generate a set of reactants that leads to the one-step synthesis of the desired product. In recent years, we have witnessed the great achievement of computer-aided synthesis planning (CASP) <ref type="bibr" target="#b5">[6]</ref> in many real-world applications, such as drug design <ref type="bibr" target="#b23">[24]</ref>, environmental protection <ref type="bibr" target="#b41">[42]</ref>, as well as materials science <ref type="bibr" target="#b50">[51]</ref>.</p><p>Given that the simplified molecular-input line-entry system (SMILES) <ref type="bibr" target="#b45">[46]</ref> can represent arbitrary molecules as strings, researchers formulate the single-step retrosynthesis task as a machine translation problem <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b52">53]</ref>. Therefore, sequence-based approaches in natural language processing <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b43">44]</ref> lead to a simple end-to-end formulation that exempts retrosynthesis from reaction templates and domain knowledge. As the extract of high-quality reaction templates requires considerable experience and expertise, the sequence-based retrosynthesis has attracted increasing attention and shown promise as a prevailing template-free approach. However, existing work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b52">53]</ref> that adapts sequence-tosequence models to retrosynthesis struggles to model the multimodal distribution over reaction types. For example, Figure <ref type="figure" target="#fig_0">1</ref> illustrates that Benzophenone can be synthesized in one step by the Reduction reaction or by the C-C bond formation reaction. As existing template-free retrosynthetic models mainly perform deterministic inference, once we finish training and fix the parameters, the retrosynthetic models tend to generate reactants in one direction. That is, existing template-free models may always output only one possible synthesis scheme due to the deterministic inference.</p><p>In this paper, we propose a novel template-free model, namely RetroDCVAE, which incorporates stochastic inference into singlestep retrosynthesis. Motivated by the fact that a large number of compounds have multiple synthetic schemes, we implicitly model reaction types with categorical latent variables. However, neural networks with discrete latent variables are challenging to train due to the inability to backpropagate through samples. Built upon the Transformer architecture <ref type="bibr" target="#b43">[44]</ref>, RetroDCVAE incorporates conditional variational autoencoders (CVAE) <ref type="bibr" target="#b35">[36]</ref> into the Transformer decoder part and uses the Gumbel-Softmax estimator <ref type="bibr" target="#b14">[15]</ref> to support backpropagation of categorical latent variables. To gain insights into the working behaviors of RetroDCVAE, we conduct analysis on a homemade dataset, namely USPTO-DIVERSE, where each target compound owns at least two sets of reactants. Both quantitative and qualitative results show that RetroDCVAE is able to generate diverse reactants for a target product. Furthermore, experiments on the public dataset demonstrate that RetroDCVAE achieves competitive results against state-of-the-art baselines.</p><p>The contributions of this work are threefold: (1) To the best of our knowledge, we are the first to model the multi-modal distribution in single-step retrosynthesis using discrete CVAE. Moreover, we leverage Gumbel-Softmax <ref type="bibr" target="#b14">[15]</ref> to optimize the discrete CVAE for the first time.</p><p>(2) To assess the capacity of modeling the multi-modal distribution over reaction types, we extract all reactions with 1-to-N (ğ‘ â‰¥ 2) products in USPTO-MIT <ref type="bibr" target="#b17">[18]</ref> to build a new dataset named USPTO-DIVERSE. That is, each product in USPTO-DIVERSE corresponds to two or more reactions. We anticipate that USPTO-DIVERSE would exert potential impacts on community.</p><p>(3) We conduct both quantitative and qualitative analysis to show the effectiveness of the proposed RetroDCVAE. Extensive experiments demonstrate that RetroDCVAE outperforms state-of-the-art template-free baselines on the benchmark dataset USPTO-50k and the homemade dataset USPTO-DIVERSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>In this section, we first formulate the single-step retrosynthetic problem and then introduce basic techniques for multi-modal distribution modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Statement</head><p>In this part, we define the single-step retrosynthetic task and present the notations throughout this paper.</p><p>Let M denote the space of all molecules and R denote the space of all chemical reaction types. The single-step retrosynthetic model takes a target molecule ğ‘¡ âˆˆ M as input, and predicts a set of precursor source reactants S âŠ‚ M leading to synthesizing ğ‘¡. Therefore, single-step retrosynthesis is the reverse problem of reaction outcome prediction, whose goal is to predict the major product given the set of reactants. Admittedly, single-step retrosynthesis is more challenging than its reverse problem due to the multi-modal distribution over reaction types for a given product. Formally, a single-step retrosynthetic model aims to find a function ğ¹ as</p><formula xml:id="formula_0">ğ¹ (â€¢) : ğ‘¡ â†’ {S ğ‘– } ğ‘ ğ‘–=1 ,</formula><p>which outputs at most ğ‘ sets of reactants. We expect that ğ¹ ranks ground truth or plausible reactants as high as possible. The singlestep retrosynthetic model can be learned from the collection of chemical reactions D train = {S ğ‘– , ğ‘¡ ğ‘– } in patents granted by United States Patent Office (USPTO). From a sequence-based perspective, we use SMILES to represent the target product ğ‘¡ as a string. For example, we represent Benzophenone as O=C(C1C=CC=CC=1)C1C=CC=CC=1. Existing sequencebased retrosynthetic models employ the encoder-decoder architecture. The encoder embeds each token of ğ‘¡ into the continuous vector space and drives a sequence of embeddings as x = {x 1 , â€¢ â€¢ â€¢ , x ğ‘‡ ğ‘– } with ğ‘‡ ğ‘– representing the length of the input SMILES string. After that, instead of directly performing deterministic inference and decoding x, we aim to implicitly model possible reaction types from x and then perform stochastic inference. Given x, the multi-modal distribution over reaction types is P(z|x). During the inference phase, we sample a possible reaction type z âˆˆ R following the distribution z âˆ¼ P(z|x). According to {x, z}, the auto-regressive decoder generates a sequence of chemical symbol predictions y = {y 1 , â€¢ â€¢ â€¢ , y ğ‘‡ ğ‘œ } with ğ‘‡ ğ‘œ representing the length of output tokens. Finally, beam search is applied to derive the string representation of reactants. For example, if the sampled z âˆˆ R means C-C bond formation reaction, then the retrosynthetic model aims to synthesize Benzophenone through C-C bond formation reaction and would output C1=CC=CC=C1.C1=CC=CC(C(Cl)=O)=C1 as the final prediction, where ". " is the delimiter of two molecules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Technical Background</head><p>CVAE allows us to tackle problems where the input-to-output mapping is one-to-many, while reducing the need to explicitly specify the structure of the output distribution. CVAE <ref type="bibr" target="#b35">[36]</ref> aims to model the underlying conditional distribution ğ‘ (y|x), i.e., the distribution over output variable y, conditioned on the observed variable x. Given observation x, the latent variable z is drawn from the prior distribution ğ‘ (z|x), and the output y is generated from the distribution ğ‘ (y|x, z). By assuming z follows multivariate Gaussian distribution with a diagonal co-variance matrix, we have</p><formula xml:id="formula_1">ğ‘ (y|x) = âˆ« z ğ‘ (y|x, z)ğ‘ (z|x)ğ‘‘z.<label>(1)</label></formula><p>The typical CVAE consists of a prior network ğ‘ ğœƒ (z|x) parameterized by ğœƒ and a recognition network ğ‘ ğœ™ (z|x, y) parameterized by ğœ™, which are used to approximate the prior distribution ğ‘ (z|x) and the posterior distribution ğ‘ (z|x, y), respectively. The evidence lower bound (ELBO) is</p><formula xml:id="formula_2">L ELBO = L REC -L KL = E ğ‘ ğœ™ (z|x,y) [log ğ‘ ğœƒ (y|x, z)] -ğ· KL (ğ‘ ğœ™ (z|x, y)âˆ¥ğ‘ ğœƒ (z|x)) â‰¤ log ğ‘ ğœƒ (y|x),<label>(2)</label></formula><p>where L REC is the negative reconstruction error and L KL denotes the Kullback-Leibler (KL) divergence between the posterior and prior. Therefore, to maximize the ELBO w.r.t. parameters ğœƒ and ğœ™ will concurrently maximize the likelihood ğ‘ (y|x) and minimize the KL divergence. Gumbel-Softmax is a continuous distribution that can approximate categorical samples. We can easily compute parameter gradients of Gumbel-Softmax via the reparameterization trick <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Let z be a categorical variable with class probabilities ğœ‹ 1 , ğœ‹  according to Gumbel(0, 1). In practice, the Gumbel(0, 1) distribution can be derived using inverse transform sampling by drawing ğ‘¢ âˆ¼ Uniform(0, 1) and computing ğ‘” =log(-log ğ‘¢). However, the arg max formulation makes it hard to backpropagate through samples. Jang et al. <ref type="bibr" target="#b14">[15]</ref> use the softmax function as a continuous and differentiable approximation to arg max, i.e.,</p><formula xml:id="formula_3">z ğ‘– = exp((log ğœ‹ ğ‘– + ğ‘” ğ‘– )/ğœ) ğ‘˜ ğ‘—=1 exp((log ğœ‹ ğ‘— + ğ‘” ğ‘— )/ğœ) , for ğ‘– âˆˆ [ğ‘˜].</formula><p>As the softmax temperature ğœ approaches 0, Jang et al. <ref type="bibr" target="#b14">[15]</ref> point out that samples from the Gumbel-Softmax distribution become one-hot and the Gumbel-Softmax distribution becomes identical to the categorical distribution ğ‘ (z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>The proposed discrete conditional stochastic inference mechanism is widely applicable to existing sequence-based models. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, we follow the encoder-decoder architecture and incorporate the stochastic inference mechanism into the decoder part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked Encoder</head><note type="other">Cross</note><p>As for the encoder part, we perform directed message passing and follow a state-of-the-art template-free baseline <ref type="bibr" target="#b42">[43]</ref> to convert the input product SMILES into a sequence of topology-aware embeddings x. Given x, we develop a variational auto-regressive decoder to generate the output sequence of chemical symbol predictions y. In this section, we first briefly introduce the topology-aware encoder in Section 3.1. After that, we elaborate the proposed variational auto-regressive decoder in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topology-aware Encoder</head><p>We can represent molecules in multiple ways, such as molecular fingerprints <ref type="bibr" target="#b28">[29]</ref>, SMILES strings <ref type="bibr" target="#b45">[46]</ref>, and molecular graphs with atoms as nodes and bonds as edges. As Jin et al. <ref type="bibr" target="#b16">[17]</ref> criticize that the linear SMILES representation can not well represent the topological context of atoms in a molecular graph, we embed molecules into the continuous vector space as graph embeddings via a topology-aware molecular graph encoder. In the following, we introduce the two critical components of the topology-aware encoder, namely directed message passing neural network (D-MPNN) <ref type="bibr" target="#b47">[48]</ref> and topologyaware positional embedding <ref type="bibr" target="#b42">[43]</ref>. ğ‘£ğ‘¤ associated with each vertex ğ‘£ are updated through aggregate function ğ‘€ (ğ‘¡ ) and update function ğ‘ˆ (ğ‘¡ ) as</p><formula xml:id="formula_4">ğ‘š (ğ‘¡ +1) ğ‘£ğ‘¤ = âˆ‘ï¸ ğ‘˜ âˆˆ {N (ğ‘£)\ğ‘¤ } ğ‘€ (ğ‘¡ ) (ğ‘¥ ğ‘£ , ğ‘¥ ğ‘˜ , â„ (ğ‘¡ ) ğ‘˜ğ‘£ ), â„ (ğ‘¡ +1) ğ‘£ğ‘¤ = ğ‘ˆ (ğ‘¡ ) (â„ ğ‘¡ ğ‘£ğ‘¤ , ğ‘š (ğ‘¡ +1) ğ‘£ğ‘¤ ),</formula><p>where N (ğ‘£) is the set of neighboring atoms of ğ‘£ in the molecular graph G. Note that the direction of messages matters since we initialize edge hidden states as â„</p><formula xml:id="formula_5">(0) ğ‘£ğ‘¤ = ReLU(ğ‘Š [ğ‘¥ ğ‘£ âˆ¥ğ‘’ ğ‘£ğ‘¤ ]</formula><p>) with the trainable matrix ğ‘Š . After ğ‘‡ iterations, we obtain final atom representations by</p><formula xml:id="formula_6">ğ‘š ğ‘£ = âˆ‘ï¸ ğ‘¤ âˆˆN (ğ‘£) â„ (ğ‘‡ ) ğ‘¤ğ‘£ , â„ ğ‘£ = GELU(ğ‘Š ğ‘œ [ğ‘¥ ğ‘£ âˆ¥ğ‘š ğ‘£ ]),</formula><p>where ğ‘Š ğ‘œ is a trainable matrix, GELU denotes the GELU activation <ref type="bibr" target="#b11">[12]</ref>, and âˆ¥ denotes the concatenation operation.</p><p>3.1.2 Topology-aware token embedding. To capture atom interactions, the atom representations coming out of D-MPNN are fed into Transformer encoders. Inspired by the relative positional embedding used in Transformer-XL <ref type="bibr" target="#b7">[8]</ref>, we follow Graph2SMILES <ref type="bibr" target="#b42">[43]</ref> and use the topology-aware positional embedding, which is dependent on the shortest path length between atoms in a molecular. Specifically, the attention score between atoms ğ‘£ and ğ‘¤ in the standard Transformer <ref type="bibr" target="#b43">[44]</ref> can be decomposed as</p><formula xml:id="formula_7">A ğ‘ğ‘ğ‘  ğ‘£,ğ‘¤ = h âŠ¤ ğ‘£ W âŠ¤ ğ‘ W ğ‘˜ h ğ‘¤ + h âŠ¤ ğ‘£ W âŠ¤ ğ‘ W ğ‘˜ p ğ‘¤ + p âŠ¤ ğ‘£ W âŠ¤ ğ‘ W ğ‘˜ h ğ‘¤ + p âŠ¤ ğ‘£ W âŠ¤ ğ‘ W ğ‘˜ p ğ‘¤ ,</formula><p>where W ğ‘ , W ğ‘˜ are weights for the keys and queries, and p ğ‘£ , p ğ‘¤ are the absolute positional embeddings corresponding to atoms ğ‘£, ğ‘¤. Similar to Transformer-XL <ref type="bibr" target="#b7">[8]</ref>, we follow Graph2SMILES <ref type="bibr" target="#b42">[43]</ref> and reparameterize the four terms as follows,</p><formula xml:id="formula_8">A ğ‘Ÿğ‘’ğ‘™ ğ‘£,ğ‘¤ = (h âŠ¤ ğ‘£ W âŠ¤ ğ‘ + c âŠ¤ )W ğ‘˜ h ğ‘¤ + (h âŠ¤ ğ‘£ W âŠ¤ ğ‘ + d âŠ¤ )W ğ‘˜,ğ‘… r ğ‘£,ğ‘¤ = (h âŠ¤ ğ‘£ W âŠ¤ ğ‘ + c âŠ¤ )W ğ‘˜ h ğ‘¤ + (h âŠ¤ ğ‘£ W âŠ¤ ğ‘ + d âŠ¤ ) rğ‘£,ğ‘¤ ,</formula><p>where the trainable biases are renamed as c and d to avoid confusion and r ğ‘£,ğ‘¤ is the learnable embedding dependent on the shortest path length between atoms ğ‘£ and ğ‘¤. For convenience, we merge W ğ‘˜,ğ‘… r ğ‘£,ğ‘¤ into rğ‘£,ğ‘¤ and use the shortest path length between atoms ğ‘£ and ğ‘¤ to look up rğ‘£,ğ‘¤ in the trainable positional embedding matrix. Therefore, our encoder integrates topological context into the final token embeddings of the input molecule. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Variational Auto-regressive Decoder</head><p>We consider the decoding process as a conditional graphical model concerning three random variables: the encoder output x, the discrete latent variable z, and the target reactants y. Figure <ref type="figure" target="#fig_3">3</ref> illustrates the conditional directed graphical model, where the encoder output is viewed as the observed context. For a given observation x, the latent variable z is drawn from the prior distribution ğ‘ ğœƒ (z|x), and the decoder output Å· is generated from the conditional distribution ğ‘ ğœƒ ( Å·|x, z). We interpret the one-hot latent variable z as the potential reaction type which directs the reactant generating process.</p><p>Our variational auto-regressive decoder (VAD) is trained to maximize the conditional log-likelihood log ğ‘ ğœƒ (y|x). As the objective function in Equation 1 is intractable, we use the variational lower bound of the log-likelihood in Equation 2 as a surrogate objective function. Under the reparameterization <ref type="bibr" target="#b20">[21]</ref>, we can replace an expectation w.r.t. ğ‘ ğœ™ (z|x, y) with one w.r.t. ğ‘ (ğœ–). Therefore, we rewrite the ELBO in Equation <ref type="formula" target="#formula_2">2</ref>as</p><formula xml:id="formula_9">L ğœƒ,ğœ™ (x, y) = E ğ‘ (ğœ–) [log ğ‘ ğœƒ (y|x, z)] -ğ· KL ğ‘ ğœ™ (z|x, y) âˆ¥ğ‘ ğœƒ (z|x) ,</formula><p>where z = ğ‘” ğœ™ (x, y, ğœ–), with ğ‘” ğœ™ (â€¢, â€¢, â€¢) indicating a deterministic, differentiable function and the distribution of the random variable ğœ– is independent of x or y. As a result, we can form a simple Monte Carlo estimator Lğœƒ,ğœ™ (x, y) of the ELBO as an empirical lower bound where we use a single noise sample ğœ– from the Gumbel distribution, i.e., ğœ– âˆ¼ Gumbel(0, 1), z = ğ‘” ğœ™ (x, y, ğœ–),</p><formula xml:id="formula_10">Lğœƒ,ğœ™ (x, y) = log ğ‘ ğœƒ (y|x, z) -ğ· KL ğ‘ ğœ™ (z|x, y)âˆ¥ğ‘ ğœƒ (z|x) .</formula><p>Inference process. Variational inference is a technique for approximating an intractable posterior distribution with a tractable surrogate. Figure <ref type="figure" target="#fig_3">3</ref> illustrates that ğ‘ (z|x, y) and ğ‘ (z|x) are approximated with a recognition model and a prior model, respectively. Instead of separately and iteratively optimizing the variational parameters per datapoint, we use the same recognition network (with parameters ğœ™) and prior network (with parameters ğœƒ ) to perform posterior inference over all of the samples (x, y) in our dataset. The strategy of sharing variational parameters across datapoints is also known as amortized variational inference <ref type="bibr" target="#b9">[10]</ref>. With amortized inference, RetroDCVAE can avoid a per-datapoint optimization loop and leverage the efficiency of stochastic gradient descent.</p><p>Parameter learning. We summarize the pipeline of variational decoding in Figure <ref type="figure" target="#fig_1">2</ref>. Both recognition network ğ‘ ğœ™ (z|x, y) and (conditional) prior network ğ‘ ğœƒ (z|x) are implemented with GRUs <ref type="bibr" target="#b3">[4]</ref> as both x and y are time series data. Taking the prior network ğ‘ ğœƒ (z|x) for an example, we can formulate the workflow at step ğ‘¡ as</p><formula xml:id="formula_11">s (ğ‘¡ ) = ğœ (W ğ‘  â€¢ x (ğ‘¡ ) + U ğ‘  â€¢ h (ğ‘¡ -1) + b ğ‘  ), r (ğ‘¡ ) = ğœ (W ğ‘Ÿ â€¢ x (ğ‘¡ ) + U ğ‘Ÿ â€¢ h (ğ‘¡ -1) + b ğ‘Ÿ ), h(ğ‘¡) = tanh(W â€¢ x (ğ‘¡ ) + U â€¢ (r (ğ‘¡ ) âŠ™ h (ğ‘¡ -1) ) + b), h (ğ‘¡ ) = (1 -s (ğ‘¡ ) ) âŠ™ h (ğ‘¡ -1) + s (ğ‘¡ ) âŠ™ h(ğ‘¡) ,</formula><p>where âŠ™ means the element-wise multiplication. For convenience, we summarize the process as</p><formula xml:id="formula_12">h (ğ‘¡ ) = GRU ğœƒ (x (ğ‘¡ ) , h (ğ‘¡ -1) ), for ğ‘¡ âˆˆ [ğ‘‡ ğ‘– ],</formula><p>where h (0) is initialized to zero vector.</p><p>As reaction types are discrete, we employ the Gumbel-Softmax distribution to approximate the latent categorical variable, i.e.,</p><formula xml:id="formula_13">ğœ‹ ğ‘— = exp(h (ğ‘‡ ğ‘– ) ğ‘— ) ğ¾ ğ‘˜=1 exp(h (ğ‘‡ ğ‘– ) ğ‘˜ ) , z ğ‘— = exp((log ğœ‹ ğ‘— + ğ‘” ğ‘— )/ğœ) ğ¾ ğ‘˜=1 exp((log ğœ‹ ğ‘˜ + ğ‘” ğ‘˜ )/ğœ) , for ğ‘— âˆˆ [ğ¾],</formula><p>where ğ‘” 1 , â€¢ â€¢ â€¢ , ğ‘” ğ¾ are independently drawn from Gumbel(0, 1) and ğ¾ is the predefined latent size. Throughout this paper, we further discretize z with arg max but use the continuous approximation in the backward pass, which is known as Straight-Through Gumbel Estimator <ref type="bibr" target="#b14">[15]</ref>. As depicted in Figure <ref type="figure" target="#fig_1">2</ref>, our VAD initializes the start of the sequence (namely the "&lt;SOS&gt;" token) based on z and then decodes subsequent tokens autoregressively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SETUP</head><p>In this section, we detail the experimental setup including datasets, metrics, baselines, and empirical tricks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We empirically evaluate our model on both public and homemade datasets derived from granted patents. We reserve all reactions containing 1-to-N products in USPTO-MIT <ref type="bibr" target="#b17">[18]</ref> to create USPTO-DIVERSE, where the maximum ğ‘ is 39 in the training set, i.e., there are 39 reactions in the training set that share the same product. In Section 5.1, we provide both quantitative and qualitative results on homemade USPTO-DIVERSE to prove the claim that RetroDCVAE is able to improve the performance of 1-to-N retrosynthesis and generate diverse reactants. In Section 5.2, we compare the proposed approach with existing single-step retrosynthetic methods on the widely used benchmark USPTO-50k <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>We mainly use two metrics to evaluate the proposed RetroDCVAE, namely top-ğ‘˜ accuracy (ğ‘˜ âˆˆ {1, 3, 5, 10}) and coverage. As the most commonly used metric for retrosynthesis, top-ğ‘˜ accuracy is defined as the fraction of correctly predicted reactants with a ranking higher than ğ‘˜. We count an output sequence as correct only if it matches the ground truth SMILES exactly. Apart from that, we use the coverage to assess whether RetroDCVAE is able to model the multi-modal distribution. The coverage is defined as the average of the proportion of the correctly predicted ground truth reactions <ref type="bibr" target="#b18">[19]</ref>. Therefore, higher coverage indicates that the model is able to generate more accurate and diverse reactant candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>Following <ref type="bibr" target="#b42">[43]</ref>, we broadly divide single-step retrosynthetic baselines into two groups according to whether additional features or techniques are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.1</head><p>Retrosynthesis with additional features/techniques. We classify template extraction, atom mapping, and data augmentation into additional features or techniques as each process is computationally expensive. Template-based approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47]</ref> match the given prodcut with large-scale refined rules or reaction templates and apply them to the product to obtain a set of reactants in singlestep retrosynthesis, which has been pointed out to be essentially a symbolic pattern recognition process <ref type="bibr" target="#b31">[32]</ref>. Graph-based approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref> are built upon atom mapping across chemical reactions, i.e., numbering atoms in a molecular graph to indicate which atom of the reactant(s) becomes which atom of the product(s) <ref type="bibr" target="#b15">[16]</ref>. Aug. Transformer <ref type="bibr" target="#b40">[41]</ref> and Chemformer <ref type="bibr" target="#b13">[14]</ref> explore fully data-driven single-step retrosynthesis by data augmentation, which increases top-ğ‘˜ prediction accuracy as well as training costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Retrosynthesis without additional features/techniques.</head><p>The proposed RetroDCVAE belongs to this category. This line of research consists of AutoSynRoute <ref type="bibr" target="#b21">[22]</ref>, SCROP <ref type="bibr" target="#b52">[53]</ref>, Latent Transformer <ref type="bibr" target="#b1">[2]</ref>, GET <ref type="bibr" target="#b25">[26]</ref>, DMP fusion <ref type="bibr" target="#b54">[55]</ref>, Tied Transformer <ref type="bibr" target="#b18">[19]</ref>, and Graph2SMILES <ref type="bibr" target="#b42">[43]</ref>. All of them treat single-step retrosynthesis as a sequence-to-sequence task and are built upon the Transformer architecture. Note that Graph2SMILES <ref type="bibr" target="#b42">[43]</ref> is the closest to RetroDCVAE in implementation, so we treat Graph2SMILES as the base model and compare it with RetroDCVAE on extensive experiments on both public and homemade datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Empirical Methodology</head><p>Although CVAE is proved effective in modeling multi-modal distribution <ref type="bibr" target="#b35">[36]</ref>, Bowman et al. <ref type="bibr" target="#b0">[1]</ref> point out that these types of models suffer from posterior collapse, i.e., z and x become independent if we minimize the KL divergence directly. To alleviate this problem, we use a simple yet effective approach, namely KL annealing <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b37">38]</ref>, where a variable weight is added to the KL term in our objective function during training. Therefore, we aim to maximize the following objective function</p><formula xml:id="formula_14">Lğœƒ,ğœ™ (x, y) = log ğ‘ ğœƒ (y|x, z) -ğ›¼ğ· KL ğ‘ ğœ™ (z|x, y)âˆ¥ğ‘ ğœƒ (z|x) ,</formula><p>where the KL annealing coefficient ğ›¼ is gradually increased from 0 to 1 over training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Implementation details</head><p>We implement our model using PyTorch and run it on a single NVIDIA GeForce 3090 GPU. Both the encoder and decoder of our model are composed of 6 layers with 8 parallel attention heads. The size of word embedding and atom representation is set to 256. We instantiate the attention sublayers and the position-wise feed-forward network sublayers with 256 hidden units. Following Graph2SMILES <ref type="bibr" target="#b42">[43]</ref>, we fix the filter size of Transformer to 2048. We train RetroDCVAE using Adam optimizer <ref type="bibr" target="#b19">[20]</ref> with Noam learning rate scheduler <ref type="bibr" target="#b43">[44]</ref>. We apply a dropout rate of 0.3 and limit the number of epochs to 2000. The layers of the recognition model and the prior model are both set to 1. We use grid search to find the best latent sizes for different datasets among {10, 20, 30, 60, 120}. Beam search is used to generate the output SMILES during inference with a beam size of 30. To avoid over-tuning, we select the models based on their top-1 accuracy during the validation phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>In this section, we conduct extensive experiments on two datasets to verify the effectiveness of RetroDCVAE. Our experiments are intended to answer the following three research questions.</p><p>â€¢ RQ1: Can RetroDCVAE model the multi-modal distribution over chemical reaction types as claimed? â€¢ RQ2: Does the proposed approach achieve competitive results on the benchmark dataset? â€¢ RQ3: What role do the discrete latent variables actually play in single-step retrosynthesis?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation on Homemade Dataset (RQ1)</head><p>To assess whether RetroDCVAE can model the multi-modal distribution as claimed, we perform a training/validation/testing split as 8:1:1 over all 1-to-N products in USPTO-MIT <ref type="bibr" target="#b17">[18]</ref> to build USPTO-DIVERSE. The product of each reaction in USPTO-DIVERSE corresponds to multiple synthetic schemes, which poses a challenge for deterministic retrosynthetic approaches. We summarize the comparison of Graph2SMILES <ref type="bibr" target="#b42">[43]</ref> and RetroDCVAE in Table <ref type="table" target="#tab_2">1</ref>, where the top-1 invalid rate means the proportion of grammatically invalid SMILES in top-1 predictions. By leveraging probabilistic inference, RetroDCVAE significantly reduces the invalid rate and consistently yields improvement in top-ğ‘˜ accuracy. We evaluate coverage of top-30 predictions on USPTO-DIVERSE as well. Among 1105 products, 942 have two distinct ground truth reactions, 113 have three distinct ground truth reactions, and 11 have more than five ground truth reactions. Table <ref type="table" target="#tab_3">2</ref> shows that the RetroDCVAE covers on average 35.8% of the actual reactions contained in the test set, which consistently improves the performance of Graph2SMILES <ref type="bibr" target="#b42">[43]</ref>.</p><p>To examine the quality of retrosynthetic prediction candidates, we compare the proposed RetroDCVAE against Graph2SMILES <ref type="bibr" target="#b42">[43]</ref> in terms of top-3 predictions. Figure <ref type="figure" target="#fig_4">4</ref> shows the comparison results. For the same input product, both Graph2SMILES <ref type="bibr" target="#b42">[43]</ref> and RetroDCVAE generate grammatically valid reactants. The proposed RetroDCVAE correctly predicts three ground-truth paths, while Graph2SMILES <ref type="bibr" target="#b42">[43]</ref> generates chemically incorrect reactants and ranks a specious synthetic scheme higher than a correct one.</p><p>Therefore, the answer to RQ1 is yes, i.e., RetroDCVAE can alleviate the problem of the multi-modal distribution as claimed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on Public Dataset (RQ2)</head><p>Table <ref type="table" target="#tab_4">3</ref> summarizes the top-ğ‘˜ accuracy evaluated on the benchmark dataset USPTO-50k. We divide existing single-step retrosynthetic approaches into two groups. The best top-ğ‘˜ accuracy of each group is highlighted with boldface. Overall, RetroDCVAE achieves competitive results for singlestep retrosynthesis and consistently outperforms Graph2SMILES <ref type="bibr" target="#b42">[43]</ref> on top-ğ‘˜ accuracy. The improvement lies in that RetroDCVAE introduces stochastic inference to structured output prediction, which lowers the risk of over-fitting and reduces the probability of invalid SMILES string output.</p><p>Following <ref type="bibr" target="#b42">[43]</ref>, we divide single-step retrosynthetic methods into two groups according to whether the additional techniques such as templates, atom mapping, and output-side data augmentation are used. Table <ref type="table" target="#tab_4">3</ref> shows that RetroDCVAE achieves the best top-1 and top-3 accuracies across methods that do not use reaction templates, atom mapping, or output SMILES augmentation. That is, RetroDCVAE achieves state-of-the-art performance while exempting the need of domain knowledge.</p><p>Compared to the second group in Table <ref type="table" target="#tab_4">3</ref>, RetroDCVAE further achieves comparable results against the methods that use additional features or techniques. Specifically, RetroDCVAE even outperforms several template-based baselines <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b46">47]</ref> in terms of the top-1 accuracy. Therefore, the answer to RQ2 is yes. We conclude that RetroDCVAE is able to achieve competitive results on the benchmark dataset USPTO-50k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Further Analysis on Latent Variables (RQ3)</head><p>As mentioned in Section 3, we use the Gumbel-Softmax distribution to approximate categorical samples and discretize the latent variable z with Straight-Through Gumbel Estimator <ref type="bibr" target="#b14">[15]</ref>. To gain insights into the working behaviors of RetroDCVAE, we conduct qualitative analysis on the discrete latent variable z on both public and homemade datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Product</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank</head><p>Ground Truth Graph2SMILES RetroDCVAE    RetroDCVAE reads an arbitrary product sequence and encodes it using a topology-aware encoder, then the latent variable z is sampled according to the product embeddings. Conditioned on z, RetroDCVAE generates possible reactants using its variational decoder. To figure out what role the latent variables play, we collect the reactions that associate with the same one-hot vector z and present examples in following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Qualitative</head><p>Examples on USPTO-DIVERSE. As depicted in Figure <ref type="figure" target="#fig_5">5</ref>, reactions corresponding to the same z share many common characteristics. We follow the chemical reaction classification criteria adopted by Schneider et al. <ref type="bibr" target="#b30">[31]</ref>. The reactions in the first column can be classified into functional group interconversion (FGI). Moreover, the reactants of the three reactions all contain bromine (Br) atoms. Reactions corresponding to z = e 8 in the second column are associated with the formation or disappearance of the C-O bond. In addition, each reaction in the second column only has one reactant. As for the third column, the reactions corresponding to z = e 9 can be categorized into functional group addition (FGA).</p><p>Therefore, the answer to RQ3 becomes clear. In general, we can view the discrete latent variables as reaction indicators that guide the generation of distinct reactants. The discrete latent variables reduce grammatically invalid rates and increase reaction diversity for single-step retrosynthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Qualitative</head><p>Examples on USPTO-50k. In addition to the qualitative examples on USPTO-DIVERSE, we analyse the discrete latent variable z on the benchmark dataset USPTO-50k <ref type="bibr" target="#b22">[23]</ref> as well. Figure <ref type="figure" target="#fig_6">6</ref> presents some reaction examples that correspond to different discrete latent variables. Following the reaction classification criteria in <ref type="bibr" target="#b30">[31]</ref>, we summarize the reactions in each column as follows.</p><p>The reactions corresponding to z = e 1 in the first column are associated with the formation of the C-C bond. And the reactions in the second column belong to the deprotection reaction, where a protecting group is modified by converting it into a functional group. Finally, reactions in the third column are all heteroatom alkylation and arylation reactions. Therefore, the conclusion in Section 5.3.1 still holds. The discrete latent variable z works as a reaction indicator that guides the generation of distinct sets of reactants. Moreover, the proposed discrete latent variables may have the potential to automatically classify unlabeled reactions in granted patents, reducing the need for experience and expertise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>In this work, we present RetroDCVAE, a novel single-step retrosynthetic approach inspired by CVAE. To delve deeper into the property of RetroDCVAE, we discuss the advantages and limitation of RetroDCVAE in Section 6.1 and Section 6.2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Advantages of RetroDCVAE</head><p>The proposed RetroDCVAE aims to model the multi-modal distribution over reaction types via discrete latent variables. Compared with state-of-the-art single-step retrosynthetic models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>, RetroDCVAE reduces the need for reaction templates, atom mapping and data augmentation, while maintaining comparable retrosynthetic performance. Compared with existing sequence-based approaches, RetroDCVAE achieves the best top-ğ‘˜ accuracy (ğ‘˜ = 1, 3) for single-step retrosynthesis, while being among the most efficient to train and predict.</p><p>Apart from its effectiveness, another significant advantage of RetroDCVAE is its interpretability. Most existing template-free methods simply output reactant candidates without any explanation. Different from them, RetroDCVAE associates each set of predicted reactants with a discrete latent variable, which we interpret as the corresponding reaction type. The interpretability of RetroDCVAE is expected to have an immediate and strong impact on computer-aided retrosynthetic tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Limitation of RetroDCVAE</head><p>One limitation of this work is that RetroDCVAE does not consider catalysts or reagents when generating reactant candidates for the desired product, which is a common fault of existing computer-aided retrosynthetic approaches. In fact, a chemical reaction is related not only to reactants and products, but also to reaction conditions including catalysts, temperature, and reagent concentration. The reaction conditions indicate the difficulty of synthesizing the product to some extent, which is important for retrosynthesis planning. Although it is a pioneer to model reaction types, RetroDCVAE lacks the ability to predict detailed reaction conditions, which can be further improved and extended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Our work is closely related to single-step retrosynthetic approaches and CVAE based natural language processing (NLP). In this part, we first review single-step retrosynthetic methods in Section 7.1, then briefly introduce CVAE and its variants in Section 7.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Single-step Retrosynthesis</head><p>An increasing number of machine learning approaches have emerged to assist in designing synthetic schemes for a target product. We broadly categorize them into template-based and template-free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Template-based retrosynthesis.</head><p>Relying on domain knowledge, template-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47]</ref> match the given products with large-scale chemical rules or reaction templates. These rules can be hand-encoded by human experts or automatically extracted from data. This procedure is challenging because encoding which adjacent functional groups influence the outcome of a reaction requires an understanding of the underlying mechanisms. Despite their interpretability, template-based methods are criticized for poor generalization to new and rare reactions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37]</ref>, which entails a paradigm shift to template-free approaches.</p><p>7.1.2 Template-free retrosynthesis. According to the leading data format during training, template-free approaches fall into two categories, namely graph edit-based and translation-based. The first category views retrosynthesis as graph transformations. Specifically, <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref> first identify reaction centers, then perform graph or sequence recovery. Translation-based methods formulate the product-to-reactant process of single-step retrosynthesis as SMILES-to-SMILES translation, typically with sequence models such as Transformer <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49]</ref>. Some variants of the second category perform pretraining, rerank the predictions, or leverage graph topology to enhance performance <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b54">55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Conditional Generative Models in NLP</head><p>Formulating single-step retrosynthesis as machine translation, we draw inspiration from conditional generative models in NLP. Sohn et al. <ref type="bibr" target="#b35">[36]</ref> propose CVAE to perform probabilistic inference and make diverse predictions, where the Gaussian latent variables are used to model complex structured output representations. Pagnoni et al. <ref type="bibr" target="#b26">[27]</ref> introduce CVAE to neural machine translation for conditional text generation, while Zhao et al. <ref type="bibr" target="#b51">[52]</ref> and Serban et al. <ref type="bibr" target="#b33">[34]</ref> apply continuous latent variables to diverse dialogue generation. In addition, MojiTalk <ref type="bibr" target="#b53">[54]</ref> leverages CVAE variants to explicitly control the emotion and sentiment of generated text. Yu et al. <ref type="bibr" target="#b49">[50]</ref> propose a multi-pass hierarchical conditional variational autoencoder to improve automatic storytelling. However, the aforementioned methods only consider continuous latent variables, which is unfit for retrosynthesis given the discrete nature of chemical reactions. Unlike them, we use discrete latent variables to guide reactant generation and leverage Gumbel-Softmax approximation to support backpropagation, leading to the novelty of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we present RetroDCVAE, a template-free retrosynthesizer that is able to generate diverse reactant candidates via discrete conditional variational autoencoders. Extensive experiments on both public and homemade datasets demonstrate that RetroDCVAE consistently outperforms template-free baselines on single-step retrosynthesis. One limitation of RetroDCVAE is that we do not take into account factors such as catalysts and reagent concentration during the synthetic process, which is a common fault of existing computer-aided retrosynthetic approaches. In future work, we aim to consider detailed reaction conditions and extend RetroDCVAE to multi-step retrosynthesis, i.e., the retrosynthesis planning task. We anticipate that RetroDCVAE would exert potential impacts on modern drug discovery, particularly in accelerating the development of treatments and drugs for COVID-19.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of single-step retrosynthesis with two possible reactions. We can synthesize the target product Benzophenone using different sets of reactants through different reactions. Either of the synthesis schemes is feasible. Specifically, Benzophenone (Left) can be synthesized by Benzhydrol through Reduction (Top) or by Benzoyl chloride and Benzene through C-C bond formation (Bottom).</figDesc><graphic coords="2,195.79,142.11,97.07,51.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of proposed RetroDCVAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3. 1 . 1 D</head><label>11</label><figDesc>-MPNN. Unlike atom-oriented message passing in edgeaware MPNNs<ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>, we follow D-MPNN<ref type="bibr" target="#b47">[48]</ref> and use a Directed Graph Convolutional Network (D-GCN) to derive topology-aware atom representations, where message updates are oriented towards directed bonds to prevent totters, or messages being passed backand-forth between neighbors<ref type="bibr" target="#b47">[48]</ref>. Specifically, suppose the message passing performed on an undirected molecular graph G with atom features ğ‘¥ ğ‘£ and bond features ğ‘’ ğ‘£ğ‘¤ consists of ğ‘‡ steps. On each step ğ‘¡, hidden states â„ (ğ‘¡ ) ğ‘£ğ‘¤ and messages ğ‘š (ğ‘¡ )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Conditional directed graphical model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Top-3 predictions of Graph2SMILES [43] and RetroDCVAE.</figDesc><graphic coords="7,142.56,320.19,109.76,63.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Reactions in USPTO-DIVERSE that corresponding to different z. As z is a discrete latent variable, z ğ‘– = 1 indicates that z is the one-hot vector e ğ‘– . RetroDCVAE reads the product SMILES then generates z and reactants.</figDesc><graphic coords="7,479.07,599.29,75.54,63.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Reactions in USPTO-50k that corresponding to different latent variables. Here z ğ‘– = 1 indicates that z is the one-hot vector e ğ‘– . RetroDCVAE reads the product SMILES then generates z and reactants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b1">2</ref> </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, ğ‘˜ } and samples ğ‘” 1 , â€¢ â€¢ â€¢ , ğ‘” ğ‘˜ are drawn i.i.d.</figDesc><table><row><cell></cell><cell>, â€¢ â€¢ â€¢ , ğœ‹ ğ‘˜ .</cell></row><row><cell cols="2">To draw samples z from a categorical distribution with class proba-</cell></row><row><cell cols="2">bilities ğœ‹, Gumbel [11] and Maddison et al. [25] propose the Gumbel-</cell></row><row><cell>Max trick, i.e.,</cell><cell></cell></row><row><cell>z = one_hot arg max</cell><cell>(ğ‘” ğ‘– + log ğœ‹ ğ‘– ) ,</cell></row><row><cell>ğ‘– âˆˆ [ğ‘˜ ]</cell><cell></cell></row><row><cell>where [ğ‘˜] = {1, 2, â€¢ â€¢ â€¢</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Model performance on USPTO-DIVERSE.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="3">Top-ğ‘˜ accuracy (%)</cell><cell>Top-1 invalid</cell></row><row><cell></cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>rate (%)</cell></row><row><cell cols="5">Graph2SMILES [43] 32.2 45.2 47.9 52.1</cell><cell>8.20</cell></row><row><cell>RetroDCVAE</cell><cell cols="4">33.4 49.8 54.1 57.1</cell><cell>4.77</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Coverage evaluation on USPTO-DIVERSE.</figDesc><table><row><cell cols="2">#Reaction #Product</cell><cell>Coverage (%)</cell><cell></cell></row><row><cell>per product</cell><cell></cell><cell cols="2">Graph2SMILES [43] RetroDCVAE</cell></row><row><cell>2</cell><cell>942</cell><cell>33.9</cell><cell>36.6</cell></row><row><cell>3</cell><cell>113</cell><cell>30.4</cell><cell>33.6</cell></row><row><cell>4</cell><cell>35</cell><cell>23.6</cell><cell>28.6</cell></row><row><cell>5</cell><cell>4</cell><cell>10.0</cell><cell>10.0</cell></row><row><cell>6</cell><cell>4</cell><cell>8.3</cell><cell>16.7</cell></row><row><cell>7</cell><cell>3</cell><cell>23.8</cell><cell>23.8</cell></row><row><cell>8</cell><cell>2</cell><cell>6.3</cell><cell>18.9</cell></row><row><cell>9</cell><cell>1</cell><cell>11.1</cell><cell>22.2</cell></row><row><cell>13</cell><cell>1</cell><cell>7.7</cell><cell>15.4</cell></row><row><cell>average</cell><cell></cell><cell>32.9</cell><cell>35.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Retrosynthesis results on unlabeled USPTO-50k sorted by top-1 accuracy. Templ.: reaction templates used; Map.: atommapping required; Aug.: output data augmentation used. Best results for each group of rows are highlighted in bold. The results of previous studies are taken from<ref type="bibr" target="#b42">[43]</ref>.</figDesc><table><row><cell>Methods</cell><cell cols="4">Top-ğ‘˜ accuracy (%)</cell><cell cols="3">Techniques used</cell></row><row><cell></cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell cols="3">Templ. Map. Aug.</cell></row><row><cell>AutoSynRoute [22]</cell><cell cols="4">43.1 64.6 71.8 78.7</cell><cell>âœ—</cell><cell>âœ—</cell><cell>âœ—</cell></row><row><cell>SCROP [53]</cell><cell cols="4">43.7 60.0 65.2 68.7</cell><cell>âœ—</cell><cell>âœ—</cell><cell>âœ—</cell></row><row><cell cols="5">Latent Transformer [2] 44.8 64.9 72.4 78.4</cell><cell>âœ—</cell><cell>âœ—</cell><cell>âœ—</cell></row><row><cell>GET [26]</cell><cell cols="4">44.9 58.8 62.4 65.9</cell><cell>âœ—</cell><cell>âœ—</cell><cell>âœ—</cell></row><row><cell>DMP fusion [55]</cell><cell cols="4">46.1 65.2 70.4 74.3</cell><cell>âœ—</cell><cell>âœ—</cell><cell>âœ—</cell></row><row><cell>Tied Transformer [19]</cell><cell cols="4">47.1 67.2 73.5 78.5</cell><cell>âœ—</cell><cell>âœ—</cell><cell>âœ—</cell></row><row><cell>Graph2SMILES [43]</cell><cell cols="4">52.9 66.5 70.0 72.9</cell><cell>âœ—</cell><cell>âœ—</cell><cell>âœ—</cell></row><row><cell>RetroDCVAE (ours)</cell><cell cols="4">53.1 68.1 71.6 74.3</cell><cell>âœ—</cell><cell>âœ—</cell><cell>âœ—</cell></row><row><cell>MEGAN [30]</cell><cell cols="4">48.1 70.7 78.4 86.1</cell><cell>âœ—</cell><cell>âœ“</cell><cell>âœ—</cell></row><row><cell>G2Gs [35]</cell><cell cols="4">48.9 67.6 72.5 75.5</cell><cell>âœ—</cell><cell>âœ“</cell><cell>âœ—</cell></row><row><cell>RetroXpert [47]</cell><cell cols="4">50.4 61.1 62.3 63.4</cell><cell>âœ“</cell><cell>âœ“</cell><cell>âœ“</cell></row><row><cell>GTA [33]</cell><cell cols="4">51.1 67.6 74.8 81.6</cell><cell>âœ—</cell><cell>âœ“</cell><cell>âœ“</cell></row><row><cell>RetroPrime [45]</cell><cell cols="4">51.4 70.8 74.0 76.1</cell><cell>âœ—</cell><cell>âœ“</cell><cell>âœ“</cell></row><row><cell>GLN [7]</cell><cell cols="4">52.5 69.0 75.6 83.7</cell><cell>âœ“</cell><cell>âœ“</cell><cell>âœ—</cell></row><row><cell cols="2">Aug. Transformer [41] 53.2</cell><cell>-</cell><cell cols="2">80.5 85.2</cell><cell>âœ—</cell><cell>âœ—</cell><cell>âœ“</cell></row><row><cell>LocalRetro [3]</cell><cell cols="4">53.4 77.5 85.9 92.4</cell><cell>âœ“</cell><cell>âœ“</cell><cell>âœ—</cell></row><row><cell>GraphRetro [37]</cell><cell cols="4">53.7 68.3 72.2 75.5</cell><cell>âœ—</cell><cell>âœ“</cell><cell>âœ—</cell></row><row><cell>Chemformer [14]</cell><cell>54.3</cell><cell>-</cell><cell cols="2">62.3 63.0</cell><cell>âœ—</cell><cell>âœ—</cell><cell>âœ“</cell></row><row><cell>EBM (Dual-TB) [39]</cell><cell cols="4">55.2 74.6 80.5 86.9</cell><cell>âœ“</cell><cell>âœ“</cell><cell>âœ“</cell></row><row><cell>ğ’› 1 = 1</cell><cell></cell><cell cols="2">ğ’› 4 = 1</cell><cell></cell><cell></cell><cell></cell><cell>ğ’› 9 = 1</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generating Sentences from a Continuous Space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>JÃ³zefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Benson</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09688</idno>
		<title level="m">Learning to make generalizable and diverse predictions for retrosynthesis</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep Retrosynthetic Reaction Prediction using Local Reactivity and Global Attention</title>
		<author>
			<persName><forename type="first">Shuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yousung</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JACS Au</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST@EMNLP</title>
		<meeting>SSST@EMNLP</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Computer-assisted retrosynthesis based on molecular similarity</title>
		<author>
			<persName><forename type="first">Connor</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">H</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><surname>Klavs</surname></persName>
		</author>
		<author>
			<persName><surname>Jensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>ACS central science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Computer-assisted design of complex organic syntheses</title>
		<author>
			<persName><forename type="first">Elias</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corey</forename></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Todd Wipke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="1969">1969. 1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Retrosynthesis Prediction with Conditional Graph Logic Network</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive Language Models beyond a Fixed-Length Context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Viet Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Retrosynthesis with attention-based NMT model and chemical analysis of</title>
		<author>
			<persName><forename type="first">Hongliang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjun</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>wrong&quot; predictions. RSC advances</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Amortized Inference in Probabilistic Reasoning</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CogSci</title>
		<meeting>of CogSci</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Statistical theory of extreme values and some practical applications: a series of lectures</title>
		<author>
			<persName><forename type="first">Emil</forename><surname>Julius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gumbel</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1954">1954</date>
			<publisher>US Government Printing Office</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Strategies for Pre-training Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Chemformer: A Pre-Trained Transformer for Computational Chemistry</title>
		<author>
			<persName><forename type="first">R</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bjerrum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Categorical Reparameterization with Gumbel-Softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic mapping of atoms across both simple and complex chemical reactions</title>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>SzymkuÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Mikulak-Klucznik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Piecuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Klucznik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">MichaÅ‚</forename><surname>KaÅºmierowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Rydzewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Gambin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bartosz</surname></persName>
		</author>
		<author>
			<persName><surname>Grzybowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Comm</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Valid, Plausible, and Diverse Retrosynthesis Using Tied Two-Way Transformers with Latent Variables</title>
		<author>
			<persName><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongseon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngchun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><forename type="middle">Sik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youn-Suk</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic retrosynthetic route planning using template-free models</title>
		<author>
			<persName><forename type="first">Kangjie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youjun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luhua</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem. Sci</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Retrosynthetic reaction prediction using neural sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Kawthekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quang</forename><surname>Luu Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Sloane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Wender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>ACS central science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Cheng-Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Korablyov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">StanisÅ‚aw</forename><surname>JastrzÄ™bski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">PaweÅ‚</forename><surname>WÅ‚odarczyk-PruszyÅ„ski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marwin</forename><forename type="middle">Hs</forename><surname>Segler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13042</idno>
		<title level="m">Approximating Retrosynthesis by Graph Neural Networks for De Novo Drug Design</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A* Sampling</title>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Molecular graph enhanced transformer for retrosynthesis prediction</title>
		<author>
			<persName><forename type="first">Kelong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Conditional variational autoencoder for neural machine translation</title>
		<author>
			<persName><forename type="first">Artidoro</forename><surname>Pagnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangyan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04405</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extended-Connectivity Fingerprints</title>
		<author>
			<persName><forename type="first">David</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathew</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PaweÅ‚ WÅ‚odarczyk-PruszyÅ„ski, and StanisÅ‚aw JastrzÄ™bski. 2021. Molecule Edit Graph Attention Network: Modeling Chemical Reactions as Sequences of Graph Edits</title>
		<author>
			<persName><forename type="first">MikoÅ‚aj</forename><surname>Sacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">MikoÅ‚aj</forename><surname>BÅ‚aÅ¼</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Byrski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">PaweÅ‚</forename><surname>DÄ…browski-TumaÅ„ski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">MikoÅ‚aj</forename><surname>ChromiÅ„ski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">RafaÅ‚</forename><surname>Loska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What&apos;s what: The (nearly) definitive guide to reaction role assignment</title>
		<author>
			<persName><forename type="first">Nadine</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Stiefl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">A</forename><surname>Landrum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural-symbolic machine learning for retrosynthesis and reaction prediction</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Marwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">P</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName><surname>Waller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemistry-A European Journal</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">GTA: Graph Truncated Attention for Retrosynthesis</title>
		<author>
			<persName><forename type="first">Seung-Woo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">June</forename><forename type="middle">Yong</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seohui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hankook</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunho</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues</title>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Graph to Graphs Framework for Retrosynthesis Prediction</title>
		<author>
			<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning Structured Output Representation using Deep Conditional Generative Models</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning Graph Models for Template-Free Retrosynthesis</title>
		<author>
			<persName><forename type="first">Ram</forename><surname>Vignesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Somnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><forename type="middle">W</forename><surname>Bunne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ladder Variational Autoencoders</title>
		<author>
			<persName><forename type="first">Tapani</forename><surname>Casper Kaae SÃ¸nderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">SÃ¸ren</forename><surname>MaalÃ¸e</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Kaae SÃ¸nderby</surname></persName>
		</author>
		<author>
			<persName><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13437</idno>
		<title level="m">Energy-based View of Retrosynthesis</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">State-of-the-art augmented NLP transformer models for direct and single-step retro synthesis</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Tetko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Karpov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Deursen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Comm</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The atom economy-a search for synthetic efficiency</title>
		<author>
			<persName><surname>Barry M Trost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="1991">1991. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Permutation invariant graph-tosequence model for template-free retrosynthesis and reaction prediction</title>
		<author>
			<persName><forename type="first">Zhengkai</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.09681</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">RetroPrime: A Diverse, plausible and Transformer-based method for Single-Step retrosynthesis predictions</title>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benben</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang-Yu</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Engineering Journal</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules</title>
		<author>
			<persName><forename type="first">David</forename><surname>Weininger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">RetroXpert: Decompose Retrosynthesis Prediction Like A Chemist</title>
		<author>
			<persName><forename type="first">Chaochao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianggang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangjia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Analyzing Learned Molecular Representations for Property Prediction</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Eiden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><surname>Guzman-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hopper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriam</forename><surname>Mathea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Settels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klavs</forename><forename type="middle">F</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Molecular Transformer unifies reaction prediction and retrosynthesis across pharma chemical space</title>
		<author>
			<persName><forename type="first">Qingyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishnu</forename><surname>Sresht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bolgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjun</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacquelyn</forename><forename type="middle">L</forename><surname>Klug-Mcleod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Butler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Communications</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Draft and Edit: Automatic Storytelling Through Multi-Pass Hierarchical Conditional Variational Autoencoder</title>
		<author>
			<persName><forename type="first">Meng-Hsuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Retrosynthesis of multi-component metal-organic frameworks</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Sheng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Lollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiandong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Comm</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders</title>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>EskÃ©nazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Predicting Retrosynthetic Reactions Using Self-Corrected Transformer Neural Networks</title>
		<author>
			<persName><forename type="first">Shuangjia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahua</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuedong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">MojiTalk: Generating Emotional Responses at Scale</title>
		<author>
			<persName><forename type="first">Xianda</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Jinhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10234</idno>
		<title level="m">Dual-view Molecule Pre-training</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
