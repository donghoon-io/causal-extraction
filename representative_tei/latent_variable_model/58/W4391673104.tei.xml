<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent Plan Transformer for Trajectory Abstraction: Planning as Latent Space Inference</title>
				<funder>
					<orgName type="full">Amazon</orgName>
				</funder>
				<funder ref="#_kZjVdwB #_2pHGCV2">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-08-18">18 Aug 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Deqian</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics and Data Science</orgName>
								<address>
									<country>UCLA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dehong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics and Data Science</orgName>
								<address>
									<country>UCLA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minglu</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics and Data Science</orgName>
								<address>
									<country>UCLA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Akool Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Lizarraga</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics and Data Science</orgName>
								<address>
									<country>UCLA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuhao</forename><surname>Huang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sirui</forename><surname>Xie</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<address>
									<country>UCLA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics and Data Science</orgName>
								<address>
									<country>UCLA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Latent Plan Transformer for Trajectory Abstraction: Planning as Latent Space Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-08-18">18 Aug 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2402.04647v4[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In tasks aiming for long-term returns, planning becomes essential. We study generative modeling for planning with datasets repurposed from offline reinforcement learning. Specifically, we identify temporal consistency in the absence of step-wise rewards as one key technical challenge. We introduce the Latent Plan Transformer (LPT), a novel model that leverages a latent variable to connect a Transformerbased trajectory generator and the final return. LPT can be learned with maximum likelihood estimation on trajectory-return pairs. In learning, posterior sampling of the latent variable naturally integrates sub-trajectories to form a consistent abstraction despite the finite context. At test time, the latent variable is inferred from an expected return before policy execution, realizing the idea of planning as inference.</p><p>Our experiments demonstrate that LPT can discover improved decisions from suboptimal trajectories, achieving competitive performance across several benchmarks, including Gym-Mujoco, Franka Kitchen, Maze2D, and Connect Four. It exhibits capabilities in nuanced credit assignments, trajectory stitching, and adaptation to environmental contingencies. These results validate that latent variable inference can be a strong alternative to step-wise reward prompting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Decision Transformer (DT) <ref type="bibr" target="#b2">(Chen et al., 2021)</ref> and some concurrent work <ref type="bibr" target="#b13">(Janner et al., 2021)</ref> have popularized the research agenda of decision-making via generative modeling. The general idea is to consider decision-making as a generative process that takes in a representation of the task objective, e.g. the rewards or returns of a trajectory, and outputs a representation of the trajectory. Intuitively, a purposeful decision-making process should shift the trajectory distribution towards regimes with higher returns. In the classical decision-making literature, this is achieved by two interweaving processes, policy evaluation and policy improvement <ref type="bibr" target="#b31">(Sutton and Barto, 2018)</ref>. Policy evaluation promotes consistency in the estimated correlations between the trajectories and the returns. In DT, this is realized by the maximum likelihood estimation (MLE) of the joint distribution of sequences consisting of states, actions, and return-to-gos (RTG). Policy improvement shifts the distribution to improve the status quo expectation of the returns. In DT, this is naturally entailed since the policy is a distribution of actions conditioned on step-wise RTGs.</p><p>In this work, we are interested in the problem of planning. Among various ways to identify planning as a special class of decision-making problems, we pay particular attention to its data specification and inductive biases. As designing step-wise rewards requires significant effort and domain expertise, we focus on the problem of learning from trajectory-return pairs, where a trajectory is a sequence of states and actions, and the return is its total rewards. This design choice forces the agents to predict into the long-term future and figure out step-wise credits by themselves. A competitive Temporal Difference (TD) learning baseline, CQL <ref type="bibr" target="#b18">(Kumar et al., 2020)</ref>, was reported to be fragile under this data specification <ref type="bibr" target="#b2">(Chen et al., 2021)</ref>.</p><p>Our design of inductive biases reflects our intuition of a plan. While a policy is a factor of the trajectory distribution, a plan is an abstraction lifted from the space of trajectories. As a plan is always made in advance of receiving returns, it implies significance, persistence, and contingency. An agent should plan for more significant returns. It should be persistent in its plan even if the return is assigned in hindsight. It should also be adaptable to the environment's changes during the execution of the plan. We formulate this hierarchy of decision-making with a top-down latent variable model. The latent variable we introduce is effectively a plan, for it decouples the trajectory generation from the expected improvement of returns. The autoregressive policy always consults this temporally extended latent variable to be persistent in the plan. The top-down structure enables the agent to disentangle the variations in its plan from the environment's contingencies.</p><p>In this work, we introduce the Latent Plan Transformer (LPT), a novel generative model featuring a latent vector modeled by a neural transformation of Gaussian white noise, a Transformer-based policy conditioned on this latent vector and a return estimation model. LPT is learned by maximum likelihood estimation (MLE). Given an expected return, posterior inference of the latent vector in LPT is an explicit process for iterative refinement of the plan. The inferred latent variable replaces RTG in the conditioning of the auto-regressive policy, providing richer information about the anticipated future. We further develop a mode-seeking sampling scheme that strongly enforce the temporal consistency for long-range planning, which is particularly effective in stitch trajectory, i.e., to compose parts of sub-optimal trajectories to reach far beyond <ref type="bibr" target="#b8">(Fu et al., 2020)</ref>. LPT demonstrates competitive performance in Gym-Mujoco locomotion, Franka kitchen, goal-reaching tasks in maze2d and antmaze, and a contingent planning task Connect Four. These empirical results support that latent variable inference can enable and improve planning in the absence of step-wise rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>A sequential decision-making problem can be formulated with a decision process ⟨S, A, H, T r, r, ρ⟩ that contains a set S of states and a set A of actions. Horizon H is the maximum number of steps the agent can execute before the termination of the sequence. We further employ S + to denote the set of all non-empty state sequences within the horizon and A + for action sequences likewise. T r : S + × A + → Π(S) is the transition that returns a distribution over the next state. r : S + × A + → R specifies the real-valued reward at each step. ρ : Π(S) is the initial state distribution that is always uncontrollable to the agent. The agent's decisions follow a policy π : S + × A + → Π(A). In each episode, the agent interacts with the transition model to generate a trajectory τ = (s 1 , a 1 , s 2 , a 2 ..., s H , a H ).</p><p>The objective of sequential decision-making is typically formulated as the expected trajectory re-</p><formula xml:id="formula_0">turn y = H t=0 r t , Q = E p(τ ) [y].</formula><p>Conventional RL algorithms solve for a policy π(a t |s t , * ), where the conditioning * denotes the optimal expected return. DT generalizes this policy to π(a t |s ≤t , a &lt;t , RT G ≤t ), by fitting the joint distribution p(s 1 , a 1 , RT G 1 , ...s T , a T , RT G T ) with a Transformer. RT G t is the return-to-go from step t to the horizon H, RT G t = H k=t r(s ≤k , a ≤k ). It is a useful indication of future rewards, especially when rewards are dense and informative.</p><p>However, RT G becomes less reliable when rewards are sparse or have non-trivial relations with the return. Distributing the return to each step is a credit assignment problem. Consider an example of an ideal credit assignment mechanism: When students receive partial credits for their incomplete answers, it's more fair to give points equal to the full marks minus the expected points for all possible ways to finish the answer, rather than assuming students have no knowledge of the remaining parts. This credit assignment mechanism can be formalized as, RT G</p><formula xml:id="formula_1">Q t = K k=t r(s ≤k , a ≤k )+E[Q(s ≤K , a ≤K )].</formula><p>Here Q can be estimated using deep TD learning with multi-step returns. <ref type="bibr" target="#b38">Yamagata et al. (2023)</ref> instantiate a Markovian version and demonstrate improvement in trajectory stiching.</p><p>Whatever credit assignment we use, be it RT G or RT G Q , the purpose is to explicitly model the statistical association between trajectory steps and final returns. This effort is believed to be necessary because of the exponential complexity of the trajectory space. This belief, however, can be reexamined given the success of sequence modeling. We explore an alternative design choice by directly associating the latent vector that generates the trajectory with the return.</p><p>3 Latent Plan Transformer (LPT) Given a variable-length trajectory τ , z ∈ R d is a vector that represents τ in the latent space. y ∈ R is the return of the trajectory. The joint distribution of the trajectory and its return is defined as p(τ, y).</p><p>The latent trajectory variable z, conceptualized as a plan, is posited to decouple the autoregressive policy and return estimation. From a statistical standpoint, with z given, we assume that τ and y are conditionally independent, positioning z as the information bottleneck. Under this assumption, the Latent Plan Transformer (LPT) can be defined as,</p><formula xml:id="formula_2">p θ (τ, y, z) = p α (z)p β (τ |z)p γ (y|z),<label>(1)</label></formula><p>where θ = (α, β, γ). LPT approximates the data distribution p data (τ, y) using the marginal distribution p θ (τ, y) = p θ (τ, y, z)dz. It also establishes a generation process,</p><formula xml:id="formula_3">z ∼ p α (z), [τ |z] ∼ p β (τ |z), [y|z] ∼ p γ (y|z).<label>(2)</label></formula><p>The prior model p α (z) is an implicit generator, defined as a learnable neural transformation of an isotropic Gaussian, z = U α (z 0 ) and z 0 ∼ N (0, I d ). U α (•) is an expressive neural network, such as the UNet <ref type="bibr" target="#b28">(Ronneberger et al., 2015)</ref>. This approach is inspired by, yet contrasts with <ref type="bibr" target="#b26">Pang et al. (2020)</ref>, wherein the latent space prior is modeled as an Energy-based Model (EBM) <ref type="bibr" target="#b35">(Xie et al., 2016)</ref>.</p><p>While EBM offers explicit unnormalized density, its sampling process is complex. Conversely, our model provides an implicit density with simpler sampling.  <ref type="bibr" target="#b2">(Chen et al., 2021)</ref>. Specifically, the latent variable z is included in trajectory generation using crossattention, as shown in Fig. <ref type="figure">1</ref> and controls each step of the autoregressive trajectory generation as p β (a t |s t-K:t , a t-K:t-1 , z). The action is assumed to follow a single-mode Gaussian distribution, i.e. a t ∼ N (g β (s t-K:t , a t-K:t-1 , z), I |A| ).</p><p>The return predictor is a non-linear regression on the latent trajectory variable z, modeled as p γ (y|z) = N (r γ (z), σ 2 ). It directly predicts the final return from the latent variable z. The function r γ (z) is a small multi-layer perceptron (MLP) that estimates y based on z. The variance σ 2 , is treated as the hyper-parameter in our setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Offline Learning</head><p>With a set of offline training examples {(τ i , y i )} n i=1 , we aim to learn Latent Plan Transformer (LPT) through maximum likelihood estimation (MLE). The log-likelihood function is defined as</p><formula xml:id="formula_4">L(θ) = n i=1 log p θ (τ i , y i ).</formula><p>The joint probability of the trajectory and final return is</p><formula xml:id="formula_5">p θ (τ, y) = p β (τ |z = U α (z 0 ))p γ (y|z = U α (z 0 ))p 0 (z 0 )dz 0 ,<label>(3)</label></formula><p>where p 0 (z 0 ) = N (0, I d ). The learning gradient of log-likelihood can be calculated according to</p><formula xml:id="formula_6">∇ θ log p θ (τ, y) = E p θ (z0|τ,y) [∇ θ log p β (τ |U α (z 0 )) + ∇ θ log p γ (y|U α (z 0 ))].<label>(4)</label></formula><p>The full derivation of the learning method is in Appendix A.1. Let δ α , δ β , δ γ represent the expected gradients of L(θ) with respect to the model parameters α, β, γ, respectively. The learning gradients for each component are formulated as follows.</p><p>For the prior model p α (z),</p><formula xml:id="formula_7">δ α (τ, y) = E p θ (z0|τ,y) [∇ α (log p β (τ |z = U α (z 0 )) + ∇ α log p γ (y|z = U α (z 0 ))].</formula><p>For the trajectory generator,</p><formula xml:id="formula_8">δ β (τ, y) = E p θ (z0|τ,y) [∇ β log p β (τ |z = U α (z 0 ))],</formula><p>For the return predictor,</p><formula xml:id="formula_9">δ γ (τ, y) = E p θ (z0|τ,y) [∇ γ log p γ (y|z = U α (z 0 ))].</formula><p>Estimating these expectations requires Markov Chain Monte Carlo (MCMC) sampling of the posterior distribution p θ (z 0 |τ, y). We use the Langevin dynamics (Neal, 2011) for MCMC sampling, iterating as follows for a target distribution π(z):</p><formula xml:id="formula_10">z k+1 = z k + s∇ z log π(z k ) + √ 2sϵ k ,<label>(5)</label></formula><p>where k indexes the time step of the Langevin dynamics, s is the step size, and ϵ k ∼ N (0, I d ) is the Gaussian white noise. Here, π(z) is instantiated as the posterior distribution p θ (z 0 |τ, y). We have p θ (z 0 |τ, y) ∝ p 0 (z 0 )p γ (y|z)p β (τ |z), where z = U α (z 0 ), such that the gradient is</p><formula xml:id="formula_11">∇ z0 log p θ (z 0 |τ, y) = ∇ z0 log p 0 (z 0 ) prior +∇ z0 log p γ (y|z) return prediction + H t=1 ∇ z0 log p β (τ (t) |τ (t-K:t-1) , z)</formula><p>aggregating finite-context sub-trajectories .</p><p>This demonstrates that the posterior inference of z is an explicit process of optimizing a plan given its likelihood. In the presence of a finite context, p β (τ |z) parametrized with Transformer can only account for sub-trajectories with a maximum length of K. The latent variable z serves as an abstraction that integrates information from both the final return and sub-trajectories using gradients.</p><p>The sampling process starts by initializing z k=0 0 from a standard normal distribution N (0, I d ). We then apply N steps of Langevin dynamics (e.g., N = 15) to approximate the posterior distribution, making our learning algorithm an approximate MLE. For a theoretical understanding of this noiseinitialized finite-step MCMC, see <ref type="bibr" target="#b26">Pang et al. (2020)</ref>; <ref type="bibr" target="#b23">Nijkamp et al. (2020)</ref>; <ref type="bibr" target="#b36">Xie et al. (2023)</ref>. However, for large horizons (e.g.,H=1000), this method becomes slow and memory-intensive. To mitigate this, we adopt the persistent Markov Chain (PMC) <ref type="bibr" target="#b32">(Tieleman, 2008;</ref><ref type="bibr" target="#b35">Xie et al., 2016;</ref><ref type="bibr" target="#b10">Han et al., 2017)</ref>, which amortizes sampling across training iterations. During training, z k=0 0 is initialized from the previous iteration and the number of updates is reduced to N = 2 steps. See Appendix A.2 for training and architecture details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Planning as Inference</head><p>The MLE learning of LPT gives us an agent that can plan. During testing, we first infer the latent z 0 given the desired return y using Bayes' rule,</p><formula xml:id="formula_12">z 0 ∼ p θ (z 0 |y) ∝ p 0 (z 0 )p γ (y|z = U α (z 0 )).<label>(6)</label></formula><p>This posterior sampling is achieved using Langevin dynamics similar to the training process. Specifically, we replace the target distribution in Eq. ( <ref type="formula" target="#formula_10">5</ref>) with p θ (z 0 |y) and run MCMC for a fixed number of steps. Sampling from p θ (z 0 |y) eliminates the need for expensive back-propagation through the trajectory generator p β (τ |z).</p><p>This posterior sampling of p(z 0 |y) is an explicit process that iteratively refines the latent plan z, increasing its likelihood given the desired final return. It aligns with our intuition that planning is an inference process. This inferred z, fixed ahead of the policy execution, effectively serves as a plan. At each step, the agent consults this plan to generate actions conditioned on the current state and recent history,</p><formula xml:id="formula_13">a t ∼ p β (a t |s t-K:t-1 , a t-K:t-1 , z = U α (z 0 )).</formula><p>Once a decision is made, the environment's (possibly non-Markovian) transition s t+1 ∼ p(s t+1 |a t , s t ) emits the next state. This sequential decision-making process iterates the sampling of s t and a t until termination at the horizon.</p><p>Exploitation-inclined Inference (EI) Inspired by the classifier guidance (CG) <ref type="bibr" target="#b4">(Dhariwal and Nichol, 2021;</ref><ref type="bibr" target="#b11">Ho and Salimans, 2022)</ref> in conditional diffusion models, we introduce a guidance weight w to the original posterior in Eq. ( <ref type="formula" target="#formula_12">6</ref>)</p><formula xml:id="formula_14">pθ (z 0 |y) ∝ p 0 (z 0 )p γ (y|z) w , z = U α (z 0 ),<label>(7)</label></formula><p>which has the score ∇ z0 log pθ (z 0 |y) = ∇ z0 log p 0 (z 0 ) + w∇ z0 log p γ (y|z). This guidance weight w controls the interpolation between exploration and exploitation. When w = 1, the sampled plans collectively represent the posterior density and account for Bayesian uncertainty, resulting in a provably efficient exploration scheme <ref type="bibr" target="#b24">(Osband and Van Roy, 2017)</ref>. When w &gt; 1, the sampled plans are more concentrated around the modes of the posterior distribution, which are plans more likely to the agent. The larger the value of w, the more confident the agent becomes, and the stronger the inclination towards exploitation.</p><p>An overview of the algorithms for both offline learning and inference can be found in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Offline learning</head><p>Input: Learning iterations T , initial parameters θ 0 = (α 0 , β 0 , γ 0 ), offline training samples D = {τ i , y i } n i=1 , posterior sampling step size s, the number of steps N , and the learning rate η 0 , η 1 , η 2 . Output: θ T for t = 1 to T do 1.Posterior sampling: For each (τ i , y i ), sample z 0 ∼ p θt (z 0 |τ i , y i ) using Eq. ( <ref type="formula" target="#formula_10">5</ref>) with N steps and step-size s, where the target distribution π is p θt (z 0 |τ i , y i ).</p><p>2.Learn prior model p α (z), trajectory generator p β (τ |z) and return predictor p γ (y|z): Sample z 0 ∼ pθ (z 0 |y) as in Eq. ( <ref type="formula" target="#formula_14">7</ref>) using Eq. ( <ref type="formula" target="#formula_10">5</ref>) with N steps and step size s, where the target distribution π is replaced by pθ (z 0 |y) ∝ p 0 (z 0 )p γ (y|z = U α (z 0 )) w and z = U α (z 0 ). else Sample z 0 ∼ p θ (z 0 |y) as in Eq. ( <ref type="formula" target="#formula_12">6</ref>) using Eq. ( <ref type="formula" target="#formula_10">5</ref>) with N steps and step size s, where π is replaced by p θ (z 0 |y) ∝ p 0 (z 0 )p γ (y|z = U α (z 0 )) and z = U α (z 0 ). end if while current time step t ≤ H do Sample a t using trajectory generator as a t ∼ p β (a t |s t-K:t-1 , a t-K:t-1 , z = U α (z 0 )).</p><formula xml:id="formula_15">α t+1 = α t +η 0 1 n i δ α (τ i , y i ); β t+1 = β t +η 1 1 n i δ β (τ i , y i ); γ t+1 = γ t +η 2 1 n i δ γ (τ i , y i ) as in Section 3.2.</formula><p>Once a decision is made, the environment's transition s t+1 ∼ p(s t+1 |a t , s t ) emits the next state. end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A Sequential Decision-Making Perspective</head><p>We approach the sequential decision-making problem with techniques from generative modeling. In particular, our data specification of trajectory-return pairs omits step-wise rewards, based on the belief that the step-wise reward function is only a proxy of the trajectory return. However, step-wise rewards are indispensable input to classical decision-making algorithms. Accumulating the rewards from the current step to the future gives us the RT G, which naturally hints the future progress of the trajectory. How is temporal consistency enforced in our model without the assistance of the RT Gs?  .</p><p>However, behavior cloning is believed to suffer from drifting errors since it ignores covariate shifts in future steps <ref type="bibr" target="#b29">(Ross and Bagnell, 2010)</ref>. This concern is unique to sequential decision-making, as the agent cannot control the next state from a stochastic environment, like generating the next text token.</p><p>This temporal consistency issue could be alleviated by additionally modeling the sequence of RT G.</p><formula xml:id="formula_16">Denote ρ = (RT G 0 , RT G 1 , ...RT G H ).</formula><p>Modeling the joint distribution is to minimize</p><formula xml:id="formula_17">D KL (p y D (τ, ρ)∥p y θ (τ, ρ)) = D KL (p y D (τ )∥p y θ (τ )) + D KL (p y D (ρ|τ )∥p y θ (ρ|τ )) =D KL (p y D,AR (τ )∥p y θ,AR (τ )) + E p y D (τ ) [ H t=1 D KL (p y D (RT G t |τ )∥p y θ (RT G t |τ )) RTG prediction ].<label>(8)</label></formula><p>Note that the RTG prediction term is conditioned on the entire trajectory, including the future steps. Minimizing this additional KL divergence correlates predicted RT Gs with hindsight trajectory-to-go.</p><p>Our modeling of the latent trajectory variable z provides an alternative solution to the temporal consistency issue. Eq. ( <ref type="formula" target="#formula_6">4</ref>) is minimizing the KL divergence</p><formula xml:id="formula_18">D KL (p y D (τ, z)∥p y θ (τ, z)) = D KL (p y D (τ )∥p y θ (τ )) + D KL (p y θ (z|τ )∥p y θ (z|τ )) =D KL (p y D,AR (τ )∥p y θ,AR (τ )) + E p y D (τ ) [D KL (p y θ (z|τ )∥p y θ (z|τ )) plan prediction ],<label>(9)</label></formula><p>where p y θ (z|τ ) = p y D (τ, z)/p y D (τ ) and θ = θ highlights these distributions have the same parameterization as p y θ but are wrapped with stop_grad() operator when calculating gradients for θ <ref type="bibr" target="#b10">(Han et al., 2017)</ref>. Comparing Eqs. ( <ref type="formula" target="#formula_17">8</ref>) and ( <ref type="formula" target="#formula_18">9</ref>), it is now clear that z plays a similar role as RT G in promoting temporal consistency in autoregressive models. Uniquely, p y θ (z|τ ) is the temporal abstraction intrinsic to the model, in contrast to step-wise rewards. From a sequential decision-making perspective, z is effectively a plan that the agent is persistent to. From a generative modeling perspective, z from different trajectory modes would decompose the density p y (a t |s 0:t , a 0:t-1 ), relieving the burden of learning the autoregressive policy p β (a t |s 0:t , a 0:t-1 , z).</p><p>One caveat is that the transition model estimation should not be conditioned on y. Mixing up more trajectory regimes could provide additional regularization for its estimation and generalization. Actually, environment stochasticity is a more concerning issue for autoregressive behavior cloning, as highlighted by <ref type="bibr" target="#b19">Yang et al. (2022)</ref>  <ref type="formula">2022</ref>). Among them, <ref type="bibr" target="#b19">Yang et al. (2022)</ref> pinpoints the issue by viewing RT Gs as deterministic latent trajectory variables, closely related to what we present here. Uniquely, the latent variable z in our model is inherently multi-modal (hence very non-deterministic) and ignorant of step-wise rewards. We postulate that the overfitting issue might be mitigated. This is validated by our empirical study inspired by <ref type="bibr" target="#b27">Paster et al. (2022)</ref>.</p><p>Although RTG prediction and plan prediction both promote temporal consistency, they function very differently when mixing trajectories from multiple return-conditioned regimes. RTG prediction is a supervised learning over the joint distribution p D (τ, ρ). Simply mixing trajectories from multiple regimes can't encourage generalization to trajectories that are stitched with those in the dataset. <ref type="bibr" target="#b38">Yamagata et al. (2023)</ref> propose to resolve this by replacing RT G with RT G Q . Intuitively, this augments the distribution p D (τ, ρ) with p D (τ ′ , ρ Q ), where τ ′ denotes trajectories covered by the offline dynamic programming, such as Q learning, and</p><formula xml:id="formula_19">ρ Q = (RT G Q 0 , RT G Q 1 , ...Q H ).</formula><p>It significantly improves tasks requiring trajectory stitching. Conversely, plan prediction is an unsupervised learning as it samples from p D (τ, y)pθ(z|τ, y). As z contains more trajectory-related information than step-wise RT Gs, trajectories lying outside of p D (τ, ρ) may be in-distribution for p D (τ, y)pθ(z|τ, y). The return prediction training further shapes the representation of z, which can be benefited from denser coverage of y. With more return values covered, we may count on neural networks' strong interpolation capability to shift the trajectory distribution with y-conditioning.  <ref type="formula">2022</ref>) explore diffusion models <ref type="bibr" target="#b12">(Ho et al., 2020)</ref> as an alternative generative model family for decision-making. Our model differentiates from all above in data specification and model formulation. <ref type="bibr" target="#b19">Yang et al. (2022)</ref>; <ref type="bibr" target="#b27">Paster et al. (2022)</ref> investigate the DT's overfitting to environment contingencies and propose latent variable solutions. Our model is closely related to theirs but unique in an EM-style algorithm for MLE. <ref type="bibr">Ajay et al. (2021)</ref>; <ref type="bibr" target="#b21">Lynch et al. (2020)</ref> propose latent variable models to make Markovian policies temporally extended. Their models are more related to VAE <ref type="bibr" target="#b15">(Kingma and Welling, 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Trajectory Variables in Behavior Cloning</head><p>Offline Reinforcement Learning Since the offline static datasets only partially cover the state transition spaces, efforts from a conventional RL perspective focus on imposing pessimistic biases to value iteration <ref type="bibr" target="#b18">(Kumar et al., 2020;</ref><ref type="bibr" target="#b5">Kostrikov et al., 2021;</ref><ref type="bibr">Uehara and Sun, 2021;</ref><ref type="bibr" target="#b37">Xie et al., 2021;</ref><ref type="bibr" target="#b3">Cheng et al., 2022)</ref>. <ref type="bibr" target="#b9">Fujimoto and Gu (2021)</ref> show that simply augmenting value-based methods with behavior cloning achieves impressive performance. <ref type="bibr" target="#b5">Emmons et al. (2021)</ref> report that supervised learning on return-conditioned policies is competitive to value-based methods in offline RL. Our MLE objective is more related to the supervised learning methods. The latent variable inference further imposes temporal consistency, acting as a replacement of value iteration.</p><p>Hierarchical RL Methods like OPAL <ref type="bibr">(Ajay et al., 2021)</ref>, OPOSM <ref type="bibr" target="#b7">(Freed et al., 2023)</ref> address TDlearning's limitations in long-range credit assignment using a two-stage approach: discovering skills from shorter subsequences to reduce the planning horizon, then applying skill-level CQL or online model-based planning on the reduced horizons. This paper focuses on comparing various methods for long-range credit assignment on the original horizon. Future work includes first discovering skills and then modeling them with a skill-level LPT to further extend the effective horizon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>The data specification of trajectory-return pairs distinguishes our empirical study from most existing works in offline RL. Omitting step-wise rewards naturally increases the challenges in decision-making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Overview</head><p>Our empirical study adopts the convention from offline RL. We first train our model with the offline data and then test it as an agent in the corresponding task. More training details and ablation studies of LPT can be found in Appendices A.2 and A.4.</p><p>OpenAI Gym-Mujoco The D4RL offline RL dataset <ref type="bibr" target="#b8">(Fu et al., 2020)</ref> features densely-rewarded locomotion tasks including Halfcheetah, Hopper, and Walker2D. We test for medium and mediumreplay. It also includes Antmaze, a locomotion and goal-reaching task with extremely sparse reward.</p><p>The agent will only receive a reward of 1 if hitting the target location and 0 otherwise. We use its umaze and umaze-diverse variants.</p><p>Franka Kitchen Franka Kitchen is a multitask environment where a Franka robot with nine degrees of freedom operates within a kitchen setting, interacting with household objects to achieve specific configurations. Our experiments focus on two datasets of the environment: mixed, and partial, which consists of non-task-directed demonstrations and partially task-directed demonstrations respectively. Maze2D Maze2D is a navigation task in which the agent reaches a fixed goal location from random starting positions. The agent is rewarded 1 point when it is around the goal. Experiments are conducted on three layouts: umaze, medium, and large, with increasing complexity. The training data of the Maze2D task contains only suboptimal trajectories from and to randomly selected locations.</p><p>Connect Four This is a tile-based game, where the agent plays against a stochastic opponent <ref type="bibr" target="#b27">(Paster et al., 2022)</ref>, receiving at the end of an episode 1 reward for winning, 0 for a draw, and -1 for losing.</p><p>Baselines We compare the performance of LPT with several representative baselines including CQL <ref type="bibr" target="#b18">(Kumar et al., 2020)</ref>, DT <ref type="bibr" target="#b2">(Chen et al., 2021)</ref> and QDT <ref type="bibr" target="#b38">(Yamagata et al., 2023)</ref>. CQL baseline results are obtained from <ref type="bibr" target="#b18">Kumar et al. (2020)</ref>. QDT baseline results are from <ref type="bibr" target="#b38">Yamagata et al. (2023)</ref>. The DT results for Gym-Mujoco and Maze2D tasks are from <ref type="bibr" target="#b38">Yamagata et al. (2023)</ref>, Antmaze from <ref type="bibr" target="#b41">Zheng et al. (2022)</ref>, and Kitchen implemented based on the published source code. CQL and DT results in the Connect Four experiments are from <ref type="bibr" target="#b27">Paster et al. (2022)</ref>. The mean and standard deviation of our model, shown as LPT and LPT-EI, are reported over 5 seeds.</p><p>Table <ref type="table">1</ref>: Evaluation results of offline OpenAI Gym MuJoCo tasks. We provide results for data specification with step-wise reward (left) and final return (right). Bold highlighting indicates top scores. LPT outperforms all final-return baselines and most step-wise-reward baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Step </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Credit assignment</head><p>When resolving the temporal consistency issue, our model doesn't have an explicit credit assignment mechanism that accounts for the actual contribution of each step. It is not aware of the step-wise rewards either. We are therefore curious about whether the inferred latent variable z can effectively assign fair credits to resolve compounding errors.</p><p>Distributing sparse rewards to high-dimensional actions The Gym-Mujoco environment was a standard testbed for high-dimensional continuous control during the development of modern RL algorithms <ref type="bibr" target="#b20">(Lillicrap et al., 2015)</ref>. In this environment, step-wise rewards were believed to be critical for TD learning methods. In the setup of offline RL, <ref type="bibr" target="#b2">Chen et al. (2021)</ref> reported the failure of the competitive CQL baseline when delaying step-wise rewards until the end of the trajectories. DT and QDT are reported to be robust to this alternation. As shown in Table <ref type="table">1</ref>, the proposed model, LPT, outperforms these baselines when the data specifications are the same. Notably, LPT even excels in most of the control tasks when compared with the baselines with step-wise rewards.</p><p>Distributing delayed rewards to long-range sequences Maze navigation tasks with fully delayed rewards align with our intuition of a planning problem, for it involves decision-making at certain critical states absent of instantaneous feedback. An ideal planner would take in the expected total return and calculate the sequential decisions, automatically distributing credits from the extremely sparse and fully delayed rewards. According to <ref type="bibr" target="#b38">Yamagata et al. (2023)</ref>, DT fails in these tasks. Our proposed model LPT outperforms QDT by a large margin in all three variants of the maze task. These results validate our hypothesis that the additional plan prediction KL imposes temporal consistency on autoregressive policies.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Trajectory stitching</head><p>In addition to credit assignment, the setup of offline RL further presents a challenge, trajectory stitching <ref type="bibr" target="#b8">(Fu et al., 2020)</ref>, which articulates the problem of shifting the trajectory distribution towards sparsely covered regimes with higher returns. In the Franka Kitchen environment, both the mixed, and partial datasets contain undirected data where the robot executes subtasks that do not necessarily achieve the goal configuration. The "mixed" dataset contains no complete solution trajectories, necessitating that the agent learn to piece together relevant sub-trajectories. A similar setting happens in Maze2D domain. Taking Maze2D-medium as an example, in the training set, the average return of all trajectories is 3.98 with a standard deviation of 10.44, where the max return is 47. DT's score is only marginally above the average return.   To probe into the agent's understanding of trajectories' returns, we visualize the representation space of the latent variables. The left of Fig. <ref type="figure" target="#fig_6">3</ref> is the aggregated posterior distribution of z 0 . We can see that z 0 infered from p θ (z 0 |y) are distant away from the training population. The agent understands they are not very likely in the training set. The right of Fig. <ref type="figure" target="#fig_6">3</ref> is the distribution of z, which is transformed from z 0 with the UNet, z = U α (z 0 ). We observe that zs from the generated trajectories become "in-distribution" in the sense that some of them are mingled into the training population and the remaining lie inside a region coverable through linear interpolation of training samples. The agent understands what trajectories to generate even if they are unlikely among what it has seen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Environment contingencies</head><p>To live in a stochastic world, contingent planning that is adaptable to unforeseen noises is desirable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitation</head><p>We omit the Antmaze-large experiment from the main text and included potential reasons for LPT's unsatisfactory performance in Appendix A.3. Another interesting direction is to study LPT's continual learning potential. During planning, LPT explores with provably efficient posterior sampling <ref type="bibr" target="#b25">(Osband et al., 2013;</ref><ref type="bibr" target="#b24">Osband and Van Roy, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Summary</head><p>We study generative modeling for planning in the absence of step-wise rewards. We propose LPT which generates trajectory and return from a latent variable. In learning, posterior sampling of the latent variable naturally gathers sub-trajectories to form an episode-wise abstraction despite finite context in training. In inference, the posterior sampling given the target final return explores the optimal regime of the latent space. It produces a latent variable that guides the autoregressive policy to execute consistently. Across diverse evaluations, LPT demonstrates competitive capacities of nuanced credit assignments, trajectory stitching, and adaptation to environmental contingencies. Contemporary work extends LPT's application to online molecule design <ref type="bibr">(Kong et al., 2024)</ref>. Future research directions include studying online and multi-agent variants of this model, exploring its application in real-world robotics, and investigating its potential in embodied agents. As a reference, Fig. <ref type="figure">5</ref> shows the distributions of final returns and the trajectory lengths of Maze2Dlarge, a task where LPT performs well. It is important to note that TD-learning methods, such as CQL and QDT, rely solely on (s, a, s ′ , r) tuples and are less affected by the trajectory length distribution in the dataset. Consequently, Antmaze-large in D4RL remains a fair dataset for these methods to perform offline RL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Planning as inferenceInput: Expected return y, a trained model on offline dataset θ, posterior sampling step size s and the number of steps N , Horizon H and an evaluation environment. Output: Trajectory τ if Exploitation-inclined Inference (EI) then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Without loss of generality, consider the trajectory distribution conditioned on a single return value y. The MLE objective is equivalent to minimizing the KL divergence between the data distribution and model distribution, D KL (p y D (τ )∥p y θ (τ )). Here, p D denotes the data distribution and p θ denotes the model distribution. MLE upon autoregressive modeling imposes additional inductive biases by transforming the objective to D KL (p y D,AR (τ )∥p y θ,AR (τ )), which is reduced to next-token prediction for behavior cloning and transition model estimation: H t=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>; Paster et al. (2022); Štrupl et al. (2022); Brandfonbrener et al. (2022); Villaflor et al. (2022); Eysenbach et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>via Sequence Modeling Chen et al. (2021) propose Decision Transformer (DT), pioneering this paradigm shift. Concurrently, Janner et al. (2021) explore beam search upon the learned Transformer for model-based planning and inspired later work that searches over the latent state space (Zhang et al., 2022). Lee et al. (2022) report DT's capability in multi-task setting. Zheng et al. (2022) explore the online extension of DT. Yamagata et al. (2023) augment the Monte Carlo RTG in DT with a Q function and show improvement in tasks requiring trajectory stitching. Janner et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 2: (a) Maze2D-medium environment (b) Maze2D-large environment. Left panels show example trajectories from the training set and right panels show LPT generations. Yellow stars represent the goal states.</figDesc><graphic coords="9,108.73,279.28,166.32,88.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc><ref type="bibr" target="#b38">Yamagata et al. (2023)</ref> attribute DT's failure in Maze2D to its difficulty with trajectory stitching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3</head><label>3</label><figDesc>Figure 3: t-SNE plot of latent variables in the Maze2D-medium. Left: Training z 0 from aggregated posterior E p D (τ,y) [p θ (z 0 |τ, y)]. Testing z 0 from p θ (z 0 |y), disjoint from training population. Right: Distribution of z = U α (z 0 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 visualizes samples from the training data and successful trajectories in testing. The left panels show that trajectories in training are suboptimal in terms of (1) being short in length and (2) containing very few goal-reaching instances. Trajectories on the right are generated by 10 random runs with LPT, where the agent successfully navigates to the end goal from random starting positions in an effective manner. This indicates that the agent can discover the correlation between different ys to facilitate such stitching.</figDesc><graphic coords="9,108.00,566.42,198.00,90.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Trajectory length and return distribution in dataset Antmaze-large-diverse</figDesc><graphic coords="15,187.20,72.00,237.61,179.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Overview of Latent Plan Transformer (LPT). z ∈ R d is the latent vector. The prior distribution of z is a neural transformation of z 0 , i.e., z = U α (z 0 ), z 0 ∼ N (0, I d ). Given z, τ and y are independent. p β (τ |z) is the trajectory generator. p γ (y|z) is the return predictor. Right: Illustration of trajectory generator p β (τ |z).</figDesc><table><row><cell>3.1 Model</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>a1, a2, . . . , • • • , aH</cell></row><row><cell>z 0</cell><cell></cell><cell>×N</cell></row><row><cell></cell><cell></cell><cell>Cross-attention</cell></row><row><cell>z = Uα(z0)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>z</cell><cell></cell></row><row><cell>z</cell><cell>pα(z)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Causal Transformer</cell></row><row><cell>τ</cell><cell>y</cell><cell></cell></row><row><cell>pβ(τ |z)</cell><cell>pγ(y|z)</cell><cell>τ = (s1, a1, s2, a2, . . . , sH , aH )</cell></row><row><cell>Figure 1: Left:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>D</head><label></label><figDesc>KL (p y D (a t |s 1:t ,a 1:t-1 )∥p y θ (a t |s 1:t ,a 1:t-1 )) KL (p y D (s t+1 |s 1:t ,a 1:t )∥p y θ (s t+1 |s 1:t ,a 1:t ))</figDesc><table><row><cell></cell><cell></cell><cell>H</cell><cell></cell></row><row><cell>behavior cloning</cell><cell>+</cell><cell>t=1</cell><cell>D transition model estimation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results of Maze2D tasks. Bold highlighting indicates top scores.</figDesc><table><row><cell>Dataset</cell><cell>CQL</cell><cell>DT</cell><cell>QDT</cell><cell>LPT</cell><cell>LPT-EI</cell></row><row><cell>Maze2D-umaze</cell><cell>5.7</cell><cell cols="4">31.0 ± 21.3 57.3 ± 8.2 65.43 ± 2.91 70.57 ± 1.39</cell></row><row><cell>Maze2D-medium</cell><cell>5.0</cell><cell>8.2 ± 4.4</cell><cell cols="3">13.3 ± 5.6 20.62 ± 1.81 26.66 ± 0.74</cell></row><row><cell>Maze2D-large</cell><cell>12.5</cell><cell>2.3 ± 0.9</cell><cell cols="3">31.0 ± 19.8 37.21 ± 2.05 45.89 ± 2.98</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results of Antmaze tasks. Bold highlighting indicates top scores.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc><ref type="bibr" target="#b27">Paster et al. (2022)</ref>;<ref type="bibr" target="#b19">Yang et al. (2022)</ref> discover that DT's performance would degrade in stochastic environments due to inevitable overfitting towards contingencies. We examine LPT and other baselines in Connect Four from<ref type="bibr" target="#b27">Paster et al. (2022)</ref>. Connect Four is a two-player game, where the opponent will make adversarial moves to deliberately disturb an agent's plan. According to the empirical study from<ref type="bibr" target="#b27">Paster et al. (2022)</ref>, the degradation of DT is more significant than in stochastic Gym tasks from<ref type="bibr" target="#b19">Yang et al. (2022)</ref>. As shown in Table4, LPT achieves the highest score with minimal variance. The ESPER baseline is from<ref type="bibr" target="#b27">Paster et al. (2022)</ref>, which is very relevant to LPT as it is also a latent variable model. ESPER learns the latent variable model with an adversarial loss. It further adds a clustering loss in the latent space. LPT's on-par performance may justify that MLE upon a more flexible prior can play an equal role. Evaluation results on Connect Four. Bold highlighting indicates top scores.Connect Four 0.61 ± 0.05 0.8 ± 0.07 0.99 ± 0.03 0.99 ± 0.01</figDesc><table><row><cell>Dataset</cell><cell>CQL</cell><cell>DT</cell><cell>ESPER</cell><cell>LPT</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The work was partially supported by <rs type="funder">NSF</rs> <rs type="grantNumber">DMS-2015577</rs>, <rs type="funder">NSF</rs> <rs type="grantNumber">DMS-2415226</rs>, and a gift fund from <rs type="funder">Amazon</rs>. We sincerely thank <rs type="person">Mr. Shanwei Mu</rs> and <rs type="person">Dr. Jiajun Lu</rs> at <rs type="affiliation">Akool Research</rs> for their computational support, as well as the anonymous reviewers for their valuable feedback.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kZjVdwB">
					<idno type="grant-number">DMS-2015577</idno>
				</org>
				<org type="funding" xml:id="_2pHGCV2">
					<idno type="grant-number">DMS-2415226</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Details about model and learning Given a trajectory τ , z ∈ R d is the latent vector to represent the variable-length trajectory. y ∈ R is the return of the trajectory. With offline training trajectory-return pairs {(τ i , y i ), i = 1, ..., n}. The log-likelihood function is L(θ) = n i=1 log p θ (τ i , y i ), with learning gradient ∇ θ L(θ) = n i=1 ∇ θ log p θ (τ i , y i ). We derive the form of ∇ θ log p θ (τ i , y i ), proving Eq. (4) below, dropping index subscript i for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training details</head><p>For Gym-Mujoco offline training, as shown in Table <ref type="table">5</ref>, most of the hyperparameters were shared across all tasks except context length and hidden size. However, due to the significant variations in the scale of the maze maps and the lengths of the trajectories within the Maze2D environments-spanning umaze, medium, and large categories-model sizes were adjusted accordingly to accommodate these differences, where the detailed setting can be found in Table <ref type="table">6</ref>. We also show the parameters for Franka Kitchen environment in Table <ref type="table">7</ref> and Connect Four in Table <ref type="table">8</ref>.</p><p>Training time for the Gym-Mujoco tasks using a single Nvidia A6000 GPU is 18 hours on average. We train Maze2d tasks using a single Nvidia A100 GPU using 30 hours on average. Kitchen tasks using a single Nvidia A6000 GPU takes 60 hours on average. Connect-4 on a single Nvidia A6000 GPU takes 10 hours. In our experiments, we encounter a curious phenomenon that LPT outperforms CQL, DT and QDT in Antmaze-umaze by a large margin but falls behind in Antmaze-large. Upon closer examination of the data from D4RL, we gained valuable insights into the potential reasons behind LPT's performance on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Ablation study</head><p>We investigate the role of the expressive prior p α (z) in our Latent Plan Transformer (LPT) model by removing the UNet component, which transforms z 0 from a non-informative Gaussian distribution. Table <ref type="table">9</ref> reports the results on three Gym-Mujoco tasks and Connect Four. We observe that the performance of LPT drops in all environments when the UNet is removed. For example, in the stochastic environment Connect Four, LPT's performance decreases from 0.99 to 0.90, while the baseline Decision Transformer (DT) without latent variables achieves 0.80. These results indicate that a more flexible prior benefits the learning and inference of LPT. Our results underscore the crucial role of the learned prior in LPT's performance. The original UNet configuration achieves the highest normalized score, indicating that our current UNet design is optimal among the variants tested. We appreciate the reviewer's suggestion, as it prompted us to perform a more detailed analysis of the prior's impact on LPT.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Opal: Offline primitive discovery for accelerating offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Ajay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">When does return-conditioned supervised learning work for offline reinforcement learning?</title>
		<author>
			<persName><forename type="first">David</forename><surname>Brandfonbrener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Bietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Laroche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1542" to="1553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Decision transformer: Reinforcement learning via sequence modeling</title>
		<author>
			<persName><forename type="first">Lili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15084" to="15097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adversarially trained actor critic for offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Ching-An</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3852" to="3878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rvs: What is essential for offline rl via supervised learning</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Emmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Eysenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10751</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imitating past successes can be very suboptimal</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Eysenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Udatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6047" to="6059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning temporally abstractworld models without online experimentation</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Freed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddarth</forename><surname>Venkatraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Sartoretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howie</forename><surname>Choset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10338" to="10356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07219</idno>
		<title level="m">D4rl: Datasets for deep data-driven reinforcement learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A minimalist approach to offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><forename type="middle">Shane</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021">20132-20145, 2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Alternating back-propagation for generator network</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1976" to="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.12598</idno>
		<title level="m">Classifier-free diffusion guidance</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Offline reinforcement learning as one big sequence modeling problem</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1273" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Planning with diffusion for flexible behavior synthesis</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9902" to="9915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, (ICLR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Molecule design by latent prompt transformer</title>
		<author>
			<persName><forename type="first">Deqian</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouardo</forename><surname>Honig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuanghong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Offline reinforcement learning with implicit q-learning</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashvin</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conservative q-learning for offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1179" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-game decision transformers</title>
		<author>
			<persName><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Mengjiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winnie</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><surname>Michalewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27921" to="27936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Timothy P Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning latent plans from play</title>
		<author>
			<persName><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohi</forename><surname>Khansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning (CoRL)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1113" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">MCMC using hamiltonian dynamics. Handbook of Markov Chain</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">Monte Carlo, 2, 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning multi-layer latent variable model via variational optimization of short run mcmc for approximate inference</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="361" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Why is posterior sampling better than optimism for reinforcement learning</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2701" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">(more) efficient reinforcement learning via posterior sampling</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning latent space energy-based prior model</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">You can&apos;t count on luck: Why decision transformers and rvs fail in stochastic environments</title>
		<author>
			<persName><forename type="first">Keiran</forename><surname>Paster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><surname>Mcilraith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="38966" to="38979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient reductions for imitation learning</title>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="661" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Upside-down reinforcement learning can diverge in stochastic environments with episodic resets</title>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Štrupl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Faccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><forename type="middle">R</forename><surname>Ashley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.06595</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Training restricted boltzmann machines using approximations to the likelihood gradient</title>
		<author>
			<persName><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1064" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pessimistic model-based offline reinforcement learning under partial coverage</title>
		<author>
			<persName><forename type="first">Masatoshi</forename><surname>Uehara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Addressing optimism bias in sequence modeling for reinforcement learning</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Adam R Villaflor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swapnil</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="22270" to="22283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A theory of generative convnet</title>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2635" to="2644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A tale of two latent flows: Learning latent space normalizing flow with short-run langevin flow for approximate inference</title>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaxuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10499" to="10509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bellman-consistent pessimism for offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Tengyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-An</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Mineiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6683" to="6694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Q-learning decision transformer: Leveraging dynamic programming for conditional sequence modelling in offline rl</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Yamagata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Santos-Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="38989" to="39007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dichotomy of control: Separating what you can control from what you cannot</title>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient planning in a compact latent action space</title>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Online decision transformer</title>
		<author>
			<persName><forename type="first">Qinqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="27042" to="27059" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
