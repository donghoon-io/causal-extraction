<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Physically Consistent Global Atmospheric Data Assimilation with Machine Learning in Latent Space</title>
				<funder>
					<orgName type="full">USMILE European Research Council</orgName>
				</funder>
				<funder ref="#_XtA46r2">
					<orgName type="full">National Science Foundation (NSF) Science and Technology Center (STC) Learning the Earth with Artificial Intelligence and Physics</orgName>
					<orgName type="abbreviated">LEAP</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-07-08">8 Jul 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hang</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Earth and Environmental Engineering</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Learning the Earth with Artificial intelligence and Physics (LEAP) Center</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Nanjing University of Information Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Bai</surname></persName>
							<email>bailei@pjlab.org.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Fei</surname></persName>
							<email>benfei@cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yubao</forename><surname>Liu</surname></persName>
							<email>ybliu@nuist.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">Nanjing University of Information Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Yongquan Qu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Earth and Environmental Engineering</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Learning the Earth with Artificial intelligence and Physics (LEAP) Center</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fenghua</forename><surname>Ling</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Gentine</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Earth and Environmental Engineering</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Learning the Earth with Artificial intelligence and Physics (LEAP) Center</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Physically Consistent Global Atmospheric Data Assimilation with Machine Learning in Latent Space</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-07-08">8 Jul 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2502.02884v2[physics.ao-ph]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data assimilation (DA) integrates observations with model forecasts to produce optimized atmospheric states, whose physical consistency is critical for stable weather forecasting and reliable climate research. Traditional Bayesian DA methods enforce these nonlinear, flow-dependent physical constraints through empirical and tunable covariance structures, but with limited accuracy and robustness. Here, we introduce Latent Data Assimilation (LDA), a framework that performs Bayesian DA in a latent space learned from multivariate global atmospheric data via an autoencoder. We demonstrate that the autoencoder can largely capture nonlinear physical relationships, enabling LDA to produce balanced analyses without explicitly modeling physical constraints. Assimilation in latent space also improves both analysis quality and forecast skill compared to traditional model-space DA,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>processing analysis outputs <ref type="bibr" target="#b23">(24,</ref><ref type="bibr" target="#b24">25)</ref>-while preserving the fundamental assimilation algorithms.</p><p>Beyond these developments, several new DA paradigms based on ML have also emerged. These include 1) diffusion-based methods <ref type="bibr" target="#b25">(26)</ref><ref type="bibr" target="#b26">(27)</ref><ref type="bibr" target="#b27">(28)</ref><ref type="bibr" target="#b28">(29)</ref>, which generate atmospheric analyses via observationguided denoising, and 2) end-to-end schemes <ref type="bibr" target="#b29">(30)</ref><ref type="bibr" target="#b30">(31)</ref><ref type="bibr" target="#b31">(32)</ref><ref type="bibr" target="#b32">(33)</ref> , which treat reanalysis data as truth and learn a direct mapping from real observations and background states to analysis states. Although proven effective, these approaches still lack the rigorous incorporation of prior information as comprehensively as traditional Bayesian DA (e.g., various background uncertainties, observation uncertainty, and model dynamics), thereby limiting their ability to handle complex operational assimilation scenarios. In addition, physical constraints among atmospheric variables are learned only implicitly in these two approaches and remain difficult to enforce explicitly. These limitations highlight the importance of developing DA frameworks that can ideally combine the non-linear representational capacity of ML with the statistical rigor of traditional methods.</p><p>Latent Data Assimilation (LDA) represents a promising avenue to address these challenges <ref type="bibr" target="#b33">(34)</ref><ref type="bibr" target="#b34">(35)</ref><ref type="bibr" target="#b35">(36)</ref><ref type="bibr" target="#b36">(37)</ref><ref type="bibr" target="#b37">(38)</ref><ref type="bibr" target="#b38">(39)</ref>. Rather than operating in the original model space, LDA performs assimilation in a compact latent space learned via nonlinear encoding of high-dimensional atmospheric states using autoencoders (AEs) <ref type="bibr" target="#b39">(40)</ref> or variational autoencoders (VAEs) <ref type="bibr" target="#b40">(41)</ref>. By optimizing the latent space to preserve maximal information from the input atmospheric states, LDA can potentially capture essential nonlinear dependencies among variables. This property may reduce the reliance on B matrix while still maintaining physical consistency in analyses. While LDA has shown promise in early studies, three critical questions remain unresolved. First, theoretical foundations of LDA</p><p>are not yet fully established, especially regarding the underlying mechanisms that contribute to its effectiveness. Second, existing applications have been restricted to univariate or idealized systems <ref type="bibr" target="#b19">(20,</ref><ref type="bibr" target="#b36">(37)</ref><ref type="bibr" target="#b37">(38)</ref><ref type="bibr" target="#b38">(39)</ref>, leaving its scalability to realistic, multivariate atmospheric problems unverified.</p><p>Third, operational feasibility for real-world assimilation and forecasting has yet to be demonstrated.</p><p>In this study, we present the first practical implementation of LDA to a high-dimensional, multivariate global atmospheric setting, and investigate the factors underlying its effectiveness. We demonstrate that the latent representation, learned via nonlinear compression, enables physically consistent DA, with a near diagonal background error covariance matrix, which provides an advantage compared to the challenging task of defining an optimal background covariance matrix in model space. By analyzing the properties of latent-space increments, we show that LDA can approximate traditional DA in model space, while in practice it outperforms traditional methods in both analysis and forecasts, and is easier to implement. Finally, we investigate the impact of latent dimensionality on assimilation performance, providing practical guidelines for selecting an optimal latent space for LDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We implement LDA by performing Bayesian assimilation in a low-dimensional representation of the atmospheric state, learned via an AE with an encoder-decoder architecture (Fig. <ref type="figure" target="#fig_0">1A</ref>). Specifically, the encoder first compresses the high-dimensional atmospheric background fields into a latent representation, where observations are then assimilated; the decoder subsequently reconstructs the latent analysis to produce the final model-space analysis. We train the AE on ERA5 reanalysis <ref type="bibr" target="#b41">(42)</ref> at 1.40625Â°resolution, covering 69 atmospheric variables-four at the surface and five in the upper air across 13 pressure levels. The model compresses the full atmospheric state (69Ã—256Ã—128) into a latent space of size (34Ã—64Ã—32), preserving both variable structure and spatial organization.</p><p>To implement a four-dimensional (4D) LDA and evaluate its ability to support accurate weather forecasts, we train an end-to-end forecast model using the same ERA5 dataset employed for AE training. The model architecture follows FengWu <ref type="bibr" target="#b42">(43)</ref>, a Swin Transformer-based medium-range AI weather prediction system. While such end-to-end models exhibit strong forecasting skill <ref type="bibr" target="#b42">(43)</ref><ref type="bibr" target="#b43">(44)</ref><ref type="bibr" target="#b44">(45)</ref><ref type="bibr" target="#b45">(46)</ref>, their limited ability to propagate small initial perturbations and accurately represent the ensemble spread has been noted in previous studies <ref type="bibr" target="#b46">(47,</ref><ref type="bibr" target="#b47">48)</ref>. Accordingly, we demonstrate the superiority of LDA by comparing the variational DA in model and latent spaces, using a static background error covariance estimated via the National Meteorological Center (NMC) method <ref type="bibr" target="#b10">(11)</ref>. For clarity, we denote the variational DA methods as "3DVar" and "4DVar" when applied in model space, and as "L3DVar" and "L4DVar" when implemented in latent space. Implementation details of the AE, forecast model, and the DA methods are provided in the Methods section.</p><p>In variational DA, obtaining the optimal analysis requires inverting the B matrix, which is computationally prohibitive due to the high-dimensional nature of weather forecasting. This matrix in model space typically exhibits strong spatial and cross-variable correlations (Fig. <ref type="figure" target="#fig_0">1B</ref>) and is therefore often simplified into a diagonal form via empirical control variable transform <ref type="bibr" target="#b48">(49)</ref> to facilitate inversion. Within the LDA framework, although the latent space can be substantially lower-dimensional than the model space, a full-rank latent background error covariance matrix (B ğ‘§ ) still contains up to 10 9 elements, making direct inversion impractical. Fortunately, consistent with previous findings from single-variable LDA studies <ref type="bibr" target="#b36">(37,</ref><ref type="bibr" target="#b38">39)</ref>, we find that B ğ‘§ is naturally approximately diagonal, with uniformly small off-diagonal elements (Fig. <ref type="figure" target="#fig_0">1C</ref>). This property allows for a simple yet accurate approximation of B -1  ğ‘§ by taking the reciprocals of its diagonal entries. Consequently, all subsequent LDA experiments adopt this diagonalized form of B ğ‘§ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Superiority of assimilation in latent space over model space</head><p>We first evaluate the effectiveness of LDA using observing system simulation experiments (OSSEs) <ref type="bibr" target="#b49">(50)</ref><ref type="bibr" target="#b50">(51)</ref><ref type="bibr" target="#b51">(52)</ref>, a standard framework for benchmarking DA methods under controlled and idealized conditions.</p><p>In these experiments, the ERA5 reanalysis is treated as the true atmospheric state for validation, from which synthetic observations are sampled based on the location of the GDAS (Global Data Assimilation System). In each experiment, observations from four time steps at six-hour intervals are assimilated into a 54-hour forecast initialized from the ERA5 reanalysis, after which a 10-day free forecast is performed. Fig. <ref type="figure" target="#fig_1">2A</ref> shows the mean percentage reduction in analysis and forecast errors for each DA method, relative to a control run without assimilation, averaged over daily OSSEs conducted throughout 2017. Overall, the variational DA methods perform substantially better in the latent space than in the model space, for both analyses and forecasts. In particular, this advantage is particularly pronounced in L4DVar, which reduces the analysis error by an average of 5.1% relative to 4DVar and maintains its superiority throughout the forecast period for most variables. In contrast, L3DVar achieves a 3% reduction in analysis error compared to 3DVar and shows reduced improvement in forecast persistence. These results indicate that the model dynamics can be effectively utilized and play a significant role in LDA.</p><p>Beyond idealized OSSEs, we further evaluate the performance of LDA using real surface and radiosonde observations from GDAS throughout 2017. After quality control and interpolation (see Methods for details), the processed dataset includes over 400 upper-air and 3,000 surface observations with an interval of 12 hours. Fig. <ref type="figure" target="#fig_1">2B</ref> evaluates 10-day forecasts initialized from L4DVar and 4DVar analyses against all conventional observations collected by GDAS. Both methods improve forecast skill, with L4DVar consistently outperforming 4DVar, across nearly all variables. Interestingly, although L4DVar and 4DVar produce noticeably different analyses at the end of assimilation, their forecast errors become statistically similar in the early hours-in contrast with OSSEs, where the advantage of L4DVar progressively amplifies during the initial forecast steps. A likely explanation is that LDA analyses, derived from real observations, deviate from the ERA5 distribution used to train the ML-based forecast model, leading to a model adjustment that temporarily narrows the performance gap.</p><p>To evaluate the analysis capability of LDA, we conducted cycling assimilation experiments throughout 2017, with a 48-hour DA window comprising four observation steps. The accuracy of the resulting analysis fields from L4DVar and 4DVar was validated using observations withheld from assimilation (Fig. <ref type="figure" target="#fig_1">2C</ref>). Across all pressure levels and time steps, L4DVar produces more accurate analyses than 4DVar for 54 of the 69 variables, with a mean reduction in error approximately 5%.</p><p>These results highlight the practical superiority of LDA in both idealized and real-world settings, supporting its potential as a more effective alternative to traditional DA frameworks.</p><p>Since the AE in LDA is trained in an unsupervised manner, we conduct a proof-of-concept experiment to examine whether LDA remains effective when the AE is trained on less accurate data. Specifically, we compare the LDA performance using an AE trained on ERA5 reanalysis versus one trained on 4-day forecasts (Fig. <ref type="figure" target="#fig_1">2D</ref>). Remarkably, despite the fact that the forecast data exhibiting substantially larger errors than ERA5, L4DVar based on the forecast-trained AE produces reanalyses of comparable quality to those based on ERA5-and still outperforms traditional 4DVar.</p><p>More importantly, LDA yields reanalyses that are significantly more accurate than the data used for training. These results suggest that, given sufficient observations, LDA can enhance the accuracy of analysis products beyond the limitations of the training data. An expanded version of Fig. <ref type="figure" target="#fig_1">2</ref>, covering a more comprehensive set of variables across multiple vertical levels, is provided in figs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1-4.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physical consistency of LDA</head><p>In model-space DA, physical balance among variables is implicitly enforced through the covariance structure of B, which becomes nearly diagonal in latent space. To evaluate whether LDA maintains physical consistency under such a diagonal B ğ‘§ , we conducted single-observation experiments with L3DVar. Fig. <ref type="figure" target="#fig_2">3A</ref> shows the analysis increment resulting from a +200ğ‘š 2 /ğ‘  2 geopotential height perturbation at 500 hPa, centered over China (left) and Australia (right). The geopotential height increment decays radially from the perturbation center, inducing anticyclonic wind anomalies.</p><p>Notably, the wind response reverses direction between hemispheres, consistent with geostrophic balance at 500 hPa. The lower panel of Fig. <ref type="figure" target="#fig_2">3A</ref> displays the associated temperature increment, which closely follows the background wind flow. Fig. <ref type="figure">S5</ref> presents the same observation increment as in Fig. <ref type="figure" target="#fig_2">3A</ref>, but under a different background state, leading to a distinct analysis increment. These results show that, even with a diagonal and static B ğ‘§ , LDA can produce physically consistent and flow-dependent analyses through the state-varying encoding, suggesting that physical constraints is implicitly encoded in the latent space.</p><p>These findings imply that the latent representation captures the inter-variable dependencies directly, making it unnecessary to rely on a complex B ğ‘§ covariance to enforce physical constraints.</p><p>Instead those constraints are learned through the encoding process. To verify this hypothesis, we first analyze the impact of each latent variable (i.e., the first dimension of the latent state) on atmospheric variables. As shown in Fig. <ref type="figure" target="#fig_2">3B</ref>, each latent variable consistently influences specific atmospheric variables to varying degrees, indicating that the latent representation captures intervariable dependencies critical for maintaining physical consistency. Fig. <ref type="figure">S6</ref> shows that different latent variables exert approximately orthogonal influences, revealing the decorrelation effect of the encoding.</p><p>To further investigate the inherent physical constraints of the latent representation, we test the ability of the AE to reconstruct physically unbalanced atmospheric states. Specifically, we replace one variable in an ERA5 sample with its spatial mean and evaluate the reconstruction (Fig. <ref type="figure" target="#fig_2">3C</ref>). The results indicate that the averaged Z500 field is accurately recovered, while Q500 is not, reflecting differences in their dependency on other variables. As shown in Fig. <ref type="figure" target="#fig_0">1B</ref>, the geopotential height (Z500), which exhibits strong vertical and inter-variable correlations, can be largely reconstructed even from an unphysical averaged input. In contrast, the specific humidity (Q500), being more independent, proves much harder to reconstruct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Why LDA works?</head><p>Although LDA achieves promising results in both analysis and forecast accuracy, its underlying mechanism remains unclear, raising potential questions about its overall reliability. A central challenge stems from the fact that LDA solves the optimal analysis in the latent space, without guaranteeing that decoding the optimal latent state will yield an optimal analysis in model space as well. Mathematically, we demonstrate that LDA and traditional model-space assimilation are equivalent if and only if the decoder ğ· (â€¢) is locally affine and error-free throughout the assimilation process, under an assumption of zero decoding error (see Supplementary Text).</p><p>This theoretical insight motivates us to evaluate the local behavior of the decoder along latent space increments Î”ğ’› that captures leading modes of atmospheric variability, which dominate the structure of DA updates. To this end, we construct Î”ğ’› by differencing latent states from randomly selected ERA5 reanalysis samples, and examine their effects on the decoded outputs. Extensive experiments reveal two key properties: the decoder response is approximately additive with respect to latent increments and nearly homogeneous in magnitude, as presented in Fig. <ref type="figure" target="#fig_3">4A</ref>. Additional examples and statistical results provided in the Supplementary Materials (fig. <ref type="figure">S8</ref> and<ref type="figure" target="#fig_0">tabel S1</ref>) indicate that the decoder behaves approximately affine near each latent state ğ’› along directions of Î”ğ’›. This behavior can be explained by a first-order Taylor expansion: a continuous function, including highly nonlinear neural networks, is approximately affine within sufficiently small regions.</p><p>To estimate the valid range of the decoder linearity, we test whether scaled latent increments ğ‘˜ â€¢Î”ğ’› produce proportionally scaled outputs. As shown in Fig. <ref type="figure" target="#fig_3">4B</ref>, the decoder exhibits approximate affine behavior across a broad range of ğ‘˜ along latent directions Î”ğ’› that represent dominant atmospheric variability. Given that DA analysis increments in model space reflect similar underlying variability but are much smaller in magnitude, the corresponding latent increments in LDA are expected to align with the directions of the tested Î”ğ’› and remain within the decoder's locally affine regime. Fig. <ref type="figure">S9</ref> supports this by showing that in both OSSE and real-observation experiments (Fig. <ref type="figure" target="#fig_1">2A,</ref><ref type="figure">B</ref>), the resulting analysis increments follow directions where the decoder is locally affine around the background state. These findings suggest why LDA can effectively and stably approximate the model-space optimal analysis in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Latent Dimensionality on LDA</head><p>The dimensionality of the latent space is a key parameter that influences the LDA performance. To investigate its effects, we evaluate both analysis and forecast skill using AEs trained with varying compression ratios. As the latent space is three-dimensional in this study, we evaluate multiple spatial compression ratios ğ¶ spatial under each fixed total compression ratio ğ¶ total . As shown in Fig. <ref type="figure">5A-C</ref>, LDA consistently outperforms traditional DA across a wide range of compression ratios, demonstrating strong robustness. L4DVar, which incorporates model dynamics, exhibits notably lower sensitivity to compression ratios than L3DVar. Interestingly, as ğ¶ total increases, LDA performance initially improves and then declines, regardless of ğ¶ spatial , with the optimal value consistently being around 32, across all experimental settings. The existence of an optimal latent size in LDA reflects a trade-off between two competing effects induced by AE compression. On the one hand, it can increase the reconstruction error due to information loss during encoding (Fig. <ref type="figure">5D</ref>), which may degrade the analysis quality. On the other hand, compression enhances the decorrelation of latent variables, supporting the validity of diagonalizing B ğ‘§ . This effect is illustrated in fig. <ref type="figure" target="#fig_0">S10</ref>, where increasing ğ¶ spatial reduces spatial correlations in B ğ‘§ . As the LDA performance initially improves with increasing ğ¶ total (Fig. <ref type="figure">5A-C</ref>), this suggests that the decorrelation effect dominates at first. However, since this benefit saturates (fig. <ref type="figure" target="#fig_3">S4</ref>), the reconstruction error grows continuously. As a result, an optimal latent size always exists and can be efficiently identified via binary search <ref type="bibr" target="#b52">(53)</ref>.</p><p>We note that a patch-like increment sometimes emerges in single-observation experiments when the latent variable dimension exceeds that of the model space (fig. <ref type="figure" target="#fig_7">S11</ref>), likely due to the ViT-based architecture of the AE. While this effect is mitigated with dense observations and remains acceptable in our experiments (Fig. <ref type="figure">5A-C</ref>), we recommend compressing both variable and spatial dimensions when using ViT-based AEs. Accordingly, we adopt a latent size of 34Ã—64Ã—32 in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Advances in DA rely on the integration of new sources of information, yet few methodological breakthroughs have emerged in the past two decades. The rapid development of ML and the increasing availability of atmospheric data offer a new opportunity to revolutionize DA. In this study, we construct a latent representation of the global atmosphere using an AE trained on decades of ERA5 reanalysis and perform DA directly in this latent space. We demonstrate that LDA enables physically constrained analyses and holds great potential for improving both analysis and forecasting tasks.</p><p>While this study demonstrates that Bayesian DA in latent space can approximate the accuracy its counterpart in the model space, our experiments indicate that latent-space assimilation offers superior practical performance. This improvement can be attributed to their fundamental differences in the treatment of atmospheric inter-variable correlations. Traditional DA frameworks depend on the quality of the background covariance B to define a physically constrained solution space <ref type="bibr" target="#b8">(9)</ref>. However, accurately estimating B remains extremely challenging due to its high dimensionality and flow-dependent characteristics. In contrast, the latent space learned by the AE naturally encodes these non-linear physical relationships through the compression process. As a result, in LDA, B ğ‘§ in LDA primarily characterizes the magnitude of background uncertainty rather than enforcing physical consistency, and can therefore be effectively diagonalised. This enables LDA to be a simpler, more physically consistent, and more effective alternative to traditional model-space DA approaches. It is worth noting that the B ğ‘§ used in this study retains some spatial and cross-variable correlations. Decomposing these components as in traditional variational DA (49) may further improve LDA performance.</p><p>The effectiveness of the LDA relies on two empirical properties: the approximate affinity of the decoder in DA process and the near-diagonal structure of B ğ‘§ . While these properties are difficult to guarantee, similar findings have been reported across LDA studies involving different variables, scales, and AE architectures <ref type="bibr" target="#b36">(37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b53">54)</ref>, suggesting that such behaviors may be general under certain conditions. A possible explanation for the decoder's local affinity is that assimilation increments are typically small compared to climatological variability, allowing the decoder to be well-approximated by a first-order Taylor expansion along directions that represent atmospheric variability. The near-diagonal structure of B ğ‘§ likely arises from the AE's compression objective, which promotes decoupling among atmospheric variables, as evidenced by the latent impact analysis in fig. <ref type="figure">S6</ref>. These characteristics underpin the potential of LDA to generalize to more complex Earth system models, including high-resolution atmospheric and oceanic simulations.</p><p>Many existing end-to-end DA methods are trained to reproduce the reanalysis used as labels <ref type="bibr" target="#b29">(30,</ref><ref type="bibr" target="#b54">55)</ref> and therefore struggle to exceed its accuracy, regardless of the amount of observations assimilated. In contrast, LDA employs an unsupervised AE that only requires high-fidelity input data with physically consistent atmospheric structures, without relying on accurate analyses or real observations. Even when the AE is trained on forecasts with substantial errors, LDA can still produce reliable analyses-only slightly less accurate than those based on reanalysis-trained models and significantly outperforming its own training data. This suggests that, with more assimilated observations, LDA has the potential to surpass existing reanalysis products such as ERA5 <ref type="bibr" target="#b41">(42)</ref>, which are based on 4DVar. Moreover, this capability is especially valuable in data-sparse or underdeveloped regions, where long-term reanalysis is difficult and expensive to construct, but physically consistent mesoscale forecasts initialized from global analyses remain accessible.</p><p>While LDA demonstrates notable advantages over both traditional and ML-based DA methods, its effectiveness remains limited by several factors, including the AE reconstruction error, the Gaussian assumption of background errors in latent space, and the architecture of the AE network. These challenges underscore the need for continued advancements in AE design to further improve assimilation performance. In addition, the current implementation of L4DVar requires a differentiable forecasting model, limiting its applicability to ML-based systems. As physics-informed neural models and end-to-end forecasting frameworks continue to advance-and increasingly demonstrate potential to replace traditional CPU-based systems <ref type="bibr" target="#b43">(44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b55">56)</ref>-this constraint is expected to diminish. We believe LDA provides a promising pathway toward next-generation DA systems, with the potential for integration into high-resolution, hybrid Earth system models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reanalysis dataset, forecast model, and observations</head><p>The AE, forecast model, and OSSE experiments are all based on ERA5 reanalysis data <ref type="bibr" target="#b41">(42)</ref>, with a spatial resolution of 1.4Â°latitude/longitude encompassing a global grid of 128 Ã— 256 points. This dataset comprises 69 variables: four surface variables and five upper-air atmospheric variables across 13 pressure levels (50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, and 1000 hPa).</p><p>The surface variables included 2-meter air temperature (t2m), 10-meter wind (u10, v10), and mean sea-level pressure (msl), while the upper-air variables included geopotential height (Z), temperature (T), zonal wind (U), meridional wind (V), and specific humidity (Q). ERA5 reanalysis data from 1979 to 2015 were used to train the AE and forecast model, with 2016 reserved for validation and 2017 for testing and data assimilation experiments. The dataset was used at hourly resolution for AE training and 6-hourly resolution for the forecast model.</p><p>Our ML-based forecast model follows the architecture of FengWu <ref type="bibr" target="#b42">(43)</ref>, which employs a modality-specific design with separate Swin Transformer (57) encoders and decoders for each atmospheric variable, and fuses cross-variable information via a combination of Vision Transformer and Swin Transformer blocks. Unlike the original FengWu model, which takes atmospheric states from two consecutive time steps as input, our implementation uses only a single time step to support 4DVar assimilation. Furthermore, since our study involves cyclic DA to generate analysis and requires high-fidelity forecast data to train the AE, we do not apply multi-step autoregressive fine-tuning <ref type="bibr" target="#b44">(45,</ref><ref type="bibr" target="#b45">46)</ref>, which may smooth out the forecasts.</p><p>The global observations for the assimilation experiments are sourced from GDAS in 2017. As a preliminary investigation, only surface and radiosonde observations with the BUFR codes of "ADPUPA" and "ADPSFC" are utilized. All observations are interpolated onto the model state grid for simplicity, and redundant observations falling within the same grid point are averaged to reduce sampling noise. High-elevation surface observations are treated as upper-air observations after altitude interpolation. In real-observation assimilation experiments, we discard observations whose deviations from ERA5 analyses exceed variable-specific thresholds to ensure assimilation stability. These thresholds are defined as the mean absolute difference between ERA5 analyses separated by 48 hours, computed over the year 2016 for all 69 variables. To account for representativeness errors and interpolation uncertainties associated with terrain effects, thresholds for surface observations are inflated by a factor of four. Post-processing finally yielded over 3000 surface and 400 radiosonde observations every 12 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AE for compressing multivariate global atmosphere</head><p>The autoencoder (AE) <ref type="bibr" target="#b39">(40)</ref> comprises an encoder ğ¸ (â€¢) and a decoder ğ· (â€¢) (Fig. <ref type="figure" target="#fig_0">1A</ref>). The encoder maps the multivariate global atmospheric state ğ’™ in model space to a latent representation ğ’›, while the decoder reconstructs ğ’™ from ğ’›. The AE is trained by minimizing the mean squared reconstruction error,âˆ¥ğ’™ -ğ· (ğ¸ (ğ’™))âˆ¥ 2 , ensuring that the latent representation ğ’› = ğ¸ (ğ’™) preserves the essential information in ğ’™.</p><p>Our AE architecture is built upon a vision transformer with window attention <ref type="bibr" target="#b56">(57)</ref>, following the design presented in <ref type="bibr" target="#b57">(58)</ref> for compressing the ERA5 dataset.</p><p>The encoder begins with a 4Ã—4 patch embedding that partitions the input field into 64Ã—32 tokens, each represented by a 1024-dimensional vector, followed by 12 Swin Transformer blocks with 16 attention heads. Subsequently, a shared fully connected network (FCN) is applied to each token to project them into a latent space of shape 34Ã—64Ã—32. The decoder mirrors the encoder structure but applies a FCN to each latent token after the transformer blocks, followed by reshaping to recover the model-space structure. The model was trained for 30 epochs using the AdamW optimizer <ref type="bibr" target="#b58">(59)</ref>.</p><p>A learning rate of 2 Ã— 10 -4 was employed, incorporating a linear warm-up followed by a cosine decay schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Traditional variational DA methods in model space</head><p>Assuming the errors of the background field should be singular ğ’™ ğ’ƒ and observations ğ’š are Gaussian and independent, the maximum posterior estimate of the atmospheric state ğ’™ can be obtained by minimizing the variational cost function ğ½ (ğ’™). For 3DVar <ref type="bibr" target="#b59">(60)</ref>, the cost function is as follows:</p><formula xml:id="formula_0">ğ½ (ğ’™) = 1 2 (ğ’™ -ğ’™ ğ’ƒ ) T B -1 (ğ’™ -ğ’™ ğ’ƒ ) + 1 2 ( ğ’š -H (ğ’™)) T R -1 (ğ’š -H (ğ’™)),<label>(1)</label></formula><p>where, B and R represent the error covariance matrix for ğ’™ ğ’ƒ and ğ’š, respectively. H (â€¢) denotes the observation operator, facilitating a projection from model space to the observational space of ğ’š.</p><p>4DVar simultaneously assimilates a sequence of observations over a time window by incorporating model dynamics as a constraint <ref type="bibr" target="#b4">(5)</ref>. In this study, we adopt a strongly constrained 4DVar formulation, which is also the standard approach used in operational systems such as the Integrated Forecasting System (IFS) <ref type="bibr" target="#b60">(61)</ref>. The cost function is defined as:</p><formula xml:id="formula_1">ğ½ (ğ’™) = 1 2 (ğ’™ -ğ’™ ğ’ƒ ) T B -1 (ğ’™ -ğ’™ ğ’ƒ ) + 1 2 ğ‘› âˆ‘ï¸ ğ‘–=0 ( ğ’š ğ’Š -H (ğ‘€ 0â†’ğ‘– (ğ’™))) T R -1 ğ‘– ( ğ’š ğ’Š -H (ğ‘€ 0â†’ğ‘– (ğ’™))),<label>(2)</label></formula><p>where the subscript ğ‘– = 0, 1, . . . , ğ‘› denotes sequential time points, and ğ‘€ 0â†’ğ‘– represents model forecast operator from the initial time to ğ‘¡ ğ‘– .</p><p>Traditional 3DVar and 4DVar minimization employ an iterative gradient descent approach, requiring the computation of âˆ‡ğ½ (ğ’™). This is particularly challenging for 4DVar, necessitating programming tangent linear and adjoint models <ref type="bibr" target="#b7">(8)</ref>. Fortunately, the use of neural network-based models facilitates automated minimization of ğ½ (ğ’™) via gradient descent and backpropagation <ref type="bibr" target="#b61">(62)</ref>.</p><p>This process resembles neural network training but uses ğ½ (ğ’™) as the loss function and optimizes only the model states ğ’™. We employ the L-BFGS optimizer for 3DVar due to its efficiency. However, for 4DVar, the incorporation of a nonlinear ML model makes its cost function not strictly convex, necessitating the use of a stochastic optimizer like Adam <ref type="bibr" target="#b62">(63)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The variational LDA methods</head><p>In LDA, the latent background state ğ’› ğ‘ is defined as the encoded representation of ğ’™ ğ‘ , obtained via the AE. Denoting the error covariance matrix of ğ’› ğ’ƒ as B ğ‘§ , the cost function of 3DVar in latent space can be expressed as:</p><formula xml:id="formula_2">ğ½ (ğ’›) = 1 2 (ğ’› -ğ’› ğ’ƒ ) T B -1 ğ‘§ (ğ’› -ğ’› ğ’ƒ ) + 1 2 ( ğ’š -H (ğ· (ğ’›))) T R -1 (ğ’š -H (ğ· (ğ’›))),<label>(3)</label></formula><p>and for L4Dvar, it is formulated as follows:</p><formula xml:id="formula_3">ğ½ (ğ’›) = 1 2 (ğ’› -ğ’› ğ’ƒ ) T B -1 ğ‘§ (ğ’› -ğ’› ğ’ƒ ) + 1 2 ğ‘› âˆ‘ï¸ ğ‘–=0 ( ğ’š ğ’Š -H (ğ‘€ 0â†’ğ‘– (ğ· (ğ’›))) T R -1 ğ‘– ( ğ’š ğ’Š -H (ğ‘€ 0â†’ğ‘– (ğ· (ğ’›)))).<label>(4)</label></formula><p>Minimizing these two functions yields the latent-space analysis ğ’› ğ‘ , whose decoded counterpart ğ’™ ğ‘ = ğ· (ğ’› ğ‘ ) represents the LDA analysis in model space. Note that the minimization process requires stochastic optimization techniques due to the incorporation of the nonlinear decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimation of B matrix</head><p>The classical NMC method ( <ref type="formula">11</ref>) is employed to provide the static forecast error covariance matrix of forecast model. The B-matrix in model space is estimated as follows:</p><formula xml:id="formula_4">B â‰ˆ 1 2 (ğ’™ 48 -ğ’™ 24 )(ğ’™ 48 -ğ’™ 24 ) T ,<label>(5)</label></formula><p>where ğ’™ 24 and ğ’™ 48 represent the 48 h and 24 h forecasts valid at the same time, and âŸ¨â€¢âŸ© denotes the average over a large number of samples. To address the computational and storage challenges posed by B matrix, we followed the NCAR-developed GEN BE 2.0 method <ref type="bibr" target="#b48">(49)</ref>, which generates background error matrices for the WRF model. This method decomposes B matrix into several components: B = UU T , where U = U ğ‘ SU ğ‘£ U â„ . The U ğ‘ , U ğ‘£ , U â„ , S matrix represents the physical variable correlation, vertical correlation, horizontal correlation, and the diagonal standard deviations of the decomposed variables, respectively.</p><p>In the latent space, the computation of the background error covariance matrix B ğ‘§ becomes substantially simpler, as it is approximately diagonal by construction. Specifically, we calculate each diagonal element of B ğ‘§ with the NMC method as follows:</p><formula xml:id="formula_5">B ğ‘§,ğ‘– â‰ˆ 1 2 ğ¸ (ğ’™ 48 ) ğ‘– -ğ¸ (ğ’™ 24 ) ğ‘– 2 ,<label>(6)</label></formula><p>where ğ‘– denotes the ğ‘– ğ‘¡â„ element of the latent space variable, and ğ¸ (â€¢) represents the AE encoder.</p><p>We computed ğ’™ 24  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head><p>In OSSEs, given the ground truth is available at each grid point, we utilize the latitude-weighted root mean square error (WRMSE) to quantify the error of each atmospheric variable ğ‘ of the model field ğ‘¥ as follows:</p><formula xml:id="formula_6">WRMSE(ğ’™, ğ’™ ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ , ğ‘) = 1 ğ» â€¢ ğ‘Š âˆ‘ï¸ â„,ğ‘¤ ğ» â€¢ cos ğ›¼ â„,ğ‘¤ ğ» â„ â€² =1 cos ğ›¼ â„ â€² ,ğ‘¤ ğ’™ ğ‘,â„,ğ‘¤ -ğ’™ ğ‘,â„,ğ‘¤ ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ 2 ,<label>(7)</label></formula><p>where superscript ğ‘, â„, ğ‘¤ denote the index for variables, latitude grid, and longitude grid, respectively. ğ›¼ â„,ğ‘¤ is the latitude of point (â„, ğ‘¤). ğ» and ğ‘Š represent the number of grid points in the longitudinal and latitudinal directions of the model space.</p><p>For real observation experiments, since the observation used for validation is sparse, we assess the accuracy of ğ’™ by directly calculating the root mean square error (RMSE) of each atmospheric variable at the observation locations as follows:</p><formula xml:id="formula_7">RMSE(ğ’™, ğ’š, ğ‘) = âˆšï¸„ 1 ğ‘ âˆ‘ï¸ ğ‘– (H (ğ’™) ğ‘,ğ‘– -ğ’š ğ‘,ğ‘– ) 2 ,<label>(8)</label></formula><p>where the superscript ğ‘, ğ‘– denote the ğ‘– ğ‘¡â„ observation of variable ğ‘, and ğ‘ represents the total number of observations for that variable.     where the subscript ğ‘– = 0, 1, . . . , ğ‘› indexes sequential time steps, and ğ‘€ 0â†’ğ‘– denotes the model forecast operator that propagates the state from the initial time to time ğ‘¡ ğ‘– .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equivalence Conditions between Latent-Space and Model-Space DA</head><p>To establish a theoretical connection between model-space and latent-space DA, we begin with a 3D variational (3DVar) framework. We formally prove the following proposition:</p><p>Proposition. Let D : R ğ‘› â†’ R ğ‘š be an error-free decoder mapping from latent space to model space. Then, the solution of latent-space 3DVar is equivalent to that of model-space 3DVar for all valid background states and observations if and only if D is affine over the region traversed during assimilation.</p><p>Proof of sufficiency. Let ğ’™ ğ‘ and ğ’› ğ‘ denote the optimal analyses from 3DVar and L3DVar, respectively. The first-order optimality conditions for these solutions are given by: B -1 (ğ’™ ğ‘ -ğ’™ ğ‘ ) + H T R -1 [H (ğ’™ ğ‘ ) -ğ’š] = 0, (S7)</p><formula xml:id="formula_8">B -1 ğ‘§ (ğ’› ğ‘ -ğ’› ğ‘ ) + J T D H T R -1 [H (D (ğ’› ğ‘ )) -ğ’š] = 0,<label>(S8)</label></formula><p>where J D denotes the Jacobian of the decoder evaluated at ğ’› ğ‘ , and H denotes the Jacobian of the observation operator evaluated at ğ’™ ğ‘ .</p><p>As the decoder is affine throughout the assimilation process, i.e., J D (ğ’›) â‰¡ J D , we have:   </p><formula xml:id="formula_9">J</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the LDA framework and comparison of background error correlations in model and latent spaces. (A), Illustration of LDA for global atmosphere. The highdimensional background atmospheric state ğ’™ ğ’ƒ is encoded into a compact latent representation ğ’› ğ’ƒ , here via a Swin Transformer-based autoencoder. A Bayesian variational assimilation is then performed in the latent space using observations ğ’š, yielding a latent analysis ğ’› ğ’‚ , which is decoded to produce the analysis state ğ’™ ğ’‚ . (B-C), Background error correlations in the model space (B) and latent space (C) estimated by "NMC" method. The model space exhibits strong and complex intervariable and long-range spatial correlations, while the latent space shows near-diagonal structures in both variable and spatial dimensions, indicating the decorrelation effect induced by the AE. The correlations of Z and Q at 500 hPa with other variables are highlighted by black lines in A.</figDesc><graphic coords="17,72.00,137.93,467.98,286.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance comparison of Variational DA methods in model space and latent space across OSSEs and real-world experiments. (A), Percentage reduction in RMSE relative to a control run without DA in idealized OSSEs, evaluated against ERA5. Results are averaged over daily experiments in 2017, each initialized at 0000 UTC and assimilating four time steps at 6-hour intervals. Observations were sampled from ERA5 using the radiosonde and surface observation locations of GDAS at 0000 UTC on January 1. (B), Same as A, but using real GDAS observations in 2017, which include radiosonde and surface data available at 12-hour intervals. Accuracy is evaluated against all available observations. (C), analysis RMSE from L4DVar and 4DVar using GDAS observations over the full year of 2017, computed against 16 radiosonde and 300 surface observations withheld from assimilation. Shaded lines indicate daily variability; solid lines show smoothed trends. (D), Comparison of L4DVar performance using AEs trained on ERA5 reanalysis (L4DVar-RA) and 5-day forecasts (L4DVar-FC). Average errors of the respective training datasets are shown as dashed lines.</figDesc><graphic coords="18,72.00,114.82,467.92,270.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Evidence of physical consistency in latent-space assimilation. (A), Analysis increments from a +200ğ‘š 2 /ğ‘  2 perturbation applied to Z500 at 200 hPa over China (left) and Australia (right), using ERA5 reanalysis at 0000 UTC on 1 December 2017 as the background. Top panels show geopotential height and wind increments; bottom panels show temperature increments and the background wind field. Perturbation locations are marked with blue triangles. (B), Influence of latent variables on atmospheric variables. Shown are the mean and standard deviation of perturbations in atmospheric variables induced by perturbations in each latent variable, computed over 10,000 randomly selected pairs from the ERA5 test dataset. In each sample, the model-space perturbation is spatially averaged and normalized. (C), AE reconstruction of physically imbalanced inputs. The fields of Z500 and Q500 at 0000 UTC on 1 January 2017 are first averaged globally and then passed through the AE.</figDesc><graphic coords="19,72.00,167.01,467.94,207.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Approximate affinity of the AE decoder along latent directions representative of atmospheric variability. (A), The impact of latent-space perturbations Î”ğ’› at ğ’› on decoding results, denoted as Î”ğ· ğ’› (Î”ğ’›), shown using Z500. Here, ğ’› denotes the latent state corresponding to the ERA5 reanalysis at 0000 UTC on February 1, 2017. The perturbations Î”ğ’› 1 and Î”ğ’› 2 represent the latent differences between ğ’› and the reanalysis at 0000 UTC on January 1 and March 1, 2017, respectively. (B), Evaluation of the near-linear response region of the AE decoder. The upper panel shows the correlation between Î”ğ· ğ’› (ğ‘˜ â€¢ Î”ğ’›) and ğ‘˜ â€¢ Î”ğ· ğ’› (Î”ğ’›) as a function of scale ratio ğ‘˜; the lower panel shows their relative â„“ 1 norm.</figDesc><graphic coords="20,72.00,186.69,467.96,233.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 : 2 ((</head><label>52</label><figDesc>Figure 5: Impact of the latent size on LDA performance. (A), Relative changes in analysis (DA) and forecast (FC) error for L3DVar compared to 3DVar in OSSEs. Experimental configuration is identical to that in Fig. 2A. Colors indicate the mean improvement across DA and FC. (B), Same as (a), but for L4DVar compared to 4DVar. (C), Forecast error changes for L4DVar with real GDAS observations, relative to 4DVar. The experimental configuration is identical to that in Fig. 2B. (D), AE reconstruction error for different latent sizes, expressed as percentage increase in RMSE relative to the largest latent size (138Ã—64Ã—32).</figDesc><graphic coords="21,72.00,155.21,467.92,317.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>D B ğ‘§ J T D = J D E[(ğ’› ğ‘ -ğ’› ğ‘¡ )(ğ’› ğ‘ -ğ’› ğ‘¡ ) T ] J T D = E[J D (ğ’› ğ‘ -ğ’› ğ‘¡ )(ğ’› ğ‘ -ğ’› ğ‘¡ ) T J T D ] = E[(D (ğ’› ğ‘ ) -D (ğ’› ğ‘¡ ))(D (ğ’› ğ‘ ) -D (ğ’› ğ‘¡ )) T ] = E[(ğ’™ ğ‘ -ğ’™ ğ‘¡ )(ğ’™ ğ‘ -ğ’™ ğ‘¡ ) T ] = B,(S9)Left-multiplying Eq.(S7) and Eq.(S8) by B and J D B ğ‘§ , respectively, we obtain:(ğ’™ ğ‘ -ğ’™ ğ‘ ) + BH T R -1 [H (ğ’™ ğ‘ ) -ğ’š] = 0,(S10)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure S2 :</head><label>S2</label><figDesc>Figure S2: Extended forecast performance comparison of 4DVar and L4DVar with real observations. This figure expands on Fig. 2B by presenting RMSE reduction over a 10-day forecast for individual variables at 300 hPa, 500 hPa, 850 hPa, and the surface in real-observation experiments.</figDesc><graphic coords="36,72.00,175.97,467.99,362.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure S11 :</head><label>S11</label><figDesc>Figure S11: Illustration of patch-like increments in single-observation experiments under varying latent dimensionalities. Analysis increments are computed using LDA with AEs of different latent sizes, in response to a 200 m 2 /s 2 perturbation applied to geopotential height at 500hPa (marked by the blue triangle). When the variable dimension (i.e., the number of channels) of the latent space exceeds that of the model space (as in the top panel), the resulting increments exhibit patch-like structures. These patterns remain physically consistent, with stronger geopotential height anomalies accompanied by stronger geostrophic wind responses. An exception occurs for the AE with a latent size of 276Ã—64Ã—32, where insufficient compression prevents the latent space from preserving balance properties.</figDesc><graphic coords="45,72.00,175.21,468.00,234.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="41,72.00,240.21,468.01,234.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="42,72.00,215.27,467.92,240.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="43,72.00,225.87,467.93,176.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="44,72.00,248.16,467.95,173.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and ğ’™ 48 at 6-hourly intervals throughout 2016, yielding 1460 paired samples, to provide B and B ğ‘§ required for DA experiments in 2017. Note that both B and B ğ‘§ are derived from the same forecast samples, ensuring a consistent environment for comparing DA and LDA. The length scales of B and the magnitude of B ğ‘§ were tuned through experiments conducted in 2016.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to express our sincere gratitude to the <rs type="institution">ECMWF</rs> for providing the invaluable ERA5 reanalysis dataset. We also gratefully acknowledge the <rs type="institution">NCEI</rs> for making the global observations available.</p></div>
			</div>
			<div type="funding">
<div><p>Funding: P.G. and Y.Q. acknowledge support from the <rs type="funder">National Science Foundation (NSF) Science and Technology Center (STC) Learning the Earth with Artificial Intelligence and Physics (LEAP)</rs>, Award #<rs type="grantNumber">2019625</rs> and <rs type="funder">USMILE European Research Council</rs> grant. Author contributions: Conceptualization: H.F., Y.L and P.G. Methodology: H.F. Data curation: H.F. and K.C. Software: H.F., Y.X., and B.F. Formal analysis: H.F., Y.X, Y.Q, and F.L. Writingoriginal draft: H.F., L.B., B.F., and P.G. Writing -review and editing: All authors. Supervision: L.B. and P.G. Visualization: H.F. Resources: L.B.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_XtA46r2">
					<idno type="grant-number">2019625</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(AIRS) at <ref type="url" target="https://www.ncei.noaa.gov/has/HAS.DsSelect">https://www.ncei.noaa.gov/has/HAS.DsSelect</ref>. The neural network model is developed using PyTorch. Codes and model checkpoints used in this study are available at <ref type="url" target="https://github.com/hangfan99/LDA_1.41">https://github.com/hangfan99/LDA_1.41</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Competing interests: There are no competing interests to declare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and materials availability:</head><p>The ERA5 dataset is available from the official website of Climate Data Store (CDS) at <ref type="url" target="https://cds.climate.copernicus.eu/">https://cds.climate.copernicus.eu/</ref>. The GDAS observational BUFR files can be accessed via the NCEI Archive Information Request System</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Text</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figs. S1 to S11</head><p>Tables S1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials for Physically Consistent Global Atmospheric Data Assimilation with Machine Learning in Latent Space</head><p>Hang Fan 1,2,3,4 , Lei Bai 1 * , Ben Fei 1,5 * , Yi Xiao 1 , Kun Chen 1 , Yubao Liu 4 * , Yongquan Qu 2,3 , Fenghua Ling 1 , Pierre Gentine 2,3 * Corresponding author. Email: bailei@pjlab.org.cn; benfei@cuhk.edu.hk; ybliu@nuist.edu.cn </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This PDF file includes:</head><note type="other">Supplementary Text</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Text Derivation of variational assimilation methods in latent space</head><p>We begin by deriving the latent-space 3DVar (L3DVar) cost function using a Bayesian maximum a posteriori (MAP) estimation framework. Specifically, given a latent background estimate ğ’› ğ‘ and observations ğ’š, L3DVar seeks the latent state ğ’› that maximizes the posterior distribution ğ‘(ğ’› | ğ’š, ğ’› ğ‘ ).</p><p>Since the observations are usually independent with the background, the posterior distribution is proportional to the product of the likelihood and the prior:</p><p>Following the derivation of 3DVar, we assume a Gaussian prior distribution for the latent variable ğ’›, centered at the latent background state ğ’› ğ‘ with covariance B ğ‘§ :</p><p>To verify the rationality of this assumption, we analyzed a large sample of background errors generated using the NMC method. As shown in Fig. <ref type="figure">S7</ref>, the distribution of background errors projected onto the latent space is unimodal and symmetrical, supporting the use of a Gaussian prior as a reasonable modeling choice.</p><p>For the likelihood, we recall that the observations are related to the latent variable through a composition of the decoder D and the observation operator H , i.e., ğ’™ = D (ğ’›), followed by H (ğ’™).</p><p>Assuming Gaussian observational errors with mean zero and covariance R, the likelihood becomes:</p><p>Combining the prior and likelihood, the posterior takes the form:</p><p>Taking the negative log-likelihood (up to a constant), we obtain the L3DVar cost function:</p><p>Compared to the derivation of conventional 3DVar in model space, L3DVar differs in two aspects: 1) it assumes the prior distribution in the latent space rather than the model space, and 2)</p><p>Substituting Eq.(S9), J D (ğ’› ğ‘ -ğ’› ğ‘ ) = ğ’™ L3DVar ğ‘ -ğ’™ ğ‘ , and D (ğ’› ğ‘ ) = ğ’™ L3DVar ğ‘ into Eq.(S8), we obtain:</p><p>Comparing Eq. (S10) and Eq. (S12), we conclude that if the decoder is affine during the DA process, then L3DVar and 3DVar yield the same solution in model space.</p><p>Proof of necessity A first-order Taylor expansion of the decoder at ğ’™ ğ‘ gives:</p><p>where ğœº denotes the higher-order residual of the Taylor expansion, which depends on both ğ’› ğ‘ and ğ’›. Assume:</p><p>As a result,</p><p>We can then express the background covariance as:</p><p>where, ğƒ = J D (ğ’› ğ‘¡ -ğ’› ğ‘ )(ğœº 2 -ğœº 1 ). Therefore,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S4</head><p>We now derive a necessary condition under which the solutions of L3DVar and 3DVar coincide.</p><p>By combining Eq.(S7) and Eq.(S8), we obtain the following criterion:</p><p>Left-multiplying Eq.(S20) by J D B ğ‘§ , we obtain:</p><p>Substituting Eq.(S15) and Eq.(S19) in Eq.(S21) yields:</p><p>After simplification, we obtain:</p><p>To ensure D (ğ’› ğ‘ ) = ğ’™ ğ‘ , Eq.S22 must hold for arbitrary DA scenarios. If D is locally affine in the DA process, the residuals ğœº 1 and ğœº 2 vanish by construction, making both terms zero and satisfying the condition. Conversely, if D is not affine, the residuals do not vanish in general, and the left-hand side becomes a sum of positive semi-definite matrices that cannot cancel for arbitrary ğ’› ğ’‚ and ğ’› ğ’ƒ . For neural network-based decoders, which are generally nonlinear, this condition is almost impossible to satisfy. Therefore, Eq. S22 establishes a necessary condition for the decoder to behave affinely during the DA process, ensuring the equivalence between L3DVar and 3DVar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extension to EnKF and 4Dvar</head><p>In the 3DVar case, the proof primarily hinges on the transformation between the background covariance matrices in model space and latent space. Importantly, the result does not rely on the specific form of the observation term, which can be arbitrary. Therefore, this proposition naturally extends to the 4DVar framework, where the structure of the cost function remains similar except for the inclusion of time-evolving dynamics. For the Ensemble Kalman Filter (EnKF), it has been shown that the analysis update is theoretically equivalent to that of 3DVar under the linear-Gaussian assumption. As a result, the same affine condition on the decoder remains valid in the EnKF setting as well.          </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Kalnay</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511802270</idno>
		<title level="m">Atmospheric Modeling, Data Assimilation and Predictability</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data Assimilation Fundamentals: A Unified Formulation of the State and Parameter Estimation Problem</title>
		<author>
			<persName><forename type="first">G</forename><surname>Evensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Vossepoel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-96709-3</idno>
	</analytic>
	<monogr>
		<title level="m">Springer Textbooks in Earth Sciences, Geography and Environment</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Carrassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bocquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bertino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Evensen</surname></persName>
		</author>
		<idno type="DOI">10.1002/wcc.535</idno>
		<title level="m">Data Assimilation in the Geosciences: An Overview of Methods, Issues, and Perspectives. WIREs Climate Change</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Quiet Revolution of Numerical Weather Prediction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brunet</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14956</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">525</biblScope>
			<biblScope unit="issue">7567</biblScope>
			<biblScope unit="page" from="47" to="55" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Strategy for Operational Implementation of 4D-Var, Using an Incremental Approach</title>
		<author>
			<persName><forename type="first">P</forename><surname>Courtier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-N</forename><surname>ThÃ©paut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hollingsworth</surname></persName>
		</author>
		<idno type="DOI">10.1002/qj.49712051912</idno>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of the Royal Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">519</biblScope>
			<biblScope unit="page" from="1367" to="1387" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sequential Data Assimilation with a Nonlinear Quasi-Geostrophic Model Using Monte Carlo Methods to Forecast Error Statistics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Evensen</surname></persName>
		</author>
		<idno type="DOI">10.1029/94JC00572</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Geophysical Research</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">C5</biblScope>
			<biblScope unit="page">10143</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data Assimilation Using an Ensemble Kalman Filter Technique</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Houtekamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">10.1175/1520-0493(1998)126âŸ¨0796:DAUAEKâŸ©2.0.CO;2</idno>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="796" to="811" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Review of Operational Methods of Variational and Ensemble-Variational Data Assimilation: Ensemble-variational Data Assimilation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Bannister</surname></persName>
		</author>
		<idno type="DOI">10.1002/qj.2982</idno>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of the Royal Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">703</biblScope>
			<biblScope unit="page" from="607" to="633" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Review of Forecast Error Covariance Statistics in Atmospheric Variational Data Assimilation. I: Characteristics and Measurements of Forecast Error Covariances: A RE-VIEW OF FORECAST ERROR STATISTICS I</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Bannister</surname></persName>
		</author>
		<idno type="DOI">10.1002/qj.339</idno>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of the Royal Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="1951" to="1970" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Three-Dimensional Variational Data Assimilation System for MM5: Implementation and Initial Results</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1175/1520-0493(2004)132âŸ¨0897:ATVDASâŸ©2.0.CO;2</idno>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The National Meteorological Center&apos;s Spectral Statistical-Interpolation Analysis System</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Parrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Derber</surname></persName>
		</author>
		<idno type="DOI">10.1175/1520-0493(1992)120âŸ¨1747:TNMCSSâŸ©2.0.CO;2</idno>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1747" to="1763" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Review of the Ensemble Kalman Filter for Atmospheric Data Assimilation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Houtekamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1175/MWR-D-15-0440.1</idno>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4489" to="4532" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Hybrid Ensemble Kalman Filter-3D Variational Analysis Scheme</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hamill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Snyder</surname></persName>
		</author>
		<idno type="DOI">10.1175/1520-0493(2000)128âŸ¨2905:AHEKFVâŸ©2.0.CO;2</idno>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2905" to="2919" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Hybrid ETKF-3DVAR Data Assimilation Scheme for the WRF Model. Part I: Observing System Simulation Experiment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hamill</surname></persName>
		</author>
		<idno type="DOI">10.1175/2008MWR2444.1</idno>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5116" to="5131" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Earth System Models from Observations: Machine Learning or Data Assimilation?</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Geer</surname></persName>
		</author>
		<idno type="DOI">10.1098/rsta.2020.0089</idno>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="page">20200089</biblScope>
			<date type="published" when="2021">2194. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Machine Learning at ECMWF: A Roadmap for the next 10 Years</title>
		<author>
			<persName><forename type="first">P</forename><surname>DÃ¼ben</surname></persName>
		</author>
		<idno type="DOI">10.21957/GE7CKGM</idno>
	</analytic>
	<monogr>
		<title level="j">ECMWF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Machine Learning With Data Assimilation and Uncertainty Quantification for Dynamical Systems: A Review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/JAS.2023.123537</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA Journal of Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1361" to="1387" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep Learning Augmented Data Assimilation: Reconstructing Missing Information with Convolutional Autoencoders</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.1175/MWR-D-21-0288.1</idno>
	</analytic>
	<monogr>
		<title level="j">MONTHLY WEATHER REVIEW</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Vandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duffy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nachmany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hartshorn</surname></persName>
		</author>
		<title level="m">Global Atmospheric Data Assimilation with Multi-Modal Masked Autoencoders</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient High-Dimensional Variational Data Assimilation with Machine-Learned Reduced-Order Models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Maulik</surname></persName>
		</author>
		<idno type="DOI">10.5194/gmd-15-3433-2022</idno>
	</analytic>
	<monogr>
		<title level="j">Geoscientific Model Development</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3433" to="3445" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Learning-Enhanced Ensemble-Based Data Assimilation for High-Dimensional Nonlinear Dynamical Systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nabizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hassanzadeh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jcp.2023.111918</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">477</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<title level="m">FengWu-4DVar: Coupling the Data-driven Weather Forecasting Model with 4D Variational Assimilation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">FuXi-En4DVar: An Assimilation System Based on Machine Learning Weather Forecasting Model Ensuring Physical Constraints</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<publisher>Geophysical Research Letters</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training a Convolutional Neural Network to Conserve Mass in Data Assimilation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ruckstuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>JanjiÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rasp</surname></persName>
		</author>
		<idno type="DOI">10.5194/npg-28-111-2021</idno>
	</analytic>
	<monogr>
		<title level="m">Nonlinear Processes in Geophysics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using Machine Learning to Correct Model Error in Data Assimilation and Forecast Applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laloyaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bonavita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bocquet</surname></persName>
		</author>
		<idno type="DOI">10.1002/qj.4116</idno>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of the Royal Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">739</biblScope>
			<biblScope unit="page" from="3067" to="3084" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Score-Based Data Assimilation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rozet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Louppe</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gianinazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Dueben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<title level="m">DiffDA: A Diffusion Model for Weather-scale Data Assimilation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nathaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gentine</surname></persName>
		</author>
		<title level="m">Deep Generative Data Assimilation in Multimodal Setting</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Generative Data Assimilation of Sparse Weather Station Observations at</title>
		<author>
			<persName><forename type="first">P</forename><surname>Manshausen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>Kilometer Scales</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Towards an End-to-End Artificial Intelligence Driven Global Weather Forecasting System</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fuxi-DA: A Generalized Deep Learning Data Assimilation Framework for Assimilating Satellite Observations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">FuXi Weather: A Data-to-Forecast Machine Learning System for Global Weather</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">ADAF: An Artificial Intelligence Data Assimilation Framework for Weather Forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2411.16807</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Latent Space Data Assimilation by Using Deep Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Peyron</surname></persName>
		</author>
		<idno type="DOI">10.1002/qj.4153</idno>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of the Royal Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">740</biblScope>
			<biblScope unit="page" from="3759" to="3777" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Data Assimilation in the Latent Space of a Convolutional Autoencoder</title>
		<author>
			<persName><forename type="first">M</forename><surname>Amendola</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-77977-130</idno>
	</analytic>
	<monogr>
		<title level="m">Computational Science -ICCS 2021</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Sloot</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12746</biblScope>
			<biblScope unit="page" from="373" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-Domain Encoder-Decoder Neural Networks for Latent Data Assimilation in Dynamical Systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cma.2024.117201</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Methods in Applied Mechanics and Engineering</title>
		<imprint>
			<biblScope unit="volume">430</biblScope>
			<biblScope unit="page">117201</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3D-Var Data Assimilation Using a Variational Autoencoder</title>
		<author>
			<persName><forename type="first">B</forename><surname>Melinc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Å½</forename><surname>Zaplotnik</surname></persName>
		</author>
		<idno type="DOI">10.1002/qj.4708</idno>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of the Royal Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="issue">761</biblScope>
			<biblScope unit="page" from="2273" to="2295" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Novel Latent Space Data Assimilation Framework with Autoencoder-Observation to Latent Space (AE-O2L) Network. Part I: The Observation-Only Analysis Method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.1175/MWR-D-24-0057.1</idno>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Novel Latent Space Data Assimilation Framework with Autoencoder-Observation to Latent Space (AE-O2L) Network. Part II: Observation and Background Assimilation with Interpretability</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.1175/MWR-D-24-0058.1</idno>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reducing the Dimensionality of Data with Neural Networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1127647</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The ERA5 Global Reanalysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hersbach</surname></persName>
		</author>
		<idno type="DOI">10.1002/qj.3803</idno>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of the Royal Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="issue">730</biblScope>
			<date type="published" when="1999">1999-2049 (2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">FengWu: Pushing the Skillful Global Medium-range Weather Forecast beyond 10 Days Lead</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Accurate Medium-Range Global Weather Forecasting with 3D Neural Networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bi</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-023-06185-3</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">619</biblScope>
			<biblScope unit="issue">7970</biblScope>
			<biblScope unit="page" from="533" to="538" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning Skillful Medium-Range Global Weather Forecasting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lam</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.adi2336</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">382</biblScope>
			<biblScope unit="page" from="1416" to="1421" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">FuXi: A Cascade Machine Learning Forecasting System for 15-Day Global Weather Forecast</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41612-023-00512-1</idno>
	</analytic>
	<monogr>
		<title level="j">Climate and Atmospheric Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">190</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Can Artificial Intelligence-Based Weather Prediction Models Simulate the Butterfly Effect?</title>
		<author>
			<persName><forename type="first">T</forename><surname>Selz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Craig</surname></persName>
		</author>
		<idno type="DOI">10.1029/2023GL105747</idno>
	</analytic>
	<monogr>
		<title level="j">Geophysical Research Letters</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">20</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>GL</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Assimilating Observed Surface Pressure into ML Weather Prediction Models</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Slivinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Frolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Agarwal</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Descombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>AulignÃ©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vandenberghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>BarrÃ©</surname></persName>
		</author>
		<idno type="DOI">10.5194/gmd-8-669-2015</idno>
		<title level="m">Generalized Background Error Covariance Matrix Model (GEN BE v2.0)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="669" to="696" />
		</imprint>
	</monogr>
	<note>Geoscientific Model Development</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Observing-Systems Simulation Experiments: Past, Present, and Future</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Dey</surname></persName>
		</author>
		<idno type="DOI">10.1175/1520-0477(1986)067âŸ¨0687:OSSEPPâŸ©2.0.CO;2</idno>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="687" to="695" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Atmospheric Observations and Experiments to Assess Their Usefulness in Data Assimilation (gtSpecial IssueltData Assimilation in Meteology and Oceanography: Theory and Practice)</title>
		<author>
			<persName><forename type="first">R</forename><surname>Atlas</surname></persName>
		</author>
		<idno type="DOI">10.2151/jmsj1965.75.1B111</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the Meteorological Society of Japan. Ser. II</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1B</biblScope>
			<biblScope unit="page" from="111" to="130" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Future Observing System Simulation Experiments</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Atlas</surname></persName>
		</author>
		<idno type="DOI">10.1175/BAMS-D-15-00200</idno>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1601" to="1616" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<title level="m">Introduction to algorithms</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Generating Unseen Nonlinear Evolution in Sea Surface Temperature Using a Deep Learning-Based Latent Space Data Assimilation Framework</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">FuXi Weather: An End-to-End Machine Learning Weather Data Assimilation and Forecasting System</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Neural General Circulation Models for Weather and Climate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kochkov</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-024-07744-y</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">632</biblScope>
			<biblScope unit="page" from="1060" to="1066" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2103.14030</idno>
		<title level="m">Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Extreme Compression of ERA5 for Portable Global Climate and Weather Research via an Efficient Variational Transformer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CRA</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1711.05101</idno>
		<title level="m">Decoupled Weight Decay Regularization</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The ECMWF Implementation of Three-Dimensional Variational Assimilation (3D-Var). I: Formulation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Courtier</surname></persName>
		</author>
		<idno type="DOI">10.1002/qj.49712455002</idno>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of the Royal Meteorological Society</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">550</biblScope>
			<biblScope unit="page" from="1783" to="1807" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">IFS Documentation CY49R1 -Part II: Data Assimilation</title>
		<author>
			<persName><surname>Ecmwf</surname></persName>
		</author>
		<idno type="DOI">10.21957/105cb1333c</idno>
	</analytic>
	<monogr>
		<title level="m">IFS Documentation CY49R1 (ECMWF) (2024)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Gradient-Based Learning Applied to Document Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>A Method for Stochastic Optimization</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
