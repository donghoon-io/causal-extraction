<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic Decomposed Linear Dynamical Systems for Robust Discovery of Latent Neural Dynamics</title>
				<funder ref="#_pbkPcRf">
					<orgName type="full">James S. McDonnell Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">The Kavli Foundation NeuroData Discovery award</orgName>
				</funder>
				<funder ref="#_eStQBWY">
					<orgName type="full">National Center for Advancing Translational Sciences</orgName>
				</funder>
				<funder ref="#_B3UGc9x">
					<orgName type="full">Georgia Tech/Emory NIH/NIBIB Training Program in Computational Neural Engineering</orgName>
				</funder>
				<funder ref="#_9X2Mahj #_qxnkW3u #_jU4Dx9V">
					<orgName type="full">National Institutes of Health</orgName>
				</funder>
				<funder>
					<orgName type="full">Julian T. Hightower Chair</orgName>
				</funder>
				<funder ref="#_rpqzDPd">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yenho</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Center</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Coulter Dept. of Biomedical Engineering</orgName>
								<orgName type="institution">Emory University and Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Noga</forename><surname>Mudrik</surname></persName>
							<email>nmudrik1@jhu.edu</email>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Mathematical Institute for Data Science</orgName>
								<orgName type="department" key="dep3">Center for Imaging Science</orgName>
								<orgName type="institution" key="instit1">Kavli Neuroscience Discovery Institute</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kyle</forename><forename type="middle">A</forename><surname>Johnsen</surname></persName>
							<email>kjohnsen@gatech.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Coulter Dept. of Biomedical Engineering</orgName>
								<orgName type="institution">Emory University and Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sankaraleengam</forename><surname>Alagapan</surname></persName>
							<email>sankar.alagapan@gatech.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<address>
									<country key="GE">Georgia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><forename type="middle">S</forename><surname>Charles</surname></persName>
							<email>adamsc@jhu.edu</email>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Mathematical Institute for Data Science</orgName>
								<orgName type="department" key="dep3">Center for Imaging Science</orgName>
								<orgName type="institution" key="instit1">Kavli Neuroscience Discovery Institute</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Rozell</surname></persName>
							<email>crozell@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Center</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<address>
									<country key="GE">Georgia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic Decomposed Linear Dynamical Systems for Robust Discovery of Latent Neural Dynamics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Time-varying linear state-space models are powerful tools for obtaining mathematically interpretable representations of neural signals. For example, switching and decomposed models describe complex systems using latent variables that evolve according to simple locally linear dynamics. However, existing methods for latent variable estimation are not robust to dynamical noise and system nonlinearity due to noise-sensitive inference procedures and limited model formulations. This can lead to inconsistent results on signals with similar dynamics, limiting the model's ability to provide scientific insight. In this work, we address these limitations and propose a probabilistic approach to latent variable estimation in decomposed models that improves robustness against dynamical noise. Additionally, we introduce an extended latent dynamics model to improve robustness against system nonlinearities. We evaluate our approach on several synthetic dynamical systems, including an empirically-derived brain-computer interface experiment, and demonstrate more accurate latent variable inference in nonlinear systems with diverse noise conditions. Furthermore, we apply our method to a real-world clinical neurophysiology dataset, illustrating the ability to identify interpretable and coherent structure where previous models cannot. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A central goal in computational neuroscience is to develop models capable of discovering latent structure within noisy, high-dimensional neural signals. By identifying hidden relationships within neural recordings, we can begin to understand, predict, and control the behaviors of the underlying systems. Modeling neural time-series is challenging due to the range of temporal dynamics present. For example, there may be gradual short-term fluctuations, abrupt shifts in response to external stimuli, and long-term global drifts resulting from changes in baseline activity levels <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Although black-box approaches based on deep learning are available <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38]</ref>, their complexity often obscures the relationships learned from the data, making it difficult to extract scientific insights from these models. As a result, practitioners may favor time-varying linear state-space models which offer mathematically interpretable representations by approximating complex dynamics with simple locally linear regimes <ref type="bibr" target="#b25">[26]</ref>. However, obtaining latent variable estimates that are robust to dynamical noise and system nonlinearity in these state-space models is challenging. When applied to neural time-series, latent variable estimation may become unstable due to inflexible model formulations or noise-sensitive inference procedures. This can incorrectly produce disparate results for signals generated from the same underlying system. For example, switching linear dynamical systems (SLDS) and related models <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44]</ref> segment time-series into discrete linear dynamical states, providing a piecewise linear approximation of the underlying system while highlighting coherent groups of activity. However, the assumption of discrete components can be a poor modeling choice for neural signals that contain continuous-valued fluctuations, such as gradual or random changes to the system speed as seen during neural ramping activity <ref type="bibr" target="#b30">[31]</ref> or Lévy walk dynamics in the cerebral cortex <ref type="bibr" target="#b26">[27]</ref>. As demonstrated in <ref type="bibr" target="#b29">[30]</ref> and our experiments in Section 4.3, when applied to real-world datasets, inference of the switching variables can result in rapid, random oscillations between the discrete modes, indicating that the model is unable to identify meaningful structure in the data.</p><p>To address the limitations of discrete states, the decomposed linear dynamical systems (dLDS) model <ref type="bibr" target="#b29">[30]</ref> learns a dictionary set of linear dynamical regimes, referred to as dynamic operators (DOs), that can be modified and combined through a linear combination of sparse coefficients. By allowing coefficients to be time-varying and continuous-valued, dLDS naturally captures both gradual changes by adjusting the coefficient magnitudes and abrupt shifts by changing the set of active DOs over time. Inference is accomplished by optimizing over a cost function that encourages data reconstruction while also constraining the structure of the dynamic coefficients to be sparse and temporally smooth.</p><p>Unfortunately, there are two critical shortcomings that prevent the robust inference of the latent variables in dLDS. First, the cost-based inference procedure is sensitive to noise, because of the regularization term encouraging temporal smoothness. This term sequentially propagates errors from noisy coefficient estimates over the length of the time series. Consequently, the model may produce inconsistent coefficient estimates on similar signals and have poor multi-step inference performance, indicating that the learned dynamics are unable to generalize well beyond a single time step. Second, the original latent dynamics model lacks a method for accurately representing systems with multiple fixed points, causing DO coefficients to oscillate or switch arbitrarily in a way that may not align with the underlying process. To learn an effective decomposed model in practice, we require a strategy that provides robust estimates of latent variables despite the presence of noise and system nonlinearity.</p><p>In this work, we address these limitations by introducing the probabilistic decomposed linear dynamical systems (p-dLDS) model. Our approach improves robustness of latent variable estimation while maintaining the richness of a decomposed dynamical systems model. First, we propose a probabilistic inference procedure that reduces the model's sensitivity to temporal noise by accounting for uncertainty in the latent variable estimates over time. Namely, we introduce time-informed hierarchical variables that encourage both sparse and smooth model coefficients. We devise a variational expectation maximization (vEM) procedure to perform inference and learning over this probabilistic structure. Second, we incorporate a time-varying offset term to model systems that orbit multiple fixed points. While we analytically identify model degeneracies with this formulation, we propose an additive decomposition strategy that prevents convergence to trivial solutions.</p><p>Through several synthetic examples, we demonstrate how these contributions lead to improved accuracy and robustness of latent variable estimation despite difficult noise conditions. We extend these results to an empirically-derived brain-computer interface experiment <ref type="bibr" target="#b7">[8]</ref>, showcasing robustness to highly nonlinear observation functions and the ability to extract meaningful insights from the learned latent variables. Finally, we illustrate how our method effectively identifies interpretable and coherent structure in a clinical neurophysiology dataset where previous models are unsuccessful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>State Space Models. Our goal is to accurately describe the evolution of high-dimensional time-series data y t ∈ R M with the following state-space equations, y t = Dx t + d + ϵ yt , ϵ yt ∼ N (0, Σ y ), (Observations)</p><p>(1)</p><formula xml:id="formula_0">x t = f t (x t-1 ) + ϵ xt , ϵ xt ∼ N (0, Σ x ),<label>(Dynamics)</label></formula><p>where x t ∈ R N is the latent state, f t (•) is the dynamics function, and D ∈ R M ×N and d ∈ R M describe a linear observation function. Our work focuses on the case when N &lt; M , which compresses high-dimensional signals into a low-dimensional latent space. By choosing f t to be a time-varying linear operator, we can approximate complex nonlinear dynamics with simple locallylinear components, balancing expressivity with mathematical interpretability. However, learning a time-varying linear operator from data can be challenging, and typically requires additional constraints on the underlying generative model to identify meaningful representations.</p><p>Switching Linear Dynamical System (SLDS). SLDS approximates nonlinear systems by introducing a discrete switching variable z t = {1, . . . , K} into the time-varying linear dynamics equation,</p><formula xml:id="formula_1">x t+1 = x t + F zt x t + b zt + ϵ xt .</formula><p>At each time step t, the latent state x t evolves according to the z t -th linear regime defined by F zt ∈ R N ×N and b zt ∈ R N while the switching variables evolve according to a Markov matrix.</p><p>Inference is performed through a vEM algorithm, where the approximate posterior of the latent variables is estimated through coordinate ascent updates over tractable subgraphs. There are many extensions of SLDS, such as rSLDS <ref type="bibr" target="#b25">[26]</ref> which modifies its generative behavior by informing the transitions of z t with x t-1 . However, switching models are inherently limited when describing complex signals due to their discrete formulation. For instance, a switched representation is unable to learn that a dynamic regime may exhibit a range of variations. In neural systems, these variations may arise from random spiking processes <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">40]</ref> or systems with randomly distributed speeds <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b11">12]</ref>. In SLDS, each variation is learned as a separate discrete state, thus obscuring that the learned states are related. Furthermore, the switching formulation cannot adapt the learned system to unseen variations (i.e. different levels of random speeds). This can produce unstable inference behavior, where the switching state oscillates unpredictably or collapses to a single uninformative state.</p><p>Decomposed Linear Dynamical Systems (dLDS). dLDS <ref type="bibr" target="#b29">[30]</ref> relaxes the discrete formulation by approximating nonlinear and nonstationary signals with a time-varying mixture of linear dynamical systems (LDS) defined by the following equations and constraints,</p><formula xml:id="formula_2">x t+1 = x t + F t x t + ϵ xt , F t = K k=1 f k c t,k , s.t. c t is sparse.<label>(2)</label></formula><p>Every transition F t is decomposed as a linear combination of sparse coefficients c t ∈ R K and a dictionary of K DOs f k ∈ R N ×N . Figure <ref type="figure" target="#fig_0">1A</ref> shows the corresponding graphical model. Inference of the latent variables is accomplished by solving the Basis Pursuit Denoising with Dynamic Filtering (BPDN-DF) <ref type="bibr" target="#b5">[6]</ref> objective sequentially for all t and λ 0 , λ 1 , λ 2 &gt; 0,</p><formula xml:id="formula_3">x t , c t = arg min xt,ct ∥y t -Dx t ∥ 2 2 + λ 0 ∥x t -x t-1 -F t x t-1 ∥ 2 2 + λ 1 ∥c t ∥ 1 + λ 2 ∥c t -c t-1 ∥ 2 2 .</formula><p>This produces a point estimate of x t and c t that matches the likelihood function resulting from Equations ( <ref type="formula">1</ref>) and <ref type="bibr" target="#b1">(2)</ref>. In this objective, the dynamic coefficients are encouraged to be sparse through the ℓ 1 penalty and temporally smooth through the ℓ 2 penalty centered around the previous coefficient estimate. However, this approach is sensitive to noise because inference relies on propagating noisy point estimates of c t-1 over time. As a result, BPDN-DF may accumulate errors that can lead to significantly different coefficient estimates on signals sampled from the same generative process. Furthermore, the lack of robustness to noise can degrade multi-step inference performance, causing the inferred system to quickly diverge from the true system. This suggests that the inferred latent variables only capture the local activity narrowly and are unable to accurately represent the dynamics beyond a single time-step. Another drawback of dLDS arises from the dynamics model in equation ( <ref type="formula" target="#formula_2">2</ref>) which implicitly assumes that the observed dynamics contain a single fixed point that revolves around the origin. This limits dLDS's ability to model systems that cannot be easily mean-centered such as those with multiple fixed points or nonstationary drifts.</p><p>Sparse Bayesian Learning with Dynamic Filtering. Sparsity is achieved in probabilistic models through hierarchical scale-mixture priors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref>. To integrate dynamical information into probabilistic sparse signal inference, previous work <ref type="bibr" target="#b32">[33]</ref> proposes the Sparse Bayesian Learning with Dynamic Filtering (SBL-DF) framework where the following hierarchical model is defined, Here, p(x t |c t ) = N (Φc t , λ t I) specifies the likelihood, where Φ is a measurement matrix. Sparsity is encouraged through the zero-mean Gaussian priors p(c t,k |γ t,k ) = N (0, γ t,k ) independently placed on each element of the sparse vector. </p><formula xml:id="formula_4">p(x t , c t , γ t ) = p(x t |c t ) K k=1 p(c t,k |γ t,k )p(γ t,k |a t,k , b t,k ).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Probabilistic Decomposed Linear Dynamical Systems</head><p>We build upon dLDS and propose a probabilistic decomposed linear dynamical systems (p-dLDS) model. Rather than propagating noisy point estimates of the latent variables during inference, we improve robustness by marginalizing over uncertainty with respect to time. Additionally, we propose a tractable method for extending the dynamics model to systems with multiple fixed points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Time-varying offset term</head><p>Note that for any parameter setting, the dLDS local dynamics (eq. ( <ref type="formula" target="#formula_2">2</ref>)) reduces to a linear dynamical system (LDS) that is characterized by a single fixed point centered around the origin. Yet, real-world dynamical systems often consist of much more complicated behaviors. Nonlinearities can cause a signal to navigate through multiple fixed points throughout its trajectory, while nonstationarities may change the behavior of the system entirely with new fixed points emerging or disappearing. Simple preprocessing measures, such as mean-centering the data, are inadequate to account for these behaviors. To enable robustness against these behaviors, we introduce a time-varying offset term b t ∈ R N into Equation (2) as a flexible way to account for dynamics not readily captured by the original dLDS latent dynamics model. Lemma 1. Let the transition between any two state vectors x t , x t+1 ∈ R N be defined by the linear dynamics matrix F t ∈ R N ×N and the dynamics offset b t ∈ R N . For any λ &gt; 0, the objective,</p><formula xml:id="formula_5">arg min Ft,bt ∥x t+1 -x t -F t x t -b t ∥ 2 2 + λ∥F t ∥ 2 2 ,</formula><p>is minimized when</p><formula xml:id="formula_6">F t = 0 and b t = x t+1 -x t .</formula><p>This result, proven in Appendix B.1, reveals that introducing a time-varying offset term makes inference of the dynamics a degenerate problem. While the solution in Lemma 1 minimizes the objective, it fails to capture any meaningful structure in F t as the result of b t being unconstrained. To prevent the convergence to these trivial solutions, we decompose the latent state space as,</p><formula xml:id="formula_7">x t = l t + b t ,<label>(4)</label></formula><p>where l t captures fast dynamics and b t captures slow-varying trend behavior. These latent variables follow the dynamic equations l t+1 = l t + F t l t + ϵ lt+1 and b t+1 = b t + ϵ bt+1 where ϵ l , ϵ b ∈ R N represent noise sampled from ϵ b ∼ N (0, Σ b ) and ϵ l ∼ N (0, Σ l ) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Probabilistic Time-Informed Sparsity</head><p>In decomposed models, we aim to achieve two goals simultaneously: sparsity and smoothness of coefficients over time. Motivated by this, we incorporate dynamics-informed probabilistic structure. First, we assume that each coefficient evolves independently of the others. Second, we introduce a hierarchical variance parameter γ t,k that controls the sparsity for each c t,k . Moreover, we introduce dynamics information during sparse inference by encouraging a similar active support set in consecutive time slices through the variance hyperpriors. Put together, the resulting coefficient transition density in p-dLDS becomes,</p><formula xml:id="formula_8">p(c t , γ t |c t-1 ) := p(c t |c t-1 , γ t )p(γ t |c t-1 ) = K k=1 p(c t,k |c t-1,k γ t,k )p(γ t,k |c t-1,k ).<label>(5)</label></formula><p>We define the first term on the right-hand side with the following functional form,</p><formula xml:id="formula_9">p(c t,k |c t-1,k , γ t ) ∝ exp - c 2 t,k 2γ t,k - (c t,k -c t-1,k ) 2 2σ 2 t-1,k ∝ N (c t-1,k , σ 2 t-1,k )N (0, γ t,k ).<label>(6)</label></formula><p>This density captures the constraints of sparsity and smoothness for the inferred coefficients c t,k . When the variance around zero γ t,k is small, this structure promotes sparsity by shrinking coefficient values towards zero. Conversely, when the variance around the previous time step σ 2 t-1,k is small, it encourages smooothness by shrinking coefficients towards the previous value. While the idea of combining two shrinkage effects in a single density has been explored in previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b17">18]</ref>, those approaches generally require manual balancing of the two penalties. In contrast, we devise a procedure in the following section that estimates these variance parameters automatically during inference and learning.</p><p>The second density on the right-hand side from equation ( <ref type="formula" target="#formula_8">5</ref>) is defined similarly to the hyperprior in SBL-DF. (i.e., p(γ t,k |c t-1,k ) = IG(ξ, ξc 2 t-1,k ) where ξ weighs the influence of the dynamics when estimating γ t,k ). The resulting graphical model is shown in Figure <ref type="figure" target="#fig_0">1B</ref>. We note that since the value of the previous coefficient is squared, the overall prior placed on the inverse gamma density follows a χ 2 distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference and Learning</head><p>The joint distribution of p-dLDS is given by, p(x, y, c, γ|θ) = p(x 1 )</p><formula xml:id="formula_10">T t=1 p(y t |x t ) T -1 t=1 p(x t+1 |x t , c t ) K k=1 p(c t+1,k |c t,k , γ t+1,k )p(γ t+1,k |c t,k ) ,<label>(7)</label></formula><p>where we denote x = x 1:T for brevity. Exact posterior inference is intractable due to the nonconjugacy introduced by incorporating time-informed sparsity-inducing structure into the graphical model. As a result, we devise a variational expectation maximization (vEM) procedure where the approximate posterior is factorized as p(x, c, γ|y, θ) ≈ q(x)q(c, γ).</p><p>Here, the parameters are given by θ ∈ {f 1:K , D, d, Σ y , Σ x , Σ c }. Our approach contrasts with BPDN-DF, which estimates latent variables through separate ℓ 1 problems at each point in time. Instead, we preserve the time-dependence structure within each class of latent variables and leverage efficient inference algorithms that marginalize over uncertainty with respect to time. In general, we seek to maximize the variational lower bound, L q (θ) = E q(x)q(c,γ) [log p(y, x, c, γ|θ) -log q(x)q(c, γ)],</p><p>with coordinate ascent updates on the latent state posterior, the dynamics coefficients posterior, and the model parameters.</p><p>Updating Latent State Posterior. The optimal coordinate ascent variational update is given by,</p><formula xml:id="formula_11">q(x) ∝ exp E q(c,γ) [log p(x, y, c, γ|θ)] .<label>(8)</label></formula><p>Our assumed decomposition in equation ( <ref type="formula" target="#formula_7">4</ref>) allows us to define the latent state transition density as p(</p><formula xml:id="formula_12">x t+1 |x t , c t ) = p(x t+1 = l t+1 + b t+1 ) = N (x t+1 ; l t + F t l t + b t )</formula><p>. Substituting this into equations ( <ref type="formula" target="#formula_10">7</ref>) and ( <ref type="formula" target="#formula_11">8</ref>), we get that the optimal coordinate ascent approximate posterior becomes,</p><formula xml:id="formula_13">q(x) = N (x 1 ; µ 1 , Σ 1 ) T t=2 N (y t ; Dx t + d, Σ y ) T t=1 N (x t ; l t-1 + F t-1 l t-1 + b t , Σ x ) ,<label>(9)</label></formula><p>where, µ 1 and Σ 1 are the mean and covariance of the initial state.</p><formula xml:id="formula_14">Lemma 2. Let l, b ∈ R N be independent random variables such that l ∼ p(l) and b ∼ p(b). Their sum x = l + b is distributed according to N (µ l + µ b , Σ l + Σ b ) when 1) p(b) = N (µ b , Σ b ) and p(l) = N (µ l , Σ l ) and when 2) p(b) = δ(b -µ b ) and p(l) = N (µ l , Σ l + Σ b ).</formula><p>We leverage Lemma 2 to reparameterize the state space trajectories into a deterministic and a stochastic component. The deterministic component captures the slow-moving offset density q(b)</p><formula xml:id="formula_15">= T t=1 δ(b t -b t )</formula><p>, where b t is estimated using a moving average of window size S which can be efficiently parallelized. The remaining dynamics are captured in the stochastic component. We define the family of variational distributions to be the class of linear Gaussian state space models, such that</p><formula xml:id="formula_16">q(l) = N (l 1 , Σ 1 ) T t=2 N (l t-1 + F t-1 l t-1 , Σ x ).</formula><p>Conditioned on estimates b and samples of c, the optimal coordinate ascent variational update for q(l) is efficiently computed using the Kalman Smoother <ref type="bibr" target="#b34">[35]</ref>. We provide a full derivation of the update rule and Lemma 2 in Appendix B.2.</p><p>Updating Dynamics Coefficient Posterior. Sparse probabilistic representations introduce non-Gaussian factors which prevent closed-form message passing inference. Specifically, nonconjugacy arises from the inverse gamma term p(γ t+1,k |c t,k ) since it is parameterized by c 2 t,k ∼ χ 2 . Moreover, the posterior distribution over the coefficients is highly multi-modal as a result of the implicit tdistribution in our hierarchical model. To update the coefficient posteriors, we propose a three-step procedure, where we factorize q(c, γ) = q(c)q(γ). First, we obtain an initial estimate of the variational distributions using SBL-DF. Second, we update q(c t,k ) = N (c * t,k , γ t,k ) using stochastic gradient descent (SGD) over</p><formula xml:id="formula_17">c * = arg max c T -1 t=1 log p( l t+1 | l t , c t ) + log p(c t+1 |c t , γ t+1 ) + log p( γ t+1 |c t ),<label>(10)</label></formula><p>where we have estimated the expectation in the optimal coordinate ascent update rule using samples from γ ∼ q(γ) and l ∼ q(l). To retain coefficient sparsity, we only update coefficients within the active support set. In our work, this is defined as coefficients that have an initial estimate of |c t,k | &gt; η where η = 10 -4 . Finally, we update q(γ) based on closed form conjugacy rules.</p><p>Update Parameters. Given our updated posteriors of the latent variables, we proceed to update the model parameters based on the ELBO,</p><formula xml:id="formula_18">θ * = arg max θ E q(x)q(c)q(γ) [p(y, c, x, γ|θ) -log q(x)q(c)q(γ)] ≈ arg max θ p(y, c, x, γ|θ),<label>(11)</label></formula><p>where we estimate the expectation with samples from our variational distributions and drop terms not dependent on θ. We use SGD to update all model parameters, which is possible when we assume that the covariance matrices have diagonal structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We demonstrate p-dLDS in a variety of synthetic examples, highlighting improved robustness to noise and system nonlinearity. Additionally, we apply our model to a clinical neurophysiology dataset, revealing interpretable patterns where previous methods fail. We compare our method against SLDS, rSLDS, and dLDS as described in Section 2. All datasets are split 50:50 for training and testing. Due to space constraints, we provide full descriptions of the simulation setup in Appendix C and metric definitions in Appendix D.  NASCAR with Random Speeds. We evaluate our inference procedure on the NASCAR dataset <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b22">23]</ref>. Since this system is easily mean-centered, we can isolate the effect of our proposed inference procedure as offset terms are not necessary. To make this dataset more realistic, we introduce speed variability into the ground truth dynamics as opposed to having a perfect constant speed at all time. Specifically, whenever a trajectory enters a new segment of the track, the system experiences a random change in speed. We trained all models on 30 trials, each consisting of 1000 time steps (Fig. <ref type="figure" target="#fig_4">2A</ref>) with randomly sampled initial points, and a randomly constructed 10-dimensional linear observation matrix (see Appendix C.1 for full details). Performance is evaluated on 30 held-out trials where the ground truth switching states are defined by the different segments of the track. In all models, we set the M = 10, N = 2 and K = 4 for DOs or switching states. In our experiments, we define the "discrete states" for decomposed models as the DO state with the largest coefficient magnitude.</p><p>Figure <ref type="figure" target="#fig_4">2B</ref> shows that changes in the system speed mask the true transition behavior between segments of the track in rSLDS. Moreover, dLDS identifies coherent segments, but inappropriately learns a different switching pattern for outer and inner edges of the track. In contrast, p-dLDS identifies a switching pattern most consistent with the true track segments despite the presence of noise and randomness in the system's speed. We note that while there are four true segments, decomposed models form a more parsimonious representation by identifying similar behaviors in different track segments such as in both edges and curves.</p><p>Table <ref type="table" target="#tab_1">1</ref> summarizes our quantitative evaluations on three metrics: 1) the mean squared error (MSE) between the learned and ground truth latent dynamics, 2) the MSE between the inferred and true switch rate to determine agreement of the discrete switching behavior, and 3) the 100-step inference R 2 to demonstrate that the learned system generalizes beyond a single step on held-out data. (See Appendix D for mathematical definitions). We see that p-dLDS broadly outperforms existing methods in all metrics and significantly improves inference for decomposed models.</p><p>Lorenz System with Random Ramping. Next, we consider the Lorenz system, a chaotic nonlinear system with multiple fixed points, exploring the effect of the offset term. The system is described by the differential equation,</p><formula xml:id="formula_19">ẋ = [σ(x 2 -x 1 ), x 1 (ρ -x 3 ) -x 2 , x 1 x 2 -βx 3 ]</formula><p>⊤ where the parameters ρ = 28, β = 8/3, and σ = 10 define a chaotic attractor with two opposing lobes (Fig. <ref type="figure" target="#fig_4">2D</ref>). We introduce continuous fluctuations in the underlying dynamics by randomly ramping the system's speed throughout each trajectory. This is accomplished by adjusting the evaluation time intervals given to an ODE solver. Similar to before, we randomly construct a linear observation function with M = 10 and train models on 30 randomly constructed trials with 1000 time points each (see Appendix C.2). Furthermore, we define the ground truth switch events as the time points when the signal transitions between the two lobes in addition to the moments when a ramping period concludes. All models are trained with a latent space of N = 3 and K = 4 states or DOs.</p><p>In figure <ref type="figure" target="#fig_4">2D</ref>, we see that rSLDS does not distinguish between the different speeds along the outer and inner sections of the attractor. Instead, the discrete states obscure the continuum of speeds by incorrectly grouping all activity in each lobe into a single regime. Furthermore, we observe that dLDS is limited without an offset term, unable to accurately represent multiple fixed points. Instead of aligning with the two attractor lobes, transitions in the dominant coefficients occur radially relative to the origin and fail to reconstruct the two orbiting fixed points. Conversely, p-dLDS's offset term enables learning a system where coefficients better match the true geometry. This representation correctly recovers differences between the outer and inner sections of the attractor while also accurately reconstructing the two orbiting fixed points. Moreover, this leads to improved estimation of latent dynamics, a switching rate that agrees with the true system, and improved multistep inference performance as shown in Table <ref type="table" target="#tab_1">1</ref>. We now turn to an empirically-derived synthetic experiment related to brain-computer interfaces, where the dynamics and observation functions are nonlinear and derived from analysis of neural data. Our focus is on the reaching task, a neuroscience experiment designed to study motor control in nonhuman primates <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref>. In this experiment, the subjects are trained to reach towards visually cued targets, while neural activity is recorded from motor-related areas such as electromyography (EMG) data from arm muscles. Each trial consists of two distinct phases: preparation and movement. In the preparation phase, the subject plans its movement while keeping their arm still. In the movement phase, the subject physically reaches towards the target. The goal of this experiment is to decode reach intention from neural data. We construct a dataset by first simulating a spiking neural network with known latent factors <ref type="bibr" target="#b7">[8]</ref> trained to reproduce empirical EMG signals from the center-out reach task in <ref type="bibr" target="#b21">[22]</ref>. Spikes are then converted to 50-channel local field potentials (LFP) recordings via a weighted, delayed sum of synaptic currents (see Figure <ref type="figure" target="#fig_2">3A</ref> and<ref type="figure">B</ref>) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16]</ref>. Our dataset contains 150 6-second trials sampled at 250 hz, where each trial represents one out of eight reach directions visually cued at a random start time. PCA identifies that three components captures 98% of the variance. Thus, we set M = 50, N = 3, and K = 4 DOs or discrete states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Simulated Motor Cortex Data in a Reaching Task</head><p>Figure <ref type="figure" target="#fig_2">3C</ref> shows the trial-averaged DO coefficients from p-dLDS, which change smoothly and cyclically according to the true reach angle. Additionally, the DOs appear to differentiate between the two distinct dynamical regimes, where the activity of f 1 and f 3 localize to the preparatory and movement phase respectively. Quantitatively, we compute the linear classification accuracy of the reach angles using the average state activity over time as features (see Appendix D.5). Figure <ref type="figure" target="#fig_2">3D</ref> shows that classifiers built from SLDS features fail to capture the full continuum of reach angles. This limitation occurs because the discrete switching states are unable to efficiently capture the diversity of activity present in the LFPs, which arise from randomness inherent in the spike sampling process. Consequently, the inferred features from SLDS generalize poorly to held-out data. In contrast, the p-dLDS classifier predictions recover the full spectrum of reach angles since features are naturally continuous and the inferred coefficients can adjust the learned DOs to accurately capture the activity in the held out data. Table <ref type="table" target="#tab_2">2</ref> shows that p-dLDS outperforms all other models in state and dynamics reconstruction as well as the top-1 and top-3 reach classification accuracy. The learned patterns broadly generalize to the held-out data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Clinical Neurophysiology Data</head><p>We demonstrate p-dLDS on LFP recordings from the subcallosal cingulate cortex (SCC) in patients with treatment-resistant depression (ClinicalTrials.gov identifier NCT01984710). Subjects are asked to watch videos with different emotional content (positive, negative, and neutral), describe the videos, and then discuss how the video made them feel. SCC dynamics have been previously shown to provide a quantitative signal for the presence of emotional content <ref type="bibr" target="#b13">[14]</ref> and depression recovery <ref type="bibr" target="#b0">[1]</ref>. Thus, we hypothesize that the underlying dynamics may provide information about emotional changes throughout the experiment. We apply p-dLDS to a single patient's LFP spectrogram data (Fig. <ref type="figure" target="#fig_3">4B,</ref><ref type="figure">E</ref>) within the 0-40 Hz frequency range (M = 40). PCA indicates that the first 7 components explain 90% of the variance. Therefore, we train a model with N = 7 latent dimension and K = 4 DOs.</p><p>In Figure <ref type="figure" target="#fig_3">4</ref>, rSLDS and dLDS produces a high degree of state oscillations making it difficult to identify time intervals with consistent emotional content. In contrast, p-dLDS infers coherent structure that corresponds to changes in emotional content in the trial. For example, f 4 (red) coincides with resting, f 2 (orange) with positive videos, and f 1 and f 3 (blue and green) to negative videos (Fig. <ref type="figure" target="#fig_3">4C</ref>). Importantly, this structure persists even on held out data from the second half of the session (Fig. <ref type="figure" target="#fig_3">4</ref> F). We note this preliminary analysis on a single subject isn't intended to make a claim about specific neurophysiological responses to emotional content in this brain region, but generally highlights that p-dLDS identifies meaningful dynamical modes where previous models are unable to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we present a probabilistic decomposed linear dynamical systems model that can be used to discover meaningful representations in neural signals. By marginalizing over uncertainty in latent variable estimates and incorporating an offset into the dynamics, we enhance robustness and improve a variety of performance metrics. Some areas of future work includes exploiting structure in the offsets to automatically identify window size and extending the probabilistic model to include more complicated emissions distributions, such as the Poisson likelihood commonly used to model neural spiking data <ref type="bibr" target="#b27">[28]</ref>.</p><p>A Appendix / supplemental material A.1 p-dLDS Algorithm Algorithm 1 describes the proposed inference algorithm. In our experiments, we set n = 1 and η = 10 -4 and observe that the model converges. Below we use the notation hat notation for latent variable estimates or samples and the variable itself to represent the parameters of the variational distributions.</p><p>Algorithm 1 Variational EM for Probabilistic dLDS Require: M observation dimension, N latent state dimension, K number of dynamic operators, S moving average window size, ξ SBL-DF trade-off parameter, n number of samples to estimate expectations, η sparsity threshold, θ model parameters.</p><p>// Initialize parameters </p><formula xml:id="formula_20">c t ← 0 D i,j ∼ N (0, σ 2 ) f k,i,j ∼ N (0, σ 2 ) x t ← D + y t ▷</formula><formula xml:id="formula_21">← MovingAverage S ( x 1:T ) c t ∼ q(c t ) F t ← K k=1 f k c k,t l 1:T , Σ x ← KalmanSmoother(y 1:T , b 1:T , F 1:T , θ)</formula><p>// Update Coefficient Posterior Initialize q(c) and q(γ) jointly with SBL-DF. Update q(c t,k ) with SGD over equation <ref type="bibr" target="#b9">(10)</ref> for densities where |c t,k | &gt; η.</p><p>Update q(γ t ) ← IG(ξ</p><formula xml:id="formula_22">+ n 2 , ξc 2 t-1,k + n i=1 (c t,k,i -c t,k )<label>2 2</label></formula><p>) // Update Parameters Update θ with SGD over equation <ref type="bibr" target="#b10">(11)</ref>. end while B Latent Variable Inference B.1 Lemma 1 Derivation Lemma 1. Let the transition between any two state vectors x t , x t+1 ∈ R N be defined by the linear dynamics matrix F t ∈ R N ×N and the dynamics offset b t ∈ R N . For any λ &gt; 0, the objective,</p><formula xml:id="formula_23">arg min Ft,bt ∥x t+1 -x t -F t x t -b t ∥ 2 2 + λ∥F t ∥ 2 2 ,</formula><p>is minimized when F t = 0 and b t = x t+1 -x t .</p><p>Proof. Let r t = x t+1 -x t . We can rewrite the reconstruction objective in the following form,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arg min</head><p>Ft,bt</p><formula xml:id="formula_24">∥r t -F t x t -b t ∥ 2 2 + λ∥F t ∥ 2 2 .</formula><p>This objective is identical to the standard ridge regression with an unpenalized intercept term <ref type="bibr" target="#b12">[13]</ref>.</p><p>The solution is obtained by first centering the data, and then solving for the parameters using the solution for the standard Tikhonov regression. Below, we define the centered data as xt and rt for inputs and outputs respectively. Finally, we can use these values to obtain the following estimates of the parameters,</p><formula xml:id="formula_25">b t = µ t F t = (x ⊤ t xt + λI) -1 x⊤ t rt</formula><p>However, when there is only a single datapoint, we get that xt = 0, which results in F = 0.</p><p>This result arises from having only a single observation for any dynamic transition, which leads to a singular design matrix. Although we can improve our estimate of F t by collecting more samples along a given trajectory, this is impractical when dealing with naturalistic time-series. For instance, it may be infeasible to collect more data from the exact same initial condition in a naturalistic environment due to noise in the experimental setup. In chaotic systems, minor deviations can lead to drastically different outcomes over long time horizons. Even if it were possible to precisely control for the initial condition of the signal, the presence of dynamical noise can cause initially aligned time series to quickly drift out of alignment. Consequently, it is not uncommon to observe a single transition between any two time points, as it is not guaranteed that events across multiple trials will be well-aligned. Proof. Case 1. Let l ∼ N (µ l , Σ l ) and b ∼ N (µ b , Σ b ). The sum of normal random variables follows a distribution that results from convolving their individual distributions,</p><formula xml:id="formula_26">q(x) = q(l + b) = q(l) * q(b) = N (µ l , Σ l ) * N (µ l , Σ b ) = N (µ l + µ b , Σ l + Σ b )</formula><p>This is a standard result from probability theory.</p><p>Case 2. Now let l ∼ N (µ l , Σ l + Σ b ) and b ∼ δ(b -µ b ). Similarly, the distribution of the sum of these variables is distributed according to their convolution,</p><formula xml:id="formula_27">q(x) = q(l) * q(b) = N (µ l , Σ l + Σ b ) * δ(b -µ b ) = ∞ -∞ N (x -τ ; µ l , Σ l + Σ b )δ(τ -µ b )dτ = N (x + µ b ; µ l , Σ l + Σ b ) = N (x; µ l + µ b , Σ l + Σ b ),</formula><p>where the fourth line is the result of the sifting property of delta distributions. Since the final distribution in Case 1 and Case 2 are identical, we complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Optimal q(x) Update</head><p>The optimal coordinate ascent variational update is given by the following equation,</p><formula xml:id="formula_28">log q * (x) ∝ E q(c,γ) [log p(x, c, y, γ|θ)] = E q(c,γ) [log p(l 1 |θ) + T t=2 log p(l t |l t-1 , c t , θ) + T t=1 log p(y t |l t + b t , θ)] + C.<label>(12)</label></formula><p>Conditioned on estimates of b 1:T and samples of c 1:T , the factor graph of equation ( <ref type="formula" target="#formula_28">12</ref>) corresponds exactly to a time-varing Linear Gaussian State Space Model. Thus we can leverage the efficient inference algorithms such as the Kalman filter and RTS smoother when computing the marginals of the variational distribution of l 1:T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Simulated Monkey Reaching Task</head><p>Our dataset is constructed from publicly available data and code from the center-out reach task in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8]</ref>. We obtain latent factors from spiking networks that are trained to reproduce empirically measured EMG signals, given a 3-dimensional input that specifies the go input and the reach angle.</p><p>In our experiments, these factors are considered ground truth. Our trained factor-based spiking network then generates spiking activity for 1200 neurons. Synaptic currents are used as inputs into the Weighted Sum of synaptic currents LFP proxy method (WSLFP) <ref type="bibr" target="#b28">[29]</ref>, as implemented in the wslfp Python package <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16]</ref>. As WSLFP is a function of the relative location of neurons and electrodes, we place neurons randomly within a 5 mm by 10 mm by 1 mm region and electrodes in a grid centered in this region. The result is a multi-channel LFP dataset with nonlinear dynamics and measurements characteristic of systems neuroscience. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Evaluation Metrics D.1 Multi-step Inference</head><p>The multi-step inference performance is computed with the following R-squared metric,</p><formula xml:id="formula_29">R 2 k = 1 - T -k t=0 ∥y t+k -y t+k ∥ 2 2 T -k t=0 ∥y t+k -ȳ∥ 2 2 , (<label>13</label></formula><formula xml:id="formula_30">)</formula><p>where k is the number of steps from the initial condition, ȳ is the mean estimator for each trajectory and y t+k is the model prediction after applying the inferred dynamics for k steps. When testing, model parameters such as the dynamics and observation matrices are frozen, while specific latent variables are estimated based on the held-out data. In Table <ref type="table" target="#tab_1">1</ref>, we show results for k = 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Inferred Dynamics Error</head><p>We measure the accuracy of the latent dynamics with the mean squared error (MSE) of the inferred speed, defined as,</p><formula xml:id="formula_31">MSE speed = 1 T -1 T -1 t=1 ∥ ẋt -U ẋt ∥ 2 2 , (<label>14</label></formula><formula xml:id="formula_32">)</formula><p>where the true speed ẋt = x t+1 -x t is computed from the denoised ground truth latent state, and the predicted speed ẋt = x t+1 -x t is computed using the model's 1-step prediction. Since latent trajectories are only identifiable up to a linear transformation, we align the inferred trajectories with the true trajectories using a least squares fit before computing this score. More specifically, we find the optimal linear transformation U ∈ R N ×N between the estimated and true states across all trajectories by solving,</p><formula xml:id="formula_33">U = arg min U 1 T T t=1 ∥x t -U x t ∥.<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Inferred Latent State Space Error</head><p>Similarly, we measure the accuracy of the latent state space by computing the MSE after a linear alignment between trajectories from the inferred and true state space. We use this metric only for the reaching example, since the true observation function is a complex nonlinear function,</p><formula xml:id="formula_34">MSE state = 1 T T t=1 ∥x t -U x t ∥ 2 2 . (<label>16</label></formula><formula xml:id="formula_35">)</formula><p>The linear alignment U ∈ R N ×N between the estimated and true states across all trajectories is computed by solving the least squares problem in equation <ref type="bibr" target="#b14">(15)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Inferred switching rate error</head><p>Evaluating the accuracy of the switching behavior is a more difficult task. In fact, developing a procedure that matches predicted switch times with true switch times can lead to a complicated optimal transport procedure. To simplify the evaluation of switching times, we marginalize over time, and compare only the MSE of the switch rate defined as,</p><formula xml:id="formula_36">MSE switch = 1 m m i=1 ∥r i -r i ∥ 2 2 , (<label>17</label></formula><formula xml:id="formula_37">)</formula><p>where m is the number of trials, r i is the true switch rate for the ith trajectory, and</p><formula xml:id="formula_38">r i = 1 T T t=1 1{z t ̸ = z t-1</formula><p>} is the predicted switch rate. Intuitively, r i is the number of times that the state or dominant DO changes between consecutive time points normalized by the length of the interval T . In switching models, switch events are defined as a time point where the current inferred dynamical state differs from the state in the previous time step. Similarly in decomposed models, switch events are defined as time points where the active set of DOs change from the previous time step.</p><p>In the NASCAR example, r i is defined with the number of transitions between ground truth segments. In the Lorenz example, r i is defined by the number of times that the trajectory switches between the two lobes in addition to the number of ramping periods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Reaching Classification Accuracy</head><p>We quantitatively evaluate the reaching experiment with a classification task. Here, we want to determine whether the learned systems can be used to distinguish between different reach directions. Recall that switched models infer a switching variable for each time point where z t ∈ {1, . . . , K} while decomposed models infer a coefficient vector c t ∈ R K . Rather than viewing z t as an index, we can equivalently view it as a one-hot encoded vector z t ∈ {0, 1} K which describes whether a particular switching state is active at any given time. This matches the dimensionality of the variables in both switched and decomposed systems.</p><p>For simplicity, we focus on linear logistic regression classifiers in our experiment. If we let the inputs be z t and c t directly, then our classifiers quickly overfits since there are many more input features than trials. Specifically, the number of features scales linearly with the number of time points and systems O(T K). Instead, we marginalize over time and compute features from the estimated latent variables by averaging state activity over time. In switched models, this is the average one-hot encoding value over time. Similarly, this is the average coefficient value in decomposed models. However, for each dynamical state, we compute separate features for positive and negative coefficient values to prevent interference between them. In this setup, the input (feature) dimensionality scales according to O(K) while the output dimensionality of the linear classifiers are the reaching directions. For all classifiers, we perform a grid search over the values {10 i } 4 i=-4 to identify an appropriate amount of L2 regularization. Top-k accuracies are a standard metric in machine learning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref> and computed using the estimated class probabilities from the logistic regression classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Synthetic Dynamical Systems</head><p>Figure <ref type="figure" target="#fig_6">6A</ref> demonstrates that our inference procedure converges to a local optimium while Figure <ref type="figure" target="#fig_6">6B</ref> shows a full sweep of the multi-step inference metric. Tables 1 in the main paper reports the final value. For completeness, we include Tables <ref type="table" target="#tab_4">3</ref> and<ref type="table" target="#tab_5">4</ref> which reports the means across 5 seeds of each model, and includes the standard deviations in parenthesis.  <ref type="table" target="#tab_4">3</ref> and<ref type="table" target="#tab_5">4</ref> report the final values.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Reaching Task</head><p>For each model, we visualize the trial-averaged dynamic regime activity of each reach direction (Fig. <ref type="figure" target="#fig_7">7</ref>). In SLDS, this is visualized by considering the discrete states as a one hot vector over time. When a dynamic regime is active, that state will have a value of 1 while the unactive states will have a value of 0. Thus the trial averaged value of each state must have a value in the interval [0, 1]. In dLDS, we plot the inferred coefficient value without any modification.</p><p>Although SLDS correctly identifies preparatory and movement phases using states 4 and 3 respectively, it fails to differentiate dynamics occurring outside of these expected phases, incorrectly grouping unrelated regions together. Furthermore, the discrete formulation produces very similar activity patterns across all reach angles, obscuring any differences that are present. In dLDS, we observe that the features change smoothly and cyclically with the reach angle. However, the dynamic operator activity do not localize to the preparatory and movement phases due to a limited inference procedure.</p><p>Table <ref type="table">5</ref>: Inference performance for the reaching experiment (see Figure <ref type="figure" target="#fig_2">3</ref>) on a held-out test set.</p><p>Top-1 and Top-3 accuracies are obtained by predicting reach directions from latent variable features using linear classifiers. State and Dynamics MSE are computed with respect to true latent variables. We report standard deviations in parenthesis across 5 seeds.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Experimental Setup F.1 Hyperparameter Settings</head><p>For switching models, we rely on the ssm package which allows for efficient Bayesian inference for a variety of state space models <ref type="bibr" target="#b24">[25]</ref>. We set the variational posterior to structured_meanfield, and the fitting procedure to laplace_em as recommended by the developers. Additionally, we set the distributional form of the dynamics and emissions matrices to Gaussian.</p><p>The hyperparameters of dLDS primarily consists of the lagrange multipliers in the BPDN-DF objective including λ 0 , λ 1 , λ 2 . We find the optimal value of these hyperparameters using a random search with a fixed budget of 1000 evaluations. For each hyperparameter, we uniformly sample over the log of the interval [10 -3 , 10 3 ] and evaluate it against the BPDN-DF objective. For the NASCAR experiment, we found that λ 0 = 1.044, λ 1 = 0.254, and λ 2 = 0.023 resulted in the best performance.</p><p>For the Lorenz experiment, we found that λ 0 = 0.628, λ 1 = 2.010, and λ 2 = 0.0124 yielded the best performance.</p><p>For p-dLDS, the relevant hyperparameters consists of the SBL-DF dynamics tradeoff ξ, and the offset window size S. We use random search with a budget of 1000 samples to determine the values of S and ξ and fit a separate model for each set of hyperparameters. In the NASCAR experiment, we isolate the effect of the probabilistic inference procedure by setting S = T , removing the influence of the time-varying offset term. For ξ, we perform a random search by uniformly sample over the log of the interval [10 -3 , 10 3 ] and found that ξ = 0.945 was optimal. For the Lorenz experiment, we also optimize for the window size S by uniformly sample a discrete index on the interval {2, . . . , T }.</p><p>For the Lorenz experiment, the optimized hyperparameters are S = 85 and ξ = 8.928. For the real dataset, the optimal offset is S = 76 which is smaller than the timescale of p-dLDS coefficient switching (around 150 time points), suggesting that the same DO dynamics may persist even as the fixed points of the system fluctuates throughout the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Hardware Specification</head><p>We perform hyperparameter sweep on our institution's HPC cluster using small-scale CPU resources which consists of Dual Intel Xeon Gold 6226 CPUs. Once hyperparameters have been optimized, it is possible to run each experiment within approximately 2 hours on the 2020 edition of the M1 Macbook Pro.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Description of Clinical Neurophysiology Data</head><p>Data was collected as part of a study investigating deep brain stimulation for treatment-resistant depression (TRD). The study is pre-registered in ClinicalTrials.gov (identifier NCT04106466). The study protocol was approved by the IRB (identifier IRB00066843). Informed consent was obtained from participants before participation in the trial. Patients receive no monetary compensation, but instead have their DBS electrodes and Summit RC+S IPG device provided free of charge. The analysis focused on LFP signals from a single participant with all personally identifiable information removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Limitations</head><p>While our proposed method demonstrates strong performance in our experiments, there are many limitations. For instance, our approach does not have a strong mechanism for generating future unseen coefficients. Our assumed coefficient transition model is primarily motivated by our desire to obtain smooth coefficients over time. However, we believe that they may be more complex transition models that can both capture persistent activity in challenging systems while also being an accurate forecaster, such as a deep learning based transition model. Another limitation of our approach is that our method assumes smoothness in the latent space. However, we do not explore the possibility of having sparse structure in the latent space which can be easily accomplished in BPDN-DF by adding an L1 penalty over x.</p><p>NeurIPS Paper Checklist</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Claims</head><p>Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We propose a probabilistic treatment and an extended dynamics formulation in decomposed models (Section 3). We demonstrate that these changes reduce estimation errors and finds coherent structure where previous models fail in many challenging synthetic examples, and a noisy real-world example (Section 4). Guidelines:</p><p>• The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Limitations</head><p>Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: A limitations section is provided in Appendix H due to space constraints. Guidelines:</p><p>• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate "Limitations" section in their paper.</p><p>• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach.</p><p>For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Theory Assumptions and Proofs</head><p>Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p><p>Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p><p>Answer: <ref type="bibr">[Yes]</ref> Justification: We provide functions to generate the synthetic datasets from our experiments. Unfortunately, we are unable to release the neurophysiological dataset due to request from our collaborators.</p><p>Guidelines:</p><p>• The answer NA means that paper does not include experiments requiring code. • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Setting/Details</head><p>Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p><p>Answer: [Yes]</p><p>Justification: We specify that all datasets are split 50:50 for train and test in the main paper. Moreover, we provide optimizer and hyperparameter settings in Appendix F.1.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiment Statistical Significance</head><p>Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p><p>Answer: [Yes]</p><p>Justification: We provide standard deviations in Appendix E.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.</p><p>• The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. Guidelines:</p><p>• The answer NA means that there is no societal impact of the work performed.</p><p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Safeguards</head><p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p><p>Answer: [NA] Justification: Our method does not generate synthetic content that is at risk of abuse.</p><p>Guidelines:</p><p>• The answer NA means that the paper poses no such risks.</p><p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.</p><p>12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</p><p>Answer: [Yes]</p><p>Justification:</p><p>Guidelines:</p><p>• The answer NA means that the paper does not use existing assets.</p><p>• The authors should cite the original paper that produced the code package or dataset.</p><p>• The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p><p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We include details on where we obtained our data (either synthetically generated, or part of an existing dataset) in the main text and appendix. We also point to the commonly package used for SLDS and rSLDS in the appendix. Guidelines:</p><p>• The answer NA means that the paper does not release new assets.</p><p>• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.">Crowdsourcing and Research with Human Subjects</head><p>Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [Yes] Justification: We provide the IRB identifier in Appendix G which includes the informed consent form with experiment details. Guidelines:</p><p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.">Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects</head><p>Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [Yes] Justification: We include the IRB identifier number in Appendix G, but removed any information related to the institution conducting the experiments. Guidelines:</p><p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (A) Graphical model of dLDS. (B) p-dLDS includes hierarchical variables for probabilistic sparse inference and reparameterizes the latent space to include a time-varying offset term.</figDesc><graphic coords="4,108.00,72.00,396.01,107.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Probabilistic model and offset term reduce estimation errors. (A) Example trial from the NASCAR experiment colored by the true switching labels (not provided during training). Each track segment has a random speed τ . (B) Inferred state space, colored by discrete state or dominant coefficients. p-dLDS identifies correct track segments. (C) Example trial from the Lorenz experiment. The speed ramps according to the time intervals ∆τ in an ODE solver. (D) Inferred state space, colored by the dominant coefficients. The time-varying offset term allows p-dLDS coefficients to switch according to the true speed and accurately model the two fixed points in the opposing lobes.</figDesc><graphic coords="7,108.00,72.00,396.00,125.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: p-dLDS efficiently captures changes in dynamics. (A) Latent factors are computed from empirical EMG of the reaching experiment in [8]. Dynamics are characterized by a preparatory and movement phase. (B) Synthetic spikes and LFPs are generated using the wslfp package [17, 16] (C) The trial-averaged coefficients for p-dLDS smoothly vary with reaching angle. DO 1 captures preparatory dynamics while DO 3 captures movement dynamics. (D) Confusion matrix for linear classification of reach directions. p-dLDS predictions closely align to true diagonal.</figDesc><graphic coords="8,108.00,433.20,396.00,81.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Learned system discovers coherent structure in clinical data. (A, D) LFP data was collected on patients watching videos with different emotional content. (B, E) LFP spectrograms are 40-dimensional signals where each channel represents a particular frequency. (C, F) Inferred states and coefficients shows that rSLDS and dLDS exhibit unpredictable switching behavior. In contrast, p-dLDS captures smooth coefficients and identifies DOs that align with the trial's emotional content. The learned patterns broadly generalize to the held-out data.</figDesc><graphic coords="9,108.00,442.32,396.00,173.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>B. 2</head><label>2</label><figDesc>Lemma 2 Derivation Lemma 2. Let l, b ∈ R N be independent random variables such that l ∼ p(l) and b ∼ p(b). Their sum x = l + b is distributed according to N (µ l + µ b , Σ l + Σ b ) when 1) p(b) = N (µ b , Σ b ) and p(l) = N (µ l , Σ l ) and when 2) p(b) = δ(b -µ b ) and p(l) = N (µ l , Σ l + Σ b ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Empirically-Derived Reach Experiment. (A) 1,200 neurons are randomly placed into a 5 mm by 10 mm by 1 mm region. Electrodes are placed in a grid centered in this region (B) Spiking activity for a subset of neurons in an example trial produced from a factor-based spiking network. (C) First 15 channels in a simulated multi-channel LFP recording. Preparatory and Movement phases are marked by the dotted lines.</figDesc><graphic coords="17,108.00,216.05,396.01,110.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: (A) ELBO converges in both synthetic dynamical systems. (B) Multi-step inference where k represents the number of steps. Tables3 and 4report the final values.</figDesc><graphic coords="19,108.00,220.27,396.00,75.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Trial-averaged activity for (A) SLDS discrete states and (B) dLDS DO coefficients for each reach angle. The preparatory and movement phases occur between the dashed lines similar to Figure 3. Time 0 represents the onset of the stimulus.</figDesc><graphic coords="20,108.00,260.34,395.99,133.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">NASCAR</cell><cell></cell><cell></cell><cell>Lorenz</cell><cell></cell></row><row><cell>Model</cell><cell>Dynamics MSE (↓) (×10 -3 )</cell><cell>Switch MSE (↓) (×10 -3 )</cell><cell>100-step R 2 (↑)</cell><cell>Dynamics MSE (↓)</cell><cell>Switch MSE (↓)</cell><cell>100-step R 2 (↑)</cell></row><row><cell>SLDS</cell><cell>0.0995</cell><cell>12.89</cell><cell>0.184</cell><cell>0.431</cell><cell>0.0204</cell><cell>-3.47</cell></row><row><cell>rSLDS</cell><cell>0.1065</cell><cell>13.17</cell><cell>0.238</cell><cell>0.304</cell><cell>0.0208</cell><cell>-11.54</cell></row><row><cell>dLDS</cell><cell>123.19</cell><cell>13.28</cell><cell>✗</cell><cell>1.123</cell><cell>0.1529</cell><cell>✗</cell></row><row><cell>p-dLDS (ours)</cell><cell>0.033</cell><cell>7.34</cell><cell>0.450</cell><cell>0.141</cell><cell>0.0137</cell><cell>0.418</cell></row><row><cell cols="2">4.1 Synthetic Dynamical Systems</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Metrics for synthetic dynamical systems. Bold means best performance. (↑) indicates higher score is better while (↓) indicates that lower is better. ✗ indicates that value diverged towards -∞. Switch events for decomposed models are defined as times where the active set of DOs change from the previous time step.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Inference performance for the reaching experiment (see Figure3) on a held-out test set. Top-1 and Top-3 accuracies are obtained by predicting reach directions from latent variable features using linear classifiers. State and Dynamics MSE are computed with respect to true latent variables.</figDesc><table><row><cell>Model</cell><cell cols="2">Top-1 Acc. (↑) Top-3 Acc. (↑)</cell><cell>State MSE (↓) (×10 -1 )</cell><cell>Dynamics MSE (↓) (×10 -2 )</cell></row><row><cell>SLDS</cell><cell>38.46</cell><cell>57.69</cell><cell>0.5289</cell><cell>0.3942</cell></row><row><cell>rSLDS</cell><cell>12.82</cell><cell>32.05</cell><cell>0.5503</cell><cell>292.41</cell></row><row><cell>dLDS</cell><cell>10.25</cell><cell>39.74</cell><cell>0.6742</cell><cell>35.680</cell></row><row><cell>pdLDS (ours)</cell><cell>42.31</cell><cell>70.51</cell><cell>0.4061</cell><cell>0.0567</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Metrics for NASCAR. Bold means best performance. (↑) indicates higher score is better while (↓) indicates that lower is better. ✗ indicates that value diverged towards -∞. All MSE values are ×10 -3 while R 2 values are not scaled. We report means across 5 seeds and include standard deviation in parenthesis.</figDesc><table><row><cell>Model</cell><cell cols="3">Speed MSE (↓) Switch MSE (↓) 100-step R 2 (↑)</cell></row><row><cell>SLDS</cell><cell>0.0995 (0.021)</cell><cell>12.89 (1.30)</cell><cell>0.184 (0.024)</cell></row><row><cell>rSLDS</cell><cell>0.1065 (0.024)</cell><cell>13.17 (2.84)</cell><cell>0.238 (0.022)</cell></row><row><cell>dLDS</cell><cell>123.19 (23.13)</cell><cell>13.28 (5.31)</cell><cell>✗</cell></row><row><cell>p-dLDS (ours)</cell><cell>0.033 (0.009)</cell><cell>7.34 (3.40)</cell><cell>0.450 (0.027)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Metrics for Lorenz. Bold means best performance. (↑) indicates higher score is better while (↓) indicates that lower is better. ✗ indicates that value diverged towards -∞. We report means across 5 seeds and include standard deviation in parenthesis.</figDesc><table><row><cell>Model</cell><cell cols="3">Speed MSE (↓) Switch MSE (↓) 100-step R 2 (↑)</cell></row><row><cell>SLDS</cell><cell>0.431 (0.233)</cell><cell>0.0204 (0.007)</cell><cell>-3.47 (1.052)</cell></row><row><cell>rSLDS</cell><cell>0.304 (0.040)</cell><cell>0.0208 (0.004)</cell><cell>-11.54 (1.353)</cell></row><row><cell>dLDS</cell><cell>1.123 (0.089)</cell><cell>0.1529 (0.070)</cell><cell>✗</cell></row><row><cell>p-dLDS (ours)</cell><cell>0.141 (0.015)</cell><cell>0.0137 (0.014)</cell><cell>0.418 (0.079)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>• Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>• It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide details about hardware in Appendix F.2. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?</figDesc><table><row><cell>Answer: [Yes]</cell></row><row><cell>Justification: Most of our datasets are synthetically generated or derived from publicly</cell></row><row><cell>available sources, which we refer to throughout the paper. For our real-world experiment,</cell></row><row><cell>we include the IRB identifier.</cell></row><row><cell>Guidelines:</cell></row><row><cell>• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.</cell></row><row><cell>• If the authors answer No, they should explain the special circumstances that require a</cell></row><row><cell>deviation from the Code of Ethics.</cell></row><row><cell>• The authors should make sure to preserve anonymity (e.g., if there is a special consid-</cell></row><row><cell>eration due to laws or regulations in their jurisdiction).</cell></row><row><cell>10. Broader Impacts</cell></row><row><cell>Question: Does the paper discuss both potential positive societal impacts and negative</cell></row><row><cell>societal impacts of the work performed?</cell></row><row><cell>Answer: [NA]</cell></row></table><note><p>Justification: Our proposed model aims to reveal coherent patterns in time-series data, serving as a scientific tool similar to PCA. As a probabilistic technique not designed for content generation or automated decision-making, we do not anticipate direct societal impacts.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Code is available at: https://github.com/siplab-gt/probabilistic-decomposed-linear-dynamical-systems 38th Conference on Neural Information Processing Systems (NeurIPS</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2024).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements and Disclosure of Funding Sources</head><p>Y.C. and C.R. were funded by the <rs type="funder">James S. McDonnell Foundation</rs> (grant number <rs type="grantNumber">22002039</rs>), with Y.C. being further funded by <rs type="funder">National Institutes of Health</rs> (grant number <rs type="grantNumber">2T32EB025816</rs>), and C.R. being further funded by the <rs type="funder">Julian T. Hightower Chair</rs>. Y.C. and K.J. were part of the <rs type="funder">Georgia Tech/Emory NIH/NIBIB Training Program in Computational Neural Engineering</rs> (<rs type="grantNumber">T32EB025816</rs>). N.M. was funded by <rs type="funder">The Kavli Foundation NeuroData Discovery award</rs>. A.S.C. were partially supported by the <rs type="funder">NSF</rs> <rs type="grantName">CAREER Award</rs> (<rs type="grantNumber">2340338</rs>) and a <rs type="grantName">Johns Hopkins Bridge Grant</rs>. S.A. is supported by the <rs type="funder">National Center for Advancing Translational Sciences</rs> of the <rs type="funder">National Institutes of Health</rs> (Award Number <rs type="grantNumber">UL1TR002378</rs> and <rs type="grantNumber">KL2TR002381</rs>). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pbkPcRf">
					<idno type="grant-number">22002039</idno>
				</org>
				<org type="funding" xml:id="_9X2Mahj">
					<idno type="grant-number">2T32EB025816</idno>
				</org>
				<org type="funding" xml:id="_B3UGc9x">
					<idno type="grant-number">T32EB025816</idno>
				</org>
				<org type="funding" xml:id="_rpqzDPd">
					<idno type="grant-number">2340338</idno>
					<orgName type="grant-name">CAREER Award</orgName>
				</org>
				<org type="funding" xml:id="_eStQBWY">
					<orgName type="grant-name">Johns Hopkins Bridge Grant</orgName>
				</org>
				<org type="funding" xml:id="_qxnkW3u">
					<idno type="grant-number">UL1TR002378</idno>
				</org>
				<org type="funding" xml:id="_jU4Dx9V">
					<idno type="grant-number">KL2TR002381</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Generating Synthetic Examples</head><p>C.1 Noisy NASCAR NASCAR data is generated by partitioning the two-dimensional state space into four regions according to the rules,</p><p>where Z(x) is the ground truth switching state function that depends on the particular location x. The ground truth dynamics matrices are defined as,</p><p>and ground truth offsets are defined as,</p><p>Given the current location in state space x t , we can transition to the next point using the continuous time dynamics equation</p><p>where each entry of the process noise is sampled from ν t,i ∼ N (0, 10 -4 ). To modulate the speed of the system, we uniformly sample a speed constant τ ∈ [0.1, 1], which is applied throughout each segment of the track. We use the continuous time formulation over the discrete-time formulation to ensure that changes to the speed do not distort the shape of the original system's state space. To generate noisy observations, we construct a linear emissions matrix with random variables such that each entry is given by D i,j ∼ N (0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Ramping Lorenz</head><p>In order to modulate the speed of the Lorenz system, we adjust the evaluation time points of an ODE integrator, specifically Runge-Kutta of the order 5(4) (RK54) as implemented in scipy's solve_ivp <ref type="bibr" target="#b8">[9]</ref>. Ramping activity is generated randomly with the following procedure, 1. Uniformly sample an evaluation interval length τ ∈ [0.25, 1.5].</p><p>2. Construct a vector T that consists n evenly spaced numbers over the interval [0, τ ]. In our experiments, we set n to be 100.</p><p>3. Perform the transformation exp( T ) -1 to obtain a vector of ramped evaluation times.</p><p>4. Plug in the transformed evaluation times into the RK45 Solver to obtain latent trajectories.</p><p>Similar to the NASCAR experiment, we generate noisy observation from a randomly constructed linear emissions matrix such that each entry is given by D i,j ∼ N (0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer: [Yes]</head><p>Justification: We provide proofs of our lemmas in B.1 and B.2. Guidelines:</p><p>• The answer NA means that the paper does not include theoretical results.</p><p>• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems.</p><p>• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Result Reproducibility</head><p>Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide synthetic data details in Appendix C, metric definitions in Appendix D, and hyperparameter details in Appendix F.1. Moreover, we release our code with our submission.</p><p>Guidelines:</p><p>• The answer NA means that the paper does not include experiments.</p><p>• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways.</p><p>For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. , with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.</p><p>In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cingulate dynamics track depression recovery with deep brain stimulation</title>
		<author>
			<persName><forename type="first">Ki</forename><forename type="middle">Sueng</forename><surname>Sankaraleengam Alagapan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricio</forename><surname>Heisig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Riva-Posse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Crowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosadoluwa</forename><surname>Tiruvadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashan</forename><surname>Obatusin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allison</forename><forename type="middle">C</forename><surname>Veerakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Waters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinead</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lydia</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><surname>Denison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marissa</forename><surname>Shaughnessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungho</forename><surname>Canal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">H</forename><surname>Hershenberg ; Martijn Figee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kopell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><forename type="middle">S</forename><surname>Butera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Mayberg</surname></persName>
		</author>
		<author>
			<persName><surname>Rozell</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-023-06541-3</idno>
		<ptr target="https://doi.org/10.1038/s41586-023-06541-3" />
	</analytic>
	<monogr>
		<title level="j">Tanya Nauvel, Faical Isbaine, Muhammad Furqan Afzal</title>
		<idno type="ISSN">1476-4687</idno>
		<imprint>
			<date type="published" when="2023-09">Sep 2023</date>
		</imprint>
	</monogr>
	<note>Nature</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gene selection using a two-level hierarchical bayesian model</title>
		<author>
			<persName><forename type="first">Kyounghwa</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bani</surname></persName>
		</author>
		<author>
			<persName><surname>Mallick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3423" to="3430" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Berrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M Pawan</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07595</idno>
		<title level="m">Smooth loss functions for deep top-k classification</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The horseshoe estimator for sparse signals</title>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">M</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">G</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="465" to="480" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Penalized regression, standard errors, and bayesian lassos</title>
		<author>
			<persName><forename type="first">George</forename><surname>Casella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malay</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjung</forename><surname>Kyung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic filtering of time-varying sparse signals via ℓ 1 minimization</title>
		<author>
			<persName><forename type="first">Aurele</forename><surname>Adam S Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Balavoine</surname></persName>
		</author>
		<author>
			<persName><surname>Rozell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="5644" to="5656" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The dynamics of resting fluctuations in the brain: metastability and its dynamical cortical core</title>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Deco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morten</forename><forename type="middle">L</forename><surname>Kringelbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Viktor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Jirsa</surname></persName>
		</author>
		<author>
			<persName><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3095</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The centrality of population-level factors to network computation is demonstrated by a versatile approach for training spiking networks</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Depasquale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">M</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName><surname>Churchland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="631" to="649" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A family of embedded runge-kutta formulae</title>
		<author>
			<persName><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Dormand</surname></persName>
		</author>
		<author>
			<persName><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive sparseness for supervised learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Mário</surname></persName>
		</author>
		<author>
			<persName><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1150" to="1159" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spontaneous fluctuations in brain activity observed with functional magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><forename type="middle">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><surname>Raichle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="700" to="711" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Brain signal variability and executive functions across the life span</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">S</forename><surname>Zachary T Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salome</forename><surname>Nomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Kornfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">A</forename><surname>Bolt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Celia</forename><surname>Saumure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sierra</forename><forename type="middle">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucina</forename><forename type="middle">Q</forename><surname>Bainter</surname></persName>
		</author>
		<author>
			<persName><surname>Uddin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network Neuroscience</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The elements of statistical learning: data mining, inference, and prediction</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Processing of emotional stimuli is reflected by modulations of beta band activity in the subgenual anterior cingulate cortex in patients with treatment resistant depression</title>
		<author>
			<persName><forename type="first">Julius</forename><surname>Huebl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Brücke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Merkl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malek</forename><surname>Bajbouj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerd-Helge</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">A</forename><surname>Kühn</surname></persName>
		</author>
		<idno type="DOI">10.1093/scan/nsw038</idno>
		<ptr target="https://doi.org/10.1093/scan/nsw038" />
	</analytic>
	<monogr>
		<title level="j">Social Cognitive and Affective Neuroscience</title>
		<idno type="ISSN">1749-5016</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1290" to="1298" />
			<date type="published" when="2016">03 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Kaoru</forename><surname>Irie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12275</idno>
		<title level="m">Bayesian dynamic fused lasso</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">AlissaW0921, and oliviaklemmer</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Johnsen</surname></persName>
		</author>
		<ptr target="https://zenodo.org/records/10819758" />
	</analytic>
	<monogr>
		<title level="m">Siplab-gt/wslfp: V0.2.1. Zenodo</title>
		<imprint>
			<date type="published" when="2024-03">March 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cleo: A testbed for bridging model and experiment by simulating closed-loop stimulation, electrode recording, and optogenetics</title>
		<author>
			<persName><forename type="first">Nathanael</forename><forename type="middle">A</forename><surname>Kyle A Johnsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">A</forename><surname>Cruzado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Willats</surname></persName>
		</author>
		<author>
			<persName><surname>Rozell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2024" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bayesian fused lasso modeling via horseshoe prior</title>
		<author>
			<persName><forename type="first">Yuko</forename><surname>Kakikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaito</forename><surname>Shimamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuichi</forename><surname>Kawano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Japanese Journal of Statistics and Data Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="705" to="727" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The roles of monkey m1 neuron classes in movement preparation and execution</title>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">M</forename><surname>Matthew T Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Churchland</surname></persName>
		</author>
		<author>
			<persName><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neurophysiology</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="817" to="825" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inferring latent dynamics underlying neural population activity via neural differential equations</title>
		<author>
			<persName><forename type="first">Timothy Doyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zhihao Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">W</forename><surname>Pillow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">D</forename><surname>Brody</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Loss functions for top-k error: Analysis and insights</title>
		<author>
			<persName><forename type="first">Maksim</forename><surname>Lapin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1468" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Different population dynamics in the supplementary motor area and motor cortex during reaching</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Lara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Churchland</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-018-05146-z</idno>
		<ptr target="https://www.nature.com/articles/s41467-018-05146-z" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<idno type="ISSN">2041-1723</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2754</biblScope>
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Warrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">I</forename><surname>Glaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">W</forename><surname>Linderman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.03291</idno>
		<title level="m">Switching autoregressive low-rank tensor models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lin</surname></persName>
		</author>
		<title level="m">The bayesian elastic net</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">SSM: Bayesian Learning and Inference for State Space Models</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Antin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zoltowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Glaser</surname></persName>
		</author>
		<ptr target="https://github.com/lindermanlab/ssm" />
		<imprint>
			<date type="published" when="2020-10">October 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Scott W Linderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Paninski</surname></persName>
		</author>
		<author>
			<persName><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.08466</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Recurrent switching linear dynamical systems</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lévy walk dynamics explain gamma burst patterns in primate cerebral cortex</title>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">G</forename><surname>Paul R Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pulin</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications Biology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">739</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Empirical models of spiking in neural populations</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Jakob H Macke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">M</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">V</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><surname>Sahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Computing the Local Field Potential (LFP) from Integrate-and-Fire Network Models</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Mazzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Lindén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Cuntz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Lansner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Panzeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaute</forename><forename type="middle">T</forename><surname>Einevoll</surname></persName>
		</author>
		<idno type="DOI">https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004584</idno>
		<ptr target="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004584" />
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<idno type="ISSN">1553-7358</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2015">1004584. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decomposed linear dynamical systems (dlds) for learning the latent components of neural dynamics</title>
		<author>
			<persName><forename type="first">Noga</forename><surname>Mudrik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yenho</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Yezerets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Rozell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">S</forename><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Ramping activity is a cortical mechanism of temporal control of action. Current opinion in behavioral sciences</title>
		<author>
			<persName><surname>Nandakumar S Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="226" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tree-structured recurrent switching linear dynamical systems for multi-scale modeling</title>
		<author>
			<persName><forename type="first">Josue</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">W</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Bugallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Il Memming</forename><surname>Park</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=HkzRQhR9YX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sparse bayesian learning with dynamic filtering for inference of time-varying sparse signals</title>
		<author>
			<persName><forename type="first">R O'</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Shaughnessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName><surname>Rozell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="388" to="403" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inferring single-trial neural population dynamics using sequential auto-encoders</title>
		<author>
			<persName><forename type="first">Chethan</forename><surname>Pandarinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J O'</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Shea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">C</forename><surname>Stavisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">T</forename><surname>Trautmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">I</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leigh</forename><forename type="middle">R</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><surname>Hochberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="805" to="815" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimates of linear dynamic systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Herbert E Rauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlotte</forename><forename type="middle">T</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><surname>Striebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIAA journal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1445" to="1450" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Slow fluctuations in ongoing brain activity decrease in amplitude with ageing yet their impact on task-related evoked responses is dissociable from behavior</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Castelo-Branco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Elife</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">75722</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contribution of lfp dynamics to single-neuron spiking variability in motor cortex during movement execution</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Michael E Rule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Vargas-Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename><surname>Donoghue</surname></persName>
		</author>
		<author>
			<persName><surname>Truccolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in systems neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">89</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learnable latent embeddings for joint behavioural and neural analysis</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Hwa</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mackenzie</forename><forename type="middle">Weygandt</forename><surname>Mathis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">617</biblScope>
			<biblScope unit="issue">7960</biblScope>
			<biblScope unit="page" from="360" to="368" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The dynamics of functional brain networks: integrated network states during cognitive task performance</title>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">G</forename><surname>James M Shine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Bissett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oluwasanmi</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">H</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><surname>Balsters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krzysztof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><forename type="middle">A</forename><surname>Gorgolewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><forename type="middle">A</forename><surname>Moodie</surname></persName>
		</author>
		<author>
			<persName><surname>Poldrack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="544" to="554" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling multiscale causal interactions between spiking and field potential signals during behavior</title>
		<author>
			<persName><forename type="first">Chuanmeizhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bijan</forename><surname>Pesaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><forename type="middle">M</forename><surname>Shanechi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neural engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">26001</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Behavior needs neural variability</title>
		<author>
			<persName><forename type="first">Leonhard</forename><surname>Waschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niels</forename><forename type="middle">A</forename><surname>Kloosterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Obleser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">D</forename><surname>Garrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="751" to="766" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Modeling and decoding motor cortical activity using a switching kalman filter</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mumford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elie</forename><surname>Bienenstock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Donoghue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="933" to="942" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Time-resolved resting-state brain networks</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zalesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fornito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Cocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><forename type="middle">L</forename><surname>Gollo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Breakspear</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page" from="10341" to="10346" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Unifying and generalizing models of neural dynamics during decision-making</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">W</forename><surname>David M Zoltowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">W</forename><surname>Pillow</surname></persName>
		</author>
		<author>
			<persName><surname>Linderman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04571</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
