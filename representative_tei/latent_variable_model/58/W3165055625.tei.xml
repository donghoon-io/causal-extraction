<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Données manquantes dans un modèle à blocs latents pour la recommandation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gabriel</forename><surname>Frisch</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 7253</orgName>
								<orgName type="institution" key="instit1">Sorbonne universités</orgName>
								<orgName type="institution" key="instit2">Université de technologie de Compiègne</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>60203</postCode>
									<settlement>Compiègne Cedex</settlement>
									<country>Heudiasyc France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jean-Benoist</forename><surname>Leger</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 7253</orgName>
								<orgName type="institution" key="instit1">Sorbonne universités</orgName>
								<orgName type="institution" key="instit2">Université de technologie de Compiègne</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>60203</postCode>
									<settlement>Compiègne Cedex</settlement>
									<country>Heudiasyc France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 7253</orgName>
								<orgName type="institution" key="instit1">Sorbonne universités</orgName>
								<orgName type="institution" key="instit2">Université de technologie de Compiègne</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>60203</postCode>
									<settlement>Compiègne Cedex</settlement>
									<country>Heudiasyc France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Données manquantes dans un modèle à blocs latents pour la recommandation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommandation sociale</term>
					<term>LBM</term>
					<term>NMAR Collaborative filtering</term>
					<term>LBM</term>
					<term>NMAR</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nous présentons un modèle statistique basé sur le LBM pour réaliser une recommandation sociale. Le modèle utilise des variables latentes pour modéliser un processus de manquement de données de type NMAR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Les systèmes de recommandation sont des outils d'aide à la décision réalisant une extraction de l'information afin de présenter des résultats pertinents aux utilisateurs. Il existe différents types de systèmes de recommandation. Certaines approches utilisent les caractéristiques des items ou des utilisateurs afin de les lier entre eux <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. D'autres, utilisant la recommandation sociale ou Collaborative Filtering, se servent de l'historique des avis pour prédire l'intérêt des utilisateurs. Les données qui y sont associées peuvent être explicites, typiquement une note indiquant l'appréciation de l'utilisateur, ou implicites ; c'est à dire basées sur le comportement des utilisateurs (achats, clics, durée sur une page). Ces systèmes de recommandation sociale présupposent l'existence de préférences qui soient communes à certains utilisateurs et à certains items.</p><p>Des modèles statistiques peuvent être développés pour découvrir ces classes de comportement commun. Un réseau de recommandation peut être modélisé par un graphe bipartite, ayant pour noeuds les utilisateurs d'une part, et les items d'autre part ; les arêtes (possiblement valuées) représentent les avis. La matrice d'adjacence de ce graphe contient ainsi l'ensemble de l'information utilisée.</p><p>Une grande variété d'algorithmes ont été proposés pour retrouver des structures dans des matrices de données. La classification double ou co-clustering <ref type="bibr" target="#b2">[3]</ref> considère simultanément les lignes et les colonnes afin d'organiser les données en blocs homogènes. Le modèle à bloc latents, ou Latent Block Model (LBM ) proposé par Govaert et Nadif <ref type="bibr" target="#b1">[2]</ref> exploite ce concept en y introduisant un modèle de mélange. Les paramètres du LBM sont ajustés par un algorithme itératif basé sur l'algorithme EM de Dempster et al. <ref type="bibr" target="#b0">[1]</ref>. Dans nos travaux, nous étendons le modèle de base du LBM pour l'utiliser à des fins de recommandations.</p><p>Classiquement, les modèles de réseaux de recommandation supposent que les données manquantes sont indépendantes de leurs valeurs ; c'est l'hypothèse Missing (completely) At Random (MAR) <ref type="bibr" target="#b4">[5]</ref>. Sous cette hypothèse, aucun processus de manquement de données n'est à considérer dans le modèle statistique et l'inférence est réalisée en utilisant uniquement la vraisemblance des données observées. Cependant, les travaux de Marlin et Zemel <ref type="bibr" target="#b5">[6]</ref> suggèrent que le processus de manquement des jeux de données des les réseaux de recommandations sont principalement de type Not Missing At Random (NMAR). C'està-dire qu'il existe un lien sous-jacent dans les données, qui influe sur la présence ou non, d'avis. Ainsi, si un biais est introduit dans l'estimation du modèle, les performances du réseau de recommandation pourraient être significativement dégradées <ref type="bibr" target="#b5">[6]</ref>. Dans nos travaux, nous faisons l'hypothèse que les données manquantes sont de type NMAR et nous modélisons le mécanisme de manquement. Nous supposons par exemple que la propension à donner son avis n'est pas la même pour un objet que l'on apprécie que pour un autre que l'on n'apprécie pas.</p><p>L'objectif de cet article est de proposer un modèle statistique du phénomène NMAR dans un LBM , ainsi que les grandes étapes de l'inférence. Dans un second temps, des résultats sur des données simulées sont présentés.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Modèle</head><p>Le modèle présenté est un modèle de graphe aléatoire bipartite, basé sur le Latent Block Model . La modélisation peut être faite en deux étapes. Dans la première, le graphe non totalement observé est modélisé comme un Latent Block Model classique. Dans la deuxième étape, le mécanisme de manquement modélise le graphe dans lequel seulement une partie des arêtes est observée.</p><p>Le graphe étant bipartite, nous aurons deux types de noeuds. Typiquement dans le cas d'un réseau de recommandation, les noeuds de type (1) représentent les personnes et les noeuds de type (2) représentent les objets. Nous considérons un graphe composé de n 1 noeuds de type (1), et n 2 noeuds de type (2). Le modèle sera présenté pour k 1 classes sur les noeuds de type (1) et k 2 classes de type (2). X (c) représente la matrice d'adjacence du graphe valué complet. Les données associées à notre réseau de recommandation sont des avis binaires et explicites de l'utilisateur : nous notons X Latent Block Model L'appartenance de chaque noeud à une classe est ici modélisée par une variable latente. Toutes ces variables latentes sont considérées comme indépendantes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∀i, Y</head><p>(1) i iid ∼ M 1; α (1)  ,</p><formula xml:id="formula_0">α (1) ∈ R k 1 , k 1 q=1 α (1) q = 1 ∀j, Y (2) j iid ∼ M 1; α (2) , α (2) ∈ R k 2 , k 2 l=1 α (2) l = 1</formula><p>Puis, l'arête entre le noeud i de type (1) et le noeud j de type (2), notée</p><formula xml:id="formula_1">X (c) ij est considérée comme dépendant uniquement des classes des noeuds i et j. ∀i, j, X (c) ij Y (1) i = q, Y (2) j = l ind ∼ B -1,+1 (π ql ), π ql ∈ [0, 1]</formula><p>Loi de l'observation Nous considérons que X (c) n'est que partiellement observé. Nous notons X (o) la matrice d'adjacence du graphe observé, c'est-à-dire soumis au mécanisme de manquement. La valeur observée de l'arête entre le noeud i de type (1) et le noeud</p><formula xml:id="formula_2">j de type (2) noté X (o) ij dépend de la valeur de X (c) ij et des caractéristiques latentes des noeuds i et j. Le fait que X (o) ij dépende de X (c)</formula><p>ij rend le modèle Not Missing At Random, c'est-à-dire que la probabilité d'observer une variable dépend de la valeur de la variable. Une donnée manquante entre l'utilisateur i et l'objet j sera notée X (o) ij = 0 Le mécanisme de manquement d'un lien est dépendant de la nature du réseau considéré. Nous considérons que le comportement des personnes peut être modélisé par deux variables latentes continues. Pour chaque personne i, nous considérons son affinité A i à évaluer des objets, et B i la sur-affinité à évaluer ces objets qu'elle apprécie. Ainsi, pour la personne i, son affinité à évaluer un objet apprécié sera A i + B i et son affinité à évaluer un objet non apprécié sera</p><formula xml:id="formula_3">A i -B i . ∀i, A i iid ∼ N (0, σ 2 A ) , σ 2 A ∈ R * + ∀i, B i iid ∼ N (0, σ 2 B ) , σ 2 B ∈ R * +</formula><p>De façon similaire, nous introduisons une variable latente continue P j modélisant la popularité d'un objet j.</p><formula xml:id="formula_4">∀j, P j iid ∼ N 0, σ 2 P , σ 2 P ∈ R * +</formula><p>On considère que les affinités à noter du noeud i de type (1) et la popularité du noeud j de type (2) influent linéairement sur la log-cote de la probabilité d'observation, telle que classiquement utilisée dans une régression logistique. On a donc : </p><formula xml:id="formula_5">∀i, j, X (o) ij X (c) ij , A i , B i , P j ind ∼ B 0,+1 logit -1 (µ +1 + A i + B i + P j ) si X (c) ij = +1 B 0,-1 logit -1 (µ -1 + A i -B i + P j ) si X (c) ij = -1<label>3</label></formula><formula xml:id="formula_6">Y (1) i Y (2) j A i B i P j LBM X (c) ij X (o) ij</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Inférence</head><p>Durant l'inférence, le modèle équivalent suivant liant X (o) à Y (1) et Y (2) sera utilisé : -La modélisation des variables latentes (excepté X (c) est la même) :</p><formula xml:id="formula_7">               ∀i, Y (1) i iid ∼ M 1; α (1) ∀j, Y (2) j iid ∼ M 1; α (2) ∀i, A i iid ∼ N (0, σ 2 A ) ∀i, B i iid ∼ N (0, σ 2 B ) ∀j, P j iid ∼ N (0, σ 2 P )</formula><p>-Le graphe observé dépend maintenant directement de toutes les variables latentes.</p><formula xml:id="formula_8">X (o) ij Y (1) i = q, Y<label>(2) j</label></formula><formula xml:id="formula_9">= l, A i , B i , P j ind ∼ Cat     +1 -1 0   ,   π ql logit -1 (µ +1 + A i + B i + P j ) (1 -π ql ) logit -1 (µ -1 + A i -B i + P j ) (• • • )     avec (• • •</formula><p>) le terme calculé tel que le vecteur somme à 1, et où Cat (X, p) représente la distribution catégorielle à valeurs dans X avec les probabilités associées p ∈ R card X + et k p k = 1. Les paramètres du modèle sont donc α (1) , α (2) , σ 2 A , σ 2 B , σ 2 P , µ -1 , µ +1 , π. Afin d'inférer les paramètres et de prédire les variables latentes, nous maximisons la vraisemblance. La vraisemblance incomplète, c'est-à-dire celle de X (o) n'est pas calculable, pour maximiser celle-ci, l'algorithme EM est utilisé. Toutefois, le calcul de l'espérance de la log-vraisemblance complète conditionnellement à X (o) à paramètres fixés est impossible. Une formulation variationnelle de l'EM est utilisée <ref type="bibr" target="#b3">[4]</ref>, et une restriction sur l'espace de recherche est faite, ce qui constitue une approximation du critère variationnel et des éléments qui le composent. Pour le calcul de certaines espérances sous la distribution variationnelle, une approximation au moyen d'un développement de Taylor à l'ordre trois est utilisée. L'erreur est contrôlée en utilisant la variance variationnelle.</p><p>Une implémentation de cette inférence a été réalisée en python, permettant d'analyser des réseaux de quelques centaines de noeuds. Cette implémentation utilise la differentiation automatique afin de calculer de manière exacte les gradients, jacobiens, et hessiens impliqués dans le développement de Taylor, et de calculer exactement les gradients des critères. Un algorithme de quasi-Newton est utilisé pour la maximisation. Le nombre de paramètres variationnel étant grand, le L-BFGS est utilisé afin de ne pas calculer ni stocker explicitement le hessien approché, et de le manipuler seulement de manière implicite. L'objectif principal de l'étude expérimentale est de vérifier si l'algorithme proposé peut inférer les paramètres du modèle et prédire de façon cohérente les données manquantes. Les données d'expérimentation sont générées suivant notre modèle. Les résultats sont présentés à nombre de classes fixées (k 1 = k 2 = 3), et en faisant varier le nombre d'individus et d'items (n 1 = n 2 ). Les paramètres µ +1 , µ -1 , σ 2 A , σ 2 B et σ 2 P sont fixés à 1 donnant ainsi un taux de manquement moyen de 0.34.</p><p>Notre algorithme rencontrant encore des difficultés d'initialisation, similaires à celles rencontrées dans le LBM standard, l'algorithme est initialisé avec des valeurs proches des variables latentes simulées afin de minimiser le risque de se retrouver dans un minimum local.</p><p>Les résultats sur 10 répétitions, présentés figure <ref type="figure">2</ref>, montrent qu'avec l'augmentation de la taille du graphe, la distance infinie entre le paramètre π simulé et π inféré tend à diminuer. Il en est de même avec l'erreur de prédiction des données manquantes, présentée figure <ref type="figure">3</ref>, calculée en utilisant un prédicteur construit à partir des maximums a posteriori sous hypothèse variationnelle. Ces premiers résultats permettent de penser que l'algorithme peut donc inférer de façon cohérente les paramètres du modèle.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(c) ij = +1 un avis positif et X (c) ij = -1 un avis négatif d'un utilisateur i pour un objet j. Nous soulignons le fait que notre modèle peut être facilement étendu pour utiliser un système de notation à valeurs entières. Nous notons B a,b (p) la loi de transformation de la variable de Bernoulli suivante : une variable aléatoire qui prend b avec probabilité p et a sinon. Conceptuellement, B a,b (p) = a + (b -a)B(p), où B est la loi de Bernouilli classique.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 -</head><label>1</label><figDesc>Figure 1 -Modèle graphique des variables aléatoires pour un individu i et un item j donnés. Le double cercle représente la variable observée.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society : Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clustering with block mixture models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Govaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nadif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="463" to="473" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Co-clustering : models, algorithms and applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Govaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nadif</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ISTE Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tutorial on variational approximation methods</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced mean field methods : theory and practice</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="129" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><surname>Rubin</surname></persName>
		</author>
		<title level="m">Statistical analysis with missing data</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collaborative prediction and ranking with nonrandom missing data</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third ACM conference on Recommender systems</title>
		<meeting>the third ACM conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="5" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Item-based collaborative filtering recommendation algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on World Wide Web</title>
		<meeting>the 10th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Matrix factorization and neighbor based algorithms for the netflix prize problem</title>
		<author>
			<persName><forename type="first">G</forename><surname>Takács</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pilászy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Németh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tikk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys&apos;08 : Proceedings of the 2008 ACM Conference on Recommender Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="267" to="274" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
