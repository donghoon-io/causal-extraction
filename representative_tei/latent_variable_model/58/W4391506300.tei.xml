<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Score-based Causal Representation Learning: Linear and General Transformations</title>
				<funder ref="#_MrfuPg5">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">IBM</orgName>
				</funder>
				<funder>
					<orgName type="full">IBM-Rensselaer Future of Computing Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-07-19">19 Jul 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Burak</forename><surname>Varıcı</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Emre</forename><surname>Acartürk</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
							<email>karthikeyanvs@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
							<email>abhishek.mlwork@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><surname>Tajer</surname></persName>
							<email>tajer@ecse.rpi.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Department</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Electrical, Computer, and Systems Engineering Rensselaer Polytechnic Institute</orgName>
								<address>
									<addrLine>Google DeepMind India Bengaluru 560043</addrLine>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country>USA India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Electrical, Computer, and Systems Engineering Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Score-based Causal Representation Learning: Linear and General Transformations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-07-19">19 Jul 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2402.00849v5[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>causal representation learning</term>
					<term>causality</term>
					<term>interventions</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the identifiability and achievability aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure the recovery of the true latent causal variables and the underlying latent causal graph. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between score functions (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a score-based class of algorithms that ensures both identifiability and achievability. First, the paper focuses on linear transformations and shows that one stochastic hard intervention per node suffices to guarantee identifiability. It also provides partial identifiability guarantees for soft interventions, including identifiability up to mixing with parents for general causal models and perfect recovery of the latent graph for sufficiently nonlinear causal models. Secondly, it focuses on general transformations and demonstrates that two stochastic hard interventions per node are sufficient for identifiability. This is achieved by defining a differentiable loss function whose global optima ensure identifiability for general CRL. Notably, one does not need to know which pair of interventional environments has the same node intervened. Finally, the theoretical results are empirically validated via experiments on structured synthetic data and image data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Overview</head><p>Causal representation learning (CRL) aims to form a causal understanding of the world by learning appropriate representations that support causal interventions, reasoning, and planning <ref type="bibr" target="#b41">(Schölkopf et al., 2021)</ref>. Specifically, CRL considers a data-generating process in which high-level latent causally-related variables are mapped to low-level, generally highdimensional observed data through an unknown transformation. Formally, consider a causal Bayesian network <ref type="bibr" target="#b38">(Pearl, 2009)</ref> encoded by a directed acyclic graph (DAG) G with n nodes and generating causal random variables Z ≜ [Z 1 , . . . , Z n ] ⊤ . These random variables are transformed by an unknown function g : R n → R d to generate the d-dimensional observed random variables X ≜ [X 1 , . . . , X d ] ⊤ according to:</p><formula xml:id="formula_0">X = g(Z) .</formula><p>(1)</p><p>CRL is the process of using the observed data X and recovering (i) the causal structure G and (ii) the latent causal variables Z. Achieving these implicitly involves another objective of recovering the unknown transformation g as well. Addressing CRL consists of two central questions:</p><p>• Identifiability, which refers to determining the necessary and sufficient conditions under which G and Z can be recovered. The scope of identifiability (e.g., perfect or partial) critically depends on the extent of information available about the data, the underlying causal structure, and the transformation. The nature of the identifiability results can be algorithm-agnostic and non-constructive without specifying how to recover G and Z. In particular, this is the case when considering CRL under a general transform g without parametric assumptions. Furthermore, the study of identifiability also investigates necessary conditions, e.g., which type of data is required for identifiability, regardless of the algorithmic approach.</p><p>• Achievability, which complements identifiability and pertains to designing algorithms that can recover G and Z while maintaining identifiability guarantees. Achievability hinges on forming reliable estimates for the transformation g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRL from interventions.</head><p>Identifiability is known to be impossible without additional supervision or sufficient statistical diversity among the samples of the observed data X.</p><p>As shown in <ref type="bibr" target="#b14">(Hyvärinen and Pajunen, 1999;</ref><ref type="bibr" target="#b31">Locatello et al., 2019)</ref>, this is the case even for the simpler settings in which the latent variables are statistically independent (i.e., graph G has no edges). On the other hand, using data generated under interventions in addition to the observational data generated by the underlying latent model creates useful statistical diversity. Specifically, an intervention on a set of causal variables alters the causal mechanisms that generate those variables. Note that these causal mechanisms capture the effect of parents on the child variable. Such interventions, even when imposing sparse changes in the statistical models, create variations in the observed data sufficient for learning latent causal representations. This has led to CRL via intervention as an important class of CRL problems, which in its general form has remained an open problem <ref type="bibr" target="#b41">(Schölkopf et al., 2021)</ref>. This paper addresses this open problem by drawing a novel connection to score functions, i.e., the gradients of the logarithm of density functions.</p><p>We note that we use the interventions as a weak form of supervision via having access to only the pair of distributions before and after an intervention. Such supervision can be quite flexible and bodes well for practical applications in genomics <ref type="bibr" target="#b48">(Tejada-Lapuerta et al., 2025)</ref> and robotics <ref type="bibr" target="#b23">(Lee et al., 2021)</ref>. For instance, genomics experiments often involve interventions, which can be modeled as deterministic or stochastic interventions, depending on the experimental mechanism. In robotics, let us consider the causal variables to be the joint angles of a robotic arm. A stochastic intervention corresponds to setting the joint angle to take values within a suitable support. We note that the interventions here can be soft since the feasible support for the joint may still depend on the other joint angles after the intervention. In this setting, the observations are simply images of the entire arm captured by a camera positioned at a specific location.</p><p>Contributions. This paper provides both identifiability and achievability results for CRL from interventions under general latent causal models and general transformations. We establish these results by uncovering hitherto unknown connections between score functions (i.e., the gradients of the logarithm of density functions) and CRL. We leverage these connections to design CRL algorithms that serve as constructive proofs for the identifiability and achievability results. We do not make any parametric assumption on the latent causal model, i.e., the relationships among elements of Z take any arbitrary form. For the transformation g, we consider a diffeomorphism (i.e., bijective such that both g and g -1 are continuously differentiable) onto its image. Our results are categorized into two main groups based on the form of g: (i) linear transformation, and (ii) general (nonparametric) transformation. We consider both stochastic hard and soft interventions, and our contributions are summarized below.</p><p>Linear transformations. We first focus on linear transformations and investigate various extents of results (perfect and partial) given one intervention per node. In this setting, we consider both hard and soft interventions.</p><p>✔ On identifiability from hard interventions, we show that one hard intervention per node suffices to guarantee perfect identifiability (Theorem 3).</p><p>✔ On identifiability from soft interventions, we show that one soft intervention per node suffices to guarantee identifiability up to ancestors -transitive closure of the latent DAG is recovered and latent variables are recovered up to a linear function of their ancestors (Theorem 2).</p><p>✔ While establishing these results, we also establish partial identifiability of a latent variable given an intervention on single node (Theorem 1).</p><p>✔ We further tighten the previous results and show that when the latent causal model is sufficiently nonlinear, perfect DAG recovery becomes possible using soft interventions. Furthermore, we recover a latent representation that is Markov with respect to the latent DAG, preserving the true conditional independence relationships (Theorem 4).</p><p>✔ On achievability, we design an algorithm referred to as Linear Score-based Causal Latent Estimation via Interventions (LSCALE-I), which achieves the identifiability guarantees under both soft and hard interventions. LSCALE-I first forms an encoder estimate using correlation matrices of the observed score differences and then uses the learned encoder to estimate the transitive closure of the latent DAG. Then, in the case of hard interventions, it refines these outputs without requiring any conditional independence test.</p><p>General transformations. In this setting, we do not have any restriction on the transformation from the latent space to the observed space. In this general setting, our contributions are as follows.</p><p>✔ On identifiability, we show that observational data and two distinct hard interventions per node suffice to guarantee perfect identifiability <ref type="bibr">(Theorem 8)</ref>. This result generalizes the recent results in the literature in two ways. First, we do not require the commonly adopted faithfulness assumption on latent causal models. Secondly, we assume the learner does not know which pair of environments intervene on the same node. While proving these results, we also establish identifiability of a single variable given two hard interventions only for the said variable (Theorem 5).</p><p>✔ More importantly, on achievability, we design the first provably correct algorithm that recovers G and Z perfectly. This algorithm is referred to as Generalized Score-based Causal Latent Estimation via Interventions (GSCALE-I). We note that GSCALE-I requires only the score functions of observed variables as its inputs and computes those of the latent variables by leveraging the Jacobian of the decoders.</p><p>✔ We also establish new results that shed light on the role of observational data. Specifically, when two interventional environments per node are given in pairs, observational data is only needed for recovering the latent DAG (Theorem 6). Furthermore, we show that observational data can be dispensed with under a weak faithfulness condition on the latent causal model (Theorem 7). Remarkably, our results are tight in terms of the identifiability objectives, which we will discuss in Section 3.4.</p><p>✔ Finally, we show that extrapolation to unseen combinations of single-node interventions can be achieved without performing CRL, just by using score functions of the observed space (Section 6.4).</p><p>Before describing our novel methodology, we recall the general approach to solving CRL. Recovering the latent causal variables hinges on finding the inverse of g based on the observed data X, which facilitates recovering Z via Z = g -1 (X). In other words, referring to (g -1 , g) as the true encoder-decoder pair, we search for an encoder that takes observed variables back to the true latent space. A valid encoder should necessarily have an associated decoder to ensure a perfect reconstruction of observed variables from estimated latent variables. However, infinitely many valid encoder-decoder pairs satisfy the reconstruction property. The purpose of using interventions is to inject variations into the observed data, which can help us distinguish among these solutions.</p><p>Score-based methodology. We start by showing that an intervention on a latent node induces changes only in the score function's coordinates corresponding to the intervened node and its parents. This is because the intervention changes only the causal mechanism (i.e., conditional distribution) of the intervened node, which is a function of the latent node and its parents. Furthermore, when we consider two hard interventions on the same node, such changes will be limited only to the intervened node (parents intact). This implies that score changes of the latent variables Z are generally sparse across different environments. Furthermore, these score changes contain all the information about the latent causal structure. Motivated by these key properties, we formalize a score-based CRL framework, based on which we design provably correct distinct algorithms, LSCALE-I and GSCALE-I, for linear and general transformations, respectively, presented in Section 5 and Section 6. We briefly describe the key technique in GSCALE-I for general transformation via two interventions. LSCALE-I involves exploiting the linearity of the transformation to achieve identifiability using only one intervention per node.</p><p>Algorithm sketch for general transformations. Consider two interventional environments in which the same node is intervened on. As described in the preceding paragraph, we show that the score functions of the latent variables under these two environments differ only at the coordinate of the intervened node. Subsequently, the key idea of the score-based framework is that tracing these sparse changes in the score functions of the latent variables can guide finding reliable estimates for the inverse of the transformation g, which in turn facilitates estimating Z. In particular, we will seek encoders that result in sparse score variations among the estimated latent variables, thereby matching the properties of the true latent variables. To this end, we consider a pair of interventional environments for each node and find an encoder that minimizes the variations in the score function across all pairs. We show that the encoder obtained via this procedure perfectly recovers G and Z. An important process in this methodology is projecting the score changes in observed data to the latent space so that we do not need to estimate latent scores for each possible encoder. We show that this can be done by multiplying the observed score difference by the Jacobian of the decoder associated with the encoder. Therefore, recovering Z i is facilitated by solving the following problem:</p><p>For i-th int. env. pair : min E Jac. of decoder × score difference of X</p><formula xml:id="formula_1">-e i 2 ,<label>(2)</label></formula><p>in which e i denotes i-th standard basis vector, reflecting the sparsity property. Score differences of observed variables are computed across two environments with the same intervened node. When we have two interventions for each latent variable, we solve this problem for i ∈ [n] simultaneously to recover complete Z, and subsequently, the graph G.</p><p>Finally, the minimization is performed over the set of valid encoder-decoder pairs that ensure perfect reconstruction of X.</p><p>Organization. The rest of the paper is organized as follows. Section 2 provides an overview of the literature with the main focus on CRL via interventions. Section 3 provides the preliminaries for formulating the problem and specifies the notations and definitions used throughout the paper. Section 4 establishes the properties of score functions under interventions and presents the key lemmas and their implications, which will be used in our CRL framework. We present our CRL algorithms and results for linear transformations in Section 5, and general transformations in Section 6. Proofs of the building blocks of the results are presented in the main body of the paper, and the rest of the proofs are provided in the appendices. In Section 7, we empirically assess the performance of the proposed CRL algorithms for recovering the latent causal variables and the latent causal graph on both structured synthetic data, biological data, and image data. Section 8 concludes the paper with a discussion of the results and the future directions. &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 u X e s c 7 y C R n l k R q M t X E N / A v i W B s = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r x q t V j 1 6 W S w F T y U p q D 0 W B P F Y 0 X 5 g G 8 p m u 2 m X b j Z h d y O U 0 J / g x Y M i X s U f 4 k / w 5 r 9 x 0 / a g r Q 8 G H u / N M D P P j z l T 2 n G + r d z a + s b m V n 7 b 3 t k t 7 O 0 X D w 5 b K k o k o U 0 S 8 U h 2 f K w o Z 4 I 2 N d O c d m J J c e h z 2 v b H l 5 n f f q B S s U j c 6 U l M v R A P B Q s Y w d p I t / f 9 a r 9 Y c i r O D G i V u A t S q h c + k / K V / d H o F 7 9 6 g 4 g k I R W a c K x U 1 3 V i 7 a V Y a k Y 4 n d q 9 R N E Y k z E e 0 q 6 h A o d U e e n s 1 C k q G 2 W A g k i a E h r N 1 N 8 T K Q 6 V m o S + 6 Q y x H q l l L x P / 8 7 q J D m p e y k S c a C r I f F G Q c K Q j l P 2 N B k x S o v n E E E w k M 7 c i M s I S E 2 3 S s U 0 I 7 v L L q 6 R V r b j n l b M b k 0 Y N 5 s j D M Z z A K b h w A X W 4 h g Y 0 g c A Q H u E Z X i x u P V m v 1 t u 8 N W c t Z o 7 g D 6 z 3 H 2 U n k B 4 = &lt; / l a t e x i t &gt; Z 2</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 8 / p R 9 s I 0 e Z g b O J N G B T v g o Y N A h Y = " &gt; A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K r 2 P Q i 8 e I 5 o H J E m Y n v c m Q 2 d l l Z l Y I S z 7 B i w d F v P p F 3 v w b J 8 k e N F r Q U F R 1 0 9 0 V J I J r 4 7 p f T m F p e W V 1 r b h e 2 t j c 2 t 4 p 7 + 4 1 d Z w q h g 0 W i 1 i 1 A 6 p R c I k N w 4 3 A d q K Q R o H A V j C 6 n v q t R 1 S a x / L e j B P 0 I z q Q P O S M G i v d P f R 4 r 1 x x q + 4 M 5 C / x c l K B H P V e + b P b j 1 k a o T R M U K 0 7 n p s Y P 6 P K c C Z w U u q m G h P K R n S A H U s l j V D 7 2 e z U C T m y S p + E s b I l D Z m p P y c y G m k 9 j g L b G V E z 1 I v e V P z P 6 6 Q m v P Q z L p P U o G T z R W E q i I n J 9 G / S 5 w q Z E W N L K F P c 3 k r Y k C r K j E 2 n Z E P w F l / + S 5 o n V e + 8 e n Z 7 W q l d 5 X E U 4 Q A O 4 R g 8 u I A a 3 E A d G s B g A E / w A q + O c J 6 d N + d 9 3 l p w 8 p l 9 + A X n 4 x s 2 H I 3 E &lt; / l a t e x i t &gt; Z i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X z y k G x F x V D n 5 L G L i p G K D e 8 c W F F k = " &gt; A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K r 2 P Q i 8 e I 5 o H J E m Y n v c m Q 2 d l l Z l Y I S z 7 B i w d F v P p F 3 v w b J 8 k e N F r Q U F R 1 0 9 0 V J I J r 4 7 p f T m F p e W V 1 r b h e 2 t j c 2 t 4 p 7 + 4 1 d Z w q h g 0 W i 1 i 1 A 6 p R c I k N w 4 3 A d q K Q R o H A V j C 6 n v q t R 1 S a x / L e j B P 0 I z q Q P O S M G i v d P f R k r 1 x x q + 4 M 5 C / x c l K B H P V e + b P b j 1 k a o T R M U K 0 7 n p s Y P 6 P K c C Z w U u q m G h P K R n S A H U s l j V D 7 2 e z U C T m y S p + E s b I l D Z m p P y c y G m k 9 j g L b G V E z 1 I v e V P z P 6 6 Q m v P Q z L p P U o G T z R W E q i I n J 9 G / S 5 w q Z E W N L K F P c 3 k r Y k C r K j E 2 n Z E P w F l / + S 5 o n V e + 8 e n Z 7 W q l d 5 X E U 4 Q A O 4 R g 8 u I A a 3 E A d G s B g A E / w A q + O c J 6 d N + d 9 3 l p w 8 p l 9 + A X n 4 x s 9 s I 3 J &lt; / l a t e x i t &gt;</p><p>Z n &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D T 9 X L / g e Q M F g z O o U f 9 e d S m 5 U 6 6 o = " &gt; A</p><formula xml:id="formula_2">A A C M H i c b V D L S s Q w F E 3 H d 3 1 V X b o J D o K r o R 1 8 L U U X u h z B 0 Y H p U N L 0 z h h M 0 5 K k g 0 O Z T 3 L j p + h G Q R G 3 f o V p p + D z Q r i H c + 5 N T k 6 Y c q a 0 6 z 5 b t a n p m d m 5 + Q V 7 c W l 5 Z d V Z W 7 9 U S S Y p t G n C E 9 k J i Q L O B L Q 1 0 x w 6 q Q Q S h x y u w p u T Q r 8 a g l Q s E R d 6 l E I v J g P B + o w S b a j A O f X L O 3 I J 0 d j 2 Q x g w k Y c x 0 Z L d j u 1 O 4 G H f x 6 Y 3 T b f 9 Y Z R o V a B O E N k + i O h r N H D q b s M t C / 8 F X g X q q K p W 4 D z 4 U U K z G I S m n C j V 9 d x U 9 3 I i N a M c j J d M Q U r o D R l A 1 0 B B Y l C 9 v D Q 7 x t u G i X A / k e Y I j U v 2 + 0 Z O Y q V G c W g m j c F r 9 V s r y P + 0 b q b 7 h 7 2 c i T T T I O j k o X 7 G s U 5 w k R 6 O m A S q + c g A Q i U z X j G 9 J p J Q b T I u Q v B + f / k v u G w 2 v P 3 G 3 v l u / e i 4 i m M e b a I t t I M 8 d I C O 0 B l q o T a i 6 A 4 9 o h f 0 a t 1 b T 9 a b 9 T 4 Z r V n V z g b 6 U d b H J x k z q G E = &lt; / l a t e x i t &gt; 2 6 6 6 4 X 1 X 2 . . . X d 3 7 7 7 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o d c E u N F s h E m O K n v Q v G j 8 R u i l u 9 U = " &gt; A A A C M X i c b V D L S g M x F M 3 U V x 1 f V Z d u g k V w V W a K r 2 X R T Z c V 7 A M 6 Z c h k b m s w k x m S T L E M / S U 3 / o m 4 6 U I R t / 6 E m b a g t l 4 I 9 3 D O v c n J C R L O l H a c i V V Y W V 1 b 3 y h u 2 l v b O 7 t 7 p f 2 D l o p T S a F J Y x 7 L T k A U c C a g q Z n m 0 E k k k C j g 0 A 4 e b n K 9 P Q S p W C z u 9 C i B X k Q G g v U Z J d p Q f q n u T e / I A p 7 C 2 P Y C G D C R B R H R k j 2 O 7 Y 7 v Y s / D p l d N t 7 1 h G G u V o 4 4 f 2 h 6 I 8 G f U L 5 W d i j M t v A z c O S i j e T X 8 0 o s X x j S N Q G j K i V J d 1 0 l 0 L y N S M 8 p z L 6 m C h N A H M o C u g Y J E o H r Z 1 O 0 Y n x g m x P 1 Y m i M 0 n r K / N z I S K T W K A j N p D N 6 r R S 0 n / 9 O 6 q e 5 f 9 T I m k l S D o L O H + i n H O s Z 5 f D h k E q j m I w M I l c x 4 x f S e S E K 1 C T k P w V 3 8 8 j J o V S v u R e X 8 9 q x c u 5 7 H U U R H 6 B i d I h d d o h q q o w Z q I o q e 0 C t 6 Q + / W s z W x P q z P 2 W j B m u 8 c o j 9 l f X 0 D A h 6 o 2 A = = &lt; / l a t e x i t &gt; 2 6 6 6 4 X 1 X 2 . . . X d 3 7 7 7 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J v A U 3 T S H 2 k d G 5 7 T p v e G y 9 y Z G T p U = " &gt; A A A C O X i c b V D L S g M x F M 3 4 r P V V d e k m 2 A p 1 Y Z k R X 8 u i G 5 c V 7 A M 6 p W T S O 2 1 o Z j I k G b E M 8 1 t u / A t 3 g h s X i r j 1 B 0 w f g r Y 9 E D i c e + 4 j x 4 s 4 U 9 q 2 X 6 y F x a X l l d X M W n Z 9 Y 3 N r O 7 e z W 1 M i l h S q V H A h G x 5 R w F k I V c 0 0 h 0 Y k g Q Q e h 7 r X v x 7 W 6 / c g F R P h n R 5 E 0 A p I N 2 Q + o 0 Q b q Z 2 r u B 7 r F h N 3 N C m R 0 E m x G 3 j i I V F U S M D C x 4 V G I U 3 x M f 7 1 e D y G + a b h q K N 2 L m + X 7 B H w L H E m J I 8 m q L R z z 2 5 H 0 D i A U F N O l G o 6 d q R b C Z G a U Q 5 p 1 o 0 V R I T 2 S R e a h o Y k A N V K R r e k + N A o H e w L a V 6 o 8 U j 9 2 5 G Q Q K l B 4 B l n Q H R P T d e G 4 r x a M 9 b + Z S t h Y R R r C O l 4 k R 9 z r A U e x o g 7 T A L V f G A I o Z K Z W z H t E U m o N m F n T Q j O 9 J d n S e 2 k 5 J y X z m 5 P 8 + W r S R w Z t I 8 O U B E 5 6 A K V 0 Q 2 q o C q i 6 B G 9 o n f 0 Y T 1 Z b 9 a n 9 T W 2 L l i T n j 3 0 D 9 b 3 D 5 n / r V 4 = &lt; / l a t e x i t &gt; score of X score of X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K 2 g M g v 9 1 U y G + C 2 K n f j C D f 2 0 c + Y s = " &gt; A A A B 7 3 i c b V D L S g N B E O y N r x i N R j 1 6 G Q y C p 7 A r + D h J w E M 8 R j A P S J Y w O 5 l N h s z M r j O z Q l h y 9 y y C B 0 W 8 + h 3 6 B d 7 8 G y e P g y Y W N B R V 3 X R 3 B T F n 2 r j u t 5 N Z W l 5 Z X c u u 5 z Y 2 8 1 v b h Z 3 d u o 4 S R W i N R D x S z Q B r y p m k N c M M p 8 1 Y U S w C T h v B 4 H L s N + 6 o 0 i y S N 2 Y Y U 1 / g n m Q h I 9 h Y q Z m 2 C e a o M u o U i m 7 J n Q A t E m 9 G i u X s / e N n 5 S N f 7 R S + 2 t 2 I J I J K Q z j W u u W 5 s f F T r A w j n I 5 y 7 U T T G J M B 7 t G W p R I L q v 1 0 c u 8 I H V q l i 8 J I 2 Z I G T d T f E y k W W g 9 F Y D s F N n 0 9 7 4 3 F / 7 x W Y s J z P 2 U y T g y V Z L o o T D g y E R o / j 7 p M U W L 4 0 B J M F L O 3 I t L H C h N j I 8 r Z E L z 5 l x d J / b j k n Z Z O r m 0 a F z B F F v b h A I 7 A g z M o w x V U o Q Y E O D z A M 7 w 4 t 8 6 T 8 + q 8 T V s z z m x m D / 7 A e f 8 B C I C S 9 g = = &lt; / l a t e x i t &gt; G &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d J D q X s c r 5 q w 3 v k 8 L a C T 7 Y H x 3 U X M = " &gt; A A A B + H i c b V D L S g M x F M 3 U V 6 2 P j r p 0 E y y C G 8 u M + F p J p Y g K L i r a B 7 R D y a S Z N j S Z D E l G r E O / x I 0 L R d z 6 K e 7 8 G 9 N 2 F t p 6 4 M L h n H u 5 9 x 4 / Y l R p x / m 2 M n P z C 4 t L 2 e X c y u r a e t 7 e 2 K w p E U t M q l g w I R s + U o T R k F Q 1 1 Y w 0 I k k Q 9 x m p + / 3 y y K 8 / E K m o C O / 1 I C I e R 9 2 Q B h Q j b a S 2 n W 9 x X z w m l 3 f l 8 5 u L / e t h 2 y 4 4 R W c M O E v c l B R A i k r b / m p 1 B I 4 5 C T V m S K m m 6 0 T a S 5 D U F D M y z L V i R S K E + 6 h L m o a G i B P l J e P D h 3 D X K B 0 Y C G k q 1 H C s / p 5 I E F d q w H 3 T y Z H u q W l v J P 7 n N W M d n H o J D a N Y k x B P F g U x g 1 r A U Q q w Q y X B m g 0 M Q V h S c y v E P S Q R 1 i a r n A n B n X 5 5 l t Q O i u 5 x 8 e j 2 s F A 6 S + P I g m 2 w A / a A C 0 5 A C V y B C q g C D G L w D F 7 B m / V k v V j v 1 s e k N W O l M 1 v g D 6 z P H 7 I B k n Y = &lt; / l a t e x i t &gt; GSCALE-I &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N C T h b Q e I 6 4 l I a Q k w M L E Z x X J u 9 0 U = " &gt; A A A B + H i c b V D L S g M x F M 3 U V 6 2 P j r p 0 E y y C G 8 u M + F p J p Q g K X V S 0 D 2 i H k k k z b W g y G Z K M W I d + i R s X i r j 1 U 9 z 5 N 6 b t L L T 1 w I X D O f d y 7 z 1 + x K j S j v N t Z R Y W l 5 Z X s q u 5 t f W N z b y 9 t V 1 X I p a Y 1 L B g Q j Z 9 p A i j I a l p q h l p R p I g 7 j P S 8 A f l s d 9 4 I F J R E d 7 r Y U Q 8 j n o h D S h G 2 k g d O 9 / m v n h M K n f l y 8 r V 4 c 2 o Y x e c o j M B n C d u S g o g R b V j f 7 W 7 A s e c h B o z p F T L d S L t J U h q i h k Z 5 d q x I h H C A 9 Q j L U N D x I n y k s n h I 7 h v l C 4 M h D Q V a j h R f 0 8 k i C s 1 5 L 7 p 5 E j 3 1 a w 3 F v / z W r E O z r 2 E h l G s S Y i n i 4 K Y Q S 3 g O A X Y p Z J g z Y a G I C y p u R X i P p I I a 5 N V z o T g z r 4 8 T + p H R f e 0 e H J 7 X C h d p H F k w S 7 Y A w f A B W e g B K 5 B F d Q A B j F 4 B q / g</formula><p>z X q y X q x 3 6 2 P a m r H S m R 3 w B 9 b n D 7 m 9 k n s = &lt; / l a t e x i t &gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSCALE-I</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H 5 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d 8 7 3 u S q m n 5</p><formula xml:id="formula_3">X 2 y I O m Z 5 8 A M X z r 4 L F O m Q X I I M = " &gt; A A A C E 3 i c b V D L S g M x F M 3 U V 6 2 v U Z d u g k U Q F 2 V G f I E g B R e 6 r G A f 2 C n l T p p p Q z O Z I c m I Z Z h / c O O v u H G h i F s 3 7 v w b 0 8 d C W w 8 E D u e c y 8 0 9 f s y Z 0 o 7 z b e X m 5 h c W l / L L h Z X V t f U N e 3 O r p q J E E l o l E Y 9 k w w d F O R O 0 q p n m t B F L C q H P a d 3 v X w 7 9 + j 2 V i k X i V g 9 i 2 g q h K 1 j A C G g j t e 0 D r w c 6 9 Q j w 9 C r L s H e O v d C P H l J P B R h E Z 6 w M I 3 d Z 2 y 4 6 J W c E P E v c C S m i C S p t + 8 v r R C Q J q d C E g 1 J N 1 4 l 1 K w W p G e E 0 K 3 i J o j G Q P n R p 0 1 A B I V W t d H R T h v e M 0 s F B J M 0 T G o / U 3 x M p h E o N Q t 8 k Q 9 A 9 N e 0 N x f + 8 Z q K D s 1 b K R J x o K s h 4 U Z B w r C M 8 L A h 3 m K R E 8 4 E h Q C Q z f 8 W k B x K I N j U W T A n u 9 M m z p H Z Y c k 9 K x z d H x f L F p I 4 8 2 k G 7 a B + 5 6 B S V 0 T W q o C o i 6 B E 9 o 1 f 0 Z j 1 Z L 9 a 7 9 T G O 5 q z J z D b 6 A + v z B 2 k w n d w = &lt; / l a t e x i t &gt; Ĝ and Ẑ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T X p u 1 q o H E X 1 9 / U Y g M 5 J S 5 Q x w z z U = " &gt; A A A C C n i c b V D L S g M x F L 1 T X 7 W + q i 7 d R I v g q s w U q u K q 4 M Z l B f u A d i i Z N N O G Z p I h y S h l 6 N q N v + L G h S J u / Q J 3 / o 1 p O w t t P R A 4 n H P v z b 0 n i D n T x n W / n d z K 6 t r 6 R n 6 z s L W 9 s 7 t X 3 D 9 o a p k o Q h t E c q n a A d a U M 0 E b h h l O 2 7 G i O A o 4 b Q W j 6 6 n f u q d K M y n u z D i m f o Q H g o W M Y G O l X v G 4 q 0 O U i J G Q D w J x b K g w S F E 7 Q l u W 1 Z T c s j s D W i Z e R k q Q o d 4 r f n X 7 k i S R H U A 4 1 r r j u b H x U 6 w M I 5 x O C t 1 E 0 x i T E R 7 Q j q U C R 1 T 7 6 e y U C T q 1 S h + F U t l n V 5 m p v z t S H G k 9 j g J b G W E z 1 I v e V P z P 6 y Q m v P R T J u L E 3 k j m H 4 U J R 0 a i a S 6 o z x Q l h o 8 t w U Q x u y s i Q 6 w w M T a 9 g g 3 B W z x 5 m T Q r Z e + 8 X L 2 t l G p X W R x 5 O I I T O A M P L q A G N 1 C H B h B 4 h G d 4 h T f n y X l x 3 p 2 P e W n O y X o O 4 Q + c z x / 9 k p s V &lt; / l a t e x i t &gt; unknown latent representation &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c 3 J s n d H s A k H J B a 5 P b B l / R E I 1 p 0 4 = " &gt; A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K r 5 M E v H h M w D w w W c L s p J O M m Z 1 d Z m a F s O Q L v H h Q x K u f 5 M 2 / c Z L s Q R M L G o q q b r q 7 g l h w b V z 3 2 8 m t r K 6 t b + Q 3 C 1 v b O 7 t 7 x f 2 D h o 4 S x b D O I h G p V k A 1 C i 6 x b r g R 2 I o V 0 j A Q 2 A x G t 1 O / + Y R K 8 0 j e m 3 G M f k g H k v c 5 o 8 Z K t Y d u s e S W 3 R n I M v E y U o I M 1 W 7 x q 9 O L W B K i N E x Q r d u e G x s / p c p w J n B S 6 C Q a Y 8 p G d I B t S y U N U f v p 7 N A J O b F K j / Q j Z U s a M l N / T 6 Q 0 1 H o c B r Y z p G a o F 7 2 p + J / X T k z / 2 k + 5 j B O D k s 0 X 9 R N B T E S m X 5 M e V 8 i M G F t C m e L 2 V s K G V F F m b D Y F G 4 K 3 + P I y a Z y V v c v y R e 2 8 V L n J 4 s j D E R z D K X h w B R W 4 g y r U g Q H C M 7 z C m / P o v D j v z s e 8 N e d k M 4 f w B 8 7 n D 7 r N j O Q = &lt; / l a t e x i t &gt; Z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m s l / X 9 q D A / l n L A U 3 k x E o S M F O z 3 I = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 3 x d Z K A F 4 8 J m A c k S 5 i d 9 C Z j Z m e X m V k h L P k C L x 4 U 8 e o n e f N v n C R 7 0 M S C h q K q m + 6 u I B F c G 9 f 9 d l Z W 1 9 Y 3 N g t b x e 2 d 3 b 3 9 0 s F h U 8 e p Y t h g s Y h V O 6 A a B Z f Y M N w I b C c K a R Q I b A W j u 6 n f e k K l e S w f z D h B P 6 I D y U P O q L F S v d 0 r l d 2 K O w N Z J l 5 O y p C j 1 i t 9 d f s x S y O U h g m q d c d z E + N n V B n O B E 6 K 3 V R j Q t m I D r B j q a Q R a j + b H T o h p 1 b p k z B W t q Q h M / X 3 R E Y j r c d R Y D s j a o Z 6 0 Z u K / 3 m d 1 I Q 3 f s Z l k h q U b L 4 o T A U x M Z l + T f p c I T N i b A l l i t t b C R t S R Z m x 2 R R t C N 7 i y 8 u k e V 7 x r i q X 9 Y t y 9 T a P o w D H c A J n 4 M E 1 V O E e a t A A B g j P 8 A p v z q P z 4 r w 7 H / P W F S e f O Y I / c D 5 / A L f F j O I = &lt; / l a t e x i t &gt; X &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t P 3 Q o E P U p 2 r f g O a R J p L k M R W I 1 9 E = " &gt; A A A B + n i c b V D L S g N B E J z 1 G e N r o 0 c v g 0 H w F H Y D P v A U 8 O I x g n l A s o T Z 2 d 5 k y O z M M j M b C T G f 4 s W D I l 7 9 E m / + j Z N k D 5 p Y 0 F B U d d P d F a a c a e N 5 3 8 7 a + s b m 1 n Z h p 7 i 7 t 3 9 w 6 J a O m l p m i k K D S i 5 V O y Q a O B P Q M M x w a K c K S B J y a I X D 2 5 n f G o H S T I o H M 0 4 h S E h f s J h R Y q z U c 0 t d H W M Z a l A j i H B E D O m 5 Z a / i z Y F X i Z + T M s p R 7 7 l f 3 U j S L A F h K C d a d 3 w v N c G E K M M o h 2 m x m 2 l I C R 2 S P n Q s F S Q B H U z m p 0 / x m V U i H E t l S x g 8 V 3 9 P T E i i 9 T g J b W d C z E A v e z P x P 6 + T m f g 6 m D C R Z g Y E X S y K M 4 6 N x L M c c M Q U U M P H l h C q m L 0 V 0 w F R h B q b V t G G 4 C + / v E q a 1 Y p / W b m 4 r 5 Z r N 3 k c B X S C T t E 5 8 t E V q q E 7 V E c N R N E j e k a v</formula><formula xml:id="formula_4">f C 9 C a D o / C o 6 / t r K c M = " &gt; A A A B 9 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h d 2 A D z w F v H i M Y B 6 Q L G F 2 0 p s M m Z 1 d Z 2 Y D Y c l 3 e P G g i F c / x p t / 4 y T Z g y Y W N B R V 3 X R 3 B Y n g 2 r j u t 7 O 2 v r G 5 t V 3 Y K e 7 u 7 R 8 c l o 6 O m z p O F c M G i 0 W s 2 g H V K L j E h u F G Y D t R S K N A Y C s Y 3 c 3 8 1 h i V 5 r F 8 N J M E / Y g O J A 8 5 o 8 Z K f l e H h M s Q F U q G v V L Z r b h z k F X i 5 a Q M O e q 9 0 l e 3 H 7 M 0 Q m m Y o F p 3 P D c x f k a V 4 U z g t N h N N S a U j e g A O 5 Z K G q H 2 s / n R U 3 J u l T 4 J Y 2 V L G j J X f 0 9 k N N J 6 E g W 2 M 6 J m q J e 9 m f i f 1 0 l N e O N n X C a p s V 8 t F o W p I C Y m s w R I n y t k R k w s o U x x e y t h Q 6 o o M z a n o g 3 B W 3 5 5 l T S r F e + q c v l Q L d d u 8 z g K c A p n c A E e X E M N 7 q E O D W D w B M / w C m /</formula><p>O 2 H l x 3 p 2 P R e u a k 8 + c w B 8 4 n z + j l p I A &lt; / l a t e x i t &gt; inference &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X p y D k + Y X O p 0 9 q s 0 B 5 3</p><formula xml:id="formula_5">8 W F C d A n t g = " &gt; A A A B 8 X i c d V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B U 9 k t W l t P B R E 8 V r A f 2 K 4 l m 6 Z t a J J d k q x Q l v 4 L L x 4 U 8 e q / 8 e a / M d t W U N E H A 4 / 3 Z p i Z F 0 S c a e O 6 H 0 5 m a X l l d S 2 7 n t v Y 3 N r e y e / u N X U Y K 0 I b J O S h a g d Y U 8 4 k b R h m O G 1 H i m I R c N o K x h e p 3 7 q n S r N Q 3 p h J R H 2 B h 5 I N G M H G S r d J l 2 C O L q d 3 o p c v u E X X o l x G K f E q r m d J t V o p l a r I m 1 m u W 4 A F 6 r 3 8 e 7 c f k l h Q a Q j H W n c 8 N z J + g p V h h N N p r h t r G m E y x k P a s V R i Q b W f z C 6 e o i O r 9 N E g V L a k Q T P 1 + 0 S C h d Y T E d h O g c 1 I / / Z S 8 S + v E 5 t B x U + Y j G J D J Z k v G s Q c m R C l 7 6 M + U 5 Q Y P r E E E 8 X s r Y i M s M L E 2 J B y N o S v T 9 H / p F k q e u X i 6 f V J o X a + i C M L B 3 A I x + D B G d T g C u r Q A A I S H u A J n h 3 t P D o v z u u 8 N e M s Z v b h B 5 y 3 T 4 L s k N E = &lt; / l a t e x i t &gt; E m &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u 1 D P d 3 l C 7 i s H 1 d 2 F q S j J F T w b n b M = " &gt; A A A B + X i c d V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V w F Z L 0 q a u C C C 4 r 2 A c 0 s U w m k 3 b o 5 M H M p F B C / 8 S N C 0 X c + i f u / B s n b Q U V P X D h c M 6 9 3 H u P l z A q p G l + a I W 1 9 Y 3 N r e J 2 a W d 3 b / 9 A P z z q i j j l m H R w z G L e 9 5 A g j E a k I 6 l k p J 9 w g k K P k Z 4 3 u c r 9 3 p R w Q e P o T s 4 S 4 o Z o F N G A Y i S V N N R 1 R 1 L m k 8 z B i M H r + X 0 4 1 M u m c d G s 2 z U b m o Z p N u x K P S d 2 o 2 p X o K W U H G W w Q n u o v z t + j N O Q R B I z J M T A M h P p Z o h L i h m Z l 5 x U k A T h C R q R g a I R C o l w s 8 X l c 3 i m F B 8 G M V c V S b h Q v 0 9 k K B R i F n q q M 0 R y L H 5 7 u f i X N 0 h l 0 H Q z G i W p J B F e L g p S B m U M 8 x i g T z n B k s 0 U Q Z h T d S v E Y 8 Q R l i q s k g r h 6 1 P 4 P + n a h l U 3 a r f V c u t y F U c R n I B T c A 4 s 0 A A t c A P a o A M w m I I H 8 A S e t U x 7 1 F 6 0 1 2 V r Q V v N H I M f 0 N 4 + A b G o k 7 M = &lt; / l a t e x i t &gt; Ẽm &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W q B e 4 + d 1 G q 4 v U O / 4 p Q s a c l n y + i E = " &gt; A A A B 8 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g a d k N + M B T w I v H C O a B y R J m J 7 P J k N n Z Z a Z X C C F / 4 c W D I l 7 9 G 2 / + j Z N k D 5 p Y 0 F B U d d P d F a Z S G P S 8 b 6 e w t r 6 x u V X c L u 3 s 7 u 0 f l A + P m i b J N O M N l s h E t 0 N q u B S K N 1 C g 5 O 1 U c x q H k r f C 0 e 3 M b z 1 x b U S i H n C c 8 i C m A y U i w S h a 6 b F r I o K a K u P 2 y h X P 9 e Y g q 8 T P S Q V y 1 H v l r 2 4 / Y V n M F T J J j e n 4 X o r B h G o U T P J p q Z s Z n l I 2 o g P e s V T R m J t g M r 9 4 S s 6 s 0 i d R o m 0 p J H P 1 9 8 S E x s a M 4 9 B 2 x h S H Z t m b i f 9 5 n Q y j 6 2 A i V J o h V 2 y x K M o k w Y T M 3 i d 9 o T l D O b a E M i 3 s r Y Q N q a Y M b U g l G 4 K / / P I q a V Z d / 9 K 9 u K 9 W a j d 5 H E U 4 g V M 4 B x + u o A Z 3 U I c G M F D w D K / w 5 h j n x X l 3 P h a t B S e f O Y Y / c D 5 / A B 1 O k I k = &lt; / l a t e x i t &gt; trans.</formula><p>Figure <ref type="figure">1</ref>: An overview of LSCALE-I and GSCALE-I algorithms. Corresponding to each latent variable Z i , there are two interventional mechanisms, denoted by red and blue.</p><p>For each pair of environments, the score functions of observed variables are fed into the GSCALE-I algorithm. Then, GSCALE-I uses this input to compute latent score differences for a given encoder and returns the encoder that minimizes these latent score differences. This encoder is used to compute estimates Ĝ and Ẑ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this paper, we study CRL in different interventional environments where the interventions act on the latent space. We first provide an overview of the literature that investigates CRL from interventional data, with the main results of the most closely related work summarized in Table <ref type="table" target="#tab_1">1</ref> and<ref type="table" target="#tab_2">Table 2</ref>. Then, we discuss the other relevant lines of work.</p><p>Interventional causal representation learning. The majority of the rapidly growing literature on CRL from interventions focuses on parametric settings, i.e., a parametric form is assumed for the latent model, the transformation, or both. Among the related studies, that in <ref type="bibr" target="#b2">(Ahuja et al., 2023)</ref>   <ref type="bibr" target="#b46">(Squires et al., 2023)</ref> considers a linear latent model with a linear mapping to observations and proves identifiability under hard interventions. It also shows the impossibility of perfect identifiability under soft interventions and proves identifiability up to ancestors. The study in <ref type="bibr" target="#b7">(Buchholz et al., 2023)</ref> focuses on linear Gaussian latent models and extends the results in <ref type="bibr" target="#b46">(Squires et al., 2023)</ref> to prove identifiability for general transformations. <ref type="bibr" target="#b59">Zhang et al. (2023)</ref> consider polynomial transformations under nonlinearity assumptions on latent models and prove identifiability up to ancestors under soft interventions. If the latent graph is restricted to polytrees, they further prove perfect identifiability. Identifying the non-intervened variables from the intervened variables by using single-node and multi-node soft interventions is studied in <ref type="bibr" target="#b3">(Ahuja et al., 2024)</ref>. It also considers a new setting in which the latent DAG can change across data points and relies on the support invariance of non-intervened variables to identify them from the rest. The study in <ref type="bibr" target="#b4">(Bing et al., 2024)</ref> considers a nonlinear latent model under linear transformation and uses multi-target do interventions to prove identifiability under certain sparsity assumptions. Linear transformation and linear non-Gaussian latent  <ref type="bibr" target="#b18">(Jin and Syrgkanis, 2024)</ref>, establishing identifiability up to surrounding parents using soft interventions. It also establishes sufficient conditions for multi-target soft interventions to ensure identifiability. The study in <ref type="bibr" target="#b40">(Saengkyongam et al., 2024)</ref> takes a different approach and considers the task of intervention extrapolation. In this formulation, interventions are applied to exogenous action variables (e.g., instrumental variables) that linearly affect the latent variables.</p><p>On the fully nonparametric setting, von <ref type="bibr">Kügelgen et al. (2023)</ref> provide the most closely related identifiability results to ours. Specifically, they show that two coupled hard interventions per node suffice for identifiability under the faithfulness assumption on latent causal models. Our results have two significant differences: 1) We address achievability via a provably correct algorithm whereas von <ref type="bibr">Kügelgen et al. (2023)</ref> focus mainly on identifiability (e.g., no algorithm for recovery of the latent variables), 2) we dispense with the restrictive assumptions on identifiability results, namely, we do not require to know which two environments share the same intervention target (hence, uncoupled interventions), and do not require faithfulness on the latent models. While the importance of this relaxation is not fully apparent for single-node interventions, being able to work without knowing intervention targets is critical for investigating CRL under more realistic cases of multi-node interventions. Among the other studies on the nonparametric setting, <ref type="bibr" target="#b18">Jin and Syrgkanis (2024)</ref> provide analogous results to <ref type="bibr">(von Kügelgen et al., 2023)</ref> by considering two coupled soft interventions and identifying latent variables up to mixing with surrounding variables. <ref type="bibr" target="#b17">Jiang and Aragam (2023)</ref> consider identifying the latent DAG without recovering latent variables, where it is shown that a restricted class of DAGs can be recovered. Finally, some studies employ stronger supervision signals, such as annotations of the ground-truth causal variables <ref type="bibr" target="#b42">(Shen et al., 2022)</ref>, or knowledge of the causal graph to recover the latent variables under hard interventions <ref type="bibr" target="#b26">(Liang et al., 2023)</ref>.</p><p>Multi-view causal representation learning. A commonly used form of weak supervision in CRL involves multi-view data, non-i.i.d. samples (or views) are observed, typically generated by the same or closely related realizations of underlying latent variables. Multiview scenarios can involve various data settings. Earlier studies have mostly considered paired data, in which pre-and post-intervention observations are generated from the same set of latent variables <ref type="bibr" target="#b32">(Locatello et al., 2020;</ref><ref type="bibr" target="#b51">von Kügelgen et al., 2021;</ref><ref type="bibr" target="#b6">Brehmer et al., 2022)</ref>. The more generalized and relaxed formulation typically involves partial observability, where multiple views are concurrently generated by an overlapping subset of latent variables, such as observing different camera angles of the same scene <ref type="bibr" target="#b47">(Sturma et al., 2023)</ref>. <ref type="bibr" target="#b56">Yao et al. (2024)</ref> generalizes the multi-view approach via a unified framework, which also allows partial observability with nonlinear transforms. We also note that <ref type="bibr" target="#b57">Yao et al. (2025)</ref> presents an even more general unifying view to interpret different CRL approaches, including interventional and multi-view, as special ways of aligning the representations to known data symmetries via leveraging the invariance principle.</p><p>Temporal causal representation learning. Temporal CRL is particularly motivated by applications in domains where time-series data is readily available, such as robotics and control systems, which has led to a growing interest in this area. Some studies in this category assume that the latent causal variables are mutually independent, meaning there are no instantaneous causal effects between variables at the same time step <ref type="bibr" target="#b27">(Lippe et al., 2022;</ref><ref type="bibr" target="#b58">Yao et al., 2022;</ref><ref type="bibr" target="#b22">Lachapelle et al., 2024)</ref>. However, more recent studies have relaxed this assumption, allowing instantaneous causal effects when sufficient diversity is present in the observed or interventional data <ref type="bibr">(Lippe et al., 2023a,b;</ref><ref type="bibr">Li et al., 2024b)</ref>.</p><p>Other approaches. In some other approaches to CRL, <ref type="bibr" target="#b60">Zhang et al. (2024)</ref> consider general mixing functions and general SCMs under sparsity constraints; <ref type="bibr" target="#b30">Liu et al. (2024)</ref> leverage nonlinear ICA principles to work with nonlinear mixing functions and polynomial latent SCMs; <ref type="bibr" target="#b36">Morioka and Hyvärinen (2024)</ref> consider disjoint groups of observational variables, with general mixing functions and pairwise latent causal relationships; <ref type="bibr" target="#b54">Welch et al. (2024)</ref> use score functions for linear mixing functions and nonlinear Gaussian SCMs to derive coarser identifiability results; and <ref type="bibr" target="#b24">Li et al. (2024a)</ref> study "domains" and interventions in a combined way for non-Markovian causal systems when the graph is known.</p><p>Identifiable representation learning. As a special case of CRL, where the latent variables are independent, there is extensive literature on identifying latent representations. Some representative approaches include leveraging knowledge of the mechanisms that govern the system's evolution <ref type="bibr" target="#b1">(Ahuja et al., 2022)</ref> and using weak supervision with auxiliary information <ref type="bibr" target="#b43">(Shu et al., 2020)</ref>. Nonlinear independent component analysis (ICA) also uses side information, in the form of structured time series, to exploit temporal information <ref type="bibr" target="#b13">(Hyvärinen and Morioka, 2017;</ref><ref type="bibr" target="#b12">Hälvä and Hyvärinen, 2020)</ref> or knowledge of auxiliary variables that make latent variables conditionally independent <ref type="bibr">(Khemakhem et al., 2020a,b;</ref><ref type="bibr" target="#b15">Hyvärinen et al., 2019)</ref>. <ref type="bibr" target="#b36">Morioka and Hyvärinen (2024)</ref> imposes additional constraints on observational mixing and the causal model to prove identifiability. <ref type="bibr" target="#b21">Kivva et al. (2022)</ref> studies the identifiability of deep generative models without auxiliary information.</p><p>Score functions for causal discovery within observed variables. Score matching has recently gained attention in the causal discovery of observed variables. <ref type="bibr" target="#b39">Rolland et al. (2022)</ref> use score matching to recover nonlinear additive Gaussian noise models. <ref type="bibr" target="#b35">Montagna et al. (2023b)</ref> focus on the same setting, recover the full graph from Jacobian scores, and dispense with the computationally expensive pruning stage of the algorithm in <ref type="bibr" target="#b39">(Rolland et al., 2022)</ref>. <ref type="bibr" target="#b34">Montagna et al. (2023a)</ref> empirically demonstrate the robustness of score matching approaches against the assumption violations in causal discovery. <ref type="bibr" target="#b61">Zhu et al. (2023)</ref> establish bounds on the error rate of score matching-based causal discovery methods. All of these studies are limited to observed causal variables, whereas in our case, we have a causal model in the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries and Definitions</head><p>Notations. For a vector a ∈ R m , the i-the entry is denoted by a i . Random vectors are denoted by bold upper-case letters and their realizations are denoted by bold lower-case letters, e.g., X and x. Matrices are denoted by bold upper-case letters, e.g., A, where A i denotes the i-th row of A and A i,j denotes the entry at row i and column j. For matrices A and B with the same shapes, A ≼ B denotes component-wise inequality. We denote the indicator function by 1, and for a matrix A ∈ R m×n , we use the convention that 1{A} ∈ {0, 1} m×n , where the entries are specified by [1{A}] i,j = 1{A i,j ̸ = 0}. For a positive integer n ∈ N, we define [n] ≜ {1, . . . , n}. The permutation matrix associated with any permutation π of [n] is denoted by P π , i.e., [π 1 π 2 . . . π n ] ⊤ = P π • [1 2 . . . n] ⊤ . The n-dimensional identity matrix is denoted by I n×n , and the Hadamard product is denoted by ⊙. We use im(f ) to denote the image of the function f . We let e i denote the i-th standard basis vector of R n . Given a function f : R s → R r that has first-order partial derivatives on R s , we denote the Jacobian of</p><formula xml:id="formula_6">f at z ∈ R s by J f (z) ∈ R r×s with entries [J f (z)] i,j = ∂f (z) i /∂z j .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Latent Causal Structure</head><p>Consider latent causal random variables Z ≜ [Z 1 , . . . , Z n ] ⊤ . An unknown transformation g : R n → R d generates the observable random variables X ≜ [X 1 , . . . , X d ] ⊤ from the latent variables Z according to: X = g(Z) .</p><p>(3)</p><p>We assume that d ≥ n, and the transformation g is continuously differentiable and a diffeomorphism onto its image (otherwise, identifiability is ill-posed). We denote the image of g by X ≜ im(g) ⊆ R d . The probability density functions (pdfs) of Z and X are denoted by p and p X , respectively. We assume that p is absolutely continuous with respect to the n-dimensional Lebesgue measure. Subsequently, p X , which is defined on the image manifold im(g), is absolutely continuous with respect to the n-dimensional Hausdorff measure rather than d-dimensional Lebesgue measure 1 . The distribution of latent variables Z factorizes with respect to a DAG that consists of n nodes and is denoted by G. Node i ∈ [n] of G represents Z i and p factorizes according to:</p><formula xml:id="formula_7">p(z) = n i=1 p i (z i | z pa(i) ) ,<label>(4)</label></formula><p>where pa(i) denotes the set of parents of node i and p i (z i | z pa(i) ) is the conditional pdf of z i given the variables of its parents. We use ch(i), an(i), and de(i) to denote the children, ancestors, and descendants of node i, respectively. Accordingly, for each node i ∈ [n] we also define</p><formula xml:id="formula_8">pa(i) ≜ pa(i) ∪ {i}, ch(i) ≜ ch(i) ∪ {i}, an(i) ≜ an(i) ∪ {i}, and de(i) ≜ de(i) ∪ {i} . (5)</formula><p>We denote the transitive closure and transitive reduction of G by G tc and G tr , respectively<ref type="foot" target="#foot_0">foot_0</ref> . The parental relationships in these graphs are denoted by pa tc and pa tr , and other graphical relationships are denoted similarly. Based on the modularity property, a change in the causal mechanism of node i does not affect those of the other nodes. We also assume that all conditional pdfs {p i (z i | z pa(i) ) : i ∈ [n]} are continuously differentiable with respect to all variables and p(z) ̸ = 0 for all z ∈ R n . We consider the general structural causal models (SCMs) based on which for each i ∈ [n],</p><formula xml:id="formula_9">Z i = f i (Z pa(i) , N i ) ,<label>(6)</label></formula><p>where {f i : i ∈ [n]} are general functions that capture the dependence of node i on its parents and {N i : i ∈ [n]} account for the exogenous noise terms that we assume to have pdfs with full support. We specialize some of the results to additive noise SCMs, in which (6) becomes</p><formula xml:id="formula_10">Z i = f i (Z pa(i) ) + N i .<label>(7)</label></formula><p>Next, we provide several definitions that we will use frequently throughout the paper to formalize the framework and analyze it.</p><p>Definition 1 (Valid Causal Order) We refer to a permutation (π 1 , . . . , π n ) of [n] as a valid causal order<ref type="foot" target="#foot_1">foot_1</ref> if π i ∈ pa(π j ) indicates that i &lt; j.</p><p>In this paper, without loss of generality, we assume that (1, . . . , n) is a valid causal order. We also define a graphical notion that will be useful for presenting our results and analysis on CRL under a linear transformation.</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P 8</p><formula xml:id="formula_11">t d O D j E + C c d 0 s g U f y Q H L Z Q F 3 q g = " &gt; A A A B 6 n i c b V D L S g N B E O z 1 G V e j U Y 9 e B k P A U 9 g V 1 B w D g n i M a B 6 Y L G F 2 M p s M m Z l d Z m a F s O Q T v H h Q x K v 4 I X 6 C N / / G y e O g i Q U N R V U 3 3 V 1 h w p k 2 n v f t r K y u r W 9 s 5 r b c 7 Z 3 8 7 l 5 h / 6 C h 4 1 Q R W i c x j 1 U r x J p y J m n d M M N p K 1 E U i 5 D T Z j i 8 n P j N B 6 o 0 i + W d G S U 0 E L g v W c Q I N l a 6 v e / 6 3 U L R K 3 t T o G X i z 0 m x m v 9 M S 1 f u R 6 1 b + O r 0 Y p I K K g 3 h W O u 2 7 y U m y L A y j H A 6 d j u p p g k m Q 9 y n b U s l F l Q H 2 f T U M S p Z p Y e i W N m S B k 3 V 3 x M Z F l q P R G g 7 B T Y D v e h N x P + 8 d m q i S p A x m a S G S j J b F K U c m R h N / k Y 9 p i g x f G Q J J o r Z W x E Z Y I W J s e m 4 N g R / 8 e V l 0 j g t + + f l s x u b R g V m y M E R H M M J + H A B V b i G G t S B Q B 8 e 4 R l e H O 4 8 O a / O 2 6 x 1 x Z n P H M I f O O 8 / Y 6 O Q H Q = = &lt; / l a t e x i t &gt; Z 1</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 u X e s c 7 y</p><formula xml:id="formula_12">C R n l k R q M t X E N / A v i W B s = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r x q t V j 1 6 W S w F T y U p q D 0 W B P F Y 0 X 5 g G 8 p m u 2 m X b j Z h d y O U 0 J / g x Y M i X s U f 4 k / w 5 r 9 x 0 / a g r Q 8 G H u / N M D P P j z l T 2 n G + r d z a + s b m V n 7 b 3 t k t 7 O 0 X D w 5 b K k o k o U 0 S 8 U h 2 f K w o Z 4 I 2 N d O c d m J J c e h z 2 v b H l 5 n f f q B S s U j c 6 U l M v R A P B Q s Y w d p I t / f 9 a r 9 Y c i r O D G i V u A t S q h c + k / K V / d H o F 7 9 6 g 4 g k I R W a c K x U 1 3 V i 7 a V Y a k Y 4 n d q 9 R N E Y k z E e 0 q 6 h A o d U e e n s 1 C k q G 2 W A g k i a E h r N 1 N 8 T K Q 6 V m o S + 6 Q y x H q l l L x P / 8 7 q J D m p e y k S c a C r I f F G Q c K Q j l P 2 N B k x S o v n E E E w k M 7 c i M s I S E 2 3 S s U 0 I 7 v L L q 6 R V r b j n l b M b k 0 Y N 5 s j D M Z z A K b h w A X W 4 h g Y 0 g c A Q H u E Z X i x u P V m v 1 t u 8 N W c t Z o 7 g D 6 z 3 H 2 U n k B 4 = &lt; / l a t e x i t &gt; Z 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C z w 8 g B d C b z B M 6 1 4 A a t q U Z + F R K 0 U = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r x q t V j 1 6 W S w F T y V R 1 B 4 L g n i s a D + w D W W z 3 b R L N 5 u w u x F K 6 E / w 4 k E R r + I P 8 S d 4 8 9 + 4 a X v Q 1 g c D j / d m m J n n x 5 w p 7 T j f V m 5 l d W 1 9 I 7 9 p b 2 0 X d n a L e / t N F S W S 0 A a J e C T b P l a U M 0 E b m m l O 2 7 G k O P Q 5 b f m j y 8 x v P V C p W C T u 9 D i m X o g H g g W M Y G 2 k 2 / v e a a 9 Y c i r O F G i Z u H N S q h U + k / K V / V H v F b + 6 / Y g k I R W a c K x U x 3 V i 7 a V Y a k Y 4 n d j d R N E Y k x E e 0 I 6 h A o d U e e n 0 1 A k q G 6 W P g k i a E h p N 1 d 8 T K Q 6 V G o e + 6 Q y x H q p F L x P / 8 z q J D q p e y k S c a C r I b F G Q c K Q j l P 2 N + k x S o v n Y E E w k M 7 c i M s Q S E 2 3 S s U 0 I 7 u L L y 6 R 5 U n H P K 2 c 3 J o 0 q z J C H Q z i C Y 3 D h A m p w D X V o</formula><p>A I E B P M I z v F j c e r J e r b d Z a 8 6 a z x z A H 1 j v P 2 a r k B 8 = &lt; / l a t e x i t &gt; Z 3</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n j r n Y A q e h W z K</p><formula xml:id="formula_13">+ K 4 6 n g D B + 0 m v 1 p k = " &gt; A A A B 6 n i c b V D L S s N A F L 2 p r x q t V l 2 6 G S w F V y U R H 1 0 W B H F Z 0 T 6 w D W U y n b R D J 5 M w M x F K 6 C e 4 c a G I W / F D / A R 3 / o 2 T t g t t P X D h c M 6 9 3 H u P H 3 O m t O N 8 W 7 m V 1 b X 1 j f y m v b V d 2 N k t 7 u 0 3 V Z R I Q h s k 4 p F s + 1 h R z g R t a K Y 5 b c e S 4 t D n t O W P L j O / 9 U C l Y p G 4 0 + O Y e i E e C B Y w g r W R b u 9 7 p 7 1 i y a k 4 U 6 B l 4 s 5 J q V b 4 T M p X 9 k e 9 V / z q 9 i O S h F R o w r F S H d e J t Z d i q R n h d G J 3 E 0 V j T E Z 4 Q D u G C h x S 5 a X T U y e o b J Q + C i J p S m g 0 V X 9 P p D h U a h z 6 p j P E e q g W v U z 8 z + s k O q h 6 K R N x o q k g s 0 V B w p G O U P Y 3 6 j N J i e Z j Q z C R z N y K y B B L T L R J x z Y h u I s v L 5 P m S c U 9 r 5 z d m D S q M E M e D u E I j s G F C 6 j B N d S h A Q Q G 8</formula><p>A j P 8 G J x 6 8 l 6 t d 5 m r T l r P n M A f 2 C 9 / w B o L 5 A g &lt; / l a t e x i t &gt; Z 4</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P 8</p><formula xml:id="formula_14">t d O D j E + C c d 0 s g U f y Q H L Z Q F 3 q g = " &gt; A A A B 6 n i c b V D L S g N B E O z 1 G V e j U Y 9 e B k P A U 9 g V 1 B w D g n i M a B 6 Y L G F 2 M p s M m Z l d Z m a F s O Q T v H h Q x K v 4 I X 6 C N / / G y e O g i Q U N R V U 3 3 V 1 h w p k 2 n v f t r K y u r W 9 s 5 r b c 7 Z 3 8 7 l 5 h / 6 C h 4 1 Q R W i c x j 1 U r x J p y J m n d M M N p K 1 E U i 5 D T Z j i 8 n P j N B 6 o 0 i + W d G S U 0 E L g v W c Q I N l a 6 v e / 6 3 U L R K 3 t T o G X i z 0 m x m v 9 M S 1 f u R 6 1 b + O r 0 Y p I K K g 3 h W O u 2 7 y U m y L A y j H A 6 d j u p p g k m Q 9 y n b U s l F l Q H 2 f T U M S p Z p Y e i W N m S B k 3 V 3 x M Z F l q P R G g 7 B T Y D v e h N x P + 8 d m q i S p A x m a S G S j J b F K U c m R h N / k Y 9 p i g x f G Q J J o r Z W x E Z Y I W J s e m 4 N g R / 8 e V l 0 j g t + + f l s x u b R g V m y M E R H M M J + H A B V b i G G t S B Q B 8 e 4 R l e H O 4 8 O a / O 2 6 x 1 x Z n P H M I f O O 8 / Y 6 O Q H Q = = &lt; / l a t e x i t &gt; Z 1</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 u X e s c 7 y The intuition behind surrounded nodes is that, the effect of i on its children ch(i) can be dominated by the effect of its surrounding node j.<ref type="foot" target="#foot_2">foot_2</ref> Specifically, any effect of i on a node k ∈ ch(i) can also be interpreted as the effect of node j since k ∈ ch(j) as well. This effect causes ambiguities in the recovery of Z i in the case of soft interventions.</p><formula xml:id="formula_15">C R n l k R q M t X E N / A v i W B s = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r x q t V j 1 6 W S w F T y U p q D 0 W B P F Y 0 X 5 g G 8 p m u 2 m X b j Z h d y O U 0 J / g x Y M i X s U f 4 k / w 5 r 9 x 0 / a g r Q 8 G H u / N M D P P j z l T 2 n G + r d z a + s b m V n 7 b 3 t k t 7 O 0 X D w 5 b K k o k o U 0 S 8 U h 2 f K w o Z 4 I 2 N d O c d m J J c e h z 2 v b H l 5 n f f q B S s U j c 6 U l M v R A P B Q s Y w d p I t / f 9 a r 9 Y c i r O D G i V u A t S q h c + k / K V / d H o F 7 9 6 g 4 g k I R W a c K x U 1 3 V i 7 a V Y a k Y 4 n d q 9 R N E Y k z E e 0 q 6 h A o d U e e n s 1 C k q G 2 W A g k i a E h r N 1 N 8 T K Q 6 V m o S + 6 Q y x H q l l L x P / 8 7 q J D m p e y k S c a C r I f F G Q c K Q j l P 2 N B k x S o v n E E E w k M 7 c i M s I S E 2 3 S s U 0 I 7 v L L q 6 R V r b j n l b M b k 0 Y N 5 s j D M Z z A K b h w A X W 4 h g Y 0 g c A Q H u E Z X i x u P V m v 1 t u 8 N W c t Z o 7 g D 6 z 3 H 2 U n k B 4 = &lt; / l a t e x i t &gt; Z 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C z w 8 g B d C b z B M 6 1 4 A a t q U Z + F R K 0 U = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r x q t V j 1 6 W S w F T y V R 1 B 4 L g n i s a D + w D W W z 3 b R L N 5 u w u x F K 6 E / w 4 k E R r + I P 8 S d 4 8 9 + 4 a X v Q 1 g c D j / d m m J n n x 5 w p 7 T j f V m 5 l d W 1 9 I 7 9 p b 2 0 X d n a L e / t N F S W S 0 A a J e C T b P l a U M 0 E b m m l O 2 7 G k O P Q 5 b f m j y 8 x v P V C p W C T u 9 D i m X o g H g g W M Y G 2 k 2 / v e a a 9 Y c i r O F G i Z u H N S q h U + k / K V / V H v F b + 6 / Y g k I R W a c K x U x 3 V i 7 a V Y a k Y 4 n d j d R N E Y k x E e 0 I 6 h A o d U e e n 0 1 A k q G 6 W P g k i a E h p N 1 d 8 T K Q 6 V G o e + 6 Q y x H q p F L x P / 8 z q J D q p e y k S c a C r I b F G Q c K Q j l P 2 N + k x S o v n Y E E w k M 7 c i M s Q S E 2 3 S s U 0 I 7 u L L y 6 R 5 U n H P K 2 c 3 J o 0 q z J C H Q z i C Y 3 D h A m p w D X V o A I E B P M I z v F j c e r J e</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Score Functions</head><p>The score function associated with a pdf is defined as the gradient of its logarithm. The score function associated with p is denoted by</p><formula xml:id="formula_16">s(z) ≜ ∇ z log p(z) .<label>(9)</label></formula><p>Noting the connection X = g(Z), the density of X under E 0 , denoted by p X , is supported on an n-dimensional manifold X embedded in R d . Hence, specifying the score function of X requires notions from differential geometry. For this purpose, we denote the tangent space of the manifold X at point x ∈ X by T</p><formula xml:id="formula_17">x X . Tangent vectors v ∈ T x X are equivalence classes of continuously differentiable curves γ : (-1, 1) → X ⊆ R d with γ(0) = x and γ ′ (0) = v. Furthermore, given a function f : X → R, denote its directional derivative at point x ∈ X along a tangent vector v ∈ T x X by D v f (x)</formula><p>, which is defined as</p><formula xml:id="formula_18">D v f (x) ≜ d dt (f • γ)(t) t=0 ,<label>(10)</label></formula><p>for any curve γ in equivalence class v. The differential of f at point x ∈ X , denoted by df x , is the linear operator mapping tangent vector v ∈ T x X to D v f x <ref type="bibr">(Simon, 2014, p. 57)</ref>, i.e.,</p><formula xml:id="formula_19">df x : T x X ∋ v → D v f (x) ∈ R . (<label>11</label></formula><formula xml:id="formula_20">)</formula><p>Let B ∈ R d×n be a matrix for which the columns of B form an orthonormal basis for T x X . Denote the directional derivative of f along the i-th column of B by</p><formula xml:id="formula_21">D i f for all i ∈ [n].</formula><p>Then, the differential operator can be expressed by the vector</p><formula xml:id="formula_22">Df x ≜ B • D 1 f x . . . D n f x ⊤ ∈ R d ,<label>(12)</label></formula><p>such that</p><formula xml:id="formula_23">df x (v) = v ⊤ • Df x , ∀x ∈ X , ∀v ∈ T x X . (<label>13</label></formula><formula xml:id="formula_24">)</formula><p>Note that the differential operator df is a generalization of the gradient. Hence, we can generalize the definition of the score function using the differential operator by setting f to the logarithm of pdf. Therefore, the score function of X under E 0 is specified as follows:</p><formula xml:id="formula_25">s X (x) ≜ D log p X (x) , ∀x ∈ X . (<label>14</label></formula><formula xml:id="formula_26">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Intervention Mechanisms</head><p>We consider two types of interventions. A soft intervention on node i (also referred to as imperfect intervention in literature), changes the conditional distribution p i (z i | z pa(i) ) to a distinct conditional distribution, which we denote by q i (z i | z pa(i) ). A soft intervention does not necessarily remove the functional dependence of an intervened node on its parents but rather alters it to a different mechanism. A stochastic hard intervention on node i (also referred to as perfect intervention) is stricter than a soft intervention and removes the edges incident on i. A hard intervention on node changes p i (z i | z pa(i) ) to q i (z i ) that emphasizes the lack of dependence of z i on z pa(i) . Finally, we note that in some settings, we assume two hard interventions per node, in which case the two hard interventional mechanisms for node i are denoted by two distinct pdfs q i (z i ) and qi (z i ).</p><p>Interventional environments. We consider atomic interventional environments in which each environment one node is intervened in, as is customary in the closely related CRL literature <ref type="bibr" target="#b46">(Squires et al., 2023;</ref><ref type="bibr" target="#b2">Ahuja et al., 2023;</ref><ref type="bibr" target="#b7">Buchholz et al., 2023)</ref>. In some settings (linear transformation), we will have one interventional environment per node and denote the interventional environments by E ≜ {E 1 , . . . , E n }, where we call E the atomic environment set. We denote the node intervened in environment E m by I m ∈ [n]. For other settings (general transformation), we will have two interventional environments per node and denote the second atomic environment set by Ẽ = { Ẽ1 , . . . , Ẽn }. Similarly, we denote the intervened node in Ẽm by Ĩm for each m ∈ [n]. We assume that node-environment pairs are unspecified, i.e., the ordered intervention sets I ≜ (I 1 , . . . , I n ) and Ĩ ≜ ( Ĩ1 , . . . , Ĩn ) are two unknown permutations of <ref type="bibr">[n]</ref>. We also adopt the convention that E 0 is the observational environment and I 0 ≜ ∅. Next, we define the notion of coupling between the environment sets E and Ẽ.</p><p>Definition 3 (Coupled/Uncoupled Environments) The two environment sets E and Ẽ are said to be coupled if for the unknown permutations I and Ĩ we know that I = Ĩ, i.e., the same node is intervened in environments E i and Ẽi . The two environment sets are said to be uncoupled if Ĩ is an unknown permutation of I.</p><p>Next, we define p m as the pdf of Z in environment E m . Hence, under soft and hard intervention for each m ∈ [n], p m can be factorized as follows.</p><formula xml:id="formula_27">soft int. in E m : p m (z) = q ℓ (z ℓ | z pa(ℓ) ) i̸ =ℓ p i (z i | z pa(i) ) , where ℓ = I m , (<label>15</label></formula><formula xml:id="formula_28">) hard int. in E m : p m (z) = q ℓ (z ℓ ) i̸ =ℓ p i (z i | z pa(i) ) , where ℓ = I m .<label>(16)</label></formula><p>Similarly, we define pm as the pdf of Z in Ẽm , which can be factorized similarly to ( <ref type="formula" target="#formula_28">16</ref>) with qℓ replaced with q ℓ . Hence, the score functions associated with p m and pm are specified as follows.</p><formula xml:id="formula_29">s m (z) ≜ ∇ z log p m (z) , and sm (z) ≜ ∇ z log pm (z) . (<label>17</label></formula><formula xml:id="formula_30">)</formula><p>We denote the score functions of the observed variables X under E m and Ẽm by s m X and sm X , respectively. Note that the score functions change across different environments, which is induced by the changes in the distribution of Z. Specifically, following ( <ref type="formula" target="#formula_7">4</ref>) and ( <ref type="formula" target="#formula_27">15</ref>), the latent scores s(z) and s m (z) are decomposed as</p><formula xml:id="formula_31">s(z) = ∇ z log p ℓ (z ℓ | z pa(ℓ) ) + i̸ =ℓ ∇ z log p i (z i | z pa(i) ) ,<label>(18)</label></formula><p>and</p><formula xml:id="formula_32">s m (z) = ∇ z log q ℓ (z ℓ | z pa(ℓ) ) + i̸ =ℓ ∇ z log p i (z i | z pa(i) ) .<label>(19)</label></formula><p>where ℓ = I m . Hence, s(z) and s m (z) differ in only the causal mechanism of the intervened node in environment E m . In Section 4, we investigate these discrepancies between s and s m (or sm ) and characterize the relationship between the scores in the observational and interventional environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Identifiability and Achievability Objectives</head><p>The objective of CRL is to use observations X generated by the observational and interventional environments and estimate the true latent variables Z and causal relations among them captured by G. The first objective is identifiability, which pertains to determining algorithm-agnostic sufficient conditions under which Z and G can be recovered uniquely up to a permutation and element-wise transform, which is the strongest form of recovery in CRL from interventions as shown in <ref type="bibr">(von Kügelgen et al., 2023)</ref>. The second objective is achievability, which refers to designing algorithms that are amenable to practical implementation and generate provably correct estimates for Z and G, foreseen by the identifiability guarantees.</p><p>In this subsection, we provide the definitions needed for formalizing these objectives.</p><p>We denote a generic estimator of Z given X by Ẑ(X) : R d → R n . We also consider a generic estimate of G denoted by Ĝ. To assess the fidelity of the estimates Ẑ(X) and Ĝ with respect to the ground truth Z and G, we provide the following identifiability measures, which will be achieved when we have a complete set of atomic interventions.</p><p>Definition 4 (Latent Graph Identifiability) For identifiability of the latent graph, we define:</p><p>1. Perfect DAG recovery: DAG recovery is said to be perfect if Ĝ is isomorphic to G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Transitive closure recovery: DAG recovery is said to maintain transitive closure if Ĝ and G have the same ancestral relationships, i.e., Ĝtr is isomorphic to G tr .</p><p>Definition 5 (Latent Variable Identifiability) For identifiability of all latent variables, we define:</p><p>1. Componentwise latent recovery: The estimator Ẑ(X) satisfies componentwise latent recovery if Ẑ(X) is a componentwise diffeomorphism of a permutation of Z, i.e., there exists a permutation π of [n] and a set diffeomorphisms</p><formula xml:id="formula_33">{ϕ i : R → R : i ∈ [n]} such that we have Ẑ(X) = (π • ϕ)(Z) , ∀z ∈ R n , (<label>20</label></formula><formula xml:id="formula_34">)</formula><p>where ϕ(Z) ≜ (ϕ 1 (Z 1 ), . . . , ϕ n (Z n )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Scaling consistency:</head><p>The estimator Ẑ(X) satisfies scaling consistency if there exists a permutation π of [n] and a constant diagonal matrix</p><formula xml:id="formula_35">C scale ∈ R n×n such that Ẑ(X) = P π • C scale • Z , ∀z ∈ R n .<label>(21)</label></formula><p>3. Consistency up to mixing with parents: The estimator Ẑ(X) satisfies consistency up to mixing with parents if there exists a permutation π of [n] and a constant matrix</p><formula xml:id="formula_36">C pa ∈ R n×n such that Ẑ(X) = P π • C pa • Z , ∀z ∈ R n ,<label>(22)</label></formula><p>where C pa has nonzero diagonal entries and for all j / ∈ pa</p><formula xml:id="formula_37">(i), [C pa ] i,j = 0. Equivalently, Ẑπ i is a linear function of {Z j : j ∈ pa(i)} for all i ∈ [n].</formula><p>4. Consistency up to mixing with surrounding parents: The estimator Ẑ(X) satisfies consistency up to mixing with surrounding parents if there exists a permutation π of [n] and a constant matrix</p><formula xml:id="formula_38">C sur ∈ R n×n such that Ẑ(X) = P π • C sur • Z , ∀z ∈ R n , (<label>23</label></formula><formula xml:id="formula_39">)</formula><p>where C sur has nonzero diagonal entries and for all j / ∈ sur</p><formula xml:id="formula_40">(i), [C sur ] i,j = 0. Equivalently, Ẑπ i is a linear function of {Z j : j ∈ sur(i)} for all i ∈ [n].</formula><p>We note that scaling consistency is a special case of component-wise latent recovery in which the diffeomorphism is restricted to scaling. Next, we provide node-level partial identifiability definitions which measure the recovery level of a single latent variable. These will be useful to assess the identifiability guarantees of the algorithms under an incomplete set of interventions.</p><p>Definition 6 (Node-level Partial Identifiability) For identifiability of a single latent variable, we define:</p><p>1. Partial latent recovery: The estimator Ẑ(X) satisfies partial latent recovery for node</p><formula xml:id="formula_41">i if [ Ẑ(X)] i = ϕ k (Z k ) ,<label>(24)</label></formula><p>for some k ∈ [n] where ϕ k : R → R is a diffeomorphism.</p><p>2. Partial consistency up to mixing with parents: The estimator Ẑ(X) satisfies partial consistency up to mixing with parents for node i if</p><formula xml:id="formula_42">[ Ẑ(X)] i = j∈pa(k) c k • Z k ,<label>(25)</label></formula><p>for some k ∈ [n] and constants {c j : j ∈ pa(k)}.  <ref type="bibr">(Squires et al., 2023, Appendix J)</ref>. Finally, <ref type="bibr">Jin and Syrgkanis (2024, Theorem 3)</ref> show that, under some non-degeneracy assumptions, consistency up to mixing with surrounding parents is the optimal result for soft interventions. Therefore, given a complete set of atomic interventions, the identifiability definitions presented in this section represent the ultimate objectives when using interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Algorithm-related Definitions</head><p>For formalizing the achievability results and designing the associated algorithms, generating the estimates Ẑ(X) and Ĝ is facilitated by estimating the inverse of g based on the observed data X. Specifically, an estimate of g -1 , where g -1 denotes the inverse of g, facilitates recovering Z via Z = g -1 (X) . Throughout the rest of this paper, we refer to g -1 as the true encoder. To formalize the procedures of estimating g -1 , we define H as the set of possible valid encoders, i.e., candidates for g -1 . A function h ∈ H can be such a candidate if it is invertible, that is, there exists an associated decoder</p><formula xml:id="formula_43">h -1 such that (h -1 • h)(X) = X.</formula><p>Hence, the set of valid encoders is specified by</p><formula xml:id="formula_44">H ≜ {h : X → R n : ∃h -1 : R n → R d such that (h -1 • h)(x) = x , ∀x ∈ X } .<label>(26)</label></formula><p>Next, corresponding to any pair of observation X and valid encoder h ∈ H, we define Ẑ(X; h) as an auxiliary estimate of Z generated by applying the valid encoder h on X, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ẑ(X</head><formula xml:id="formula_45">; h) ≜ h(X) = (h • g)(Z) , ∀ h ∈ H .<label>(27)</label></formula><p>The estimate Ẑ(X; h) inherits its randomness from X, and its statistical model is governed by that of Z under the chosen h. To emphasize the dependence on h, we denote the score functions associated with the pdfs of Ẑ(X; h) under environments E 0 , E m , and Ẽm , respectively, by s Ẑ(•; h) , s m Ẑ (•; h), and sm Ẑ (•; h) .</p><p>(28)</p><p>We will be addressing both general and linear transformations g. In the linear transformation setting, the true linear transformation g is denoted by the matrix G ∈ R d×n . Accordingly, we denote a valid linear encoder by H ∈ R n×d . For a given valid encoder H, the associated valid decoder is given by its Moore-Penrose inverse, i.e.,</p><formula xml:id="formula_46">H † ≜ H ⊤ • (H • H ⊤ ) -1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Properties of Score Functions under Interventions</head><p>Score functions and their variations across different interventional environments play pivotal roles in our approach to identifying latent representations. In this section, we present the key properties of the score functions that will be leveraged in Section 5 and Section 6 to construct identifiability results, along with the corresponding algorithms.</p><p>We first investigate score variations across pairs of environments, such as the observational environment and an interventional one (under both soft and hard atomic interventions) or two interventional environments, either coupled or uncoupled. The key insight is that an intervention causes changes in only certain coordinates of the score function, as indicated by the sparse changes in the decompositions of the score functions in ( <ref type="formula" target="#formula_31">18</ref>) and ( <ref type="formula" target="#formula_32">19</ref>). These sparse changes further reflect the graph structure in the score differences. For instance, for a single-node intervention</p><formula xml:id="formula_47">I m = {i} in m-th environment, j-th coordinate of the score difference s -s m becomes s(z) -s m (z) j = ∂ ∂z j log p i (z i | z pa(i)) ) - ∂ ∂z j log q i (z i | z pa(i) ) ,<label>(29)</label></formula><p>which is zero if j / ∈ pa(i). The following lemma formalizes this property for all relevant cases.</p><p>Lemma 1 (Score Changes under Interventions) Consider the observational environment E 0 and an interventional environment E m with unknown intervention target I m . Then, for any causal model and intervention type, E s(Z)s m (Z) i ̸ = 0 implies that i ∈ pa(I m ).</p><p>For the reverse direction, we have the following results specified for each relevant case.</p><p>(i) Hard interventions: If the intervention in E m (or Ẽm ) is hard, then score functions s and s m (or sm ) differ in their i-th coordinate if and only if node i or one of its children is intervened in</p><formula xml:id="formula_48">E m (or in Ẽm ). E s(Z) -s m (Z) i ̸ = 0 ⇐⇒ i ∈ pa(I m ) , (<label>30</label></formula><formula xml:id="formula_49">)</formula><formula xml:id="formula_50">and E s(Z) -sm (Z) i ̸ = 0 ⇐⇒ i ∈ pa( Ĩm ) .<label>(31)</label></formula><p>(ii) Soft interventions: If the intervention in E m is soft and the latent causal model is an additive noise model, then score functions s and s m differ in their i-th coordinate if and only if node i or one of its children is intervened in</p><formula xml:id="formula_51">E m . E s(Z) -s m (Z) i ̸ = 0 ⇐⇒ i ∈ pa(I m ) .<label>(32)</label></formula><p>(iii) Coupled environments I m = Ĩm : In the coupled environment setting, s m and sm differ in their i-th coordinate if and only if i is intervened.</p><formula xml:id="formula_52">E s m (Z) -sm (Z) i ̸ = 0 ⇐⇒ i = I m . (<label>33</label></formula><formula xml:id="formula_53">)</formula><p>(iv) Uncoupled environments I m ̸ = Ĩm : Consider two interventional environments E m and Ẽm with different intervention targets I m ̸ = Ĩm , and consider additive noise models specified in (7). Given that p(Z) is twice differentiable, the score functions s m and sm differ in their i-th coordinate if and only if node i or one of its children is intervened.</p><formula xml:id="formula_54">E s m (Z) -sm (Z) i ̸ = 0 ⇐⇒ i ∈ pa(I m , Ĩm ) . (<label>34</label></formula><formula xml:id="formula_55">)</formula><p>Proof: See Appendix A.1.</p><p>Lemma 1 provides the necessary and sufficient conditions for the invariance of the coordinates of the score functions of latent variables. The core idea of the score-based framework is that tracing these sparse changes in the score functions of the latent variables guides finding reliable estimates for the inverse of the transformation g, which in turn facilitates estimating Z. Intuitively, we will look for the encoders h ∈ H such that the variations between the score estimates s Ẑ(ẑ; h), s m Ẑ (ẑ; h), and sm Ẑ (ẑ; h) will be similar to the true score variations given by Lemma 1. However, the scores of the latent variables are not directly accessible. To circumvent this, we need to understand the connection between score functions of X and Z.</p><p>In the following lemma, we leverage the change of variables formula for injective mappings and establish this relationship for any injective mapping f from latent to observed space.</p><formula xml:id="formula_56">Lemma 2 (Score Difference Transformation) Consider random vectors Y 1 , Y 2 ∈ R r and W 1 , W 2 ∈ R s that are related through Y 1 = f (W 1 ) , and Y 2 = f (W 2 ) ,<label>(35)</label></formula><p>such that r ≥ s, probability measures of W 1 , W 2 are absolutely continuous with respect to the s-dimensional Lebesgue measure, and f : R s → R r is an injective and continuously differentiable function. The difference of the score functions of Y 1 and Y 2 , and that of W 1 and W 2 are related as</p><formula xml:id="formula_57">s W 1 (w) -s W 2 (w) = J f (w) ⊤ • s Y 1 (y) -s Y 2 (y) , where y = f (w) ,<label>(36)</label></formula><p>where J f (w) denotes the Jacobian of f at point w. Furthermore, for the reverse direction, we have</p><formula xml:id="formula_58">s Y 1 (y) -s Y 2 (y) = J f (w) † ⊤ • s W 1 (w) -s W 2 (w) , where y = f (w) . (<label>37</label></formula><formula xml:id="formula_59">) Proof: See Appendix A.2.</formula><p>For CRL under linear transformations in Section 5, we use the following corollary of Lemma 2.</p><formula xml:id="formula_60">Corollary 1 In Lemma 2, if f is a linear transform, that is Y = F•W for a full-rank matrix F ∈ R r×s , then the score functions of Y and W are related through s W (w) = F ⊤ • s Y (y) and s Y (y) = F † ⊤ • s W (w), where y = F • w.</formula><p>For CRL under general transformations in Section 6, we customize Lemma 2 as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consider a candidate encoder</head><formula xml:id="formula_61">h ∈ H. Recall that Ẑ(X; h) = h(X) = (h • g)(Z). Then, by letting f = (h • g), Lemma 2 gives between E 0 and E m : s Ẑ(ẑ; h) -s m Ẑ (ẑ; h) = J -⊤ f (z) • s(z) -s m (z) ,<label>(38)</label></formula><p>between E 0 and Ẽm :</p><formula xml:id="formula_62">s Ẑ(ẑ; h) -sm Ẑ (ẑ; h) = J -⊤ f (z) • s(z) -sm (z) ,<label>(39)</label></formula><p>between E m and Ẽm :</p><formula xml:id="formula_63">s m Ẑ (ẑ; h) -sm Ẑ (ẑ; h) = J -⊤ f (z) • s m (z) -sm (z) .<label>(40)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CRL under Linear Transformations</head><p>In this section, we consider CRL under linear transformation, in which the general transformation model in (3) becomes:</p><formula xml:id="formula_64">X = G • Z ,<label>(41)</label></formula><p>where G ∈ R d×n is an unknown full-rank matrix mapping the latent variables to the observed ones. We will present steps for leveraging the properties of the score functions presented in Section 4 to design an algorithm that identifies the true encoder and recovers the true causal representations. The algorithm is referred to as Linear Score-based Causal Latent Estimation via Interventions (LSCALE-I). The theoretical guarantees associated with the algorithm steps will also be presented, serving as constructive proof of identifiability. LSCALE-I consists of three stages outlined in Algorithm 1. In the first stage, we will learn an encoder Ĥ, row by row, where each row is an estimate of a row of the true encoder G † . This encoder estimate will recover each Z i up to mixing with Z pa(i) . In the second stage, we will use the estimated latent variables Ĥ to obtain the transitive closure of the latent graph G. Finally, as an optional third stage for hard interventions, we will leverage the independence statements implied by the hard interventions to refine our encoder estimate. We will demonstrate that this refinement yields scaling consistency and perfect DAG recovery. We will also discuss the relevance and distinctions of our results vis-à-vis the results in the existing literature.</p><p>Statistical diversity. Similarly to the existing identifiability results, it is necessary to have some regularity conditions 5 on the probability distributions of observational and interventional environments. We adopt the following assumption for the case of linear transformations.</p><p>Assumption 1 For every possible pair (i, k) where i ∈ [n], k ∈ pa(i), the following term cannot be a constant function in z,</p><formula xml:id="formula_65">∂ ∂z k log p i (z i | z pa(i) ) q i (z i | z pa(i) ) ∂ ∂z i log p i (z i | z pa(i) ) q i (z i | z pa(i) ) -1 . (42) Essentially, ∂ ∂z k log p i (z i |z pa(i) ) q i (z i |z pa(i) )</formula><p>captures the effect of intervening on node i on the score associated with node k. Therefore, Assumption 1 ensures that an intervention sufficiently differentiates the target variable and its parents in the score function. We note that this 5. Some examples include generic interventions in <ref type="bibr" target="#b46">(Squires et al., 2023)</ref>, no pure shift interventions condition in <ref type="bibr" target="#b7">(Buchholz et al., 2023)</ref>, and the genericity condition in <ref type="bibr">(von Kügelgen et al., 2023)</ref>.</p><p>is a very mild assumption and holds for a wide range of commonly used models (including additive noise models under hard interventions) and is discussed in more detail in Section 5.5. Before we explain the details of LSCALE-I and establish identifiability results in the following subsections, we first present the rationale and derive the partial identifiability result, which will serve as the building block. In the rest of the paper, for brevity, we denote the score difference functions for each m ∈ [n] by</p><formula xml:id="formula_66">d m Z (z) ≜ s m (z) -s(z) and d m X (x) ≜ s m X (x) -s X (x) . (<label>43</label></formula><formula xml:id="formula_67">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Rationale for LSCALE-I and Partial Identifiability</head><p>To construct an algorithm that achieves identifiability under linear transformations, we first analyze the score differences for the linear transformation. Note that for linear transformation X = G • Z, Jacobian J G (z) is independent of z and equal to matrix G. Also, we have z = G † • x for all x ∈ col(G). Then, using Corollary 1 we have</p><formula xml:id="formula_68">d m X (x) = [G † ] ⊤ • d m Z (z) . (<label>44</label></formula><formula xml:id="formula_69">)</formula><p>Let us denote the correlation matrices of d m X (x) and d m Z (z), respectively, by</p><formula xml:id="formula_70">R m X ≜ E d m X (x) • [d m X (x)] ⊤ and R m Z ≜ E d m Z (z) • [d m Z (z)] ⊤ . (<label>45</label></formula><formula xml:id="formula_71">) Note that (44) implies that R m X = [G † ] ⊤ • R m Z • G † .</formula><p>The next lemma specifies that the structure of the column space of correlation matrix R m X is heavily constrained by the true encoder G † , the graph G, and the intervention target I m in m-th environment.</p><formula xml:id="formula_72">Lemma 3 For any interventional environment E m , col(R m X ) ⊆ span [G † i ] ⊤ : i ∈ pa(I m ) .</formula><p>Proof: Using the sparse score changes property in (29), we know that d m Z (z) i = 0 for all i / ∈ pa(I m ). Then, using (44), for any x = G • z we have</p><formula xml:id="formula_73">d m X (x) = i∈pa(I m ) [G † i ] ⊤ • d m Z (z) i ∈ span [G † i ] ⊤ : i ∈ pa(I m ) .<label>(46)</label></formula><p>Since this holds for any x ∈ col(G), we have</p><formula xml:id="formula_74">span d m X (x) : x ∈ col(G) ⊆ span [G † i ] ⊤ : i ∈ pa(I m ) . (<label>47</label></formula><formula xml:id="formula_75">) By definition of R m X , we have col(R m X ) = span{d m X (x) : x ∈ col(G)} which concludes the proof.</formula><p>□ Lemma 3 crucially implies that we can achieve partial identifiability of a single latent variable given that we have an environment in which the said variable is intervened.</p><p>Theorem 1 (Linear -Node-level partial identifiability) A single-node soft interventional environment E m with intervention target ℓ = I m is sufficient to recover Z ℓ up to mixing with its parents.</p><p>Proof: Let us pick y uniformly at random from unit sphere</p><formula xml:id="formula_76">S d-1 ⊂ R d and let h = R m X • y. Note that d m Z (z) i = 0 for all i / ∈ pa(ℓ) implies that R m Z i = 0 for all i / ∈ pa(ℓ). Then, using R m X = [G † ] ⊤ • R m Z • G † , we have h = R m X • y = [G † ] ⊤ • R m Z • G † • y (48) = i∈pa(ℓ) R m Z i • G † • y • [G † i ] ⊤ (49) Denote c i = R m Z i • G † • y for each i ∈ pa(ℓ). Then, using G † i • G = e i , i-th standard basis vector, we have [h ⊤ • G] i = c i for i ∈ pa(ℓ).</formula><p>As such, we can use h to obtain</p><formula xml:id="formula_77">h ⊤ • X = h ⊤ • G • Z = c ℓ • Z ℓ + i∈pa(ℓ) c i • Z i . (50) Finally, note that R m Z ℓ is not a zero vector since [d m Z ] ℓ ̸ = 0.</formula><p>Therefore, since rows of G † are linearly independent, we know that R m Z i • G † is a nonzero vector. Thus, y ∈ S d-1 ensures that c ℓ is nonzero with probability 1. Then, h ⊤ • X is an estimate of Z ℓ that satisfies consistency up to mixing with parents. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Identifiability via Soft Interventions</head><p>After obtaining the partial identifiability result from a single interventional environment, our goal is to construct an algorithm that uses the complete set of interventional environments</p><formula xml:id="formula_78">E = {E 1 , . . . , E n } with targets ∪ m∈[n] I m = [n]</formula><p>to form estimates of the true encoder G † and the latent graph G.</p><p>Encoder estimation. In Stage L1 of Algorithm 1, we form an encoder estimate Ĥ via using the column spaces of {R m X : m ∈ [n]}. Specifically, for each m ∈ [n] we select a vector randomly from col(R m X ) and assign its transpose to m-th row of Ĥ.</p><p>Lemma 4 (Linear -Encoder via Soft Interventions) Under soft interventions, output Ĥ of Algorithm 1 achieves identifiability up to mixing with parents. Specifically, Ẑ(X; Ĥ) = P I • C pa • Z such that diagonal entries of C pa are nonzero and C pa i,j = 0 for all j / ∈ pa(i).</p><p>Proof: Following Theorem 1 for all m ∈ [n] immediately implies the desired result. Specifically, according to (50),</p><formula xml:id="formula_79">[ Ĥm ] ⊤ ∈ col(R m X ) satisfies [ Ẑ(X; Ĥ)] m = Ĥm • X = c ℓ • Z ℓ + i∈pa(ℓ) c i • Z i , where ℓ = I m ,<label>(51)</label></formula><p>for some constants {c i : i ∈ pa(ℓ)} where c ℓ ̸ = 0 with probability 1. Then, combining all n identities, we have</p><formula xml:id="formula_80">Ẑ(X; Ĥ) = P I • C pa • Z ,<label>(52)</label></formula><p>where P I is the permutation matrix of the intervention order (I 1 , . . . , I n ), and C pa has nonzero diagonal entries and satisfies C pa i,j = 0 for all j / ∈ pa(i). for m ∈ (π 1 , . . . , π n ) do ▷ refine rows of Ĥ sequentially 13:</p><p>Ẑ ← Ẑm (X; Ĥ)</p><formula xml:id="formula_81">14: u ← Cov( Ẑm , Ẑ pa(m) ) • Cov( Ẑ pa(m) ) -1 15: Ĥm ← Ĥm -u • Ĥ pa(m)</formula><p>16:</p><p>Refine Ĝ according to</p><formula xml:id="formula_82">pa(m) ← i ̸ = m : E d m Ẑ ( Ẑ; Ĥ) i ̸ = 0 , ∀m ∈ [n]</formula><p>17: Return Ĝ, Ĥ, and Ẑ Latent graph estimation. Next, we use the encoder estimate Ĥ to recover the transitive closure of the latent graph G. Motivated by Lemma 1(ii), we form Ĝ with parent sets</p><formula xml:id="formula_83">pa(m) ≜ i ̸ = m : E s Ẑ( Ẑ; Ĥ) -s m Ẑ ( Ẑ; Ĥ) i ̸ = 0 . (<label>53</label></formula><formula xml:id="formula_84">)</formula><p>Using the result that Ẑ(X; Ĥ) recovers Z up to mixing with parents, we show that the transitive closures of the estimate Ĝ and the true graph G are the same under a graph isomorphism.</p><p>Lemma 5 (Linear -Graph via Soft Interventions) Under Assumption 1 and soft interventions, output Ĝ of Algorithm 1 recovers the transitive closure of G.</p><formula xml:id="formula_85">Proof: Let Ẑ denote Ẑ(X; Ĥ) = P I • C pa • Z.</formula><p>Recall that C pa i,j = 0 for all j / ∈ pa(i), and since nodes (1, . . . , n) are topologically ordered, C pa is a lower triangular matrix. By using standard linear algebra for the expansion of an inverse of a lower triangular matrix, we can show that C -1 pa i,j = 0 for all j / ∈ an(i) (see Lemma 13 in Appendix B.1 for the detailed steps). Then, using Corollary 1 for the scores of Z, we have</p><formula xml:id="formula_86">d m Ẑ ( Ẑ) = P I • [C -1 pa ] ⊤ • d m Z (Z) .<label>(54)</label></formula><p>Next, taking the transpose of C -1 pa , we have C -⊤ pa i,j = 0 for all j / ∈ de(i). Combining this observation with the fact that [d m Z ] j = 0 for all j / ∈ pa(I m ), we obtain</p><formula xml:id="formula_87">d m Ẑ ( Ẑ) k = C -⊤ pa I k • d m Z (Z) = j ∈ de(I k ) ∩ pa(I m ) C -⊤ pa I k ,j • [d m Z ] j (55) Subsequently, E |d m Ẑ ( Ẑ)| k ̸ = 0 implies that de(I i )∩pa(I m ) ̸ = ∅, and I k / ∈ an(I m ).</formula><p>Therefore, the estimated parent set in (53) satisfies pa(m) ⊆ an(I m ), and the transitive closure of G is a supergraph of Ĝ under relabeling of the nodes with permutation I. Next, we show that Ĝ contains all the edges in the transitive reduction of G. Let I k → I m be an edge in the transitive reduction of G, i.e., there is no other directed path between nodes I k and I m in G. This implies that de(I k ) ∩ pa(I m ) = {I k , I m }, and (54) becomes</p><formula xml:id="formula_88">d m Ẑ ( Ẑ) k = C -⊤ pa I k ,I k • [d m Z ] I k + C -⊤ pa I k ,I m • [d m Z ] I m .<label>(56)</label></formula><p>Assumption 1 ensures that d m Ẑ ( Ẑ) k is not constantly zero, which means that Ĝ contains k → m edge, preserving I k → I m in G. Therefore, Ĝ preserves all the edges in the transitive reduction of G, and taking its transitive closure gives G tc , under relabeling of the nodes with permutation I. □</p><p>Theorem 2 (Linear -Soft Interventions for General SCMs) Under Assumption 1 for linear transformations, using observational data and interventional data from one soft intervention per node suffice to identify (i) the transitive closure of the latent DAG G and (ii) the latent variables Z with consistency up to mixing with parents. Specifically, Algorithm 1 achieves these identifiability guarantees.</p><p>Proof: Combining Lemma 4 and Lemma 5 gives the statements of the theorem. □ We note that the existing literature on CRL with linear transformations and one soft intervention requires the latent causal model to be either linear Gaussian <ref type="bibr" target="#b46">(Squires et al., 2023;</ref><ref type="bibr" target="#b7">Buchholz et al., 2023)</ref> or satisfy nonlinearity conditions <ref type="bibr" target="#b59">(Zhang et al., 2023)</ref> <ref type="foot" target="#foot_3">foot_3</ref> . In contrast, Theorem 2 achieves identifiability guarantees for soft interventions without imposing any restrictions (distributional or structural) on the latent causal model. Furthermore, recovering Z up to mixing with parents improves upon the existing guarantees of recovering up to mixing with ancestors <ref type="bibr" target="#b46">(Squires et al., 2023;</ref><ref type="bibr" target="#b7">Buchholz et al., 2023)</ref>. Finally, we note that G cannot be identified beyond its transitive closure when using soft interventions without making additional assumptions <ref type="bibr">(Squires et al., 2023, Appendix J)</ref>. Therefore, the graph identifiability result in Theorem 2 is tight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Identifiability via Hard Interventions</head><p>Next, we investigate hard interventions (i.e., perfect interventions) for general latent causal models. Hard interventions are special cases of soft interventions, in which the intervened node becomes functionally independent of its parents. Consequently, the identifiability guarantees for hard interventions are usually stronger, as we show in this section. Our analysis builds on leveraging the following property, which is exclusive to hard interventions.</p><p>Proposition 1 For the environment E m in which node ℓ = I m is hard intervened, we have</p><formula xml:id="formula_89">Z m ℓ ⊥ ⊥ Z m j , ∀j ∈ nd(ℓ) ,<label>(57)</label></formula><p>where nd(ℓ) is the set of non-descendants of ℓ in G.</p><p>The Markov property readily implies this property, that is, each variable in a DAG is independent of its non-descendants, given its parents. When node ℓ is hard-intervened, it has no parents, and the statement follows directly. Motivated by this property, we want to ensure that the estimated latent variables conform to Proposition 1. To this end, we aim to update rows of the encoder estimate Ĥ to remove the effects of Z pa(i) from the estimate of Z i . We achieve this objective as follows.</p><p>Note that Proposition 1 provides us with random variable pairs that are supposed to be independent, and the covariance of two independent random variables is necessarily zero. In Stage L3 of Algorithm 1, for each row Ĥm , we consider Ẑ(X; Ĥ) in the corresponding environment in which Ẑm is intervened, and compute an unmixing vector u via a linear minimum mean square error (MMSE) estimator,</p><formula xml:id="formula_90">u = Cov( Ẑm , Ẑ pa(m) ) • Cov( Ẑ pa(m) ) -1 . (<label>58</label></formula><formula xml:id="formula_91">)</formula><p>Then, we update the corresponding row of the encoder as Ĥm ← Ĥmu • Ĥ pa(m) . Finally, for the perfect DAG recovery, we compute the latent score differences under the final encoder estimate and reconstruct the graph Ĝ with parent sets</p><formula xml:id="formula_92">pa(m) ≜ i ̸ = m : E d m Ẑ ( Ẑ; Ĥ) i ̸ = 0 . (<label>59</label></formula><formula xml:id="formula_93">)</formula><p>Theorem 3 (Linear -Hard Interventions) Under Assumption 1 for linear transformations, using observational data and interventional data from one hard intervention per node suffices to identify (i) the latent DAG G perfectly and (ii) the latent variables Z with scaling consistency. Specifically, Algorithm 1 achieves these identifiability guarantees.</p><p>Proof: See Appendix B.2. Similar to the restrictions in the existing results for soft interventions, the identifiability results for hard interventions in the existing literature restrict the latent causal model, e.g., to linear causal models <ref type="bibr" target="#b46">(Squires et al., 2023;</ref><ref type="bibr" target="#b7">Buchholz et al., 2023)</ref>. In contrast, Theorem 3 does not impose any restriction on the latent causal model and shows that one stochastic hard intervention per node is sufficient for the identifiability of general latent causal models.</p><p>Finally, we comment on the differences between the identifiability results for soft and hard interventions under missing intervention targets and discuss the identifiability of latent subgraphs under special cases.</p><p>Remark 1 Let J ⊂ [n] denote a non-complete set of intervention targets, i.e., J ̸ = [n]. We note the immediate implications of our results as follows.</p><p>1. Partial identifiability of latent variables. Theorem 1 immediately implies that for all i ∈ J we can recover Z i up to mixing with parents. However, the unmixing procedure for hard interventions resolves the mixing with parents sequentially. Specifically, to identify Z i up to scaling, we use the fact that the parents of Z i are already identified up to scaling. Therefore, even if we have a hard intervention on Z i , we cannot identify Z i up to scaling consistency when its parents are not identified up to scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Ancestrally closed set of interventions.</head><p>Let G J denote the induced subgraph of G over the set of nodes J . If J is not ancestrally closed, i.e., pa(J ) ̸ ⊆ J , then the marginal distribution of Z J is not a causally sufficient model. Conversely, if J is an ancestrally closed set, then the marginal of Z J comes from a causally sufficient model with graph G J . Subsequently, using Stage L2 of Algorithm 1 for score differences of ẐJ , we obtain the transitive closure of G J . Finally, performing the unmixing procedure over the ancestrally closed set of hard interventions, Stage L3 of Algorithm 1 ensures the perfect recovery of G J .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Identifiability up to Surrounding Parents</head><p>For linear transformations, we finally investigate the conditions under which soft interventions are guaranteed to achieve identifiability results stronger than transitive closure and mixing up to parents. In particular, we specify one condition on the rank of the score function</p><formula xml:id="formula_94">differences {d m Z : m ∈ [n]}, which is formalized next. Assumption 2 (Full-rank Score Difference) For all interventional environments E m ∈ E we have rank(R m Z ) = |pa(I m )| . (<label>60</label></formula><formula xml:id="formula_95">)</formula><p>For insight into this assumption, it can be readily verified that for linear Gaussian latent models rank(R m Z ) ≤ 2 and on the other hand, for sufficiently nonlinear causal models, rank(R m Z ) is pa(I m ). This assumption is stronger than Assumption 1 since it implies that the effects of an intervention on all parents of the target variable are different. We will provide more discussions on this assumption in Section 5.5. Under this condition, Algorithm 2 identifies the latent graph and the latent variables with the following key intuitions.</p><formula xml:id="formula_96">Latent graph estimation. Recall that R m X = [G † ] ⊤ • R m Z • G † . Since G † has full row-rank n, Assumption 2 implies that rank(R m X ) = |pa(I m )| for all m ∈ [n],</formula><p>which further implies that Lemma 3 becomes an equality as well, i.e.,</p><formula xml:id="formula_97">col(R m X ) = span [G † i ] ⊤ : i ∈ pa(I m ) .<label>(61)</label></formula><p>Then, for any t, k ∈ [n], we have</p><formula xml:id="formula_98">col(R t X ) ∩ col(R k X ) = span [G † i ] ⊤ : i ∈ pa(I t ) ∩ pa(I k ) , (<label>62</label></formula><formula xml:id="formula_99">) dim col(R t X ) ∩ col(R k X ) = |pa(I t ) ∩ pa(I k )| (63) = |pa(I t ) ∩ pa(I k )| + 1 {I t ∈ pa(I k )} ∨ {I k ∈ pa(I t )} . (<label>64</label></formula><formula xml:id="formula_100">)</formula><p>Algorithm 2 LSCALE-I for sufficiently nonlinear latent causal models 1: Input: R m X for all m ∈ [n], topological order π from Algorithm 1 2: Latent graph estimation 3: Initialize Ĝ with empty graph 4: for k ∈ (π 1 , . . . , π n ) do 5:</p><formula xml:id="formula_101">for t ∈ (π 1 , . . . , π k-1 ) do 6: if dim col(R t X ) ∩ col(R k X ) &gt; | pa(t) ∩ pa(k)| then 7: Update pa(k) ← pa(k) ∪ {t} 8: Encoder estimation 9: Initialize Ĥ ← 0 n×d 10: for m ∈ (1, . . . , n) do 11: Select any h ∈ k ∈ ĉh(m) ∪ m col(R k X )</formula><p>12:</p><p>Ĥm ← h ⊤ 13: Return Ĝ, Ĥ, and Ẑ</p><p>In Algorithm 2, we first obtain a topological order of the true DAG using the graph estimate from Algorithm 1 and initialize the new estimate Ĝ with the empty graph. Consider a pair (t, k) such that</p><formula xml:id="formula_102">I t ∈ pa(I k ). If pa(k) does not contain node t in Ĝ, then by (64) we have dim col(R t X ) ∩ col(R k X ) &gt; | pa(t) ∩ pa(k)|.</formula><p>We use this observation to sequentially identify the parents of each node, starting from the root node(s) and gradually advancing to the leaf node(s). In the end, resulting Ĝ equals G under permutation I.</p><p>Encoder estimation. In Algorithm 2, we refine the encoder estimation step of Algorithm 1 as follows. Extending (62) and using the graph recovery result, we have</p><formula xml:id="formula_103">k ∈ ĉh(m) ∪ m col(R k X ) = span [G † i ] ⊤ : i ∈ k : I k ∈ch(I m ) pa(I k ) .<label>(65)</label></formula><p>Now, consider I t / ∈ sur(I m ). By the definition of surrounding parents, there exists I j ∈ ch(I m ) such that I j / ∈ ch(I t ). Then, I j is not in the intersection of the parent sets in the right-handside of (65), which means [G † j ] ⊤ is not in the column space of the left-hand-side of (65). Therefore, by choosing Ĥm from this column space, we obtain for all m ∈</p><formula xml:id="formula_104">[n], [ Ẑ(X; Ĥ)] m = Ĥm • X = c ℓ • Z ℓ + i∈sur(ℓ) c i • Z i , where ℓ = I m ,<label>(66)</label></formula><formula xml:id="formula_105">Ẑ(X; Ĥ) = P I • C pa • Z , (<label>67</label></formula><formula xml:id="formula_106">)</formula><p>where P I is the permutation matrix of the intervention order (I 1 , . . . , I n ), C sur has nonzero diagonal entries and satisfies C sur i,j = 0 for all j / ∈ sur(i). □</p><p>Theorem 4 (Linear -Soft Interventions for Nonlinear SCMs) Under Assumption 2 for linear transformations, using observational data and interventional data from one soft intervention per node suffice to identify (i) the latent DAG G perfectly, and (ii) the latent variables Z with consistency up to mixing with surrounding parents, and recovered latent variables satisfy Markov property with respect to G. Specifically, Algorithm 2 achieves these identifiability guarantees.</p><p>Proof: Proof of the consistency of latent variables up to mixing with surrounding parents is given above in complete detail. For the proof of perfect graph recovery and Ẑ satisfies Markov property with respect to Ĝ, see Appendix B.3. □ Theorem 4 has two important implications. First, the latent DAG can be identified using only soft interventions under mild nonlinearity assumptions on the latent causal model. To our knowledge, this is the first result in the literature for fully recovering latent DAG with soft interventions without restricting the graphical structure, e.g., <ref type="bibr" target="#b59">Zhang et al. (2023)</ref> require linear faithfulness assumption to achieve similar results, which is only shown to hold for nonlinear latent models with polytree structure. Secondly, the estimated latent variables reveal the true conditional independence relationships since they satisfy the Markov property with respect to the estimated latent graph, which is isomorphic to the latent graph. Recalling that the motivation of CRL is to learn useful representations that preserve causal relationships, our result shows that this can be achieved without perfect identifiability for a large class of models. Furthermore, Jin and Syrgkanis (2024, Theorem 3) establish that under some nondegeneracy assumptions, mixing consistency up to surrounding parents is the best possible result when using single-node soft interventions. Hence, the results in Theorem 4 are tight for the considered setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Discussion on Assumptions 1 and 2</head><p>In this subsection, we elaborate on Assumption 1 and Assumption 2, which are relevant to Theorem 2 and Theorem 4, respectively. Assumption 1 essentially states that score changes in the coordinates of the intervened node and a parent of the intervened node are linearly independent. This property holds for (but is not limited to) the widely adopted additive noise models specified in (7) when we apply hard interventions.</p><p>Lemma 6 (Proof in Appendix D.1) Assumption 1 is satisfied for additive noise models under hard interventions.</p><p>When soft interventions are applied, the transitive closure of G cannot be identified without making assumptions about the effect of the interventions. Specifically, for linear latent causal models, <ref type="bibr" target="#b7">Buchholz et al. (2023)</ref> prove impossibility results for pure shift interventions and <ref type="bibr" target="#b46">Squires et al. (2023)</ref> show that a genericity condition is necessary for identifying the transitive closure of the latent DAG. Therefore, Assumption 1 can be interpreted as the counterpart of the commonly adopted assumptions in the literature on soft interventions adapted to the setting of general latent causal models. Finally, the next example demonstrates the working of Assumption 1 on a linear Gaussian latent model.</p><p>Example 1 Consider a linear Gaussian latent model with Z ∼ N (0, Σ). Score function of Z is given by s</p><formula xml:id="formula_107">(z) = -Σ -1 • z. Let Z i = w • Z pa(i) + N i where N i ∼ N (0, σ 2 i ) for the node i. Consider an intervention on node i on environment E m such that Z m i = w • Z m pa(i) + Ni , where Z m ∼ N (0, Σ), and Ni ∼ N (0, σ2 i ), which yields d m Z (z) = -(Σ -1 -Σ-1 ) • z. Then, for a node k ∈ pa(i), we obtain [d m Z (z)] i = 1 σ 2 i - 1 σ2 i z i - w σ 2 i - w σ2 i z pa(i) ,<label>(68)</label></formula><formula xml:id="formula_108">[d m Z (z)] k = - w k σ 2 i - wk σ2 i z i + w k w σ 2 i -wk w σ2 i z pa(i) ,<label>(69)</label></formula><p>where w k and wk correspond to weights of the parent Z k in observational and interventional models, respectively. Note that, for Assumption 1 to be violated, there must exists a constant</p><formula xml:id="formula_109">κ ∈ R such that κ • d m Z (z) i = d m Z (z) k , ∀z ∈ R n . (<label>70</label></formula><formula xml:id="formula_110">)</formula><p>However, using (68) and (69), this is possible if and only if w k = wk . Therefore, if the weight of the node k ∈ pa(i) changes, Assumption 1 is satisfied for the node pair (i, k).</p><p>Given the known result that perfect identifiability is impossible for linear Gaussian models given soft interventions <ref type="bibr" target="#b46">(Squires et al., 2023)</ref>, the purpose of Assumption 2 is to get more insight into the extent of identifiability guarantees under soft interventions. Intuitively, the mentioned impossibility results for linear Gaussian models are due to the rank deficiency of score differences for linear models -specifically, we know that rank(R m Z ) ≤ 2 for linear Gaussian models. In contrast, for sufficiently nonlinear causal models, rank(R m Z ) can be as high as pa(I m ). Assumption 2 ensures that this upper bound is satisfied with equality for all nodes. This condition holds for the class of sufficiently nonlinear models, such as quadratic causal models. In particular, we show that this condition holds for the two-layer neural networks (NNs) as a function class that can effectively approximate any continuous function. This result is formalized in the next lemma.</p><p>Lemma 7 Consider the additive model in (7) where f i is a two-layer NN with sigmoid activation function, and weight matrices W i and Wi for observational and interventional mechanisms, respectively. If max{rank(W i ) , rank( Wi )} = |pa(i)| for all i ∈ [n], then Assumption 2 holds.</p><p>We discuss the nonlinearity and the proof of Lemma 7 in Appendix D.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CRL under General Transformations</head><p>In this setting, we consider general transformations without any parametric assumption for the transformation g. In the previous section, we exploited the transformation's linearity and recovered the true encoder's parameters sequentially using one intervention per node. For general transformations (parametric or nonparametric), however, we cannot use the same parametric approach and rely on the properties of linear transforms. To rectify these and design the general CRL algorithm, we use more information in the form of two interventions per node. Specifically, we will present the steps of leveraging the score functions under two interventions and design an algorithm that identifies the true encoder and recovers the true causal representations. The algorithm is referred to as General Score-based Causal Latent Estimation via Interventions (GSCALE-I), which will be summarized in Algorithm 3. We also present additional results and discuss the role of various inputs and the distinction of our results compared to the existing literature.</p><p>Inputs. The inputs of GSCALE-I are the observed data from the observational environment, the data from two interventional environments per node, whether environments are coupled/uncoupled, and a set of valid encoders H. Two sets of interventional environments are denoted by E = {E 1 , . . . , E n } and Ẽ = { Ẽ1 , . . . , Ẽn }, as defined in Section 3.3. For these inputs, we compute the score functions s X , {s 1 X , . . . , s n X } and {s 1 X , . . . , sn X }. Note that we will use Lemma 2 again to ensure access to the latent score differences by using these observed score functions.</p><p>Statistical diversity. For being more informative than a single intervention mechanism, we assume that the two intervention mechanisms per node are sufficiently distinct. This is formalized by defining interventional discrepancy <ref type="bibr" target="#b26">(Liang et al., 2023)</ref> among the causal mechanisms of a latent variable.</p><p>Definition 7 (Interventional Discrepancy) Two intervention mechanisms with pdfs p, q : R → R are said to satisfy interventional discrepancy if</p><formula xml:id="formula_111">∂ ∂u p(u) q(u) ̸ = 0 , ∀u ∈ R \ T , (<label>71</label></formula><formula xml:id="formula_112">)</formula><p>where T is a null set (i.e., has a zero Lebesgue measure).</p><p>This condition ensures that the two distributions are sufficiently distinct, formally expressed as the partial derivative of their ratio with respect to the intervened variable is nonzero almost everywhere. For instance, two univariate, distinct Gaussians trivially satisfy this discrepancy condition. As shown by <ref type="bibr" target="#b26">Liang et al. (2023)</ref>, even when the latent graph G is known, for identifiability via one intervention per node, it is necessary to have interventional discrepancy between observational distribution p i and interventional distribution q i , for all z pa(i) ∈ R |pa(i)| .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Rationale of GSCALE-I and Partial Identifiability</head><p>Similarly to LSCALE-I, analyzing GSCALE-I involves leveraging score differences. However, in contrast to linear transformations, estimated and true latent score differences are not always related by a constant matrix for general transformations. To circumvent this issue, we rely on Lemma 1(iii), i.e., the score difference of coupled interventional environments is one-sparse. As such, the key idea is to find an encoder h ∈ H that adheres to this sparsity structure, and we will show that such an encoder ensures component-wise identifiability of the latent variables. To formalize these, corresponding to each valid encoder h ∈ H, we define the score change matrix D t (h) with entries:</p><formula xml:id="formula_113">[D t (h)] i,m ≜ E s m Ẑ ( Ẑ; h) -sm Ẑ ( Ẑ; h) i ,<label>(72)</label></formula><p>where expectations are under the measures of latent score functions induced by the probability measure of observational data. We also denote the true score change matrix under true encoder g -1 by D t ≜ D t (g -1 ) with entries</p><formula xml:id="formula_114">[D t ] i,m ≜ E s m (Z) -sm (Z) i , ∀i, m ∈ [n] .<label>(73)</label></formula><p>For clarity in the exposition of the ideas, we consider coupled environments here, i.e., I m = Ĩm for all m ∈ [n]. Then, using (33) in Lemma 1 we have</p><formula xml:id="formula_115">[D t ] i,m ̸ = 0 ⇐⇒ i = I m . (<label>74</label></formula><formula xml:id="formula_116">)</formula><p>This implies that 1{D t } is a permutation matrix, 1{D t } = P ⊤ I and</p><formula xml:id="formula_117">1{[D t ] :,m } = e ℓ , where ℓ = I m ,<label>(75)</label></formula><p>and e ℓ denotes the ℓ-th standard basis vector. Subsequently, the key idea for identifying the encoder is that the changes between score estimates s m Ẑ (ẑ; h) and sm Ẑ (ẑ; h) should match the sparsity structure for those under the true encoder g -1 . In the next result, we leverage the sparsity structure in (75) for two interventions on the same node to identify an intervened variable, formalized as follows.</p><p>Theorem 5 (General -Node-level Identifiability) Two hard interventions (with interventional discrepancy) on the same target node I m = Ĩm = ℓ suffice to recover Z ℓ up to a diffeomorphism. In particular, the following encoder</p><formula xml:id="formula_118">ĥm = arg min h∈H [D t (h)] :,m -e m 2 (76) satisfies Ẑ(X; ĥm ) m = ϕ ℓ (Z ℓ ) for a diffeomorphism ϕ ℓ : R → R.</formula><p>We will show shortly that given a complete set of coupled interventions, Theorem 5 will readily imply the identifiability of all variables (in a similar spirit to going from partial to complete identifiability in Section 5). Furthermore, the results for uncoupled interventions will also leverage this key result. Hence, we provide the detailed proof of Theorem 5 as follows.</p><p>Proof: We start by providing a direct result of Corollary 1, which will be used in multiple instances in this section. Consider an encoder h ∈ H and an invertible matrix A ∈ GL(n, R). Then, using Corollary 1, score functions of Ẑ(X; h) and Ẑ(X; A • h) are related by</p><formula xml:id="formula_119">s Ẑ( Ẑ; A • h) = A -⊤ • s Ẑ( Ẑ; h) .<label>(77)</label></formula><p>Recall that score change matrices are defined via the absolute value of the score differences. Therefore, (77) implies that for a positively scaled permutation matrix A we have</p><formula xml:id="formula_120">D t (A • h) = A -⊤ • D t (h) , ∀h ∈ H . (<label>78</label></formula><formula xml:id="formula_121">)</formula><p>Denote the interventional environments by E m and Ẽm with I m = Ĩm = ℓ. We show that the minimum value of ( <ref type="formula">76</ref>) is zero as follows. Let h * = D ⊤ t •g -1 . Recall that 1{D t } = P ⊤ I , and by definition, D t has only nonzero entries. Thus, using (78), we have D t (h * ) = D -1 t • D t = I n×n , which makes the objective in (76) zero. Next, we show that any solution of (76) recovers Ẑℓ up to a diffeomorphism.</p><p>Consider encoder ĥm ∈ H that satisfies [D t ( ĥm )] :,m = e m . Let Ẑ denote Ẑ(X; ĥm ) and f ≜ ĥm • g, so we have Ẑ = f (Z) and Z = f -1 ( Ẑ). Using score difference transformation property in (40) and one-sparse property of [s m (Z)sm (Z)] via Lemma 1(iii), we have</p><formula xml:id="formula_122">[D t ( ĥm )] i,m = E s m Ẑ ( Ẑ) -sm Ẑ ( Ẑ) i (79) = E J -⊤ f (Z) i • s m (Z) -sm (Z) (80) = E J -1 f (Z) ℓ,i • s m (Z) -sm (Z) ℓ .<label>(81)</label></formula><p>By definition of ĥm , [D t ( ĥm )] i,m = 0 for all i ̸ = m. Also, interventional discrepancy between q ℓ (z l ) and qℓ (z l ) implies that s m (z)sm (z) ℓ ̸ = 0 except for a null set. Hence, if J -1 f (z) ℓ,i is nonzero over a nonzero-measure set within R n , then [D t ( ĥm )] i,m would not be zero. Therefore, we have J -1 f (z) ℓ,i = 0 except for a null set. Since J -1 f is a continuous function, this implies that</p><formula xml:id="formula_123">J -1 f (z) ℓ,i = 0 , ∀i ∈ [n] \ m, ∀z ∈ R n ,<label>(82)</label></formula><p>Furthermore, since J -1 f (z) must be invertible, we have</p><formula xml:id="formula_124">[J -1 f (z)] ℓ,m ̸ = 0 for all z ∈ R n . Consider Z ℓ = [f -1 ( Ẑ)] ℓ . Since J -1 f (z) = J f -1 (ẑ), (82) implies that Z ℓ = ψ( Ẑm )</formula><p>for some diffeomorphism ψ : R → R. Thus, Ẑm = ψ -1 ( Ẑℓ ) where ℓ = I m , which concludes the proof. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Complete Identifiability under Coupled Interventions</head><p>After achieving the node-level latent variable identifiability from one pair of coupled interventions, we extend the objective function in Theorem 5 for a complete set of coupled interventions.</p><p>Encoder estimation. In Stage G1 of Algorithm 3, we combine the objectives in (76) for all m ∈ [n] and form our encoder estimate by solving the following optimization problem</p><formula xml:id="formula_125">min h∈H D t (h) -I n×n 2 F . (<label>83</label></formula><formula xml:id="formula_126">)</formula><p>Recall that the valid encoders set H consists of invertible encoders over the input space X . Hence, ( <ref type="formula" target="#formula_125">83</ref>) is readily equivalent to solving the following problem for any λ ∈ R + :</p><formula xml:id="formula_127">min h D t (h) -I n×n 2 F + λ E h -1 (h(X)) -X 2 2 . (<label>84</label></formula><formula xml:id="formula_128">)</formula><p>Lemma 8 Given a complete set of coupled interventions that satisfy interventional discrepancy, output ĥ of Algorithm 3 satisfies component-wise latent recovery.</p><p>Proof: Following Theorem 5 for all m ∈ [n] immediately implies the desired result. First, recall that h * = D t • g -1 satisfies D t (h * ) = I n×n and belongs to H. Therefore, h * makes the objective (84) zero. Next, consider a solution ĥ that makes (84) zero. Since ĥ makes the reconstruction loss term zero, i.e., it has a valid inverse ĥ-1 , ĥ belongs to the set of valid encoders H. Then, for all m ∈ [n], we have [D t ( ĥ)] :,m = e m . By Theorem 5, this implies Ẑ(X; ĥ</p><formula xml:id="formula_129">) = P I • ϕ(Z) ,<label>(85)</label></formula><p>where </p><formula xml:id="formula_130">ϕ(Z) = (ϕ 1 (Z 1 ), . . . , ϕ n (Z n )) is a componentwise diffeomorphism,</formula><formula xml:id="formula_131">ĥ ← arg min h D t (h) -I n×n 2 F + λ E h -1 (h(X)) -X 2 2</formula><p>4: Latent variable estimates: Ẑ = ĥ(X).</p><p>5: Stage G2: Latent graph estimation 6: Construct latent DAG estimate Ĝ with parent sets</p><formula xml:id="formula_132">pa(m) ≜ i ̸ = m : E s m Ẑ ( Ẑ) -sm Ẑ ( Ẑ) i ̸ = 0 , ∀m ∈ [n]</formula><p>.</p><p>7: return Ĝ, ĥ, and Ẑ Latent graph estimation. Next, we use the encoder estimate ĥ for recovering the latent graph G. In Stage G2 of Algorithm 3, we use the score differences between observational and interventional environments and form Ĝ with parent sets</p><formula xml:id="formula_133">pa(m) ≜ i ̸ = m : E s Ẑ( Ẑ; ĥ) -s m Ẑ ( Ẑ; ĥ) i ̸ = 0 , ∀m ∈ [n] . (<label>86</label></formula><formula xml:id="formula_134">)</formula><p>Lemma 9 Given a complete set of coupled interventions that satisfy interventional discrepancy, Algorithm 3 output Ĝ and true latent DAG G are related through a graph isomorphism.</p><p>Proof: Denote f = ĥ • g, so we have Ẑ = f (Z). Using the score transform between Z and Ẑ and the structure of J -1 f given in (82), we have</p><formula xml:id="formula_135">s Ẑ( Ẑ; ĥ) -s m Ẑ ( Ẑ; ĥ) i = J -1 f (Z) i,I i • s(Z) -s m (Z) i (87) Since [J -1 f (Z)] i,I i ̸ = 0 for all z ∈ R n , using Lemma 1(i), we find i ∈ pa(m) ⇐⇒ E s(Z) -s m (Z) I i ̸ = 0 ⇐⇒ I i ∈ pa(I m ),<label>(88)</label></formula><p>which concludes the proof that G and Ĝ are related through a graph isomorphism. □</p><p>Theorem 6 (General -Coupled Environments) Using observational data and interventional data from two coupled hard environments for which the pair (q i , qi ) satisfies interventional discrepancy for all i ∈ [n], suffice to identify (i) the latent DAG G perfectly and (ii) the latent variables Z up to componentwise diffeomorphisms specified in Definition 5. Specifically, Algorithm 3 achieves these identifiability guarantees.</p><p>Proof: Combining Lemma 8 and Lemma 9 gives the theorem statements. □ We emphasize that a key contribution of Theorem 6 beyond the identifiability result is that it provides a well-defined objective function, a minimizer of which provably recovers the latent variables and the graph up to identifiability guarantees. To our knowledge, this is the first such result for interventional CRL with general transformations. In comparison, von Kügelgen et al. ( <ref type="formula">2023</ref>) also use two hard interventions per node to prove identifiability, albeit without providing a provably correct algorithm. Furthermore, the result in <ref type="bibr">(von Kügelgen et al., 2023)</ref> requires that the estimated latent distribution is faithful to the associated candidate graph for all h ∈ H. Although a faithfulness assumption does not compromise the identifiability result, it is a strong requirement to verify and poses challenges to devise recovery algorithms. In contrast, we only require observational data, which is generally accessible in practice.</p><p>Next, we shed light on the role of observational data. Lemma 8 requires only interventional data for identifying the encoder, whereas Lemma 9 uses observational data. We further tighten this result by showing that for DAG recovery, the observational data becomes unnecessary when we have additive noise models and a weak faithfulness condition holds.</p><p>Theorem 7 (No Observational Data) Using interventional data from two coupled hard environments for which the pair (q i , qi ) satisfies interventional discrepancy for all i ∈ [n], suffices to identify the latent variables Z up to component-wise diffeomorphisms. If the latent causal model has additive noise, p(Z) is twice differentiable and satisfies the adjacencyfaithfulness with respect to G, then the latent DAG G is also identifiable.</p><p>Proof sketch: The recovery of latent variables follow from Lemma 8. For the recovery of latent DAG, we leverage Lemma 1(iv) under the additive noise assumption. Specifically, using the score difference between environments E i and Ẽj , we obtain pa(i, j) for all i, j ∈ [n], i ̸ = j. Finally, we perform at most n conditional independence tests to identify all parent sets {pa(i) : i ∈ [n]}. See Appendix C.1 for the complete proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Complete Identifiability under Uncoupled Interventions</head><p>In this setting, we consider the case where the interventional environments corresponding to the same nodes are not specified in pairs. That is, not only is it unknown which node is intervened in an environment, but the learner also does not know which two environments intervene on the same node. Hence, additionally, we need to determine the correct coupling between the interventional environment sets E and Ẽ as well. However, this is not a straightforward objective since we cannot readily determine the intervention target in an environment. To address this issue, we solve the problems of finding the correct coupling and optimizing the estimated score variations together. Let σ denote the permutation that takes Ĩ = ( Ĩ1 , . . . , Ĩn ) to I = (I 1 , . . . , I n ), i.e., σ( Ĩm ) = I m for all m ∈ [n]. We will modify the objective function in (84) to solve for permutation π as an estimate of σ, in addition to solving for encoder h. To this end, we first modify the score change matrix in (72) and define D t (h; π) as</p><formula xml:id="formula_136">[D t (h; π)] i,m ≜ E s m Ẑ ( Ẑ; h) -sπm Ẑ ( Ẑ; h) i ,<label>(89)</label></formula><p>Unlike the case of coupled interventions, we will also leverage the observational data to identify the encoder. As such, we also define the score change matrices for E and Ẽ with respect to E with entries</p><formula xml:id="formula_137">[D(h)] i,m ≜ E s Ẑ( Ẑ; h) -s m Ẑ ( Ẑ; h) i ,<label>(90)</label></formula><formula xml:id="formula_138">[ D(h)] i,m ≜ E s Ẑ( Ẑ; h) -sm Ẑ ( Ẑ; h) i . (<label>91</label></formula><formula xml:id="formula_139">)</formula><p>Encoder and coupling estimation. We modify the objective in (84) and solve the following constrained optimization problem for any λ ∈ R + :</p><formula xml:id="formula_140">min h,π D t (h; π) -I n×n 2 F + λ E h -1 (h(X)) -X 2 2 such that 1{D(h)} = 1{ D(h)} • P ⊤ π 1{D(h)} ⊙ 1{[D(h)] ⊤ } = I n×n . (<label>92</label></formula><formula xml:id="formula_141">)</formula><p>Note that for the true encoder g -1 , both D(g -1 ) and D(g -1 ) capture the graph structure via Lemma 1(i), albeit under different permutations. Hence, this equivalence of graph structures for the learned encoder is ensured by the first constraint of 1{D(h)} = P π • 1{ D(h)}. Also, the acyclicity of the graphs implied by the score differences is ensured by the second constraint, that is D(h) does not contain 2-cycles. To prove that solving this problem leads to the identifiability of the latent variables Z, we first show that there exists a global minimizer of this problem.</p><p>Lemma 10 (Existence) Encoder h * = D t (g -1 ; σ) ⊤ • g -1 and permutation π * = σ minimize the objective in (92).</p><p>Proof sketch: See Appendix C.2 for the complete proof. The main intuition is that π * = σ makes the problem more similar to the case of coupled interventions. The proof then follows by using the results of the coupled interventions to show that D t (g -1 ; σ) is a scaled permutation matrix, using (77) to derive the relationship between the score differences under ĥ * and the true encoder g -1 , and using the sparsity structure of D(g -1 ) to show that the constraints are satisfied.</p><p>Next, we show that the objective in (92) can attain its minimum value zero only for the correct coupling.</p><p>Lemma 11 (Feasibility) The optimization problem in (92) attains its minimum value zero only for the correct coupling π = σ.</p><p>Proof sketch: See Appendix C.3 for the complete proof. The main intuition is that the constraints make it impossible to achieve D t (h; π) = I n×n for an incorrect coupling π ̸ = σ. We prove it by contradiction. We assume that h * is a solution, hence, D t (h * ) = I n×n , and 1{D(h)} = 1{ D(h)}. Then, by scrutinizing the eldest node in G with mismatched interventional environments, we show that D(h * ) ⊙ [D(h * )] ⊤ cannot be a diagonal matrix, which contradicts the premise that h * is a feasible solution of (84).</p><p>Finally, we denote the minimizer encoder of (92) by ĥ and form the graph estimate similarly to the coupled intervention case via (86). Combining these results, we present our strongest result for general transformations.</p><p>Theorem 8 (General -Uncoupled Environments) Using observational data and interventional data from two uncoupled hard environments for which each pair in (p i , q i , qi ) satisfies interventional discrepancy for all i ∈ [n], suffice to identify (i) the latent DAG G perfectly and (ii) the latent variables Z up to component-wise diffeomorphisms.</p><p>Proof: Lemma 11 shows that minimizers of the objective in (92) necessarily have π = σ. Under this correct coupling, any minimizer of the constrained optimization problem in (92) that attains the minimum value zero is also a minimizer of the unconstrained optimization problem in (84). Finally, Lemma 8 and Lemma 10 show that such a shared minimizer exists, that is h * = [D t (g -1 )] ⊤ • g -1 . Hence, by Theorem 6, minimizer ĥ of (92) satisfies component-wise latent recovery. Similarly, using ĥ, proof of the perfect graph recovery follows from Lemma 9.</p><p>□ Theorem 8 shows that using observational data enables us to resolve any mismatch between the uncoupled environment sets and shows identifiability in the setting of uncoupled environments. This generalizes the identifiability result of von Kügelgen et al. ( <ref type="formula">2023</ref>), which requires coupled environments.</p><p>Comparison to Theorem 6. We note that Theorem 8 requires slightly stronger interventional discrepancy conditions than Theorem 6. In particular, when environments are coupled, we only need (q i , qi ) to satisfy the interventional discrepancy. On the other hand, to find the correct coupling while performing CRL, Theorem 8 requires each of the pairs {(q i , qi ), (p i , q i ), (p i , qi )} to satisfy interventional discrepancy.</p><p>Remark 2 For the nonparametric identifiability results, having an oracle that solves the functional optimization problems (84) and (92) is sufficient. Solving these two problems in their most general form requires calculus of variations. These two problems, however, for any desired parameterized family of functions H (e.g., linear, polynomial, and neural networks), reduce to parametric optimization problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Intervention Extrapolation without CRL</head><p>So far, we have studied learning the latent causal representations using the data from single-node interventional environments. A related research problem is extrapolating to unseen combinations of interventions. Namely, given a set of interventions I = {I 1 , . . . , I k }, it is desired to emulate the data from an unseen combination of these interventions, e.g., accessing multi-node interventional data using only the single-node interventional data. This is especially important in domains where interventions can be costly or not viable, e.g., not all combinations of different drugs can be clinically tested. Causal representation learning literature has also taken an interest in this problem. Specifically, <ref type="bibr">Zhang et al. (2023, Theorem 3)</ref> show for polynomial transformations that given interventions I = {I 1 , . . . , I k }, one can sample from any intervention I ⊆ I after learning the latent representations. We argue that extrapolation to unseen combinations of interventions can be achieved on the observed space without performing CRL for the general transformations.</p><p>Consider two single-node interventional environments, E 1 and E 2 . Without loss of generality, suppose that I 1 = {1} and I 2 = {2}. Also consider the observational environment E 0 with I 0 = ∅ and the unseen double-node interventional environment E m with I m = {1, 2}.</p><p>First, using the score function decompositions in ( <ref type="formula" target="#formula_31">18</ref>) and ( <ref type="formula" target="#formula_32">19</ref>), we have</p><formula xml:id="formula_142">s(z) = ∇ z log p 1 (z 1 | z pa(1) ) + ∇ z log p 2 (z 2 | z pa(2) ) + n k=3 ∇ z log p k (z k | z pa(k) ) , (93) s 1 (z) = ∇ z log q 1 (z 1 | z pa(1) ) + ∇ z log p 2 (z 2 | z pa(2) ) + n k=3 ∇ z log p k (z k | z pa(k) ) ,<label>(94)</label></formula><formula xml:id="formula_143">s 2 (z) = ∇ z log p 1 (z 1 | z pa(1) ) + ∇ z log q 2 (z 2 | z pa(2) ) + n k=3 ∇ z log p k (z k | z pa(k) ) ,<label>(95)</label></formula><formula xml:id="formula_144">s m (z) = ∇ z log q 1 (z 1 | z pa(1) ) + ∇ z log q 2 (z 2 | z pa(2) ) + n k=3 ∇ z log p k (z k | z pa(k) ) .<label>(96)</label></formula><p>Then, we have</p><formula xml:id="formula_145">s 1 (z) -s(z) + s 2 (z) -s(z) = s m (z) -s(z) .<label>(97)</label></formula><p>Next, recall that Lemma 2 gives us the relationship for going from the latent score differences to observed score differences. Applying it to observed X in environment pairs (E 1 , E 0 ), (E 2 , E 0 ), and (E m , E 0 ), and using (97) we obtain</p><formula xml:id="formula_146">s 1 X (x) -s X (x) + s 2 X (x) -s X (x) = [J g (z)] † ⊤ • s 1 (z) -s(z) + s 2 (z) -s(z) (98) = [J g (z)] † ⊤ s m (z) -s(z) (<label>99</label></formula><formula xml:id="formula_147">) (37) = s m X (x) -s X (x) . (<label>100</label></formula><formula xml:id="formula_148">)</formula><p>This means that, given score functions of observed data from environments E 0 , E 1 , and E 2 , we can obtain the score function of the unseen interventional environment with I m = {I 1 , I 2 }, and subsequently generate data from this synthetic environment (using techniques like Langevin sampling <ref type="bibr" target="#b55">(Welling and Teh, 2011</ref>) that uses score function as the drift vector field). Note that this process does not require learning the latent causal representations and can be applied to obtain the observed score function s m X for any intervention I m ⊂ I. Finally, we note that concurrent work by Jain et al. ( <ref type="formula">2024</ref>) makes a similar observation for the extrapolation of two single-node interventions without performing CRL. In particular, <ref type="bibr" target="#b16">(Jain et al., 2024)</ref> focuses on detecting pairwise interactions between two biological perturbations (i.e., interventions), investigates the conditions under which two perturbations are separable, e.g., distinct single-node interventions, and designs statistical tests for the separability of two perturbations. If the two perturbations are confirmed to be separable, our results indicate that we can use their score functions to generate synthetic environments that emulate combined perturbation effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Empirical Studies</head><p>In this section, we provide empirical assessments of our theoretical guarantees. Specifically, we empirically evaluate the performance of the LSCALE-I (Section 7.1) and GSCALE-I (Section 7.2) algorithms for recovering the latent causal variables Z and the latent DAG G on synthetic data. In Section 7.3, we compare the performance of LSCALE-I to that of the existing algorithms in the closely related literature on both synthetic and biological data. Next, we also apply GSCALE-I on image data to demonstrate the potential of our approach in realistic high-dimensional datasets (Section 7.5). Finally, we note that any desired score estimator can be modularly incorporated into our algorithms. In Section 7.6, we assess our performance's sensitivity to the estimators' quality. <ref type="foot" target="#foot_4">7</ref> Additional results and further implementation details are deferred to Appendix E.</p><p>Evaluation metrics. The objectives are recovering the graph G and the latent variables Z. We use the following metrics to evaluate the accuracy of LSCALE-I and GSCALE-I in recovering these (depending on the specifics of the transformations and interventions, we will also have more specific metrics). For each metric, we will report the mean and standard error over multiple runs.</p><p>• Structural Hamming distance: For assessing the recovery of the latent DAG, we report structural Hamming distance (SHD) between the estimate Ĝ and true DAG G. This captures the number of edge operations (add, delete, flip) needed to transform Ĝ to G.</p><p>• Mean correlation coefficient: For the recovery of the latent variables, we use the mean correlation coefficient (MCC), which was introduced in <ref type="bibr">(Khemakhem et al., 2020b)</ref> and commonly used as a standard metric in CRL. Specifically, MCC measures the linear correlation between the estimated and ground-truth latent variables. Since the recovery of latent variables is up to permutations, it is reported for the best matching permutation between the components of Ẑ and Z, i.e.,</p><formula xml:id="formula_149">MCC(Z, Ẑ) ≜ max π 1 n i∈[n] corr(Z i , Ẑπ(i) ) . (<label>101</label></formula><formula xml:id="formula_150">)</formula><p>Score functions. LSCALE-I and GSCALE-I algorithms, in their first steps, compute estimates of the score differences in the observational environment. The designs of our algorithms are agnostic to how this is performed, i.e., any reliable method for estimating the score differences can be adopted and incorporated into our algorithms in a modular way. In our experiments, we adopt two score estimators necessary for describing the different aspects of our results.</p><p>• Perfect score oracle for identifiability: Identifiability, by definition, refers to the possibility of recovering the causal graph and latent variables under all idealized assumptions for the data. Assessing the identifiability guarantees formalized in Theorems 2-8 requires using perfect estimates for the score differences. Hence, we adopt a perfect score oracle for evaluating identifiability. Specifically, we use a perfect score oracle that computes the score difference inputs in LSCALE-I and GSCALE-I by leveraging Lemma 2 and using the ground truth score functions s, s m and sm (see Appendix E.1 for details).</p><p>• Data-driven score estimates: For evaluating the accuracy of our algorithms in practice, we need real score estimates, which are inevitably noisy. For this purpose, when the pdf p X has a parametric form and the score function s X has a closed-form expression, then we can estimate the parameters to form an estimated score function. For instance, when X follows a linear Gaussian distribution, then we have s X (x) = -Θ • x in which Θ is the precision matrix of X and can be estimated from samples of X. In other cases in which s X does not have a known closed-form, we adopt nonparametric score estimators.</p><p>In particular, we use sliced score matching with variance reduction (SSM-VR) for score estimation due to its efficiency and accuracy for downstream tasks <ref type="bibr" target="#b45">(Song et al., 2020)</ref>.</p><p>We also introduce a classification-based score difference estimation method, inspired by Gutmann and Hyvärinen (2012, Section 2.1). The key observation is that given two distributions p and q, the optimal minimum cross-entropy classifier for distinguishing the samples from two distributions is the log density ratio function log p/q. The difference between the score functions of these distributions, ∇ log p-∇ log q = ∇ log p/q is exactly the gradient of the learned function, which enables us to estimate score differences using a classifier directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">LSCALE-I Algorithm for Linear Transformations</head><p>Data generation. To generate G, we use Erdős-Rényi model with density 0.5 and n ∈ {5, 8} nodes, which is generally the size of the latent graphs considered in CRL literature. We consider the observed dimension d = 100 and generate 100 latent graphs. For the causal mechanisms, we adopt both linear and nonlinear models:</p><p>1. Linear causal model: We adopt the linear Gaussian model with</p><formula xml:id="formula_151">Z i = A i • Z + N i , ∀i ∈ [n] ,<label>(102)</label></formula><p>where A i ∈ R 1×n are the rows of the weight matrix A in which A i,j ̸ = 0 if and only j ∈ pa(i). The nonzero edge weights are sampled from Unif(±[0.5, 1.5]), and the noise terms are zero-mean Gaussian variables with variances σ 2 i sampled from Unif([0.5, 1.5]). For node i, a hard intervention is given by Z i = Ni where Ni ∼ N (0, σ 2 i 4 ), and a soft intervention is given by Z i = Āi 2 • Z + N i . 2. Quadratic causal model: We adopt an additive noise model with</p><formula xml:id="formula_152">Z i = Z ⊤ pa(i) • Q i • Z pa(i) + N i ,<label>(103)</label></formula><p>where {Q i : i ∈ [n]} are positive-definite matrices, and the noise terms are zero-mean Gaussian variables with variances σ 2 i sampled randomly from Unif([0.5, 1.5]). For node i, a hard intervention is given by Z i = Ni where Ni ∼ N (0, 5 • σ 2 i ) to ensure a non-negligible change. A soft intervention is given by</p><formula xml:id="formula_153">Z i = 1 2 Z ⊤ pa(i) • Q i • Z pa(i) + Ni .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-layer perceptron (MLP) causal model:</head><p>We adopt an additive noise model with Z i = f i (Z pa(i) ) + N i where f i is parameterized as a randomly initialized two-layer 5 100 5000 1.00 ± 0.00 0.02 ± 0.00 0.17 ± 0.04 1.00 ± 0.00 0.06 ± 0.01 0.11 ± 0.03 5 100 10000 1.00 ± 0.00 0.01 ± 0.00 0.09 ± 0.03 1.00 ± 0.00 0.04 ± 0.00 0.09 ± 0.04 5 100 50000 1.00 ± 0.00 0.01 ± 0.00 0.03 ± 0.01 1.00 ± 0.00 0.03 ± 0.00 0.07 ± 0.03 8 100 5000 1.00 ± 0.00 0.03 ± 0.00 0.03 ± 0.02 1.00 ± 0.00 0.08 ± 0.00 0.46 ± 0.08 8 100 10000 1.00 ± 0.00 0.02 ± 0.00 0.06 ± 0.02 1.00 ± 0.00 0.06 ± 0.00 0.18 ± 0.04 8 100 50000 1.00 ± 0.00 0.01 ± 0.00 0.02 ± 0.01 1.00 ± 0.00 0.03 ± 0.00 0.04 ± 0.02 MLP, with hidden dimension 32 and ReLU activation function. For node i, a hard intervention is given by Z i = Ni where N (0, 5 • σ 2 i ), and a soft intervention is given by</p><formula xml:id="formula_154">Z i = 1 2 f i (Z pa(i) ) + Ni .</formula><p>For each graph, we sample n s independent and identically distributed (i.i.d.) samples of Z from each environment. We consider n s ∈ {5000, 10000, 50000} to investigate the effect of the number of samples on the performance of LSCALE-I. The observed variables X are generated according to X = G • Z, in which G ∈ R d×n is a randomly sampled full-rank matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Hard Interventions</head><p>Theorem 3 ensures scaling consistency and perfect DAG recovery under hard interventions for linear transformations. As such, we assess the recovery of the latent DAG by the SHD between the estimate Ĝ and true graph G. For latent variable recovery, we report MCC between Ẑ and Z. Note that Ẑ = (H • G) • Z implies that the effective transform recovery can also be measured by the closeness of H • G to the identity matrix. Therefore, in addition to MCC, we also report the normalized effective transform error, defined as</p><formula xml:id="formula_155">ℓ scale ≜ ∥H • G -I n×n ∥ 2 . (<label>104</label></formula><formula xml:id="formula_156">)</formula><p>Table <ref type="table" target="#tab_7">3</ref> shows the performance of the LSCALE-I algorithm using perfect scores and noisy scores under hard interventions on linear causal models. The first observation is that we achieve excellent performance in latent variable recovery, as demonstrated by a perfect MCC and nearly zero effective transform error ℓ scale , even when using noisy score estimates. For graph recovery, SHD between the estimated and true latent graphs reduces to less than 0.1 even when using noisy scores given enough samples, e.g., n s = 50000 in Table <ref type="table" target="#tab_7">3</ref>. Another key observation is that the results remain consistent while the dimension of observed variables increases from d = 25 to d = 200, as shown in Table <ref type="table" target="#tab_8">4</ref>. This confirms our analysis that the performance of LSCALE-I is agnostic to the dimension of the observations. Hence, we suffice by using d = 100 for the rest of the experiments.</p><p>Causal models with more complex score functions. Next, we test the performance of LSCALE-I on quadratic and MLP causal models specified earlier. A difference in this setting compared to linear causal models is that we estimate the score functions of the observed variables using SSM-VR <ref type="bibr" target="#b45">(Song et al., 2020)</ref> as p X is not amenable to parameter estimation. 5 25 10000 1.00 ± 0.00 0.01 ± 0.00 0.28 ± 0.05 1.00 ± 0.00 0.03 ± 0.00 0.03 ± 0.02 5 50 10000 1.00 ± 0.00 0.01 ± 0.00 0.23 ± 0.04 1.00 ± 0.00 0.03 ± 0.00 0.04 ± 0.02 5 100 10000 1.00 ± 0.00 0.01 ± 0.00 0.06 ± 0.02 1.00 ± 0.00 0.04 ± 0.00 0.09 ± 0.03 5 200 10000 1.00 ± 0.00 0.01 ± 0.00 0.07 ± 0.03 1.00 ± 0.00 0.05 ± 0.00 0.16 ± 0.04 For quadratic causal models, Table <ref type="table" target="#tab_9">5</ref> shows that using perfect scores, LSCALE-I performs nearly perfectly. Under noisy scores, an MCC of 0.93 indicates a strong performance for the latent variable recovery. The graph recovery performance suffers more from noisy score estimates, yet it remains reasonable with an approximate SHD of 2.62 (where the expected number of true edges is 5). Table <ref type="table" target="#tab_10">6</ref> shows a similar trend, where perfect scores ensure perfect performance, whereas noisy score estimates lead to a degradation in graph recovery performance.</p><p>Effect of noisy score estimates on graph recovery under noisy scores. Table <ref type="table" target="#tab_9">5</ref> and Table <ref type="table" target="#tab_10">6</ref> show that graph recovery performance suffers more than the latent variable recovery under noisy score estimates. This discrepancy is due to the difficulty of applying (59) under noisy scores. Specifically, we observe that the nonzero entries in true score differences (entries of d m ) can be small, especially for MLP causal models. As such, under noisy score estimates, it becomes more difficult to successfully threshold the empirical score difference quantities to identify the edges of the latent graph. In Section 7.6, we provide further empirical evaluations on the effect of score estimation quality on the performance of LSCALE-I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Soft Interventions</head><p>To evaluate soft interventions, we first consider linear causal models. In this setting, Theorem 2 ensures the recovery of the latent variables and the latent DAG up to ancestors when using soft interventions. So, we report the SHD between the transitive closure of the estimate Ĝtc and that of the true graph G tc . Recall that recovery of latent variables for generic soft interventions is guaranteed up to mixing with parents. To measure the accuracy of Ẑ = H•G•Z with respect to this guarantee, we define L pa with entries [L pa ] i,j = 1{j ∈ pa(i)}, and define ℓ pa by</p><formula xml:id="formula_157">ℓ pa ≜ ∥H • G ⊙ (1 n×n -L pa )∥ 2 ,<label>(105)</label></formula><p>which effectively measures the effect of incorrect mixing in estimated latent variables. Table <ref type="table" target="#tab_11">7</ref> shows that by using perfect scores from as few as n s = 5000 samples, we meet the theoretical 5 100 5000 0.98 ± 0.00 0.00 ± 0.00 0.01 ± 0.00 0.98 ± 0.00 0.04 ± 0.00 0.59 ± 0.11 5 100 10000 0.98 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.98 ± 0.00 0.03 ± 0.00 0.36 ± 0.08 5 100 50000 0.98 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.98 ± 0.00 0.01 ± 0.00 0.28 ± 0.06 8 100 5000 0.98 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.98 ± 0.00 0.07 ± 0.00 3.84 ± 0.36 8 100 10000 0.98 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.98 ± 0.00 0.05 ± 0.00 1.23 ± 0.20 8 100 50000 0.98 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.98 ± 0.00 0.02 ± 0.00 0.49 ± 0.10 identifiability guarantees, implied by zero values of SHD and incorrect mixing norm ℓ pa . Similar to the case of hard interventions, we observe that increasing the number of samples improves the performance under noisy scores. For instance, given n s = 50000 samples, the average SHD between the transitive closures Ĝtc and G tc is approximately 0.5 for n = 8, where graph density being 0.5 implies that the expected number of edges is 14. Also, the latent variable recovery becomes near perfect, indicated by MCC of 0.98 and near-zero incorrect mixing norm ℓ pa . Next, we consider quadratic causal models. In this case, Assumption 2 is satisfied, and Theorem 4 ensures the perfect recovery of the latent DAG and the recovery of the latent variables up to surrounding variables. Hence, we report SHD between true G and estimate Ĝ, and the incorrect mixing norm for this setting given by</p><formula xml:id="formula_158">ℓ sur ≜ ∥H • G ⊙ (1 n×n -L sur )∥ 2 , (<label>106</label></formula><formula xml:id="formula_159">)</formula><p>where L sur is defined with entries [L pa ] i,j = 1{j ∈ sur(i)}. Table <ref type="table" target="#tab_12">8</ref> shows that when using perfect scores, LSCALE-I performs nearly perfectly, verifying the results of Theorem 4. Similarly to the case of hard interventions, the performance of LSCALE-I suffers under noisy score estimations, while it remains reasonable, e.g., an SHD of 2.79 with respect to the true graph when using only soft interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">GSCALE-I Algorithm for General Transformations</head><p>Next, we focus on nonlinear transformations to showcase the settings for which the existing interventional CRL literature lacks provably correct algorithms and provides only identifiability results under nonlinear transformations.</p><p>Choice of nonlinearity. In this section, we consider M-layer perceptrons with tanh activation as our class of nonlinear transformations. Specifically, we consider functions of the </p><formula xml:id="formula_160">form X = g(Z) = (tanh •A M • • • • tanh •A 1 )(Z) , (<label>107</label></formula><formula xml:id="formula_161">)</formula><p>in which tanh is applied element-wise, and parameters {A 1 , . . . , A M } are compatible, randomly sampled full column rank matrices. We consider two settings of increasing complexity: 1-layer MLP, and 3-layer MLP (for nonparametric transformations, see experiments on images in Section 7.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Single-layer MLP</head><p>First, consider the single-layer perceptron as the nonlinear transformation g, given by</p><formula xml:id="formula_162">X = g(Z) = tanh(G • Z) ,<label>(108)</label></formula><p>where G ∈ R d×n . Because it is a relatively simple nonlinear transformation, this setting enables us to evaluate the performance of GSCALE-I more effectively. Specifically, leveraging (108), we parameterize valid encoders h with parameter H ∈ R n×d , which gives</p><formula xml:id="formula_163">Ẑ(X; h) = h(X) = H • arctanh(X) ,<label>(109)</label></formula><formula xml:id="formula_164">X = h -1 ( Ẑ(X; h)) = tanh H † • Ẑ(X; h) ,<label>(110)</label></formula><formula xml:id="formula_165">Ẑ = H • G • Z . (<label>111</label></formula><formula xml:id="formula_166">)</formula><p>This is equivalent to reducing the original nonparametric functional estimation problem to a finite-dimensional parameter estimation problem. This enables stable training and large-scale testing. Furthermore, this parametrization enables us to compute the ground truth score functions of the observed variables, allowing us to directly assess the effect of score estimation errors on GSCALE-I performance.</p><p>Data generation. In this setting, we focus on quadratic causal models. To generate G we use the Erdős-Rényi model with density 0.5 and n ∈ {5, 8} nodes. For observational causal mechanisms, we use Equation ( <ref type="formula" target="#formula_152">103</ref>) and the corresponding parameterization. For the two hard interventions on node i, Z i is set to N q,i ∼ N (0, σ 2 q,i ) and N q,i ∼ N (0, σ 2 q,i ), where we set σ 2 q,i = σ 2 i /4 and σ 2 q,i = 4 • σ 2 i . Similarly to LSCALE-I experiments, we consider a target dimension of d = 100 and generate 20 latent graphs, with n s = 30000 samples per environment for each graph.</p><p>Results. Recall that for general transformations, we can guarantee Ẑi = ϕ i (Z i ) for a diffeomorphism ϕ i . However, MCC(Z i , ϕ(Z i )) can largely deviate from 1 for a nonlinear ϕ i . For instance, for Z i ∼ N (0, 1) and ϕ(Z i ) = Z 3 i + 0.1Z i , we have MCC(Z i , ϕ(Z i )) ≈ 0.786. On the other hand, when we use the parameterization in ( <ref type="formula" target="#formula_163">109</ref>), the only element-wise diffeomorphism between Z and Ẑ is an element-wise scaling. Therefore, MCC remains a perfectly informative metric. Table <ref type="table" target="#tab_13">9</ref> shows that we can almost perfectly recover the latent variables (indicated by MCC of 1) latent DAG for n = 5 nodes by using perfect scores. Furthermore, increasing the latent dimension to n = 8 has no significant impact on performance.</p><p>The effect of the quality of score estimation. When using noisy scores, the performance of GSCALE-I degrades significantly. For instance, MCC goes down to approximately 0.85 for n = 5 and 0.75 for n = 8. This degradation is similar to what happens when using LSCALE-I on quadratic causal models in Table <ref type="table" target="#tab_12">8</ref>. It is noteworthy that the transition from perfect to noisy scores is remarkably smoother when using LSCALE-I on linear causal models (Table <ref type="table" target="#tab_7">3</ref> and<ref type="table" target="#tab_11">Table 7</ref>). This discrepancy is attributed to the distinct score estimation procedures adopted in the two experimental settings. Specifically, linear Gaussian latent models allow us to directly estimate the parameters of the closed-form score function s X . However, when using a quadratic latent model, we rely on a nonparametric score estimation via SSM-VR <ref type="bibr" target="#b45">(Song et al., 2020)</ref>. This comparison between two experiment settings and results underscores that the performance gap between the theoretical guarantees and practical results can be significantly mitigated through advances in general score estimation techniques. In Section 7.6, we provide a further empirical evaluation of how the quality of the score estimation affects the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Multi-layer nonlinearity</head><p>Next, we consider general MLPs by setting the number of layers to 3, which is in line with the MLP depths considered in CRL literature <ref type="bibr" target="#b26">(Liang et al., 2023)</ref>. In this setting, we do not use any knowledge of the transformation parametrization and aim to learn a generic NN-based encoder-decoder pair h, h -1 parameterized by θ enc , θ dec . In essence, this enables us to test our algorithm's performance on a standard procedure of function approximation via neural networks.</p><p>Data generation. In this setting, we focus on linear causal models. To generate G, we use the Erdős-Rényi model with density 0.5 and n = 5 nodes. For observational causal mechanisms, we use Equation ( <ref type="formula" target="#formula_151">102</ref>) and its associated parameterization. For the two hard interventions on node i, Z i is set to N q,i ∼ N (0, σ 2 q,i ) and N q,i ∼ N (0, σ 2 q,i ), where we set σ 2 q,i = σ 2 i /4 and σ 2 q,i = σ 2 i • 4. Due to the increased computational complexity of training, Results. GSCALE-I algorithm ensures latent variables recovery up to element-wise transformations. As a proxy, we report its linear analogue, i.e., MCC. We summarize the results of these experiments in Table <ref type="table" target="#tab_14">10</ref>.</p><p>In this setting, we obtain a smaller average MCC rate (0.54) compared to the 1-layer nonlinearity experiments in Table <ref type="table" target="#tab_13">9</ref>. Performance degradation can be attributed to two factors: First, making the transformation more complex and making it more difficult to construct any kind of autoencoder. In contrast to the parametric single-layer MLP setting in the previous section, fitting a generic NN has considerably higher sample complexity. Second, our methodology requires the estimation of score functions with relatively good accuracy. However, complex distributions can push forward distributions into very complex and, similarly, difficult-to-estimate landscapes, which can limit the accuracy of our algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Comparison with the Existing CRL Studies</head><p>In this section, we compare the performance of LSCALE-I with those of the approaches designed for comparable settings in <ref type="bibr" target="#b46">(Squires et al., 2023;</ref><ref type="bibr" target="#b59">Zhang et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">Comparison with linear causal models on synthetic data</head><p>A closely related study in the linear transformation setting is <ref type="bibr" target="#b46">(Squires et al., 2023)</ref>. As discussed in Section 5, <ref type="bibr" target="#b46">(Squires et al., 2023)</ref> presents identifiability guarantees and provides algorithms for linear transformations with linear causal models. For a fair comparison, we perform comparisons for linear Gaussian SEMs under the parameterization detailed in Section 7.1. Experiments with synthetic linear causal models in <ref type="bibr" target="#b46">(Squires et al., 2023)</ref> mostly focus on recovering the parameters of the true encoder G † . Hence, in Table <ref type="table" target="#tab_15">11</ref>, we report performance comparisons in terms of the latent variable recovery metric ℓ scale . Table <ref type="table" target="#tab_15">11</ref> shows that LSCALE-I achieves close to perfect accuracy with as few as n s = 5000 samples, whereas the algorithm of <ref type="bibr" target="#b46">Squires et al. (2023)</ref> requires significantly more samples (more than n s = 50, 000) to achieve a reasonable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">Comparison with DiscrepancyVAE on synthetic data</head><p>In this section, we compare LSCALE-I with the DiscrepancyVAE algorithm of <ref type="bibr" target="#b59">(Zhang et al., 2023)</ref> on the setting described in Section 7.1. We consider 10 runs of the quadratic causal model setting with soft interventions, latent dimension n = 5, observed dimension d = 100, and 50000 samples per environment. We observe that the average SHD of DiscrepancyVAE on this dataset is 3.1 ± 0.7, which is comparable to LSCALE-I performance of 2.79 ± 0.18 reported in Table <ref type="table" target="#tab_12">8</ref>. </p><formula xml:id="formula_167">&lt; l a t e x i t s h a _ b a s e = " g K t t R k G / V K I V a G V / k w j t h d F = " &gt; A A A D B H i c f Z L N b t Q w E M e a u E r x a O X C J W l R C H V Y K q w r E C D r g q r b V j S r l e N M N t b G H I n p S s r x w k N w Q x j C T G z G T d F h j J k / / G c t / j y f T F b c Y x e M b N W f v r N N / P D R + s b j I t q w D I V K X M S U Y t V F z C E D l W c K I N U J F V c J x N b z M w l i t i D M N I E n k h e c U W y l j + M k H K / E j + g J B S R d e r z R X r B Y g k V X U t M k j h y C B n F T R h u B m l t Q V N Z R O w E A C U A z a I l d S j p A L s y M f U S b X s m j Q h m / J E Z z d f m E o L a m c h p a B Y q u V v x X D k v x R u t W j e V n X N v O v n C p x L F P H J e R p B s b u o q w h V H Y y r k B h t X M A W G + E r K S G M v T / s H I y F p w B O E t i x r a u y U b N o U g P X N o a z z J H g n D M B L Q Y C g q K l E w E l c p b + V j P / x R C l M J n A E Z U C b m / c Y Q f t d C R X Z + E H L c J N u D f t / s b b k W y F P y j D w n C X l F d s g u S N D w k h J P p M v G t w E X w L v g c / F q V B r z v z h K x E P M u S v E g = = &lt; / l a t e x i t &gt; Z 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p 7 i y s E u b c G D j M d o e c 5 z H a o x f P X M = " &gt; A A A D A 3 i c f Z L N b t Q w E M e 9 4 a u E r x a O X C J W l R C H V V J V h W N V O H B B l N J t K 5 r V y n E m W W v j D 9 m T t i s r R 4 5 c 4 S G 4 I a 4 8 C K / A U + D s B m m 3 B U a y / N N / x v L f 4 8 l 0 x S 3 G 8 c 9 e c O 3 6 j Z u 3 1 m 6 H d + 7 e u / 9 g f e P h k V W 1 Y T B k q l L m J K M W K i 5 h i B w r O N E G q M g q O M 6 m L 9 v 8 8 R k Y y 5 U 8 x J m G k a C l 5 A V n F L 3 0 / s N 4 a 7 z e j w f x P K K r k H T Q J 1 3 s j z d 6 v 9 J c s V q A R F Z R a 0 + T W O P I U Y O c V d C E 4 W a U 1 h Y 0 Z V N a g i t B C U A z a 8 I l 9 d S j p A L s y M 0 f 0 U S b X s m j Q h m / J E Z z d f m E o 8 L a m c h 8 p a A 4 s Z d z r f i v H E 7 E X 1 O 5 b e 9 a N Z a f c W 0 7 a x c L b 6 v G s X g x c l z q G k G y h e + i r i J U U d v g K O c G G F Y z D 5 Q Z 7 n s S s Q k 1 l K H / h p X b Q d a C I w j f s W V Z U 2 O n X L d q D k V 6 4 N L W e J a 5 g 8 Y 7 e Q W + 5 w b e e O m t B k N R m W c u p a Y U X D Z z S F v 6 X y G 9 + F P o K U w l n O M E l A H h u r 1 x h x 2 E f j q S y 7 N w F Y 6 2 B s n O Y O f d d n 9 3 r 5 u T N f K Y P C F P S U K e k 1 3 y m u y T I W G k J J / I Z / I l + B h 8 D b 4 F 3 x e l Q a 8 7 8 4 i s R P D j N 3 q 7 + / 8 = &lt; / l a t e x i t &gt; Z 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S W e 7 L 6 m j H H n E b 3 8 D O q N h b 5 j c q 0 0 = " &gt; A A A D A 3 i c f Z L N b t Q w E M e 9 4 a u E r x a O X C J W l R C H V U J R 6 b E q H L g g S u m 2 F c 1 q 5 T i T r L X x h + x J 6 c r K k S N X e A h u i C s P w i v w F D i 7 Q d p t g Z E s / / S f s f z 3 e D J d c Y t x / L M X X L l 6 7 f q N t Z v h r d t 3 7 t 5 b 3 7 h / Z F V t G A y Z q p Q 5 y a i F i k s Y I s c K T r Q B K r I K j r P p i z Z / f A b G c i U P c a Z h J G g p e c E Z R S + 9 e z / e G q / 3 4 0 E 8 j + g y J B 3 0 S R f 7 4 4 3 e r z R X r B Y g k V X U 2 t M k 1 j h y 1 C B n F T R h u B m l t Q V N 2 Z S W 4 E p Q A t D M m n B J P f U o q Q A 7 c v N H N N G m V / K o U M Y v i d F c X T 7 h q L B 2 J j J f K S h O 7 M V c K / 4 r h x P x 1 1 R u 2 7 t W j e V n X N v O 2 v n C 2 6 p x L H Z G j k t d I 0 i 2 8 F 3 U V Y Q q a h s c 5 d w A w 2 r m g T L D f U 8 i N q G G M v T f s H I 7 y F p w B O E 7 t i x r a u y U 6 1 b N o U g P X N o a z z J 3 0 H g n L 8 H 3 3 M B r L 7 3 R Y C g q 8 8 S l 1 J S C y 2 Y O a U v / K 6 T n f w o 9 h a m E D z g B Z U C 4 b m / c Y Q e h n 4 7 k 4 i x c h q O n g 2 R 7 s P 3 2 W X 9 3 r 5 u T N f K Q P C K P S U K e k 1 3 y i u y T I W G k J J / I Z / I l + B h 8 D b 4 F 3 x e l Q a 8 7 8 4 C s R P D j N 3 1 Y / A A = &lt; / l a t e x i t &gt; Z 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E w l C S j 6 v M s a p k 1 l 6 / q Q g W H l O h 5 o = " &gt; A A A D A 3 i c f Z L N b t Q w E M e 9 4 a u E j 7 Z w 5 B K x q o Q 4 r B J U F Y 4 V 5 c A F U U q 3 r W h W K 8 e Z Z K 2 N P 2 R P 2 q 6 s H D l y h Y f g h r j y I L w C T 4 G z G 6 T d F h j J 8 k / / G c t / j y f T F b c Y x z 9 7 w b X r N 2 7 e W r s d 3 r l 7 7 / 7 6 x u a D I 6 t q w 2 D I V K X M S U Y t V F z C E D l W c K I N U J F V c J x N 9 9 r 8 8 R k Y y 5 U 8 x J m G k a C l 5 A V n F L 3 0 / s N 4 e 7 z R j w f x P K K r k H T Q J 1 3 s j z d 7 v 9 J c s V q A R F Z R a 0 + T W O P I U Y O c V d C E 4 V a U 1 h Y 0 Z V N a g i t B C U A z a 8 I l 9 d S j p A L s y M 0 f 0 U R b X s m j Q h m / J E Z z d f m E o 8 L a m c h 8 p a A 4 s Z d z r f i v H E 7 E X 1 O 5 b e 9 a N Z a f c W 0 7 a x c L b 6 v G s X g x c l z q G k G y h e + i r i J U U d v g K O c G G F Y z D 5 Q Z 7 n s S s Q k 1 l K H / h p X b Q d a C I w j f s W V Z U 2 O n X L d q D k V 6 4 N L W e J a 5 g 8 Y 7 e Q W + 5 w b e e O m t B k N R m a c u p a Y U X D Z z S F v 6 X y G 9 + F P o K U w l n O M E l A H h u r 1 x h x 2 E f j q S y 7 N w F Y 6 e D Z K d w c 6 7 7 f 7 u y 2 5 O 1 s g j 8 p g 8 I Q l 5 T n b J a 7 J P h o S R k n w i n 8 m X 4 G P w N f g W f F + U B r 3 u z E O y E s G P 3 3 / 1 / A E = &lt; / l a t e x i t &gt; Z 4</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J B l X c z h z L / t 7 x d W 9 s v p 5 j p q x Z d w (a) Ground truth To provide a visual comparison, we consider a single run and plot the ground truth latent graph (Figure <ref type="figure" target="#fig_10">3a</ref>), the estimate generated by LSCALE-I (Figure <ref type="figure" target="#fig_10">3b</ref>), and the estimate generated by DiscrepancyVAE (Figure <ref type="figure" target="#fig_10">3c</ref>). The differences between the latter and the ground truth graph are shown in these figures by red (added) and dashed (deleted) edges. We refer further comparison between these methods to Appendix E.3.</p><formula xml:id="formula_168">= " &gt; A A A D A 3 i c f Z L N b t Q w E M e 9 4 a u E r x a O X C J W l R C H V Y J K 6 b E q H L g g S u m 2 F c 1 q 5 T i T r L X x h + x J 6 c r K k S N X e A h u i C s P w i v w F D i 7 Q d p t g Z E s / / S f s f z 3 e D J d c Y t x / L M X X L l 6 7 f q N t Z v h r d t 3 7 t 5 b 3 7 h / Z F V t G A y Z q p Q 5 y a i F i k s Y I s c K T r Q B K r I K j r P p i z Z / f A b G c i U P c a Z h J G g p e c E Z R S + 9 e z 9 + N l 7 v x 4 N 4 H t F l S D r o k y 7 2 x x u 9 X 2 m u W C 1 A I q u o t a d J r H H k q E H O K m j C c D N K a w u a s i k t w Z W g B K C Z N e G S e u p R U g F 2 5 O a P a K J N r + R R o Y x f E q O 5 u n z C U W H t T G S + U l C c 2 I u 5 V v x X D i f i r 6 n c t n e t G s v P u L a d t f O F t 1 X j W O y M H J e 6 R p B s 4 b u o q w h V 1 D Y 4 y r k B h t X M A 2 W G + 5 5 E b E I N Z e i / Y e V 2 k L X g C M J 3 b F n W 1 N g p 1 6 2 a Q 5 E e u L Q 1 n m X u o P F O X o L v u Y H X X n q j w V B U 5 o l L q S k F l 8 0 c 0 p b + V 0 j P / x R 6 C l M J H 3 A C y o B</formula><formula xml:id="formula_169">&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g K t t R 8 k G / V K I V a G V / k w j t h 6 2 d F 0 = " &gt; A A A D B H i c f Z L N b t Q w E M e 9 4 a u E r x a O X C J W l R C H V Y K q w r E C D r 0 g 2 q r b V j S r l e N M N t b G H 7 I n p S s r 1 x 6 5 w k N w Q 1 x 5 j 7 4 C T 4 G z G 6 T d F h j J 8 k / / G c t / j y f T F b c Y x 5 e 9 4 M b N W 7 f v r N 0 N 7 9 1 / 8 P D R + s b j I 6 t q w 2 D I V K X M S U Y t V F z C E D l W c K I N U J F V c J x N 3 7 b 5 4 z M w l i t 5 i D M N I 0 E n k h e c U W y l j + M k H K / 3 4 0 E 8 j + g 6 J B 3 0 S R d 7 4 4 3 e r z R X r B Y g k V X U 2 t M k 1 j h y 1 C B n F T R h u B m l t Q V N 2 Z R O w E 1 A C U A z a 8 I l 9 d S j p A L s y M 1 f 0 U S b X s m j Q h m / J E Z z d f m E o 8 L a m c h 8 p a B Y 2 q u 5 V v x X D k v x 1 1 R u 2 7 t W j e V n X N v O 2 v n C 2 6 p x L F 6 P H J e 6 R p B s 4 b u o q w h V 1 H Y 4 y r k B h t X M A 2 W G + 5 5 E r K S G M v T / s H I 7 y F p w B O E 7 t i x r a u y U 6 1 b N o U g P X N o a z z J 3 0 H g n 7 8 D 3 3 M B 7 L 3 3 Q Y C g q 8 8 K l 1 E w E l 8 0 c 0 p b + V 0 j P / x R 6 C l M J n 7 A E Z U C 4 b m / c Y Q f t d C R X Z + E 6 H L 0 c J N u D 7 f 2 t / s 6 b b k 7 W y F P y j D w n C X l F d s g u 2 S N D w k h J P p M v 5 G t w E X w L v g c / F q V B r z v z h K x E 8 P M 3 u S v 8 E g = = &lt; / l a t e x i t &gt; Z 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p 7 i y s E u b c G D j M d o e c 5 z H a o x f P X M = " &gt; A A A D A 3 i c f Z L N b t Q w E M e 9 4 a u E r x a O X C J W l R C H V V J V h W N V O H B B l N J t K 5 r V y n E m W W v j D 9 m T t i s r R 4 5 c 4 S G 4 I a 4 8 C K / A U + D s B m m 3 B U a y / N N / x v L f 4 8 l 0 x S 3 G 8 c 9 e c O 3 6 j Z u 3 1 m 6 H d + 7 e u / 9 g f e P h k V W 1 Y T B k q l L m J K M W K i 5 h i B w r O N E G q M g q O M 6 m L 9 v 8 8 R k Y y 5 U 8 x J m G k a C l 5 A V n F L 3 0 / s N 4 a 7 z e j w f x P K K r k H T Q J 1 3 s j z d 6 v 9 J c s V q A R F Z R a 0 + T W O P I U Y O c V d C E 4 W a U 1 h Y 0 Z V N a g i t B C U A z a 8 I l 9 d S j p A L s y M 0 f 0 U S b X s m j Q h m / J E Z z d f m E o 8 L a m c h 8 p a A 4 s Z d z r f i v H E 7 E X 1 O 5 b e 9 a N Z a f c W 0 7 a x c L b 6 v G s X g x c l z q G k G y h e + i r i J U U d v g K O c G G F Y z D 5 Q Z 7 n s S s Q k 1 l K H / h p X b Q d a C I w j f s W V Z U 2 O n X L d q D k V 6 4 N L W e J a 5 g 8 Y 7 e Q W + 5 w b e e O m t B k N R m W c u p a Y U X D Z z S F v 6 X y G 9 + F P o K U w l n O M E l A H h u r 1 x h x 2 E f j q S y 7 N w F Y 6 2 B s n O Y O f d d n 9 3 r 5 u T N f K Y P C F P S U K e k 1 3 y m u y T I W G k J J / I Z / I l + B h 8 D b 4 F 3 x e l Q a 8 7 8 4 i s R P D j N 3 q 7 + / 8 = &lt; / l a t e x i t &gt; Z 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S W e 7 L 6 m j H H n E b 3 8 D O q N h b 5 j c q 0 0 = " &gt; A A A D A 3 i c f Z L N b t Q w E M e 9 4 a u E r x a O X C J W l R C H V U J R 6 b E q H L g g S u m 2 F c 1 q 5 T i T r L X x h + x J 6 c r K k S N X e A h u i C s P w i v w F D i 7 Q d p t g Z E s / / S f s f z 3 e D J d c Y t x / L M X X L l 6 7 f q N t Z v h r d t 3 7 t 5 b 3 7 h / Z F V t G A y Z q p Q 5 y a i F i k s Y I s c K T r Q B K r I K j r P p i z Z / f A b G c i U P c a Z h J G g p e c E Z R S + 9 e z / e G q / 3 4 0 E 8 j + g y J B 3 0 S R f 7 4 4 3 e r z R X r B Y g k V X U 2 t M k 1 j h y 1 C B n F T R h u B m l t Q V N 2 Z S W 4 E p Q A t D M m n B J P f U o q Q A 7 c v N H N N G m V / K o U M Y v i d F c X T 7 h q L B 2 J j J f K S h O 7 M V c K / 4 r h x P x 1 1 R u 2 7 t W j e V n X N v O 2 v n C 2 6 p x L H Z G j k t d I 0 i 2 8 F 3 U V Y Q q a h s c 5 d w A w 2 r m g T L D f U 8 i N q G G M v T f s H I 7 y F p w B O E 7 t i x r a u y U 6 1 b N o U g P X N o a z z J 3 0 H g n L 8 H 3 3 M B r L 7 3 R Y C g q 8 8 S l 1 J S C y 2 Y O a U v / K 6 T n f w o 9 h a m E D z g B Z U C 4 b m / c Y Q e h n 4 7 k 4 i x c h q O n g 2 R 7 s P 3 2 W X 9 3 r 5 u T N f K Q P C K P S U K e k 1 3 y i u y T I W G k J J / I Z / I l + B h 8 D b 4 F 3 x e l Q a 8 7 8 4 C s R P D j N 3 1 Y / A A = &lt; / l a t e x i t &gt; Z 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E w l C S j 6 v M s a p k 1 l 6 / q Q g W H l O h 5 o = " &gt; A A A D A 3 i c f Z L N b t Q w E M e 9 4 a u E j 7 Z w 5 B K x q o Q 4 r B J U F Y 4 V 5 c A F U U q 3 r W h W K 8 e Z Z K 2 N P 2 R P 2 q 6 s H D l y h Y f g h r j y I L w C T 4 G z G 6 T d F h j J 8 k / / G c t / j y f T F b c Y x z 9 7 w b X r N 2 7 e W r s d 3 r l 7 7 / 7 6 x u a D I 6 t q w 2 D I V K X M S U Y t V F z C E D l W c K I N U J F V c J x N 9 9 r 8 8 R k Y y 5 U 8 x J m G k a C l 5 A V n F L 3 0 / s N 4 e 7 z R j w f x P K K r k H T Q J 1 3 s j z d 7 v 9 J c s V q A R F Z R a 0 + T W O P I U Y O c V d C E 4 V a U 1 h Y 0 Z V N a g i t B C U A z a 8 I l 9 d S j p A L s y M 0 f 0 U R b X s m j Q h m / J E Z z d f m E o 8 L a m c h 8 p a A 4 s Z d z r f i v H E 7 E X 1 O 5 b e 9 a N Z a f c W 0 7 a x c L b 6 v G s X g x c l z q G k G y h e + i r i J U U d v g K O c G G F Y z D 5 Q Z 7 n s S s Q k 1 l K H / h p X b Q d a C I w j f s W V Z U 2 O n X L d q D k V 6 4 N L W e J a 5 g 8 Y 7 e Q W + 5 w b e e O m t B k N R m a c u p a Y U X D Z z S F v 6 X y G 9 + F P o K U w l n O M E l A H h u r 1 x h x 2 E f j q S y 7 N w F Y 6 e D Z K d w c 6 7 7 f 7 u y 2 5 O 1 s g j 8 p g 8 I Q l 5 T n b J a 7 J P h o S R k n w i n 8 m X 4 G P w N f g W f F + U B r 3 u z E O y E s G P 3 3 / 1 / A E = &lt; / l a t e x i t &gt; Z 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J B l X c z h z L / t 7 x d W 9 s v p 5 j p q x Z d w = " &gt; A A A D A 3 i c f Z L N b t Q w E M e 9 4 a u E r x a O X C J W l R C H V Y J K 6 b E q H L g g S u m 2 F c 1 q 5 T i T r L X x h + x J 6 c r K k S N X e A h u i C s P w i v w F D i 7 Q d p t g Z E s / / S f s f z 3 e D J d c Y t x / L M X X L l 6 7 f q N t Z v h r d t 3 7 t 5 b 3 7 h / Z F V t G A y Z q p Q 5 y a i F i k s Y I s c K T r Q B K r I K j r P p i z Z / f A b G c i U P c a Z h J G g p e c E Z R S + 9 e z 9 + N l 7 v x 4 N 4 H t F l S D r o k y 7 2 x x u 9 X 2 m u W C 1 A I q u o t a d J r H H k q E H O K m j C c D N K a w u a s i k t w Z W g B K C Z N e G S e u p R U g F 2 5 O a P a K J N r + R R o Y x f E q O 5 u n z C U W H t T G S + U l C c 2 I u 5 V v x X D i f i r 6 n c t n e t G s v P u L a d t f O F t 1 X j W O y M H J e 6 R p B s 4 b u o q w h V 1 D Y 4 y r k B h t X M A 2 W G + 5 5 E b E I N Z e i / Y e V 2 k L X g C M J 3 b F n W 1 N g p 1 6 2 a Q 5 E e u L Q 1 n m X u o P F O X o L v u Y H X X n q j w V B U 5 o l L q S k F l 8 0 c 0 p b + V 0 j P / x R 6 C l M J H 3 A C y o B w 3 d 6 4 w w 5 C P x 3 J x V m 4 D E d P B 8 n 2 Y P v t V n 9 3 r 5 u T N f K Q P C K P S U K e k 1 3 y i u y T I W G k J J / I Z / I l + B h 8 D b 4 F 3 x e l Q a 8 7 8 4 C s R P D j N 4 K S / A I = &lt; / l a t e x i t &gt; Z 5 (b) LSCALE-I &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g K t t R 8 k G / V K I V a G V / k w j t h 6 2 d F 0 = " &gt; A A A D B H i c f Z L N b t Q w E M e 9 4 a u E r x a O X C J W l R C H V Y K q w r E C D r 0 g 2 q r b V j S r l e N M N t b G H 7 I n p S s r 1 x 6 5 w k N w Q 1 x 5 j 7 4 C T 4 G z G 6 T d F h j J 8 k / / G c t / j y f T F b c Y x 5 e 9 4 M b N W 7 f v r N 0 N 7 9 1 / 8 P D R + s b j I 6 t q w 2 D I V K X M S U Y t V F z C E D l W c K I N U J F V c J x N 3 7 b 5 4 z M w l i t 5 i D M N I 0 E n k h e c U W y l j + M k H K / 3 4 0 E 8 j + g 6 J B 3 0 S R d 7 4 4 3 e r z R X r B Y g k V X U 2 t M k 1 j h y 1 C B n F T R h u B m l t Q V N 2 Z R O w E 1 A C U A z a 8 I l 9 d S j p A L s y M 1 f 0 U S b X s m j Q h m / J E Z z d f m E o 8 L a m c h 8 p a B Y 2 q u 5 V v x X D k v x 1 1 R u 2 7 t W j e V n X N v O 2 v n C 2 6 p x L F 6 P H J e 6 R p B s 4 b u o q w h V 1 H Y 4 y r k B h t X M A 2 W G + 5 5 E r K S G M v T / s H I 7 y F p w B O E 7 t i x r a u y U 6 1 b N o U g P X N o a z z J 3 0 H g n 7 8 D 3 3 M B 7 L 3 3 Q Y C g q 8 8 K l 1 E w E l 8 0 c 0 p b + V 0 j P / x R 6 C l M J n 7 A E Z U C 4 b m / c Y Q f t d C R X Z + E 6 H L 0 c J N u D 7 f 2 t / s 6 b b k 7 W y F P y j D w n C X l F d s g u 2 S N D w k h J P p M v 5 G t w E X w L v g c / F q V B r z v z h K x E 8 P M 3 u S v 8 E g = = &lt; / l a t e x i t &gt; Z 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p 7 i y s E u b c G D j M d o e c 5 z H a o x f P X M = " &gt; A A A D A 3 i c f Z L N b t Q w E M e 9 4 a u E r x a O X C J W l R C H V V J V h W N V O H B B l N J t K 5 r V y n E m W W v j D 9 m T t i s r R 4 5 c 4 S G 4 I a 4 8 C K / A U + D s B m m 3 B U a y / N N / x v L f 4 8 l 0 x S 3 G 8 c 9 e c O 3 6 j Z u 3 1 m 6 H d + 7 e u / 9 g f e P h k V W 1 Y T B k q l L m J K M W K i 5 h i B w r O N E G q M g q O M 6 m L 9 v 8 8 R k Y y 5 U 8 x J m G k a C l 5 A V n F L 3 0 / s N 4 a 7 z e j w f x P K K r k H T Q J 1 3 s j z d 6 v 9 J c s V q A R F Z R a 0 + T W O P I U Y O c V d C E 4 W a U 1 h Y 0 Z V N a g i t B C U A z a 8 I l 9 d S j p A L s y M 0 f 0 U S b X s m j Q h m / J E Z z d f m E o 8 L a m c h 8 p a A 4 s Z d z r f i v H E 7 E X 1 O 5 b e 9 a N Z a f c W 0 7 a x c L b 6 v G s X g x c l z q G k G y h e + i r i J U U d v g K O c G G F Y z D 5 Q Z 7 n s S s Q k 1 l K H / h p X b Q d a C I w j f s W V Z U 2 O n X L d q D k V 6 4 N L W e J a 5 g 8 Y 7 e Q W + 5 w b e e O m t B k N R m W c u p a Y U X D Z z S F v 6 X y G 9 + F P o K U w l n O M E l A H h u r 1 x h x 2 E f j q S y 7 N w F Y 6 2 B s n O Y O f d d n 9 3 r 5 u T N f K Y P C F P S U K e k 1 3 y m u y T I W G k J J / I Z / I l + B h 8 D b 4 F 3 x e l Q a 8 7 8 4 i s R P D j N 3 q 7 + / 8 = &lt; / l a t e x i t &gt; Z 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S W e 7 L 6 m j H H n E b 3 8 D O q N h b 5 j c q 0 0 = " &gt; A A A D A 3 i c f Z L N b t Q w E M e 9 4 a u E r x a O X C J W l R C H V U J R 6 b E q H L g g S u m 2 F c 1 q 5 T i T r L X x h + x J 6 c r K k S N X e A h u i C s P w i v w F D i 7 Q d p t g Z E s / / S f s f z 3 e D J d c Y t x / L M X X L l 6 7 f q N t Z v h r d t 3 7 t 5 b 3 7 h / Z F V t G A y Z q p Q 5 y a i F i k s Y I s c K T r Q B K r I K j r P p i z Z / f A b G c i U P c a Z h J G g p e c E Z R S + 9 e z / e G q / 3 4 0 E 8 j + g y J B 3 0 S R f 7 4 4 3 e r z R X r B Y g k V X U 2 t M k 1 j h y 1 C B n F T R h u B m l t Q V N 2 Z S W 4 E p Q A t D M m n B J P f U o q Q A 7 c v N H N N G m V / K o U M Y v i d F c X T 7 h q L B 2 J j J f K S h O 7 M V c K / 4 r h x P x 1 1 R u 2 7 t W j e V n X N v O 2 v n C 2 6 p x L H Z G j k t d I 0 i 2 8 F 3 U V Y Q q a h s c 5 d w A w 2 r m g T L D f U 8 i N q G G M v T f s H I 7 y F p w B O E 7 t i x r a u y U 6 1 b N o U g P X N o a z z J 3 0 H g n L 8 H 3 3 M B r L 7 3 R Y C g q 8 8 S l 1 J S C y 2 Y O a U v / K 6 T n f w o 9 h a m E D z g B Z U C 4 b m / c Y Q e h n 4 7 k 4 i x c h q O n g 2 R 7 s P 3 2 W X 9 3 r 5 u T N f K Q P C K P S U K e k 1 3 y i u y T I W G k J J / I Z / I l + B h 8 D b 4 F 3 x e l Q a 8 7 8 4 C s R P D j N 3 1 Y / A A = &lt; / l a t e x i t &gt; Z 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E w l C S j 6 v M s a p k 1 l 6 / q Q g W H l O h 5 o = " &gt; A A A D A 3 i c f Z L N b t Q w E M e 9 4 a u E j 7 Z w 5 B K x q o Q 4 r B J U F Y 4 V 5 c A F U U q 3 r W h W K 8 e Z Z K 2 N P 2 R P 2 q 6 s H D l y h Y f g h r j y I L w C T 4 G z G 6 T d F h j J 8 k / / G c t / j y f T F b c Y x z 9 7 w b X r N 2 7 e W r s d 3 r l 7 7 / 7 6 x u a D I 6 t q w 2 D I V K X M S U Y t V F z C E D l W c K I N U J F V c J x N 9 9 r 8 8 R k Y y 5 U 8 x J m G k a C l 5 A V n F L 3 0 / s N 4 e 7 z R j w f x P K K r k H T Q J 1 3 s j z d 7 v 9 J c s V q A R F Z R a 0 + T W O P I U Y O c V d C E 4 V a U 1 h Y 0 Z V N a g i t B C U A z a 8 I l 9 d S j p A L s y M 0 f 0 U R b X s m j Q h m / J E Z z d f m E o 8 L a m c h 8 p a A 4 s Z d z r f i v H E 7 E X 1 O 5 b e 9 a N Z a f c W 0 7 a x c L b 6 v G s X g x c l z q G k G y h e + i r i J U U d v g K O c G G F Y z D 5 Q Z 7 n s S s Q k 1 l K H / h p X b Q d a C I w j f s W V Z U 2 O n X L d q D k V 6 4 N L W e J a 5 g 8 Y 7 e Q W + 5 w b e e O m t B k N R m a c u p a Y U X D Z z S F v 6 X y G 9 + F P o K U w l n O M E l A H h u r 1 x h x 2 E f j q S y 7 N w F Y 6 e D Z K d w c 6 7 7 f 7 u y 2 5 O 1 s g j 8 p g 8 I Q l 5 T n b J a 7 J P h o S R k n w i n 8 m X 4 G P w N f g W f F + U B r 3 u z E O y E s G P 3 3 / 1 / A E = &lt; / l a t e x i t &gt; Z 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J B l X c z h z L / t 7 x d W 9 s v p 5 j p q x Z d w = " &gt; A A A D A 3 i c f Z L N b t Q w E M e 9 4 a u E r x a O X C J W l R C H V Y J K 6 b E q H L g g S u m 2 F c 1 q 5 T i T r L X x h + x J 6 c r K k S N X e A h u i C s P w i v w F D i 7 Q d p t g Z E s / / S f s f z 3 e D J d c Y t x / L M X X L l 6 7 f q N t Z v h r d t 3 7 t 5 b 3 7 h / Z F V t G A y Z q p Q 5 y a i F i k s Y I s c K T r Q B K r I K j r P p i z Z / f A b G c i U P c a Z h J G g p e c E Z R S + 9 e z 9 + N l 7 v x 4 N 4 H t F l S D r o k y 7 2 x x u 9 X 2 m u W C 1 A I q u o t a d J r H H k q E H O K m j C c D N K a w u a s i k t w Z W g B K C Z N e G S e u p R U g F 2 5 O a P a K J N r + R R o Y x f E q O 5 u n z C U W H t T G S + U l C c 2 I u 5 V v x X D i f i r 6 n c t n e t G s v P u L a d t f O F t 1 X j W O y M H J e 6 R p B s 4 b u o q w h V 1 D Y 4 y r k B h t X M A 2 W G + 5 5 E b E I N Z e i / Y e V 2 k L X g C M J 3 b F n W 1 N g p 1 6 2 a Q 5 E e u L Q 1 n m X u o P F O X o L v u Y H X X n q j w V B U 5 o l L q S k F l 8 0 c 0 p b + V 0 j P / x R 6 C l M J H 3 A C y o B w 3 d 6 4 w w 5 C P x 3 J x V m 4 D E d P B 8 n 2 Y P v t V n 9 3 r 5 u T N f K Q P C K P S U K e k 1 3 y i u y T I W G k J J / I Z / I l + B h 8 D b 4 F 3 x e l Q a 8 7 8 4 C s R P D j N 4 K S / A I = &lt; / l a t e x i t &gt; Z 5 (c) DiscrepancyVAE</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.3">Comparison with DiscrepancyVAE on biological data</head><p>In this section, we apply our algorithms to the Perturb-seq dataset of <ref type="bibr" target="#b37">Norman et al. (2019)</ref>, which is also used in <ref type="bibr" target="#b59">Zhang et al. (2023)</ref>. Following the same pre-processing steps, there are 8,907 unperturbed and 99,590 perturbed cells. Each cell (sample) is denoted by a sparse vector of dimension 5000, which represents the expression levels of select genes. The perturbed cells are generated by CRISPR activation <ref type="bibr" target="#b9">(Gilbert et al., 2014)</ref> on one or two target genes out of 105 genes. Hence, this is modeled by a 5000-dimensional observable variable X and a 105-dimensional latent variable Z representing the perturbed genes. In this dataset, the ground truth latent graph and variables are unknown, with the exception of limited gene interactions that have been experimentally verified <ref type="bibr" target="#b37">(Norman et al., 2019)</ref>. For graph estimation, instead of working with a latent dimension of 105, <ref type="bibr" target="#b59">Zhang et al. (2023)</ref> first learns groups of perturbation targets subject to a regularity constraint and then learns a latent graph among these groups. Since the methodology of LSCALE-I does not naturally &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P 8  c Z e k 1 C h 8 J u U r + 6 M 5 K H 7 1 h x F J Q i o 0 4 V i p n u v E 2 s u w 1 I x w O r X 7 i a I x J h M 8 o j 1 D B Q 6 p 8 r L 5 q V N U N s o Q B Z E 0 J T S a q 7 8 n M h w q l Y a + 6 Q y x H q t V b y b + 5 / U S H d S 9 j I k 4 0 V S Q x a I g 4 U h H a P Y 3 G j J J i e a p I Z h I Z m 5 F Z I w l J t q k Y 5 s Q 3 N W X 1 0 m 7 W n F r l d q N S a M O C + T h F M 7 g H F y 4 g A Z c Q x N a Q G A E j / A M L x a 3 n q x X 6 2 3 R m r O W M y f w B 9 b 7 D 2 V 5 k B 8 = &lt; / l a t e x i t &gt; Z 2</p><formula xml:id="formula_170">t d O D j E + C c d 0 s g U f y Q H L Z Q F 3 q g = " &gt; A A A B 6 n i c b V D L S g N B E O z 1 G V e j U Y 9 e B k P A U 9 g V 1 B w D g n i M a B 6 Y L G F 2 M p s M m Z l d Z m a F s O Q T v H h Q x K v 4 I X 6 C N / / G y e O g i Q U N R V U 3 3 V 1 h w p k 2 n v f t r K y u r W 9 s 5 r b c 7 Z 3 8 7 l 5 h / 6 C h 4 1 Q R W i c x j 1 U r x J p y J m n d M M N p K 1 E U i 5 D T Z j i 8 n P j N B 6 o 0 i + W d G S U 0 E L g v W c Q I N l a 6 v e / 6 3 U L R K 3 t T o G X i z 0 m x m v 9 M S 1 f u R 6 1 b + O r 0 Y p I K K g 3 h W O u 2 7 y U m y L A y j H A 6 d j u p p g k m Q 9 y n b U s l F l Q H 2 f T U M S p Z p Y e i W N m S B k 3 V 3 x M Z F l q P R G g 7 B T Y D v e h N x P + 8 d m q i S p A x m a S G S j J b F K U c m R h N / k Y 9 p i g x f G Q J J o r Z W x E Z Y I W J s e m 4 N g R / 8 e V l 0 j g t + + f l s x u b R g V m y M E R H M M J + H A B V b i G G t S B Q B 8 e 4 R l e H O 4 8 O a / O 2 6 x 1 x Z n P H M I f O O 8 / Y 6 O Q H Q = = &lt; / l a t e x i t &gt; Z 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 5 g R v j l E Z L o R B C 5 x H y v j 1 8 W W K a c = " &gt; A A A B 6 n i c b V D L S g N B E O z 1 G V e j U Y 9 e B k P A U 9 j 1 E H M M C O I x o n l g s o T Z y W w y Z H Z 2 m Y c Q l n y C F w + K e B U / x E / w 5 t 8 4 e R w 0 s a C h q O q m u y t M O V P a 8 7 6 d t f W N z a 3 t 3 I 6 7 u 5 f f P y g c H j V V Y i S h D Z L w R L Z D r C h n g j Y 0 0 5 y 2 U 0 l x H H L a C k e X U 7 / 1 Q K V i i b j T 4 5 Q G M R 4 I F j G C t Z V u 7 3 u V X q H o l b 0 Z 0 C r x F 6 R Y y 3 + a 0 p X 7 U e 8 V v r r 9 h J i Y C k 0 4 V q r j e 6 k O M i w 1 I 5 x O 3 K 5 R N M V k h A e 0 Y 6 n A M V V B N j t 1 g k p W 6 a M o k b a E R j P 1 9 0 S G Y 6 X G c W g 7 Y 6 y H a t m b i v 9 5 H a O j a p A x k R p N B Z k v i g x H O k H T v 1 G f S U o 0 H 1 u C i W T 2 V k S G W G K i b T q u D c F f f n m V N M / L f q V c u b F p V G G O H J z A K Z y B D x d Q g 2 u o Q w M I D O A R n u H F 4 c 6 T 8 + q 8 z V v X n M X M M f y B 8 / 4 D a 4 m Q I w = = &lt; / l a t e x i t &gt; Z 6 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d P 4 8 8 / T F f Z e h F A f x 2 F H Q l T l 8 S I k = " &gt; A A A B 6 n i c b V D L S s N A F L 2 p r x q t V l 2 6 G S w F V y X p o n Z Z E M R l R f v A N p T J d N I O n U z C z E Q I o Z / g x o U i b s U P 8 R P c + T d O H w t t P X D h c M 6 9 3 H u P H 3 O m t O N 8 W 7 m N z a 3 t n f y u v b d f O D g s H h 2 3 V Z R I Q l s k 4 p H s + l h R z g R t a a Y 5 7 c a S 4 t D n t O N P L m d + 5 4 F K x S J x p 9 O Y e i E e C R Y w g r W R b u 8 H 1 U G x 5 F S c O d A 6 c Z e k 1 C h 8 J u U r + 6 M 5 K H 7 1 h x F J Q i o 0 4 V i p n u v E 2 s u w 1 I x w O r X 7 i a I x J h M 8 o j 1 D B Q 6 p 8 r L 5 q V N U N s o Q B Z E 0 J T S a q 7 8 n M h w q l Y a + 6 Q y x H q t V b y b + 5 / U S H d S 9 j I k 4 0 V S Q x a I g 4 U h H a P Y 3 G j J J i e a p I Z h I Z m 5 F Z I w l J t q k Y 5 s Q 3 N W X 1 0 m 7 W n F r l d q N S a M O C + T h F M 7 g H F y 4 g A Z c Q x N a Q G A E j / A M L x a 3 n q x X 6 2 3 R m r O W M y f w B 9 b 7 D 2 V 5 k B 8 = &lt; / l a t e x i t &gt; Z 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p C I b s 6 0 E b j Y z 3 J h 8 T Z I 0 E v X + h / A = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r x q t V j 1 6 W S w F T y U R q j 0 W B P F Y 0 X 5 g G 8 p m u 2 m X b j Z h d y O U 0 J / g x Y M i X s U f 4 k / w 5 r 9 x 0 / a g r Q 8 G H u / N M D P P j z l T 2 n G + r d z a + s b m V n 7 b 3 t k t 7 O 0 X D w 5 b K k o k o U 0 S 8 U h 2 f K w o Z 4 I 2 N d O c d m J J c e h z 2 v b H l 5 n f f q B S s U j c 6 U l M v R A P B Q s Y w d p I t / f 9 a r 9 Y c i r O D G i V u A t S q h c + k / K V / d H o F 7 9 6 g 4 g k I R W a c K x U 1 3 V i 7 a V Y a k Y 4 n d q 9 R N E Y k z E e 0 q 6 h A o d U e e n s 1 C k q G 2 W A g k i a E h r N 1 N 8 T K Q 6 V m o S + 6 Q y x H q l l L x P / 8 7 q J D m p e y k S c a C r I f F G Q c K Q j l P 2 N B k x S o v n E E E w k M 7 c i M s I S E 2 3 S s U 0 I 7 v L L q 6 R 1 V n H P K 9 U b k 0 Y N 5 s j D M Z z A K b h w A X W 4 h g Y 0 g c A Q H u E Z X i x u P V m v 1 t u 8 N W c t Z o 7 g D 6 z 3 H 2 m z k C E = &lt; / l a t e x i t &gt; Z 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C z w 8 g B d C b z B M 6 1 4 A a t q U Z + F R K 0 U = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r x q t V j 1 6 W S w F T y V R 1 B 4 L g n i s a D + w D W W z 3 b R L N 5 u w u x F K 6 E / w 4 k E R r + I P 8 S d 4 8 9 + 4 a X v Q 1 g c D j / d m m J n n x 5 w p 7 T j f V m 5 l d W 1 9 I 7 9 p b 2 0 X d n a L e / t N F S W S 0 A a J e C T b P l a U M 0 E b m m l O 2 7 G k O P Q 5 b f m j y 8 x v P V C p W C T u 9 D i m X o g H g g W M Y G 2 k 2 / v e a a 9 Y c i r O F G i Z u H N S q h U + k / K V / V H v F b + 6 / Y g k I R W a c K x U x 3 V i 7 a V Y a k Y 4 n d j d R N E Y k x E e 0 I 6 h A o d U e e n 0 1 A k q G 6 W P g k i a E h p N 1 d 8 T K Q 6 V G o e + 6 Q y x H q p F L x P / 8 z q J D q p e y k S c a C r I b F G Q c K Q j l P 2 N + k x S o v n Y E E w k M 7 c i M s Q S E 2 3 S s U 0 I 7 u L L y 6 R 5 U n H P K 2 c 3 J o 0 q z J C H Q z i C Y 3 D h A m p w D X V o A I E B P M I z v F j c e r J e r b d Z a 8 6 a z x z A H 1 j v P 2 a r k B 8 = &lt; / l a t e x i t &gt; Z 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G v Z d E 7 1 X K t 3 N Z T U 0 9 E P R 6 2 u 3 i P c = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r x q t V j 1 6 W S w F T y U R q T 0 W B P F Y 0 X 5 g G 8 p m u 2 m X b j Z h d y O U 0 J / g x Y M i X s U f 4 k / w 5 r 9 x 0 / a g r Q 8 G H u / N M D P P j z l T 2 n G + r d z a + s b m V n 7 b 3 t k t 7 O 0 X D w 5 b K k o k o U 0 S 8 U h 2 f K w o Z 4 I 2 N d O c d m J J c e h z 2 v b H l 5 n f f q B S s U j c 6 U l M v R A P B Q s Y w d p I t / f 9 8 3 6 x 5 F S c G d A q c R e k V C 9 8 J u U r + 6 P R L 3 7 1 B h F J Q i o 0 4 V i p r u v E 2 k u x 1 I x w O r V 7 i a I x J m M 8 p F 1 D B Q 6 p 8 t L Z q V N U N s o A B Z E 0 J T S a q b 8 n U h w q N Q l 9 0 x l i P V L L X i b + 5 3 U T H d S 8 l I k 4 0 V S Q + a I g 4 U h H K P s b D Z i k R P O J I Z h I Z m 5 F Z I Q l J t q k Y 5 s Q 3 O W X V 0 n r r O J W K 9 U b k 0 Y N 5 s j D M Z z A K b h w A X W 4 h g Y 0 g c A Q H u E Z X i x u P V m v 1 t u 8 N W c t Z o 7 g D 6 z 3 H 2 i B k C E = &lt; / l a t e x i t &gt; Z 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i l h 9 h h I e + o O Y I + I C I s W 1 + d 5 X P 5 8 = " &gt; A A A B 6 n i c b V D L S g N B E O y N r 7 g a j X r 0 M h g C n s K u B 8 0 x I I j H i O a B y R J m J 7 P J k N n Z Z R 5 C W P I J X j w o 4 l X 8 E D / B m 3 / j 5 H H Q x I K G o q q b 7 q 4 w 5 U x p z / t 2 c m v r G 5 t b + W 1 3 Z 7 e w t 1 8 8 O G y q x E h C G y T h i W y H W F H O B G 1 o p j l t p 5 L i O O S 0 F Y 4 u p 3 7 r g U r F E n G n x y k N Y j w Q L G I E a y v d 3 v e 8 X r H k V b w Z 0 C r x F 6 R U K 3 y a 8 p X 7 U e 8 V v 7 r 9 h J i Y C k 0 4 V q r j e 6 k O M i w 1 I 5 x O 3 K 5 R N M V k h A e 0 Y 6 n A M V V B N j t 1 g s p W 6 a M o k b a E R j P 1 9 0 S G Y 6 X G c W g 7 Y 6 y H a t m b i v 9 5 H a O j a p A x k R p N B Z k v i g x H O k H T v 1 G f S U o 0 H 1 u C i W T 2 V k S G W G K i b T q u D c F f f n m V N M 8 q / n n l / M a m U Y U 5 8 n A M J 3 A K P l x A D a 6 h D g 0 g M I B H e I Y X h z t P z q v z N m / N O Y u Z I / g D 5 / 0 H Y n G Q H Q = = &lt; / l a t e x i t &gt; Z 0 (a) LSCALE-I &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P 8 t d O D j E + C c d 0 s g U f y Q H L Z Q F 3 q g = " &gt; A A A B 6 n i c b V D L S g N B E O z 1 G V e j U Y 9 e B k P A U 9 g V 1 B w D g n i M a B 6 Y L G F 2 M p s M m Z l d Z m a F s O Q T v H h Q x K v 4 I X 6 C N / / G y e O g i Q U N R V U 3 3 V 1 h w p k 2 n v f t</formula><formula xml:id="formula_171">g c H j V V Y i S h D Z L w R L Z D r C h n g j Y 0 0 5 y 2 U 0 l x H H L a C k e X U 7 / 1 Q K V i i b j T 4 5 Q G M R 4 I F j G C t Z V u 7 3 u V X q H o l b 0 Z 0 C r x F 6 R Y y 3 + a 0 p X 7 U e 8 V v r r 9 h J i Y C k 0 4 V q r j e 6 k O M i w 1 I 5 x O 3 K 5 R N M V k h A e 0 Y 6 n A M V V B N j t 1 g k p W 6 a M o k b a E R j P 1 9 0 S G Y 6 X G c W g 7 Y 6 y H a t m b i v 9 5 H a O j a p A x k R p N B Z k v i g x H O k H T v 1 G f S U o 0 H 1 u C i W T 2 V k S G W G K i b T q u D c F f f n m V N M / L f q V c u b F p V G G O H J z A K Z y B D x d Q g 2 u o Q w M I D O A R n u H F 4 c 6 T 8 + q 8 z V v X n M X M M f</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p C I b s 6 0 E b j Y z 3 J h 8 T Z I 0 E v X + h / A = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r x q t V j 1 6 W S w F T y U R q j 0 W B P F Y 0 X 5 g G 8 p m u 2 m X b j Z h d y O U 0 J / g x Y M i X s U f 4 k / w 5 r 9 x 0 / a g r Q 8 G H u / N M D P P j z l T 2 n G + r d z a + s b m V n 7 b 3 t k  </p><formula xml:id="formula_172">t 7 O 0 X D w 5 b K k o k o U 0 S 8 U h 2 f K w o Z 4 I 2 N d O c d m J J c e h z 2 v b H l 5 n f f q B S s U j c 6 U l M v R A P B Q s Y w d p I t / f 9 a r 9 Y c i r O D G i V u A t S q h c + k / K V / d H o F 7 9 6 g 4 g k I R W a c K x U 1 3 V i 7 a V Y a k Y 4 n d q 9 R N E Y k z E e 0 q 6 h A o d U e e n s 1 C k q G 2 W A g k i a E h r N 1 N 8 T K Q 6 V m o S + 6 Q y x H q l l L x P / 8 7 q J D m p e y k S c a C r I f F G Q c K Q j l P 2 N B k x S o v n E E E w k M 7 c i M s I S E 2 3 S s U 0 I 7 v L L q 6 R 1 V n H P K 9 U b k 0 Y N 5 s j D M Z z A K b h w A X W 4 h g Y 0 g c A Q H u E Z X i x u P V m v 1 t u 8 N W c t Z</formula><formula xml:id="formula_173">v P V C p W C T u 9 D i m X o g H g g W M Y G 2 k 2 / v e a a 9 Y c i r O F G i Z u H N S q h U + k / K V / V H v F b + 6 / Y g k I R W a c K x U x 3 V i 7 a V Y a k Y 4 n d j d R N E Y k x E e 0 I 6 h A o d U e e n 0 1 A k q G 6 W P g k i a E h p N 1 d 8 T K Q 6 V G o e + 6 Q y x H q p F L x P / 8 z q J D q p e y k S c a C r I b F G Q c K Q j l P 2 N + k x S o v n Y E E w k M 7 c i M s Q S E 2 3 S s U 0 I 7 u L L y 6 R 5 U n H P K 2 c 3 J o 0 q z J C H Q z i C Y 3 D h A m</formula><formula xml:id="formula_174">B h F J Q i o 0 4 V i p r u v E 2 k u x 1 I x w O r V 7 i a I x J m M 8 p F 1 D B Q 6 p 8 t L Z q V N U N s o A B Z E 0 J T S a q b 8 n U h w q N Q l 9 0 x l i P V L L X i b + 5 3 U T H d S 8 l I k 4 0 V S Q + a I g 4 U h H K P s b D Z i k R P O J I Z h I Z m 5 F Z I Q l J t q k Y 5 s Q 3 O W X V 0 n r r O J W K 9 U b k 0 Y N 5 s j D M Z z A K b h w A X W 4 h g Y 0 g c A Q H u E Z X i x u P V m v 1 t u 8 N W c t Z o 7 g D 6 z 3 H 2 i B k C E = &lt; / l a t e x i t &gt; Z 4</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i l h 9 h h I e + o O Y I + I C I s W 1 + d 5 X P 5 8 = " &gt; A A A B 6 n i c b V D L S g N B E O y N r 7 g a j X r 0 M h g C n s K u B 8 0 x I I j H i O a B y R J m J 7 P J k N n Z Z R 5 C W P I J X j w o 4 l X 8 E D / B m 3 / j 5 H H Q x I K G o q q b 7 q 4 w 5 U x p z / t 2 c m v r G 5 t b + W 1 3 Z 7 e w t 1 8 8 O G y q extend to learning such groups of latent nodes, we use the perturbation groupings reported by <ref type="bibr" target="#b59">Zhang et al. (2023)</ref> as our super-nodes and run LSCALE-I to learn a latent graph over them. We list representative perturbation targets from each super-node in Table <ref type="table" target="#tab_16">12</ref>.</p><formula xml:id="formula_175">x E h C G y T h i W y H W F H O B G 1 o p j l t p 5 L i O O S 0 F Y 4 u p 3 7 r g U r F E n G n x y k N Y j w Q L G I E a y v d 3 v e 8 X r H k V b w Z 0 C r x F 6 R U K 3 y a 8 p X 7 U e 8 V v 7 r 9 h J i Y C k 0 4 V q r j e 6 k O M i w 1 I 5 x O 3 K 5 R N M V k h A e 0 Y 6 n A M V V B N j t 1 g s p W 6 a M o k b a E R j P 1 9 0 S G Y 6 X G c W g 7 Y 6 y H a t m b i v 9 5 H a O j a p A x k R p N B Z k v i g x H O k H T v 1 G f S U o 0 H 1 u C i W T 2 V k S G W G K i b T q u D c F f f n m V N M 8 q / n n l / M a m U Y U 5 8 n A M J 3 A K P l x A D a 6 h D g 0 g M I B H e I Y X h z t P z q v z N m / N O Y u Z I / g D 5 / 0 H Y n G Q H Q = = &lt; / l a t e x i t &gt; Z 0 (b) DiscrepancyVAE</formula><p>The most important observation is that LSCALE-I and DiscrepancyVAE both estimate a causal edge from DUSP9 to the MAPK1/ETS2 group, which is a gene activation relation that is demonstrated experimentally <ref type="bibr" target="#b37">(Norman et al., 2019)</ref>. Due to the lack of ground truth for other graph edges, comparing the algorithms for recovering other edges is not informative. Nevertheless, we provide the estimated latent graph from LSCALE-I (Figure <ref type="figure" target="#fig_8">4a</ref>) and DiscrepancyVAE (Figure <ref type="figure" target="#fig_8">4b</ref>) for completeness. This verifies that our score-based methodology can recover causal genomics relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Intervention Extrapolation on Biological Data</head><p>In Section 6.4, we have demonstrated that it is possible to sample from unseen combinations of given interventions via only learning the score functions of observed variables. We verify this on the Perturb-seq data discussed in Section 7.3 through the following steps: 1. Collect observed variables' data from observational distribution p X , two single-node interventions p 1 X and p 2 X , and double-node intervention p {1,2} x .</p><p>2. Compute score functions s X , s 1 X , and s 2 X . 3. Perform score function extrapolation for double-node intervention via (100):</p><p>s{1,2}</p><formula xml:id="formula_176">X = s X + s 2</formula><p>Xs X . 4. Single-node setting: Generate samples from s 1 X and s 2 X using Langevin dynamics <ref type="bibr" target="#b55">(Welling and Teh, 2011)</ref>, compare them to the original samples from p 1 X and p 2 X . 5. Double-node intervention extrapolation: Generate samples from s{1,2} X using Langevin dynamics, compare them to the original samples from p {1,2} X . We note that <ref type="bibr" target="#b59">Zhang et al. (2023)</ref> also report experiments on the same dataset for the single-node data generation and double-node intervention extrapolation settings. However, as discussed in Section 6.4, they perform these tasks using the VAE model for CRL, whereas our approach shows that learning the latent causal representations is unnecessary. Similarly to <ref type="bibr" target="#b59">Zhang et al. (2023)</ref>, we use the maximum mean discrepancy (MMD) <ref type="bibr" target="#b10">(Gretton et al., 2012)</ref> to measure performance quantitatively. In Table <ref type="table" target="#tab_7">13</ref>, we show that our approach performs at least as well as DiscrepancyVAE on the Perturb-seq dataset. Finally, in Appendix E.4, we illustrate the simulated interventional distributions via UMAP clustering, which visually supports the performance implied by Table <ref type="table" target="#tab_7">13</ref>. These results validate the theoretical analysis that we do not need to learn latent variables to perform intervention extrapolation.</p><p>Table <ref type="table" target="#tab_7">13</ref>: MMD evaluation for single-node and double-node intervention extrapolation of our score-based approach and DiscrepancyVAE <ref type="bibr" target="#b59">(Zhang et al., 2023)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Experiments on Image Data</head><p>In this section, we perform experiments on CRL where the general transformation is image rendering, a highly nonlinear transformation, by applying the GSCALE-I algorithm to synthetic image data.</p><p>Image data generation. For image-based experiments, we follow the setup of the closely related studies in <ref type="bibr" target="#b2">(Ahuja et al., 2023;</ref><ref type="bibr" target="#b7">Buchholz et al., 2023)</ref>. Specifically, we consider images of the form in Figure <ref type="figure">6</ref>, which are generated as follows. The pairs of latent variables (Z 2i-1 , Z 2i ) describe the coordinates of the i-th ball's center in a 64 × 64 × 3 RGB image. We use two and three balls in our experiments, which corresponds to n ∈ {4, 6} latent variables. We sample the latent graph G from Erdős-Rényi model for n nodes and set the expected number of edges to 2n. Given this graph G, we adopt a truncated linear Gaussian latent causal model. The details of the linear model are the same as the Section 7.1, with the addition of a second set of interventions represented by the causal mechanisms qi , by setting Z i = Ñi , where Ñi ∼ N (0,</p><formula xml:id="formula_177">σ 2 i 4</formula><p>). Latent variables Z are sampled from the distribution defined by this linear model, but samples with any coordinate absolute value greater than 1 are discarded to truncate the distribution in the box [-1, 1] n . For each latent sample Z, we render an image of the balls centered at the specified coordinates (shifted and scaled to fit entirely within rendered image boundaries) with a radius of 8 pixels and use these as our observed data X. Balls are color-coded, filled with three different colors, and the background is white. For both two and three balls, we generate 5 graphs and 10000 samples from each graph under each environment.</p><p>Candidate encoder and training. For learning the latent variables, we construct a twostep autoencoder, depicted in Figure <ref type="figure">5</ref>. Specifically, in step one, we train an autoencoder on the observational image dataset with a bottleneck dimension of 64 using only a reconstruction objective. In step two, we train another autoencoder on the 64-dimensional output of the first encoder with the bottleneck dimension of n (the latent dimension) using both reconstruction loss and the score-based loss described earlier in Section 7.2. Architecture details and training to be perfect. For instance, the samples in Figure <ref type="figure">6</ref> are from a run with 3 balls with an MCC of 0.98. Conversely, for the runs with non-convergent reconstruction loss in either of the autoencoder training steps, latent variable recovery also struggled. Hence, as long as we can ensure reasonable reconstruction performance when we are minimizing the score-based loss, our algorithm manages to disentangle the true causal variables embedded in the latent domain. We report the results from related work in Table <ref type="table" target="#tab_18">14</ref> and discuss them as follows.</p><p>Overall, we achieve a better performance than that of <ref type="bibr" target="#b2">Ahuja et al. (2023)</ref>, and slightly weaker than that of <ref type="bibr" target="#b7">Buchholz et al. (2023)</ref> for linear SCMs. This can perhaps be explained by the versatility of our approach, which works for general SCMs, whereas the algorithm of <ref type="bibr" target="#b7">Buchholz et al. (2023)</ref> is designed to work exclusively for linear SCMs, both theoretically and due to its loss function design. We demonstrate in Table <ref type="table" target="#tab_18">14</ref> that our algorithm performs even better when the latent SCM is nonlinear (specifically, quadratic) than when it is linear, thereby verifying its versatility. Finally, we also test our algorithm when using one intervention per node, using the score differences between interventional and observational environments instead of two interventional environments. While two interventions per node are required for theoretical guarantees, our experiments show that using one intervention per node generally leads to competitive performance, as illustrated in Table <ref type="table" target="#tab_18">14</ref>.</p><p>Gap between the theory and practice in CRL. We highlight that the nature of the results presented in this paper is theoretical. We have established unknown identifiability guarantees in several settings. Furthermore, we have designed algorithms by defining a differentiable loss function whose global optima achieve identifiability for general CRL and established that the algorithms generate provably correct representations. The experiments presented on various synthesized and real datasets demonstrate the potential of the scorebased framework for complex nonlinear transforms, such as image rendering. We achieve this goal by showing reasonable performance in latent variable recovery. That being said, the considered image dataset and other synthetic datasets are still relatively simple compared to real-world problems, and a gap remains between theoretical guarantees and practical applications. In other words, the theory developed is only a necessary step for guaranteed practical performance. From our perspective, closing this gap and obtaining sufficient conditions for practical performance requires improvements in two aspects. First, using better score difference estimators (which we empirically investigate in Section 7.6) would enable a more effective application of our score-based CRL framework. Second, designing more suitable architectures for leveraging the invariance properties of causal models can help scale up CRL applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Sensitivity to Score Estimation Noise</head><p>As discussed earlier, score estimators can be modularly incorporated into our algorithm.</p><p>For cases where p X is not amenable to parameter estimation, we can use the noisy score estimates generated by SSM-VR as achievable baselines, as presented in this section. In this subsection, we evaluate the potential performance improvement that can be achieved as the score estimates become more accurate. To this end, we test the LSCALE-I algorithm under varying levels of score estimation noise. Specifically, we run the LSCALE-I algorithm using the scores generated by the following model:</p><formula xml:id="formula_178">ŝX (x; σ 2 ) = s X (x) • 1 + Ξ , where Ξ ∼ N (0, σ 2 • I d×d ) . (<label>112</label></formula><formula xml:id="formula_179">)</formula><p>We consider hard interventions on graph size of n = 5, set d = 25, and vary the value of σ 2 within [10 -4 , 10 -2 ]. We repeat the experiments 100 times with n s = 10000 samples from each environment. We plot mean normalized Z error, defined as</p><formula xml:id="formula_180">ℓ norm (Z, Ẑ) ≜ ∥Z -Ẑ∥ 2 ∥Z∥ 2 . (<label>113</label></formula><formula xml:id="formula_181">)</formula><p>and mean SHD in Figure <ref type="figure" target="#fig_9">7</ref> with respect to the signal-to-noise ratio (SNR). It is clear that when the score estimation error is small, indicated by a high SNR value, the LSCALE-I algorithm demonstrates a strong performance in recovering both latent causal variables and the latent graph. We also note that the baseline results with noisy scores computed via SSM-VR in Table <ref type="table" target="#tab_9">5</ref> yield similar success at graph recovery at the SNR of approximately 25 dB and latent recovery at the SRM of approximately 8 dB. The curves also confirm our observations in Section 7.1 and Section 7.2 that graph recovery is more sensitive to score estimation errors. The trend of the curves in Figure <ref type="figure" target="#fig_9">7</ref> indicates that our algorithm would greatly benefit from a better score estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Discussion and Concluding Remarks</head><p>In this paper, we have proposed a score function-based CRL framework that uses stochastic interventions to learn latent causal representations and the latent causal graph underlying them. In this framework, by uncovering novel connections between score functions and CRL, we have established identifiability results for linear and general transformations without restricting the latent causal models, and designed LSCALE-I and GSCALE-I algorithms that achieve these identifiability guarantees. There are several exciting directions for future work. As discussed in Section 3.4 after defining the identifiability objectives, and emphasized when presenting the corresponding results through the paper, our results are tight given a complete set of atomic interventions. A natural direction for future work is relaxing the atomic intervention requirement. In this aspect, the paper <ref type="bibr" target="#b49">(Varıcı et al., 2024)</ref> extends the score-based framework for linear transformations into the setting of unknown multi-node interventions by using combinations of multi-node score differences. Given a sufficiently diverse multi-node intervention set, similar identifiability results to those in Section 5 are shown. That being said, establishing the necessary conditions for multi-node interventions and the case of general transformations remains an open problem. Next, we note that our LSCALE-I algorithm for linear transformations is agnostic to the intervention type and latent causal model, meaning that it can be used for both soft and hard interventions and different causal models. Designing similar universal CRL algorithms that can handle general transformations under different sizes and types of interventional environments is the ultimate goal for studying the identifiability of CRL from interventions. A missing component of existing CRL literature is the finite-sample analysis. Probabilistic identifiability results for a given number of interventional data samples can be useful, especially in applications where performing interventions is costly. In this direction, the recent paper <ref type="bibr" target="#b0">(Acartürk et al., 2024)</ref> establishes finite sample guarantees for CRL under linear transformations. Extending such analysis to general transformations can provide insights for bridging the gap between theory and practice in CRL. We start by providing the following facts that will be used repeatedly in the proofs.</p><p>Proposition 2 Consider two continuous functions f, g : R n → R with full support. Then, for any α &gt; 0,</p><formula xml:id="formula_182">∃z ∈ R n f (z) ̸ = g(z) ⇐⇒ E f (Z) -g(Z) α ̸ = 0 .<label>(114)</label></formula><p>Specifically, for α = 1, we have</p><formula xml:id="formula_183">∃z ∈ R n f (z) ̸ = g(z) ⇐⇒ E f (Z) -g(Z) ̸ = 0 . (<label>115</label></formula><formula xml:id="formula_184">) Proof: If there exists z ∈ R n such that f (z) ̸ = g(z), then f (z) -g(z) is nonzero over a nonzero-measure set due to continuity. Then, E |f (Z) -g(Z)| α ̸ = 0 since p (pdf of Z) has full support. On the other direction, if f (z) = g(z) for all z ∈ R n , then E |f (Z) -g(Z)| α = 0. This means that E |f (Z)-g(Z)| α ̸ = 0 implies that there exists z ∈ R n such that f (z) ̸ = g(z).</formula><p>A.1 Proof of Lemma 1</p><p>Our score-based methodology builds on the changes in score functions under interventions.</p><p>For proving Lemma 1, we start by showing that E s(Z)s m (Z) i ̸ = 0 =⇒ i ∈ pa(I m ), which holds true regardless of the causal model and the intervention type.</p><p>Proof of E s(Z)s m (Z) i ̸ = 0 =⇒ i ∈ pa(I m ): Let ℓ denote the node intervened in E m , i.e., I m = ℓ. Recalling ( <ref type="formula" target="#formula_31">18</ref>) and ( <ref type="formula" target="#formula_32">19</ref>), the latent scores s(z) and s m (z) are decomposed as</p><formula xml:id="formula_185">s(z) = ∇ z log p ℓ (z ℓ | z pa(ℓ) ) + i̸ =ℓ ∇ z log p i (z i | z pa(i) ) ,<label>(116)</label></formula><p>and</p><formula xml:id="formula_186">s m (z) = ∇ z log q ℓ (z ℓ | z pa(ℓ) ) + i̸ =ℓ ∇ z log p i (z i | z pa(i) ) .<label>(117)</label></formula><p>Hence, s(z) and s m (z) differ in only the causal mechanism of node ℓ. Next, we check the derivatives of p ℓ (z ℓ | z pa(ℓ) ) and q ℓ (z ℓ | z pa(ℓ) ) in their i-th coordinates. Note that these two depend on Z only through {Z j : j ∈ pa(ℓ)}. Therefore, if i / ∈ pa(ℓ),</p><formula xml:id="formula_187">∂ ∂z i log p ℓ (z ℓ | z pa(ℓ) ) = ∂ ∂z i log q ℓ (z ℓ | z pa(ℓ) ) = 0 , (<label>118</label></formula><formula xml:id="formula_188">) which indicates that if i / ∈ pa(ℓ), then [s(z)] i = [s m (z)] i for all z. This, equivalently, means that if E |s(Z) -s m (Z)| i ̸ = 0, then i ∈ pa(ℓ).</formula><p>□ For the reverse direction, we will use the following intermediate result which formalizes the weakest possible requirement for a meaningful intervention and shows that it is a property of (i) hard interventions under any causal model, and (ii) additive noise model under either soft or hard interventions.</p><p>Lemma 12 (Interventional Regularity) Causal mechanisms p i and q i of node i are said to satisfy interventional regularity if</p><formula xml:id="formula_189">∃z ∈ R n such that ∂ ∂z k q i (z i | z pa(i) ) p i (z i | z pa(i) ) ̸ = 0 , ∀k ∈ pa(i) .<label>(119)</label></formula><p>Then, p i and q i satisfy interventional regularity if at least one of the following conditions is true:</p><p>1. The intervention is hard, i.e., q i (z i | z pa(i) ) = q i (z i ).</p><p>2. The causal model is an additive noise model in which the pdfs of the noise variables are analytic.</p><p>Proof: See Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case (i) and Case</head><formula xml:id="formula_190">(ii). E s(Z) -s m (Z) i ̸ = 0 =⇒ i ∈ pa(I m</formula><p>) is already shown above.</p><p>For the reverse direction, we give the proof for soft interventions on additive noise models, Case (ii). We will use interventional regularity since Lemma 12 shows that it is satisfied for additive noise models. The proof for hard interventions, Case (i), follows from similar arguments since interventional regularity is also satisfied for hard interventions by Lemma 12.</p><p>Proof of i ∈ pa(I m ) =⇒ E s(Z)s m (Z) i ̸ = 0: Note that the two score functions s and s m are equal in their coordinate i ∈ pa(ℓ) only if</p><formula xml:id="formula_191">0 = ∂ log q ℓ (z ℓ | z pa(ℓ) ) ∂z i - ∂ log p ℓ (z ℓ | z pa(ℓ) ) ∂z i = ∂ ∂z i log q ℓ (z ℓ | z pa(i) ) p ℓ (z ℓ | z pa(ℓ) ) .<label>(120)</label></formula><p>However, (120) contradicts with interventional regularity. Therefore, if i ∈ pa(ℓ), [s(z) i ] and [s m (z)] i are not identical and by Proposition 2, E |s(Z)s m (Z)| i ̸ = 0.</p><p>Case (iii) Coupled environments. Suppose that I m = Ĩm = ℓ. Following ( <ref type="formula" target="#formula_28">16</ref>), we have</p><formula xml:id="formula_192">s m (z) = ∇ z log q ℓ (z ℓ ) + i̸ =ℓ ∇ z log p i (z i | z pa(i) ) ,<label>(121)</label></formula><p>and</p><formula xml:id="formula_193">sm (z) = ∇ z log qℓ (z ℓ ) + i̸ =ℓ ∇ z log p i (z i | z pa(i) ) .<label>(122)</label></formula><p>Then, subtracting (122) from ( <ref type="formula" target="#formula_192">121</ref>) and looking at i-th coordinate, we have</p><formula xml:id="formula_194">s m (z) -sm (z) i = ∂ log q ℓ (z ℓ ) ∂z i - ∂ log qℓ (z ℓ ) ∂z i . (<label>123</label></formula><formula xml:id="formula_195">)</formula><p>If i ̸ = ℓ, the right-hand side is zero and we have s m (z)sm (z) i = 0 for all z. On the other hand, if i = ℓ, since q ℓ (z ℓ ) and qℓ (z ℓ ) are distinct, there exists z ∈ R n such that q ℓ (z ℓ ) ̸ = qℓ (z ℓ ). Subsequently, by Proposition 2, we have E |s m (Z)sm (Z)| i ̸ = 0.</p><p>Case (iv) Uncoupled environments. Suppose that I m = ℓ and Ĩm = j, and ℓ ̸ = j. Following ( <ref type="formula" target="#formula_28">16</ref>), we have</p><formula xml:id="formula_196">s m (z) = ∇ z log q ℓ (z ℓ ) + ∇ z log p j (z j | z pa(j) ) + k∈[n]\{ℓ,j} ∇ z log p k (z k | z pa(k ) , (124) and sm (z) = ∇ z log q j (z j ) + ∇ z log p ℓ (z ℓ | z pa(ℓ) ) + k∈[n]\{ℓ,j} ∇ z log p k (z k | z pa(k) ) . (125)</formula><p>Then, subtracting (125) from (124) we have</p><formula xml:id="formula_197">s m (z) -sm (z) = ∇ z log q ℓ (z ℓ ) + ∇ z log p j (z j | z pa(j) ) -∇ z log q j (z j ) -∇ z log p ℓ (z ℓ | z pa(ℓ) ) . (<label>126</label></formula><formula xml:id="formula_198">)</formula><p>Scrutinizing the i-th coordinate, we have</p><formula xml:id="formula_199">s m (z) -sm (z) i = ∂ log q ℓ (z ℓ ) ∂z i + ∂ log p j (z j | z pa(j) ) ∂z i - ∂ log q j (z j ) ∂z i - ∂ log p ℓ (z ℓ | z pa(ℓ) ) ∂z i . (<label>127</label></formula><formula xml:id="formula_200">) Proof of E s m (Z) -sm (Z) i ̸ = 0 =⇒ i ∈ pa(ℓ, j): Suppose that i / ∈ pa(ℓ, j). Then,</formula><p>none of the terms in the RHS of ( <ref type="formula" target="#formula_199">127</ref>) is a function of z i . Therefore, all the terms in the RHS of ( <ref type="formula" target="#formula_199">127</ref>) are zero, and we have s m (z)sm (z) i = 0 for all z. By Proposition 2,</p><formula xml:id="formula_201">E |s m (Z) -sm (Z)| i = 0. This, equivalently, means that if E |s m (Z) -sm (Z)| i ̸ = 0, then i ∈ pa(ℓ, j).</formula><p>Proof of E s m (Z)sm (Z) i ̸ = 0 ⇐= i ∈ pa(ℓ, j): We prove it by contradiction.</p><p>Assume that s m (z)sm (z) i = 0 for all z. Without loss of generality, let ℓ / ∈ pa(j).</p><p>1. If i = ℓ. In this case, ( <ref type="formula" target="#formula_199">127</ref>) is simplified to</p><formula xml:id="formula_202">0 = s m (z) -sm (z) ℓ = ∂ log q ℓ (z ℓ ) ∂z ℓ - ∂ log p ℓ (z ℓ | z pa(ℓ) ) ∂z ℓ . (<label>128</label></formula><formula xml:id="formula_203">)</formula><p>If ℓ is a root node, i.e., pa(ℓ) = ∅, (128) implies that (log q ℓ ) ′ (z ℓ ) = (log p ℓ ) ′ (z ℓ ) for all z ℓ . Integrating, we get p ℓ (z ℓ ) = αq ℓ (z ℓ ) for some constant α. Since both p ℓ and q ℓ are pdfs, they both integrate to one, implying α = 1 and p ℓ (z ℓ ) = q ℓ (z ℓ ), which contradicts the premise that observational and interventional mechanisms are distinct. If ℓ is not a root node, consider some k ∈ pa(ℓ). Then, taking the derivative of (128) with respect to z k , we have</p><formula xml:id="formula_204">0 = ∂ 2 log p ℓ (z ℓ | z pa(ℓ) ) ∂z ℓ ∂z k . (<label>129</label></formula><formula xml:id="formula_205">)</formula><p>Recall the equation Z ℓ = f ℓ (Z pa(ℓ) ) + N ℓ for additive noise models specified in (7). Denote the pdf of the noise term N ℓ by p N . Then, the conditional pdf p ℓ (z ℓ | z pa(ℓ) ) is given by p</p><formula xml:id="formula_206">ℓ (z ℓ | z pa(ℓ) ) = p N (z ℓ -f ℓ (z pa(ℓ) )).</formula><p>Denoting the score function of p N by r p ,</p><formula xml:id="formula_207">r p (u) ≜ d du log p N (u) ,<label>(130)</label></formula><p>A.2 Proof of Lemma 2</p><p>Let us recall the setting. Consider random vectors Y 1 , Y 2 ∈ R r and W 1 , W 2 ∈ R s that are related through Y 1 = f (W 1 ) and Y 2 = f (W 2 ) such that r ≥ s, probability measures of W 1 , W 2 are absolutely continuous with respect to the s-dimensional Lebesgue measure and f : R s → R r is an injective and continuously differentiable function.</p><p>In this setting, the realizations of W 1 and Y 1 , and that of W 2 and Y 2 , are related through y = f (w). Since f is injective and continuously differentiable, volume element dw in R s gets mapped to det [J f (w)] ⊤ • J f (w)</p><p>1/2 dw on im(f ). Since W 1 has density p W 1 absolutely continuous with respect to the s-dimensional Lebesgue measure, using the area formula <ref type="bibr" target="#b5">(Boothby, 2003)</ref>, we can define a density for Y 1 , denoted by p Y 1 , supported only on manifold M ≜ im(f ) which is absolutely continuous with respect to the s-dimensional Hausdorff measure:</p><formula xml:id="formula_208">p Y 1 (y) = p W 1 (w) • det [J f (w)] ⊤ • J f (w) -1/2</formula><p>, where y = f (w) .</p><p>Densities p Y 2 and p W 2 of Y 2 and W 2 are related similarly. Subsequently, score functions of {W 1 , W 2 } and {Y 1 , Y 2 } are specified similarly to ( <ref type="formula" target="#formula_16">9</ref>) and ( <ref type="formula" target="#formula_25">14</ref>), respectively. Denote the Jacobian matrix of f at point w ∈ R s by J f (w), which is an r × s matrix with entries given by</p><formula xml:id="formula_210">J f (w) i,j = ∂ f (w) i ∂w j = ∂y i ∂w j , ∀i ∈ [r] , j ∈ [s] .<label>(138)</label></formula><p>Next, consider a function ϕ : M → R. Since the domain of ϕ is a manifold, its differential, denoted by Dϕ, is defined according to (12). By noting y = f (w), we can also differentiate ϕ with respect to w ∈ R s as <ref type="bibr">(Simon, 2014, p. 57</ref>)</p><formula xml:id="formula_211">∇ w ϕ(y) = ∇ w (ϕ • f )(w) = J f (w) ⊤ • Dϕ(y) .<label>(139)</label></formula><p>Next, given the identities in ( <ref type="formula" target="#formula_209">137</ref>) and ( <ref type="formula" target="#formula_211">139</ref>), we find the relationship between score functions of W 1 and Y 1 as follows.</p><formula xml:id="formula_212">s W 1 (w) = ∇ w log p W 1 (w)<label>(140)</label></formula><formula xml:id="formula_213">(137) = ∇ w log p Y 1 (y) + ∇ w log det [J f (w)] ⊤ • J f (w) 1/2 (141) (139) = J f (w) ⊤ • D log p Y 1 (y) + ∇ w log det [J f (w)] ⊤ • J f (w) 1/2 (142) = J f (w) ⊤ • s Y 1 (y) + ∇ w log det [J f (w)] ⊤ • J f (w) 1/2 . (<label>143</label></formula><formula xml:id="formula_214">)</formula><p>Following the similar steps that led to (143) for W 2 and Y 2 , we obtain</p><formula xml:id="formula_215">s W 2 (w) = J f (w) ⊤ • s Y 2 (y) + ∇ w log det [J f (w)] ⊤ • J f (w) 1/2 . (<label>144</label></formula><formula xml:id="formula_216">)</formula><p>Subtracting ( <ref type="formula" target="#formula_215">144</ref>) from ( <ref type="formula" target="#formula_213">143</ref>), we obtain the desired result</p><formula xml:id="formula_217">s W 1 (w) -s W 2 (w) = J f (w) ⊤ • s Y 1 (y) -s Y 2 (y) .<label>(145)</label></formula><p>Proof of the reverse direction. Multiplying (145) from left with [J f (w)] † ⊤ , we obtain</p><formula xml:id="formula_218">J f (w) † ⊤ • s W 1 (w) -s W 2 (w) = J f (w) † ⊤ • J f (w) ⊤ • s Y 1 (y) -s Y 2 (y) . (146) Note that J f (w) † ⊤ • J f (w) ⊤ = J f (w) • J f (w) † . (<label>147</label></formula><formula xml:id="formula_219">)</formula><p>By properties of the Moore-Penrose inverse, for any matrix A, we have A • A † • A = A. This means that A • A † acts as a left identity for vectors in the column space of A. By definition, s Y 1 and s Y 2 have values in T w im(f ), the tangent space of the image manifold f at point w. This space is equal to the column space of the matrix J f (w). Therefore, J f (w) • [J f (w)] † acts as a left identity for s Y 1 (y) and s Y 2 (y), and we have</p><formula xml:id="formula_220">J f (w) • J f (w) † • s Y 1 (y) -s Y 2 (y) = s Y 1 (y) -s Y 2 (y) .<label>(148)</label></formula><p>Substituting ( <ref type="formula" target="#formula_218">147</ref>) and ( <ref type="formula" target="#formula_220">148</ref>) into ( <ref type="formula">146</ref>) completes the proof.</p><p>Proof of Corollary 1 For a given linear transform F, we have J f (w) = F, which is independent of w. Then, in the proof of Lemma 2, (143) reduces to s</p><formula xml:id="formula_221">W (w) = F ⊤ • s Y (y).</formula><p>Finally, we note that the score difference of Y can be similarly written in terms of the score difference of W.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Lemma 12</head><p>We will prove that causal mechanisms p i and q i satisfy interventional regularity for (i) hard interventions and (ii) additive noise models. To this end, we first define</p><formula xml:id="formula_222">ψ(z i , z pa(i) ) ≜ q i (z i | z pa(i) ) p i (z i | z pa(i) ) . (<label>149</label></formula><formula xml:id="formula_223">)</formula><p>We start by showing that ψ(z i , z pa(i) ) varies with z i . We prove it by contradiction. Assume the contrary, i.e., let ψ(z i , z pa(i) ) = ψ(z pa(i) ). By rearranging (149) we have</p><formula xml:id="formula_224">q i (z i | z pa(i) ) = ψ(z pa(i) ) p i (z i | z pa(i) ) .<label>(150)</label></formula><p>Fix a realization of z * pa(i) and integrate both sides of (150) with respect to z i . Since both p i and q i are pdfs, we have</p><formula xml:id="formula_225">1 = R q i (z i | z * pa(i) ) dz i = R ψ(z * pa(i) ) p i (z i | z * pa(i) )dz i dz i (151) = ψ(z * pa(i) ) R p i (z i | z * pa(i) )dz i (152) = ψ(z * pa(i) ) .<label>(153)</label></formula><p>This identity implies that p</p><formula xml:id="formula_226">i (z i | z * pa(i) ) = q i (z i | z * pa(i) )</formula><p>for any arbitrary realization z * pa(i) . This contradicts the premise that observational and interventional distributions are distinct. As a result, to check if a model satisfies interventional regularity for node i, it suffices to investigate whether the function ψ is not invariant with respect to z k for k ∈ pa(i). To this end, from (149) we know that ψ(z i , z pa(i) ) varies with z k if and only if</p><formula xml:id="formula_227">∂ ∂z i log ψ(z i , z pa(i) ) = ∂q i (z i | z pa(i) ) ∂z i • 1 q i (z i | z pa(i) ) - ∂p i (z i | z pa(i) ) ∂z i • 1 p i (z i | z pa(i) ) ̸ = 0 .<label>(154)</label></formula><p>Next, we investigate the sufficient conditions listed in Lemma 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Hard Interventions</head><p>Under hard interventions, note that for any k ∈ pa(i),</p><formula xml:id="formula_228">∂ ∂z k p i (z i | z pa(i) ) ̸ = 0 , and ∂ ∂z k q i (z i ) = 0 .<label>(155)</label></formula><p>Then, it follows directly from (154) that</p><formula xml:id="formula_229">∂ ∂z k log ψ(z i , z pa(i) ) = ∂q i (z i ) ∂z k = 0 • 1 q i (z i ) - ∂p i (z i | z pa(i) ) ∂z k ̸ = 0 • 1 p i (z i | z pa(i) ) ̸ = 0 .<label>(156)</label></formula><p>Thus, hard interventions on any latent causal model satisfy interventional regularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Additive Noise Models</head><p>The additive noise model for node i is given by</p><formula xml:id="formula_230">Z i = f i (Z pa(i) ) + N i ,<label>(157)</label></formula><p>as specified in (7). When node i is soft intervened, Z i is generated according to</p><formula xml:id="formula_231">Z i = fi (Z pa(i) ) + Ni ,<label>(158)</label></formula><p>in which fi and Ni specify the interventional mechanism for node i. Then, denoting the pdfs of N i and Ni by p N and q N , respectively, ( <ref type="formula" target="#formula_230">157</ref>) and ( <ref type="formula" target="#formula_231">158</ref>) imply that</p><formula xml:id="formula_232">p i (z i | z pa(i) ) = p N z i -f i (z pa(i) ) , and q i (z i | z pa(i) ) = q N z i -fi (z pa(i) ) . (<label>159</label></formula><formula xml:id="formula_233">)</formula><p>Denote the score functions associated with p N and q N by</p><formula xml:id="formula_234">r p (u) ≜ d du log p N (u) = p ′ N (u) p N (u)</formula><p>, and r q (u) ≜</p><formula xml:id="formula_235">d du log q N (u) = q ′ N (u) q N (u) . (<label>160</label></formula><formula xml:id="formula_236">)</formula><p>We will prove that (154) holds by contradiction. Assume the contrary and let</p><formula xml:id="formula_237">∂q i (z i | z pa(i) ) ∂z k • 1 q i (z i | z pa(i) ) = ∂p i (z i | z pa(i) ) ∂z k • 1 p i (z i | z pa(i) ) .<label>(161)</label></formula><p>From ( <ref type="formula" target="#formula_232">159</ref>) and ( <ref type="formula" target="#formula_235">160</ref>), for the numerators in (161) we have,</p><formula xml:id="formula_238">∂p i (z i | z pa(i) ) ∂z k = - ∂f i (z pa(i) ) ∂z k • p ′ N z i -f i (z pa(i) ) ,<label>(162)</label></formula><p>and</p><formula xml:id="formula_239">∂q i (z i | z pa(i) ) ∂z k = - ∂ fi (z pa(i) ) ∂z k • q ′ N z i -fi (z pa(i) ) .<label>(163)</label></formula><p>Hence, the identity in (161) can be written as</p><formula xml:id="formula_240">∂f i (z pa(i) ) ∂z k • r p z i -f i (z pa(i) ) = ∂ fi (z pa(i) ) ∂z k • r q z i -fi (z pa(i) ) .<label>(164)</label></formula><p>Define n i and ni as the realizations of N i and Ni when Z i = z i and Z pa(i) = z pa(i) . By defining δ(z pa(i) ) ≜ f i (z pa(i) ) -fi (z pa(i) ), we have ni = n i + δ(z pa(i) ). Then, (164) can rewritten as</p><formula xml:id="formula_241">∂f i (z pa(i) ) ∂z k • r p (n i ) = ∂ fi (z pa(i) ) ∂z k • r q (n i + δ(z pa(i) )) .<label>(165)</label></formula><p>Note that</p><formula xml:id="formula_242">∂f i (z pa(i) ) ∂z k</formula><p>is a nonzero continuous function. Hence, there exists an interval</p><formula xml:id="formula_243">Φ ⊆ R |pa(i)| over which ∂f i (z pa(i) ) ∂z k ̸ = 0.</formula><p>Likewise, r p (n i ) cannot be constantly zero over all possible intervals Ω ⊆ R. This is because otherwise, it would have to necessarily be a constant zero function (since it is analytic), which is an invalid score function. Hence, there exists an open interval Ω ⊆ R over which r p (n i ) is nonzero for all n i ∈ Ω. Then, we can rearrange (165) as</p><formula xml:id="formula_244">r p (n i ) r q (n i + δ(z pa(i) )) = ∂ fi (z pa(i) ) ∂z k ∂f i (z pa(i) ) ∂z k , ∀(n i , z pa(i) ) ∈ Ω × Φ .<label>(166)</label></formula><p>Note that the RHS of ( <ref type="formula" target="#formula_244">166</ref>) is not a function of n i . Then, taking the derivative of both sides with respect to n i , we get</p><formula xml:id="formula_245">r ′ p (n i ) r p (n i ) = r ′ q (n i + δ(z pa(i) )) r q (n i + δ(z pa(i) )) .<label>(167)</label></formula><p>In the next step, we show that δ is not a constant function. We prove this by contradiction. Suppose that δ(z pa(i) ) = δ * is a constant function. Then, the gradients of f i and fi are equal. From (166), this implies that</p><formula xml:id="formula_246">r p (n i ) = r q (n i + δ * ) , ∀n i ∈ Ω .<label>(168)</label></formula><p>Since r p (n i ) and r q (n i +δ * ) are analytic functions that agree on an open interval of R, they are equal for all n i ∈ R. This implies that p N (n i ) = η •q N (n i +δ * ) for some constant η ∈ R. Since p N and q N are pdfs, η = 1 is the only choice that maintains p N and q N are pdfs. Therefore, p N (n i ) = q N (n i + δ * ). However, using (159), this implies that p i (z i | z pa(i) ) = q i (z i | z pa(i) ), which contradicts the premise that an intervention changes the causal mechanism of target node i. Therefore, δ is a continuous, non-constant function, and its image over z pa(i) ∈ Φ includes an open interval Θ ⊆ R. With this result in mind, we return to (167). Consider a Then, since (1, . . . , n) is assumed to be topological order of G, ρ ≜ (I π 1 , . . . , I πn ) is also a topological order of G. According to this notation, in the π t -th environment, node ρ t is intervened.</p><p>Next, since ρ 1 has no parent in G, by Theorem 2 we already know that Ẑπ 1 = b 1 • Z ρ 1 for some constant b 1 ∈ R + . Consider m = π k step of the algorithm, and assume that for all t ∈ [k -1], the updated encoder rows satisfy</p><formula xml:id="formula_247">Ẑπt = Ĥπt • G • Z = b t • Z ρt ,<label>(182)</label></formula><p>for some constant b t ∈ R + . We will show that the update in step m = π k will ensure that Ẑπ k also satisfies (182). Before the update, by Theorem 2 we know that Ẑπ k is a linear function of Z ρ k and Z pa(ρ k ) . Then, using (182), we have</p><formula xml:id="formula_248">Ẑπ k = c 0 • Z ρ k + c • Ẑ pa(π k ) ,<label>(183)</label></formula><p>for | pa(π k )|-dimensional vector c and nonzero constant c 0 ∈ R + . We will use Proposition 1 in environment E π k . For brevity, using m = π k and letting</p><formula xml:id="formula_249">V ≜ Ẑm pa(π k ) , Ẑm m = c 0 • Z m ρ k + c • V . (184) Since node ρ k is intervened in environment E m , by Proposition 1, we know that Z m ρ k ⊥ ⊥ V, which implies that Cov(Z m ρ k , V) = 0 1×|V| .<label>(185)</label></formula><p>Consider a |V|-dimensional row vector u and let</p><formula xml:id="formula_250">Y = Ĥm -u • Ĥ pa(m) • X m = Ẑm m -u • V . (<label>186</label></formula><formula xml:id="formula_251">)</formula><p>Note that u = c would yield that Y = c 0 • Z m ρ k , and subsequently Y ⊥ ⊥ V. On the other hand, Y ⊥ ⊥ V implies that</p><formula xml:id="formula_252">0 1×|V| = Cov(Y, V) = Cov( Ẑm m -u • V, V) ,<label>(187)</label></formula><p>which has a unique solution</p><formula xml:id="formula_253">u = Cov( Ẑm m , V) • Cov(V) -1 .<label>(188)</label></formula><p>i.e., linear minimum mean square error (LMMSE) estimator. Finally, Cov(V) is invertible since the causal relationships among the entries in Ẑm pa(π k ) are not deterministic. Using this row vector u, we update</p><formula xml:id="formula_254">Ĥm ← H m -u • H pa(π k ) ,<label>(189)</label></formula><p>and achieve Ẑm = c 0 • Z ρ k . Therefore, (182) holds for t = k as well, and by induction we obtain</p><formula xml:id="formula_255">Ẑ = Ĥ • G • Z = P I • C s • Z ,<label>(190)</label></formula><p>for a diagonal matrix C s with nonzero diagonal entries. □</p><p>Let τ be the permutation that maps {1, . . . , n} to I, i.e., I τ i = i for all i ∈ [n] and P τ to denote the permutation matrix that corresponds to τ , i.e.,</p><formula xml:id="formula_256">[P τ ] i,m = 1 , m = ρ i , 0 , else .<label>(205)</label></formula><p>By Lemma 1(iv), true latent score changes across {E τ i , Ẽτ j } gives us pa(i, j) for i ̸ = j. First, we use the perfect latent recovery result to show that Lemma 1(iv) also applies to estimated latent score changes. Denote f = ĥ • g. Using score transform between Z and Ẑ in (40), and recalling 1{J -1 f } = P τ , we have s τ i Ẑ (ẑ; ĥ)sτ j Ẑ (ẑ; ĥ)</p><formula xml:id="formula_257">τ k = J -⊤ f (z) τ k • s τ i (z) -sτ j (z) (206) = J -⊤ f (z) τ k ,k • s τ i (z) -sτ j (z) k . (<label>207</label></formula><formula xml:id="formula_258">)</formula><p>Using J -⊤ f (z) τ k ,k ̸ = 0 for all z ∈ R n , we have E s τ i Ẑ ( Ẑ; ĥ)sτ j Ẑ ( Ẑ; ĥ)</p><formula xml:id="formula_259">τ k ̸ = 0 ⇐⇒ E s τ i (Z) -sτ j (Z) τ k ̸ = 0 .<label>(208)</label></formula><p>Hence, by Lemma 1(iv), E s τ i Ẑ ( Ẑ; ĥ)sτ j Ẑ ( Ẑ; ĥ)</p><formula xml:id="formula_260">τ k ̸ = 0 ⇐⇒ k ∈ pa(i, j) .<label>(209)</label></formula><p>Let us denote the graph G τ that is related to G by permutation τ , i.e., i ∈ pa(j) if and only if τ i ∈ pa τ (τ j ) for which pa τ (τ j ) denotes the parents of node τ j in G τ . Using (209), we have E s τ i Ẑ ( Ẑ; ĥ)sτ j Ẑ ( Ẑ; ĥ) τ k ̸ = 0 ⇐⇒ τ k ∈ pa τ (τ i , τ j ) .</p><p>(210)</p><p>In the rest of the proof, we will show how to obtain {pa τ (i) : i ∈ [n]} using {pa τ (i, j) : i, j ∈</p><p>[n], i ̸ = j}. Since G τ is a graph isomorphism of G, this problem is equivalent to obtaining {pa(i) : i ∈ [n]} using {pa(i, j) : i, j ∈ [n], i ̸ = j}. Note that Ẑi (which corresponds to node i in G τ ) is intervened in environments E i and Ẽi . We denote the set of root nodes by</p><formula xml:id="formula_261">K ≜ {i ∈ [n] : pa(i) = ∅} ,<label>(211)</label></formula><p>and also define</p><formula xml:id="formula_262">B i ≜ j̸ =i</formula><p>pa(i, j) , ∀i ∈ [n] , and B ≜ {i</p><formula xml:id="formula_263">: |B i | = 1} . (<label>212</label></formula><formula xml:id="formula_264">)</formula><p>Note that pa(i) ⊆ B i . Hence, |B i | = 1 implies that i is a root node. We investigate the graph recovery in three cases. </p><p>Note that the last equality is due to j∈K\{i} {j} = ∅ since there are at least two root nodes excluding i. Then, B i = pa(i) for all i ∈ [n] and we are done.</p><p>Since D t (g -1 ; σ) is a scaled permutation matrix, using (77) we obtain</p><formula xml:id="formula_266">D t (h * ; σ) = [D t (g -1 ; σ)] -1 • D t (g -1 ; σ) = I n×n ,<label>(218)</label></formula><p>D(h * ) = [D t (g -1 ; σ)] -1 • D(g -1 ) (219) D(h * ) = [D t (g -1 ; σ)] -1 • D(g -1 ) .</p><p>(220)</p><p>Thus, using 1{ D(g -1 )} = 1{D(g -1 )} • P σ , we have </p><formula xml:id="formula_267">1{ D(h * )} = 1{D(h * )} • P σ ,<label>(221)</label></formula><formula xml:id="formula_268">= E J -1 f (Z) ℓ,i • s m (Z) -sπm (Z) ℓ .<label>(225)</label></formula><p>By definition of h * , [D t (h * )] i,m = 0 for all i ̸ = m. Also, interventional discrepancy between q ℓ (z ℓ ) and qℓ (z ℓ ) implies that s m (z)-sπm (z) ℓ ̸ = 0 except for a null set. Then, [D t (h * )] i,m = 0 implies that J -1 f (z) ℓ,i = 0 except for a null set. Since J -1 f is a continuous function, this implies that J -1 f (z) ℓ,i = 0 for all z ∈ R n . Furthermore, since J -1 f is invertible for all z, none of its columns can be a zero vector. Hence, for all z ∈ R n , J -1 f (z) ℓ,m ̸ = 0. To summarize, if π m = σ m , then Step 1: Show that E [J -1 f (Z)] I a ,a ̸ = 0. First, using (40) and Lemma 1(i), we have  <ref type="formula" target="#formula_140">92</ref>). Therefore, if (h * , π) is a minimizer of (92), then π must be the correct coupling σ.</p><formula xml:id="formula_269">∀z ∈ R n J -1 f (z) I m ,i ̸ = 0 ⇐⇒ i = m . (<label>226</label></formula><formula xml:id="formula_270">D(h * ) a,a = E J -⊤ f (Z) a • s(Z) -s a (Z)<label>(232)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Analysis of Assumption 2</head><p>In this section, we establish the necessary and sufficient conditions under which Assumption 2 holds for additive models. Furthermore, we show that a large class of nonlinear models in the latent space satisfy these conditions, including the two-layer neural networks. We have focused on such NNs since they effectively approximate continuous functions <ref type="bibr" target="#b8">(Cybenko, 1989)</ref>. Readily, the necessary and sufficient conditions can be investigated for other choices of nonlinear functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.1 Interpreting Assumption 2</head><p>The implication of Assumption 2 is that the effect of an intervention is not lost in any linear combination of the varying coordinates of the scores. To formalize this, for each node i ∈ [n], we define</p><formula xml:id="formula_271">C i ≜ {c ∈ R n : ∃j ∈ pa(i) such that c j ̸ = 0} ,<label>(252)</label></formula><p>Let I m = i. Note that R m Z is positive semi-definite and [R m Z ] j,k = 0 for all (j, k) / ∈ pa(i)×pa(i). Therefore, for any c ∈ C i ,</p><formula xml:id="formula_272">c ⊤ • R m Z • c = E (c ⊤ • d m Z ) • (c ⊤ • d m Z ) ⊤ ̸ = 0 ,<label>(253)</label></formula><p>since R m Z has rank |pa(i)|. The reverse direction also holds true, i.e., if R m Z has rank less than |pa(i)|, then there exists cC i which makes c ⊤ • R m Z • c = 0. Therefore, Assumption 2 is equivalent to the following statement:</p><formula xml:id="formula_273">E |c ⊤ • d m Z | ̸ = 0 , ∀c ∈ C i . (<label>254</label></formula><formula xml:id="formula_274">)</formula><p>Necessary and sufficient conditions. Consider the additive noise model for node i</p><formula xml:id="formula_275">Z i = f i (Z pa(i) ) + N i ,<label>(255)</label></formula><p>as specified in (7). When node i is soft intervened, Z i is generated according to</p><formula xml:id="formula_276">Z i = fi (Z pa(i) ) + Ni ,<label>(256)</label></formula><p>in which fi and Ni specify the interventional mechanism for node i. The following lemma characterizes the necessary and sufficient conditions under which (254) (equivalently, Assumption 2) is satisfied. In this subsection, we use φ as the shorthand for z pa(i) .</p><p>Lemma 14 For each node i ∈ [n] consider the following two set of equations for c ∈ R n :</p><formula xml:id="formula_277">   c i -c ⊤ • ∇ z f i (φ) = 0 c i -c ⊤ • ∇ z fi (φ) = 0 , ∀φ ∈ R |pa(i)| . (<label>257</label></formula><formula xml:id="formula_278">)</formula><p>Assumption 2 holds if and only if the only all solutions c to (257) satisfy c / ∈ C i , or based on (252), equivalently c j = 0 for all j ∈ pa(i).</p><p>ν ∈ R w i as the weights between the hidden layer and output in f i and fi , respectively. Finally, define ν 0 and ν0 as the bias terms. Hence, we have </p><formula xml:id="formula_279">f i (φ) = ν ⊤ • σ(W i • φ) + ν 0 = w i j=1 ν j • σ(W i j • φ) + ν 0 ,<label>(261)</label></formula><p>then Assumption 2 holds.</p><p>Proof: See Appendix D.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.2 Proof of Lemma 14</head><p>We show that for node I m = i, the condition in ( <ref type="formula" target="#formula_273">254</ref>)</p><formula xml:id="formula_281">E c ⊤ • d m Z ̸ = 0 , ∀c ∈ C i ,<label>(264)</label></formula><p>holds if and only if the following continuum of equations admit their solutions c in</p><formula xml:id="formula_282">R n \ C i .    c i -c ⊤ • ∇ z f i (φ) = 0 c i -c ⊤ • ∇ z fi (φ) = 0 , ∀φ ∈ R |pa(i)| ,<label>(265)</label></formula><p>in which shorthand φ is used for z pa(i) . We first note that the condition in (264) can be equivalently stated using ( <ref type="formula" target="#formula_31">18</ref>) and ( <ref type="formula" target="#formula_32">19</ref>) as</p><formula xml:id="formula_283">E c ⊤ • ∇ z log p i (Z i | Z pa(i) ) -∇ z log q i (Z i | Z pa(i) ) ̸ = 0 , ∀c ∈ C i .<label>(266)</label></formula><p>Also, using Proposition 2, (266) is equivalent to</p><formula xml:id="formula_284">∀c ∈ C i , ∃z : c ⊤ • ∇ z log p i (z i | z pa(i) ) ̸ = c ⊤ • ∇ z log q i (z i | z pa(i) ) . (<label>267</label></formula><formula xml:id="formula_285">)</formula><p>The additive noise model for node i is given by</p><formula xml:id="formula_286">Z i = f i (Z pa(i) ) + N i ,<label>(268)</label></formula><p>as specified in (7). When node i is soft intervened, Z i is generated according to</p><formula xml:id="formula_287">Z i = fi (Z pa(i) ) + Ni ,<label>(269)</label></formula><p>Note that f i is a continuously differentiable function and also a function of z j for all j ∈ pa(i).</p><p>Hence, there exists an open set Φ ⊆ R |pa(i)| for φ for which (c * ) ⊤ • ∇ z f i (φ * )c * i is nonzero everywhere in Φ. Likewise, r p cannot constantly be zero over all possible intervals Ω. This is because otherwise, it would have to necessarily be a constant zero function (since it is analytic), which is an invalid score function. Hence, there exists an open interval Ω ⊆ R over which r p (n i ) is nonzero for all n i ∈ Ω. Subsequently, the left-hand side of ( <ref type="formula">276</ref>) is nonzero over the Cartesian product (n i , φ) ∈ Ω × Φ. This means that if (276) is true, then both functions on its right-hand side must also be nonzero over Ω × Φ. Hence, by rearranging the terms in (276) we have</p><formula xml:id="formula_288">r q n i + δ(φ) r p (n i ) = c * i -(c * ) ⊤ • ∇ z f i (φ) c * i -(c * ) ⊤ • ∇ z fi (φ) , ∀(n i , φ) ∈ Ω × Φ . (<label>278</label></formula><formula xml:id="formula_289">)</formula><p>In two steps, we show that (278) cannot be valid.</p><p>Step 1. First, we show that function δ cannot be a constant over Φ (interval specified above). Suppose the contrary and assume that δ(φ) = δ * for all φ ∈ Φ. Hence, the gradient of δ is zero. Using the definition of δ, this implies that</p><formula xml:id="formula_290">∇ z f i (φ) = ∇ z fi (φ) , ∀φ ∈ Φ .<label>(279)</label></formula><p>Then, by leveraging (278), we conclude that r p (n i ) = r q (n i + δ * ) for all n i ∈ Ω. Since r p (n i ) and r q (n i + δ * ) are analytic functions that agree on an open interval of R, they are equal for all n i ∈ R as well. This implies that p N (n i ) = η • q N (n i + δ * ) for some constant η. Since p N and q N are pdfs, the only choice is η = 1 and p N (n i ) = q N (n i + δ * ), then p i (z i | z pa(i) ) = q i (z i | z pa(i) ), which contradicts the premise that observational and interventional mechanisms are distinct.</p><p>Step 2. Finally, we will show that (278) cannot be true when δ is not a constant function over Φ. Note that the right-hand side of ( <ref type="formula" target="#formula_288">278</ref>) is not a function of n i . Then, taking the derivative of both sides with respect to n i and rearranging, we obtain r ′ p (n i ) r p (n i ) = r ′ q (n i + δ(φ)) r q (n i + δ(φ)) , ∀(n i , φ) ∈ Ω × Φ .</p><p>Next, consider a fixed realization n i = n * i , and denote the value of LHS by α. Since δ is continuous and not constant over Φ, its image contains an open interval Θ ⊆ R. Denoting u ≜ δ(φ), we get</p><formula xml:id="formula_292">α = r ′ q (n * i + u) r q (n * i + u) , ∀u ∈ Θ . (<label>281</label></formula><formula xml:id="formula_293">)</formula><p>The only solution to this equality is that r q (n * i + u) is an exponential function, r q (u) = k 1 exp(αu) over interval u ∈ Θ. Since r q is an analytic function that equals to an exponential function over an interval, it is exponential over entire R. This implies that the pdf q N (u) is of the form q N (u) = k 2 exp((k 1 /α) exp(αu)). However, this cannot be a valid pdf since its integral over R diverges. Hence, (278) is not true, and the premise that there exists c * ∈ C i and φ ∈ R |pa(i)| corresponding to which (277) holds is invalid, concluding that for all </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Implementation Details of GSCALE-I Algorithm</head><p>We perform experiments for the coupled interventions setting, and solve the following optimization problem in Step G2 of GSCALE-I,</p><formula xml:id="formula_294">min h D t (h) -I n×n 1,1 + λE h -1 (h(X)) -X 2 2 . (<label>302</label></formula><formula xml:id="formula_295">)</formula><p>where λ &gt; 0 is a regularization parameter to ensure injectivity. In the following, first, we show why this problem is equivalent to solving (83). Then, we describe the computation of the ground truth score differences for X and discuss other implementation details.</p><p>Implementation steps. We use n s samples from the observational environment to compute empirical expectations. Since encoder h is parameterized by H, we use gradient descent to learn this matrix. To do so, we minimize the loss described above in (302). We denote the final parameter estimate by H * and the encoder parameterized by H * by h * . In the simulation results reported in Section 7.2, we set λ = 1 and solve (302) using RMSprop optimizer with learning rate 10 -3 for 3 × 10 4 steps for n = 5 and 4 × 10 4 steps for n = 8. We also use early stopping when the training converges before the maximum number of steps. Recall that the latent graph estimate Ĝ is constructed using 1{D(h * )}. Similarly to (301), We use a threshold λ G to obtain the graph from the upper triangular part of D(h * ).</p><p>Architecture details and hyperparameter selection for image experiments. In Table <ref type="table" target="#tab_22">17</ref> and Table <ref type="table" target="#tab_23">18</ref>, we list the architecture details and training hyperparameters used in the image experiments in Section 7.5. Specifically, Table <ref type="table" target="#tab_22">17</ref> specifies the autoencoder architecture we used to estimate the latent variables, and  Minimize binary cross entropy between sigmoid of negation of output and labels</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Experimental Methodology Comparison with DiscrepancyVAE</head><p>One major difference between the LSCALE-I algorithm and the publicly available Discrep-ancyVAE implementation is that DiscrepancyVAE tries to learn the intervention effects as an encoding of the intervention labels. Specifically, for each intervention target, the VAE model learns a vector of latent intervention targets (a neural network with softmax activation) to denote which latent node is intervened on. In practice, learning an intervention encoding runs the risk of learning encodings where different interventions affect the same latent node, consequently, having nodes that are not affected by any intervention encoding. We demonstrate this phenomenon for DiscrepancyVAE in Figure <ref type="figure">8</ref>, plotting the learned intervention embeddings for each single-node intervention. We observe that despite hav-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>&lt;</head><label></label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " x 4 P O l x k U j z 9 T k Q8 Q D C E O D Z O D R I M = " &gt; A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B h P B U 9 g N + D g G v X i M a B 6 Q L G F 2 M r s Z M j u z z M w K Y c k n e P G g i F e / y J t / 4 y T Z g y Y W N B R V 3 X R 3 B Q l n 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H b S 1 T R W i L S C 5 V N 8 C a c i Z o y z D D a T d R F M c B p 5 1 g f D v z O 0 9 U a S b F o 5 k k 1 I 9 x J F j I C D Z W e q h G 1 U G 5 4 t b c O d A q 8 X J S g R z N Q f m r P 5 Q k j a k w h G O t e 5 6 b G D / D y j D C 6 b T U T z V N M B n j i P Y s F T i m 2 s / m p 0 7 R m V W G K J T K l j B o r v 6 e y H C s 9 S Q O b G e M z U g v e z P x P 6 + X m v D a z 5 h I U k M F W S w K U 4 6 M R L O / 0 Z A p S g y f W I K J Y v Z W R E Z Y Y W J s O i U b g r f 8 8 i p p 1 2 v e Z e 3 i v l 5 p 3 O R x F O E E T u E c P L i C B t x B E 1 p A I I J n e I U 3 h z s v z r v z s W g t O P n M M f y B 8 / k D h r 2 N T w = = &lt; / l a t e x i t &gt; g &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P 8 t d O D j E + C c d 0 s g U f y Q H L Z Q F 3 q g = " &gt; A A A B 6 n i c b V D L S g N B E O z 1 G V e j U Y 9 e B k P A U 9 g V 1 B w D g n i M a B 6 Y L G F 2 M p s M m Z l d Z m a F s O Q T v H h Q x K v 4 I X 6 C N / / G y e O g i Q U N R V U 3 3 V 1 h w p k 2 n v f t r K y u r W 9 s 5 r b c 7 Z 3 8 7 l 5 h / 6 C h 4 1 Q R W i c x j 1 U r x J p y J m n d M M N p K 1 E U i 5 D T Z j i 8 n P j N B 6 o 0 i + W d G S U 0 E L g v W c Q I N l a 6 v e / 6 3 U L R K 3 t T o G X i z 0 m x m v 9 M S 1 f u R 6 1 b + O r 0 Y p I K K g 3 h W O u 2 7 y U m y L A y j H A 6 d j u p p g k m Q 9 y n b U s l F l Q H 2 f T U M S p Z p Y e i W N m S B k 3 V 3 x M Z F l q P R G g 7 B T Y D v e h N x P + 8 d m q i S p A x m a S G S j J b F K U c m R h N / k Y 9 p i g x f G Q J J o r Z W x E Z Y I W Js e m 4 N g R / 8 e V l 0 j g t + + f l s x u b R g V m y M E R H M M J + H A B V b i G G t S B Q B 8 e 4 R l e H O 4 8 O a / O 2 6 x 1 x Z n P H M I f O O 8 / Y 6 O Q H Q = = &lt; / l a t e x i t &gt; Z 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>6 M 1 5 c l 6 c d + d j 0 b r m 5 D P H 6 A + c z x / i s J O + &lt; / l a t e x i t &gt; observed data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>w 3 d 6 4 w w 5 C P x 3 J x V m 4 D E d P B 8 n 2 Y P v t V n 9 3 r 5 u T N f K Q P C K P S U K e k 1 3 y i u y T I W G k J J / I Z / I l + B h 8 D b 4 F 3 x e l Q a 8 7 8 4 C s R P D j N 4 K S / A I = &lt; / l a t e x i t &gt; Z 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Latent graph recovery comparison. Extra edges are marked in red, and dashed lines mark the missing edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>r K y u r W 9 s 5 r b c 7 Z 3 8 7 l 5 h / 6 C h 4 1 Q R W i c x j 1 U r x J p y J m n d M M N p K 1 E U i 5 D T Z j i 8 n P j N B 6 o 0i + W d G S U 0 E L g v W c Q I N l a 6 v e / 6 3 U L R K 3 t T o G X i z 0 m x m v 9 M S 1 f u R 6 1 b + O r 0 Y p I K K g 3 h W O u 2 7 y U m y L A y j H A 6 d j u p p g k m Q 9 y n b U s l F l Q H 2 f T U M S p Z p Y e i W N m S B k 3 V 3 x M Z F l q P R G g 7 B T Y D v e h N x P + 8 d m q i S p A x m a S G S j J b F K U c m R h N / k Y 9 p i g x f G Q J J o r Z W x E Z Y I W J s e m 4 N g R / 8 e V l 0 j g t + + f l s x u b R g V m y M E R H M M J + H A B V b i G G t S B Q B 8 e 4 R l e H O 4 8 O a / O 2 6 x 1 x Z n P H M I f O O 8 / Y 6 O Q H Q = = &lt; / l a t e x i t &gt; Z 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 5 g R v j l E Z L o R B C 5 x H y v j 1 8 W W K a c = " &gt; A A A B 6 n i c b V D L S g N B E O z 1 G V e j U Y 9 e B k P A U 9 j 1 E H M M C O I x o n l g s o T Z y W w y Z H Z 2 m Y c Q l n y C F w + K e B U /x E / w 5 t 8 4 e R w 0 s a C h q O q m u y t M O V P a 8 7 6 d t f W N z a 3 t 3 I 6 7 u 5 f f P y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>y B 8 / 4 D a 4 m Q I w = = &lt; / l a t e x i t &gt; Z 6 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d P 4 8 8/ T F f Z e h F A f x 2 F H Q l T l 8 S I k = " &gt; A A A B 6 n i c b V D L S s N A F L 2 p r x q t V l 2 6 G S w F V y X p o n Z Z E M R l R f v A N p T J d N I O n U z C z E Q I o Z / g x o U i b s U P 8 R P c + T d O H w t t P X D h c M 6 9 3 H u P H 3 O m t O N 8 W 7 m N z a 3 t n f y u v b d f O D g s H h 2 3 V Z R I Q l s k 4 p H s + l h R z g Rt a a Y 5 7 c a S 4 t D n t O N P L m d + 5 4 F K x S J x p 9 O Y e i E e C R Y w g r W R b u 8 H 1 U G x 5 F S c O d A 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>o 7 g D 6 z 3 H 2 m z k C E = &lt; / l a t e x i t &gt; Z 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C z w 8g B d C b z B M 6 1 4 A a t q U Z + F R K 0 U = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r x q t V j 1 6 W S w F T y V R 1 B 4 L g n i s a D + w D W W z 3 b R L N 5 u w u x F K 6 E / w 4 k E Rr + I P 8 S d 4 8 9 + 4 a X v Q 1 g c D j / d m m J n n x 5 w p 7 T j f V m 5 l d W 1 9 I 7 9 p b 2 0 X d n a L e / t N F S W S 0 A a J e C T b P l a U M 0 E b m m l O 2 7 G k O P Q 5 b f m j y 8 x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>p w D X V o A I E B P M I z v F j c e r J e r b d Z a 8 6 a z x z A H 1 j v P 2 a r k B 8 = &lt; / l a t e x i t &gt;Z 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G v Z d E 7 1 X K t 3 N Z T U 0 9 E P R 6 2 u 3 i P c = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r x q t V j 1 6 W S w F T y U R q T 0 W B P F Y 0 X 5 g G 8 p m u 2 m X b j Z h d y O U 0 J / g x Y M i X s U f 4 k / w 5 r 9 x 0 / a g r Q 8 G H u / N M D P P j z l T 2 n G + r d z a + s b m V n 7 b 3 t k t 7 O 0 X D w 5 b K k o k o U 0 S 8 U h 2 f K w o Z 4 I 2 N d O c d m J J c e h z 2 v b H l 5 n f f q B S s U j c 6 U l M v R A P B Q s Yw d p I t / f 9 8 3 6 x 5 F S c G d A q c R e k V C 9 8 J u U r + 6 P R L 3 7 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Graph estimate for Perturb-seq dataset. The edge from DUSP9 to MAPK1/ETS2 is marked in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The performance of LSCALE-I under noisy scores with varying SNR for n = 5 and d = 25. The dashed vertical lines correspond to SNR values that correspond to the performance attained by SSM-VR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>1. |B| ≥ 3 :</head><label>3</label><figDesc>For any node i ∈ [n], we havepa(i) ⊆ B i ⊆ j∈K\{i} pa(i, j) = pa(i) ∪ j∈K\{i} {j} = pa(i) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>which satisfies the first constraint. Also, using 1{D(h)} = P σ • 1{ D(h)},1{D(h * )} = P I • 1{D(g -1 )} . (222)Therefore, we have1{D(h * )} ⊙ 1{D(h * ) ⊤ } = I n×n . Finally, (h * ) -1 ≜ g • D t (g -1 ; σ) -⊤ satisfies (h * ) -1 • h(X) = X.Hence, (h * , σ) minimizes the objective in (92) at value zero. □C.3 Proof of Lemma 11We will prove the desired result by contradiction. Suppose that (h * , π) is a solution to the optimization problem specified in (92) and let π̸ = σ. Denote f = h * •g, so we have Ẑ = f (Z). If π m = σ m for some m ∈ [n], this means I m = Ĩπm = ℓ for some node ℓ ∈ [n].We follow the same approach in the proof of Theorem 5. Specifically, using score difference transformation property in (40) and one-sparse property of [s m (Z)sπm (Z)] via Lemma 1(iii), we have[D t (h * )] i,m = E s m Ẑ ( Ẑ)sπm Ẑ ( Ẑ) i (223) = E J -⊤ f (Z) i • s m (Z)sπm (Z)(224)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>) Next, consider the set of mismatched nodes A ≜ {I m : I m ̸ = Ĩπm } . (227) Let I a ∈ A be a non-descendant of all the other nodes in A. There exist nodes I b , I c ∈ A, not necessarily distinct, such that I a = Ĩπ b , and I c = Ĩπa .(228)In four steps, we will show that [D(h * )] a,b ̸ = 0 and [D(h * )] b,a ̸ = 0, which violates the constraint 1{D(h * )} ⊙ 1{D ⊤ (h * )} = I n×n and will conclude the proof by contradiction. Before giving the steps, we provide the following argument which we repeatedly use in the rest of the proof. For any continuous function f : R n → R, we haveE f (Z) ̸ = 0 ⇐⇒ E f (Z) • s(Z)s a (Z) I a ̸ = 0 ,(229)andE f (Z) ̸ = 0 ⇐⇒ E f (Z) • s(Z)sπ b (Z) I a ̸ = 0 .(230)First, suppose that E |f (Z)| ̸ = 0. Then, there exists an open set Ψ ⊆ R n for which f (z) ̸ = 0 for all z ∈ Ψ. Due to the interventional discrepancy condition, there exists an open set withinΨ for which [s(Z)s a (Z)] I a ̸ = 0. This implies that E f (Z) • s(Z)s a (Z) I a ̸ = 0 (231)For the other direction, suppose that E f (Z) • s(Z)s a (Z) I a ̸ = 0, which implies that there exists an open set Ψ for which both f (z) and [s(z)s a (z)] I a are nonzero. Then, E |f (Z)| ̸ = 0, and we have (229). Similarly, due to Ĩπ b = I a and interventional discrepancy, we obtain (230).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>I j ,a • s(Z)s a (Z) I j . (233) Note that pa(I a ) ∩ A = {I a } since I a is non-descendant of the other nodes in A. Consider I j ∈ pa(I a ), which implies that I j / ∈ A and I j = Ĩπ j . By (226), we have [J -1 f (Z)] I j ,a = 0. Then, (233) becomesD(h * ) a,a = E J -1 f (Z) I a ,a • s(Z)s a (Z) I a ̸ = 0 ,(234)since diagonal entries of D(h * ) are nonzero due to the last constraint in (92). Then, (229)implies that E [J -1 f (Z)] I a ,a ̸ = 0.Step 2: Show that D(h * ) a,b ̸ = 0. Next, we use I a = Ĩπ b and Lemma 1(i) to obtainD(h * ) a,b = E J -⊤ f (Z) a • s(Z)sπ b (Z) I j ,a • s(Z)sπ b (Z) I j (236) = E J -1 f (Z) I a ,a • s(Z)sπ b (Z) I a .(237)Using (230) and Step 1 result, we have D(h * ) a,b ̸ = 0.Step 3: Show that E [J -1 f (Z)] I a ,b ̸ = 0. Using (40) and Lemma 1(i), we haveD(h * ) b,b = E J -1 f (Z) b • s(Z)sπ b (Z) I j ,b • s(Z)sπ b (Z) I j (239) = E J -1 f (Z)] I a ,b • s(Z)sρc (Z) I a . (240) Since 1{D(h * )} = 1{ D(h * )}, the diagonal entry D(h * ) b,b is nonzero. Then, using(230)we haveE [J -1 f (Z)] I a ,b ̸ = 0.Step 4: Show that D(h * ) b,a ̸ = 0. Next, we use I a = Ĩπ b and Lemma 1(i) to obtainD(h * ) b,a = E J -⊤ f (Z) b • s(Z)s a (Z) (241) = E I j ∈pa(I a ) J -1 f (Z) I j ,b • s(Z)s a (Z) I j (242) = E J -1 f (Z) I a ,b • s(Z)s a (Z) I a .(243)Using (229) and Step 3 result, we have [D(h * )] b,a ̸ = 0. Finally, using the constraint 1{D(h * )} = 1{ D(h * )}, Step 2 implies that [D(h * )] a,b ̸ = 0. Combining with Step 4 result, we have [D(h * ) ⊙ D ⊤ (h * )] a,b ̸ = 0, which violates the last constraint in (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>and fi (φ) = ν⊤ • σ( Wi • φ) + ν 0 = wi j=1 νj • σ( Wi j • φ) + ν0 ,(262)in which activation function σ is applied element-wise.Lemma 7 Consider NNs f i and fi specified in (261) and (262). If for all i ∈ [n] we have max{rank(W i ) , rank( Wi )} = |pa(i)| ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Parametric Settings. Comparison of the results to prior studies in parametric settings.Only the main results of the most closely related studies are listed, and standard shared assumptions are omitted. Formal definitions of identifiability measures are provided in Section 3.</figDesc><table><row><cell>Work</cell><cell>Transform</cell><cell>Latent</cell><cell>Int. Data</cell><cell>Identifiability</cell></row><row><cell></cell><cell></cell><cell>Model</cell><cell>(one env. per node)</cell><cell>result</cell></row><row><cell>Squires et al. (2023)</cell><cell>Linear</cell><cell>Linear</cell><cell>Hard</cell><cell>perfect</cell></row><row><cell></cell><cell>Linear</cell><cell>Linear</cell><cell>Soft</cell><cell>up to ancestors</cell></row><row><cell>Ahuja et al. (2023)</cell><cell>Polynomial</cell><cell>General</cell><cell>do</cell><cell>perfect</cell></row><row><cell></cell><cell>Polynomial</cell><cell>Bounded RV</cell><cell>Soft</cell><cell>perfect</cell></row><row><cell>Buchholz et al. (2023)</cell><cell>General</cell><cell>Lin. Gaussian</cell><cell>Hard</cell><cell>perfect</cell></row><row><cell></cell><cell>General</cell><cell>Lin. Gaussian</cell><cell>Soft</cell><cell>up to ancestors</cell></row><row><cell>Zhang et al. (2023)</cell><cell>Polynomial</cell><cell>nonlinear</cell><cell>Soft</cell><cell>up to ancestors</cell></row><row><cell></cell><cell cols="2">Polynomial nonlinear (polytree)</cell><cell>Soft</cell><cell>perfect</cell></row><row><cell>Theorem 3</cell><cell>Linear</cell><cell>General</cell><cell>Hard</cell><cell>perfect</cell></row><row><cell>Theorem 2</cell><cell>Linear</cell><cell>General</cell><cell>Soft</cell><cell>up to ancestors</cell></row><row><cell>Theorem 4</cell><cell>Linear</cell><cell>nonlinear</cell><cell>Soft</cell><cell>perfect DAG and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>mixing w. surrounding</cell></row><row><cell>models are studied in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>General (Nonparametric) Settings. Comparison of the results to prior studies in the general setting. Formal definitions of identifiability measures are provided in Section 3. The shared assumptions (interventional discrepancy) and additional assumptions (faithfulness) are discussed in Section 6.</figDesc><table><row><cell>Work</cell><cell cols="2">Transform and Obs.</cell><cell>Int. Data</cell><cell>Faithfulness</cell><cell>Identifiability</cell><cell>Provable</cell></row><row><cell></cell><cell cols="3">Latent Model Data (env. per node)</cell><cell></cell><cell>result</cell><cell>algorithm</cell></row><row><cell>von Kügelgen et al. (2023)</cell><cell>General</cell><cell>No</cell><cell>2 coupled hard</cell><cell>Yes</cell><cell>perfect</cell><cell>✘</cell></row><row><cell>Jin and Syrgkanis (2024)</cell><cell>General</cell><cell>No</cell><cell>2 coupled soft</cell><cell>No</cell><cell>perfect DAG and</cell><cell>✘</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mix w. surrounding</cell><cell></cell></row><row><cell>Theorem 6</cell><cell>General</cell><cell>Yes</cell><cell>2 coupled hard</cell><cell>No</cell><cell>perfect</cell><cell>✔</cell></row><row><cell>Theorem 7</cell><cell>General</cell><cell>No</cell><cell>2 coupled hard</cell><cell>Yes</cell><cell>perfect</cell><cell>✔</cell></row><row><cell>Theorem 8</cell><cell>General</cell><cell cols="2">Yes 2 uncoupled hard</cell><cell>No</cell><cell>perfect</cell><cell>✔</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>□</head><label></label><figDesc>Algorithm 1 Linear Score-based Causal Latent Estimation via Interventions (LSCALE-I)</figDesc><table><row><cell cols="2">1: Input: R m X for all m ∈ [n]</cell><cell>▷ compute using (45)</cell></row><row><cell cols="3">2: Stage L1: Encoder estimation</cell></row><row><cell cols="2">3: Ĥ ← 0 n×d 4: for m ∈ (1, . . . , n) do 5: Select any h ∈ col(R m X ) 6: Ĥm ← h ⊤</cell><cell></cell></row><row><cell cols="3">7: Stage L2: Latent graph estimation</cell></row><row><cell>10:</cell><cell cols="2">Stage L3: Unmixing procedure</cell></row><row><cell>11: 12:</cell><cell>π ← a topological order of</cell><cell>Ĝ</cell></row></table><note><p><p>8: Construct latent DAG estimate Ĝ with parent sets pa(m) ← i ̸ = m : E d m Ẑ ( Ẑ; Ĥ) i ̸ = 0 , ∀m ∈ [n]</p>9: if the interventions are hard then</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Input: H, samples of X from environment E 0 and coupled environments {(E m , Ẽm ) : m ∈ [n]} 2: Compute score functions: s X , s m X , and sm</figDesc><table><row><cell cols="2">Algorithm 3 Generalized Score-based Causal Latent Estimation via Interventions</cell></row><row><cell>(GSCALE-I)</cell><cell></cell></row><row><cell></cell><cell>which concludes the</cell></row><row><cell>proof.</cell><cell>□</cell></row></table><note><p>1: X for all m ∈ [n]. 3: Stage G1: Encoder estimation:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>LSCALE-I for a linear causal model with one hard intervention per node (varying n s )</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>perfect scores</cell><cell></cell><cell></cell><cell>noisy scores</cell><cell></cell></row><row><cell>n</cell><cell>d</cell><cell>ns</cell><cell>MCC</cell><cell>ℓ scale</cell><cell>SHD(G, Ĝ)</cell><cell>MCC</cell><cell>ℓ scale</cell><cell>SHD(G, Ĝ)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>LSCALE-I for a linear causal model with one hard intervention per node(varying d)    </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>perfect scores</cell><cell></cell><cell></cell><cell>noisy scores</cell><cell></cell></row><row><cell>n</cell><cell>d</cell><cell>ns</cell><cell>MCC</cell><cell>ℓ scale</cell><cell>SHD(G, Ĝ)</cell><cell>MCC</cell><cell>ℓ scale</cell><cell>SHD(G, Ĝ)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>LSCALE-I for a quadratic causal model with one hard intervention per node (n s = 50000).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>perfect scores</cell><cell></cell><cell></cell><cell>noisy scores</cell><cell></cell></row><row><cell>n</cell><cell>d</cell><cell>MCC</cell><cell>ℓ scale</cell><cell>SHD(G, Ĝ)</cell><cell>MCC</cell><cell>ℓ scale</cell><cell>SHD(G, Ĝ)</cell></row><row><cell cols="8">5 100 1.00 ± 0.00 0.01 ± 0.00 0.03 ± 0.02 0.93 ± 0.01 0.69 ± 0.02 2.62 ± 0.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>LSCALE-I for an MLP causal model with one hard intervention per node (n s = 50000).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>perfect scores</cell><cell></cell><cell></cell><cell>noisy scores</cell><cell></cell></row><row><cell>n</cell><cell>d</cell><cell>MCC</cell><cell>ℓ scale</cell><cell>SHD(G, Ĝ)</cell><cell>MCC</cell><cell>ℓ scale</cell><cell>SHD(G, Ĝ)</cell></row><row><cell cols="8">5 100 1.00 ± 0.00 0.03 ± 0.00 0.01 ± 0.01 0.94 ± 0.01 0.62 ± 0.02 4.27 ± 0.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>LSCALE-I for a linear causal model with one soft intervention per node.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">perfect scores</cell><cell></cell><cell>noisy scores</cell><cell></cell></row><row><cell>n</cell><cell>d</cell><cell>ns</cell><cell>MCC</cell><cell>ℓpa</cell><cell>SHD(Gtc, Ĝtc)</cell><cell>MCC</cell><cell>ℓpa</cell><cell>SHD(Gtc, Ĝtc)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>LSCALE-I for a quadratic causal model with one soft intervention per node (n s = 50000).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>perfect scores</cell><cell></cell><cell></cell><cell>noisy scores</cell><cell></cell></row><row><cell>n</cell><cell>d</cell><cell>MCC</cell><cell>ℓ sur</cell><cell>SHD(G, Ĝ)</cell><cell>MCC</cell><cell>ℓ sur</cell><cell>SHD(G, Ĝ)</cell></row><row><cell cols="8">5 100 0.87 ± 0.01 0.34 ± 0.04 0.53 ± 0.09 0.60 ± 0.01 0.51 ± 0.03 2.79 ± 0.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>GSCALE-I for a quadratic causal model with two coupled hard interventions per node. Noisy scores are obtained using SSM-VR with n score = 30000 samples.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>expected num.</cell><cell cols="2">perfect scores</cell><cell cols="2">noisy scores</cell></row><row><cell>n</cell><cell>d</cell><cell>n s</cell><cell>edges in G</cell><cell>MCC</cell><cell>SHD(G, Ĝ)</cell><cell>MCC</cell><cell>SHD(G, Ĝ)</cell></row><row><cell cols="3">5 100 200</cell><cell>5</cell><cell cols="4">1.00 ± 0.00 0.00 ± 0.00 0.85 ± 0.02 4.50 ± 0.38</cell></row><row><cell cols="3">8 100 500</cell><cell>14</cell><cell cols="4">0.95 ± 0.01 1.50 ± 0.27 0.75 ± 0.02 12.9 ± 0.44</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>MCC across different runs in MLP transform + MLP causal model setting</figDesc><table><row><cell>Runs</cell><cell>Mean (std. error)</cell></row><row><cell>0.64 0.54 0.43 0.58 0.54</cell><cell>0.54 ± 0.03</cell></row><row><cell cols="2">we consider target dimension value d = 5, and we generate 5 latent graphs and n s = 10000</cell></row><row><cell>samples per graph.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Comparison of LSCALE-I with the algorithm of Squires et al. (2023) for a linear causal model with one hard intervention per node.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>LSCALE-I</cell><cell>(Squires et al., 2023)</cell></row><row><cell>n</cell><cell>d</cell><cell>n s</cell><cell cols="3">mean ℓ scale fraction of ℓ scale &lt; 0.1 mean ℓ scale fraction of ℓ scale &lt; 0.1</cell></row><row><cell cols="2">5 100</cell><cell></cell><cell>0.06</cell><cell>0.92</cell><cell>0.66</cell><cell>0.67</cell></row><row><cell cols="3">5 100 10000</cell><cell>0.04</cell><cell>0.96</cell><cell>0.51</cell><cell>0.74</cell></row><row><cell cols="3">5 100 50000</cell><cell>0.03</cell><cell>0.94</cell><cell>0.24</cell><cell>0.88</cell></row><row><cell cols="2">8 100</cell><cell></cell><cell>0.08</cell><cell>0.86</cell><cell>1.28</cell><cell>0.36</cell></row><row><cell cols="3">8 100 10000</cell><cell>0.06</cell><cell>0.95</cell><cell>1.09</cell><cell>0.45</cell></row><row><cell cols="3">8 100 50000</cell><cell>0.03</cell><cell>0.96</cell><cell>0.68</cell><cell>0.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>Representative target genes from super nodes.</figDesc><table><row><cell cols="2">Super node Representative</cell></row><row><cell>index</cell><cell>gene(s)</cell></row><row><cell></cell><cell>OSR2</cell></row><row><cell></cell><cell>TBX2</cell></row><row><cell></cell><cell>DUSP9</cell></row><row><cell></cell><cell>MAPK1, ETS2</cell></row><row><cell></cell><cell>COL2A1</cell></row><row><cell></cell><cell>SET</cell></row><row><cell></cell><cell>KLF1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>on Perturb-seq dataset.</figDesc><table><row><cell></cell><cell>Single-node</cell><cell>Double-node</cell></row><row><cell>Score-based DiscrepancyVAE</cell><cell cols="2">0.057 ± 0.013 0.208 ± 0.036 ≥ 0.15 ≥ 0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 14 :</head><label>14</label><figDesc>MCC comparison in image experiments (over 5 runs).</figDesc><table><row><cell>Algorithm</cell><cell>SCM</cell><cell cols="4"># balls # int. / node int. type mean (std. error)</cell></row><row><cell>GSCALE-I GSCALE-I GSCALE-I</cell><cell>linear linear nonlinear</cell><cell>2 3 2</cell><cell>2 2 2</cell><cell>hard hard hard</cell><cell>0.80 ± 0.03 0.76 ± 0.08 0.93 ± 0.02</cell></row><row><cell>GSCALE-I GSCALE-I</cell><cell>linear nonlinear</cell><cell>2 2</cell><cell>1 1</cell><cell>hard hard</cell><cell>0.79 ± 0.03 0.92 ± 0.02</cell></row><row><cell>Ahuja et al. (2023) Ahuja et al. (2023) Ahuja et al. (2023)</cell><cell>linear linear linear</cell><cell>2 2 2</cell><cell>1 3 5</cell><cell>do do do</cell><cell>0.13 ± 0.03 0.73 ± 0.03 0.83 ± 0.03</cell></row><row><cell>Buchholz et al. (2023) Buchholz et al. (2023)</cell><cell>linear linear</cell><cell>2 5</cell><cell>1 1</cell><cell>hard hard</cell><cell>0.87 ± 0.03 0.94 ± 0.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 15 :</head><label>15</label><figDesc>Notation. : parents of node i in G ch(i) : children of node i in G an(i) : ancestors of node i in G de(i) : descendants of node i in G X : [X 1 , . . . , X d ] ⊤ observed random variables Z : [Z 1 , . . . , Z n ] ⊤ latent random variables Z pa(i) : vector formed by Z i for all i ∈ pa(i) : the intervened node(s) in E mĨm : the intervened node(s) in Ẽm I : the set of intervened nodes (I 1 , . . . , I n ) Ĩ : the set of intervened nodes ( Ĩ1 , . . . , Ĩn ) Ẽm s, s m , sm : score functions of Z in E 0 , E m , and Ẽm s X , s m X , sm in E 0 , E m , and Ẽm for encoder h pa(i) : parents of node i in Ĝ matrix A † : Pseudo-inverse of matrix A notations A i : row i of matrix A A i,j : entry of matrix A at row i and column j P π : Permutation matrix associated with permutation π of [n] D, D, D t : True score change matrices D(h), D(h), D t (h) : Score change matrices under encoder h Appendix A. Proofs of Score Function Properties and Transformations</figDesc><table><row><cell></cell><cell>[n] : {1, . . . , n} 1 : indicator function</cell></row><row><cell>ground-truth variables</cell><cell>G : latent causal graph over Z G tc : transitive closure of G G tr : transitive reduction of G pa(i) g : true decoder</cell></row><row><cell></cell><cell>h : a valid encoder</cell></row><row><cell></cell><cell>H : the set of valid encoders h G : true linear decoder</cell></row><row><cell></cell><cell>H : a valid linear encoder</cell></row><row><cell>intervention</cell><cell>E</cell></row></table><note><p>0 : observational environment notations E : (E 1 , . . . , E n ) first interventional environments Ẽ : ( Ẽ1 , . . . , Ẽn ) second interventional environments I m statistical Ẑ(X) : generic estimator of Z given X models Ẑ(X; h) : an auxiliary estimator of Z given X and encoder h Ĝ : estimate of G p, p m , pm : pdfs of Z in E 0 , E m , and Ẽm p X , p m X , pm X : pdfs of X in E 0 , E m , and X : score functions of X in E 0 , E m , and Ẽm s Ẑ, s m Ẑ , sm Ẑ : score functions of Ẑ</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 16 :</head><label>16</label><figDesc>Choice of thresholding parameter λ G for latent graph estimation.Implementation details of Algorithm 2. The latent graph estimation part of Algorithm 2 involves rank tests, i.e., determining the dimension of the intersection of column spaces. Therefore, we also need a threshold, denoted by λ eigv to determine the numerical ranks in practice. In experiments reported in Table8, we set λ eigv = 0.01.Using the algorithms of related work. For the comparisons in Section 7.3, we used the shared sources codes of<ref type="bibr" target="#b46">Squires et al. (2023)</ref> 8 and Zhang et al. (2023) 9 .</figDesc><table><row><cell cols="3">Experiment perfect scores noisy scores</cell></row><row><cell>Table 3</cell><cell>0.001</cell><cell>0.1</cell></row><row><cell>Table 6</cell><cell>0.001</cell><cell>0.05</cell></row><row><cell>Table 7</cell><cell>0.0001</cell><cell>0.001</cell></row><row><cell>Table 8</cell><cell>0.001</cell><cell>0.1</cell></row><row><cell>Table 9</cell><cell>0.01</cell><cell>0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>Table 18 details the classificationbased score difference network we trained. Finally, Table 19 lists the first level CNN-based autoencoder we used for image experiments with 2 balls.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 17 :</head><label>17</label><figDesc>2-step autoencoder architecture for learning latent representations of images. Autoencoder-2 Training: 100 epochs, Adam optimizer, batch size: 16, weight decay: 0.01 Learning rate: 0.95 decay/epoch for 75 epochs, reset to 10 -3 for 25 epochs Minimize ∥D t (h) -I n×n ∥ 1,1 + λE ∥Y -Ŷ∥ 2 ▷ score loss + recons. loss</figDesc><table><row><cell>Step</cell><cell>Details</cell><cell></cell></row><row><cell cols="2">Step 1 Encoder Input: Image X ∈ R 64×64×3 Flatten</cell><cell></cell></row><row><cell></cell><cell>FC(256), ReLU, LayerNorm</cell><cell></cell></row><row><cell></cell><cell>FC(64)</cell><cell cols="2">▷ Intermediate representation Y</cell></row><row><cell cols="2">Step 1 Decoder FC(256), ReLU, LayerNorm</cell><cell></cell></row><row><cell></cell><cell>FC(64 × 64 × 3), Sigmoid</cell><cell cols="2">▷ Reconstructed image Xc</cell></row><row><cell cols="3">Autoencoder-1 Training: 100 epochs, Adam optimizer, batch size:16</cell></row><row><cell></cell><cell>Learning rate: 10 -3 , weight decay: 0.01</cell><cell></cell></row><row><cell cols="2">Minimize E ∥X -Xc ∥ 2 Step 2 Encoder Input: Y ∈ R 64 FC(256), ReLU, LayerNorm</cell><cell cols="2">▷ recons. loss</cell></row><row><cell></cell><cell>FC(6)</cell><cell>▷ Latent causal variables</cell><cell>Ẑ</cell></row><row><cell cols="2">Step 2 Decoder FC(256), ReLU, LayerNorm</cell><cell></cell></row><row><cell></cell><cell>FC(64)</cell><cell>▷ Reconstructed</cell><cell>Ŷ</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 18 :</head><label>18</label><figDesc>Architecture for LDR (log-density-ratio) model for score-difference estimation.</figDesc><table><row><cell>Layers:</cell><cell>Input: Image X ∈ R 64×64×3 , class (intervention) label y ∈ {0, 1} Conv(3, 32, kernel size=3, stride=1, padding=1), ReLU, BatchNorm, Dropout(0.1)</cell></row><row><cell></cell><cell>MaxPool(kernel size=2, stride=2)</cell></row><row><cell></cell><cell>Conv(32, 64, kernel size=3, stride=1, padding=1), ReLU, BatchNorm, Dropout(0.1)</cell></row><row><cell></cell><cell>MaxPool(kernel size=2, stride=2)</cell></row><row><cell></cell><cell>Flatten, FC(128), ReLU</cell></row><row><cell></cell><cell>FC(1)</cell></row><row><cell cols="2">Training: 10 epochs, Adam optimizer, weight decay: 0.01, batch size: 16</cell></row><row><cell></cell><cell>Learning rate: 10 -5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Transitive closure of a DAG G, denoted by Gtr, is a DAG with parents denoted by pa tr (i) = an(i) for each node i. The transitive reduction of G is the DAG with the fewest edges that preserves the same reachability relation as G.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>It is also called topological ordering or topological sort in the literature.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Surrounded node concept is first defined by<ref type="bibr" target="#b50">(Varıcı et al., 2023)</ref>, and later adopted by<ref type="bibr" target="#b18">Jin and Syrgkanis (2024)</ref> when considering soft interventions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>The "linear interventional faithfulness"<ref type="bibr" target="#b59">(Zhang et al., 2023</ref>, Assumption 2) implies nonlinearity, which we elaborate in Appendix D.3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>The codebase for the algorithms and simulations is available at: https://github.com/acarturk-e/score-based-crl.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>This work was supported in part by <rs type="funder">IBM</rs> through the <rs type="funder">IBM-Rensselaer Future of Computing Research</rs> and the <rs type="funder">National Science Foundation</rs> under Award <rs type="grantNumber">ECCS-193310</rs>.   </p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MrfuPg5">
					<idno type="grant-number">ECCS-193310</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p 8 I Q X s c X U z s m B i d T X 4 Q w q h f t z Y Y = " &gt; A A A C B 3 i c b V D L S g M x F M 3 U V 6 2 v U Z e C B F v B V Z m p W o u r g h u X F e w D 2 q F k 0 o w N z W S G 5 I 5 Q S n d u / B U 3 L h R x 6 y + 4 8 2 9 M 2 w G 1 9 U D g c M 6 9 n N z j x 4 J r c J w v K 7 O 0 v L K 6 l l 3 P b W x u b e / Y u 3 s N H S W K s j q N R K R a P t F M c M n q w E G w V q w Y C X 3 B m v 7 g a u I 3 7 5 n S P J K 3 M I y Z F 5 I 7 y Q N O C R i p a x 9 y I 7 B L X C i f 4 Q 7 w k G n 8 w 0 4 L X T v v F J 0 p 8 C J x U 5 J H K W p d + 7 P T i 2 g S M g l U E K 3 b r h O D N y I K O B V s n O s k m s W E D k x o 2 1 B J T I 4 3 m t 4 x x s d G 6 e E g U u Z J w F P 1 9 8 a I h F o P Q 9 9 M h g T 6 e t 6 b i P 9 5 7 Q S C i j f i M k 6 A S T o L C h K B I c K T U n C P K 0 Z B D A 0 h V H H z V 0 z 7 R B E K p r q c K c G d P 3 m R N E p F t 1 w 8 v y n l q 5 W 0 j i w 6 Q E f o B L n o A l X R N a q h O q L o A T 2 h F / R q P V r P Z 2 R n &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C d F m 4 X A G O z K l D a N P 5 W t W p V H W r Y k = " &gt; A A A C A H i c b V C 7 T s M w F H X K q 5 R X g I G B x a J F Y q q S C k r H S i y M B d E H a k L l u E 5 r 1 X E i 2 0 G q o i z 8 C g s D C L H y G W z 8 D U 6 b A   schedule are given in Table <ref type="table">17</ref>. Finally, we repeat the training of the second autoencoder 5 times for each dataset.</p><p>Score difference estimation. The main loss function for the second autoencoder of our setup is ∥D t (h) -I n×n ∥ 1,1 , which is equivalent to the score-based loss in (83). For computing D t (h) for an encoder h, we need to compute the score differences of image datasets. To this end, we adopt a binary classifier-based log density ratio (LDR) estimator and use the gradients of these learned LDRs as our score difference functions, as described earlier. Specifically, we parameterize the LDR at any image through a CNN-based model, details of which are given in Table <ref type="table">18</ref>. The output of this model is used to compute class probabilities, which are used to minimize cross-entropy to train the model. We train one LDR model for each pair of hard interventions on the same node and one for the observational-interventional environment pairs for one set of hard interventions.</p><p>Results. GSCALE-I ensures perfect graph recovery and latent variables recovery up to permutation and element-wise scaling. Similarly to related work <ref type="bibr" target="#b2">(Ahuja et al., 2023;</ref><ref type="bibr" target="#b7">Buchholz et al., 2023)</ref>, we report MCC between Ẑ and Z as a metric of latent variables recovery, shown in Table <ref type="table">14</ref>. During training, we observed that MCC highly correlates with the reconstruction performance, i.e., when reconstruction errors in both autoencoders converge to small values, latent variable recovery is also successful. We note that the reconstruction does not have </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table of Contents</head><p>Substituting this into (129), we obtain</p><p>Since k is a parent of ℓ, there exists a fixed Z pa(ℓ) = z * pa(ℓ) realization for which ∂f ℓ (z * pa(ℓ) )/∂z k is nonzero. Otherwise, f ℓ (z pa(ℓ) ) would not be sensitive to z k which is contradictory to k being a parent of ℓ. Note that Z ℓ can vary freely after fixing Z pa(ℓ) . Therefore, for (132) to hold, the derivative of r p must always be zero. However, the score function of a valid pdf with full support cannot be constant. Therefore, s m (z) ism (z) i is not always zero, and we have</p><p>We investigate case by case and reach a contradiction for each case. First, suppose that i / ∈ pa(ℓ). Then, we have i ∈ pa(j), and ( <ref type="formula">133</ref>) becomes</p><p>If i = j, the impossibility of (134) directly follows from the impossibility of (128). The remaining case is i ∈ pa(j). In this case, taking the derivative of the right-hand side of (134) with respect to z j , we obtain</p><p>which is a realization of (129) for i ∈ pa(j) and j in place of k ∈ pa(ℓ) and ℓ, which we proved to be impossible in i = ℓ case. Therefore, i / ∈ pa(ℓ) is not viable. Finally, suppose that i ∈ pa(ℓ). Then, taking the derivative of the right-hand side of (133) with respect to z ℓ , we obtain</p><p>which is again a realization of (129) for k = i, which we proved to be impossible.</p><p>Hence, we showed that s m (z)-s m (z) i cannot be zero for all z values. Then, by Proposition 2 we have E |s m (Z)sm (Z)| i ̸ = 0, and the proof is concluded.</p><p>fixed realization n i = n * i and denote the value of the left-hand side (LHS) for n * i by α. By defining u ≜ δ(z pa(i) ), we get</p><p>This is only possible if r q is an exponential function, i.e., r q (u) = k 1 exp(αu) over interval u ∈ Θ. Since r q is an analytic function, it is, therefore, exponential over entire R. Then, the associated pdf must have the form q N (u) = k 2 exp((k 1 /α) exp(αu)). However, the integral of this function over the entire domain R diverges. Hence, it is not a valid pdf, rendering a contradiction. Hence, the additive noise model satisfies interventional regularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Proofs of the Results for Linear Transformations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Auxiliary Results</head><p>First, we provide a linear algebraic property, which will be used in the proofs.</p><p>Lemma 13 Consider the latent causal graph G, and a matrix L ∈ R n×n .</p><p>1. Let L pa and L an be binary matrices that denote the parental and ancestral relationships in G, respectively, i.e.,</p><p>Then, if I n×n ≼ 1{L} ≼ L pa , we also have</p><p>2. Let L sur be a binary matrix that denotes the surrounding relationships in G, i.e.,</p><p>Then, if I n×n ≼ 1{L} ≼ L sur , we also have</p><p>Proof: First, note that L sur ≼ L pa ≼ L an . Hence, we start with considering a generic lower triangular matrix L such that I n×n ≼ 1{L} ≼ L pa . Matrix L can be decomposed as</p><p>in which E is a diagonal matrix and Λ is a strictly lower triangular matrix that satisfies 1{Λ} ≼ L pa . Subsequently, since E is a diagonal matrix, we have</p><p>Note that Λ is a strictly lower triangular n × n matrix, which implies that Λ n is a zero matrix. Therefore, the inverse of (I n×n + Λ) can be expanded as</p><p>in which all terms are nonzero. Next, we prove the two cases as follows.</p><p>Case 1:</p><p>Therefore, we conclude that</p><p>Hence,</p><p>which implies [L sur ] i,j = 1 since ch(i) ⊆ ch(j). Therefore, we conclude that if</p><p>Hence,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Theorem 3</head><p>Proof of the scaling consistency. First, by Theorem 2, encoder estimate Ĥ at the end of Stage L1 of Algorithm 1 satisfies</p><p>where P I is the permutation matrix of the intervention order (I 1 , . . . , I n ), L has nonzero diagonal entries and satisfies L i,j = 0 for all j / ∈ pa(i). We will show that the output of Stage L3 of Algorithm 1 for hard interventions satisfies that</p><p>which will imply scaling consistency as</p><p>where C s is a constant diagonal matrix with nonzero diagonal entries. We prove this as follows.</p><p>First, consider the topological order π of Ĝ. By Theorem 2, transitive closures of Ĝ and G are the same under relabeling of the nodes with permutation (1, . . . , n) → (I 1 , . . . , I n ).</p><p>Proof of the perfect DAG recovery. We show that the graph construction process in (59) achieves perfect DAG recovery as follows. First, using Lemma 2 and (190), we have</p><p>Subsequently, using the fact that C s is a diagonal matrix, for all i, m ∈ [n] we obtain</p><p>Then, Lemma 1 and (59), we have</p><p>This concludes the proof that Ĝ and G are related through a graph isomorphism by permutation I, which denotes the intervention order. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Proof of Theorem 4</head><p>Latent graph estimation. We continue from the intuitions provided in Section 5.4 and prove the perfect recovery of the latent graph as follows. First, recall that output Ĝ of Algorithm 1 for soft interventions satisfies that transitive closures of Ĝ and G are the same under permutation (I 1 , . . . , I n ). Since (1, . . . , n) is assumed to be a topological order of G, ρ ≜ (I π 1 , . . . , I πn ) is also a topological order of G. Then, for any k = π u and</p><p>By induction, we will prove that the parent sets generated by Algorithm 2 satisfy</p><p>At the base case, consider k = π 2 . The only possible parent is t = π 1 . Since ρ 1 is a root node in G, (194) implies</p><p>Since pa(π 1 ) = pa(π 2 ) = ∅ at this step, the algorithm adds π 1 to pa(π 2 ) if and only if ρ 1 ∈ pa(ρ 2 ), and ( <ref type="formula">195</ref>) is satisfied for k = π 2 . Next, as the induction step, assume that the algorithm output pa(k) satisfies (195) for k ∈ {π 1 , . . . , π u-1 }. We prove that k = π u also satisfies the condition by induction again. At the base case, consider t = π 1 . Similar to the previous base case, <ref type="bibr">(194)</ref> implies</p><p>and since pa(π u ) does not contain π 1 yet, the algorithm adds π 1 to pa(π u ) if and only if ρ 1 ∈ pa(ρ u ). Next, as the induction step, assume that for π j ∈ {π 1 , . . . , π v-1 } where v &lt; u, the algorithm correctly identified the parents, i.e., π j ∈ pa(π u ) if and only if ρ j ∈ pa(ρ u ).</p><p>By the induction hypothesis, we know that</p><p>Therefore, the algorithm adds π v to pa(π v ) if and only if ρ v ∈ pa(ρ u ). Therefore, by the inner induction, pa(k) satisfies <ref type="bibr">(195)</ref>. Then, by the outer induction, ( <ref type="formula">195</ref>) is satisfied for all parent set estimates, which concludes the proof of perfect graph recovery. □</p><p>Markov property of consistency up to mixing with surrounding parents. Let Ẑ denote Ẑ(X; Ĥ). We further investigate the properties of consistency up to mixing with surrounding parents to prove that Ẑ is Markov with respect to Ĝ. Using the results of the encoder estimation step, we have</p><p>where C sur has nonzero diagonal entries and satisfies [C sur ] i,j = 0 for all j / ∈ sur(i). Recall the following SCM specified in (6)</p><p>Let τ be the permutation that maps {1, . . . , n} to I, i.e., <ref type="bibr">199)</ref> implies</p><p>which is a function of Z pa(i) and N i . To prove Markov property, we need to specify Z τ i in terms of Ẑ pa(τ i ) . To this end, it suffices to show that for any k ∈ pa(i), Z k is a function of Ẑ pa (τ i ). Using (199), we have</p><p>By Lemma 13, we know that [C -1 sur ] i,j = 0 if j / ∈ sur(i) for distinct i and j. Subsequently, as counterpart of (201), we have</p><p>Since k ∈ pa(i), we have τ k ∈ pa(τ i ) and Ẑτ k is in Ẑ pa(τ i ) . Note that if j ∈ sur(k), j is also in pa(i). Therefore, every term in the RHS of (204) belongs to Ẑ pa(τ i ) and Z k is a function of Ẑ pa(τ i ) . Then, using (202), we know that Ẑτ i is a function of only Ẑ pa(τ i ) and N i , which concludes the proof that Ẑ is Markov with respect to Ĝ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Proofs of the Results for General Transformations</head><p>The proof of Theorem 6 is already given in Section 6.2. In this section, we first prove Theorem 7, that is, identifiability for coupled interventions without observational environment. Then, we prove the results in Section 6.3 related to the uncoupled interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Proof of Theorem 7</head><p>We will show that if p(Z) is adjacency-faithful to G and the latent causal model is an additive noise model, then we can recover G without having access to observational environment E 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">|B| = 2:</head><p>The two nodes in B are root nodes. If there were at least three root nodes, we would have at least three nodes in B. Hence, the two nodes in B are the only root nodes. Subsequently, every i / ∈ B is also not in K and we have</p><p>Hence, B i = pa(i) for every non-root node i and we already have the two root nodes in B, which completes the graph recovery.</p><p>3. |B| ≤ 1: First, consider all (i, j) pairs such that |pa(i, j)| = 2. For such an (i, j) pair, at least one of the nodes is a root node; otherwise pa(i, j) would contain a third node. Using these pairs, we identify all root nodes as follows. Note that a hard intervention on node i makes Z i independent of all of its non-descendants, and all conditional independence relations are preserved under componentwise diffeomorphisms such as f . Then, using the adjacency-faithfulness assumption, we infer that</p><p>• if Ẑi ⊥ ⊥ Ẑj in E i and Ẑi ⊥ ⊥ Ẑj in Ẽj , then both i and j are root nodes.</p><p>• if Ẑi ̸ ⊥ ⊥ Ẑj in E i , then i → j and i is a root node.</p><p>• if Ẑi ̸ ⊥ ⊥ Ẑj in Ẽj , then j → i and j is a root node.</p><p>This implies that we can determine whether i and j nodes are root nodes by using at most two independence tests. Hence, we identify all root nodes by using at most n independence tests. We also know that there are at most two root nodes. If we have two root nodes, then B i = pa(i) for all non-root nodes, and the graph is recovered. If we have only one root node i, then for any j ̸ = i we have pa(j) ⊆ B j ⊆ pa(i, j) = pa(j) ∪ {i} .</p><p>Finally, if Ẑj ⊥ ⊥ Ẑi | { Ẑℓ : ℓ ∈ B j \ {i}} in Ẽj , we have i / ∈ pa(j) due to adjacencyfaithfulness. Otherwise, we conclude that i ∈ pa(j). Hence, an additional (n -1) conditional independence tests ensure the recovery of all pa(j) sets, and the graph recovery is complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proof of Lemma 10</head><p>Let us start by scrutinizing the constraints. For the true encoder g -1 , Lemma 1 gives us</p><p>and</p><p>Therefore, using Ĩ = σ • I, we have 1{ D(g -1 )} = 1{D(g -1 )} • P σ . Note that acyclicity of graph G also implies that 1{D(g -1 )} ⊙ 1{D(g -1 ) ⊤ } = I n×n . Next, note that D t (g -1 ; σ) is equal to the true score change matrix D t (g -1 ) defined for the coupled interventions, which satisfies 1{D t (g</p><p>Appendix D. Analysis of the Assumptions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Analysis of Assumption 1</head><p>In this subsection, we prove Lemma 6 statement, i.e., Assumption 1 is satisfied for additive noise models under hard interventions, as follows. Consider a hard interventional environment E m and let I m = i be the intervened node. Recall (29), which becomes</p><p>for the hard intervention on node i and parent node k ∈ pa(i). Then, the ratio in Assumption 1 is given by</p><p>We will prove the desired result by contradiction. Assume that there exists a nonzero constant</p><p>Additive noise model. The additive noise model for node i is given by Z i = f i (Z pa(i) )+N i as specified in (7). When node i is hard intervened, Z i is generated according to Z i = Ni in which Ni denotes the exogenous noise term for the interventional model. Then, denoting the pdfs of N i and Ni by p N and q N , respectively, we have</p><p>Denote the score functions associated with p N and q N by</p><p>, and r q (u)</p><p>Define n i and ni as the realizations of N i and Ni when Z i = z i and Z pa(i) = z pa(i) . Then, we have ni = n i + f i (z pa(i) ). Using ( <ref type="formula">246</ref>) and ( <ref type="formula">247</ref>), we can express ( <ref type="formula">245</ref>) as</p><p>We scrutinize (249) in two cases.</p><p>Case 1:</p><p>Then, the RHS of ( <ref type="formula">249</ref>) is also constant for all z pa(i) ∈ R |pa(i)| and n i ∈ R. Fix a realization n i = n * i and note that f i (z pa(i) ) needs to be constant, denoted by δ * , for all z pa(i) ∈ R |pa(i)| . However, this implies that ∂f i (z pa(i) )</p><p>∂z k = 0, and we have r q (n i + δ * ) = r p (n i ) for all n i ∈ R. This implies that p N (n i ) = η * q N (n i + δ * ) for some constant η * . Since p N and q N are pdfs, the only choice is η * = 1 and p N (n i ) = q N (n i + δ * ), then p i (z i | z pa(i) ) = q i (z i ), which contradicts the premise that an intervention changes the causal mechanism of the target node i.</p><p>Case 2:</p><p>∂z k is not constant. In this case, note that LHS of ( <ref type="formula">249</ref>) is not a function of n i . Then, taking the derivative of both sides with respect to n i and rearranging, we obtain</p><p>Next, consider a fixed realization n i = n * i , and denote the value of LHS by α. Since f i (z pa(i) ) is continuous and not constant, its image contains an open interval Θ ⊆ R. Denoting u ≜ f i (z pa(i) ), we have</p><p>The only solution to this equality is that r q (n * i + u) is an exponential function, r q (u) = k 1 exp(αu) over interval u ∈ Θ. Since r q is an analytic function that equals to an exponential function over an interval, it is exponential over entire R. This implies that the pdf q N (u) is of the form q N (u) = k 2 exp((k 1 /α) exp(αu)). However, this cannot be a valid pdf since its integral over R diverges. Then,</p><p>] i cannot be true, which concludes the proof.</p><p>Proof: See Appendix D.2.2.</p><p>To provide some intuition about the conditions in Lemma 14, we consider a node i ∈ [n] and discuss the conditions in the context of a few examples. Note that by sweeping φ ∈ R |pa(i)| we generate a continuum of linear equations of the form:</p><p>Note that for all j ̸ ∈ pa(i) we have [∇ z f i (φ)] j = [∇ z fi (φ)] j = 0. Hence, in finding the solutions to (258) only the coordinates {j ∈ pa(i)} of c are relevant. Let us define</p><p>which are the gradients of f i and f i by considering only the coordinates of z in {j ∈ pa(i)}.</p><p>Accordingly, we also define b by concatenating only the coordinates of c with their indices in {j ∈ pa(i)}. Next, consider w distinct choices of φ and denote them by</p><p>By concatenating the two equations in (259) specialized to these realizations, we get the following linear system with 2w equations and |pa(i</p><p>When V is full-rank, i.e., rank(V) = |pa(i)| + 1, the system has only the trivial solutions c i = 0 and b = 0. Then, we make the following observations.</p><p>1. If f i and fi are linear functions, the vector spaces generated by u and ū have dimensions 1. Subsequently, we always have rank(V) ≤ 2, rendering an underdetermined system when |pa(i)| ≥ 2. Hence, when the maximum degree of G is at least 2, a linear causal model does not satisfy Assumption 2.</p><p>2. If f i and fi are quadratic with full-rank matrices, i.e.,</p><p>| + 1 and the system in ( <ref type="formula">258</ref>) admits only the trivial solutions a i = 0 and b = 0. Hence, quadratic causal models satisfy Assumption 2.</p><p>3. If f i and fi are two-layer NNs with a sufficiently large number of hidden neurons, they also render a fully determined system, and as a result, they satisfy Assumption 2. We investigate the last example in detail as follows. Assume that f i and fi are two-layer NNs with |pa(i)| inputs, w i and wi hidden nodes, respectively, and with sigmoid activation functions. Denote the weight matrices between input and hidden layers in f i and fi by W i ∈ R w i ×|pa(i)| and Wi ∈ R wi ×|pa(i)| , respectively. Furthermore, define ν ∈ R w i and in which fi and Ni specify the interventional mechanism for node i. Then, denoting the pdfs of N i and Ni by p N and q N , respectively, ( <ref type="formula">268</ref>) and ( <ref type="formula">269</ref>) imply that</p><p>Denote the score functions associated with p N and q N by</p><p>, and r q (u)</p><p>Define n i and ni as the realizations of N i and Ni when</p><p>. Using ( <ref type="formula">271</ref>) and ( <ref type="formula">270</ref>), we can express the relevant entries of ∇ z log p i (z i | z pa(i) ) and ∇ z log q i (z i | z pa(i) ) as</p><p>and</p><p>By substituting ( <ref type="formula">272</ref>)-( <ref type="formula">273</ref>) in ( <ref type="formula">267</ref>) and rearranging the terms, the statement in ( <ref type="formula">267</ref>) becomes equivalent to following statement. For all c ∈ C i , there exist</p><p>which by using the shorthand φ for z pa(i) can be compactly presented as follows. For all c ∈ C i , there exists</p><p>Hence, Assumption 2 is equivalent to the statement in ( <ref type="formula">275</ref>), which we use for the rest of the proof.</p><p>Sufficient condition. We show that if (265) admit solutions c only in R n \ C i , then the statement in ( <ref type="formula">275</ref>) holds. By contradiction, assume that there exists c * ∈ C i such that for all</p><p>We show that c * ∈ C i is also a solution to (265), contradicting the premise. In order to show that c * ∈ C i is also a solution to (265), suppose, by contradiction, that (276) holds, and there exists φ * ∈ R |pa(i)| corresponding to which</p><p>which means that we have found a solution to (265) that is not in R n \ C i , contradicting the premise.</p><p>Necessary condition. Assume that Assumption 2, and equivalently, the statement in (275) holds but (265) has a solution c * in C i . Then,</p><p>Multiplying the left side by r p (n i ) and the right side by r q (n i + δ(φ)), we obtain that for all</p><p>which implies that for all c ∈ C i ,</p><p>This contradicts (275), and equivalently Assumption 2. Hence, the proof is complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.3 Proof of Lemma 7</head><p>Approach. We will use the same argument as at the beginning of the proof of Lemma 14. Specifically, we will show that for any node i and two-layer NNs f i and fi with weight matrices W i and Wi , the following continuum of equations admit their solutions</p><p>in which shorthand φ is used for z pa(i) . Hence, by invoking Lemma 14, Assumption 2 holds.</p><p>The rest of the proof follows similarly for the case of rank( Wi ) = d i . We use shorthand {W, w} for {W i , w i } when it is obvious from context.</p><p>Parameterization. Note that f i can be represented by different parameterizations, some containing more hidden nodes than others. Without loss of generality, let w be the fewest number of nodes that can represent f i . This implies that the entries of ν are nonzero. Otherwise, if ν i = 0, we can remove i-th hidden node and still have the same f i . Similarly, the rows of W are distinct. Otherwise, if there exist rows</p><p>removing j-th hidden node and using (ν i + ν j ) in place of ν i results the same function as f i with (w -1) hidden nodes. Similarly, we have W i ̸ = 0 for all i ∈ [w]. Otherwise, we have</p><p>and by removing i-th hidden node and using (ν 0 + ν i 2 ) instead of ν 0 , we reach the same function as f i with (w -1) hidden nodes. Finally, we have</p><p>and by removing j-th hidden node and using (ν iν j ) instead of ν i and (ν 0 + ν j ) instead of ν 0 , we reach the same function as f i with (w -1) hidden nodes. In summary, we have</p><p>We will show that there does not exist c</p><p>Assume the contrary, and assume there exist c * ∈ C i such that</p><p>This is equivalent to showing that there exists nonzero b * ∈ R d i such that</p><p>Based on (261), the gradient of f i (φ) is</p><p>where diag(ν) is the diagonal matrix with ν as its diagonal elements, and σ is the derivative of the sigmoid function, applied element-wise to its argument. Hence, based on (290), the contradiction premise is equivalent to having a nonzero b * ∈ R d i such that</p><p>We note that since W is full-rank and ν i is nonzero for all i ∈ [w], the matrix diag(ν) • W is full-rank as well and it has a trivial null space. Subsequently, b</p><p>We will use the following lemma to show that (293) cannot be true. This establishes that the contradiction premise is not correct, and completes the proof.</p><p>Lemma 15 Let u ∈ R p have nonzero entries with distinct absolute values, i.e., u i ̸ = 0 and |u i | ̸ = |u j | for all i ̸ = j, and α ∈ R be a constant. Then, for every nonzero vector c ∈ R p , there exists α ∈ R such that [ σ(αu)] ⊤ • c ̸ = a.</p><p>Proof: See Appendix D.2.4. Let us define ξ ≜ W • φ. We will show that there exists φ * ∈ R d i such that ξ = W • φ * satisfies the conditions in Lemma 15. Then, using Lemma 15 with the choice of u = W • φ * , d = diag(ν) • W • b * , and a = c i , we find that there exists α ∈ R such that</p><p>Hence, ( <ref type="formula">293</ref>) is false since it is violated for φ = αφ * and the proof is completed. We show the existence of such φ * as follows. We first construct the set of φ values for which conditions of Lemma 15 on w are not satisfied. The set in question is the union of the following cases: (i)</p><p>, or equivalently, (W i ± W j )φ = 0. Note that W i ̸ = 0 and W i ± W j ̸ = 0 by (289). For a nonzero vector</p><p>Then, there is w number of (d i -1)-dimensional subspaces that fall under case (i), and w(w -1) number of (d i -1)-dimensional subspaces that fall under case (ii). Therefore, there are w 2 lower-dimensional subspaces for which the conditions of Lemma 15 do not hold. However, R d i cannot be covered by a finite number of lower-dimensional subspaces of itself. Therefore, there exists φ * ∈ R d i such that ξ = W • φ * satisfies the conditions of Lemma 15, and the proof is completed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.4 Proof of Lemma 15</head><p>Assume the contrary and suppose that there exists a nonzero c and α for a given u. Define the function g u (α) ≜ c ⊤ • σ(αu). Note that</p><p>is an even analytic function, and its Taylor series expansion at 0 has the domain of convergence {x ∈ R : |x| &lt; π 2 }. Thus, for all α ∈ (-</p><p>Note that g u (α) is a constant function of α ∈ (-</p><p>). Thus, its Taylor coefficients, i.e., (γ i p j=1 c j u 2i j ), are zero for all i ∈ N + . However, the coefficients of even powers in Taylor expansion of σ, i.e., γ i 's, are nonzero <ref type="bibr" target="#b53">(Weisstein, 2002)</ref>. Therefore, we have p i=1 (c j u 2i j ) = 0 for all i ∈ N + . Next, construct the following system of linear equations,   </p><p>This is equivalent to</p><p>Note that the Vandermonde matrix in (298) has determinant 1≤i≤j&lt;p (u 2 iu 2 j ), which is nonzero since |u i | ̸ = |u j | for i ̸ = j. Multiplying an invertible matrix with a diagonal invertible matrix generates another invertible matrix, and c must be a zero vector which is a contradiction. Hence, there does not exist such c for which c ⊤ • σ(αu) is constant for every α ∈ R. <ref type="bibr" target="#b59">(Zhang et al., 2023)</ref> We clarify that Assumption 2 of <ref type="bibr" target="#b59">Zhang et al. (2023)</ref>, which is referred to as "linear interventional faithfulness", requires nonlinear SCMs. Subsequently, both of their main results (Theorem 1 and Theorem 2) require this assumption. First, note that <ref type="bibr">Zhang et al. (2023, p.6)</ref> implicitly explain that their results are for nonlinear SCMs: "In general, we show in Appendix B that a large class of nonlinear SCMs and soft interventions satisfy this assumption." Next, we show that this assumption is violated for linear additive noise models as follows. First, we quote their assumption: Linear interventional faithfulness <ref type="bibr" target="#b59">(Zhang et al., 2023</ref>, Assumption 2): Intervention I on node i satisfies linear interventional faithfulness if for every j ∈ ch(i) ∪ {i} such that pa(j) ∩ de(i) = ∅, it holds that P(Z j + c ⊤ Z S ) ̸ = P I (Z j + c ⊤ Z S ) for all constant vectors c ∈ R |S| where S = [n] \ de(i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Assumptions of</head><p>Linear SCMs violate linear interventional faithfulness. Next, we show that linear SCMs violate the above linear interventional faithfulness assumption. Consider an intervention on node i, and let j ∈ ch(i) such that pa(i) ∩ de(i) = ∅. Let</p><p>where w ∈ R |pa(j)| denotes the weight vector. Note that we have pa(j) ⊆ S = [n] \ de(i) since pa(i) ∩ de(i) = ∅. Consider vector c ∈ R |S| such that c pa(j) = -W and c S\pa(j) = 0 where c pa(j) denotes the entries corresponding to nodes in pa(j). Then, we have</p><p>which remains invariant after an intervention on node i and yields P(Z j + c ⊤ Z S ) = P I (Z j + c ⊤ Z S ). Therefore, linear SCMs violate linear interventional faithfulness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Empirical Evaluations: Details and Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Implementation Details of LSCALE-I Algorithm</head><p>Preprocessing: Dimensionality reduction. Since X = G • Z, we can compute im(G) using n random samples of X almost surely. Specifically, im(G) equals the column space of the sample covariance matrix of n samples of X. In LSCALE-I, as a preprocessing step, we compute this subspace and express samples of X and s X in this basis. This procedure effectively reduces the dimension of X and s X from d to n. Then, we perform steps of LSCALE-I using this n dimensional observed data.</p><p>Implementation details of Algorithm 1. In Stage L1 of Algorithm 1, choosing any h ∈ col(R m X ) suffices. To make our algorithm deterministic, we choose the top eigenvector of col(R m X ) that corresponds to the largest eigenvalue. In Stage L2, we use a nonzero threshold λ G , i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>pa(m)</head><p>In Table <ref type="table">16</ref>, we list all λ G values used in the experiments.  ing exhaustive single-node interventions, the intervention embedding network fails to learn exhaustive intervention targets/effects. Therefore, the learned graph is not immediately interpretable as a DAG on n causal latent variables since the learned intervention targets (i.e., node labels) are not surjective.</p><p>In contrast, in LSCALE-I, the effect of an intervention is estimated using score function differences. While the distinction may seem small, this approach ensures that the estimated graph's nodes are always interpretable since score differences between observational and interventional environments directly correspond to the intervention targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Visualizations for Intervention Extrapolation on Biological Data</head><p>In Figure <ref type="figure">9</ref>, we plot certain double-node intervention samples, both actual and sampled using our score-difference Langevin method in a UMAP <ref type="bibr" target="#b33">(McInnes et al., 2018)</ref> plot. We plot the double-node interventions that <ref type="bibr" target="#b59">Zhang et al. (2023)</ref> plots in their double-node intervention extrapolation results that have more than 1000 samples for both single and double-node environments.sec:discrvae-invert-enc-problem We see across intervention pairs, score-based intervention extrapolation method leads to a sampling pattern that is relatively compliant with the actual double node interventional data. However, we observe that in some pairs, intervention effects are overestimated (particularly for CEBPE+SPI and ETS2+MAPK1). </p><p>Since N i is a zero-mean Gaussian random vector and (I n×n -A) -1 is a full rank matrix, Z is a zero-mean Gaussian random vector. Hence,</p><p>Note that the covariance of Z has the following form</p><p>Similarly, the score function of Z in E m is given by</p><p>in which Z m denotes the latent variables Z in environment E m . Finally, by setting f = G, Corollary 1 specifies the score differences of X in terms of Z under different environment pairs as s X (x)s m X (x) = G † ⊤ • s(z)s m (z) .</p><p>(308)</p><p>Score estimation for a linear Gaussian model. For all environments E m ∈ E, given n s i.i.d. samples of X, we first compute the sample covariance matrix denoted by Σm . Then, we compute the sample precision matrix as</p><p>which leads to the score function estimate given by</p><p>Score function of the quadratic model. Following ( <ref type="formula">16</ref>), score functions s m and sm are decomposed as follows.</p><p>s m (z) = ∇ z log q ℓ (z ℓ ) + i̸ =ℓ</p><p>and sm (z) = ∇ z log qℓ (z ℓ ) + i̸ =ℓ</p><p>For additive noise models, the terms in ( <ref type="formula">311</ref>) and ( <ref type="formula">312</ref>) have closed-form expressions. Specifically, using ( <ref type="formula">131</ref>) and denoting the score functions of the noise terms {N i : i ∈ ∂f i (z pa(j) ) ∂z i • r j (n j ) .</p><p>(313)</p><p>Recall that we consider a quadratic latent model with</p><p>which implies ∂f j (z pa(j) )</p><p>, and</p><p>Components of the score functions s m and sm can be computed similarly. Subsequently, using Corollary 1 of Lemma 2, we can compute the score differences of observed variables as follows.</p><p>s X (x)s m X (x) = J g (z) † ⊤</p><p>• s(z)s m (z) , (316)</p><p>s m X (x)sm X (x) = J g (z) † ⊤</p><p>• s m (z)sm (z) .</p><p>(318)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sample complexity of interventional causal representation learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Acartürk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Varıcı</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tajer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-12">December 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Properties from mechanisms: An equivariance perspective on identifiable representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations, virtual</title>
		<meeting>International Conference on Learning Representations, virtual</meeting>
		<imprint>
			<date type="published" when="2022-04">April 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interventional causal representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07">July 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-domain causal representation learning via weak distributional invariances</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Artificial Intelligence and Statistics</title>
		<meeting>International Conference on Artificial Intelligence and Statistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-05">May 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identifying linearly-mixed causal representations from multi-node interventions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ninad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Causal Learning and Reasoning</title>
		<meeting>Causal Learning and Reasoning<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-04">April 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An introduction to differentiable manifolds and Riemannian geometry, Revised</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Boothby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Gulf Professional Publishing</publisher>
			<biblScope unit="volume">120</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly supervised causal representation learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-12">December 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning linear causal representations from interventions under general nonlinear mixing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12">December 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of control, signals and systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Genome-scale CRISPRmediated control of gene repression and activation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Horlbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Adamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Villalta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guimaraes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Panning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Ploegh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Bassik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="647" to="661" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hidden Markov nonlinear ICA: Unsupervised learning from nonstationary time series</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hälvä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Uncertainty in Artificial Intelligence, virtual</title>
		<meeting>Conference on Uncertainty in Artificial Intelligence, virtual</meeting>
		<imprint>
			<date type="published" when="2020-08">August 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonlinear ICA of temporally dependent stationary sources</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Morioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Artificial Intelligence and Statistics</title>
		<meeting>International Conference on Artificial Intelligence and Statistics<address><addrLine>Ft. Lauderdale, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nonlinear independent component analysis: Existence and uniqueness results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pajunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="429" to="439" />
			<date type="published" when="1999-04">April 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonlinear ICA using auxiliary variables and generalized contrastive learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Artificial Intelligence and Statistics</title>
		<meeting>International Conference on Artificial Intelligence and Statistics<address><addrLine>Naha, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-04">April 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automated discovery of pairwise interactions from unstructured data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whitfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Didolkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Earnshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hartford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.07594</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning nonparametric latent causal graphs with unknown interventions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12">December 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning linear causal representations from general environments: Identifiability and intrinsic ambiguity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Syrgkanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-12">December 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Variational autoencoders and nonlinear ICA: A unifying framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Artificial Intelligence and Statistics, virtual</title>
		<meeting>International Conference on Artificial Intelligence and Statistics, virtual</meeting>
		<imprint>
			<date type="published" when="2020-08">August 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ice-beem: Identifiable conditional energy-based deep models based on nonlinear ICA</title>
		<author>
			<persName><forename type="first">I</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems, virtual</title>
		<meeting>Advances in Neural Information essing Systems, virtual</meeting>
		<imprint>
			<date type="published" when="2020-12">December 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Identifiability of deep generative models under mixture priors without auxiliary information</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kivva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-12">December 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Priol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.04890</idno>
		<title level="m">Nonparametric partial disentanglement via mechanism sparsity: Sparse actions, interventions and sparse temporal dependencies</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Causal reasoning in simulation for structure and transfer learning of robot manipulation policies</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kroemer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Robotics and Automation</title>
		<meeting>IEEE International Conference on Robotics and Automation<address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Disentangled representation learning in non-markovian causal systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances on Neural Information Processing Systems</title>
		<meeting>Advances on Neural Information essing Systems<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-12">December 2024a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.15325</idno>
		<title level="m">On the identification of temporally causal representation with instantaneous dependence</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Causal component analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kekić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12">December 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Intervention design for causal representation learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI 2022 Workshop on Causal Representation Learning</title>
		<meeting><address><addrLine>Eidhoven, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Causal representation learning for instantaneous and temporal effects in interactive systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations<address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-05">May 2023a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Biscuit: Causal representation learning from binary interactions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Uncertainty in Artificial Intelligence</title>
		<meeting>Uncertainty in Artificial Intelligence<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-08">August 2023b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Identifiable latent polynomial causal models through the lens of change</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-05">May 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning<address><addrLine>Long Beach, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weaklysupervised disentanglement without compromises</title>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning, virtual</title>
		<meeting>International Conference on Machine Learning, virtual</meeting>
		<imprint>
			<date type="published" when="2020-04">April 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">UMAP: Uniform manifold approximation and projection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Großberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">29</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Assumption violations in causal discovery and the robustness of score matching</title>
		<author>
			<persName><forename type="first">F</forename><surname>Montagna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Mastakouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eulig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Noceti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12">December 2023a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scalable causal discovery with score matching</title>
		<author>
			<persName><forename type="first">F</forename><surname>Montagna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Noceti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Causal Learning and Reasoning</title>
		<meeting>Conference on Causal Learning and Reasoning<address><addrLine>Tübingen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-04">April 2023b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Causal representation learning made identifiable by grouping of observational variables</title>
		<author>
			<persName><forename type="first">H</forename><surname>Morioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-07">July 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring genetic interaction manifolds constructed from rich single-cell phenotypes</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Horlbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Replogle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Weissman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="issue">6455</biblScope>
			<biblScope unit="page" from="786" to="793" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Score matching enables causal discovery of nonlinear additive noise models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kleindessner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07">July 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Identifying representations for intervention extrapolation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saengkyongam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-05">May 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Toward causal representation learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="612" to="634" />
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Weakly supervised disentangled generative causal representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10994" to="11048" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Weakly supervised disentanglement with guarantees</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations, virtual</title>
		<meeting>International Conference on Learning Representations, virtual</meeting>
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Introduction to geometric measure theory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tsinghua Lectures</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sliced score matching: A scalable approach to density and score estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Uncertainty in Artificial Intelligence, virtual</title>
		<meeting>Uncertainty in Artificial Intelligence, virtual</meeting>
		<imprint>
			<date type="published" when="2020-08">August 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Linear causal disentanglement via interventions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Bhate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07">July 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unpaired multi-domain causal representation learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sturma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12">December 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Causal machine learning for single-cell genomics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tejada-Lapuerta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bertin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aliee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Theis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Genetics</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Linear causal representation learning from unknown multi-node interventions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Varıcı</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Acartürk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tajer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-12">December 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Score-based causal representation learning with interventions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Varıcı</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Acartürk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tajer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.08230</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Self-supervised learning with data augmentations provably isolates content from style</title>
		<author>
			<persName><forename type="first">J</forename><surname>Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems, virtual</title>
		<meeting>Advances in Neural Information essing Systems, virtual</meeting>
		<imprint>
			<date type="published" when="2021-12">December 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Nonparametric identifiability of causal representations from unknown interventions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kekić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12">December 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Weisstein</surname></persName>
		</author>
		<ptr target="https://mathworld.wolfram.com/" />
		<title level="m">Sigmoid function</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Identifiability guarantees for causal disentanglement from purely observational data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-12">December 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient Langevin dynamics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning<address><addrLine>Bellevue, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07">July 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-view causal representation learning with partial observability</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Taslakian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Martius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-05">May 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unifying causal representation learning with the invariance principle</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rancati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cadei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fumero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2025-04">April 2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning temporally causal latent processes from general temporal data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations, virtual</title>
		<meeting>International Conference on Learning Representations, virtual</meeting>
		<imprint>
			<date type="published" when="2022-04">April 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Identifiability guarantees for causal disentanglement from soft interventions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12">December 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Causal representation learning from multiple distributions: A general setting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-07">July 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Sample complexity bounds for score-matching: Causal discovery and generative modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12">December 2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
