<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents</title>
				<funder>
					<orgName type="full">MIT-DSTA</orgName>
				</funder>
				<funder ref="#_mqpbbDn #_FxQnj3E">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_qFbVn28">
					<orgName type="full">MIT-IBM</orgName>
				</funder>
				<funder>
					<orgName type="full">DTRA Discovery of Medical Countermeasures Against New and Emerging (DOMANE)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-07-03">3 Jul 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yilun</forename><surname>Xu</surname></persName>
							<email>&lt;yilunx@nvidia.com&gt;.</email>
						</author>
						<author>
							<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
						</author>
						<title level="a" type="main">DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-07-03">3 Jul 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2407.03300v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Diffusion models (DMs) have revolutionized generative learning. They utilize a diffusion process to encode data into a simple Gaussian distribution. However, encoding a complex, potentially multimodal data distribution into a single continuous Gaussian distribution arguably represents an unnecessarily challenging learning problem. We propose Discrete-Continuous Latent Variable Diffusion Models (DisCo-Diff) to simplify this task by introducing complementary discrete latent variables. We augment DMs with learnable discrete latents, inferred with an encoder, and train DM and encoder end-to-end. DisCo-Diff does not rely on pre-trained networks, making the framework universally applicable. The discrete latents significantly simplify learning the DM's complex noise-to-data mapping by reducing the curvature of the DM's generative ODE. An additional autoregressive transformer models the distribution of the discrete latents, a simple step because DisCo-Diff requires only few discrete variables with small codebooks. We validate DisCo-Diff on toy data, several image synthesis tasks as well as molecular docking, and find that introducing discrete latents consistently improves model performance. For example, DisCo-Diff achieves state-of-the-art FID scores on class-conditioned ImageNet-64/128 datasets with ODE sampler.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Diffusion models (DMs) <ref type="bibr" target="#b82">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b29">Ho et al., 2020;</ref><ref type="bibr">Song et al., 2021)</ref> have recently led to breakthroughs for generative modeling in diverse domains. For instance, they can synthesize expressive high-resolution imagery <ref type="bibr" target="#b31">(Saharia et al., 2022;</ref><ref type="bibr" target="#b71">Ramesh et al., 2022;</ref><ref type="bibr">Rombach</ref> Proceedings of the 41 st International Conference on Machine <ref type="bibr">Learning, Vienna, Austria. PMLR 235, 2024.</ref> Copyright 2024 by the author(s). <ref type="bibr">et al., 2022;</ref><ref type="bibr" target="#b2">Balaji et al., 2022)</ref> or they can generate accurate molecular structures <ref type="bibr">(Corso et al., 2023;</ref><ref type="bibr" target="#b99">Yim et al., 2023;</ref><ref type="bibr" target="#b36">Ingraham et al., 2023;</ref><ref type="bibr" target="#b94">Watson et al., 2023)</ref>. DMs leverage a forward diffusion process that effectively encodes the training data in a simple, unimodal Gaussian prior distribution. Generation can be formulated either as a stochastic or, more conveniently, as a deterministic process that takes as input random noise from the Gaussian prior and transforms it into data through a generative ordinary differential equation (ODE) <ref type="bibr">(Song et al., 2021)</ref>. The Gaussian prior corresponds to the DM's continuous latent variables, where the data is uniquely encoded through the ODE-defined mapping.</p><p>However, realistic data distributions are typically highdimensional, complex and often multimodal. Directly encoding such data into a single unimodal Gaussian distribution and learning a corresponding reverse noise-to-data mapping is challenging. The mapping, or generative ODE, necessarily needs to be highly complex, with strong curvature, and one may consider it unnatural to map an entire data distribution to a single Gaussian distribution. In practice, conditioning information, such as class labels or text prompts, often helps to simplify the complex mapping by offering the DM's denoiser additional cues for more accurate denoising. However, such conditioning information is typically of a semantic nature and, even given a class or text prompt, the mapping remains highly complex. For instance, in the case of images, even within a class we find images with vastly different styles and color patterns, which corresponds to large distances in pixel space.</p><p>Here, we propose Discrete-Continuous Latent Variable Diffusion Models (DisCo-Diff), DMs augmented with additional discrete latent variables that encode additional highlevel information about the data and can be used by the main DM to simplify its denoising task (Fig. <ref type="figure">1</ref>). These discrete latents are inferred through an encoder network and learnt end-to-end together with the DM. Thereby, the discrete latents directly learn to encode information that is beneficial for reducing the DM's score matching objective and making the DM's hard task of mapping simple noise to complex data easier. Indeed, in practice, we find that they significantly reduce the curvature of the DM's generative ODE and reduce the DM training loss in particular for large diffusion times, where denoising is most ambiguous and challenging. In contrast to previous work <ref type="bibr" target="#b3">(Bao et al., 2022;</ref><ref type="bibr">Hu et al., 2023;</ref><ref type="bibr">Har-Figure 1</ref>. Discrete-Continuous Latent Variable Diffusion Models (DisCo-Diff) augment DMs with additional discrete latent variables that capture global appearance patterns, here shown for images of huskies. (a) During training, discrete latents are inferred through an encoder, for images a vision transformer <ref type="bibr" target="#b17">(Dosovitskiy et al., 2021)</ref>, and fed to the DM via cross-attention. Backpropagation is facilitated by continuous relaxation with a Gumbel-Softmax distribution. To sample novel images, an additional autoregressive model is learnt over the distribution of discrete latents. (b) Schematic visualization of generative denoising diffusion trajectories. Different colors indicate different discrete latent variables, pushing the trajectories toward different modes.</p><p>vey &amp; Wood, 2023), we do not rely on domain-specific pretrained encoder networks, making our framework general and universally applicable. To facilitate sampling of discrete latent variables during inference, we learn an autoregressive model over the discrete latents in a second step. We only use a small set of discrete latents with relatively small codebooks, which makes the additional training of the autoregressive model easy. We specifically advocate for the use of auxiliary discrete instead of continuous latents; see Sec. 3.2.</p><p>While previous works <ref type="bibr">(Esser et al., 2021;</ref><ref type="bibr">Ramesh et al., 2021;</ref><ref type="bibr">Chang et al., 2022;</ref><ref type="bibr">Yu et al., 2022;</ref><ref type="bibr" target="#b64">Pernias et al., 2023;</ref><ref type="bibr">Chang et al., 2023)</ref> use fully discrete latent variablebased approaches to model images, this typically requires large sets of spatially arranged latents with large codebooks, which makes learning their distribution challenging. DisCo-Diff, in contrast, carefully combines its discrete latents with the continuous latents (Gaussian prior) of the DM and effectively separates the modeling of discrete and continuous variations within the data. It requires only a few discrete latents.</p><p>To demonstrate its universality, we validate the DisCo-Diff framework on several different tasks. As a motivating example, we study 2D toy distributions, where the discrete latents learn to capture different modes with smaller curvature during sampling. We then tackle image synthesis, where the discrete latents learn large-scale appearance, often associated with global style and color patterns. Thereby, they offer complementary benefits to semantic conditioning information. Quantitatively, DisCo-Diff universally boosts output quality and achieves state-of-the-art performance on several Ima-geNet generation benchmarks. In addition, we experimentally validate that auxiliary discrete latents are superior to continuous latents in our setup, and study different network architectures for injecting the discrete latents into the DM network. A careful hierarchical design can encourage different discrete latents to encode different image characteristics, such as shape vs. color, reminiscent of observations from the literature on generative adversarial networks <ref type="bibr" target="#b39">(Karras et al., 2019;</ref><ref type="bibr">2020)</ref>. We also apply DisCo-Diff to molecular docking, a critical task in drug discovery, where the discrete la-tents again improve performance by learning to indicate critical atoms in the interaction and, in this way, deconvolving the multimodal uncertainty given by different possible poses from continuous variability of each pose. Moreover, we augment Poisson Flow Generative Models <ref type="bibr">(Xu et al., 2022;</ref><ref type="bibr" target="#b81">2023b)</ref> with discrete latent variables to showcase that the framework can also be applied to other "iterative" generative models, other than regular DMs, observing similar benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions. (i)</head><p>We propose DisCo-Diff, a novel framework for combining discrete and continuous latent variables in DMs in a universal manner. (ii) We extensively validate DisCo-Diff, significantly boosting model quality in all experiments, and achieving state-of-the-art performance on several image synthesis tasks. (iii) We present detailed analyses as well as ablation and architecture design studies that demonstrate the unique benefits of discrete latent variables and how they can be fed to the main denoiser network. (iv) Overall, we provide insights for designing performant generative models. We make the case for discrete latents by showing that real-world data is best modeled with generative frameworks that leverage both discrete and continuous latents. We intentionally developed a simple and universal framework that does not rely on pre-trained encoders to offer a broadly applicable modeling approach to the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>DisCo-Diff builds on (continuous-time) DMs <ref type="bibr">(Song et al., 2021)</ref>, and we follow the EDM framework <ref type="bibr" target="#b41">(Karras et al., 2022)</ref>. DMs perturb the clean data y ∼ p data (x) in a fixed forward process using σ 2 (t)-variance Gaussian noise, where y ∈ R d and t denotes the time along the diffusion process. The resulting distribution is denoted as p(x; σ(t)) with x ∈ R d . For sufficiently large σ max , this distribution is almost identical to pure random Gaussian noise. DMs leverage this observation to sample x 0 ∼ N (x 0 ; 0, σ 2 max I) and then iteratively denoise the sample through a sequence of M + 1 gradually decreasing noise levels σ i+1 &lt; σ i (σ 0 = σ max ), where i ∈ [0, ..., M ] and x i ∼ p(x; σ i ). The σ i correspond to a discretization of a continuous σ(t) function. If σ M = 0, then the final x M follows the data distribution. Sampling corresponds to simulating a deterministic or stochastic differential equation</p><formula xml:id="formula_0">dx = -σ(t)σ(t)∇ x log p(x; σ(t))dt Probability Flow ODE -β(t)σ 2 (t)∇ x log p(x; σ(t))dt + 2β(t)σ(t)dω t Langevin Diffusion SDE , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where dω t is a standard Wiener process and ∇ x log p(x; σ(t)) is the score function of the diffused distribution p(x; σ(t)). The first term in Eq. ( <ref type="formula" target="#formula_0">1</ref>) is the Probability Flow ODE, which pushes samples from large to small noise levels. The second term is a Langevin Diffusion SDE, an equilibrium sampler for different noise levels σ(t), which can help correct errors during synthesis <ref type="bibr" target="#b41">(Karras et al., 2022)</ref>. This component can be scaled by the time-dependent parameter β(t). Setting β(t) = 0 leads to pure ODE-based synthesis. Generally, different sampling methods can be used to solve the generative ODE/SDE.</p><p>Training a DM corresponds to learning a model to approximate the intractable score function ∇ x log p(x; σ(t)).</p><p>Following the EDM framework, we parametrize</p><formula xml:id="formula_2">∇ x log p(x; σ(t)) = (D θ (x, σ(t)) -x)/σ 2 (t), where D θ (x, σ(t)</formula><p>) is a learnable denoiser neural network that is trained to predict clean data from noisy inputs and is conditioned on the noise level σ(t). It can be trained using denoising score matching <ref type="bibr" target="#b35">(Hyvärinen, 2005;</ref><ref type="bibr" target="#b58">Lyu, 2009;</ref><ref type="bibr" target="#b92">Vincent, 2011;</ref><ref type="bibr" target="#b83">Song &amp; Ermon, 2019)</ref>, minimizing</p><formula xml:id="formula_3">E y∼pdata(y) E t,n λ(t)||D θ (y + n, σ(t)) -y|| 2 (2)</formula><p>where t ∼ p(t) for a distribution p(t) over diffusion times t, n ∼ N (n; 0, σ 2 (t)I), and λ(t) is a function that gives different weight to the objective for different noise levels.</p><p>In this work, we use σ(t) = t and follow the EDM work's configuration <ref type="bibr" target="#b41">(Karras et al., 2022)</ref>, unless otherwise noted. Moreover, we also leverage classifier-free guidance in DisCo-Diff when conditioning on the discrete latent variables. Classifier-free guidance combines the score functions of an unconditional and a conditional diffusion model to amplify the conditioning; see <ref type="bibr">Ho &amp; Salimans (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DisCo-Diff</head><p>In Sec. 3.1, we first formally define DisCo-Diff's generative model and training framework, before discussing and carefully motivating our approach in detail in Sec. 3.2. In Sec. 3.3, we highlight critical architecture considerations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generative Model and Training Objective</head><p>In our DisCo-Diff framework (Fig. <ref type="figure">1</ref>), we augment a DM's learning process with an m-dimensional discrete latent z ∈ N m , where each dimension is a random variable from DisCo-Diff's training process is divided into two stages. In the first stage, the denoiser D θ and the encoder E ϕ are co-optimized in an endto-end fashion. This is achieved by extending the denoising score matching objective (as expressed in Eq. 2) to include learnable discrete latents z associated with each data y:</p><formula xml:id="formula_4">E y E z∼E ϕ (y) E t,n λ(t)||D θ (y + n, σ(t), z) -y|| 2 , (3)</formula><p>where y ∼ p data (y). In contrast to the standard objective in Eq. 2, which focuses on learning the reparameterization of the score ∇ x log p(x; σ(t)), the denoiser in our approach is essentially learning the reparameterization of the conditional score ∇ x log p(x|z; σ(t)), with the convolution of the probability density functions p(•|z; σ(t)) = p(•|z) * N (0, σ 2 (t)I). This conditional score originates from conditioning the DM on the discrete latents z, which are inferred by the encoder E ϕ . The denoiser network D θ can better capture the time-dependent score (i.e., achieving a reduced loss) if the score for each sub-distribution p(x|z; σ(t)) is simplified. Therefore, the encoder E ϕ , which has access to clean input data, is encouraged to encode useful information into the discrete latents and help the denoiser to more accurately reconstruct the data. Naively backpropagating gradients into the encoder through the sampling of the discrete latent variables z is not possible. Hence, during training we rely on a continuous relaxation based on the Gumbel-Softmax distribution <ref type="bibr" target="#b38">(Jang et al., 2016)</ref> (see App. D for details).</p><p>When training the denoiser network, we randomly replace the discrete latent variables with a non-informative nullembedding with probability 0.1. Thereby, the DM learns both a discrete latent variable-conditioned and a regular, unconditional score. During sampling, we can combine these scores for classifier-free guidance <ref type="bibr">(Ho &amp; Salimans, 2021)</ref> with respect to the model's own discrete latents, and amplify their conditioning effect (details in App. B).</p><p>We can interpret DisCo-Diff as a variational autoencoder (VAE) <ref type="bibr" target="#b46">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b72">Rezende et al., 2014;</ref><ref type="bibr" target="#b90">van den Oord et al., 2017;</ref><ref type="bibr" target="#b73">Rolfe, 2017)</ref> with discrete latents and a DM as decoder. VAEs often employ regularization on their latents. We did not find this to be necessary, as we use only very low-dimensional latent variables, e.g., 10 in our ImageNet experiments, with relatively small codebooks. Moreover, we employ a strictly non-zero temperature in the Gumbel-Softmax relaxation, encouraging stochasticity.</p><p>In the second stage, we train the autoregressive model A ψ to capture the distribution of the discrete latent variables p ϕ (z) defined by pushing the clean data through the trained encoder. We use a maximum likelihood objective as follows:</p><formula xml:id="formula_5">E y∼pdata(y),z∼E ϕ (y) m i=1 log p ψ (z i |z &lt;i )<label>(4)</label></formula><p>Since we set m to a relatively small number, it becomes very easy for the model to handle such short discrete vectors, which makes this second-stage training efficient. Also the additional sampling overhead due to this autoregressive component on top of the DM becomes negligible. At inference time, when using DisCo-Diff to generate novel samples, we first sample a discrete latent variable from the autoregressive model, and then sample the DM with an ODE or SDE solver. We provide the algorithm pseudocode for training and sampling in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Motivation and Related Work</head><p>We will now critically discuss and motivate our design choices and also discuss the most relevant related works.</p><p>For an extended discussion of related work see App. A.</p><p>The curvature of diffusion models. DMs, in their simpler ODE-based formulation (β(t) = 0 in Eq. ( <ref type="formula" target="#formula_0">1</ref>)), learn a com- plex noise-to-data mapping. The noise is drawn from an analytically tractable, unimodal Gaussian distribution. As the data is encoded in this distribution, we can consider this high-dimensional Gaussian distribution the DM's continuous latent variables (DMs can generally be seen as deep latent variable models <ref type="bibr">(Huang et al., 2021;</ref><ref type="bibr" target="#b47">Kingma et al., 2021)</ref>). However, the mapping from unstructured noise to a diverse, typically multimodal data distribution necessarily needs to be highly complex. This corresponds to a highly non-linear generative ODE with strong curvature, which is challenging to learn and also makes synthesis slow by requiring a fine discretization. To illustrate this point, we trained a DM on a simple 2D mixture of Gaussians, where we observe bent ODE trajectories near the data (Fig. <ref type="figure" target="#fig_1">3</ref>, middle). This effect is significantly stronger in high dimensions.</p><p>A simpler mapping with discrete latent variables. The role of the discrete latents in DisCo-Diff is to reduce this complexity and make the DM's learning task easier. The single noise-to-data mapping is effectively partitioned into a set of simpler mappings, each with less curvature in its generative ODE. We argue that it is unnatural to map an entire multimodal complex data distribution to a single continuous Gaussian distribution.  <ref type="figure" target="#fig_2">4</ref>, left, we quantitatively show strongly reduced curvature along the entire diffusion time t. In Fig. <ref type="figure" target="#fig_2">4</ref>, right, as a measure of network complexity we also show the norms of the Jacobians of the employed denoiser networks. We see significantly reduced norms for DisCo-Diff for all t, suggesting that the denoiser's task is indeed strongly simplified and less network capacity is required.</p><p>Using few, global latents with relatively small codebooks is important. DisCo-Diff is fundamentally different from most contemporary generative modeling frameworks using discrete latent variables (van den <ref type="bibr" target="#b90">Oord et al., 2017;</ref><ref type="bibr">Esser et al., 2021;</ref><ref type="bibr">Ramesh et al., 2021;</ref><ref type="bibr">Chang et al., 2022;</ref><ref type="bibr">Yu et al., 2022;</ref><ref type="bibr" target="#b64">Pernias et al., 2023;</ref><ref type="bibr">Chang et al., 2023)</ref>. These works use autoencoders to encode images in its entirety into spatially-arranged, downsampled representations of the inputs, focusing more on preserving image fidelity than on capturing diverse training data modes. However, this is also unnatural: Encoding continuous variability, like smooth pose, shape, or color variations in images, into discrete latents requires the use of very large codebooks and, on top of that, these models generally rely on very high-dimensional spatial grids of discrete latents (e.g. 32x32=1024 latents with codebooks &gt;1, 000 <ref type="bibr">(Esser et al., 2021)</ref>, while we use just 10 latents with a codebook size of 100 in our main image models). This makes learning the distribution over the discrete latents very challenging for these types of models, while it is simple in DisCo-Diff, where they just supplement the DM. In DisCo-Diff, we get the best from both continuous and discrete latent variables, using only few global latents.</p><p>End-to-end training is essential. DisCo-Diff's discrete latents are in spirit similar to leveraging non-learnt conditioning information. As pointed out by <ref type="bibr" target="#b3">Bao et al. (2022)</ref>, this has been crucial to facilitate training high-performance generative models like strong class-conditional <ref type="bibr">(Dhariwal &amp; Nichol, 2021;</ref><ref type="bibr" target="#b45">Kingma &amp; Gao, 2023)</ref> or text-to-image DMs <ref type="bibr" target="#b71">(Ramesh et al., 2022;</ref><ref type="bibr" target="#b31">Ho et al., 2022;</ref><ref type="bibr">Rombach et al., 2022)</ref>. However, DisCo-Diff aims to fundamentally address the problem, rather than relying on given conditioning data. Moreover, the data usually has significant variability even given, for instance, a class label. Our discrete latents can further reduce the complexity (as observed, see Sec. 4).</p><p>However, could we use pre-trained encoder networks, such as CLIP <ref type="bibr">(Radford et al., 2021)</ref> or others <ref type="bibr" target="#b24">(He et al., 2020;</ref><ref type="bibr">Caron et al., 2021)</ref>, to produce encodings to condition on and whose distribution could be modeled in a second stage? This is explored by previous works <ref type="bibr" target="#b23">(Harvey &amp; Wood, 2023;</ref><ref type="bibr" target="#b3">Bao et al., 2022;</ref><ref type="bibr">Hu et al., 2023;</ref><ref type="bibr" target="#b51">Li et al., 2023)</ref>, but has important disadvantages: (i) The most crucial downside is that such encoders are not universally available, but typically only for images. However, we seek to develop a universally applicable framework. For instance, we also apply DisCo-Diff to molecular docking (see Sec. 4.2), where no suitable pre-trained networks are available. (ii) In DisCo-Diff, the job of the discrete latents is to make the denoising task of the DM easier, which is especially ambiguous at large noise levels (in fact, we find that the latents help in particular to reduce the loss at these high noise levels, see Fig. <ref type="figure" target="#fig_5">7</ref>). It is not obvious what information about the data the latents should best encode for this. By learning them jointly with the DM objective itself, they are directly trained to help the DM learn better denoisers and lower curvature generative ODEs. (iii) A generative model needs to be trained over the encodings in the second stage. In DisCo-Diff, we can freely choose an appropriate number of latents and codebook size to simplify the DM's denoising task, while also facilitating easy learning of the autoregressive model in the second stage. When using pre-trained encoders, one must work with the encodings by these methods, which were not developed for generative modeling. We attribute DisCo-Diff's strong generation performance to its end-to-end learning.</p><p>The latent variables must be discrete. Could we also use auxiliary continuous latent variables? Generative models on continuous latents are almost always based on mappings from a uni-modal Gaussian distribution to the distribution of latents. Hence, if such continuous latents learnt multimodal structure in the data to simplify the main DM's denoising task, as DisCo-Diff's discrete latents do, then learning a distribution over them in the second stage would again require a highly non-linear difficult-to-learn mapping from Gaussian noise to the multimodal encodings. This is the problem DisCo-Diff aims to solve in the first place. <ref type="bibr">Preechakul et al. (2022)</ref> augment DMs with non-spatial continuous latent variables, but they only focus on semantic face image manipulation. InfoDiffusion <ref type="bibr">(Wang et al., 2023)</ref> conditions DMs on discrete latent variables. However, it focuses on learning disentangled representations, also primarily for low-resolution face synthesis, and uses a mutual information-based objective. Contrary to DisCo-Diff, neither of these works tackles high-quality synthesis for challenging, diverse datasets.</p><p>In our ablation studies (Sec. 4.1), we further validate our design choices and motivations that we presented here.</p><p>Table <ref type="table">1</ref>. FID score together with NFE on ImageNet-64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FID NFE</head><p>without class-conditioning IC-GAN <ref type="bibr" target="#b8">(Casanova et al., 2021)</ref> 9.20 1 BigGAN <ref type="bibr" target="#b5">(Brock et al., 2018)</ref> 16.90 1 iDDPM <ref type="bibr">(Nichol &amp; Dhariwal, 2021)</ref> 16.38 50 EDM <ref type="bibr" target="#b41">(Karras et al., 2022)</ref> 6.20 50 SCDM <ref type="bibr" target="#b3">(Bao et al., 2022)</ref> 3.94 50 DisCo-Diff (ours)</p><p>3.70 50 class-conditioned, ODE sampler EDM <ref type="bibr" target="#b41">(Karras et al., 2022)</ref> 2.36 79 PFGM++ <ref type="bibr">(Xu et al., 2023b)</ref> 2.32 79 DisCo-PFGM++ (ours)</p><p>1.92 78 DisCo-Diff (ours)</p><p>1.65 78 class-conditioned, stochastic sampler iDDPM <ref type="bibr">(Nichol &amp; Dhariwal, 2021)</ref> 2.92 250 ADM <ref type="bibr">(Dhariwal &amp; Nichol, 2021)</ref> 2.07 250 CDM <ref type="bibr">(Ho et al., 2021)</ref> 1.48 8000 VDM++ <ref type="bibr" target="#b45">(Kingma &amp; Gao, 2023)</ref> 1.43 511 EDM (w/ Restart <ref type="bibr">(Xu et al., 2023a))</ref> 1.36 623 RIN <ref type="bibr" target="#b37">(Jabri et al., 2022)</ref> 1.23 1000 DisCo-Diff (ours; w/ Restart <ref type="bibr">(Xu et al., 2023a)</ref>)</p><p>1.22 623 class-conditioned, w/ adversarial objective IC-GAN <ref type="bibr" target="#b8">(Casanova et al., 2021)</ref> 6.70 1 BigGAN-deep <ref type="bibr" target="#b5">(Brock et al., 2018)</ref> 4.06 1 CTM <ref type="bibr">(Kim et al., 2023a)</ref> 1.92 1 StyleGAN-XL <ref type="bibr" target="#b77">(Sauer et al., 2022)</ref> 1.51 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture</head><p>As discussed, DisCo-Diff enhances the training of continuous DMs by incorporating learnable discrete latent variables that are meant to capture the global underlying discrete structure of the data. To ensure that DisCo-Diff works as intended, suitable network architectures are necessary. Below, we summarize our design choices, focusing on DisCo-Diff for image synthesis. However, the framework is general, requiring only an encoder to infer discrete latents from clean input data and a conditioning mechanism that integrates these discrete latents into the denoiser network. In fact, we also apply our model to 2D toy data and molecular docking.</p><p>Encoder. For image modeling, we utilize a ViT <ref type="bibr" target="#b17">(Dosovitskiy et al., 2021)</ref> as the backbone for the encoder. We extend the classification mechanism in ViTs, and treat each discrete token as a different classification token. Concretely, we add m extra classification tokens to the sequence of image patches. This architectural design naturally allows each discrete latent to effectively capture the global characteristic of the images, akin to performing data classification.</p><p>Discrete latent variable conditioning. For image experiments, DisCo-Diff's denoisers are U-Nets as widely used for DMs <ref type="bibr" target="#b41">(Karras et al., 2022;</ref><ref type="bibr">Hoogeboom et al., 2023)</ref>. For the discrete latent variable conditioning, we utilize crossattention <ref type="bibr">(Rombach et al., 2022)</ref>. Drawing inspiration from text-to-image generation, DisCo-Diff's discrete latents function analogously to text, exerting a global influence on the denoiser's output. Specifically, image features act as queries and discrete latents are keys and values in the cross-attention layer, enabling discrete latents to globally shape the image features. We add a cross-attention layer after each selfattention layer within the U-Net. In our main models, all discrete latents are given to all cross-attention layers.</p><p>Group hierarchical models. To enhance the interpretability of discrete latents, we also explore the inductive bias inherent in the U-Net architecture and feed distinct latent groups into various resolution features in the up-sampling branch of the U-Net, as shown in Fig. <ref type="figure" target="#fig_3">5</ref>. This approach draws inspiration from StyleGAN <ref type="bibr" target="#b39">(Karras et al., 2019)</ref>, where distinct latents are introduced at different resolutions, enabling each to capture different image characteristics by the neural network's inductive bias. This design fosters a group hierarchy, where the groups associated with higher-resolution features offer supplementary information, conditioned upon the groups related to lower-resolution features. We refer to this refined model as the group hierarchical DisCo-Diff.</p><p>In the molecular docking task, existing denoisers operate through message passing in a permutation equivariant way over 3D point clouds representing molecular structures <ref type="bibr">(Corso et al., 2023)</ref>. We build this property and architectural bias directly into the latent variables, allowing them to take values indicating one node in the point cloud (therefore, for every point cloud, the codebook size equals the number of nodes). This latent design choice aligns with the intuition of the encoder determining the atoms playing key roles in the structure and allows for minimal modification of the score model where the latents simply represent additional features for every node. The encoder is also composed of a similar equivariant message passing, e3nn <ref type="bibr" target="#b20">(Geiger &amp; Smidt, 2022)</ref>, network where for each node one logit per latent will be predicted. More details on the architecture for the molecular docking task can be found in App. E.4.</p><p>The auto-regressive model over the distribution of the discrete latents is implemented in image experiments using a standard Transformer decoder <ref type="bibr" target="#b91">(Vaswani et al., 2017)</ref>. For molecular docking, it again uses an e3nn network that is fed the conditioning information of the protein structure and molecular graph. Generally, DisCo-Diff is compatible with other conditional inputs, e.g. class labels, which can be added as inputs to denoiser and auto-regressive model.</p><p>We use an auto-regressive model for simplicity and expect DisCo-Diff's second stage to work equally well with other discrete data generative models, e.g. discrete state diffusion models <ref type="bibr" target="#b0">(Austin et al., 2021;</ref><ref type="bibr" target="#b6">Campbell et al., 2022)</ref>. Architecture details, also for 2D toy data experiments, in App. F.</p><p>An important question surrounding the architecture design is how to choose an appropriate number of latents and codebook size. While intuitively, increasing the number of latents and the codebook size might seem like a straightforward method to reduce the reconstruction error further by capturing more intricate data structures, this approach also introduces additional complexity. Specifically, a larger set of latents or an expanded codebook size complicates the auto-regressive model's task, potentially leading to increased errors. Thus, finding a balance between enhancing model performance through more detailed discrete structures and maintaining manageable modeling complexity for the auto-regressive model is crucial. We recommend using a modest number of latent (e.g. 10 30) and codebook size (e.g. 50 100) for current diffusion models and leaving the exploration of optimal hyper-parameters to future works. For example, in our image generation experiments, we found that a configuration of 10 latents with a codebook size of 100 significantly enhances performance on the complex Im-ageNet dataset. We did not tune this hyper-parameter due to computational constraints. We believe that more careful hyper-parameter optimization over the exact number of latents and the codebook size would further boost the performance of DisCo-Diff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Synthesis</head><p>We use the ImageNet <ref type="bibr" target="#b13">(Deng et al., 2009)</ref> dataset and tackle both class-conditional (at varying resolutions 64×64 and 128×128) and unconditional synthesis. To measure sample quality, we follow the literature and use Fréchet Inception Distance (FID) <ref type="bibr" target="#b25">(Heusel et al., 2017)</ref> (lower is better). We also report the number of neural function evaluations (NFE).</p><p>In the class-conditional setting, the DisCo-Diff's denoiser is initialized using pre-trained ImageNet models, except for the new components: the cross-attention layers between discrete latents and images, and the encoder. We fine-tune the pre-trained U-Net in EDM <ref type="bibr" target="#b41">(Karras et al., 2022)</ref> with discrete latents for ImageNet-64. For ImageNet-128, we implement the U-ViT in VDM++ <ref type="bibr" target="#b45">(Kingma &amp; Gao, 2023)</ref>, and fine-tune our trained VDM++ model using discrete latents. We also adhere to their respective noise schedules and loss weightings during the training process. We use Heun's second-order method as ODE sampler, and a 12layer Transformer as the auto-regressive model. We set the latent dimension to m = 10 and the codebook size to k = 100 in DisCo-Diff.</p><p>Results. See Tables <ref type="table">1</ref> and<ref type="table" target="#tab_1">2</ref> (2) DisCo-Diff outperforms all baselines in the unconditional setting, or when using stochastic sampler. DisCo-Diff also surpasses the previous best method (SCDM <ref type="bibr" target="#b3">(Bao et al., 2022)</ref>) in the unconditional setting, even though their method relies on pre-trained MoCo features. In addition, DisCo-Diff sets the new record ImageNet-64 FID of 1.22 when using Restart sampler <ref type="bibr">(Xu et al., 2023a)</ref>. Note that the competitive method RIN <ref type="bibr" target="#b37">(Jabri et al., 2022)</ref> employs a novel architecture distinct from conventional U-Nets/U-ViTs.</p><p>On ImageNet-128, we observe that DisCo-Diff does not perform well with stochastic samplers. When using the DDPM sampler as in VDM++ <ref type="bibr" target="#b45">(Kingma &amp; Gao, 2023)</ref>, DisCo-Diff achieves an FID score of 2.80, which is worse than VDM++'s FID score of 1.88. As the discrete latents reduce the loss at larger times (c.f. Figure <ref type="figure" target="#fig_5">7</ref>) and the training targets at these times typically correspond to low-frequency components of images, we hypothesize that in this model the discrete latents learnt to overly emphasize global information, diverting the model to overlook some high-frequency details necessary at smaller times. We provide empirical evidence in Appendix G.3 to show that VDM++ w/ DDPM better captures details at smaller times, which supports this hypothesis. Note that, in theory, VDM++ and DisCo-Diff should perform similarly at smaller times. A potential solution could concatenate the discrete latents with a null token, similar to text-to-image models <ref type="bibr" target="#b2">(Balaji et al., 2022)</ref>, allowing the model to learn more easily to exclude the influence of discrete latents at smaller times. We leave it for future exploration. We would like to emphasize that we only observed this behavior for this one model and dataset. In all other experiments, discrete latents universally improved performance for all stochastic and non-stochastic samplers, and when used for all times t. To better utilize the DDPM sampler for the current model, we substituted the DisCo-Diff ODE with the VDM++ DDPM trajectories at smaller times (t &lt; 10), (DisCo-Diff, w/ ODE sampler, VDM++ correction in Table <ref type="table" target="#tab_1">2</ref>), which improves the FID to 1.73. In conclusion, while the discrete latents did not help at small times here, they still boosted performance at larger times and allowed us to outperform the pure VDM++ model and, once again, achieve state-of-the-art performance.</p><p>(3) Discrete latents capture variability complementary to class semantics. Fig. <ref type="figure">2</ref> (b) illustrates that samples sharing the same discrete latent exhibit similar characteristics, and there are noticeable distinctions for different discrete latents under the same class. It suggests that the discrete latents capture variations that are useful in simplifying the diffusion process defined in Euclidean space beyond class labels, underpinning the improvements of DisCo-Diff over the pre-trained class-conditioned DMs. (4) Discrete latents boost the performance on PFGM++. When applied to another ODE-based generative model PFGM++ <ref type="bibr">(Xu et al., 2023b)</ref>, DisCo-PFGM++ also improves over the baseline version (see Table <ref type="table">1</ref>). More samples in App. G.</p><p>Ablations and Analyses. Table <ref type="table" target="#tab_2">3</ref> shows that employing moderate classifier-free guidance with respect to the discrete latents (scale cfg=1) enhances the FID score (studied using ODE sampler), implying that the discrete latents effectively learn modes similar to the role of class labels and text. We further substituted the discrete latents with 1000dim. continuous latents (1000 to offer capacity at least as high as with the m=10 and k=100 discrete latents), using Kullback-Leibler divergence-based (KLD) regularization as in VAEs to control the information retained. For fair comparison, we trained a DiT-based DM <ref type="bibr" target="#b63">(Peebles &amp; Xie, 2023)</ref> on the continuous latents using the same Transformer architecture as in DisCo-Diff's auto-regressive model. Table <ref type="table" target="#tab_2">3</ref> shows that with a low KLD weight (0.1), the continuous latents are under-regularized, challenging the DiT in modeling the complex encoding distribution and leading to a significant gap between oracle FID (latents predicted from training images) and generative FID (latents sampled from second-stage latent generative models). Conversely, a higher KLD weight (1) causes encoder collapse, and the continuous latents are not used (no latent (EDM), oracle latents and generative latents all produce same FIDs). In contrast, DisCo-Diff's generative FID shows only a minor degradation compared to the oracle FID, indicating the ease of modeling the discrete prior with a simple Transformer.</p><p>The DM training objective (Eq. ( <ref type="formula">2</ref>)) has most variability at large diffusion times due to the multimodal posterior of clean data given noisy inputs <ref type="bibr">(Xu et al., 2023c)</ref>. Conditioning information can reduce this ambiguity. For instance, <ref type="bibr" target="#b2">Balaji et al. (2022)</ref> show that text conditioning primarily influences the denoiser at larger times. Fig. <ref type="figure" target="#fig_5">7</ref> (a) shows that the learned discrete latents behave similarly to text conditioning, significantly lowering the training loss at higher time steps. Complementarily, Fig. <ref type="figure" target="#fig_5">7</ref> (b) indicates that switching discrete latents towards the end of sampling barely affects the samples, implying they are not used at smaller times t.</p><p>In DisCo-Diff, the sampling time of the auto-regressive model is negligible compared to the DM's. For instance, for generating 32 images on ImageNet-128, the auto-regressive models requires only 0.44 seconds, while DisCo-Diff's DM component takes 78 seconds for 114 NFE, with an average of 0.68 second/NFE, all on a single NVIDIA A100 GPU.</p><p>Group Hierarchical DisCo-Diff. We evaluate the group hierarchical DisCo-Diff (Sec. 3.3), feeding three separate 10-dim. discrete latents into the U-Net at each level of resolution. Fig. <ref type="figure" target="#fig_4">6</ref> shows that latents for lower-resolution features mainly govern overall shape and layout, while latents for higher-resolution control color and texture. For example, in the bottom figure, when gradually fixing groups in order, the images first converge in shape and then in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Molecular Docking</head><p>We test DisCo-Diff also on molecular docking, a fundamental task in drug discovery that consists of generating the 3D structure with which a small molecule will bind to a protein. We build on top of DiffDock <ref type="bibr">(Corso et al., 2023)</ref>  For the middle two images, the process involves initially employing the discrete latent from the leftmost grid for a certain proportion of the total sampling steps (e.g., 75%), before transitioning to the discrete latent from the rightmost grid to complete the remaining steps (e.g., the last 25% of the total sampling steps).</p><p>particularly strong on the harder component of the test set, where the baseline model is, likely, highly uncertain. This supports the intuition that DisCo-Diff boosts performance by more appropriately modeling discrete and continuous variations in the data. In Fig. <ref type="figure" target="#fig_6">8</ref>, we visualize two examples from the test set which highlight how the model learns to associate distinct sets of poses with different latents, decomposing the multimodal components of the pose distribution from the continuous variations that each pose can have.</p><p>Table <ref type="table">4</ref>. Molecular docking performance on PDBBind. For each method, we report the percentage of top-1 predictions within 2 Å of the ground truth for the full test set and the subset restricted to unseen proteins. Runtime in seconds (* refers to run on CPU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full Unseen Runtime</head><p>GNINA <ref type="bibr" target="#b60">(McNutt et al., 2021)</ref> 22.9 14.0 127 SMINA <ref type="bibr" target="#b48">(Koes et al., 2013)</ref> 18.7 14.0 126* GLIDE <ref type="bibr" target="#b22">(Halgren et al., 2004)</ref> 21.8 19.6 1405* EquiBind <ref type="bibr" target="#b85">(Stärk et al., 2022)</ref> 5.5 0.7 0.04 TankBind <ref type="bibr" target="#b57">(Lu et al., 2022)</ref> 20. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have proposed Discrete-Continuous Latent Variable Diffusion Models (DisCo-Diff), a novel and universal framework for combining discrete latent variables with continuous DMs. The approach significantly boosts performance by simplifying the DM's denoising task through the help of auxiliary discrete latent variables, while introducing negligible overhead. Extensive experiments and analyses demonstrate the unique benefits of global discrete latent variables that are learnt end-to-end with the denoiser. DisCo-Diff does not rely on any pre-trained encoder networks. As such, we validated our method not only on image synthesis, but also for molecular docking, demonstrating its universality.</p><p>Limitations and Future Work. There are several potential future directions and limitations in both the experiments and design of DisCo-Diff. First, our experiments have been primarily focused on standard benchmarks such as ImageNet.</p><p>With more compute resources, DisCo-Diff could be further validated on tasks such as text-to-image generation, where we would expect discrete latent variables to offer complementary benefits to the text conditioning, similar to how discrete latents boost performance in our class-conditional experiments. Secondly, the Group Hierarchical model relies on inductive biases in its architecture, such as the different image characteristics captured at different resolutions in the U-Net. It would be interesting to explore how such architectures could be constructed and similar hierarchical effects could be achieved when working with different data modalities (molecules, etc.). Thirdly, one could apply the idea of DisCo-Diff to other continuous flow models, such as flow-matching <ref type="bibr" target="#b53">(Lipman et al., 2022)</ref> or rectified flow <ref type="bibr" target="#b55">(Liu et al., 2022)</ref>, to further boost their performance. Conceptually, due to the close relation between diffusion models and flow matching, we expect discrete latents to behave similarly there and improve performance. Finally, the current DisCo-Diff framework leverages a two-stage training process. Initially, we jointly train the denoiser and the encoder, followed by the post-hoc auto-regressive model in the second stage. Future work could investigate combining the two-stage training into a seamless end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact Statement</head><p>Deep generative modeling is a burgeoning research field with widespread implications for science and society. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related Work</head><p>Our work builds on DMs <ref type="bibr" target="#b82">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b29">Ho et al., 2020;</ref><ref type="bibr">Song et al., 2021;</ref><ref type="bibr" target="#b41">Karras et al., 2022)</ref>, which have been widely used not only for image generation <ref type="bibr">(Dhariwal &amp; Nichol, 2021;</ref><ref type="bibr">Nichol &amp; Dhariwal, 2021;</ref><ref type="bibr">Rombach et al., 2022;</ref><ref type="bibr">Dockhorn et al., 2022b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b31">Saharia et al., 2022;</ref><ref type="bibr" target="#b71">Ramesh et al., 2022;</ref><ref type="bibr" target="#b66">Podell et al., 2023)</ref>, but also for video <ref type="bibr" target="#b66">(Blattmann et al., 2023;</ref><ref type="bibr">Singer et al., 2023a;</ref><ref type="bibr" target="#b31">Ho et al., 2022;</ref><ref type="bibr">Ge et al., 2023)</ref>, 3D <ref type="bibr" target="#b62">(Nichol et al., 2022;</ref><ref type="bibr">Zeng et al., 2022;</ref><ref type="bibr">Kim et al., 2023b;</ref><ref type="bibr">Poole et al., 2023;</ref><ref type="bibr" target="#b79">Schwarz et al., 2023;</ref><ref type="bibr">Liu et al., 2023)</ref> and 4D <ref type="bibr" target="#b81">(Singer et al., 2023b;</ref><ref type="bibr" target="#b52">Ling et al., 2023;</ref><ref type="bibr" target="#b1">Bahmani et al., 2023;</ref><ref type="bibr" target="#b102">Zheng et al., 2023)</ref> synthesis, as well as in various other domains, including, for instance, molecular docking and protein design <ref type="bibr">(Corso et al., 2023;</ref><ref type="bibr" target="#b99">Yim et al., 2023;</ref><ref type="bibr" target="#b36">Ingraham et al., 2023;</ref><ref type="bibr" target="#b94">Watson et al., 2023)</ref>.</p><p>In the DM literature, latent variables have been most popular as part of latent diffusion models, where a DM is trained in a compressed, usually continuous, latent space <ref type="bibr">(Rombach et al., 2022;</ref><ref type="bibr">Vahdat et al., 2021)</ref>. In contrast, DisCo-Diff leverages discrete latent variables and uses them to augment a DM. The first models using discrete latent variables for high-dimensional generative modeling tasks include Boltzmann machines <ref type="bibr" target="#b76">(Salakhutdinov &amp; Hinton, 2009;</ref><ref type="bibr" target="#b27">Hinton, 2012)</ref> and early discrete variational autoencoders <ref type="bibr" target="#b73">(Rolfe, 2017;</ref><ref type="bibr">Vahdat et al., 2018a;</ref><ref type="bibr">b)</ref>. More recently, a variety of works encode images into large 2D spatial grids of discrete tokens with vector quantization or similar techniques <ref type="bibr" target="#b90">(van den Oord et al., 2017;</ref><ref type="bibr">Esser et al., 2021;</ref><ref type="bibr">Ramesh et al., 2021;</ref><ref type="bibr">Chang et al., 2022;</ref><ref type="bibr">Yu et al., 2022;</ref><ref type="bibr" target="#b64">Pernias et al., 2023;</ref><ref type="bibr">Chang et al., 2023)</ref>. As discussed, these models typically require a very large number of tokens and rely on large codebooks, which makes modeling their distribution challenging. DisCo-Diff, in contrast, leverages only few discrete latents with small codebooks that act in harmony with the additional continuous DM.</p><p>There are previous related works that also condition DMs on auxiliary encodings. <ref type="bibr">Preechakul et al. (2022)</ref> augment DMs with non-spatial latent variables, but their latents are continuous and high-dimensional, which makes training their latent DM more challenging. This is precisely what we avoid by instead using low-dimensional and discrete latents. Moreover, they focus on semantic face image manipulation, not high-quality synthesis for challenging, diverse datasets.</p><p>Harvey &amp; Wood (2023) use the representations of a pre-trained CLIP image encoder <ref type="bibr">(Radford et al., 2021)</ref> for conditioning a DM and learn another DM over the CLIP embeddings for sampling. Similarly, <ref type="bibr" target="#b3">Bao et al. (2022)</ref> and <ref type="bibr">Hu et al. (2023)</ref> use clustered MoCo-based <ref type="bibr" target="#b24">(He et al., 2020)</ref> and clustered DINO-based <ref type="bibr">(Caron et al., 2021)</ref> features, respectively, for conditioning. Hence, these three approaches are strictly limited to image synthesis, where such encoders, pre-trained on large-scale datasets, are available. In contrast, we purposefully avoid the use of pre-trained networks and learn the discrete latents jointly with the DM, making our framework universally applicable. Another related work is InfoDiffusion <ref type="bibr">(Wang et al., 2023)</ref>, which also conditions DMs on discrete latent variables. However, contrary to DisCo-Diff, this work focuses on learning disentangled representations, similar to β-VAEs <ref type="bibr" target="#b26">(Higgins et al., 2017)</ref>, primarily for low-resolution face synthesis.</p><p>It uses a mutual information-based objective and does not focus on diverse and high-quality synthesis of complex data such as ImageNet.</p><p>In contrast to the above works, we show how discrete latent variables boost generative performance itself and we significantly outperform these works in complex and diverse high-quality synthesis. Furthermore, we motivate DisCo-Diff fundamentally, with reduced ODE curvature and model complexity, providing a new and complementary perspective.</p><p>In the molecular docking literature, since DiffDock <ref type="bibr">(Corso et al., 2023)</ref> introduced the use of diffusion models in the task, a number of works have proposed different modifications to its framework. In particular, some <ref type="bibr" target="#b59">(Masters et al., 2023;</ref><ref type="bibr" target="#b65">Plainer et al., 2023;</ref><ref type="bibr" target="#b21">Guo et al., 2023)</ref> have proposed to separate the blind docking task between pocket identification (i.e. identifying the region of the protein where the small molecule would bind) and pose prediction (i.e. predicting the specific pose with which the ligand would bind to the protein), as previously done in many traditional approaches <ref type="bibr" target="#b49">(Krivák &amp; Hoksza, 2018)</ref>. One could see this as hand-crafting a (roughly discrete) latent variable in the pocket and using it to decompose the task. By allowing the encoder to learn arbitrary discrete latents through its interaction with the denoiser, DisCo-Diff largely includes the above-mentioned strategy as a particular case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discrete Latent Variable Classifier-Free Guidance</head><p>Classifier-free guidance <ref type="bibr">(Ho &amp; Salimans, 2021</ref>) (cfg) is a mode-seeking technique commonly used in diffusion literature, such as class-conditioned genreation <ref type="bibr" target="#b63">(Peebles &amp; Xie, 2023)</ref> or text-to-image generation <ref type="bibr">(Rombach et al., 2022)</ref>. It generally guides the sampling trajectories toward higher-density regions. We can similarly apply classifier-free guidance in the DisCo-Diff, where we treat the discrete latent as conditional inputs. We follow the convention in <ref type="bibr" target="#b31">(Saharia et al., 2022)</ref>, and the classifier-free guidance at time step t is as follows:</p><formula xml:id="formula_6">Dθ (x, σ(t), z) = wD θ (x, σ(t), z) + (1 -w)D θ (x, σ(t), ∅),</formula><p>where D θ (x, σ(t), z)/D θ (x, σ(t), ∅) is the conditional/unconditional models, sharing parameters. We drop the discrete latent with probability 0.1 during training, to train the unconditional model D θ (x, σ(t), ∅). A mild w would usually lead to improvement in sample diversity <ref type="bibr" target="#b63">(Peebles &amp; Xie, 2023)</ref>. Table <ref type="table" target="#tab_2">3</ref> demonstrates that using a moderate guidance scale w=1 (we use w = 1 and cfg=1 interchangeably in the paper) improves the FID score, suggesting that the learned discrete latent in the DisCo-Diff framework has strong indications of mode of data distribution. We further explore varying the guidance scale on ImageNet-128. As shown in Fig <ref type="figure" target="#fig_7">9</ref>, increasing the classifier-free guidance scale w would strengthen the effect of guidance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Algorithm Pseudocode</head><p>We provide algorithm pseudocode for the training in the first stage for denoiser and encoder (Alg 1) and the second stage for auto-regressive model (Alg 2) for clarity. We also include the pseudocode for sampling in Alg 3. Note that we generalize the equations in the main text by considering the conditional generation with condition c. </p><formula xml:id="formula_7">for i = 0, . . . , T -1 do 3: Sample mini-batch data {(y i , c i )} B i=1 from D 4: Sample time variables {t i } B i=1 from p(t) and noise vectors {n i ∼ N (0, σ 2 i I)} B i=1 5: Get perturbed data {ŷ i = y i + n i } B i=1 6:</formula><p>Sample the discrete latent from the encoder {z i ∼ E ϕ (y i )} B i=1 using Gumbel-Softmax relaxation with temperature τ 7:</p><p>Calculate loss ℓ(θ, ϕ) Sample the discrete latent from the encoder {z i ∼ E ϕ (y i )} B i=1 using Gumbel-Softmax relaxation with temperature τ</p><formula xml:id="formula_8">= B i=1 λ(t)∥D θ (ŷ i , t i , z i , c i ) -y i ∥ 2</formula><formula xml:id="formula_9">5: Calculate loss ℓ(ψ) = B i=1 m j log p ψ ((z i ) j |(z i ) &lt;j , c i ) 6:</formula><p>Update the network parameter ψ via Adam optimizer 7: end for D. ImageNet Experiments  Sample z i ∼ p ψ (z i |z &lt;i , c) 5: end for 6: / * Diffusion ODE (Heun's 2nd order method) * / 7: Sample x 0 ∼ N (0, t 2 0 I) 8: for i = 0, . . . , n -1 do 9:</p><formula xml:id="formula_10">d i = (x i -D θ (x i , t i , z, c))/t i 10: x i+1 = x i + (t i+1 -t i )d i 11: if t i+1 ̸ = 0 then 12: d ′ i = (x i+1 -D θ (x i+1 , t i+1 , z, c))/t i+1</formula><p>13:</p><formula xml:id="formula_11">x i+1 = x i + (t i+1 -t i )( 1 2 d i + 1 2 d ′ i ) 14:</formula><p>end if 15: end for discrete latent to 1000. Below we provide architecture details for the denoiser network, encoder, and auto-regressive model. Table <ref type="table" target="#tab_7">5</ref> also lists some key network configurations. Please see the source code in the Supplementary Material for all low-level details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Denoiser Neural Network.</head><p>(1) ImageNet-64: We use the same UNet architecture in EDM <ref type="bibr" target="#b41">(Karras et al., 2022)</ref> for ImageNet-64, with newly injected cross-attention layers after each self-attention layers in each residual block. We feed the discrete vector into a six-layer Transformer encoder (with latent dimension 192) to obtain the corresponding embeddings for discrete variables. These embeddings are then input into the cross-attention layers. (2) ImageNet-128: We employ the UViT design in simple diffusion <ref type="bibr">(Hoogeboom et al., 2023)</ref> and VDM++ <ref type="bibr" target="#b45">(Kingma &amp; Gao, 2023)</ref>. UViT uses convolutional layers for down-/up-sampling, and a 36-layer ViT to process the lowest-resolution feature maps in the bottleneck, to strike a better balance between expressiveness and computation. Since the authors didn't release the code and model, we reimplemented the architecture by ourselves. We empirically observe that the convolutional blocks in EDM work better than the ones described in the simple diffusion paper, so we combine the up-/down-sampling blocks in EDM with the 36-layer ViT at the bottleneck layer. We further introduce a cross-attention layer for discrete latent in each up-/down-sampling block, and every three Transformer blocks in the ViT (e.g., 12 new cross-attention layers in the ViT) to save computation.</p><p>Encoder. We utilize a 12-layer standard ViT <ref type="bibr" target="#b17">(Dosovitskiy et al., 2021)</ref> as the backbone for encoder. Its latent dimension is 768 and the number of attention heads is 12. The patch size for ImageNet-64 is 8 × 8 and for ImageNet-128 is 16 × 16. We treat each of the m discrete latents as a classification token, and concatenate their embeddings with the path embeddings.</p><p>Auto-regressive model. We use a standard Transformer decoder <ref type="bibr" target="#b91">(Vaswani et al., 2017)</ref>, with a depth of 12, a number of heads of 8, and a latent dimension of 512. The inference time of the auto-regressive model is much smaller than the iterative denoising process, given that the discrete latent only has 10 dimensions. To generate 32 images on ImageNet-128, the auto-regressive model takes 0.44 seconds, while the diffusion model takes 78 seconds for 114 NFE, with an average of 0.68 s/NFE on a single NVIDIA A100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Training and Sampling</head><p>We borrow the preconditioning techniques, training noisy schedule, optimizers, exponential moving average (EMA) schedule, and hyper-parameters from previous state-of-the-art diffusion model EDM <ref type="bibr" target="#b41">(Karras et al., 2022)</ref> on ImageNet-64. We employ the shifted EDM-monotonic noisy schedule proposed in VDM++ <ref type="bibr" target="#b45">(Kingma &amp; Gao, 2023)</ref> on ImageNet-128, and keep other training details the same in ImageNet-64. We use the Gumbel-Softmax <ref type="bibr" target="#b38">(Jang et al., 2016)</ref> as the continuous relaxation for the discrete latents. The temperature τ in Gumbel-Softmax controls the smoothness of the categorical distribution. When τ → 0, the expected value of the Gumbel-Softmax is the same as the one of the underlying predicted distribution. As we increase t, the Gumbel-Softmax would gradually converge to a uniform distribution. Hence, a relatively large τ effectively provides regualization effects. During training, we set the τ to a constant 1. However, for the extraction of latents from training images, which aids in constructing the dataset for the second stage of the auto-regressive model, we adjust τ to a lower value of 0.01.</p><p>We use Heun's second-order ODE solver as the default ODE sampler, which is proven effective in previous works <ref type="bibr" target="#b41">(Karras et al., 2022;</ref><ref type="bibr">Xu et al., 2023b)</ref>. We directly use the hyper-parameters for the 623 NFE setting in Restart sampler <ref type="bibr">(Xu et al., 2023a)</ref> on ImageNet-64, for DisCo-Diff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Evaluation</head><p>For the evaluation, we follow the standard protocol and compute the Fréchet distance between 50000 generated samples and the training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Molecular Docking</head><p>In this appendix, we will introduce the task of molecular docking and some of the existing approaches to tackle it for readers who are not familiar with this field. Experienced readers may skip to Section E.3 where we start describing the details of our docking approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Task Overview</head><p>Molecular docking consists of finding the 3D structure that a protein (also referred to as receptor) and a small molecule (or ligand) take when binding. This is an important task in drug design because most drugs are small molecules that operate by binding to a specific protein of interest in our body and inhibiting or enhancing its function. The common particular instantiation of the docking problem that we consider is also referred to as rigid blind docking i.e. where we are given as input the correct protein structure (rigid) but are not provided any information about where the ligand will bind on the protein nor the conformations it will take (blind).</p><p>Ground truth data for training and testing is obtained through experimental methods, like X-ray crystallography, that, for each protein-ligand complex, allow to observe a particular pose that protein and ligand took when binding together inside of the crystal. Although there may be other poses that this particular protein and ligand may take when binding in a natural environment, methods are evaluated based on their capacity to retrieve the crystal pose. This accuracy is typically computed as the percentage of test complexes where the predicted structure of the ligand (also referred to as ligand pose) is within a root mean square distance (RMSD) of 2 Å from the ground truth when aligning the protein structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Related Work</head><p>Traditional approaches tackled the task via a search-based paradigm where, given a scoring function, they would search over possible ligand poses with an optimization algorithm to find a global minimum <ref type="bibr" target="#b22">(Halgren et al., 2004;</ref><ref type="bibr" target="#b86">Trott &amp; Olson, 2010)</ref>. Recently, deep learning methods have been trying to speed up this search process by generating poses directly through a neural network. Initial approaches <ref type="bibr" target="#b85">(Stärk et al., 2022;</ref><ref type="bibr" target="#b57">Lu et al., 2022)</ref> used regression-based frameworks to predict the pose, but, although significantly faster, they did not outperform traditional methods. <ref type="bibr">Corso et al. (2023)</ref> argued that the issue with these regression-based approaches is their treatment of the uncertainty in the multimodal model posterior pose distribution. They also proposed DiffDock, a diffusion-based generative model to generate docked poses that was able to outperform previous methods, which we use as a starting point for the integration of our DisCo-Diff approach to diffusion.</p><p>Most deep learning approaches to docking model the data as a geometric graph or point cloud in 3D. The nodes of this graph are the (heavy) atoms of the ligand and, typically, the C-alpha carbon atoms of the protein backbone (sometimes full-atom representations are also used for the protein but these are less common for computational complexity reasons). These nodes are connected by edges in case of chemical bonds or pairwise distances below a certain cutoff. Neural architectures learn features over the nodes of this graph through a number of message passing layers, the geometric structure is encoded via invariant (e.g. relying only on distance embeddings, see <ref type="bibr" target="#b78">Schütt et al. (2017)</ref>) or equivariant operations <ref type="bibr" target="#b20">(Geiger &amp; Smidt, 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Latent Variables</head><p>We design each latent variable to take values indicating one of the nodes in the protein-ligand joint graph. Therefore the codebook size for the latent variable of any given protein-ligand complex is equal to the total number of nodes in the graph i.e. the sum of the number of atoms in the ligand and the number of residues in the protein. With this choice, intuitively each latent variable will indicate one particular atom or residue involved in some key component of the protein-ligand interaction. For example, using two latents the model can learn to indicate a geometric contact between a pair of nodes in the final representation. Further note, each "codebook", when considered as a set of one-hot vectors indicating notes, has a permutation equivariance property with the nodes of the graph (because they are associated with node properties): if the nodes of the input graph are permuted each latent variable coming out of the encoder or autoregressive model will also have its codebook representation permuted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4. Architecture Details</head><p>Denoiser. This design choice for the discrete latents codebooks fits very well with the preexisting DiffDock's denoiser architecture composed of equivariant message passing layers <ref type="bibr" target="#b20">(Geiger &amp; Smidt, 2022)</ref>. Each latent variable is encoded in a binary label for each node which is set to zero for all nodes except the one indicated by the latent. These binary labels are concatenated to the initial node features while the rest of the denoiser is kept unchanged. With probability 0.1 during training we drop the latents, in this case, the binary labels are set to zero for all latents, and a learnable null-embedding is fed to all initial node features.</p><p>Encoder. The encoder and autoregressive models adopt very similar architectures to the denoiser with a few key distinctions. The encoder takes as input the ground truth pose of the ligand, learns features for each node through message passing, and finally m separate feedforward MLPs (where m is the number of latents) with a one-dimensional output are applied to each node representation. The concatenated outputs of each of these MLPs form the logit vectors for each of the latent variables which are passed through the Gumbel-Softmax discretization step.</p><p>Autoregressive model. Unlike the image synthesis experiments setting where the images are often generated with relatively vague conditioning information, for docking, we are interested in generating ligand poses conditioned on a particular protein (structure) and ligand. This conditioning information significantly influences the posterior pose distribution and consequently the learned latent variables. Therefore, we need to condition the autoregressive model on the protein structure and the ligand. We achieve this, once again, through an equivariant message passing network, operating on an input composed of the protein structure and the ligand. The latter is centered at the protein's center, given an arbitrary conformer (i.e. molecular conformation) from RDKit <ref type="bibr" target="#b50">(Landrum, 2013)</ref> and a uniformly random orientation. Like the denoiser, the autoregressive model takes as input the additional binary node labels for the existing latents (masked out appropriately during training), and, like the encoder, it uses its final node embeddings to predict the logits for the next latent variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5. Experimental Details</head><p>For the docking experiments, we follow the datasets and procedures established by <ref type="bibr" target="#b85">Stärk et al. (2022)</ref> and <ref type="bibr">Corso et al. (2023)</ref>. Data for training and evaluation comes from the PDBBind dataset <ref type="bibr" target="#b56">(Liu et al., 2017)</ref> with time-based splits (complexes before 2019 for training and validation, selected complexes from 2019 for testing).</p><p>Denoiser. We use a denoiser architecture analogous to the one proposed by <ref type="bibr" target="#b12">Corso et al. (2024)</ref>, which is a smaller version of DiffDock's original architecture where the main changes are: (1) 5 convolutional layers (vs 6 of the original DiffDock's architecture) (2) node representations with 24 scalars and pseudoscalars and 6 vectors and pseudovectors (vs respectively 48 and 10) (3) spherical harmonics order limited to 1 (vs 2). These changes, although somewhat affecting the inference quality, make training and testing of the models significantly more affordable (from 18 days on 4 GPUs to 9 days on 2 GPUs for training).</p><p>We keep the same denoiser architecture for both the baseline without discrete latents (DiffDock-S) and our model and apply similar hyperparameter searches when applicable to both models. At inference time, similarly to <ref type="bibr">Corso et al. (2023)</ref>, we take 40 independent samples and use the original DiffDock's confidence model<ref type="foot" target="#foot_0">foot_0</ref> to select the top one. For DisCo-DiffDock each of the samples is taken by independently sampling from the autoregressive model and then the (conditioned) denoiser.</p><p>Encoder. For the encoder, we use a similar but slightly smaller architecture with 3 convolutional layers, 24 scalars, and 4 vectors. We set the number of discrete latent variables (each taking values over the whole set of possible nodes in the joint graph) to two, as we found this to equilibrate the complexity of the generative task between the score and autoregressive models.</p><p>Autoregressive model. One challenge with the autoregressive model in this domain is its tendency to overfit the latent variables in the training set given the limited training data, the complexity of the conditioning information, and the low training signal that discrete labels provide. Therefore we found it beneficial to design the autoregressive model to use the pretrained layers of the denoiser itself. In particular, we simply add independent MLPs for each latent variable that are applied to the final scalar representations of the nodes. During the autoregressive training, for the first five epochs, the weights of the convolutional layers are frozen.</p><p>Inference hyperparameters. For inference, we maintain the number of inference steps from DiffDock (20) and, for both DisCo-DiffDock and the baseline, we tune on the validation set the sampling temperature for the different components of the diffusion similarly to how it was done by <ref type="bibr" target="#b42">Ketata et al. (2023)</ref>. For DisCo-DiffDock we also tune the temperature used to sample the autoregressive model. We find, with 40 samples, to be beneficial to set this temperature &gt; 1 while the diffusion sampling temperature &lt; 1, this corresponds to encouraging exploration of different binding modes while trying to obtain the maximum likelihood pose for each mode. This further highlights the advantage provided by enabling the decomposition of different degrees of uncertainty. Please see the source code in the Supplementary Material for all low level details and hyperparameters used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Gaussian Mixture Experiments</head><p>F.1. Data Generation For the toy example in section 3, we set the true data distribution to a mixture of eight Gaussian components:</p><formula xml:id="formula_12">p data (x) = 1 8 8 i=1 N (x; µ i , σ i I 2×2 )</formula><p>where ∀i, σ i = 0.2, and</p><formula xml:id="formula_13">µ 1 = 3 0 , µ 2 = -3 0 , µ 3 = 0 3 , µ 4 = 0 -3 , µ 5 = 3 √ 2 3 √ 2 , µ 6 = 3 √ 2 -3 √ 2 , µ 7 = 3 √ 2 -3 √ 2 , µ 8 = -3 √ 2 -3 √ 2</formula><p>.</p><p>To construct the toy dataset, we randomly sampled 1000 data points from each component, totaling 8000 data points. We visualize the KDE plot of the generated data in Fig. <ref type="figure" target="#fig_1">3</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Training and Sampling</head><p>We employ a four-layer MLP as the diffusion decoder (Denoiser Neural Network) G, for both Disco-Diff and diffusion models. We use a three-layer MLP as the encoder E in Disco-Diff. We set the latent dimension of discrete latent to 1 and the vocabulary size to 8. Ideally, each discrete latent should correspond to a Gaussian component, and the time-dependent scores for a single Gaussian component have a simple analytical expression. We leverage this simplicity and reparameterize the output of diffusion decoder as G(x, t, z) = F (z)-x t 2 +σ 2 1 + H(x, t), where F is the embedding for each discrete latent z and H is a four-layer MLP. The model optimization uses the Adam optimizer with a learning rate of 1e-3.</p><p>For sampling, we use the Heun's second-order sampler. We followed the time discretization scheme in EDM <ref type="bibr" target="#b41">(Karras et al., 2022)</ref> with 50 sampling steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. Metric</head><p>We detail the metrics used in Fig. <ref type="figure" target="#fig_2">4</ref>. The curvature for points x(t) on ODE trajectory dx/dt = G(x, t, z) (z is null in diffusion models) is defined as κ(x(t)) = ||∂tT (x(t),t)|| ||x ′ (t)|| where T (x, t) = G(x(t),t,z) ||G(x(t),t,z)|| is the unit tangent vector. We can approximate the curvature by finite difference: κ(x(t)) = ||∂tT (t)|| ||x ′ (t)|| ≈ ||T (t)-T (t-∆t)|| ||x(t)-x(t-∆t)|| . We approximate x(t -∆t) by a single Euler step, i.e., x(t -∆t) = x(t) -G(x, t, z)∆t. In Fig. <ref type="figure" target="#fig_2">4</ref>(a), we report the expected curvature given the backward time when simulating the ODE, i.e., E x(t) ||T (t)-T (t-∆t)|| ||x(t)-x(t-∆t)|| . We set the time elapsed to ∆t = 0.001. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5. Overfitting and Encoder Collapse in Continuous Latent</head><p>In this section, we show that it is difficult to control the amount of information stored in the continuous latents, which would lead to either overfitting or encoder collapse. To illustrate the issue, we derived the continuous / discrete latents from a specific real image, and fed the corresponding latents into the denoisers. As shown in Figure <ref type="figure" target="#fig_5">17</ref>, when KLD weight=0.1, the continuous latent model exhibits the overfitting issue, as all the generated images are very similar to the training image. It also indicates that the encoder squeezes excessive information in the continuous latent when using a smaller KLD weight, which complicates generative training in the second stage. When KLD weight=1, the model exhibits encoder collapsethe denoiser would ignore the continuous latent. We observe that even using a different continuous latent, the model will still generate an identical batch of samples. In contrast, DisCo-Diff generated a batch of diverse samples, sharing a similar high-level layout and color with the training image. This indicates that the discrete latents in DisCo-Diff encode global layout and color attributes -key statistical elements crucial for the diffusion process in Euclidean space. This aligns with more direct and straighter ODE trajectories.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) 128 × 128 (b) Shared discrete latents Figure 2. Samples generated from DisCo-Diff trained on the Ima-geNet dataset: (a) randomly sampled discrete latents and class labels; (b) samples in each grid sharing the same discrete latent. The class label for the top/bottom row is fixed to coffeepot/malamute. a categorical distribution of codebook size k. There are three learnable components: the denoiser neural network D θ : R d × R × N m → R d , corresponding to DisCo-Diff's DM, which predicts denoised images conditioned on diffusion time t and discrete latent z; an encoder E ϕ : R d → N m , used to infer discrete latents given clean images y. It outputs a categorical distribution over the k categories for each discrete latent; and a post-hoc auto-regressive model A ψ , which approximates the distribution of the learned discrete latents z by m i=1 p ψ (z i |z &lt;i ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Modeling 2D mixture of Gaussians. Left: Data distribution. Middle: Generated data by regular DM. Right: Generated data by DisCo-Diff. We use different colors to distinguish data generated by different discrete latents. We further provide zoomins and visualize some ODE trajectories by dotted lines.</figDesc><graphic coords="4,55.44,67.06,243.00,120.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Modeling 2D mixture of Gaussians: analysis. The mean curvature (left) and norm of the neural networks' Jacobians (right) along the reverse-time ODE trajectories as function of t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Group hierarchical DisCo-Diff. Different discrete latents are fed to the denoiser U-Net at different feature resolutions.</figDesc><graphic coords="5,70.45,67.05,202.54,136.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Top: Images created from two 30-dim discrete latents z and ẑ, with the far-right column combining their sub-coordinates.Bottom: Variations in images by fixing portions of z (originating from the red-boxed image). We see that lower-resolution latents affect layout / shape; high-resolution latents alter color / texture.</figDesc><graphic coords="8,61.85,67.06,218.70,157.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Left: Loss versus time. Right: Impact of discrete latent switching during the iterative sampling process of DisCo-Diff's diffusion model component. The numbers represent the percentage of the total sampling steps. The blue/green arrows mean the sampling steps that utilize the discrete latent associated with the leftmost/rightmost grid in the figure.For the middle two images, the process involves initially employing the discrete latent from the leftmost grid for a certain proportion of the total sampling steps (e.g., 75%), before transitioning to the discrete latent from the rightmost grid to complete the remaining steps (e.g., the last 25% of the total sampling steps).</figDesc><graphic coords="9,202.66,72.04,311.00,90.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Examples of alternative docking poses modeled when conditioning on different discrete latents, the "correct" z (i.e. same as the encoder) and an incorrect ẑ. The DM maps them to two distinct sets of orientations with which the ligand could fit in the pocket. Notably, the correct latent corresponds to poses within 2 Å of the ground truth. The colored beads are set on the atoms corresponding to the first latent variable.</figDesc><graphic coords="9,98.02,578.37,144.29,64.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Generated samples in DisCo-Diff with a cfg scale ranging from 0 to 8, under the class label "malamute" on ImageNet-128.</figDesc><graphic coords="17,55.44,72.04,111.78,111.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>parameter θ and ϕ via Adam optimizer 9: end for Algorithm 2 Mini-batch training of auto-regressive model in DisCo-Diff 1: Input: Auto-regressive model A ψ , encoder E ϕ , training dataset D, batch size B, Gumbel-Softmax temperature τ , training iteration T 2: for i = 0, . . . , T -1 do 3: Sample mini-batch data {(y i , c i )} B i=1 from D 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>For</head><label></label><figDesc>all the ImageNet experiments, we fix the latent dimension m = 10 in DisCo-Diff, and the codebook size for each DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents Algorithm 3 Sampling procedure of DisCo-Diff 1: Input: Auto-regressive model A ψ , denoiser D ϕ , time discretization {t i } n i=0 , condition c 2: / * Sample discrete latent from auto-regressive model * / 3: for i = 1, . . . , m do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure13. Images generated by different samplers. When using the DDPM sampler at smaller times (b), the generated images exhibit a slight over-smoothing issue, losing some high-frequency details in comparison to those produced with the VDM++ DDPM sampler at smaller times. Note that FID score typically penalizes over-smooth samples. This observation supports our hypothesis that the use of the DDPM sampler in DisCo-Diff at smaller times can in certain situations overlook high-frequency details.</figDesc><graphic coords="25,55.44,72.04,155.52,155.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Generated samples by DisCo-Diff on class-conditioned ImageNet-128, with ODE sampler. Samples in each grid share the same latent, and grids in each row share the same class labels. We can see that generally, images sharing the same discrete latents demonstrate similar global characteristics, such as shape, layout, and color, despite being under the same class. It suggests that discrete latents provide complementary information to the class labels.</figDesc><graphic coords="26,102.79,101.72,388.80,522.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Generated images with a shared latent, using group hierarchical DisCo-Diff trained on ImageNet-64. Left: Shared latent z.Middle: Shared latent ẑ. Right: Shared latent (z0:20, ẑ20:30), where the first 20 coordinates are from z and the last 10 coordinates are from ẑ. We can see that the generated images from composed latents generally inherit the shape from images generated by z, and the color from images generated by ẑ.</figDesc><graphic coords="27,102.79,101.02,388.80,524.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 .Figure 17 .</head><label>1617</label><figDesc>Figure 16. Progressively fixing more subcoordinates of the discrete latents, using our group hierarchical DisCo-Diff on ImageNet-64. Left: Randomly sampled z. Middle: Fixing the first 20 coordinates z:20 as the one derived from the red-boxed image, sampling the rest. Right: Fixing the whole 30-dim. z as the one derived from the red-boxed image. The figure shows the effect when progressively fixing more coordinates of the discrete latent, and sampling the remaining coordinates by the auto-regressive model. The images first converge in shape/layout, and subsequently converge in color/texture.</figDesc><graphic coords="28,102.79,100.15,388.81,515.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,50.90,67.96,311.73,175.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="23,127.10,124.80,340.20,509.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>FID score and NFE on class-cond. ImageNet-128.</figDesc><table><row><cell>FID NFE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablations on class-cond. ImageNet-64.</figDesc><table><row><cell></cell><cell>FID</cell></row><row><cell>EDM (Karras et al., 2022)</cell><cell>2.36</cell></row><row><cell>Oracle setting</cell><cell></cell></row><row><cell>Continuous latent (KLD weight=0.1)</cell><cell>1.67</cell></row><row><cell>Continuous latent (KLD weight=1)</cell><cell>2.36</cell></row><row><cell>DisCo-Diff (cfg=0)</cell><cell>1.65</cell></row><row><cell>Generative prior on latent</cell><cell></cell></row><row><cell>Continuous latent (KLD weight=0.1)</cell><cell>11.12</cell></row><row><cell>Continuous latent (KLD weight=1, cfg=0)</cell><cell>2.36</cell></row><row><cell>Continuous latent (KLD weight=1, cfg=1)</cell><cell>2.36</cell></row><row><cell>DisCo-Diff (cfg=0)</cell><cell>1.81</cell></row><row><cell>DisCo-Diff (cfg=1)</cell><cell>1.65</cell></row><row><cell>DisCo-Diff (cfg=2)</cell><cell>2.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. (1) DisCo-Diff achieves the new state-of-the-art on class-conditioned ImageNet-64/ImageNet-128 when using ODE sampler. Specifically, DisCo-Diff reduces the previous state-of-the-art FID score from 2.36 to 1.65 on ImageNet-64, and from 2.29 to 1.98 on ImageNet-128. This aligns with our analysis (Sec. 3.2) that DisCo-Diff yields straighter ODE trajectories.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 D.2 Training and Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 D.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19</figDesc><table><row><cell>Our model DisCo-Diff advances the modeling power of</cell><cell></cell></row><row><cell>diffusion models for data generation. While enhancing</cell><cell></cell></row><row><cell>data generation capabilities, notably in high-quality image</cell><cell></cell></row><row><cell>and video creation, these models also present challenges,</cell><cell></cell></row><row><cell>such as the potential misuse in deepfake technology</cell><cell></cell></row><row><cell>leading to social engineering concerns. Addressing</cell><cell></cell></row><row><cell>these issues necessitates further research into watermark</cell><cell></cell></row><row><cell>algorithms for diffusion models and collaboration with</cell><cell></cell></row><row><cell>socio-technical disciplines to balance innovation with D ImageNet Experiments</cell><cell>17</cell></row><row><cell>ethical considerations. We would also like to highlight the promise of deep generative models like DisCo-Diff in the natural sciences, as exemplified by our molecular docking experiments. Such models have the potential to provide novel insights into, for instance, the interactions between proteins and ligands and advance drug discovery. D.1 E Molecular Docking</cell><cell>19</cell></row></table><note><p>E.1 Task Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 E.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 E.3 Latent Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 E.4 Architecture Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 E.5 Experimental Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 F Gaussian Mixture Experiments 21 F.1 Data Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 F.2 Training and Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 F.3 Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 G Additional Samples and Experiments 22 G.1 Loss Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 G.2 Class-conditoned ImageNet-64 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 G.3 Class-conditoned ImageNet-128 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 G.4 Group Hierarchical DisCo-Diff . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 G.5 Overfitting and Encoder Collapse in Continuous Latent . . . . . . . . . . . . . . . . . . . . . . . . . . . 25</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Algorithm 1 Mini-batch training of denoiser and encoder in DisCo-Diff 1: Input: Denoiser D θ , encoder E ϕ , training dataset D, batch size B, Gumbel-Softmax temperature τ , training iteration T 2:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Specific network configurations on ImageNet</figDesc><table><row><cell>D.1. Architecture</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Feature maps</cell><cell cols="2">Attention resolution Encoder patch size</cell></row><row><cell>ImageNet-64</cell><cell>1-2-3-4 (×192)</cell><cell>(8,16,32)</cell><cell>8 × 8</cell></row><row><cell cols="2">ImageNet-128 1-2-4-16 (×128)</cell><cell>(16,32,64)</cell><cell>16 × 16</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The confidence model is an additional model,Corso et al. (2023)  trained to select the most likely correct poses out of the diffusion models samples. The reader can think of this as trying to select the maximum likelihood pose.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>YX and TJ acknowledge support from <rs type="funder">MIT-DSTA</rs> Singapore collaboration; YX, GC, and TJ from <rs type="funder">NSF</rs> <rs type="grantName">Expeditions grant</rs> (award <rs type="grantNumber">1918839</rs>) "<rs type="programName">Understanding the World Through Code</rs>", from <rs type="funder">MIT-IBM</rs> Grand <rs type="projectName">Challenge</rs> project, <rs type="funder">NSF</rs> <rs type="grantName">Expeditions grant</rs> (award 1918839: Collaborative Research: Understanding the World Through Code), <rs type="person">Machine Learning</rs> for Pharmaceutical Discovery and Synthesis (MLPDS) consortium; GC and TJ further acknowledge support from the <rs type="affiliation">Abdul Latif Jameel Clinic</rs> for <rs type="person">Machine Learning in Health</rs> and the <rs type="funder">DTRA Discovery of Medical Countermeasures Against New and Emerging (DOMANE)</rs> threats program.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mqpbbDn">
					<idno type="grant-number">1918839</idno>
					<orgName type="grant-name">Expeditions grant</orgName>
					<orgName type="program" subtype="full">Understanding the World Through Code</orgName>
				</org>
				<org type="funded-project" xml:id="_qFbVn28">
					<orgName type="project" subtype="full">Challenge</orgName>
				</org>
				<org type="funding" xml:id="_FxQnj3E">
					<orgName type="grant-name">Expeditions grant</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Fig. <ref type="figure">4</ref>(b), we measure the complexity of the trained neural networks using the expected squared Frobenius norm of the network's Jacobians, i.e., E x(t) ||∇ x G(x, t, z)|| 2 F . Additionally, to quantitatively evaluate the generation quality, we report the Wasserstein-2 (W-2) distance between the generated distribution and the ground truth distribution. In the DisCo-Diff model, the W-2 distance is at 0.118, compared to 0.27 in the standard diffusion model. It suggests that DisCo-Diff better captures the multimodal distribution, even in 2-dim space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Additional Samples and Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Loss Analysis</head><p>In Fig. <ref type="figure">10</ref>, we provide the loss versus time curve on both ImageNet-64 and ImageNet-128 datasets. We have also included a log-scale version of the x-axis in the inset plot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Class-conditoned ImageNet-64</head><p>We provide extended samples generated by DisCo-Diff in Fig. <ref type="figure">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. Class-conditoned ImageNet-128</head><p>We provide extended samples generated by DisCo-Diff in Fig. <ref type="figure">12</ref>. We also visualize samples with shared discrete latents in Fig. <ref type="figure">14</ref>. We further provide samples using different samplers in Fig. <ref type="figure">13</ref>, to highlight the over-smoothing issue of DisCo-Diff at smaller times we observed for this model when using the DDPM sampler. Although, in theory, the VDM++ model and DisCo-Diff should learn the same field at small times, in practice, we observe that VDM++ DDPM provides samplers with richer details compared to DisCo-Diff DDPM at smaller times. It supports our hypothesis that the discrete latents tend to divert the model to overlook the high-level details on ImageNet-128, when using the DDPM sampler and the network architecture in <ref type="bibr" target="#b45">(Kingma &amp; Gao, 2023)</ref>. We would like to emphasize that we only observed this behavior for this one model and dataset. In all other experiments, discrete latents universally improved performance for all stochastic and non-stochastic samplers, even when used for all times t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4. Group Hierarchical DisCo-Diff</head><p>We further provide extended samples from the Group hierarchical DisCo-Diff. Fig. <ref type="figure">15</ref> showcases the generated images when composing two discrete latents together, i.e., (z 0:20 , ẑ20:30 ). We can see that the generated images from composed latent generally inherit the shape from images generated by z, and the color from images generated by ẑ.</p><p>Fig. <ref type="figure">16</ref> further shows the effect when progressively fixing more coordinates of the discrete latent, and sampling the remaining coordinates by the auto-regressive model. The images first converge in shape/layout, and subsequently converge in color/texture.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Structured denoising diffusion models in discrete state-spaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bahmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Skorokhodov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Lindell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.17984</idno>
		<title level="m">Text-to-4d generation using hybrid score distillation sampling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01324</idno>
		<title level="m">eDiff-I: Text-to-Image Diffusion Models with Ensemble of Expert Denoisers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Why are conditional generative models better than unconditional ones</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.00362</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>ArXiv, abs/1809.11096</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A continuous time framework for discrete denoising models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Bortoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Deligiannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Instance-conditioned gan</title>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Careil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero-Soriano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Masked generative image transformer</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><surname>Maskgit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Textto-image generation via masked generative transformers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><surname>Muse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning (ICML)</title>
		<meeting>the 40th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stärk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><surname>Diffdock</surname></persName>
		</author>
		<title level="m">Diffusion steps, twists, and turns for molecular docking. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The discovery of binding modes requires rethinking docking generalization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Polizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diffusion Models Beat GANs on Image Synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Higherorder denoising diffusion solvers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><surname>Genie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Score-based generative modeling with critically-damped langevin diffusion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>b</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.09453</idno>
		<title level="m">e3nn: Euclidean neural networks</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Diffdocksite: A novel paradigm for enhanced protein-ligand predictions through binding site identification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mingdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2023 Generative AI and Biology</title>
		<imprint>
			<publisher>GenBio) Workshop</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glide: a new approach for rapid, accurate docking and scoring. 2. enrichment factors in database screening</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Halgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Friesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Beard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Frye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Banks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Visual chain-of-thought diffusion models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.16187</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">beta-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A practical guide to training restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="599" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Classifier-Free Diffusion Guidance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Denoising Diffusion Probabilistic Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cascaded diffusion models for high fidelity image generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Imagen</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><surname>Video</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02303</idno>
		<title level="m">High Definition Video Generation with Diffusion Models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simple Diffusion: End-to-End Diffusion for High Resolution Images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning (ICML)</title>
		<meeting>the 40th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-guided diffusion models</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Burghouts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A variational perspective on diffusion-based generative models and score matching</title>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Estimation of non-normalized statistical models by score matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<idno type="ISSN">1532-4435</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="695" to="709" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Illuminating protein space with a programmable generative model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baranov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Frappier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ismail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grigoryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">623</biblScope>
			<biblScope unit="page" from="1070" to="1078" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scalable adaptive computation for iterative generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno>ArXiv, abs/1611.01144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Elucidating the design space of diffusion-based generative models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<idno>ArXiv, abs/2206.00364</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ketata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Laue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mammadov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stärk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.03889</idno>
		<title level="m">Diffdock-pp: Rigid protein-protein docking with diffusion models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Consistency trajectory models: Learning probability flow ode trajectory of diffusion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Murata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Takida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Uesaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno>ArXiv, abs/2310.02279</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scene Generation with Hierarchical Latent Diffusion Models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><surname>Neuralfield-Ldm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>b</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Understanding diffusion objectives as the ELBO with simple data augmentation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Variational diffusion models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Lessons learned in empirical scoring with smina from the csar 2011 benchmarking exercise</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Koes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Camacho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Journal of chemical information and modeling</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">P2rank: machine learning based tool for rapid and accurate prediction of ligand binding sites from protein structure</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krivák</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoksza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Landrum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Rdkit documentation. Release</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Self-conditioned image generation via generating representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.03701</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.13763</idno>
		<title level="m">Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Flow matching for generative modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Le</surname></persName>
		</author>
		<idno>ArXiv, abs/2210.02747</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">252734897</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Zero-1-to-3: Zero-shot One Image to 3D Object</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Hoorick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Flow straight and fast: Learning to generate and transfer data with rectified flow</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.03003</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Forging the basis for developing protein-ligand interaction scoring functions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Accounts of Chemical Research</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Trigonometry-aware neural networks for drugprotein binding structure prediction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><surname>Tankbind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Interpretation and generalization of score matching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI &apos;09</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI &apos;09<address><addrLine>Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fusiondock: Physics-informed diffusion model for molecular docking</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Masters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Lill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML Workshop on Computational Biology</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Gnina 1.0: molecular docking with deep learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Mcnutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Francoeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Masuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ragoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sunseri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Koes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno>ArXiv, abs/2102.09672</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Point-E</surname></persName>
		</author>
		<title level="m">A System for Generating 3D Point Clouds from Complex Prompts</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4195" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Pernias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rampas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aubreville</surname></persName>
		</author>
		<author>
			<persName><surname>Wuerstchen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.00637</idno>
		<title level="m">An efficient architecture for large-scale text-to-image diffusion models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Diffdock-pocket: Diffusion for pocket-level docking with sidechain flexibility</title>
		<author>
			<persName><forename type="first">M</forename><surname>Plainer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dobers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stärk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2023 Workshop on New Frontiers of AI for Drug Discovery and Development</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Podell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Penna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><surname>Sdxl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.01952</idno>
		<title level="m">Improving Latent Diffusion Models for High-Resolution Image Synthesis</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Dream-Fusion: Text-to-3D using 2D Diffusion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Diffusion autoencoders: Toward a meaningful and decodable representation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Preechakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chatthee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wizadwongsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning Transferable Visual Models From Natural Language Supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning (ICML)</title>
		<meeting>the 38th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Zero-shot text-toimage generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning (ICML)</title>
		<meeting>the 38th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<title level="m">Hierarchical Text-Conditional Image Generation with CLIP Latents</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Discrete variational autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Rolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">High-Resolution Image Synthesis with Latent Diffusion Models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K S</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gontijo-Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Stylegan-xl: Scaling stylegan to large diverse datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2022 Conference Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Sauceda Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><surname>Wildfusion</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.13570</idno>
		<title level="m">Learning 3D-Aware Latent Diffusion Models in View Space</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Textto-Video Generation without Text-Video Data</title>
		<author>
			<persName><forename type="first">U</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gafni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><surname>Make-A-Video</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Text-to-4D Dynamic Scene Generation</title>
		<author>
			<persName><forename type="first">U</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sheynin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Makarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023">2023b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual Conference on Neural Information Processing Systems</title>
		<meeting>the 33rd Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Score-Based Generative Modeling through Stochastic Differential Equations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Geometric deep learning for drug binding structure prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Stärk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><surname>Equibind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading</title>
		<author>
			<persName><forename type="first">O</forename><surname>Trott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational chemistry</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Dvae#: Discrete variational autoencoders with relaxed Boltzmann priors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Andriyash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>a</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">DVAE++: Discrete variational autoencoders with overlapping transformations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macready</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khoshaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Andriyash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML)</title>
		<meeting>the 35th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>b</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Score-based Generative Modeling in Latent Space</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">InfoDiffusion: Representation learning using information maximizing diffusion models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Schiff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kuleshov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning (ICML)</title>
		<meeting>the 40th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">De novo design of protein structure and function with rfdiffusion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Juergens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Trippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Eisenach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ahern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Borst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Ragotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Milles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I M</forename><surname>Wicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hanikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pellock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courbet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sheffler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sappington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lauko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Bortoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dimaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">620</biblScope>
			<biblScope unit="page" from="1089" to="1100" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Poisson flow generative models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Restart sampling for improving generative processes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno>ArXiv, abs/2306.14878</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">PFGM++: Unlocking the potential of physics-inspired generative models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning (ICML)</title>
		<meeting>the 40th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>b</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Stable target field for reduced variance score estimation in diffusion models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno>ArXiv, abs/2302.00670</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Se(3) diffusion model with application to protein backbone generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Trippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Bortoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.02277</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Scaling autoregressive models for contentrich text-to-image generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Baid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Latent Point Diffusion Models for 3D Shape Generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><surname>Lion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">A unified approach for textand image-guided 4d scene generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Mello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.16854</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
