<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ladder Variational Autoencoders</title>
				<funder>
					<orgName type="full">Novo Nordisk Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Danish Innovation Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2016-05-27">27 May 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Casper</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
							<email>casperkaae@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Bioinformatics Centre</orgName>
								<orgName type="department" key="dep2">Department of Biology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lars</forename><surname>Maaløe</surname></persName>
							<email>larsma@dtu.dk</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Applied Mathematics and Computer Science</orgName>
								<orgName type="institution">Technical University of Denmark</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Søren</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
							<email>skaaesonderby@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Bioinformatics Centre</orgName>
								<orgName type="department" key="dep2">Department of Biology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ole</forename><surname>Winther</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Bioinformatics Centre</orgName>
								<orgName type="department" key="dep2">Department of Biology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Applied Mathematics and Computer Science</orgName>
								<orgName type="institution">Technical University of Denmark</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Aalto University</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ladder Variational Autoencoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-05-27">27 May 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1602.02282v3[stat.ML]</idno>
					<note type="submission">Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Variational autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recently introduced variational autoencoder (VAE) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref> provides a framework for deep generative models. In this work we study how the variational inference in such models can be improved while not changing the generative model. We introduce a new inference model using the same top-down dependency structure both in the inference and generative models achieving state-of-the-art generative performance.</p><p>VAEs, consisting of hierarchies of conditional stochastic variables, are highly expressive models retaining the computational efficiency of fully factorized models, Figure <ref type="figure">1 a</ref>). Although highly flexible these models are difficult to optimize for deep hierarchies due to multiple layers of conditional stochastic layers. The VAEs considered here are trained by optimizing a variational approximate posterior lower bounding the intractable true posterior. Recently used inference are calculated purely bottom-up with no interaction between the inference and generative models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. We propose a new structured inference model using the same top-down dependency structure both in the inference and generative models. Here the approximate posterior distribution can be viewed as merging information from a bottom up computed approximate likelihood with top-down prior information from the generative distribution, see Figure <ref type="figure">1 b</ref>). The sharing of information (and parameters) with the generative model gives the inference model knowledge of the current state of the generative model in each layer and the top down-pass recursively corrects the generative distribution with the data dependent approximate log-likelihood using a simple precision-weighted addition. This parameterization allows interactions between the bottom-up and top-down signals resembling the recently proposed Ladder Network <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b15">16]</ref>, and we therefore denote it Ladder-VAE (LVAE). For the remainder of this paper we will refer to VAEs as both the inference and generative model seen in Figure <ref type="figure">1 a</ref>) and similarly LVAE as both the inference and generative model in Figure <ref type="figure">1 b</ref>). We stress that the VAE and LVAE models only differ in the inference model, however these have similar number of parameters, whereas the generative models are identical.</p><p>Previous work on VAEs have been restricted to shallow models with one or two layers of stochastic latent variables. The performance of such models are constrained by the restrictive mean field approximation to the intractable posterior distribution. We found that purely bottom-up inference normally used in VAEs and gradient ascent optimization are only to a limited degree able to utilize the two layers of stochastic latent variables. We initially show that a warm-up period [1, 15, Section 6.2] to support stochastic units staying active in early training and batch normalization (BN) <ref type="bibr" target="#b5">[6]</ref> can significantly improve performance of VAEs. Using these VAE models as competitive baselines we show that LVAE improves the generative performance achieving as good or better performance than other (often complicated) methods for creating flexible variational distributions such as: The Variational Gaussian Processes <ref type="bibr" target="#b19">[20]</ref>, Normalizing Flows <ref type="bibr" target="#b16">[17]</ref>, Importance Weighted Autoencoders <ref type="bibr" target="#b1">[2]</ref> or Auxiliary Deep Generative Models <ref type="bibr" target="#b11">[12]</ref>. Compared to the bottom-up inference in VAEs we find that LVAE: 1) have better generative performance 2) provides a tighter bound on the true log-likelihood and 3) can utilize deeper and more distributed hierarchies of stochastic variables. Lastly we study the learned latent representations and find that these differ qualitatively between the LVAE and VAE with the LVAE capturing more high level structure in the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In summary our contributions are:</head><p>• A new inference model combining an approximate Gaussian likelihood with the generative model resulting in better generative performance than the normally used bottom-up VAE inference</p><p>• We provide a detailed study of the learned latent distributions and show that LVAE learns both a deeper and more distributed representation when compared to VAE</p><p>• We show that a deterministic warm-up period and batch normalization are important for training deep stochastic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>VAEs and LVAEs simultaneously train a generative model p θ (x, z) = p θ (x|z)p θ (z) for data x using latent variables z, and an inference model q φ (z|x) by optimizing a variational lower bound to the likelihood p θ (x) = p θ (x, z)dz. In the generative model p θ , the latent variables z are split into L layers z i , i = 1 . . . L as follows:</p><formula xml:id="formula_0">p θ (z) = p θ (z L ) L-1 i=1 p θ (z i |z i+1 )<label>(1)</label></formula><formula xml:id="formula_1">p θ (z i |z i+1 ) = N z i |µ p,i (z i+1 ), σ 2 p,i (z i+1 ) , p θ (z L ) = N (z L |0, I)<label>(2)</label></formula><p>p θ (x|z 1 ) = N x|µ p,0 (z 1 ), σ 2 p,0 (z 1 ) or P θ (x|z 1 ) = B (x|µ p,0 (z 1 ))</p><p>where observation models is matching either continuous-valued (Gaussian N ) or binary-valued (Bernoulli B) data, respectively. We use subscript p (and q) to highlight if µ or σ 2 sigma belongs to the generative or inference distributions respectively. The hierarchical specification allows the lower layers of the latent variables to be highly correlated but still maintain the computational efficiency of fully factorized models. The variational principle provides a tractable lower bound on the log likelihood which can be used as a training criterion L.</p><formula xml:id="formula_3">log p(x) ≥ E q φ (z|x) log p θ (x, z) q φ (z|x) = L(θ, φ; x) (4) = -KL(q φ (z|x)||p θ (z)) + E q φ (z|x) [log p θ (x|z)] ,<label>(5)</label></formula><p>where KL is the Kullback-Leibler divergence. A strictly tighter bound on the likelihood may be obtained at the expense of a K-fold increase of samples by using the importance weighted bound <ref type="bibr" target="#b1">[2]</ref>:</p><formula xml:id="formula_4">log p(x) ≥ E q φ (z (1) |x) . . . E q φ (z (K) |x) log K k=1 p θ (x, z (k) ) q φ (z (k) |x) ≥ L K (θ, φ; x) .<label>(6)</label></formula><p>The generative and inference parameters, θ and φ, are jointly trained by optimizing Eq. ( <ref type="formula" target="#formula_3">5</ref>) using stochastic gradient descent where we use the reparametrization trick for stochastic backpropagation through the Gaussian latent variables <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>. The KL[q φ |p θ ] is calculated analytically at each layer when possible and otherwise approximated using Monte Carlo sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Variational autoencoder inference model</head><p>VAE inference models are parameterized as a bottom-up process similar to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>. Conditioned on the stochastic layer below each stochastic layer is specified as a fully factorized gaussian distribution:</p><formula xml:id="formula_5">q φ (z|x) = q φ (z 1 |x) L i=2 q φ (z i |z i-1 )<label>(7)</label></formula><formula xml:id="formula_6">q φ (z 1 |x) = N z 1 |µ q,1 (x), σ 2 q,1 (x)<label>(8)</label></formula><formula xml:id="formula_7">q φ (z i |z i-1 ) = N z i |µ q,i (z i-1 ), σ 2 q,i (z i-1 ) , i = 2 . . . L.<label>(9)</label></formula><p>In this parameterization the inference and generative distributions are computed separately with no explicit sharing of information. In the beginning of the training procedure this might cause problems since the inference models have to approximately match the highly variable generative distribution in order to optimize the likelihood. The functions µ(•) and σ 2 (•) in the generative and VAE inference models are implemented as:</p><formula xml:id="formula_8">d(y) =MLP(y)<label>(10)</label></formula><formula xml:id="formula_9">µ(y) =Linear(d(y))<label>(11)</label></formula><formula xml:id="formula_10">σ 2 (y) =Softplus(Linear(d(y))) , (<label>12</label></formula><formula xml:id="formula_11">)</formula><p>where MLP is a two layered multilayer perceptron network, Linear is a single linear layer, and Softplus applies log(1 + exp(•)) nonlinearity to each component of its argument vector ensuring positive variances. In our notation, each MLP(•) or Linear(•) gives a new mapping with its own parameters, so the deterministic variable d is used to mark that the MLP-part is shared between µ and σ 2 whereas the last Linear layer is not shared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ladder variational autoencoder inference model</head><p>We propose a new inference model that recursively corrects the generative distribution with a data dependent approximate likelihood term. First a deterministic upward pass computes the approximate likelihood contributions:</p><formula xml:id="formula_12">d n =MLP(d n-1 )<label>(13)</label></formula><formula xml:id="formula_13">μq,i =Linear(d i ), i = 1 . . . L<label>(14)</label></formula><formula xml:id="formula_14">σ2 q,i =Softplus(Linear(d i )), i = 1 . . . L<label>(15)</label></formula><p>where d 0 = x. This is followed by a stochastic downward pass recursively computing both the approximate posterior and generative distributions:</p><formula xml:id="formula_15">q φ (z|x) =q φ (z L |x) L-1 i=1 q φ (z i |z i+1 )<label>(16)</label></formula><formula xml:id="formula_16">σ q,i = 1 σ-2 q,i + σ -2 p,i<label>(17)</label></formula><formula xml:id="formula_17">µ q,i = μq,i σ-2 q,i + µ p,i σ -2 p,i σ-2 q,i + σ -2 p,i<label>(18)</label></formula><formula xml:id="formula_18">q φ (z i |•) = N z i |µ q,i , σ 2 q,i ,<label>(19)</label></formula><p>where µ q,L = μq,L and σ 2 q,L = σ2 q,L . The inference model is a precision-weighted combination of μq and σ2</p><p>q carrying bottom-up information and µ p and σ 2 p from the generative distribution carrying top-down prior information. This parameterization has a probabilistic motivation by viewing μq and σ2 q as the approximate gaussian likelihood that is combined with a gaussian prior µ p and σ 2 p from the generative distribution. Together these form the approximate posterior distribution q θ (z|z, x) using the same top-down dependency structure both in the inference and generative model.</p><p>A line of motivation, already noted in <ref type="bibr" target="#b2">[3]</ref>, is that a purely bottom-up inference process as in i.e. VAEs does not correspond well with real perception, where iterative interaction between bottom-up and top-down signals produces the final activity of a unit <ref type="foot" target="#foot_0">4</ref> . Notably it is difficult for the purely bottom-up inference networks to model the explaining away phenomenon, see <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">Chapter 5</ref>] for a recent discussion on this phenomenon. The LVAE model provides a framework with the wanted interaction, while not increasing the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Warm-up from deterministic to variational autoencoder</head><p>The variational training criterion in Eq. ( <ref type="formula" target="#formula_3">5</ref>) contains the reconstruction term p θ (x|z) and the variational regularization term. The variational regularization term causes some of the latent units to become inactive during training <ref type="bibr" target="#b12">[13]</ref> because the approximate posterior for unit k, q(z i,k | . . . ) is regularized towards its own prior p(z i,k | . . . ), a phenomenon also recognized in the VAE setting <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>. This can be seen as a virtue of automatic relevance determination, but also as a problem when many units collapse early in training before they learned a useful representation. We observed that such units remain inactive for the rest of the training, presumably trapped in a local minima or saddle point at KL(q i,k |p i,k ) ≈ 0, with the optimization algorithm unable to re-activate them. We alleviate the problem by initializing training using the reconstruction error only (corresponding to training a standard deterministic auto-encoder), and then gradually introducing the variational regularization term:</p><formula xml:id="formula_19">L(θ, φ; x) T = -βKL(q φ (z|x)||p θ (z)) + E q φ (z|x) [log p θ (x|z)] , (<label>20</label></formula><formula xml:id="formula_20">)</formula><p>where β is increased linearly from 0 to 1 during the first N t epochs of training. We denote this scheme warm-up (abbreviated WU in tables and graphs) because the objective goes from having a delta-function solution (corresponding to zero temperature) and then move towards the fully stochastic variational objective. This idea have previously been considered in [15, Section 6.2] and more recently in <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>To test our models we use the standard benchmark datasets MNIST, OMNIGLOT <ref type="bibr" target="#b9">[10]</ref> and NORB <ref type="bibr" target="#b10">[11]</ref>. The largest models trained used a hierarchy of five layers of stochastic latent variables of sizes 64, 32, 16, 8 and 4, going from bottom to top. We implemented all mappings using MLP's with two layers of deterministic hidden units. In all models the MLP's between x and z 1 or d 1 were of size 512. Subsequent layers were connected by MLP's of sizes 256, 128, 64 and 32 for all connections in both the VAE and LVAE. Shallower models were created by removing latent variables from the top of the hierarchy. We sometimes refer to the five layer models as 64-32-16-8-4, the four layer models as 64-32-16-8 and so fourth. The models were trained end-to-end using the Adam <ref type="bibr" target="#b6">[7]</ref> optimizer with a mini-batch size of 256. We report the train and test log-likelihood lower bounds, Eq. ( <ref type="formula" target="#formula_3">5</ref>) as well as the approximated true log-likelihood calculated using 5000 importance weighted samples, Eq. ( <ref type="formula" target="#formula_4">6</ref>). The models were implemented using the Theano <ref type="bibr" target="#b18">[19]</ref>, Lasagne <ref type="bibr" target="#b3">[4]</ref> and Parmesan 5 frameworks. The source code is available at 6   For MNIST, we used a sigmoid output layer to predict the mean of a Bernoulli observation model and leaky rectifiers (max(x, 0.1x)) as nonlinearities in the MLP's. The models were trained for 2000 epochs with a learning rate of 0.001 on the complete training set. Models using warm-up used N t = 200. Similarly to <ref type="bibr" target="#b1">[2]</ref>, we resample the binarized training values from the real-valued images using a Bernoulli distribution after each epoch which prevents the models from over-fitting. Some of the models were fine-tuned by continuing training for 2000 epochs while multiplying the learning rate with 0.75 after every 200 epochs and increase the number of Monte Carlo and importance weighted samples to 10 to reduce the variance in the approximation of the expectations in Eq. ( <ref type="formula">4</ref>) and improve the inference model, respectively.</p><p>Models trained on the OMNIGLOT dataset 7 , consisting of 28x28 binary images images were trained similar to above except that the number of training epochs was 1500.</p><p>Models trained on the NORB dataset 8 , consisting of 32x32 grays-scale images with color-coding rescaled to [0, 1], used a Gaussian observation model with mean and variance predicted using a linear and a softplus output layer respectively. The settings were similar to the models above except that: hyperbolic tangent was used as nonlinearities in the MLP's and the number of training epochs was 2000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generative log-likelihood performance</head><p>In Figure <ref type="figure" target="#fig_1">3</ref> we show the train and test set log-likelihood on MNIST dataset for a series of different models with varying number of stochastic layers.</p><p>Consider the L test 1 , Figure <ref type="figure" target="#fig_1">3 b</ref>), the VAE without batch-normalization and warm-up does not improve for additional stochastic layers beyond one whereas VAEs with batch normalization and warm-up 5 github.com/casperkaae/parmesan 6 github.com/casperkaae/LVAE 7 The OMNIGLOT data was partitioned and preprocessed as in <ref type="bibr" target="#b1">[2]</ref>, <ref type="url" target="https://github.com/yburda/iwae/tree/master/datasets/OMNIGLOT">https://github.com/yburda/iwae/tree/master/datasets/OMNIGLOT</ref>  8 The NORB dataset was downloaded in resized format from github.com/gwtaylor/convnet_matlab ≤ log p((x)) VAE 1-layer + NF <ref type="bibr" target="#b16">[17]</ref> -85.10 IWAE, 2-layer + IW=1 <ref type="bibr" target="#b1">[2]</ref> -85.33 IWAE, 2-layer + IW=50 <ref type="bibr" target="#b1">[2]</ref> -82.90 VAE, 2-layer + VGP <ref type="bibr" target="#b19">[20]</ref> -81.90 LVAE, 5-layer -82.12 LVAE, 5-layer + finetuning -81.84 LVAE, 5-layer + finetuning + IW=10 -81.74 improve performance up to three layers. The LVAE models performs better improving performance for each additional layer reaching L test 1 = -85.23 with five layers which is significantly higher than the best VAE score at -87.49 using three layers. As expected the improvement in performance is decreasing for each additional layer, but we emphasize that the improvements are consistent even for the addition of the top-most layers. In Figure <ref type="figure" target="#fig_1">3 c</ref>) the approximated true log-likelihood estimated using 5000 importance weighted samples is seen. Again the LVAE models performs better than the VAE reaching L test 5000 = -82.12 compared to the best VAE at -82.74. These results show that the LVAE achieves both a higher approximate log-likelihood score, but also a significantly tighter lower bound on the log-likelihood L test 1 . The models in Figure <ref type="figure" target="#fig_1">3</ref> were trained using fixed learning rate and one Monte Carlo (MC) and one importance weighted (IW) sample. To improve performance we fine-tuned the best performing five layer LVAE models by training these for a further 2000 epochs with annealed learning rate and increasing the number of IW samples and see a slight improvements in the test set log-likelihood values, Table <ref type="table" target="#tab_0">1</ref>. We saw no signs of over-fitting for any of our models even though the hierarchical latent representations are highly expressive as seen in Figure <ref type="figure">2</ref>.</p><p>Comparing the results obtained here with current state-of-the art results on permutation invariant MNIST, Table <ref type="table" target="#tab_0">1</ref>, we see that the LVAE performs better than the normalizing flow VAE and importance weighted VAE and comparable to the Variational Gaussian Process VAE. However we note that these results are not directly comparable to these due to differences in the training procedure.</p><p>To test the models on more challenging data we used the OMNIGLOT dataset, consisting of characters from 50 different alphabets with 20 samples of each character. The log-likelihood values, shows similar trends as for MNIST with the LVAE achieving the best performance using five layers of latent variables, see the appendix for further results. The best log-likelihood results obtained here, -102.11, is higher than the best results from <ref type="bibr" target="#b1">[2]</ref> at -103.38, which were obtained using more latent variables (100-50 vs 64-32-16-8-4) and further using 50 importance weighted samples for training.</p><p>We tested the models using a continuous Gaussian observation model on the NORB dataset consisting of gray-scale images of 5 different toy objects under different illuminations and observation angles. The LVAE achieves a slightly higher score than the VAE, however none of the models see an increase in performance for more using more than three stochastic layers. We found the Gaussian observation models to be harder to optimize compared to the Bernoulli models, a finding also recognized in <ref type="bibr" target="#b22">[23]</ref>, which might explain the lower utilization of the topmost latent layers in these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Latent representations</head><p>The probabilistic generative models studied here automatically tune the model complexity to the data by reducing the effective dimension of the latent representation due to the regularization effect of the priors in Eq. ( <ref type="formula">4</ref>). However, as previously identified <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b1">2]</ref>, the latent representation is often overly sparse with few stochastic latent variables propagating useful information.</p><p>To study the importance of individual units, we split the variational training criterion L into a sum of terms corresponding to each unit k in each layer i. For stochastic latent units, this is the KLdivergence between q(z i,k |•) and p(z i |z i+1 ). Figure <ref type="figure" target="#fig_2">4</ref> shows the evolution of these terms during training. This term is zero if the inference model is collapsed onto the prior carrying no information about the data, making the unit inactive. For the models without warm-up we find that the KLdivergence for each unit is stable during all training epochs with only very few new units activated during training. For the models trained with warm-up we initially see many active units which are then gradually pruned away as the variational regularization term is introduced. At the end of training warm-up results in more active units indicating a more distributed representation and the LVAE model produces both the deepest and most distributed latent representation.</p><p>We also study the importance of layers by splitting the training criterion layer-wise as seen in Figure <ref type="figure" target="#fig_3">5</ref>. This measures how much of the representation work (or innovation) is done on each layer. The VAEs use the lower layers the most whereas the highest layers are not (or only to a limited degree) used. Contrary to this, the LVAE puts much more importance to the higher layers which shows that it learns both a deeper and qualitatively different hierarchical latent representation which might explain the better performance of the model.</p><p>To qualitatively study the learned representations, PCA plots of z i ∼ q(z i |•) are seen in Figure <ref type="figure">6</ref>. For vanilla VAE, the latent representations above the second layer are completely collapsed on a standard normal prior. Including Batch normalization and warm-up activates one additional layer each in the VAE. The LVAE utilizes all five latent layers and the latent representation shows progressively more </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Discussion</head><p>We presented a new inference model for VAEs combining a bottom-up data-dependent approximate likelihood term with a prior information from the generative distribution. We showed that this parameterization 1) increases the approximated log-likelihood compared to VAEs, 2) provides a tighter bound on the log-likelihood and 3) learns a deeper and qualitatively different latent representation of the data. Secondly we showed that deterministic warm-up and batch-normalization are important for optimizing deep VAEs and LVAEs. Especially the large benefits in generative performance and depth of learned hierarchical representations using batch normalization were surprising given the additional noise introduced. This is something that is not fully understood and deserves further investigation and although batch normalization is not novel we believe that this finding in the context of VAEs are important.</p><p>The inference in LVAE is computed recursively by correcting the generative distribution with a data-dependent approximate likelihood contribution. Compared to purely bottom-up inference, this parameterization makes the optimization easier since the inference is simply correcting the generative distribution instead of fitting the two models separately. We believe this explicit parameter sharing between the inference and generative distribution can generally be beneficial in other types of recursive variational distributions such as DRAW <ref type="bibr" target="#b4">[5]</ref> where the ideas presented here are directly applicable. Further the LVAE is orthogonal to other methods for improving the inference distribution such as Normalizing flows <ref type="bibr" target="#b16">[17]</ref>, Variational Gaussian Process <ref type="bibr" target="#b19">[20]</ref> or Auxiliary Deep generative models <ref type="bibr" target="#b11">[12]</ref> and combining with these might provide further improvements.</p><p>Other directions for future work include extending these models to semi-supervised learning which will likely benefit form the learned deep structured hierarchies of latent variables and studying more elaborate inference schemes such as a k-step iterative inference in the LVAE <ref type="bibr" target="#b13">[14]</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Inference (or encoder/recognition) and generative (or decoder) models for a) VAE and b) LVAE. Circles are stochastic variables and diamonds are deterministic variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: MNIST log-likelihood values for VAEs and the LVAE model with different number of latent layers, Batch normalization (BN) and Warm-up (WU). a) Train log-likelihood, b) test log-likelihood and c) test log-likelihood with 5000 importance samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: log KL(q|p) for each latent unit is shown at different training epochs. Low KL (white) corresponds to an inactive unit. The units are sorted for visualization. It is clear that vanilla VAE cannot train the higher latent layers, while introducing batch normalization helps. Warm-up creates more active units early in training, some of which are then gradually pruned away during training, resulting in a more distributed final representation. Lastly, we see that the LVAE activates the highest number of units in each layer.</figDesc><graphic coords="7,107.91,86.25,396.20,150.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Layer-wise KL[q|p] divergence going from the lowest to the highest layers. In the VAE models the KL divergence is highest in the lowest layers whereas it is more distributed in the LVAE modelFigure 6: PCA-plots of samples from q(z i |z i-1 ) for 5-layer VAE and LVAE models trained on MNIST. Color-coded according to true class label</figDesc><graphic coords="9,310.98,71.85,198.10,184.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: OMNIGLOT samples. a) True data, b) Conditional Reconstructions and c) Samples from the prior distribution</figDesc><graphic coords="13,361.94,321.02,113.60,113.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note><p>Test set MNIST performance for importance weighted autoencoder (IWAE), VAE with normalizing flows (NF) and VAE with variational gaussian process(VGP). Number of importance weighted (IW) samples used for training is one unless otherwise stated.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 ,</head><label>2</label><figDesc></figDesc><table><row><cell>OMNIGLOT 64 64-32 64-32-16 64-32-16-8 64-32-16-8-4</cell><cell cols="4">VAE -111.21 -105.62 -104.51 VAE +BN VAE +BN +WU -110.58 -105.51 -102.61 -102.63 LVAE +BN +WU --111.26 -106.09 -102.52 -102.18 -111.58 -105.66 -102.66 -102.21 -110.46 -105.45 -102.48 -102.11</cell></row><row><cell>NORB 64 64-32 64-32-16 64-32-16-8 64-32-16-8-4</cell><cell>2741 2792 2786 2689 2654</cell><cell>3198 3224 3235 3201 3198</cell><cell>3338 3483 3492 3482 3422</cell><cell>-3272 3519 3449 3455</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test set log-likelihood scores for models trained on the OMNIGLOT and NORB datasets. The left most column show dataset and the number of latent variables i each model.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>The idea was dismissed at the time, since it could introduce substantial theoretical complications.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by the <rs type="funder">Novo Nordisk Foundation</rs>, <rs type="funder">Danish Innovation Foundation</rs> and the <rs type="institution">NVIDIA Corporation</rs> with the donation of TITAN X and Tesla K40 GPUs.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Results</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06349</idno>
		<title level="m">Generating sentences from a continuous space</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00519</idno>
		<title level="m">Importance weighted autoencoders</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Helmholtz machine</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="889" to="904" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Lasagne</surname></persName>
		</author>
		<title level="m">First release</title>
		<imprint>
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational Bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">One-shot learning by inverting a compositional causal process</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Auxiliary deep generative models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Local minima, symmetry-breaking, and model pruning in variational free energy minimization. Inference Group</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Cavendish Laboratory, Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Iterative neural autoregressive distribution estimator NADE-k</title>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Building blocks for variational Bayesian learning of latent variable models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05770</idno>
		<title level="m">Variational inference with normalizing flows</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName><forename type="first">Theano</forename><surname>Development</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Team</forename></persName>
		</author>
		<idno>abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06499</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Variational Gaussian process. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From neural PCA to deep unsupervised learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7783</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Independent Component Analysis and Learning Machines</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L E</forename><surname>Bingham</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Lampinen</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="143" to="171" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">What auto-encoders could learn from brains -generation as feedback in unsupervised deep learning and inference</title>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Broeke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>Finland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Aalto University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">MSc thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<title level="m">Pixel recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
