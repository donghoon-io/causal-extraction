<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DIFFERENTIABLE CAUSAL DISCOVERY FOR LATENT HIERARCHICAL CAUSAL MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11-29">29 Nov 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Parjanya</forename><surname>Prajakta Prashant</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California San Diego</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of Artificial Intelligence</orgName>
								<address>
									<postCode>1988</postCode>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California San Diego</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DIFFERENTIABLE CAUSAL DISCOVERY FOR LATENT HIERARCHICAL CAUSAL MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11-29">29 Nov 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2411.19556v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Discovering causal structures with latent variables from observational data is a fundamental challenge in causal discovery. Existing methods often rely on constraint-based, iterative discrete searches, limiting their scalability to large numbers of variables. Moreover, these methods frequently assume linearity or invertibility, restricting their applicability to real-world scenarios. We present new theoretical results on the identifiability of nonlinear latent hierarchical causal models, relaxing previous assumptions in literature about the deterministic nature of latent variables and exogenous noise. Building on these insights, we develop a novel differentiable causal discovery algorithm that efficiently estimates the structure of such models. To the best of our knowledge, this is the first work to propose a differentiable causal discovery method for nonlinear latent hierarchical models. Our approach outperforms existing methods in both accuracy and scalability. We demonstrate its practical utility by learning interpretable hierarchical latent structures from high-dimensional image data and demonstrate its effectiveness on downstream tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Causal discovery, the task of inferring causal relationships from observational data, is fundamental to understanding complex systems across various scientific disciplines. Traditional causal discovery methods often assume the absence of latent confounders, a property known as causal sufficiency <ref type="bibr" target="#b51">(Spirtes et al., 2001;</ref><ref type="bibr" target="#b11">Chickering, 2002)</ref>. However, this condition is frequently violated in realworld scenarios, where unobserved variables can introduce spurious correlations among observed variables. For instance, in image analysis, latent semantic variables often act as common causes for multiple pixels, creating complex dependencies that are not directly observable.</p><p>Recognizing the limitations of causal sufficiency conditions, researchers have developed various approaches to handle latent confounders. Fast Causal Inference (FCI) and its extensions <ref type="bibr" target="#b51">(Spirtes et al., 2001;</ref><ref type="bibr" target="#b43">Pearl et al., 2000;</ref><ref type="bibr" target="#b61">Zhang, 2008;</ref><ref type="bibr" target="#b15">Colombo et al., 2012;</ref><ref type="bibr" target="#b14">Claassen et al., 2013;</ref><ref type="bibr" target="#b2">Akbari et al., 2021)</ref> leverage conditional independence information to infer a class of possible causal graphs. While these methods can identify the presence of latent confounders, they do not provide information about causal relationships among the latent variables themselves. Therefore, another line of research has emerged, focusing on methods that can identify causal relations between latent variables. These approaches typically introduce additional parametric conditions, such as linearity or discrete data, to make the problem tractable. Notable examples include methods based on rank constraints <ref type="bibr" target="#b49">(Silva et al., 2006;</ref><ref type="bibr" target="#b32">Kummerfeld &amp; Ramsey, 2016;</ref><ref type="bibr" target="#b25">Huang et al., 2022;</ref><ref type="bibr" target="#b57">Xie et al., 2022;</ref><ref type="bibr" target="#b17">Dong et al., 2023)</ref>, higher-order moments <ref type="bibr" target="#b48">(Shimizu et al., 2009;</ref><ref type="bibr" target="#b56">Xie et al., 2020;</ref><ref type="bibr" target="#b0">Adams et al., 2021;</ref><ref type="bibr" target="#b10">Chen et al., 2022)</ref>, copula models <ref type="bibr" target="#b16">(Cui et al., 2018)</ref>, multiple domains based methods <ref type="bibr" target="#b60">(Zeng et al., 2021;</ref><ref type="bibr" target="#b34">Li et al., 2023;</ref><ref type="bibr" target="#b52">Sturma et al., 2024)</ref>, matrix-decomposition <ref type="bibr" target="#b3">(Anandkumar et al., 2013)</ref> and mixture models <ref type="bibr" target="#b29">(Kivva et al., 2021)</ref>. While these methods have shown promise in specific settings, they often rely on strong conditions about the underlying causal structure or data distribution. variables are discrete, which often does not hold in many real-world scenarios, such as images. <ref type="bibr" target="#b31">Kong et al. (2024)</ref> allows continuous variables to be adjacent to latent discrete variables. However, causal relationships and the hierarchical structure are only learned for discrete variables. Moreover, latent variables are assumed to be a deterministic invertible function of the measured variables, similar to <ref type="bibr" target="#b30">Kong et al. (2023)</ref>.</p><p>Differentiable Causal Discovery: NOTEARS introduced a continuous optimization-based algorithm to learn linear directed acyclic graphs (DAG) by reformulating graphical constraints into differentiable ones <ref type="bibr" target="#b63">Zheng et al. (2018)</ref>. Subsequent work parameterized DAGs using neural networks <ref type="bibr" target="#b59">Yu et al. (2019)</ref>; <ref type="bibr" target="#b62">Zhang et al. (2019)</ref>; <ref type="bibr" target="#b38">Ng et al. (2022)</ref>. <ref type="bibr" target="#b64">Zheng et al. (2020)</ref> extended these approaches to non-parametric DAGs. While these approaches scale well, they usually assume the absence of any latent variables in the DAG.</p><p>To address the existence of latent variables, recent differentiable causal discovery algorithms have been proposed <ref type="bibr" target="#b7">Bhattacharya et al. (2021)</ref>; <ref type="bibr" target="#b6">Bellot &amp; van der Schaar (2021)</ref>. However, these approaches only focus on the causal relationships among observed variables, failing to capture causal relationships involving latent variables, and often assume linearity in the causal structure. <ref type="bibr" target="#b36">Ma et al. (2024)</ref> builds on similar assumptions but employs a supervised learning framework for causal discovery. Despite these advances, there are ongoing concerns regarding the evaluation of differentiable causal discovery methods. Critics argue that some of these methods may not truly perform causal discovery, as highlighted by <ref type="bibr" target="#b44">Reisach et al. (2021)</ref>; <ref type="bibr" target="#b46">Seng et al. (2024)</ref>. To address these issues, <ref type="bibr" target="#b39">Ng et al. (2024)</ref> advocates for improved evaluation practices, emphasizing the need for rigorous and reliable assessment criteria.</p><p>Causal Representation Learning: Causal representation learning is closely related to latent casual discovery. Non-linear Independent Component Analysis methods aim to recover independent latent sources from a set of measured variables. However, they do not consider generic dependence between latent variables and rely on additional assumptions such as existence of auxiliary variables <ref type="bibr" target="#b26">(Hyvarinen et al., 2019)</ref> or sparsity <ref type="bibr" target="#b65">(Zheng et al., 2022)</ref>. Some methods attempt to model dependencies among latent variables explicitly. For example, <ref type="bibr" target="#b22">He et al. (2018)</ref> propose using a graphical structure to represent these dependencies, though their approach lacks identifiability guarantees. <ref type="bibr" target="#b58">Yang et al. (2021)</ref> assume linear relations and incorporate additional concept data to establish the identifiability. <ref type="bibr" target="#b8">Brehmer et al. (2022)</ref>; <ref type="bibr" target="#b53">Subramanian et al. (2022)</ref> utilize interventional data to learn these dependencies. We consider a latent hierarchical causal model represented by a directed acyclic graph (DAG) G = (V, E), where V = Z ∪ X comprises latent variables Z = {z 1 , z 2 , . . . , z nz } and measured variables X = {x 1 , x 2 , . . . , x nx }, and E denotes the set of edges representing causal relationships. The variables follow the data-generating procedure:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NON-LINEAR LATENT HIERARCHICAL CAUSAL MODELS</head><formula xml:id="formula_0">z j = f zj (Pa(z j ), ε zj ), x i = f xi (Pa(x i ), ε xi ) (1)</formula><p>where Pa(•) represents the set of parent variables of a given node in G, and Pa(x i ), Pa(z j ) ⊆ Z.</p><p>The structure of DAG G can be characterized by binary matrices M z and M x , where M z ij = 1 if and only if there is an edge z i → z j , and M x ij = 1 if and only if there is an edge z i → x j . Without loss of generality, we assume M z is upper triangular. The binary adjacency matrix M is obtained by horizontally concatenating M z and M</p><formula xml:id="formula_1">x , i.e., M = [M z | M x ].</formula><p>The goal of this work is to recover the binary matrix M which characterizes the structure of DAG G. Since the labeling of latent variables in general cannot be identified, we aim to recover M upto the relabeling of the latent variables.</p><p>In general, latent hierarchical models are not identifiable. For example, consider the model shown in Figure <ref type="figure">7a</ref>. It is in general not possible to disentangle the effects of z 1 , z 2 , z 3 and identify the structure or their values. Hence, we require structural conditions on the model to make it identifiable. Prior work has addressed this challenge through various constraints. <ref type="bibr" target="#b49">Silva et al. (2006)</ref>; <ref type="bibr" target="#b32">Kummerfeld &amp; Ramsey (2016)</ref>; <ref type="bibr" target="#b25">Huang et al. (2022)</ref>; <ref type="bibr" target="#b30">Kong et al. (2023)</ref> proposed requirements on the minimum number of measured pure children, allowing latent variables to leave sufficient footprints in the measured variables. Additionally, researchers have often assumed tree-like structures to prove identifiability of hierarchical models <ref type="bibr" target="#b13">(Choi et al., 2011;</ref><ref type="bibr" target="#b19">Drton et al., 2017;</ref><ref type="bibr" target="#b25">Huang et al., 2022;</ref><ref type="bibr" target="#b30">Kong et al., 2023)</ref>.</p><p>In order for our model to be identifiable, we consider the following structural conditions: Definition 1 (Pure Children). v i is a pure child of another variable v j , if v j is the only parent of v i in the graph, i.e., Pa(v i ) = {v j }. Condition 1. (i) Each latent variable has two at least pure children. (ii) For any latent variable z i ∈ Z, let D i = De(z i ) ∩ X be the set of measured descendants of z i where De(.) denotes the descendants. Then, for all x j , x k ∈ D i , d(z i , x j ) = d(z i , x k ) where d(•, •) denotes the length of the directed path between two nodes in the graph G.</p><p>We provide additional discussion on the above condition in Appendix C. Define Z l = {z i ∈ Z : ∀x j ∈ De(z i ) ∩ X, d(z i , x j ) = l}. This denotes the set of latent variables in the l th layer of the model. Henceforth, we denote the vector obtained by concatenating the elements in Z l as z l and z l i as the i th element of layer l. Note that since any node in Z i has parents only in Z i-1 , the adjacency matrix M can be transformed via suitable column and row permutations to the block upper-triangular structure as shown below:</p><formula xml:id="formula_2">M =      0 M k 0 • • • 0 0 0 M k-1 • • • 0 . . . . . . . . . . . . . . . 0 0 0 • • • M 1     <label>(2)</label></formula><p>where M i ∈ {0, 1} |Zi|×|Zi-1| are binary matrices that model the causal structure between Z i and Z i-1 . M 1 ∈ {0, 1} |Z1|×|X| is the binary matrix between Z 1 and X. Henceforth in the paper, we assume M is modeled this way and hence always satisfies condition 1 (ii).</p><p>Our structural conditions are fairly general. We reduce the required number of measured variables relative to <ref type="bibr" target="#b49">Silva et al. (2006)</ref> and <ref type="bibr" target="#b32">Kummerfeld &amp; Ramsey (2016)</ref>. Unlike Choi et al. ( <ref type="formula">2011</ref>) and <ref type="bibr" target="#b19">Drton et al. (2017)</ref>, we do not restrict children to have only one latent parent. Furthermore, our method imposes no constraints on the neighborhood structure of variables as in <ref type="bibr" target="#b25">Huang et al. (2022)</ref>; <ref type="bibr" target="#b57">Xie et al. (2022)</ref>.</p><p>Additional Notation: For any matrix A, we use A i,: to denote its i-th row, A :,j for its j-th column, and A i,j for the element at the i-th row and j-th column. For a set A, a denotes the vector obtained by concatenating all the elements in A and a i denotes the i th element of a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IDENTIFIABILITY THEORY</head><p>In this section, we describe the identifiability theory for general latent hierarchical models. Prior work has used rank constraints on the observed distribution as a graphical indicator of latent variables <ref type="bibr" target="#b49">(Silva et al., 2006;</ref><ref type="bibr" target="#b25">Huang et al., 2022;</ref><ref type="bibr" target="#b17">Dong et al., 2023)</ref>. They use the rank of cross-covariance matrix between two sets of measured variables to determine the number of latent variables that dseparate the two measured sets. However, this criterion only works for linear cases hence limiting the identifiability results. We propose a novel indicator which allows us to determine the number of latent variables which d-separate the two measured sets in the general case.</p><p>Intuition: Consider the case where a set of latent variables, denoted by Z, d-separates two sets of measured variables, X and Y. In this scenario, the conditional distribution p(y|x) can be expressed as: p(y|x) = p(y|z)p(z|x)dz. If the cardinality of Z is smaller than that of X or Y, this imposes a constraint on the observable distribution. In most cases, the size of Z would to the minimum dimension of latent variables such that p(y|x) can be written as p(y|z)p(z|x)dz. Based on this observation, we formulate a criterion based on the rank of the Jacobian of the function E[y|x], which allows us to show that the general case holds with probability one.</p><p>In order to rigorously prove identifiability results, we introduce some standard differentiability and faithfulness conditions <ref type="bibr" target="#b25">(Huang et al., 2022;</ref><ref type="bibr" target="#b17">Dong et al., 2023)</ref>.</p><p>Condition 2 (Generalized Faithfulness). A probability distribution P is faithful to a DAG G if every rank Jacobian constraint on a pair of set of measured variables that holds in P is entailed by every structural equation model with respect to G.</p><p>Faithfulness conditions are widely used in causal discovery <ref type="bibr" target="#b51">(Spirtes et al., 2001;</ref><ref type="bibr" target="#b61">Zhang, 2008;</ref><ref type="bibr" target="#b49">Silva et al., 2006;</ref><ref type="bibr" target="#b25">Huang et al., 2022)</ref>. This is often justified by the fact that the Lebesgue measure of distributions violating faithfulness has been shown to be zero. The following proposition justifies the faithfulness condition for non-linear latent hierarchical models along similar lines. Proposition 1. The probability of a distribution P generated by a structural model with respect to G violating Generalized Faithfulness is zero. Condition 3 (Differentiability). (i) For every pair of measured sets X and Y, the function f :</p><formula xml:id="formula_3">R |X| → R |Y| defined as f (x) = E[y|x] is continuously differentiable. (ii)</formula><p>For every pair of measured set X and latent set Z, there exists a continuous differentiable function g : R |X| → R |Z| such that p(z|x) = p(z|g(x)).</p><p>Our approach considerably relaxes the constraints compared to existing work. Unlike previous methods that require linear relationships <ref type="bibr" target="#b25">(Huang et al., 2022;</ref><ref type="bibr" target="#b17">Dong et al., 2023)</ref> or deterministic functions (z, ϵ = f (x)) <ref type="bibr" target="#b30">(Kong et al., 2023)</ref>, our framework accommodates a broader class of non-linear relationships between variables. Also, note that these conditions are sufficient but not necessary. In Section 6 we show we can identify structures even when Condition 3 does not hold.</p><p>We introduce a theorem that relates the graphical structure to a constraint of the distribution between two sets of measured variables. Theorem 1. Let G be a hierarchical latent causal model satisfying Condition 1. For any two sets of measured variables X and Y in G, let f (x) = E[y|x]. Under the faithfulness and differentiability conditions, for any r &lt; |X|, |Y|, the rank of the Jacobian matrix J f = ∂f ∂x = r if and only if the size of the smallest set of latent variables that d-separates X from Y is r. Formally,</p><formula xml:id="formula_4">rank(J f ) = min Z |Z| such that X ⊥ ⊥ G Y|Z (3)</formula><p>where Z is a subset of latent variables in G, and ⊥ ⊥ G denotes d-separation in the graph G.</p><p>Henceforth, we use r(X, Y) to denote the rank of the Jacobian of the function E[y|x]. Moreover, it can be shown that pure descendants of latent variables can be used as a surrogate to calculate d-separation between sets of latent variables as stated in Theorem 2 below. This theorem is partly inspired by <ref type="bibr" target="#b25">Huang et al. (2022)</ref>.</p><p>Theorem 2. Let G be a hierarchical latent causal model satisfying Condition 1. Let Z X , Z Y ⊆ Z be two disjoint subsets of latent variables in G, i.e., Z X ∩ Z Y = ∅. Let X be the set of measured variables that are d-separated by Z X from all other measured variables in G and let Y be the set of measured variables that are d-separated by Z Y from all other measured variables in G. Then,</p><formula xml:id="formula_5">r(Z X , Z Y ) = r(X, Y)</formula><p>The two theorems presented above establish a crucial link between the measured distribution and the underlying graph structure. Building upon this foundation, we now introduce three lemmas that are instrumental in proving identifiability. These lemmas provide a systematic approach to uncover the latent structure: Lemma 1. Let G be a graph satisfying Conditions 1. A set of measured variables S are pure children of the same parent if and only if for any subset T ⊆ S, r(T, X \ T) = 1. Lemma 2. Let G be a hierarchical latent causal model satisfying Conditions 1. Let X ⊂ V be the set of measured variables. Under the generalized faithfulness condition, for any measured variable c ∈ X and any set of latent variables P ⊆ Z 1 , c is a child of exactly the variables in P if and only if the following conditions hold:</p><formula xml:id="formula_6">1. For each S ⊆ X such that |S ∩ Ch(z i )| = 1 for each z i ∈ P, where Ch(z i ) denotes the set of pure children of z i : r(S, X \ (S ∪ {c})) = r(S ∪ {c}, X \ (S ∪ {c}))</formula><p>2. The equality in condition (1) does not hold for any proper subset of P.</p><p>Lemma 3. Let G be a graph satisfying Conditions 1. A measured variable c has no parent if and only if r({c}, X \ {c}) = 0.</p><p>Discussion: Lemma 1 enables us to identify pure children among the measured variables X. For example, in Figure <ref type="figure" target="#fig_0">1</ref> all subsets of {x 1 , x 2 } are d-separated from the rest of the variables by one variable {z 4 }. However, for the set {x 1 , x 2 , x 3 , x 4 }, the subset {x 1 , x 3 } requires two variables {z 4 , z 5 } to be d-separated from the rest of the variables. Lemma 2 provides a method to determine the parents of non-pure children. For example, in Figure <ref type="figure" target="#fig_0">1</ref>, consider {x 12 } whose parents are {z 8 , z 9 }. For a set like {x 9 , x 12 }, which contains exactly one pure child of both parents of {x 12 }, r({x 9 , x 12 }, X \ {x 9 , x 12 }) = r({x 9 , x 12 , x 11 }, X \ {x 9 , x 12 , x 11 }). However, this is not true for any other set of latent variables. Lemma 3 allows us to identify measured variables that have no latent parents.</p><p>Having established the necessary lemmas, we now present the identifiability of the graph structure in hierarchical latent causal models. Theorem 3. Let G = (V, E) be a hierarchical latent causal model satisfying Condition 1. Let M be the binary adjacency matrix representing the structure of G. Let data X be generated according to the structural equation model defined in Equation <ref type="formula">1</ref>. Given a function r(S, T) which outputs the minimum number of latent variables that d-separate any two measured sets S and T, M is identifiable up to the permutation of the latent variables.</p><p>The proof for Theorem 3 leverages the preceding lemmas and recursion. We begin by applying Lemmas 1, 2, and 3 to infer the structure between Z 1 and X. Theorem 2 then allows us to relate d-separation between sets in Z 1 to their pure children in X. Thus, this process can be applied recursively to higher levels of the hierarchy, enabling the identification of the entire graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DIFFERENTIABLE CAUSAL DISCOVERY APPROACH</head><p>In the previous section, we demonstrated that hierarchical models satisfying Condition 1 yield a unique hierarchical structure for a given distribution of measured variables. To learn the causal structure, two key steps must be performed: (i) matching the observed data distribution, and (ii) enforcing structural constraints on the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MATCHING THE DATA DISTRIBUTION</head><p>To learn the causal structure, we consider the structural equation models (SEMs) in Equation <ref type="formula">1</ref>explicitly parameterized by the binary adjacency matrix M .</p><formula xml:id="formula_7">z i j = f i j (M i+1 ⊙ z i+1 , ε z i j ), x j = g j (M 1 ⊙ z 1 , ε xj ). (4)</formula><p>We employ a variational autoencoder (VAE) (Kingma, 2013) as a generative model to learn the distribution over the measured variables. Let θ be the parameters of the VAE, and M represent the binary adjacency matrix controlling the structure of the SEM. We aim to maximize the evidence lower bound (ELBO) to approximate the true data distribution.</p><formula xml:id="formula_8">log p(x; θ, M ) = log p(x|ϵ; θ, M )p(ϵ; θ)dϵ = log q(ϵ|x) q(ϵ|x) p(x|ϵ; θ, M )p(ϵ; θ)dϵ ≥ -KL(q(ϵ|x)||p(ϵ; θ)) + E q [log p(x|ϵ; θ, M )]</formula><p>(5) Here, ϵ represents the latent variable vector obtained by concatenating all individual noise terms ε z i j and ε xj . The objective of the VAE is to minimize the negative ELBO L ELBO , where the KL divergence regularizes the variational posterior, and the second term encourages the generative model to match the observed data distribution.</p><p>The encoder of the VAE models the approximate posterior q(ϵ|x), mapping the observed data x to the latent space ϵ. An advantage of modeling q(ϵ|x) over q(z|x) is that it simplifies the process of enforcing the independence of each dimension of ϵ. The decoder models the conditional likelihood p(x|ϵ; θ, M ), and is designed to follow the SEM equations in Equation <ref type="formula">4</ref>. This allows the decoder to respect the structural constraints encoded in the binary adjacency matrix M , ensuring that the learned distribution also reflects the underlying causal structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ENFORCING STRUCTURAL CONSTRAINTS</head><p>In order to enforce structural constraints, we relax the binary adjacency matrix M for gradientbased optimization by using the Gumbel-softmax trick <ref type="bibr" target="#b27">(Jang et al., 2016)</ref>. Here, M ∼ σ(γ), where σ represents the softmax function and γ is a trainable parameter representing the logits.</p><p>To ensure the causal structure satisfies Condition 1 (ii), we define M as a block upper-triangular matrix as shown in Equation <ref type="formula" target="#formula_2">2</ref>. We now introduce the following lemma to justify the constraint required for Condition 1 (i). Lemma 4. Consider a DAG G with a binary adjacency matrix M . G satisfies Condition 1 (i) if and only if:</p><formula xml:id="formula_9">M i,: ⊙ j̸ =i (1 -M j,: ) 1 ≥ 2 ∀i.<label>(6)</label></formula><p>This lemma ensures that each row in M with descendants must account for at least two pure children, where pure children are those that do not share another parent. The elementwise product with j̸ =i (1 -M j,: ) counts pure children, and the ℓ 1 norm helps ensure that each row satisfies the minimum number of pure children.</p><p>To encourage sparsity and avoid learning spurious edges, we apply an ℓ 1 regularization on σ(γ), similar to other differentiable causal discovery methods <ref type="bibr" target="#b38">(Ng et al., 2022;</ref><ref type="bibr" target="#b9">Brouillard et al., 2020)</ref>.</p><p>The optimization objective is formulated as:</p><formula xml:id="formula_10">max θ,γ E M ∼σ(γ) [ELBO(θ, M )] -λ∥σ(γ)∥ 1 ,<label>(7)</label></formula><p>subject to ∥M i,:</p><formula xml:id="formula_11">∥ 1 M i,: ⊙ j̸ =i (1 -M j,: ) 1 -2 ≥ 0 ∀i.<label>(8)</label></formula><p>Note, that we allow some rows of M to go zero. This allows us to learn the number of latent variables. The above method of using Gumbel softmax to approximate the binary adjacency matrices is inspired by <ref type="bibr" target="#b38">Ng et al. (2022)</ref>; <ref type="bibr" target="#b9">Brouillard et al. (2020)</ref>.</p><p>Furthermore, to ensure the independence of the noise terms ε, we introduce the following independence loss, denoted as L ind (ϵ), which minimizes the KL divergence between the joint distribution of ϵ and the product of individual noise distributions:</p><formula xml:id="formula_12">L ind (ϵ) = KL p(ϵ)∥ j i p(ε z i j ) j p(ε xj ) .<label>(9)</label></formula><p>This can be estimated using the Donsker-Varadhan representation <ref type="bibr" target="#b18">(Donsker &amp; Varadhan, 1983;</ref><ref type="bibr" target="#b5">Belghazi et al., 2018)</ref>.</p><p>Therefore, the final loss function is:</p><formula xml:id="formula_13">L final = -E M ∼σ(γ) [ELBO(θ, M )] + KL(q(ϵ|x)∥p(ϵ)) + λ 1 L ind (ϵ) + λ 2 ∥σ(γ)∥ 1 + λ 3 i max(0, ∥M i,: ∥ 1 (2 -∥M i,: ⊙ j̸ =i (1 -M j,: )∥ 1 )) 2 . (<label>10</label></formula><formula xml:id="formula_14">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>We conduct empirical studies to examine the efficacy of our differentiable causal discovery method. Specifically, we experiment with synthetic data in Section 6.1 and real image data in Section 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">SYNTHETIC DATA</head><p>We conduct experiments on four causal structures given in Figure <ref type="figure" target="#fig_7">4</ref> to validate our method. We consider both trees and v-structures. We compare against other methods designed to discover latent hierarchical causal models, namely KONG <ref type="bibr" target="#b30">(Kong et al., 2023)</ref>, HUANG <ref type="bibr" target="#b25">(Huang et al., 2022)</ref>, GIN <ref type="bibr" target="#b56">(Xie et al., 2020)</ref> and DeCAMFounder <ref type="bibr" target="#b1">(Agrawal et al., 2023)</ref>. The structural Hamming distance (SHD) and F1 score are computed for each structure and reported in Table <ref type="table" target="#tab_0">1</ref>. We also report the time taken for each method in seconds. We did not run 1-factor model methods like FOFC <ref type="bibr" target="#b32">(Kummerfeld &amp; Ramsey, 2016)</ref> since our data does not meet their conditions, and their implementation results in runtime errors.</p><p>For the ground truth graphs, the functions in Equation 1 are modeled using a linear transformation We observe substantial improvement in both the SHD and F1 score compared to the baselines. We note that this improvement is despite the fact that the data does not satisfy Condition 3 since LeakyReLU is not differentiable everywhere. Since other methods are designed for a restrictive class of latent hierarchical models, they are unable to identify the causal graph. <ref type="bibr" target="#b56">Xie et al. (2020)</ref> does not predict edges for most of the runs resulting in a mean F1 score close to zero. <ref type="bibr" target="#b1">Agrawal et al. (2023)</ref> only discovers edges between observed variables, hence has a high SHD and low F1 score.</p><p>The linear baselines <ref type="bibr" target="#b25">(Huang et al., 2022;</ref><ref type="bibr" target="#b56">Xie et al., 2020)</ref> are faster than the non-linear methods. This is because they do not have to train a non-linear model like a neural network since all relationships are linear. However, we can see that we require a much shorter runtime compared to <ref type="bibr" target="#b30">Kong et al. (2023)</ref> since we only train one neural network instead of O(ln 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">IMAGE DATA</head><p>In this section, we learn a latent causal graph for the MNIST dataset <ref type="bibr" target="#b33">(LeCun et al., 2010)</ref>. As shown in Figure <ref type="figure" target="#fig_5">3a</ref>, we model a latent hierarchical causal structure followed by a decoder that generates the images. Since our causal discovery approach can be trained end-to-end, incorporating the decoder does not alter our methodology. The convolution decoder allows us to use spatial information and reduce the number of measured variables. We initialize the causal model with three layers and learn the underlying structure. Further training details are provided in Appendix B.2.</p><p>For real image data, where the ground truth causal graph is unavailable, we validate our methodology using indirect evaluation techniques. These include visualizing the learned latent features, conducting interventions to interpret the learned representations, and evaluating their transferability across domains and under distribution shifts.    Note: SHD = Structural Hamming Distance (lower is better ↓). F1 scores range from 0 to 1 (higher is better ↑). Standard deviations are reported in parentheses.</p><p>The top layer typically captures global features, such as digit identity, while the middle layer learns variations within the same digit. The lowest layer focuses on local features that have minimal impact on the overall digit structure. Figure <ref type="figure" target="#fig_5">3b</ref> visualizes a subset of the learned causal graph, highlighting these patterns, which were generated by intervening on different nodes in the subgraph. The top node denotes the concept of a digit three. The lower nodes are visualized upon intervention. The full causal graph, shown in Appendix B.2, has 62 latent variables demonstrating the scalability of our method. Appendix B.2 also discusses visualization and intervention details.</p><p>Causal representations can contribute to better generalization and transfer learning due to the transfer of causal relations <ref type="bibr" target="#b45">(Schölkopf et al., 2021)</ref>. To demonstrate the effectiveness of our learned representations in domain transfer, we evaluate them on the CMNIST dataset <ref type="bibr" target="#b4">(Arjovsky et al., 2019)</ref> and CelebA dataset <ref type="bibr" target="#b35">(Liu et al., 2015)</ref>. We describe the problem setting and results for CMNIST here, while CelebA is discussed in Appendix B.2. In this dataset, the training set consists of digits 0 and 1, colored either red or green. The color acts as a cause for the digit, with P (digit = 0|color = red) = 0.9 and P (digit = 0|color = green) = 0.1 in the training set. These probabilities are reversed in the test set. We further evaluate on an additional test set where all digits are colored blue. Samples from these sets are shown in Figure <ref type="figure" target="#fig_5">3c</ref>. Although the correlation between color and digit varies across datasets, the causal relationship between the digit's image features and its label remains unchanged.</p><p>To predict the digit labels, we first learn a latent causal structure from the dataset. We then train a logistic regression classifier on these latent representations, applying L1 regularization to encourage sparsity in the model weights. This regularization promotes the use of features within the Markov blanket of the label. We evaluate the model on the test set, and the results are presented in Table <ref type="table" target="#tab_1">2</ref>.</p><p>We compare our method with standard Autoencoders, Variational Autoencoders (VAEs) (Kingma, 2013), β-VAEs <ref type="bibr">(Higgins et al., 2017a)</ref>, Causal VAEs <ref type="bibr" target="#b58">(Yang et al., 2021)</ref>, and Graph VAEs <ref type="bibr" target="#b22">(He et al., 2018)</ref>. For each baseline, a logistic regression classifier is trained on the learned latent represen-  tations. All methods are evaluated over three random trials, with the mean and standard deviation reported to assess performance consistency.</p><p>Table <ref type="table" target="#tab_1">2</ref> compares performance on the two test sets: 'Reverse,' where the color-digit relationship is reversed in the test set, and 'Blue,' where all digits in the test set are blue. We observe that our method achieves higher accuracy than the baselines for both the datasets. Notably, β-VAE for β = 1e-1, β = 10, and VAE have the same accuracy for all runs because they predict the same label for any input. While our representations demonstrate better transferability under distribution shifts compared to the evaluated baselines, we acknowledge that task-specific methods not focused on identifiable representation learning might have the potential to achieve higher accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this work, we introduce a differentiable causal discovery method for recovering the structure of latent hierarchical causal graphs under rather mild conditions. Our approach significantly outperforms existing baselines and is scalable to high-dimensional datasets such as images. Additionally, we establish novel identifiability results without imposing restrictive assumptions on the structural equation models. Notably, our result that provides graphical information based on rank of the Jacobian matrix may inspire future work in this area.</p><p>Despite relaxing several key assumptions, certain conditions, such as the two pure children requirement, remain necessary, as seen in other latent hierarchical methods <ref type="bibr" target="#b25">(Huang et al., 2022;</ref><ref type="bibr" target="#b30">Kong et al., 2023)</ref>. Furthermore, our method does not yet account for structures where measured variables have children. Future work could extend our identifiability results to more general structures, similar to <ref type="bibr" target="#b17">Dong et al. (2023)</ref>. We also speculate that Condition 3 may be not be necessary, and future research could focus on relaxing this condition further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOFS</head><p>A.1 PROOF OF PROPOSITION 1</p><p>Proposition 1. The probability of a distribution P generated by a structural model with respect to G violating Generalized Faithfulness is zero.</p><p>Proof. We begin our proof with the following lemma.</p><p>Lemma 5. Let A ∈ R m×p and B ∈ R p×n be random matrices drawn from a continuous distribution with m, n ≥ p, whose entries are drawn from continuous distributions. Let C = AB be their product. Then, P(rank(C) &lt; p) = 0 with respect to the Lebesgue measure on R m×n .</p><p>Proof. The lebesgue measure of non-full rank matrices is zero. Therefore P(rank(A) = p) = 1 and P(rank(B) = p) = 1.</p><p>Since C = AB, rank(C) ≤ min(rank(A), rank(B)) = p. For rank(C) &lt; p, the vectors Ab :,i would have to be linearly dependent. This adds hard constraints on the matrices which has lebesgue measure zero.</p><p>Using the proof of Theorem 1, we know</p><formula xml:id="formula_15">J h (x) = J f (g(x)) • J g (x) where J f (g(x)) ∈ R |Y|×|Z| , J g ∈ R |Z|×|X| .</formula><p>Condition 3 ensures the Jacobian matrices are continuous. By Lemma 5, rank(J h (x)) = |Z| with probability one. Therefore, the probability of rank(J h (x)) &lt; |Z| for all x is zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 PROOF OF THEOREM 1</head><p>Theorem 1. Let G be a hierarchical latent causal model satisfying Condition 1. Under the faithfulness and differentiability conditions, for any two sets of measured variables X and Y in G, the rank of the Jacobian matrix J f = ∂f ∂x = r &lt; |X|, |Y| (where f (x) = E[y|x]) if and only if the size of the smallest set of latent variables that d-separates</p><formula xml:id="formula_16">X from Y is r. Formally, rank(J f ) = min Z |Z| such that X ⊥ ⊥ G Y | Z (<label>11</label></formula><formula xml:id="formula_17">)</formula><p>where Z is a subset of latent variables in G, and ⊥ ⊥ G denotes d-separation in the graph G.</p><p>Proof. Let X and Y be two sets of measured variables in G. By the structure of the hierarchical latent causal model and Condition 1, there are no direct edges between measured variables. Therefore, any d-separation between X and Y must be mediated through a set of latent variables.</p><p>Let Z be the minimal set of latent variables that d-separates X from Y, i.e.,</p><p>X ⊥ ⊥ G Y | Z. This implies that the conditional distribution satisfies p(y|x) = p(y|z)p(z|x)dz.</p><p>Taking expectations, we obtain Substituting this into the expectation, we have</p><formula xml:id="formula_18">E[y|x] = E[y|z] p(z|g(x))dz = h(g(x)),</formula><p>where we define the function h :</p><formula xml:id="formula_19">R |Z| → R |Y| by f (w) = E[y|z] p(z|w)dz.</formula><p>Applying the chain rule to the composition of functions, the Jacobian of E[y|x] with respect to x is</p><formula xml:id="formula_20">J f (x) = J h (g(x)) • J g (x),</formula><p>where J h (g(x)) is an |Y| × |Z| matrix and J g (x) is an |Z| × |X| matrix.</p><p>Using the rank inequality for matrix multiplication, we have rank(J f (x)) ≤ min (rank(J h (g(x))), rank(J g (x))) .</p><p>Therefore, rank(J f (x)) ≤ min(|Z|, |X|, |Y|)</p><p>Proposition 1 implies that the Jacobian achieves its maximal possible rank almost everywhere. Thus, using Condition 2, rank(</p><formula xml:id="formula_21">J f (x)) = min(|Z|, |X|, |Y|) Therefore, J f = ∂f ∂x = r &lt; |X|, |Y| if and only if |Z| = r.</formula><p>This completes the proof. Linear Case: We show that linear latent hierarchical models <ref type="bibr" target="#b25">(Huang et al. (2022)</ref>) are a special case of this theorem. If the causal relationsip between y and z is linear, we have:</p><formula xml:id="formula_22">E[y|x] = E[y|E[z|x]].</formula><p>Therefore, E[z|x]] being a continuous differentiable function of x suffices and we do not require Condition 3 (ii).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 PROOF OF THEOREM 2</head><p>Theorem 2. Let G be a hierarchical latent causal model satisfying Condition 1. Let Z X , Z Y ⊆ Z be two disjoint subsets of latent variables in G, i.e., Z X ∩ Z Y = ∅. Let X be the set of observed variables that are d-separated by Z X from all other observed variables in G and let Y be the set of observed variables that are d-separated by Z Y from all other observed variables in G. Then,</p><formula xml:id="formula_23">r(Z X , Z Y ) = r(X, Y)</formula><p>Proof. Since Z X and Z Y d-separate X and Y from all other variables, we know that X are the measured pure descendants of Z X , and Y are the measured pure descendants of Z Y . Therefore, if Z d-separates Z X and Z Y , if Z d-separates X and Y. Moreover, if Z d-separates X and Y then given our structure, it must d-separate Z X and Z Y . This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 PROOF OF LEMMA 1</head><p>Lemma 1. Let G be a graph satisfying Conditions 1. A set of measured variables S are pure children of the same parent if and only if for any subset T ⊆ S, r(T, X \ T) = 1.</p><p>Proof. (⇒) Suppose the measured variables in S are pure children of the same parent node p. For any subset T ⊆ S, T and X \ T are d-separated by node p. Therefore, we have r(T, X \ T) = 1.</p><p>Therefore, by the principle of mathematical induction, the theorem holds for hierarchical latent causal models with any number of layers.</p><p>A.8 PROOF OF LEMMA 4 Lemma 4. Consider a DAG G with a binary adjacency matrix M . G satisfies Condition 1 (i) if and only if:</p><formula xml:id="formula_24">∥M i,: ⊙ j̸ =i (1 -M j,: )∥ 1 ≥ 2 ∀i.<label>(16)</label></formula><p>Proof. To prove this lemma, we need to demonstrate that the given condition on the adjacency matrix M holds if and only if each latent variable has at least two pure children, as stated in Condition 1.</p><p>Let's first analyze the term inside the ℓ 1 norm:</p><formula xml:id="formula_25">M i,: ⊙ j̸ =i</formula><p>(1 -M j,: )</p><p>The kth element of this vector is given by:</p><formula xml:id="formula_26">M ik • j̸ =i (1 -M jk )</formula><p>This product equals 1 if and only if M ik = 1 and M jk = 0 for all j ̸ = i. In other words, this term is 1 if and only if the vertex v k is a child of v i and not a child of any other v j (j ̸ = i). Hence, this product identifies whether v k is a pure child of v i .</p><p>The ℓ 1 norm, ∥•∥ 1 , sums up all these terms, meaning it counts the number of pure children of v i . Now, according to Condition 1, each latent variable v i must have at least 2 pure children. Therefore, the condition:</p><formula xml:id="formula_27">∥M i,: ⊙ j̸ =i</formula><p>(1 -M j,: )∥ 1 ≥ 2 ∀i ensures that each v i has at least 2 pure children, satisfying Condition 1.</p><p>Conversely, if each latent variable v i has at least 2 pure children, the sum M i,: ⊙ j̸ =i (1 -M j,: )</p><p>1 must be at least 2 for each i, proving the equivalence.</p><p>Thus, the lemma is proven.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXPERIMENTAL DETAILS</head><p>In this section, we provide a detailed explanation of our experimental setup, model architecture, training procedure, hyperparameter settings, and evaluation metrics used in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 SYNTHETIC DATA</head><p>We follow the same procedure and hyperparameter values for all four graphs.</p><p>Model architecture: We use a VAE to learn our causal structure as described in Section 5. The model consists of several components. The VAE encoder is a two-hidden-layer fully connected neural network with 64 and 32 hidden neurons, followed by ReLU activations. The encoder outputs both the mean (µ) and the log variance (log σ 2 ) for the latent variables. For the decoder, each function in Equation <ref type="formula">4</ref>is modeled using a one-hidden-layer fully connected neural network with 32 hidden neurons. The masking matrices have shape |X|</p><formula xml:id="formula_28">2 i+1 × |X| 2 i since Condition 1 allows a maximum of |X| 2 i</formula><p>latent variables in Z i . We use ReLU activation for all neural networks.</p><p>Training Procedure: We use mean squared error as the reconstruction loss. We calculate the L ind (ϵ) = KL(p(ϵ)∥ j i p(ε z i j ) j p(ε xj )) using a method similar to MINE (Belghazi et al. ( <ref type="formula">2018</ref>)). We warm up the MINE model for 100 epochs before training. Our model is trained using the Adam optimizer with a learning rate of 1 × 10 -3 for 400 epochs. We use a batch size of 32. For Gumbel-Softmax, we set the temperature to 1.0 throughout training. The coefficient for L ind (ϵ) is set to 10. λ 2 is set to 1e-4 and λ 3 is set as 10 -3+ epoch 100 . Since the training objective is non-convex, we may get suboptimal solutions <ref type="bibr" target="#b39">Ng et al. (2024)</ref>. Therefore, we run the model with 10 random initializations and select the one with the lowest loss.</p><p>Baselines: For <ref type="bibr" target="#b30">Kong et al. (2023)</ref>, we use the code shared with us by the authors. For <ref type="bibr" target="#b25">Huang et al. (2022)</ref> and <ref type="bibr" target="#b56">Xie et al. (2020)</ref>, we use the publicly available implementation. Default hyperparameters were used for all methods. We attempted to use FOFC <ref type="bibr" target="#b32">Kummerfeld &amp; Ramsey (2016)</ref> as a baseline, however an error occurred for our data since it does not satisfy the conditions for their code to run.</p><p>Evaluation Metrics: We evaluate our model using the Structural Hamming Distance (SHD) and F1 score between the learned adjacency matrix and the ground truth. We train each run three times for different seed values and report the mean and standard deviation across the three runs of two graphs. Since the latent graph may only be recovered up to a permutation of the latent variables, we calculate the SHD over all possible |Z|! permutations of the estimated graph and select the lowest SHD. Since the evaluation time is O(n!) in the number of latent variables, evaluating methods becomes intractable for very large graphs. The time was calculated in seconds. For the standard deviation of time, we report the mean of the standard devation of each graph since time can vary a lot based on the size of the graph. Additional Experiments: To further validate our methodology, we randomly sample 10 causal structures, applying our method alongside the baselines. We generate graphs with the number of variables ranging from 7 to 13, ensuring the structures satisfy Condition 1. Since the latent graph can only be recovered up to a permutation of the latent variables, we compute the Structural Hamming Distance (SHD) over all possible |Z|! permutations of the estimated graph and select the permutation with the lowest SHD. Due to the factorial complexity, O(|Z|!), of this evaluation in the number of latent variables, the computation becomes intractable for large graphs.</p><p>We report SHD and F1 scores in Table <ref type="table" target="#tab_2">3</ref>. We see signficant improvement over baselines, in line with Table <ref type="table" target="#tab_0">1</ref>. Note: SHD = Structural Hamming Distance (lower is better ↓). F1 scores range from 0 to 1 (higher is better ↑).</p><p>Standard deviations are reported in parentheses.</p><p>Evolution of loss: In Figure <ref type="figure">5</ref>, we plot the loss versus epochs for one of our training runs corresponding to the causal structure in Figure <ref type="figure" target="#fig_7">4a</ref>. Metrics such as SHD and F1 are omitted, as they can only be computed once the mask values converge to either 0 or 1.</p><p>We see the structural regularization loss decreases over training and converges to zero, enforcing Condition 1. The ELBO loss initially decreases but then slightly increases as we enforce the structural constraint. The proposed Hierarchical VAE model consists of a convolutional encoder, a hierarchical latent structure, and a transposed convolutional decoder. The convolutional encoder has two convolution layers (32 and 64 filters, 3x3 kernels, stride 2) followed by a fully connected layer. Each structural equation (Equation <ref type="formula">4</ref>) is modeled using a neural network with three hidden layer. The first two layers are shared to reduce the number of parameters. The convolutional decoder reconstructs images from the final latent layer. The decoder consists of a fully connected layer that maps the final latent representation of dimension 49 to a 1568-dimensional space. This output is then reshaped to a 32-channel 7x7 feature map. We then have two transposed convolutional layers with 32 and 16 filters respectively (both using 3x3 kernels, stride 2, padding 1, and output padding 1), and a final transposed convolutional layer that reconstructs the image. We initialize M with three layers with 10, 20 and 49 nodes in each of the three layers. We use ReLU activation for all neural networks.</p><p>Training: We train the model on a subset of 10,000 images. The model was trained for 300 epochs. We use batch size of 256 and do not use MINE to enforce independence between the exogenous variables. We find it makes little difference to the final output. The temperature for gumbel softmax starts at 100 and exponentially decreases to 0.1 at 120 epochs and then stays constant. λ 2 is 0.03 and λ 3 is exponentially increased from 10 -3 to 10 at 100 epochs and then stays constant. We used a Adam optimizer with learning rate 1e-3.</p><p>Visualization: Figure <ref type="figure" target="#fig_9">6</ref> displays the complete causal graph constructed from the MNIST dataset.</p><p>Note that due to non-convexity, we could not achieve zero loss for the pure children constraint term. Hence, the learnt graph does not exactly satisfy Conditions 1. To interpret the semantics of each latent feature, we perform targeted interventions designed to isolate their individual effects. Specifically, for each latent variable at the topmost layer, we set its ancestral nodes to values 5 standard deviations above or below their means, while keeping the remaining variables at their mean values. We then intervene on the current node by setting its value to the mean, effectively neutralizing its direct influence, and observe the resulting changes in the generated images. This procedure allows us to visualize and understand the specific contribution of each latent variable to the overall image structure.</p><p>By comparing the images before and after the intervention, we can discern the unique effects attributable to each latent feature. Table <ref type="table">5</ref> presents these visualizations for each feature. For each feature, the first image shows the output when the top latent variable is set 5 standard deviations below the mean; the second image shows the result when, in addition, the target feature is intervened upon and set to the mean; the third image displays the output when the top latent variable is set 5 standard deviations above the mean; and the fourth image shows the result when the target feature is intervened upon and set to the mean under this condition.</p><p>For Figure <ref type="figure" target="#fig_5">3b</ref>, we adopt a different methodology to visualize the influence of latent variables. Starting from the topmost layer of the causal graph, we traverse downward through each subsequent layer. At each node, we randomly assign its value to be either five standard deviations above or below its mean. This stochastic intervention allows us to observe the cumulative effects of these variations as they propagate through the graph.</p><p>Discussion on Learned MNIST Graph: As detailed above, Table <ref type="table">5</ref> provides visualizations for all latent features. The contrast between the first and second images (or between the third and fourth images) illustrates the concept represented by each feature. We observe that the hierarchical structure of the learned latent variables effectively captures features at different levels of abstraction. Specifically, the top layer encodes global features (e.g., digit-level information), the middle layer encodes intermediate features, and the bottom layer captures local characteristics.</p><p>For instance, consider z 0 8 : the difference between the first and second images reveals that this feature represents the digit 9. A positive value of z 0 8 correlates with the presence of the digit 9. The children of z 0 8 in the causal graph, z 1 6 and z 1 13 , further decompose this concept into its components. The difference between their respective visualizations shows that z 1 6 represents the straight line in the lower half of the digit 9, while z 1 13 captures the circular component in the upper half, along with the overall thickness of the digit.</p><p>Continuing this decomposition, the children of z 1 6 and z 1 13 , such as z 2 4 , z 2 10 , z 2 11 , and z 2 16 , represent finer-grained local features. This hierarchical organization aligns with the semantics of the images and supports the interpretability of the latent representations.</p><p>CMNIST details: For the coloured MNIST dataset, we have around 12,000 training samples and 2,000 test samples. Since we do not aim to visualize the images, we downsample them to 14x14 and train our model. We train our model for 50 epochs with early stopping with patience 3. After training the latent hierarchical model, we train a logistic regression classifier to predict the digit from the latent representations. The coefficient of the L1 regularization is 10. For all the baselines, we train the model for 50 epochs with early stopping with patience 3. For all models, we used a Adam optimizer with learning rate 1e-3.</p><p>CelebA details: For the CelebA dataset, we consider the task of predicting whether a face image has blonde hair. Since gender and hair color are highly correlated, models often use gender to predict hair color. In this task, we reverse the correlation of gender and color and test the transferability of representations. We use approximately 160,000 samples for training. For the test set, we evaluate the model exclusively on two groups: blonde males and non-blonde females. Since visualization of the images is not a focus of this work, we downsample all images to a resolution of 64×64.</p><p>The model is trained for 50 epochs, employing early stopping with a patience of 3 epochs. After training the latent hierarchical model, we fit a logistic regression classifier on the latent representations to predict the target attribute. The coefficient for the L1 regularization in the logistic regression is set to 10. For all baseline models, we adopt the same training setup of 50 epochs with early stopping (patience = 3). We use the Adam optimizer with a learning rate of 10 -3 for all models.</p><p>CelebA Results: The results are available in Table <ref type="table" target="#tab_3">4</ref>. Our model achieves a test AUC of 0.8228, outperforming both Graph VAE (AUC 0.500) and Causal VAE (AUC 0.7289). The results highlight the transferability of our representations.</p><p>C DISCUSSION ON CONDITION 1</p><p>In Figure <ref type="figure">7</ref>, we see two examples of causal graphs which violate Condition 1. Figure <ref type="figure">7a</ref> violates the pure children condition since each latent does not have two pure children. Figure <ref type="figure">7b</ref> violates the Condition 1(ii) since d(z 1 , x 4 ) = 2 ̸ = 1 = d(z 1 , x 3 ). While Condition 1(ii) may not hold in all cases, it is a reasonable assumption to make for image data. Several prior works <ref type="bibr" target="#b54">Vahdat &amp; Kautz (2020)</ref>; <ref type="bibr" target="#b31">Kong et al. (2024)</ref> have effectively modeled images using a multi-level latent hierarchical structure.  <ref type="table">5</ref>: Visualization of MNIST layer dimensions. For each feature, the first image shows the output when the top latent variable is set 5 standard deviations below the mean; the second image shows the result when, in addition, the target feature is intervened upon and set to the mean; the third image displays the output when the top latent variable is set 5 standard deviations above the mean; and the fourth image shows the result when the target feature is intervened upon and set to the mean under this condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dim</head><p>Image Dim Image Dim Image</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of a graph we consider. Note that we allow multiple paths between two nodes and hence generalize trees. The latent variables are shaded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>of the input followed by a Tanh or LeakyReLU activation function with α = 0.2. The weights for the linear transformation are uniformly sampled from [-5, -2] ∪ [2, 5]. Exogenous noise is sampled from [-α, α] where α is sampled from [-3, -1] ∪ [1, 3]. We run three random trials for each graph, and report the mean and standard deviation for each metric. Further details are given in Appendix B.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Hamming Distance (SHD) vs. Time. Lower SHD is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Score vs. Time. Higher F1 is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance vs. Time for different causal discovery methods. Time is plotted on a logarithmic scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Figures for the Image experiments. (a) Latent causal graph for digit images (b) Visualization of subgraph of the learnt latent causal graph on MNIST (c) Samples from the CMNIST dataset illustrating digit-label associations under different conditions. Top row: training set samples with default color-label mapping. Middle row: test set samples with reversed color-label mapping. Bottom row: test set samples with a consistent blue color irrespective of labels.</figDesc><graphic coords="10,367.89,93.55,118.80,120.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>y|z)dy p(z|x)dz (14) = E[y|z] p(z|x)dz. (15) By Condition 3, there exists a differentiable function g : R |X| → R |Z| such that p(z|x) = p(z|g(x)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Ground truth causal graphs for Synthetic Experiments. (a) and (b) are trees (only one path between any two nodes). (c) and (d) allow v-structures (multiple paths between two nodes)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 5: Evolution of different loss components during training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Latent causal graph for the MNIST Dataset. z i,j denotes the j th latent variable in Z i .</figDesc><graphic coords="24,151.12,379.15,309.78,97.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of latent hierarchical causal discovery methods on various graphs</figDesc><table><row><cell></cell><cell>Ours</cell><cell></cell><cell cols="2">KONG</cell><cell cols="2">HUANG</cell><cell cols="2">GIN</cell><cell cols="2">DeCAMFounder</cell></row><row><cell>Structure</cell><cell>SHD ↓</cell><cell>F1 ↑</cell><cell>SHD ↓</cell><cell>F1 ↑</cell><cell>SHD ↓</cell><cell>F1 ↑</cell><cell>SHD ↓</cell><cell>F1 ↑</cell><cell>SHD ↓</cell><cell>F1 ↑</cell></row><row><cell>Tree (LeakyReLU)</cell><cell>0.67</cell><cell>0.96</cell><cell>5.83</cell><cell>0.63</cell><cell>6.00</cell><cell>0.65</cell><cell>7.50</cell><cell>0.00</cell><cell>11.83</cell><cell>0.00</cell></row><row><cell></cell><cell cols="7">(1.49) (0.08) (2.04) (0.09) (3.00) (0.08) (1.50)</cell><cell>(0.00)</cell><cell>(0.37)</cell><cell>(0.00)</cell></row><row><cell>V-structure (LeakyReLU)</cell><cell>0.67</cell><cell>0.97</cell><cell>7.67</cell><cell>0.61</cell><cell>5.50</cell><cell>0.72</cell><cell>8.00</cell><cell>0.17</cell><cell>17.33</cell><cell>0.00</cell></row><row><cell></cell><cell cols="7">(1.10) (0.05) (4.08) (0.14) (2.50) (0.08) (0.00)</cell><cell cols="3">(0.17) (2.867) (0.00)</cell></row><row><cell>Tree (Tanh)</cell><cell>1.00</cell><cell>0.95</cell><cell>5.50</cell><cell>0.63</cell><cell>4.50</cell><cell>0.70</cell><cell>7.50</cell><cell>0.00</cell><cell>16.50</cell><cell>0.00</cell></row><row><cell></cell><cell cols="7">(1.67) (0.09) (1.52) (0.06) (1.50) (0.03) (1.50)</cell><cell cols="3">(0.00) (4.924) (0.00)</cell></row><row><cell>V-structure (Tanh)</cell><cell>1.17</cell><cell>0.95</cell><cell>4.33</cell><cell>0.79</cell><cell>4.50</cell><cell>0.76</cell><cell>9.50</cell><cell>0.36</cell><cell>18.50</cell><cell>0.00</cell></row><row><cell></cell><cell cols="10">(1.33) (0.06) (2.58) (0.08) (1.50) (0.03) (1.50) (0.064) (3.253) (0.00)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Test Accuracy on the CMNIST dataset. Standard deviations are reported in parentheses.</figDesc><table><row><cell></cell><cell>Ours</cell><cell>Autoencoder</cell><cell>VAE</cell><cell cols="2">β-VAE (β = 1e-3) β-VAE (β = 10)</cell><cell>Graph VAE</cell><cell>Causal VAE</cell></row><row><cell cols="4">Reverse 0.979 (0.004) 0.854 (0.037) 0.536 (0.000)</cell><cell>0.843 (0.140)</cell><cell>0.536 (0.000)</cell><cell>0.665 (0.231) 0.916 (0.075)</cell></row><row><cell>black</cell><cell cols="3">0.753 (0.106) 0.6492 (0.195) 0.536 (0.000)</cell><cell>0.487 (0.215)</cell><cell>0.536 (0.000)</cell><cell>0.766 (0.174) 0.653 (0.183)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance of latent hierarchical causal discovery methods on additional randomly generated graphs</figDesc><table><row><cell>Ours</cell><cell></cell><cell cols="2">KONG</cell><cell cols="2">HUANG</cell><cell>GIN</cell><cell></cell><cell cols="2">DeCAMFounder</cell></row><row><cell>SHD ↓</cell><cell>F1 ↑</cell><cell>SHD ↓</cell><cell>F1 ↑</cell><cell>SHD ↓</cell><cell>F1 ↑</cell><cell>SHD ↓</cell><cell>F1 ↑</cell><cell>SHD ↓</cell><cell>F1 ↑</cell></row><row><cell>1.00</cell><cell>0.94</cell><cell>4.60</cell><cell>0.68</cell><cell>5.50</cell><cell>0.65</cell><cell>7.60</cell><cell>0.05</cell><cell>13.8</cell><cell>0.00</cell></row><row><cell cols="9">(1.41) (0.08) (1.58) (0.09) (3.56) (0.12) (1.80) (0.15) (2.30)</cell><cell>(0.00)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Test AUC on the CelebA dataset</figDesc><table><row><cell>Ours</cell><cell cols="2">Graph VAE Causal VAE</cell></row><row><cell>CelebA 0.8228</cell><cell>0.500</cell><cell>0.7289</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(⇐) We prove the contrapositive. Suppose the union of parents of measured variables in S contains more than one node. Then there exist distinct parent nodes p and p ′ such that at least one of their respective children is in S. We can choose T such that both T and X \ T contains at least one pure child of p and one pure child of p ′ . This choice is possible due to Condition 1, which states that all latent variables have at least two pure children.</p><p>For this choice of T, we have r(T, X \ T) ≥ 2, as both p and p ′ are needed to d-separate the two sets. This contradicts the condition that r(T, S \ T) = 1 for all T ⊆ S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 PROOF OF LEMMA 2</head><p>Lemma 2. Let G be a hierarchical latent causal model satisfying Conditions 1. Let X ⊂ V be the set of observed variables. Under the faithfulness condition, for any observed variable c ∈ X and any set of latent variables P ⊆ Z 1 , c is a child of exactly the variables in P if and only if the following conditions hold:</p><p>The equality in condition (1) does not hold for any proper subset of P.</p><p>Proof. We will prove both directions of the if and only if statement.</p><p>(⇒) Suppose c is a child of exactly the variables in P.</p><p>Let S ⊆ X be any set such that |S ∩ Ch(z i )| = 1 for each z i ∈ P, and let T = X \ (S ∪ {c}).</p><p>By the structure of the graph, P d-separates S from T, and P also d-separates S ∪ {c} from T. This is because c is a child of all nodes in P, so including it with S doesn't create any new paths to T that aren't blocked by P. Therefore, we have r(S, T) = r(S ∪ {c}, T) = |P|.</p><p>This satisfies condition (1). Condition (2) is satisfied because no proper subset of P contains all parents of c, so no proper subset of P can d-separate S ∪ {c} from T.</p><p>(⇐) Now suppose conditions (1) and (2) hold. We will prove that c is a child of exactly the variables in P.</p><p>First, we show that P d-separates c from all other variables in X not in S ∪ {c}.</p><p>Let S be as defined in condition (1), and T = X \ (S ∪ {c}). The equality in condition (1) implies: This implies that the parents of c must be a subset of P, as any path from c to T not going through P would violate the d-separation. Now, condition (2) states that this equality doesn't hold for any proper subset of P. This means that every variable in P is necessary for the d-separation. If any variable in P were not a parent of c, then we could remove it and still maintain the d-separation, contradicting condition (2).</p><p>Therefore, c must be a child of exactly the variables in P.</p><p>A.6 PROOF OF LEMMA 3</p><p>Lemma 3. Let G be a graph satisfying Conditions 1. A measured variable c has no parent if and only if r({c}, X \ {c}) = 0.</p><p>Proof. We will prove both directions of the if and only if statement.</p><p>(⇒) Suppose c has no parent.</p><p>In this case, c is independent of all other variables in the graph. Therefore, r({c}, X \ {c}) = 0.</p><p>(⇐) Suppose r({c}, X \ {c}) = 0.</p><p>This means that no latent variables are needed to render c independent of all other observed variables.</p><p>In other words, c is already independent of all other observed variables. Now, suppose for the sake of contradiction that c has a parent z. By Condition 1, z must have at least two pure children, one of which could be c, and let's call the other one x. Then c and x would be dependent through their common parent z, contradicting the independence of c from all other observed variables.</p><p>Therefore, c cannot have any parent.</p><p>A.7 PROOF OF THEOREM 3</p><p>Theorem 3. Let G = (V, E) be a hierarchical latent causal model satisfying Condition 1. Let M be the binary adjacency matrix representing the structure of G. Let data X be generated according to the structural equation model defined in Equation <ref type="formula">1</ref>. Given a function r(S, T) which outputs the minimum number of latent variables which d-separate any two sets measured sets S and T, M is identifiable up to the permutation of the latent variables.</p><p>Proof. We prove Theorem 3 using the principle of mathematical induction on the number of layers.</p><p>We begin with three lemmas that we require for our proof.</p><p>We prove this theorem by induction on the number of layers in the hierarchical latent causal model.</p><p>Base case: Let G = (V, E) be a hierarchical latent causal model with one latent layer, i.e., Z = Z 1 .</p><p>We begin by identifying the pure children of each latent variable in Z 1 . By Lemma 1, for any set of measured variables S ⊆ X, if r(T, X \ T) = 1 for all T ⊆ S, then all variables in S are pure children of the same parent. For all |T| = 1, this trivially holds. Using Theorem 1, we can exhaustively check this condition for all subsets |T| &gt; 1 of X to identify all sets of pure children.</p><p>Next, we identify the parents of non-pure children using Lemma 2. For each observed variable c ∈ X that is not identified as a pure child in the previous step, we determine its parents by verifying the conditions stated in Lemma 2 for all possible subsets of Z 1 . For most cases, |S∪{c}|, |X\(S∪{c})| &gt; |P| which allows us to use Theorem 1. For cases, where this does not hold, it implies P = Z 1 . In this case, Lemma 2 can be applied for all other subsets of Z 1 and if none of them satisfy the conditions, the set of parents has to be Z 1 .</p><p>Through these two steps, we fully identify the structure between Z 1 and X, thus recovering the binary adjacency matrix M for the one-layer model.</p><p>Inductive step: Assume the theorem holds for all models with L -1 layers, where L &gt; 1. We will prove it holds for models with L layers.</p><p>Let G = (V, E) be a hierarchical latent causal model with L layers. We first identify the structure between Z 1 and X using the same process as in the base case, applying Lemmas 1 and 2.</p><p>Let G ′ = (V ′ , E ′ ) be the subgraph of G obtained by removing all measured variables X. We claim that G ′ satisfies Condition 1. Each latent variable in Z 2 has at least two pure children in G ′ , and these belong to Z 1 . Moreover, the equal path length condition is preserved since the path from any latent to any variable in Z 1 is one less than the path to any variable in X. Some variables Z 1 may not have any parent. We can identify those using Lemma 3.</p><p>To determine the d-separation relations between variables in Z 1 , we utilize Theorem 2. For any two subsets Z X , Z Y ⊆ Z 1 , let X and Y be sets of their respective pure children. By Theorem 2, we have r(Z X , Z Y ) = r(X, Y), allowing us to infer the d-separation relations within Z 1 . Now, G ′ is a hierarchical latent causal model with L-1 layers that satisfies the conditions of the theorem. By the induction hypothesis, we can identify the structure of G ′ , recovering the corresponding part of the binary adjacency matrix M . </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Identification of partially observed linear causal models: Graphical conditions for the non-gaussian and heterogeneous cases</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niels</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="22822" to="22833" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The decamfounder: nonlinear causal discovery in the presence of hidden variables</title>
		<author>
			<persName><forename type="first">Raj</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neha</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1639" to="1658" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recursive causal structure learning in the presence of latent variables and selection bias</title>
		<author>
			<persName><forename type="first">Sina</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Mokhtarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amiremad</forename><surname>Ghassami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Negar</forename><surname>Kiyavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10119" to="10130" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning linear bayesian networks with latent variables</title>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adel</forename><surname>Javanmard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="249" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Invariant risk minimization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ishmael Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15106</idno>
		<title level="m">Deconfounded score method: Scoring dags with dense unobserved confounding</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Differentiable causal discovery under unmeasured confounding</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Malinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Shpitser</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2314" to="2322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly supervised causal representation learning</title>
		<author>
			<persName><forename type="first">Johann</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pim</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="38319" to="38331" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Differentiable causal discovery from interventional data</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Brouillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Drouin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21865" to="21877" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identification of linear latent variable model with arbitrary distribution</title>
		<author>
			<persName><forename type="first">Zhengming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="6350" to="6357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chickering</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-sample learning of bayesian networks is np-hard</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1287" to="1330" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning latent tree graphical models</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Myung</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Vincent Yf Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1771" to="1812" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning sparse causal models is not np-hard</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Claassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Heskes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.6824</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning highdimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marloes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="294" to="321" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning the causal structure of copula models with latent variables</title>
		<author>
			<persName><forename type="first">Ruifei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perry</forename><surname>Groot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Schauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Heskes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A versatile causal discovery framework to allow causallyrelated hidden variables</title>
		<author>
			<persName><forename type="first">Xinshuai</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Legaspi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11001</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Asymptotic evaluation of certain markov process expectations for large time</title>
		<author>
			<persName><forename type="first">D</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><surname>Donsker</surname></persName>
		</author>
		<author>
			<persName><surname>Sr Srinivasa Varadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">iv. Communications on pure and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="212" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Marginal likelihood and model selection for gaussian latent tree and forest models</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaowei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Zwiernik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unsupervised learning of transcriptional regulatory networks via latent tree graphical models</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Gitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ragupathyraj</forename><surname>Valluvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Fraenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06335</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bayesian pyramids: Identifiable multilayer discrete latent structure models for discrete data</title>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="399" to="426" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variational autoencoders with jointly optimized latent dependency structure</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lehrmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">M</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Sonnerat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matko</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murray</forename><surname>Bosnjak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><surname>Lerchner</surname></persName>
		</author>
		<author>
			<persName><surname>Scan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03389</idno>
		<title level="m">Learning hierarchical compositional visual concepts</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Latent hierarchical causal structure discovery with rank constraints</title>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="5549" to="5561" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nonlinear ica using auxiliary variables and generalized contrastive learning</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Turner</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="859" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning latent causal graphs via mixture oracles</title>
		<author>
			<persName><forename type="first">Bohdan</forename><surname>Kivva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goutham</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18087" to="18101" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Identification of nonlinear latent hierarchical models</title>
		<author>
			<persName><forename type="first">Lingjing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuejie</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2010">2010-2032, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning discrete concepts in latent hierarchical models</title>
		<author>
			<persName><forename type="first">Lingjing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuejie</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.00519</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Causal clustering for 1-factor measurement models</title>
		<author>
			<persName><forename type="first">Erich</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Ramsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1655" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mnist handwritten digit database</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
	</analytic>
	<monogr>
		<title level="j">ATT Labs</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Causal discovery from observational and interventional data across multiple environments</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Jaber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="16942" to="16956" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Scalable differentiable causal discovery in the presence of latent confounders with skeleton posterior (extended version)</title>
		<author>
			<persName><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.10537</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Achille</forename><surname>Nazaret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elham</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.10263</idno>
		<title level="m">Stable differentiable causal discovery</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Masked gradient-based causal structure learning</title>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuangyan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the 2022 SIAM International Conference on Data Mining (SDM)</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Structure learning with continuous optimization: A sober look and beyond</title>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Causal Learning and Reasoning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="71" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Comprehensive review and empirical evaluation of causal discovery algorithms for numerical data</title>
		<author>
			<persName><forename type="first">Wenjin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingbo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.13054</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Causes of severe pneumonia requiring hospital admission in children without hiv infection from africa and asia: the perch multi-country case-control study</title>
		<author>
			<persName><forename type="first">L O'</forename><surname>Katherine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><forename type="middle">C</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdullah</forename><surname>Baggett</surname></persName>
		</author>
		<author>
			<persName><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">L</forename><surname>Daniel R Feikin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melissa</forename><forename type="middle">M</forename><surname>Hammitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen Rc</forename><surname>Higdon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">Deloria</forename><surname>Howie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><forename type="middle">L</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orin</forename><forename type="middle">S</forename><surname>Kotloff</surname></persName>
		</author>
		<author>
			<persName><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet</title>
		<imprint>
			<biblScope unit="volume">394</biblScope>
			<biblScope unit="page" from="757" to="779" />
			<date type="published" when="2019">10200. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Probabilistic reasoning in intelligent systems; network of plausible inference</title>
		<author>
			<persName><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988. 1988</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Models, reasoning and inference</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>CambridgeUniversityPress</publisher>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">3</biblScope>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Beware of the simulated dag! causal discovery benchmarks may be easy to game</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Reisach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Seiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Weichwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27772" to="27784" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Toward causal representation learning</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="612" to="634" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning large dags is harder than you think: Many losses are minimal for the wrong dag</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matej</forename><surname>Zečević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Dhami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Nodags-flow: Nonlinear cyclic causal structure learning</title>
		<author>
			<persName><forename type="first">Romain</forename><surname>Muralikrishnna G Sethuraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faramarz</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Fekri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Christian</forename><surname>Biancalani</surname></persName>
		</author>
		<author>
			<persName><surname>Hütter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="6371" to="6387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Estimation of linear non-gaussian acyclic models for latent factors</title>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">7-9</biblScope>
			<biblScope unit="page" from="2024" to="2027" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning the structure of linear latent variable models</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chickering</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Introduction to causal inference</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Causation, prediction, and search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unpaired multi-domain causal representation learning</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Sturma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Jithendaraa</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashas</forename><surname>Annadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivaxi</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.13583</idno>
		<title level="m">Learning latent structural causal models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Nvae: A deep hierarchical variational autoencoder</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">19667-19679, 2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Eli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.05330</idno>
		<title level="m">Hierarchical causal models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Generalized independent noise condition for estimating latent variable causal graphs</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14891" to="14902" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Identification of linear non-gaussian latent hierarchical structure</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangbo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="24370" to="24387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Causalvae: Disentangled representation learning via neural structural causal models</title>
		<author>
			<persName><forename type="first">Mengyue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9593" to="9602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dag-gnn: Dag structure learning with graph neural networks</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7154" to="7163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Causal discovery with multi-domain lingam for latent factors</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michio</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Hao</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Causal Analysis Workshop Series</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias</title>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="issue">16-17</biblScope>
			<biblScope unit="page" from="1873" to="1896" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">D-vae: A variational autoencoder for directed acyclic graphs</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shali</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Dags with no tears: Continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Pradeep K Ravikumar</surname></persName>
		</author>
		<author>
			<persName><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning sparse nonparametric dags</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>Pmlr</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3414" to="3425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">On the identifiability of nonlinear ica: Sparsity and beyond</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="16411" to="16422" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
