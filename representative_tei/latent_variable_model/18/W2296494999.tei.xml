<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling High-Dimensional Humans for Activity Anticipation using Gaussian Process Latent CRFs</title>
				<funder ref="#_dyzxg9u">
					<orgName type="full">ARO</orgName>
				</funder>
				<funder ref="#_ups28Bj">
					<orgName type="full">ONR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yun</forename><surname>Jiang</surname></persName>
							<email>yunjiang@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
							<email>asaxena@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling High-Dimensional Humans for Activity Anticipation using Gaussian Process Latent CRFs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Activity: Taking out Microwave Cup RGB-D videos Low-dimensional Human Representations High-dimensional Human Representations Anticipation Task</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For robots, the ability to model human configurations and temporal dynamics is crucial for the task of anticipating future human activities, yet requires conflicting properties: On one hand, we need a detailed high-dimensional description of human configurations to reason about the physical plausibility of the prediction; on the other hand, we need a compact representation to be able to parsimoniously model the relations between the human and the environment.</p><p>We therefore propose a new model, GP-LCRF, which admits both the high-dimensional and low-dimensional representation of humans. It assumes that the high-dimensional representation is generated from a latent variable corresponding to its lowdimensional representation using a Gaussian process. The generative process not only defines the mapping function between the high-and low-dimensional spaces, but also models a distribution of humans embedded as a potential function in GP-LCRF along with other potentials to jointly model the rich context among humans, objects and the activity. Through extensive experiments on activity anticipation, we show that our GP-LCRF consistently outperforms the state-of-the-art results and reduces the predicted human trajectory error by 11.6%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The ability to anticipate possible future moves of a human is a necessary social skill for humans as well as for robots that work in assembly-line environments (e.g., Baxter) or in homes and offices (e.g., PR2). With such a skill, the robots can work better with humans by performing appropriate tasks and by avoiding conflict. For instance, Koppula et. al. <ref type="bibr" target="#b19">[20]</ref> used anticipation in assistive robotic settings, such as in the tasks of opening doors for people or serving drinks to people.</p><p>Human activity anticipation is a very challenging task, especially in unstructured environments with a large variety of objects and activities. Koppula et. al. <ref type="bibr" target="#b19">[20]</ref> have shown that the rich context (such as object-object and human-object spatial relations) is important for predicting high-level human activities. However for anticipation and robotic planning, predicting detailed human motions is also crucial. In this work, our goal is to model the detailed human motions, along with the rich context, in anticipating the human activities. We specifically focus on how to represent (and learn with) high-dimensional human configurations and their temporal dynamics.</p><p>Recently, high-dimensional description of human motions is widely available through motion capture data or RGB-D cameras (e.g., Kinect), where a human configuration is specified by the joint locations and orientations and often has more than 30 degrees of freedom. While it captures human kinematics and dynamics accurately, modeling human motions Fig. <ref type="figure">1</ref>: Given an RGB-D video of a human interacting with the environment, we are interested in predicting the future: what activity will he perform and how the environment and human pose will change. The key idea in this work is to compactly represent the highdimensional human configuration in a low-dimensional space so that we can model relations between the human, activity and objects more effectively in a graphical model. in such space (much higher than 30 DOF when considering velocities and accelerations) often requires a detailed musculoskeletal human model and a large number of spatial and timing constraints to produce smooth and realistic motions <ref type="bibr" target="#b3">[4]</ref>.</p><p>Such a high-DOF model does not lend itself to use in learning models where rich modeling of the human with the environment is needed. Therefore, some works assume a few static human poses are representative enough <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12]</ref> or simplify a human configuration to a 2D point for navigation task <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b22">23]</ref> or to a 3D trajectory of one hand while keeping the rest body static neglecting kinematic constraints <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19]</ref>. In these works, human motions are underrepresented and would fail when a more elaborate human motion prediction is required.</p><p>In this work, we design a model that can handle the two competing requirements: (a) having access to a highdimensional model of a human skeleton so that physical feasibility and other kinematic criterion can be reasoned about, and (b) having a low-dimensional representation that allows its use in learning algorithms that model the human's relation to entities in the environment containing objects. The general idea is to learn a two-way mapping between the high-dimensional and low-dimensional representations of the human configurations. Meanwhile, we associate the mapping with a probability distribution over the low-dimensional space so that data-driven learning approaches can use the distribution to build a probabilistic model that captures the relationships between human and other entities in the environment.</p><p>We therefore propose a model GP-LCRF (stands for Gaussian Process Latent Conditional Random Field), which is a conditional random field (CRF) augmented with latent nodes in low-dimensional space corresponding to the compressed high-dimensional nodes in the CRF. The correspondence is modeled using a Gaussian process that explicitly defines a mapping function and a likelihood function over the two spaces. The likelihood function is incorporated as the potential function for the edges between the low-dimensional and high-dimensional nodes. In our application to the task of anticipating human activities, GP-LCRF models human configurations as the high-dimensional nodes and learns their compact representation as latent nodes. Edges between latent nodes are used to model the continuity of human motions. Inspired by <ref type="bibr" target="#b19">[20]</ref>, GP-LCRF also models objects and subactivities as other nodes and their spatial-temporal relations as edges. In this way, GP-LCRF provides a joint graphical model for humans, objects and sub-activities together.</p><p>We show that, during training the model, the likelihood could be decomposed into two disjoint sets, one for learning the mapping of the latent space and the other for learning the parameters of the CRF. During inference, our goal is to anticipate future human actions, for which we use Gibbs sampling for inferring probable human configurations.</p><p>We test our model on CAD-120 human activity dataset <ref type="bibr" target="#b21">[22]</ref>, which contains 120 RGB-D videos of daily human activities such as eating food, cleaning, etc. Through extensive experiments, we show that our GP-LCRF achieves the state-of-theart results while comparing against multiple baselines with or without modeling humans. Particularly, we reduce the human trajectory prediction error by 11.6% (with a p-value of 0.0107), which could make significant difference to robot planning that uses the predicted future human motions.</p><p>In summary, our contributions are:</p><p>• We propose a new graphical model, GP-LCRF, that bypasses the difficulty of modeling high-dimensional variables in traditional CRFs by augmenting the graph with latent nodes corresponding to the low-dimensional representations of those variables. • We apply GP-LCRF to the human activity anticipation task. GP-LCRF is able to capture both the rich context in the environment and the temporal dynamics of humans.</p><p>• We test our model on a large human activity dataset (with various objects and activities) and achieve the state-ofthe-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Modeling human configurations has attracted great attention in many computer vision and robotic applications. There is a significant body of work building a full human body model for tracking and reconstruction human motions. For example, Navaratnam et al. <ref type="bibr" target="#b29">[30]</ref> learn a mapping from image features to human joint angles to estimate human pose in 2D images. Demircan et al. <ref type="bibr" target="#b3">[4]</ref> utilize a detailed subject-customized biomechanical model and multiple markers from motion capture data to reconstruct realistic human motions in real-time. Kulić et al. <ref type="bibr" target="#b23">[24]</ref> consider clustering human configurations into motion segments. In the field of character animation, human model is also used to synthesize and plan the high-dimensional human motions <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b27">28]</ref>. There are some works focusing on learning a low-dimensional representation of humans that can be used to interpolate new human motions <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b6">7]</ref>. While these works can model detailed human configuration well, the highdimensional representation is not suitable for probabilistic modeling in the problem of human activity anticipation, and thus they are complimentary to ours. Some works reduce the large space by using a few static poses. Grabner et al. <ref type="bibr" target="#b5">[6]</ref> and Gupta et al. <ref type="bibr" target="#b7">[8]</ref> utilize imaginary human actors to detect objects and human workspace in 2D images. Jiang et al. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> apply hallucinated human configurations to a robotic task of arranging 3D indoor scenes. In another previous work of anticipating human activities <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19]</ref>, human motions are implicitly modeled through object trajectories. Some other works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b22">23]</ref> predict possible human navigation trajectories in 2D from visual data. In all the aforementioned works, human motions are under-represented and would fail when a more elaborate prediction on human movements is required. Some other works consider human robot collaborations without anticipating human activities <ref type="bibr" target="#b0">[1]</ref>, focus on high-level actions <ref type="bibr" target="#b30">[31]</ref>, or consider object affordances for manipulation <ref type="bibr" target="#b15">[16]</ref>. These works are orthogonal to ours.</p><p>In terms of capturing the context in human activities, conditional Random Fields (CRFs) <ref type="bibr" target="#b24">[25]</ref> have emerged as a popular way to model contextual relations. Many variants augment CRFs with latent variables in order to model hidden states, such as latent CRFs <ref type="bibr" target="#b32">[33]</ref> that have been applied to object recognition <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b41">42]</ref>, scene understanding <ref type="bibr" target="#b34">[35]</ref>, gesture recognition <ref type="bibr" target="#b39">[40]</ref> and grounding natural language to robotic tasks <ref type="bibr" target="#b28">[29]</ref>. However, in these models, the predefined latent space is discrete and small to keep the learning and inference tractable. Our GP-LCRF, on the contrary, learns the latent space in a non-parametric way and admits continuous latent values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OVERVIEW</head><p>We define the anticipation task as follows: Given an RGB-D video of a human interacting with the surrounding environment, our goal is to predict what will happen to the environment in a time span in terms of the next sub-activity label, object affordance labels and object trajectories. Modeling future human configurations, in the context of the activity and the environment, is a key ingredient for a good anticipation.</p><p>Human configuration has two sides of nature:   <ref type="bibr" target="#b19">[20]</ref> and our GP-LCRF. Our model adds latent low-dimensional nodes X to the model, which are related to the original high-dimensional human configuration nodes H through Gaussian Process latent variable model with parameters α, β, γ. Shaded nodes indicate observations. In both models, temporal segment t is given for anticipation with observed human poses H t and object locations L t , and the goal is to infer the next segment t + 1 where nothing is observed.</p><p>possesses. One would need the location and orientation of each joint of a human skeleton to fully describe a static human pose, and the velocities and accelerations to describe a sequence of human motions. The high-dimensional representation is a guarantee for generating realistic human poses/motions. We need it to perform (self-)collision detection, inverse kinematics and path planning.</p><p>However, most dynamic human behaviors are intrinsically low-dimensional, as our arms and legs operate in a coordinated way and they are far from independent with each other. Many daily behaviors such as walking and jumping have been represented in low-dimensional space <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34]</ref>. The low-dimensional representation is a requisite for probabilistic models of human motions. The distribution of human poses can be used to synthesize or predict new poses.</p><p>Our main idea is to keep both the high-and low-dimensional representation of human dynamics in anticipating human activities. Our learning model thus has two parts:</p><p>Learning low-dimensional human dynamic distributions. For each human pose, indexed by i, we use h i to denote its high-dimensional and x i for its corresponding lowdimensional representation. The correspondence is specified by a mapping function, i.e. h i = f (x i ). Additionally, we are also interested in associating the mapping with a probabilistic model, so that we can generate new human dynamics (x i , h i ) from the learned distribution. Hence, the objective of this part is to learn the parameters of f as well as a likelihood function L(x i , h i ) from the training data {h i }.</p><p>Modeling the spatial and temporal context of human activities. We use a graphical model, following <ref type="bibr" target="#b19">[20]</ref> to capture the three important aspects in a human activity-sub-activities A, objects O and humans H. Given a video segment t,<ref type="foot" target="#foot_0">foot_0</ref> each entity is represented by a node in the graph modeling its prior distribution and the edges in the graph model their relations, as shown in Fig. <ref type="figure" target="#fig_0">2</ref>. The whole video is a repetition of such a graph. Edges between consecutive segments are used to model temporal dynamics. In particular, for each human pose in segment t, in addition to the original human node h t i , we add a low-dimensional latent node x t i . The edges between h t i and O t or A t are used to capture human-object and human-activity relations, while the edges between x t-1 i and x t i are for modeling the human dynamics. This graphical model thus defines a joint distribution P (A, O, H, X ) as a product of parameterized edge potentials. We learn those parameters from labeled data and then sample future segments from this distribution for anticipation.</p><p>By combining these two parts, our proposed GP-LCRF possesses many advantages: First, we can now use the context of high-dimensional data that is difficult to model for a traditional CRF. Second, as the low-dimensional representation is modeled as latent nodes and the mapping is learned in an unsupervised way, our model does not require any extra label/data to learn. Third, being able to learn the distribution of the low-dimensional latent node makes our GP-LCRF a generative model that suits the anticipation problem. Before presenting our GP-LCRF, we first briefly review the background of the two parts in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PRELIMINARIES A. Dimensionality Reduction with a Gaussian Processes</head><p>Consider a general setting for regression problems: Our goal is to learn a mapping h = f (x) for a set of N training pairs (x i , h i ). However, from a Bayesian point of view, instead of mapping to one point, a Gaussian process (GP) "maps" x to a distribution of h. Let µ be the mean of the training data µ = h i /N , and let</p><formula xml:id="formula_0">H k = [h 1,k -µ k , . . . , h N,k -µ k ]</formula><p>T be the feature vectors of the k th dimension. In a GP model, H k can be viewed as one sample from a multivariate Gaussian distribution:</p><formula xml:id="formula_1">P (H k |{x i }) = 1 (2π) N/2 |K| 1/2 exp(- 1 2 H T k K -1 H k ) (1)</formula><p>K is the covariance matrix of all inputs x i . We can use many non-linear kernel functions, such as the popular "RBF kernel", to admit non-linear mappings:</p><formula xml:id="formula_2">K i,j = k(x i , x j ) = α exp(- γ 2 ||x i -x j || 2 ) + δ xi,xj β -1</formula><p>where δ xi,xj is the Kronecker delta. Using this kernel means that the two points, x i and x j , that are correlated in the latent space, will also be highly correlated after the mapping. The parameter α imposes a prior on how much the two points are correlated, γ is the inverse width of the similarity function, and β reflects how noisy the prediction is in general.</p><p>In a more general setup, only h 1 , . . . h N are given and the goal is to determine the mapping function f as well as the corresponding x i . This can be solved using Gaussian process latent variable models (GPLVM) proposed in <ref type="bibr" target="#b25">[26]</ref>. GPLVM maximizes the likelihood of the training data, based on Eq. ( <ref type="formula">1</ref>), to learn the parameters of the kernel function (γ, α, β) and the latent variables x 1 , . . . , x N .</p><p>Since GPLVM provides a probabilistic model of nonlinear mappings and generalizes well for small datasets, it has been extended to model human motions in many works. For example, it is integrated with a dynamical model to capture dynamical patterns in human motions <ref type="bibr" target="#b38">[39]</ref> so that it can provide a strong prior for tracking human activities <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b4">5]</ref>. In this work, we also adopt GPLVM as a dimensionality reduction approach, however, our goal is to incorporate this with Latent CRFs to model high-dimensional human motions and rich context in the environment at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Capturing Context with CRF</head><p>Conditional Random Fields (CRFs) have been widely applied to a variety of vision and robotic applications for its ability to model rich contextual relations. Given a graph, where V denotes the nodes and E for the edges, A CRF defines the joint distribution of V conditioned on observed features:</p><formula xml:id="formula_3">P (V) ∝ i ψ i (v i ) (i,j)∈E ψ i,j (v i , v j )</formula><p>where ψ i denotes the unary potential and ψ i,j denotes the pairwise potentials. For instance, in scene labeling, each segment is modeled as a node whose value indicates the object class label, and the edges between nodes describe the object-object (spatial) context <ref type="bibr" target="#b1">[2]</ref>. The potentials are usually parameterized as a inner product of the feature and parameter vectors. The parameters are learned by maximizing the joint likelihood of training data.</p><p>In the task of anticipating human activities, previous work <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19]</ref> propose a novel CRF, ATCRF, to model the relationships between sub-activity labels A, object affordance labels O, object locations L and human poses H, as shown in Fig. <ref type="figure" target="#fig_0">2-(a)</ref>. Inside one temporal segment, a graph structure is defined as G t = (V t , E t ) with V t = {O t , H t , A t }. The edges capture object-activity, object-object and human-activity relations. They also model temporal relations with edges between the sub-activities nodes and the same object nodes from the two consecutive segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. REPRESENTING HUMANS</head><p>In this work, we define the high-dimensional representation of a human pose h as:</p><p>• Joint locations: We include the head location and the local locations (with respect to the head) of eight upper-body joints (neck, torso, left and right shoulders, elbows and hands). This gives 27 features in total. • Head 3D orientation in a global coordinate system.</p><p>• Velocity and acceleration: To distinguish motions with different directions (e.g., moving left vs. right), we additionally include the difference of consecutive poses. For all the 30 features above, the velocity and acceleration at time t are computed as h t -h t-1 and h t -2h t-1 + h t-2 . In total, each h is a 90-dimension vector.</p><p>Curse of H's high dimensionality. In ATCRFs <ref type="bibr" target="#b19">[20]</ref>, anticipation is conducted by sampling future segments, including possible sub-activities, object affordance labels and object trajectories. Because of the high dimensionality of the human configuration H, instead of sampling the full human body they assume that the sampled object trajectory is always reachable by hands and only samples the hand location around the object. As a result, the anticipated human configuration is not realistic and this inaccuracy propagates to the computation of features and the potential function scores. Furthermore, because they assume that h is fully determined by the object trajectory, its temporal potential ψ(h t , h t-1 ) overlaps with ψ(o t , o t-1 ) and thus does not capture the human dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. GP-LCRF FOR HUMAN ACTIVITY ANTICIPATION</head><p>We propose a model, GP-LCRF, that learns a probabilistic mapping between the high-and low-dimensional representation of human dynamics based on Gaussian processes. Then it embeds the compactly represented humans as latent nodes in a CRF to capture a variety of context between the human, objects and activities.</p><p>Our GP-LCRF introduces a layer of latent nodes in a CRF: each node h i is now linked to a latent node x i and their relation is defined by a GPLVM with parameters (α, β, γ). Because latent nodes have much lower dimensions, we can model the edges between latent nodes (e.g., (x t i , x t+1 i</p><p>)) instead of attempting to capture it with high-dimensional nodes directly. (The high-dimensionality of the human nodes makes the edge distribution ill-conditioned.) Figure <ref type="figure" target="#fig_0">2</ref> shows the corresponding graphical model.</p><p>GP-LCRF differs from other latent CRFs in two aspects: Prior. We adopt GPLVM to impose a Gaussian process prior on the mapping and a 2 -norm prior on the latent nodes. This prior regulates the mapping so that the high-dimensional human configurations h i that are close in the original space would remain close in the latent space x i . This property of local distance preservation is very desirable in many applications, especially for time series analysis. Non-parametric. In many latent CRFs, the values of latent nodes are discrete and finite <ref type="bibr" target="#b32">[33]</ref>. Some other works consider a non-parametric Bayesian prior over the latent values but they do not handle dimensionality reduction. In our GP-LCRF, the latent space is completely determined by the training data, making it more adaptive to various applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Likelihood of GP-LCRF</head><p>As shown in Fig. <ref type="figure" target="#fig_0">2</ref>-(b), a GP-LCRF is a repetition of small graphs (one per each temporal segment). A segment Fig. <ref type="figure">3</ref>: An example of the learned mapping from the high-dimensional human configurations to a 2-dimensional space. The intensity of each pixel x visualizes the its probability -D 2 ln σ 2 (x) -1 2 ||x|| 2 . We also plot the projected 2D points for different activities in different colors. We can see that human configurations from the same activity are mapped to a continuous area in the 2D space while the ones from different activities are separated.</p><p>t contains one sub-activity label node A t , object affordance label nodes O t = {O t i }, object location nodes L t = {L t i }, high-dimensional human configurations H t = {h t i } and lowdimensional human representations X t = {x t i }. Following the independence assumptions imposed by the edges in the graph, the likelihood of one temporal segment</p><formula xml:id="formula_4">P (A t , O t , X t |L t , H t ) is, P t ∝ ψ(A t , H t ) i ψ(A t , o t i ) i ψ(o t i , L t i ) (i,j) ψ(o t i , o t j ) i φ(x t i , h t i )<label>(2)</label></formula><p>where the first four terms capture human-activity relations, object-activity relations, object affordances and object-object relations respectively. These potentials are parameterized as log-linear functions of feature vectors <ref type="bibr" target="#b19">[20]</ref>. We define the last term, potential of the mapping between x i and h i as the likelihood derived from GPLVM:</p><formula xml:id="formula_5">φ(x i , h i ) = exp L(x i , h i )<label>(3)</label></formula><formula xml:id="formula_6">L(x, h) = - ||h -f (x)|| 2 2σ 2 (x) - D 2 ln σ 2 (x) - 1 2 ||x|| 2 (4)</formula><p>where</p><formula xml:id="formula_7">f (x) = µ + H T K -1 k(x) σ 2 (x) = k(x, x) -k(x) T K -1 k(x) k(x) = [k(x, x 1 . . . , k(x, x N )] T</formula><p>The three terms in L(x, h) measure the discrepancy between the given h and the prediction f (x), the uncertainty of the prediction, and the prior of the latent value x.</p><p>We now consider the temporal relations between the two consecutive temporal segments t -1 and t:</p><formula xml:id="formula_8">P t-1,t ∝ ψ(A t , A t-1 ) i ψ(o t i , o t-1 i )φ(x t i , x t-1 i )<label>(5)</label></formula><p>where the first two terms capture the temporal transitions of sub-activity labels and object affordance labels. They are also parameterized as log-linear functions of features <ref type="bibr" target="#b19">[20]</ref>. We define the last term, the temporal transitions of latent nodes, as Gaussian distributions:</p><formula xml:id="formula_9">φ(x t i , x t-1 i ) ∝ N (||x t i -x t-1 i || 2 ; 0, 1)<label>(6)</label></formula><p>Hence, the overall likelihood of a GP-LCRF is</p><formula xml:id="formula_10">L GP-LCRF ∝ T t=1 P t × T t=2 P t-1,t<label>(7)</label></formula><p>Using this function, we learn the parameters by maximize the training data's likelihood and to predict the future activities and human dynamics by sampling from this distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning</head><p>During training, given all observations (H and L) and labels (A and O), our goal is to learn the parameters in every potentials and latent nodes X by maximizing the likelihood in Eq. ( <ref type="formula" target="#formula_10">7</ref>), which can be written into two parts:</p><formula xml:id="formula_11">L GP-LCRF = t ψ(A t , H t ) i ψ(A t , o t i )ψ(o t i , L t i ) (i,j) ψ(o t i , o t j )ψ(A t , A t-1 ) i ψ(o t i , o t-1 i )   × t i φ(x t i , h t i )φ(x t i , x t-1 i )</formula><p>The first pair of parentheses contains the CRF terms, with parameters denoted by Θ CRF . (They are similar to the terms in ATCRF.) The second pair of parentheses contains all terms related to latent nodes in GP-LCRF with parameters including K, α, γ, β, denoted by Θ latent . Note that Θ CRF and Θ latent are two disjoint sets. Therefore, learning can be decomposed into two independent problems: 1) learning Θ CRF by using the cutting-plane method in the structural learning for SVM <ref type="bibr" target="#b14">[15]</ref>, same as <ref type="bibr" target="#b19">[20]</ref>; 2) learning Θ latent by minimizing the negative log-likelihood, given by:</p><formula xml:id="formula_12">-ln P ({x i }, α, γ, β|{h i }) = -ln P ({h i }|{x i }, α, γ, β)P ({x i })P (α, γ, β) = D 2 ln |K| + 1 2 D k=1 H T k K -1 H k + 1 2 N i=1 ||x i || 2 + ln αβγ</formula><p>where the priors on the unknowns are: P (x) = N (0, I) and P (α, β, γ) ∝ α -1 β -1 γ -1 . We use numerical optimization method L-BFGS <ref type="bibr" target="#b31">[32]</ref> to minimize it. Algorithms Anticipated sub-activities Anticipated object affordances Anticipated traj. micro-P/R@1 macro-F1@1 Pre@3 micro-P/R@1 macro-F1@1 Pre@3 MHD@1 (cm) Chance 10.0±0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Inference for Anticipation</head><p>Given the observed segment t, we predict the next future segment t + 1 in the following way: We first sample possible object trajectories, represented in locations L t+1 . Then we sample human configurations H t+1 and X t+1 . We now use the sampled L t+1 and H t+1 as observations and infer the most likely sub-activity labels A t+1 and object affordance labels O t+1 by maximizing the conditional likelihood in Eq. ( <ref type="formula" target="#formula_10">7</ref>). All the samples together form a distribution over the future possibilities and we use the one with maximum a posterior (MAP) as our final anticipation.</p><p>We now present how to sample H t+1 and X t+1 in particular. (Sampling other terms is similar as in <ref type="bibr" target="#b19">[20]</ref>.) Given object locations, we generate a human motion of either moving or reaching an object. In both cases, the hand trajectory is given and the problem is formulated as: Given a target hand location * , compute the most likely human configurations where both x and h are unknown. A good pose should reach to the target as well as being reasonable which can be measured by the likelihood from GPLVM, L(x, h) in Eq. ( <ref type="formula">4</ref>). Hence, we define the objective function as:</p><formula xml:id="formula_13">arg min x,h -L(x, h) + λ|| * -(h)|| 2 (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>where λ is the penalty of the new pose deviating from the target. In our implementation, we start with a simple IK solution h 0 , and use the inverse mapping function g(h) = x (given by GPLVM with back constraints <ref type="bibr" target="#b26">[27]</ref>) to compute its corresponding x 0 . In this way, the first term in Eq. ( <ref type="formula">4</ref>) is always zero and can be neglected. So the new objective becomes a function of h only:</p><formula xml:id="formula_15">arg min h D 2 ln σ 2 (g(h)) + 1 2 ||g(h)|| 2 + λ|| * -(h)|| 2<label>(9)</label></formula><p>We then use L-BFGS to optimize it.</p><p>VII. EXPERIMENTS Data. We test our model on the Cornell Activity Dataset-120 (CAD-120), same as used in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19]</ref>. It contains 120 3D videos of four different subjects performing 10 high-level activities, where each high-level activity was performed three times with different objects. It contains a total of 61,585 total 3D video frames. The dataset is labeled with both subactivity and object affordance labels. The sub-activity labels are: {reaching, moving, pouring, eating, drinking, opening, placing, closing, scrubbing, null} and the affordance labels are: {reachable, movable, pourable, pour-to, containable, drinkable, openable, placeable, closable, scrubbable, scrubber, stationary}. Baselines. We compare against the following baselines: 1) Chance. Labels are chosen at random. 2) ATCRF-KGS <ref type="bibr" target="#b19">[20]</ref>. ATCRF with fixed temporal structure.</p><p>3) ATCRF <ref type="bibr" target="#b18">[19]</ref>. ATCRF with sampled temporal structures. 4) HighDim-LCRF. In this method, we do not compress the human configuration into a low-dimensional representation but directly model human dynamics in the high-dimensional space. We replace φ(x t i , h t i ) with a Gaussian based on the distance between h t i to its nearest neighbor h * in the training data. For an anticipated frame, we use inverse kinematics to generate a new pose that is closest to the target trajectory (without considering its GPLVM likelihood). We also change φ</p><formula xml:id="formula_16">(x t-1 i , x t i ) to φ(h t-1 i , h t i ) ∼ N (||h t-1 i</formula><p>-h t i || 2 ; 0, 1). 5) PPCA-LCRF. We use probabilistic principal component analysis (PPCA) instead of GPLVM for dimensionality reduction of human configurations. PPCA only learns a linear mapping and do not impose any prior on the latent space and the mapping. We verify through experiments that it does not model low-dimensional human dynamics well and thus is outperformed by our GP-LCRF model. Evaluation. We train our model on activities performed by three subjects and test on activities of a new subject. We report the results obtained by 4-fold cross validation and evaluated by the following metrics (same are used in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19]</ref>): 1) Labeling Metrics (on top#1 prediction). For anticipated subactivity and affordance labels, we compute the overall micro accuracy (P/R) and macro F1 score. Micro precision/recall is equal to the percentage of correctly classified labels. Macro precision and recall are averaged over all classes. 2) Pre@3. In practice a robot should plan for multiple future activity outcomes. Therefore, we measure the accuracy of the anticipation task for the top three predictions of the future. If the actual label matches one of the top three predictions, then it counts towards positive.</p><p>3) Trajectory Metric (on top#1 prediction). For anticipated human trajectories, we compute the modified Hausdorff distance (MHD) to the true trajectories. MHD finds the best local point correspondence of the two trajectories over a small temporal window to compute distance between those points. The distance is normalized by the length of the trajectory.</p><p>Table <ref type="table" target="#tab_1">I</ref> shows the frame-level metrics for anticipating subactivity and object affordance labels for 3 seconds in the future on the CAD-120 dataset. We can see that our proposed GP-LCRF outperforms all the baseline algorithms and achieves a consistent increase across all metrics. Especially as our GP-LCRF aims to model human configurations better, we can see that the anticipated human trajectory error is reduced from 30.2 cm to 26.7 cm which is a 11.6% improvement and has a p-value of 0.0107 indicating the difference is statistically very significant. We now inspect the results in detail from the following aspects:</p><p>The importance of dimensionality reduction. Table <ref type="table" target="#tab_1">I</ref> shows that when not using any dimensionality reduction, HighDim-LCRF performs even worse than ATCRF even though it tries to model the human temporal dynamics. This is because that in the high-dimensional space, φ(h t-1 , h t ) can be noisy and over-fitted, thus modeling it actually hurts the performance. On the other hand, with dimensionality reduction, PPCA-LCRF outperforms HighDim-LCRF, however it only achieves comparable results as ATCRF. This shows that the quality of the dimensionality reduction is quite important. Figure <ref type="figure" target="#fig_2">5</ref> illustrates a learned mapping of human configurations. Although both mapped to a 2D space, compared to GPLCRF in Fig. <ref type="figure">3</ref>, PPCA learns a flat mapping and does not distinguish different motions well enough. For instance, the motions in the activity of 'taking medicine' (in magenta) and 'microwaving food' (in green) are very different, however they are mapped to an overlapped area using PPCA in Fig. <ref type="figure" target="#fig_2">5</ref>. As a result, the effect of the dimensionality reduction in PPCA-LCRF is not as significant as our GP-LCRF.</p><p>Sensitivity of the results to the degree of dimensionality reduction. We investigate the performance of GP-LCRF with different dimensions of the latent space, from 1-D to 5-D in Fig. <ref type="figure" target="#fig_3">6</ref>, in terms of the trajectory distance error. We can see that under various learning conditions (where the anticipated segment is observed in different percentages), GP-LCRF with latent dimensions of two to five all give similar performance. We evaluate the performance under different conditions where the percentage of the future segment observed is 0%, 10%, 50% and 80%, i.e., the task is to anticipate is the rest of 100%, 90%, 50% and 20% of that segment respectively.</p><p>Dimensions of one has an obvious performance drop but is still better than ATCRF. However, with the observation's percentage increase to 80%, the gap diminishes as the anticipation problem becomes easier. Evaluations with the labeling metrics share similar trends. Hence, this shows that our GP-LCRF is very robust to the choices of the latent dimensions. The impact of the observation time. The first plot in Fig. <ref type="figure" target="#fig_1">4</ref> shows how the trajectory distance error, averaged over all the moving sub-activities in the dataset, changes with the increase of the observed part (in percentage) in the segment to be anticipated. While all approaches achieve better predictions with increased observation time, our GP-LCRF consistently performs better than the others, especially in the range of 20% to 60%. Because this part, unlike the beginning where the evidence of human motions is too weak to be useful and unlike the near end where the evidence human-object interactions weighs more than humans alone, is where the momentum of human motions can be captured from the observation by our model (through the velocity and acceleration features described in Sec. V) and be fully utilized for anticipation. Results with change in the future anticipation time. The last two plots in Fig. <ref type="figure" target="#fig_1">4</ref> show the changes of Pre@3 with the anticipation time lengthened. The longer the anticipation time, the harder the task gets and thus the performances of all approaches decrease. However, the improvement of our GP-LCRF against ATCRF grows from 1.7% to 2.2% for sub-activity anticipation and from 1.6% to 2.1% for object affordance anticipation. This demonstrates the potential of modeling human kinematics well in long-term anticipations.</p><p>How does modeling human dynamics improve anticipated trajectories? In addition to the quantitative results in Fig. <ref type="figure" target="#fig_1">4</ref>-(a), we also sample some qualitative results showing the topranked predicted trajectories in Fig. <ref type="figure" target="#fig_4">7</ref> using ATCRF (top) and using our GP-LCRF (bottom). In each image, we illustrate the predicted hand trajectories in blue dots, the ground-truth trajectories in green dots and human skeletons in red. We performed an ablative analysis and we now discuss some failures in the original ATCRF but are avoided by our GP-LCRF, arranged in three major categories: 1) Unrealistic skeletons leading to impossible trajectories:</p><p>In the first two cases/columns, the trajectories sampled by ATCRF are both not reachable (without making any effort such as bending over or leaning forward). As ATCRF does not consider any human kinematics and it simply changes the hand location to match the trajectory, the forearms in these two cases are stretched out. The features computed from these false human skeletons are erroneous and thus wrong trajectories are picked out. Our GP-LCRF, however, generates kinematically-plausible skeletons (because of availability of high-dimensional configurations in the model) so that the outof-reach trajectories will have high penalty in the likelihood L(x, h) and out-ranked by those reachable ones.</p><p>2) Unnatural poses leading to unlikely trajectories: In other cases, such as the third column in Fig. <ref type="figure" target="#fig_4">7</ref> where the subject picked up a rag on the table along the green dots to clean the microwave, both trajectories are physically possible but the top one requires raising the right hand to cross the left hand making a very unnatural pose. Because GP-LCRF learns the distribution of human poses from the training data, it assigns a low probability to uncommon poses such as the top one and prefers the bottom poses and the trajectory instead.</p><p>3) Not modeling motions leading to discontinuous trajectories:</p><p>How human body moved in the past gives such a strong cue that sometimes we can have decent anticipated trajectories purely based on the continuity and smoothness of human motions. For instance, the two subjects are lifting the box (4th column) and reaching towards the microwave door (last column). While our GP-LCRF chooses trajectories matching the moving directions best, ATCRF which does not model human temporal relations (i.e., no edges between H t-1 and H t ) produces trajectories with sudden changes in the direction.</p><p>Runtime. On a 16-core 2.7GHz CPU, our code takes 11.2 seconds to anticipate 10 seconds in the future, thus achieving near real-time (1.12X) performance. VIII. CONCLUSION AND DISCUSSION In this paper, we proposed a new model, GP-LCRF, in order to compactly model human configurations and dynamics in the task of anticipating human activities. The key idea is to have access to high-dimensional representations of humans for generating detailed and realistic human poses in the future, and meanwhile to have a compact low-dimensional representation for the ease of modeling and learning the context among humans, objects and the activity. Bearing this in mind, our GP-LCRF models the low-dimensional representations as latent nodes through a Gaussian process, and models the relations between human and other entities in a CRF. We tested our model on 120 RGB-D videos of different activities and it outperformed the state-of-the-art results consistently, and reduced the predicted human trajectory error by 11.6%.</p><p>This improvement could make a significant difference to robots working in the presence of humans. With more accurate human trajectory prediction, robots can plan more relevant actions and paths <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10]</ref>. Furthermore, with real-time anticipation, our work can be used for human-robot interaction, such as to improve the efficiency of collaborative tasks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b8">9]</ref>, or to avoid intrusion/collision during navigation <ref type="bibr" target="#b16">[17]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: Graphical representations of the original ATCRF<ref type="bibr" target="#b19">[20]</ref> and our GP-LCRF. Our model adds latent low-dimensional nodes X to the model, which are related to the original high-dimensional human configuration nodes H through Gaussian Process latent variable model with parameters α, β, γ. Shaded nodes indicate observations. In both models, temporal segment t is given for anticipation with observed human poses H t and object locations L t , and the goal is to infer the next segment t + 1 where nothing is observed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: Plots showing (from left to right): a) how the trajectory distance error changes with the observed percentage in the segment to anticipate increases from 0% to 100%; b) The Pre@3 of the anticipated sub-activity labels as a function the length of future prediction time in seconds; c) The Pre@3 of the anticipated object affordance labels as a function of the length of future prediction time in seconds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: The learned mapping using PPCA. The colored points corresponding to the activities in Fig. 3.</figDesc><graphic coords="7,175.55,409.63,123.43,71.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Fig.6: The trajectory distance error of GP-LCRF with different dimensions of latent space (from 1D to 5D, shown in the parentheses). We evaluate the performance under different conditions where the percentage of the future segment observed is 0%, 10%, 50% and 80%, i.e., the task is to anticipate is the rest of 100%, 90%, 50% and 20% of that segment respectively.</figDesc><graphic coords="7,175.55,409.63,123.43,71.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Top-ranked trajectories predicted by ATCRF (top) and our GP-LCRF (bottom) for different activities. In each image, the ground-truth trajectory is shown in green dots, predicted trajectory in blue, and the anticipated human skeletons in red in the order of from dark to bright.</figDesc><graphic coords="8,60.05,142.79,97.66,73.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Anticipation Results, computed over 3 seconds in the future averaged by 4-fold cross validation. The first six columns are in percentage and a higher value is better. The last column is in centimeters and a lower value is better.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Frames of a video are grouped into temporal segments, and each segment spans a set of contiguous frames, during which the sub-activity and object affordance labels do not change.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Hema Koppula</rs> and <rs type="person">Ozan Sener</rs> for useful discussions. This work was supported by <rs type="funder">ARO</rs> award <rs type="grantNumber">W911NF-12-1-0267</rs>, <rs type="funder">ONR</rs> award <rs type="grantNumber">N00014-14-1-0156</rs> and <rs type="grantName">NSF Career Award</rs> to Saxena.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dyzxg9u">
					<idno type="grant-number">W911NF-12-1-0267</idno>
				</org>
				<org type="funding" xml:id="_ups28Bj">
					<idno type="grant-number">N00014-14-1-0156</idno>
					<orgName type="grant-name">NSF Career Award</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Human-humanoid joint haptic table carrying task with height stabilization using vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Agravante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cherubini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bussy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kheddar</surname></persName>
		</author>
		<editor>IROS</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contextually guided semantic labeling and search for 3d point clouds</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="34" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning motion patterns of people for compliant robot motion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bennewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cielniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="48" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human motion reconstruction and synthesis of human skills</title>
		<author>
			<persName><forename type="first">E</forename><surname>Demircan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Besier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Khatib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Robot Kinematics: Motion in Man and Machine</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rank priors for continuous non-linear dimensionality reduction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What makes a chair a chair</title>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Style-based inverse kinematics</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Grochow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoran</forename><surname>Popovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="522" to="531" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From 3d scene geometry to human workspace</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic human action prediction and wait-sensitive planning for responsive human-robot collaboration</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HUMANOIDS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning trajectory preferences for manipulators via iterative improvement</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wojcik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hallucinating humans for learning robotic placement of objects</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISER</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Infinite latent conditional random fields for modeling environments through humans</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning object arrangements in 3d scenes using human context</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hallucinated humans as the hidden context for labeling 3d scenes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cutting-plane training of structural svms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cn</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="59" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceiving, learning, and exploiting object affordances for autonomous pile manipulation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Venkatraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stentz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial representation and reasoning for human-robot collaboration</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bugajska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fransen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Perzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Trafton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal structure from rgb-d videos for human activity detection and anticipation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Anticipating human activities using object affordances for reactive robotic response</title>
		<author>
			<persName><forename type="first">H</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Anticipatory planning for humanrobot teams</title>
		<author>
			<persName><forename type="first">H</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISER</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning human activities and object affordances from rgb-d videos</title>
		<author>
			<persName><forename type="first">H</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="951" to="970" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature-based prediction of trajectories for socially compliant navigation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kuderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sprunk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Incremental learning of full body motion primitives and their sequencing through human motion observation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="330" to="345" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gaussian process latent variable models for visualisation of high dimensional data</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Local distance preservation in the gp-lvm through back constraints</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quiñonero-Candela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time planning for parameterized human motion</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH/Eurographics Symposium on Computer Animation</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tell me dave: Context-sensitive grounding of natural language to mobile manipulation instructions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The joint manifold model for semi-supervised multi-valued regression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Navaratnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Human-robot cross-training: Computational formulation, modeling and evaluation of a human team training strategy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nikolaidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<editor>HRI</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Numerical optimization, series in operations research and financial engineering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hidden conditional random fields</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1848" to="1852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Synthesizing physically realistic human motion in low-dimensional, behaviorspecific spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Safonova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="514" to="521" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning depth from single monocular images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic discovery of meaningful object parts with latent crfs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schnitzspan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Implicit probabilistic models of human motion for synthesis and tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sidenbladh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3d people tracking with gaussian process dynamical models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName><forename type="first">J M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="298" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hidden conditional random fields for gesture recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Demirdjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optimization of temporal dynamics for adaptive human-robot interaction in assembly manufacturing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nikolaidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hierarchical semantic labeling for task-relevant rgb-d perception</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning probabilistic non-linear latent variable models for tracking complex activities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Planningbased prediction for pedestrians</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<editor>IROS</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
