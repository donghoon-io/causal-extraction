<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Context on a Humanoid Robot using Incremental Latent Dirichlet Allocation</title>
				<funder ref="#_keUkCBg">
					<orgName type="full">Marie Curie International Outgoing</orgName>
				</funder>
				<funder ref="#_bbWQZAu">
					<orgName type="full">Scientific and Technological Research Council of Turkey (T ÜB İTAK)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hande</forename><forename type="middle">C</forename><surname>¸elikkanat</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Güner</forename><surname>Orhan</surname></persName>
							<email>guner.orhan@ceng.metu.edu.tr</email>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Pugeault</surname></persName>
							<email>n.pugeault@exeter.ac.uk</email>
						</author>
						<author>
							<persName><roleName>Erol S ¸ahin</roleName><forename type="first">Frank</forename><surname>Guerin</surname></persName>
							<email>f.guerin@abdn.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Sinan</forename><surname>Kalkan</surname></persName>
							<email>skalkan@ceng.metu.edu.tr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engi- neering</orgName>
								<orgName type="laboratory">Sinan Kalkan are with KOVAN Research Lab</orgName>
								<orgName type="institution">Hande C ¸elikkanat</orgName>
								<address>
									<addrLine>Güner Orhan</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="laboratory">KOVAN Research Lab</orgName>
								<orgName type="institution">Middle East Technical University</orgName>
								<address>
									<settlement>Ankara</settlement>
									<country key="TR">TURKEY</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">with Robotics Institute</orgName>
								<orgName type="institution">Middle East Technical University</orgName>
								<address>
									<settlement>Ankara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">College of Engineering, Mathematics and Physical Sciences</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Exeter</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Univer- sity of Aberdeen</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Context on a Humanoid Robot using Incremental Latent Dirichlet Allocation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Context</term>
					<term>Situated Concepts</term>
					<term>Latent Dirichlet Allocation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this article, we formalize and model context in terms of a set of concepts grounded in the sensorimotor interactions of a robot. The concepts are modeled as a web using Markov Random Field, inspired from the concept web hypothesis for representing concepts in humans. On this concept web, we treat context as a latent variable of Latent Dirichlet Allocation (LDA), which is a widely-used method in computational linguistics for modeling topics in texts. We extend the standard LDA method in order to make it incremental so that (i) it does not re-learn everything from scratch given new interactions (i.e., it is online) and (ii) it can discover and add a new context into its model when necessary. We demonstrate on the iCub platform that, partly owing to modeling context on top of the concept web, our approach is adaptive, online and robust: It is adaptive and online since it can learn and discover a new context from new interactions. It is robust since it is not affected by irrelevant stimuli and it can discover contexts after a few interactions only. Moreover, we show how to use the context learned in such a model for two important tasks: object recognition and planning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>We tackle the problem of using contextual information to improve the performance of a cognitive robot, specifically in perception and planning. We define context as the totality of the information characterizing the situation of a cognitive system; e.g., it can include objects, persons, places, and temporally extended information related to ongoing tasks, but also information not directly related to these tasks <ref type="bibr" target="#b0">[1]</ref>.</p><p>There is ample evidence that natural cognitive systems modulate their response to stimuli depending on a wide range of other, seemingly irrelevant stimuli (context). Yeh and Barsalou <ref type="bibr" target="#b1">[2]</ref> demonstrated in a series of experiments that human subjects perform better at a variety of cognitive tasks when taking context into account. This is because context can promote relevant information and behaviors, while suppressing irrelevant ones, based on statistical likelihood of various objects and behaviors in a certain setting. A concept such as a chair does not exist in isolation, but is associated in memory with other concepts that also occurred in the concrete situations where the concept was previously encountered by the system; e.g., the chair's location, office or living room, but also the actions performed with the chair, such as reclining. These connections between concepts in memory then allow the system, when detecting a concept, to draw inferences about connected concepts; this is illustrated in Figure <ref type="figure" target="#fig_1">1</ref>. The activation of a 'chair' concept promotes related objects such as 'table' and 'lamp', and draws inferences on their plausible position. Furthermore, a 'living room' concept will promote chair properties such as 'large' and 'soft', rather than 'small' and 'hard' (contrary to, e.g., a 'classroom' concept). Similarly, actions usually associated with the active concepts, such as 'sitting' in our example, are promoted, whereas unlikely actions ('lifting') are suppressed. In sum, what forms context depends on the concept of interest, and consists of all other concepts present at the same time. Through experience, a cognitive system forms an interconnected network of related concepts and situations that allows efficient filtering of context and inference.</p><p>In this article, motivated by the concept-based nature of human cognition, we formulate context to be the set of active concepts in the scene, rather than relating it directly to raw sensorimotor data. For this, we employ a widely-used topic model in computational linguistics, called Latent Dirichlet Allocation (LDA), and apply it to the active concepts in the scene. For modeling the concepts, we use a concept-web model that we developed using Markov Random Fields in our previous work <ref type="bibr" target="#b2">[3]</ref>.   The concept web model that we developed in our previous study <ref type="bibr" target="#b2">[3]</ref>: A densely connected concept web connecting perception, action and language; however, there is no notion of context in this model. (c) We propose a system that learns in context the links between concepts and sensorimotor primitives, based on the statistics of its interactions in real-life environments. For clarity, only a few links and concepts are shown. [Sub-figures (a) and (c) adapted from <ref type="bibr" target="#b0">[1]</ref>]</p><p>We demonstrate how context can be learned and used by such a model for several tasks by a humanoid robot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Context in Cognitive Science and Robotics</head><p>It is a matter of consensus across fields that context processing is an essential part of embodied cognition (e.g., psychology <ref type="bibr" target="#b1">[2]</ref>, language <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, AI <ref type="bibr" target="#b6">[7]</ref>, robotics <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> and computer vision <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>). Schank and Abelson <ref type="bibr" target="#b6">[7]</ref> argued that reasoning about situations in daily life relies on "scripts" that inform reasoners about the prototypical features of these situations. A restaurant, for example, tends to come with a menu, dishes, a waiter, a chef, and so on. This work has gone on to influence today's formal ontologies. Probably the earliest research on context focused on linguistic phenomena, studying how the understanding of an expression (e.g., a personal pronoun like "it") is affected by the rest of the sentence or text <ref type="bibr" target="#b4">[5]</ref>. Later research applied these ideas to other aspects of communication, including speech (e.g., pitch accent) and body language (e.g., <ref type="bibr" target="#b11">[12]</ref>). Even more drastically, the notion of a context has been extended to all symbolic systems (e.g., <ref type="bibr" target="#b12">[13]</ref>). Perhaps most notably, McCarthy <ref type="bibr" target="#b8">[9]</ref> proposed the rectification of context in classical (logical) AI, arguing that Artificial Intelligence needs to put the notion of a context centre stage. In McCarthy's view, intelligent machines "must construct or choose a limited context containing a suitable theory whose predicates and functions connect to the machine's inputs and outputs in an appropriate way" <ref type="bibr" target="#b13">[14]</ref>. This work gave rise to a wave of theoretical work focusing on issues like the problem of "lifting" information about one knowledge base to another. Work in all these traditions continues to inspire Cognitive Science and AI. But times have changed: the rise of embodied cognition theories in the 90's, for instance, has offered a different perspective on context, based on a perceptual and action-based rather than symbolic approach <ref type="bibr" target="#b7">[8]</ref>. This perceptual perspective is particularly relevant for robotics, where contexts typically need to be acquired from perception (i.e., they cannot be programmed in advance). <ref type="bibr">Barsalou</ref>, for example, has advocated the necessity for concepts to be situated <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b14">[15]</ref>; in other words, for an abstract concept to be related to concrete contexts. Coventry et al. <ref type="bibr" target="#b5">[6]</ref> studied the difference between geometric and functional contexts in the use of spatial prepositions ("over" vs. "above") and of linguistic quantifiers ("few" vs. "many" vs. "several").</p><p>One striking example concerns work on affordances. Until some years ago, behavioral studies on affordances tended to highlight the fact that affordances are automatically activated, independently from the kind of task and context. This was shown through compatibility effects in which, for example, size resulted as a relevant dimension even if the task did not require subjects to judge objects on the basis of their size but, instead, of their category (e.g., <ref type="bibr" target="#b15">[16]</ref>). Recent evidence has questioned this view of affordances, showing that the activation of affordances is modulated by the physical and by the social context. A variety of studies have shown that the embedding in a context given by a specific scene (e.g., <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>) or by the presence of other objects influences affordances activation (e.g., <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>).</p><p>Robotics has achieved significant success in terms of both theory and applications in the past five decades <ref type="bibr" target="#b22">[23]</ref>; however, research involving context has focused on the environmental aspect only, e.g., in scene interpretation <ref type="bibr" target="#b9">[10]</ref>, urban search for rescue tasks <ref type="bibr" target="#b23">[24]</ref>, home security <ref type="bibr" target="#b24">[25]</ref> and elderly people's living environments <ref type="bibr" target="#b25">[26]</ref>, object recognition in daily activities <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, and trying to fulfill possibly incomplete natural language instructions of humans <ref type="bibr" target="#b28">[29]</ref>.</p><p>Of all these works, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref> stand out for attempting more explicit utilization of contextual information. In an attempt to provide a common representation for multiple agents sharing knowledge, Padovitz et al. <ref type="bibr" target="#b29">[30]</ref> and Mastrogiovanni et al. <ref type="bibr" target="#b30">[31]</ref> define context as explicit and crisp conjunction rules of a priori known predicates for each context. However, these representations are therefore both overly restrictive to conjunctive phrases, and also naive in the sense that the programmer is assumed to know how to encode each context rule in conjunctions. The assumption of existing and perfectly known conjunctive rules for each context is indeed a strong one, and would be over-sensitive, for instance, to the failures in the sensing of any one of the necessary premises, or to the emergence of unpredicted contexts. Indeed, the main emphasis in both works is more on providing a common ground for facilitating information sharing among multiple agents, rather than elaborating on contextual representation. Anand et al. <ref type="bibr" target="#b26">[27]</ref> define and use a more restricted notion of spatial context, limited to the spatial relationships between canonical placements of objects in the environment: A computer is usually found on-top-of a table, and this information can be used facilitating object search and labeling (see <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> for similar behavior in humans). On the other hand, Misra et al. <ref type="bibr" target="#b28">[29]</ref> treat context as multiple-choice values of the states of known objects in the environment (i.e., microwave door is closed or open), used afterwards for completing missing information in natural-language commands of humans. Given an incomplete command from a naive human partner, the robot can therefore use this contextual background to complete missing implied links to achieve these commands. An example might be, when commanded to "heap up the milk", reasoning that "the milk is currently in the carton, but it needs to be on the oven, and the oven must be in the on state."</p><p>In computer vision, the notion of context has grown in prominence over the last decade, both explicitly and implicitly <ref type="bibr" target="#b9">[10]</ref>. Explicitly, the study of visual gist <ref type="bibr" target="#b31">[32]</ref> showed that holistic encodings of the visual input could carry a large amount of information, allowing scene identification <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, urban scene detection <ref type="bibr" target="#b33">[34]</ref>, and autonomous navigation <ref type="bibr" target="#b34">[35]</ref>, as well as action recognition <ref type="bibr" target="#b35">[36]</ref>, object categorization <ref type="bibr" target="#b36">[37]</ref>, and detection <ref type="bibr" target="#b10">[11]</ref>. Implicitly, the now popular data-driven, machine learning-based approach to vision led to algorithms that efficiently extract all predictive information from the visual data, making heavy use of context to reach high performance (see <ref type="bibr" target="#b37">[38]</ref> for a criticism).</p><p>A promising approach for developing an explicit model of context seems to be Latent Dirichlet Allocation (LDA), a hidden topic model developed for categorizing documents of large text corpora <ref type="bibr" target="#b38">[39]</ref>. As a robust, unsupervised Bayesian method, it has been utilized as well in a variety of applications, ranging from fraud detection <ref type="bibr" target="#b39">[40]</ref> to the identification of functional regulatory networks of miRNA-mRNAs <ref type="bibr" target="#b40">[41]</ref>. Since the method provides the statistical tools for discovering hidden topics in unsupervised data, we propose that it can also be used for modeling context. In fact, ours is not the first attempt to use LDA formulation in robotics: It has been utilized successfully for object categorization from multi-modal sensory data <ref type="bibr" target="#b41">[42]</ref>- <ref type="bibr" target="#b43">[44]</ref>, and for autonomous drive annotation <ref type="bibr" target="#b44">[45]</ref>. However, our work is the first to use LDA for modeling context in robotics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. This Study</head><p>We see in existing works various piecemeal efforts to tackle particular facets of context in specific domains. In contrast, following the intuitions of <ref type="bibr" target="#b1">[2]</ref>, we argue that a principled approach is needed to learn, represent and process context in a developing cognitive system.</p><p>We study how we can equip a robot with the ability of detecting and using context, e.g., in object recognition and planning as proof of concept. The novelty and contributions of our approach can be summarized as:</p><p>• Formalization of context on a robot using Latent Dirichlet Allocation (LDA). To the best of our knowledge, this is the first time that context is tackled systematically, as a separate entity but also in direct relation with other conceptual entities, in a robotics scenario. In contrast to the attempts of Anand et al. <ref type="bibr" target="#b26">[27]</ref> and Misra et al. <ref type="bibr" target="#b28">[29]</ref> for using contextual information, which do not introduce a general model of context, resorting to defining it in terms of predefined geometric relations or objectpart states, we formalize context to develop an adaptive system in which contextual information can be extracted, represented, and utilized explicitly. • We provide an incremental extension of LDA so that (i) it does not re-learn everything from scratch given new sensorimotor interactions (i.e., it is online) and (ii) it can discover and add a new context into its representations when necessary. • Finally, motivated by our findings of the computational advantages in <ref type="bibr" target="#b2">[3]</ref>, we propose applying LDA on a concept web representation of the scene, instead of its raw features directly. We subsequently demonstrate how learning context from high-level concepts, instead of raw features, is easier and achieves higher performance. The current article extends an earlier version of our work <ref type="bibr" target="#b0">[1]</ref>, where preliminary results on integrating context were presented using the standard LDA with an ad hoc concept web. The current article differs in the following aspects: (i) The LDA is extended in order to make it online and incremental. (ii) The ad hoc concept web is replaced with a formally developed concept-web modeled using Markov Random Fields. (iii) A more extensive evaluation of the system is presented.</p><p>The current article uses the concept web model that we developed in <ref type="bibr" target="#b2">[3]</ref>. This previous work introduced a concept web model and showed why it is important and useful. However, the current work goes beyond that and integrates context on top of this model, to demonstrate how context can be learned and used by a robot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. EXPERIMENTAL SETUP</head><p>We conduct our experiments using the iCub humanoid robot platform <ref type="bibr" target="#b45">[46]</ref> (Figure <ref type="figure" target="#fig_2">2</ref>). iCub has tactile sensors in each fingertip to detect the degree of grasping of an object and collect haptic information about the its hardness. Joint encoder values are used for collecting the proprioceptive information about the hand and the arm status. A Kinect device is used to get 3D visual information from the environment. iCub also has an external microphone to record the sound of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Object Set</head><p>We have an object set O of 60 objects, arbitrarily divided into a training set of 45 and a test set of 15 objects. The training objects are labeled via supervision as belonging to one of the 6 noun categories, {box, ball, cylinder, cup, plate, tool} (Figure <ref type="figure" target="#fig_3">3</ref>), and one of the two adjectives in each 5 dichotomic adjective pair, {hard × soft, noisy × silent, tall × short, thin × thick, round × edgy} (Figure <ref type="figure" target="#fig_4">4</ref>). The mapping between nouns and adjectives is not 1-to-1; e.g., a box can be soft or hard, silent or noisy etc. Table I depicts these co-occurrences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Behaviors</head><p>We have a repertoire of 13 behaviors, {grasp, push left, push right, push forward, push backward, move left, move right, move forward, move backward, drop, throw, knock down, shake}, which are performed via hard-coded scripts on perceived objects with variable positions and poses. To ensure realism, some objects are (assumed to be) fragile and certain behaviors are not applied on them: We prevent iCub from dropping, shaking, throwing, knocking and pushing plates and cups. We also refrain from pushing balls, since they tend to roll down and disappear from the table. Table <ref type="table" target="#tab_3">II</ref> shows the allowed behaviors for each noun category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Features and Data Collection</head><p>iCub interacts with each object o ∈ O as follows:</p><p>1. The object o is placed on the table to an arbitrary location. 2. iCub "looks" at the object (i.e., takes a 3D snapshot using the Kinect sensor) and extracts the initial visual features e v . 3. For each allowable action on the object (Table <ref type="table" target="#tab_3">II</ref> The first 6 visual features are basic position information and three dimensional properties of the object, and the next 40 features are the zenith and azimuth normal vectors of each point on the object. In addition to the normal information, we use histogram of shape index values. Shape index <ref type="bibr" target="#b46">[47]</ref> is essentially a representation of the local surface type, calculated from the maximum and minimum principal curvatures (Q 1 , Q 2 , respectively) of the point as follows: Q1+Q2 Q1-Q2 .   The following 13 are auditory features (e a ) used to determine whether an object produces sound when interacted with. We use MFCC (Mel-Frequency Cepstrum Coefficients) on the raw audio data, yielding a set of 13-feature vectors. As features, we use the differences between the maximum and minimum values of each vector.</p><p>Haptic and proprioceptive features (e h and e p ) are obtained from the index finger of iCub only. They are collected through the grasping action, and encode the difference between initial and final sensor readings for haptic/proprioceptive data, the minimum and maximum readings, and also the mean, variance, and the standard deviation values.</p><p>The concatenation of these features (e v , e a , e h , e p ) is called an entity feature vector and is denoted by e. Each object is described by an entity feature vector. For describing behaviors, we use effect feature vectors, denoted by f, capturing the effect of a behavior on an object. They give the difference between the visual feature of the object before and after a behavior is applied, obtained by f = e v -e v . See Figure <ref type="figure" target="#fig_5">5</ref> for an illustration.  </p><formula xml:id="formula_0">) Backward) Box A A A A A A A Ball NA A A A A A A Cylinder A A A A A A A Cup NA A NA A NA NA NA Tool A A A A A A A Plate NA A NA A NA NA NA</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Contextual Setting</head><p>Our experimental setting is comprised of three contexts, Kitchen, Playroom, and Workshop. Some concepts in our framework occur in certain contexts, such as plates and cups existing in a Kitchen, balls and boxes occurring in a Playroom, as so on. Notice that this tendency is mostly a characteristic of noun concepts, which have more clear-cut divisions into contexts. On the contrary,  ev and e v are the initial and final visual features of the object before and after a behavior's execution. f = e v -ev. some concepts are so general that they do not have such clear-cut divisions. This is a characteristic of most adjective concepts in our setting: Adjectives such a round or tall are so generic that they are not limited to certain contexts. Table <ref type="table" target="#tab_6">IV</ref> summarizes the prevalent concepts of the three contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. A CONCEPT WEB USING MARKOV RANDOM FIELD</head><p>In our system, context is formalized over a set of concepts that are extracted from the scene, and represented in a densely-connected web structure, called the concept web <ref type="bibr" target="#b2">[3]</ref>. Since this web is central to our model, before continuing with the exact formalization and use of context in the system, we briefly describe the extraction of relevant concepts from a scene, and the formation of the concept web. We describe a framework consisting of three kinds of concepts: Noun concepts N ={box, ball, cylinder, cup, plate, tool}, adjective concepts A ={hard × soft, noisy × silent, tall × short, thin × thick, round × edgy}, and verb concepts V = {grasp, push left, push right, push forward, push backward, move left, move right, move forward, move backward, drop, throw, knock down, shake}. <ref type="foot" target="#foot_0">1</ref> Before evaluating each scene in terms of its context, the robot views and possibly interacts with the objects, makes initial predictions about the concepts associated with the scene, and then builds a web of these concepts to make use of their related semantics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Reasoning with Individual Concepts</head><p>The initial task of the robot is to predict the individual concept(s) that are related to an object in its environment. This mapping of the world from raw features to a concept can be learned in a variety of manners, e.g., using Support Vector Machines, k-Nearest Neighbors, Neural Networks, etc. In this work, we adopt a prototype-based approach <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> following previous work <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b50">[51]</ref>; however, this choice is not central to the rest of the article; any method that provides a measure of similarity to a category from raw features is sufficient for this part. For a review of alternative representation schemes, the reader may for instance refer to <ref type="bibr" target="#b51">[52]</ref>- <ref type="bibr" target="#b53">[54]</ref>.</p><p>In our framework, we describe the noun (N), adjective (A) and verb (V) concepts in terms of their prototypes, which we learn from accordingly labeled interactions during the training phase (Figure <ref type="figure">6</ref>(a), see Appendix for more detail). These prototypes are learned from the entity feature vectors e and effect feature vectors f during training, and summarize which features are highly relevant for the concept (need to be strongly positive, strongly negative, or close to zero), and which are irrelevant for the concept. During execution, an incoming instance is compared with the concept taking into account the concept prototype: The Euclidean distance D(c, x) between the incoming feature vector x and the concept prototype of c is calculated by disregarding the irrelevant features of the concept:</p><formula xml:id="formula_1">D(c, x) = 1 |R c \ R c | i∈Rc\R c (x i -µ i c ) 2 ,<label>(1)</label></formula><p>where R c is the set of indices that are not relevant for concept c; x i is the i th dimension of x, and µ c is the prototype of concept c. The complete procedure of prototype extraction and concept assignment is provided in Appendix. Interested readers can also refer to <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. From Individual Concepts to a Densely Connected Web</head><p>In <ref type="bibr" target="#b2">[3]</ref>, we show how concepts can be extracted and represented in a densely connected web based on Markov Random Fields (MRF) <ref type="bibr" target="#b54">[55]</ref>, providing greater robustness of reasoning than considering concepts in isolation. For the sake of completeness, we describe the method here briefly. A Markov Random Field is an undirected graph of random variables, over which inference is often carried out by a minimization of a predefined energy function. In the energy function, the consistency of the categories (i.e., the nodes) with the input (by the "data term" in MRF) and the consistency between the categories (by the "smoothness term") are specified. By minimizing this energy function, an MRF finds the most likely categories for an input, satisfying also the regularization constraints specified in the smoothness term.</p><p>In our representation of the concept web as an MRF, the nodes correspond to concepts, and commonly cooccurring concepts are connected via edges. With C = N ∪ A ∪ V being the set of all concepts, the concept web W is defined as a graph, W = G(C, E), with each concept c ∈ C being a node in W , and edge ij ∈ E meaning that concepts c i and c j have co-occurred in the training set. In other words, the edges between the nodes (concepts) are learned from the training data, which is composed of observations of individual objects and behaviors executed on them.</p><p>What happens when a new observation arrives is depicted with a schematic representation in Figure <ref type="figure">7</ref>. The connections from the input to the nodes correspond to the data term (represented with ψ C , unary potentials), and the connections between the nodes model the smoothness term (represented with ψ K , clique potentials). The energy U (ω), of a given MRF configuration ω, is then:</p><formula xml:id="formula_2">U (ω) = U data (ω) + U smooth (ω) = c∈ω ψ c (c) + K ∈K ψ K (K , ω),<label>(2)</label></formula><p>where the first term, i.e., the data term, is a summation of the unary potentials for each active concept c in ω, and the second term is the smoothness term, as a summation of clique potential functions. The unary potential function denoted by ψ C is defined as:</p><formula xml:id="formula_3">ψ C (c) . . = D (c, x) ,<label>(3)</label></formula><p>with x being the instantaneous observation, D(c, x) its distance to concept c (Equation <ref type="formula" target="#formula_1">1</ref>). The potential function for cliques, denoted by ψ K , is defined as:</p><formula xml:id="formula_4">ψ K (K, ω) . . = V K (x K ) . . = xi∈x K |val(x i ) -E(x i |x K-i )|<label>(4</label></formula><p>) where V K (x K ) is defined as an (abused) shorthand notation for the potential of a clique node consisting of active variables x K , x i is the i th variable in the clique, x K-i are the variables in the clique excluding the i th variable, val(x i ) is the current value assignment of the variable x i , |.| is the absolute value function, E(.) is the expected value function, and E(x i |x K-i ) is the expected value of the i th variable given the values of the remaining variables in the clique.</p><p>The energy function in Equation 2 must be minimized to find the most likely configuration ω * = arg min ω (U (ω)). We use the Loopy Belief Propagation (LBP) algorithm <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref> designed specifically for cyclic MRFs, for this energy minimization. A schematic depiction of the complete system is presented in Figure <ref type="figure">6</ref>(b), showing the information flow from perception space through a feature-extraction mid-layer, as well as from language, and action spaces. The finalized concept web has all the relevant concepts in the active state (indicated with white color), and connected to their relevant counterparts in the three spaces. A sample concept web that is constructed is shown in Figure <ref type="figure" target="#fig_7">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MODELING CONTEXT USING INCREMENTAL LATENT DIRICHLET ALLOCATION</head><p>In our framework, context is linked to the set of concepts that the robot perceives from its immediate environment. We use Latent Dirichlet Allocation (LDA) to detect the latent (unobserved) context(s) of the scenes. Initially, the scene is represented as a concept web (Section III), which is then used as an input to contextual analysis. The detected contexts are in turn fed back to the concept web to guide its reasoning. In this section, we provide the details of these steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Latent Dirichlet Allocation (LDA)</head><p>Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b38">[39]</ref> is a method for modeling topics of documents in large text corpora. Assuming a document d ∈ D is a set of words {w 1 , • • • , w N } drawn from a fixed vocabulary (w i ∈ W, vocabulary size is |W|, |.| denotes set cardinality), LDA posits a finite mixture over a fixed set of topics</p><formula xml:id="formula_5">{z 1 , • • • , z k } (z t ∈ Z, |Z| = K is the topic count).</formula><p>Then, a document can be described by its probabilities of being related to each of these topics, P (z t |d i ). Meanwhile, a topic is modeled by its probability of producing each word in the vocabulary, P (w j |z t ). LDA tries to infer these document and topic probability distributions, given a corpus D.</p><p>Being a generative model, LDA assumes that the corpus had previously been generated by choosing a Dirichlet prior α, and a K × |W| matrix, called β, that contains the probabilities of each word given each topic, i.e., with entries β jk = P (w j |z t ). Furthermore, it assumes that every document d ∈ D had been generated by first choosing a probability distribution of topics for this document, θ ∼ Dir(α), followed by, for each word location n in the document, choosing a topic z n ∼ Discrete(θ), and eventually a word w n , given the chosen topic z n and the β matrix denoting P (w n |z n , β).</p><p>LDA effectively tries to estimate the unknown α and β parameters from the given corpus, through which it is possible to infer any other parameter. This problem, however, is infamously intractable <ref type="bibr" target="#b38">[39]</ref>. There are various solutions though, including a variational inference method <ref type="bibr" target="#b38">[39]</ref>, a collapsed Gibbs sampling solution <ref type="bibr" target="#b58">[59]</ref> and collapsed variational inference approach <ref type="bibr" target="#b59">[60]</ref>.</p><p>The strengths of LDA are two-fold: First, it is a generative model. There exists other powerful, nongenerative models for topic analysis (for instance, see <ref type="bibr" target="#b60">[61]</ref>), however, being a generative model, LDA can assign probabilities to documents that have not been seen before. Second, it allows non-strict memberships of words to topics: A word may be generated by multiple Algorithm 1 Batch Gibbs sampling algorithm <ref type="bibr" target="#b58">[59]</ref>. Algorithm formulation adapted from <ref type="bibr" target="#b61">[62]</ref>. <ref type="formula" target="#formula_7">5</ref>) end while topics, and according to which document it occurs in, considering the topic probability distribution of the document, a different topic might be assigned to the different occurrences of the word.</p><formula xml:id="formula_6">initialize z = [z 1 , • • • z N ] randomly from the set {1, 2, • • • K} while not converged do choose a word index j from {1, 2, • • • N } sample z j according to P (z j | z \j , w N ) (Equation</formula><p>Batch Gibbs Sampling Approach for Solving LDA Introduced by Griffiths and Steyvers <ref type="bibr" target="#b58">[59]</ref>, the Batch Gibbs Sampling Approach (Algorithm 1) is a "collapsed" method for solving the LDA problem, because it integrates out the Dirichlet parameters and instead directly samples the topic variables z = [z 1 , • • • , z N ] for every word position n ∈ {1, • • • , N } . The algorithm starts by randomly assigning z, and then until convergence samples the topic assignment z j for the word w j in document d, according to the instantaneous state:</p><formula xml:id="formula_7">P (z j | z \j , w N ) ∝ n wj zj ,\j + ξ n zj ,\j + |W|ξ × n d zj ,\j + α N d \j + Kα ,<label>(5)</label></formula><p>where (.) \j notation stands for all items excluding the currently considered index j, therefore letting z \j : the vector of all topics except z j , w N : the vector of all words, n wj zj ,\j : the number of times that word w j has been assigned to topic z j except at index j, n zj ,\j : the number of times that any word has been assigned to topic z j except at index j, n d zj ,\j : the number of times that any word in document d has been assigned to topic z j except at index j, N d \j : the total number of all words in document d except at index j, |W|: the size of the vocabulary set, and K: the topic count. The approach assumes symmetric Dirichlet priors α and ξ, i.e., that they are vectors with the same value in all entries. The α vs. ξ trade-off controls the compromise between having few topics per document, vs. having few topics per word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Modeling Contextual Information with LDA</head><p>We now describe how we model our robotics scenario within the Latent Dirichlet Allocation framework. The components of our system correspond to the specific LDA terms as follows:</p><p>1. Each scene the robot encounters is represented as an LDA document. In our concept web-based model, this scene/document is then a set of active concepts. 2. The sum of all the encountered scenes is analogous to the corpus D. </p><formula xml:id="formula_8">c act ∈ C = N ∪ A ∪ V) topic</formula><p>a 'context', either Kitchen, Playroom, or Workshop 3. Each active concept c act in this scene corresponds to a word w i in the document. 4. Finally, the "context"s that we are trying to discover correspond to the latent topics of LDA. Our aim is to associate each scene with the relevant contexts. Table V summarizes the correspondence between the LDA terms, and the notions in our robotics scenario. Also note that LDA works on the bag-of-words assumption that the order of the words in a document is not important, which is compatible with our scenario: Indeed, concepts exist or do not exist in a scene, there is no ordering between them. That is, the probabilities of the existence of concepts are not dependent on an order of appearance, in contrary to, for instance, certain natural language processing scenarios.</p><p>C. An Incremental and Online Version: Incremental-LDA Since a robot operates in a dynamic world, it needs to be able to discover newly emerging contexts with new interactions. To truly comply with developmental principles, the robot not only needs to estimate itself the ideal number of contexts, but also to validate its own prediction continuously and revise and update it if necessary; we cannot foresee this for it (for a very good discussion on what makes a system developmental, see <ref type="bibr" target="#b62">[63]</ref>).</p><p>One limitation of LDA is that it requires a fixed number of topics. This requirement is characteristic of the parametric approaches, where the parameters of the solution are defined a priori and do not change no matter how many training examples are encountered. Although they are very widely used and successful in general (among well-known examples are regression, Fisher's discriminant analysis, Bayesian graphical methods), the necessity of predefining parameters can be restrictive. In the case of latent feature models, different methods have been proposed for dealing with an unknown number of clusters, focusing especially on the Dirichlet process and Bayesian solutions <ref type="bibr" target="#b63">[64]</ref>- <ref type="bibr" target="#b65">[66]</ref>. Targeting specifically the LDA problem, Teh et al. <ref type="bibr" target="#b66">[67]</ref> proposed a Hierarchical Dirichlet Process framework which can start with infinitely many possible topics, and settle on the likeliest Algorithm 2 The proposed Incremental-LDA algorithm initialize context count K ← 1. for all encountered scenes do run K-Incremental Gibbs sampler with K while C low = ∅ do increment context count K ← K + 1 run K-Incremental Gibbs sampler with K end while output converged context assignments z N for the scene end for number of topics itself. Wang et al. <ref type="bibr" target="#b67">[68]</ref> developed an online solution for this hierarchical setting.</p><p>Since the previously proposed variations are either batch or parametrically dependent on the number of topics K, we enhance the original LDA methodology with a simple mechanism that allows both online learning, and dynamic updating of the ideal K value over time. This new variant, henceforth called Incremental-LDA, does not need the number of contexts to have been predefined, starting instead with the most general case of K = 1, and increasing the context count when necessary.</p><p>Incremental LDA Incremental-LDA (Algorithm 2) decides on K dynamically, starting the with most general case, K = 1, and incrementing the context count as necessary. For deciding when to increase K, we define and use C low , the set of words whose confidence values for contextual assignments are lower than a threshold value τ . If there exists such words with low confidences, i.e., C low = ∅, Incremental-LDA attempts to increase their confidences by incrementing the context count. <ref type="bibr" target="#b58">[59]</ref> is not suitable for use with Incremental-LDA, because it needs to start from scratch each time the context count K is incremented. The previous solution is forgotten completely, whereas parts of it would still be applicable. This is especially true for the parts of the previous solution that exhibited high enough confidence. Therefore we introduce K-Incremental Gibbs Sampling (Algorithm 3) as an incremental variant: When the context count is incremented to K, K-Incremental Gibbs Sampling resumes its search from the previously converged solution for K -1 contexts, conducting a local search in the close vicinity. This is done by retaining the previous assignments of the high-confidence terms, while initializing low-confidence terms (C low ) to the newest context id K. Effectively, the highly confident part of the solution is reused. Note that for escaping possible local minima, a high-confidence term can also be reassigned to the new context with a low probability δ 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K-Incremental Gibbs Sampling Standard batch Gibbs sampler proposed by Griffiths and Steyvers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Making Use of Context: Feeding the Contextual Information back to the Concept Web</head><p>Since the system does not employ an attentional mechanism, it focuses on each object in the scene one by one, identifying the concepts related to each one with a concept web. The set of all these active concepts for all objects is then used for deducing the context of the scene. After determining the context, the probabilities of concepts are updated with the conditional likelihood of concepts in that context:</p><formula xml:id="formula_9">P (c) * = σ × P (c) + (1 -σ) × P (c| χ ),<label>(6)</label></formula><p>where c ∈ C = N ∪ A ∪ V is a concept, P (c) is the MRF-decided probability of the concept c, χ is the context, P (c| χ ) is the probability of the concept given the context (decided by Incremental-LDA), and P (c) * is the updated value of the concept probability. The whole system, which consists of (1) reiteration of the object concept webs, (2) context deduction, and (3) probabilistic update of concept webs according to the context, is then repeated until the convergence of the individual concept webs and context analysis. See Figure <ref type="figure">6</ref>(b) for a schematic visualization.</p><p>σ in Equation <ref type="formula" target="#formula_9">6</ref>is responsible with regulating the strength of contextual feedback in our world, with σ = 0 corresponding to using only contextual information, and σ = 1 corresponding to pure concept web decision. An average log likelihood l is calculated over the test set as follows and depicted in Figure <ref type="figure">9</ref>:</p><formula xml:id="formula_10">l = 1 N |C n+ | N i=1 c∈C n+ logP (c|x n , σ),<label>(7)</label></formula><p>with N denoting the observation count, x n being the n th observation, C n+ with cardinality |C n+ | being the set of concepts related with the n th observation, and P (c|x n , σ) denoting the probability of obtaining the related concept c given observation x n , under the setting σ. The results estimate a reasonable interval between [0.4, 0.5]; from this interval, we select σ as 0.5. Note that the convergence of l for σ ≥ 0.7 corresponds to the contextual feedback being too weak to affect concept Algorithm 3 The K-Incremental Gibbs sampling approach we propose as a companion to Incremental-LDA initialize z N from the previous solution for K -1 contexts <ref type="formula" target="#formula_7">5</ref>) end while 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 -0. web decision at all, therefore the average log-likelihood does not vary in this region.</p><formula xml:id="formula_11">∀ context t | ct ∈ C low , initialize zt ← K ∀ context t | c t ∈ C low , reassign z t ← K with prob. δ 1 while not converged do choose a concept index j from {1, 2, • • • N } sample z j according to P (z j | z N \j , w N ) (Equation</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Entropy-Based Evaluation of the System</head><p>We define an entropy-based metric of disorder to evaluate the performance of the system, with two terms:</p><formula xml:id="formula_12">H = ρ × H(C|X) + (1 -ρ) × H(X|S),<label>(8)</label></formula><p>where H(.) is the entropy function, C, X, S are random variables denoting concepts, contexts, and scenes respectively, H(C|X) is the conditional entropy of concepts given the context, H(X|S) is the conditional entropy of contexts given the scene, and ρ is a parameter determining relative importance of the two terms (set to 0.25 experimentally). These two terms stem from two possibly opposing targets: We would like as few contexts as possible assigned to a scene, giving us more specific "documents"; and at the same time as few concepts as possible associated with a context, thereby more specific "topics". A combination of the two terms is expected to give us the most specific contextualization of the scene 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS AND RESULTS</head><p>We now evaluate our framework and assumptions from three different aspects:</p><p>1. We first test whether Incremental-LDA can determine the optimal number of contexts; e.g., if it stops adding new contexts at the optimal point. We also test if reusing partial solutions in K-Incremental Gibbs sampler leads to better performance. 2. Then we compare extracting context directly from raw features of the scene, against modeling it on top of the concept web. 3. Finally, we demonstrate how contextual information can improve reasoning, in three different scenarios:</p><p>2 Similar multi-objective optimization of these two metrics can be found in the literature, for instance see <ref type="bibr" target="#b58">[59]</ref>.</p><p>(1) scene interpretation, (2) object recognition, and (3) planning.</p><p>The training and test scenes in the experiments can belong to 3 different contexts (Kitchen, Playroom, and Workshop). Unless explicitly mentioned, a scene is a pure context scene, i.e., contains elements of a single context. A scene can also contain elements from multiple contexts, in which case it is denoted as a mixed context scene. For generating each scene in the set, a context is decided randomly and then the scene is populated with randomly chosen objects that have the noun, adjective, and verb attributes related to the selected context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Performance of Incremental-LDA and K-Incremental Gibbs Sampling</head><p>First, we analyze the dynamics of Incremental-LDA under two variables: One is a varying number of encountered scenes, in which we hope to detect the correct number of contexts as soon as possible, and the second is the varying number of contexts K, in which we look for a preference for the expected number of contexts, i.e., K = 3 for our case. Figure <ref type="figure" target="#fig_1">11</ref> depicts the number of highly uncertain concepts (|C low |) and the entropies ( H, Equation 8) of the system for different configurations. Note that, left alone, Incremental-LDA would itself converge to a certain K setting, which is ideally K = 3 here, however, for the sake of comparison, we force varying K values in these experiments.</p><p>For each configuration, we use 10 test sets of |D| scenes with random contexts. The number of encountered scenes in a test set, |D|, is one of the free variables. Each scene d ∈ D is populated with 3-5 random objects of the randomly selected context. Figures <ref type="figure" target="#fig_1">11(a</ref>) and <ref type="bibr">11(b)</ref> show that with a reasonable number of scenes (|D| ≥ 3), |C low | remains positive until K reaches 3, and then diminishes. For cross-check of the results, Figures <ref type="figure" target="#fig_1">11(c</ref>   <ref type="formula" target="#formula_12">8</ref>) of K-Incremental Gibbs solver, versus the standard batch Gibbs solver. The K-Incremental Gibbs solver is fed a partial solution for 2 contexts and then run for K = 3 contexts. The batch Gibbs sampler is directly run for K = 3 contexts. and <ref type="bibr">11(d)</ref> presents the change of the entropy of the system, H, for varying K and scene count. The mean and standard deviation values for the 10 test sets are indicated with error bars. We hope to achieve as early convergence as possible to the correct context count, which is duly achieved by the |D| = 3 scenes mark. Note that for reasonable numbers of scenes, the lowest possible entropy values are achieved when K = 3, which conforms our expectations since, in our experiments, since we truly have three contexts, namely the Kitchen, Playroom, and Workshop contexts.</p><p>In all four cases, the system converges with about 3 encountered scenes, and shows preference (in terms of minimal entropy and minimal number of highly uncertain concepts) at K = 3 contexts. Since this is also the point at which |C low | reaches to zero, Incremental-LDA then stops adding new counts, correctly deducing the minimum entropy setting of our system.</p><p>Next, we compare the performance of K-Incremental Gibbs sampling with batch Gibbs sampling. The question is whether reusing the previous partial solution leads to faster convergence times for K-Incremental Gibbs sampler. Our test set includes 100 scenes.</p><p>Figure <ref type="figure" target="#fig_10">10</ref> presents the results over this test set that conform with our expectations: Using a partial solution for 2 contexts, K-Incremental Gibbs sampler converges  faster compared to the batch solver. We measure the convergence of the system in terms of its entropy 3 .</p><p>Note that K-Incremental Gibbs Sampling is fundamentally a variant of the standard Gibbs sampler employing an informed initialization, which has been successful in various challenging problems with very high number of contexts (topics), e.g., in <ref type="bibr" target="#b58">[59]</ref>, which extracts ≈ 300 "hot" scientific topics over 28,154 abstracts published in PNAS between 1991 and 2001. Therefore, in spite of the physical limitations on the data set used in this study, resulting in a modest number of concepts and contexts, it is reasonable to expect that K-Incremental 3 Also note that the entropy value eventually reached by the two solvers is indeed the expected minimum entropy value for these environmental conditions. Gibbs sampler will also be able to scale up for a high number of contexts as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Context from the Concept Web against Context from Raw Features</head><p>Next we evaluate how useful the concept web is in guiding contextualization. Figure <ref type="figure" target="#fig_2">12</ref> shows the comparison of LDA on concept web versus LDA on rawfeatures-only. First, we contrast how the two schemes fare in case of insufficient scene encounters. Concurrently, we also investigate to what degree the discretization of the raw-features is necessary, if at all. In the second type of tests, we conduct a grid parameter search in the LDA space, to decide the best parameter settings for the two algorithms, as well as their sensitivity level to the changes in these parameters. Note that these two sets of experiments must be thought of in unison, in the sense that we have iteratively updated the parameters used in one set according to the best results of the other set, therefore we hope to present meaningful results in both sets. In the figures, we present the predicted likelihoods assigned by these algorithms to the contexts that we "know" to be true. The correct contexts have been decided through supervision for evaluation purposes only.</p><p>Figures <ref type="figure" target="#fig_2">12(a-b</ref>) depict the contextualization performance on raw features directly, with Figures <ref type="figure" target="#fig_2">12(c-d</ref>) showing the performance of the concept web. Figure <ref type="figure" target="#fig_2">12</ref>(a) versus Figure <ref type="figure" target="#fig_2">12</ref>(c) compare the results of the first set, i.e., the effects of scene count and discretization (with the trade-off parameters α and ξ from Equation <ref type="formula" target="#formula_7">5</ref>both set to 0.1) An important result that pops out is that the raw features approach needs 50 scenes to settle on a meaningful partitioning, while the concept web method manages to converge with an impressive speed at as few as 3-5 scenes. Even at 50 scenes, the raw features approach needs to be supported by coarse discretization of the features (i.e., being divided into 10 bins at most), since LDA is unable to locate statistically significant co-occurrences otherwise. For other settings, the decisions of the raw-features approach are at chance level: 33.3% for a 3-way decision.</p><p>Figures <ref type="figure" target="#fig_2">12(b</ref>) and 12(d), on the other hand, present the results of the grid search in the α-ξ space (with 50 scenes, 10-bins of discretization). Once again, we see that LDA-on-raw-features is more fragile against parameter changes, while the concept web method proves robust under most settings. Indeed, even for the worst parameter settings, notice that the concept-web case provides confidences of over 50%, which are sufficient for correct decision making, and are well over the chance level of 33.3%.  Fig. <ref type="figure" target="#fig_3">13</ref>: The combined results of object recognition in context, over all 15 objects in the test set. The prediction accuracies over all determined noun and adjective concepts, using (i) only perceptual features, (ii) the concept web, and (iii) contextual information are compared. In the plot, the red lines denote the median values, the boxes denote the data that fall between the 25 th and 75 th percentiles, the whiskers cover the extreme data that are not outliers, and stars indicate the outliers.</p><p>The results confirm that learning context from concepts is better than learning them from raw features in two aspects: (i) Learning converges faster, and is therefore more reliable even after as few as 3-5 scene encounters, and (ii) It is less sensitive to the model parameters, which increases the robustness of learning without needing a careful tuning of parameters.  <ref type="table" target="#tab_8">VI</ref> demonstrates the predicted context(s), showing that the robot can distinguish between pure and mixed-context scenes correctly, and decide on the correct components in case of a mixedcontext scene. These results are important because they demonstrate that our interpretation of the scene context is correct, regardless of the scene being composed of a single context or multiple contexts. Therefore, we obtain justification for our next step of using this contextual interpretation for guiding reasoning in other cognitive tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Using Context, Part 2: Object Recognition in Context</head><p>The second scenario considers the effect of context on object recognition. Table VII demonstrates the recognition results for seven sample objects that are either (i) individually perceived (columns 2-3), (ii) assessed in an individual concept web (columns 4-5), or (iii) evaluated in context 4 (columns 7-8).</p><p>The results show that concept web itself can correct certain mistakes of the perception-only assessment, while also boosting confidences of guesses to 100% certainty. However, it is not flawless and is also albeit in a lesser amount, to errors (see rows 2-3 in the table). In such cases, it is especially difficult to correct these errors, due to the (unfounded) high confidence associated with them. Contextual information can be beneficial in these settings.</p><p>Remembering our fundamental assumption that related objects occur together in context (which allowed us to develop an LDA-based model in the first place), the system can use context to revise and correct its previous judgments. The loop of (a) context deduction, (b) probabilistic update of concept web, and (c) reiteration of MRF, as described in Section IV-D and Equation <ref type="formula" target="#formula_9">6</ref>, also visualized in Figure <ref type="figure">6</ref>, is utilized for refining predictions in context. Combined results for all 15 test objects are demonstrated in Figure <ref type="figure" target="#fig_3">13</ref>, which also show an improvement of performance for the context-guided recognition.</p><p>In all these results, however, the individual predictions made solely using prototypes are quite good already, thereby making it difficult to adequately estimate the benefits of using context. Hence we have conducted an additional set of experiments, depicted in Figure <ref type="figure" target="#fig_4">14</ref>, under artificial noise specifically added to the prototype 4 The objects are given in pure-context environments, for the sake of easy analysis. predictions. An average of the prediction accuracies (scaled to [0, 1]) over 15 random trials over the test set are shown. A noise probability parameter is determined in the range [10%, 90%], and this parameter defines the probability of selection of each concept for addition of artificial noise. In case a concept is selected, its prototype-predicted probability p% is reversed to (100 -p)%. The σ trade-off parameter of Equation 6 is varied in the range [0.1, 0.9]. Each noise probability vs. σ combination is repeated 15 times<ref type="foot" target="#foot_1">foot_1</ref> and the average performance results are presented in Figure <ref type="figure" target="#fig_4">14</ref>. The system is shown to be quite resilient under increasing artificial noise: Combining information from many sources all of which contributes to the contextual analysis, the system is able to detect the context correctly and thereby correct individual wrong predictions using the majority vote.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Using Context, Part 3: Planning in Context</head><p>Finally, we show how contextual information can be useful in a planning task. It is known that humans hugely rely on contextual information for planning their actions <ref type="bibr" target="#b68">[69]</ref>- <ref type="bibr" target="#b72">[73]</ref>, possibly due to a severely restricted working memory capacity <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b74">[75]</ref>, which results in efficient day-to-day planning, but maybe less-than-favorable performances in chess. The robots would also benefit from similar contextual guidance in planning.</p><p>To show how context can be used similarly in a robotic planning scenario, we provide two simple situations as proof-of-concept: The robot has to move two objects over a table (Figure <ref type="figure" target="#fig_5">15(a)</ref>) from an initial to a goal position. Since the robot has learned the effect features of behaviors on training objects, it is theoretically able to expand a planning tree starting from the initial state and expanding behavior nodes until the goal condition is reached. These scenarios are simulated; however, the decisions of the robot are based on real world data: The robot plans according to the expected results of actions as learned by the verb prototypes. Although it does not physically move to perform the plan, theoretically the plans are executable. In other words, we are interested not in the physical success of the plans, but in the computational efficiency of producing these plans.</p><p>In the first scenario, Figure <ref type="figure" target="#fig_5">15</ref>(b), the robot is asked to move a cup from position 8 to position 5. goal can be achieved with three consecutive move right actions in our setting. A fully-expanded tree, therefore, would consist of three levels, and with a branching factor of 13, it will consist of 13 0 + 13 1 + 13 2 + 13 3 = 2380 nodes. However, given the contextual information of the scene, which is the Kitchen context, the robot can refrain from expanding the inappropriate behaviors in a Kitchen <ref type="foot" target="#foot_2">6</ref> , leaving only the move left, move right, move forward, move backward and grasp as possible actions to be expanded. Such an elimination gives a drastic reduction in the size of the planning tree, resulting in 5 0 + 5 1 + 5 2 + 5 3 = 156 nodes instead of 2380.</p><p>Figure <ref type="figure" target="#fig_5">15</ref>(c) shows another scenario in the Playroom context. This time, the robot refrains from applying the push actions on associated objects, since balls, which are also in this context, tend to roll down and fall from the table when pushed. Therefore, the push nodes are pruned, leaving 9 0 +9 1 +9 2 +9 3 = 820 nodes in the tree. We use a breadth-first forward planning scheme subject to context-dependent pruning.</p><p>Figure <ref type="figure" target="#fig_16">16</ref> compares un-pruned and pruned node counts for 10000 random scenarios in the move-overthe-table scenario presented above, presented for the three contexts separately. Each scenario is prepared by randomly determining a context, as well as initial and goal positions on the table environment, and then asking the robot to plan a behavior sequence from the initial to the goal position in this contextual background. Note that the amount of node reduction in these experiments depend on the randomly chosen target position. If the goal position is very close to the initial position, then relatively little reduction is possible, since the height of the planning tree will already be fairly shallow even in the unpruned case. However, if the random target is chosen sufficiently far from the initial position, which would normally require a very deep and wide planning tree, significant pruning is possible. The outliers in the graph correspond to such points. Note that the amount of pruning in the Kitchen case is greater than the Playroom case, since potentially greater number of actions are nonapplicable in the Kitchen case. In the Workshop case, where all actions are applicable, there are no possible reductions.</p><p>The reductions shown here are only provided as proofof-concepts, but it is clear how important it is for a robot to learn to prune its search trees in a real world setting. For a very limited robot of a small, or maybe even intermediate set of actions, considering each action for every situation might be an option, but for any robot who aims to operate in the real world, the actions will be so varied and planning chains will necessarily be so long that even most basic reductions (i.e., no need to consider opening the kitchen door for heating a glass of milk) will be of critical importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Running Time Performance of the System</head><p>The whole system is able to work close to real-time: 10 test runs with non-optimized code on a standard desktop PC (i5 core, 8GB RAM) provided an average running time of 209.82ms ± 3.38ms for the detection of context with Incremental LDA, and 1395.22ms ± 15.31ms for the convergence of the concept web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SUMMARY AND DISCUSSION</head><p>In the article, we studied how a humanoid robot can model, learn and use context. For modeling context, we employed and extended Latent Dirichlet Allocation (LDA), a widely-used topic model in the computational linguistics literature. Unlike the existing applications of LDA in robotics for, e.g., word learning, where LDA is directly applied onto low-level sensorimotor data, we were motivated by the concept web hypotheses in humans and its computational advantages to apply LDA onto a concept web model that we developed in our previous work using Markov Random Fields. We demonstrated the following important aspects:</p><p>• In an unsupervised fashion, the robot can learn context even if the number of contexts is not given. By using an online version of the Gibbs sampler proposed in the article, the robot can work online to process new observations and can tackle new contexts. By a systematic analysis, we show that the model finds the correct number of contexts in different settings. • The robot can use the learned contexts to improve its performance in cognitive tasks. In the article, we showed this aspect for object recognition and planning.</p><p>• Finally we show how learning context over a web of abstracted concepts is easier and provides better performance for an LDA-based architecture, which deals with the sensorimotor complexity of real world better than raw features themselves. Below, we discuss several aspects of our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Basing Context on the Concept Web</head><p>Basing context on a concept web has significant computational advantages: In <ref type="bibr" target="#b2">[3]</ref>, we show how concept web enables a superior performance of object recognition and conceptualization as compared to a raw-feature based scheme. In this work, we provide further evidence regarding the performance of concept web for LDAbased conceptualization. We demonstrate how the concept web provides better performance with significantly fewer training examples, as well as reduced sensitivity against system parameters. These advantages are due to its abstraction capability: The real world presents an overwhelming amount of complex information, which needs some structure to be imposed before statistically significant relations can be discovered. This is argued to be the driving reason of conceptualization in humans as well (e.g., <ref type="bibr" target="#b75">[76]</ref>- <ref type="bibr" target="#b81">[82]</ref>, for a slightly different but interesting argument, see also <ref type="bibr" target="#b82">[83]</ref>.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Planning in the Real World</head><p>Bylander <ref type="bibr" target="#b83">[84]</ref> and Chapman <ref type="bibr" target="#b84">[85]</ref> show that planning is intractable in the general sense, unless it is restricted severely, for instance, to propositional planning with strictly positive preconditions and exactly one postcondition. Such restricted cases can be defined to reduce the planning problem to a polynomial-time subset; however, small deviations make the problem intractable again: e.g., the NP-hard problem of allowing two postconditions along with one precondition, or the NP-complete problem of one strictly positive postcondition along with one precondition. As Bylander <ref type="bibr" target="#b83">[84]</ref> and Hendler <ref type="bibr" target="#b85">[86]</ref> note, it is difficult to describe any interesting world in propositional logic, let alone such restrictions for the sake of tractability. We have to find a workaround. We propose that this workaround can be, and for humans is, context <ref type="bibr" target="#b68">[69]</ref>- <ref type="bibr" target="#b71">[72]</ref>.</p><p>Also supporting our hypothesis is the work of Siegler, e.g., <ref type="bibr" target="#b86">[87]</ref>, who, from a developmental point of view, stresses how important context is in helping children choose which skill or problem solving strategy to apply in a certain situation. So important is this process of choosing, he claims, that the question is not "whether children 'have' a concept or strategy or theory at a given age", but it is rather "the set of conceptualizations and strategies and theories that children know and the mechanisms by which they choose among them" <ref type="bibr" target="#b86">[87]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Limitations and Future Work</head><p>Overall, we provide promising results that a learning scheme which includes background information, instead of leaving it out, is feasible and useful for a robot when dealing with the real world. Our work can be extended in several directions.</p><p>The experiments were performed on real objects, although the settings are not realistic. This limitation was due to the interaction capabilities of iCub: iCub cannot walk and is confined to a table-top environment. Moreover, due to its delicate hands and the limited precision of the touch sensors on the hands, the range of objects that can be interacted with was limited to light-weight and convex objects. This also restricted us in the varieties of contexts. However, LDA is shown to scale up extremely well in natural language processing settings, where it could be tested with huge corpora (e.g., <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b58">[59]</ref>) as well in a number of other complicated real-life scenarios including functional miRNA-mRNA regulatory modules identification <ref type="bibr" target="#b40">[41]</ref> and fraud detection <ref type="bibr" target="#b39">[40]</ref>; therefore, we believe that our framework will scale well in realistic robotics settings.</p><p>In Incremental-LDA, we assumed that the number of contexts can only increase in the environment, and therefore it is not necessary to check if the context count K can go down. We observe similar assumptions in the literature, e.g., <ref type="bibr" target="#b66">[67]</ref>, where the number of topics can only increase in time. We believe that there is no reason for a biological cognitive agent to remove learned contexts from its system; although they might be merged as new contexts or split into sub-contexts, the only case where the number of contexts might decrease is when the agent forgets learned associations.</p><p>It should also be noted that, although our current concept web is composed of noun, adjective, and verb concepts, a cognitive model should include spatial, temporal, adverb, and social concepts as well. With the incorporation of these types of concepts in our concept web, contexts related to their semantics will also be able to manifest themselves in our model.</p><p>Another plausible extension is regarding the concept web: The current concept web is a model of longterm memory only, with links holding information about the robot's experiences about the world. This long-term memory is activated based on the current perception, yet, there is no clear separation between short-term and longterm memory akin to humans.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Verbs</head><p>Knock Down 0+0++++00++00+-------------0-0-+0-0--+0000000-* ---------------+++++ None Throw * 0 * 000 * 0-000+00------------0-----000-00--0----0---------------0++++ None</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: (a) Existing cognitive systems have concepts which have links to perceptual features and motor actions which were programmed by a designer or trained in context-free environments. (b) The concept web model that we developed in our previous study [3]: A densely connected concept web connecting perception, action and language; however, there is no notion of context in this model. (c) We propose a system that learns in context the links between concepts and sensorimotor primitives, based on the statistics of its interactions in real-life environments. For clarity, only a few links and concepts are shown. [Sub-figures (a) and (c) adapted from [1]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The setup used in the experiments. iCub senses the environment with tactile sensors, a microphone and a Kinect.</figDesc><graphic coords="4,94.80,75.59,182.42,117.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The objects used in the experiments, divided to each noun category.</figDesc><graphic coords="5,95.40,151.99,421.21,94.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The objects for each adjective category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Extraction of entity (e) and effect (f) feature vectors.ev and e v are the initial and final visual features of the object before and after a behavior's execution. f = e v -ev.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>Fig.6: The schematic presentation of the whole system. (a) The training phase is the only phase that includes supervision. Features are detected from instances automatically, while simultaneously the instances are labeled by a human partner. Prototypes are extracted from these two types of information. (b) The execution phase is fully autonomous. Information can flow in from the perception space, through a feature extraction mid-level, where they are compared against previously extracted prototypes, or from the language and action. Detected concepts effect each other in a concept web to converge to a common interpretation. (A number of nodes are randomly illustrated with white color to exemplify active concepts.) The concept webs of each object in the scene are then analyzed to elicit the contextual setting. The contextual information in turn is fed back to the concept webs to guide their activation. There is constant information flow between these two kinds of nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig.8: A sample concept web constructed by the iCub. Noun, adjective, and verb concepts are indicated with red, blue, and green respectively. Connections between concepts are shown with gray. Ubigraph 3D visualization library<ref type="bibr" target="#b55">[56]</ref> is used for displaying the graph, presented here as a projection on the 2D plane. [Taken from<ref type="bibr" target="#b2">[3]</ref>, best viewed in color]</figDesc><graphic coords="8,46.13,66.96,262.52,213.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>2 -Fig. 9 :</head><label>29</label><figDesc>Fig.9: Average log likelihood l for varying σ (Equation6, σ = 0: Pure contextual information, σ = 1: Pure concept web decision). The interval [0.4, 0.5] is depicted as maximizing l.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>Fig.10:A comparison of the entropy ( H) evolution (Equation8) of K-Incremental Gibbs solver, versus the standard batch Gibbs solver. The K-Incremental Gibbs solver is fed a partial solution for 2 contexts and then run for K = 3 contexts. The batch Gibbs sampler is directly run for K = 3 contexts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 :Fig. 12 :</head><label>1112</label><figDesc>Fig. 11: The effect of encountered scene counts and varying context counts K. Note that Incremental-LDA would itself stop at K = 3, however we force increasing K for the sake of comparison. (a) Effect of increasing K on the number of uncertain concepts, |C low |, for varying number of scenes. By K = 3 contexts |C low | diminishes to 0, therefore Incremental-LDA would stop adding new contexts at this point. (b) Effect of encountered scenes on the number of uncertain concepts, |C low |, for different context counts. (c) Effect of increasing K on the entropy of the system, H, for varying number of scenes. (d) Effect of encountered scenes on the entropy of the system, H, for different context counts. In all the experiments, 10 test sets of |D| scenes each are used. The mean values for the 10 test sets are plotted, while the standard deviations are indicated with error bars. In (b) and (d), the x-axis is in log-scale. [Best viewed in color]</figDesc><graphic coords="12,218.04,312.21,83.39,87.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Prediction of context for a few example scenes where confidences are indicated in parentheses. Bold text indicates correct decisions. [Best viewed in color]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Object recognition in context. Prediction confidences are indicated in parentheses. Bold text indicates correct decisions whereas stroked text indicates wrong decisions. [Best viewed in color]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>C</head><label></label><figDesc>. Using Context, Part 1: Making Sense of Pure-and Mixed-Context Environments Now we demonstrate how our context model can be utilized in reasoning and decision making. The first scenario is designed for assessing how successful our model is in recognizing contexts of scenes. The robot encounters six different scenes, three of which are composed of items of a single context, and the remaining three of multiple contexts. Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 14 :Fig. 15 :</head><label>1415</label><figDesc>Fig.14:The performance of the individual prototype-based predictions, versus context enhanced concept web predictions, under artificially added noise, presented as prediction accuracies scaled to [0,1]. The noise probability denotes the probability of artificial noise being added to each single concept, via reversing its prototype-predicted probability from p% to reversed to (100 -p)%. σ refers to the trade-off parameter in Equation6.</figDesc><graphic coords="15,165.53,488.40,100.77,104.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 16 :</head><label>16</label><figDesc>Fig.16: The node counts of unpruned vs. pruned planning trees of 10000 random scenarios, grouped by their contexts. The Kitchen context is subject to more pruning, as expected, due to a large number of NA behaviors. The Workshop context, on the other hand, is not subject to any pruning, since all behaviors are potentially applicable. In the plot, the boxes denote the data that fall between the 25 th and 75 th percentiles, and stars indicate the outliers. [Best viewed in color]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>-++---++--------------++--+--------** --------------------+-----** + ** -++-* + * -------+ ** --Ball --+-+-+----++---------------+++++-----------------------------+----******* -** +-+ * -**** -+++--Adjectives Hard +-+ * +++++++++--------------+ * ++ * +--------* --------------------+++++ ****** + ** +++ * * + ** + * -+ ** --Noisy --+++ * +---* ++ * -------------++++ * + * -**** -----------------------++---+++++++++++++ * -** -* -+ ** --</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>The co-occurrences of noun and adjective labels for the dataset. Numbers denote the number of objects (out of 60) belonging to both categories.</figDesc><table><row><cell></cell><cell cols="10">Hard Soft Noisy Silent Tall Short Thin Thick Round Edgy</cell></row><row><cell>Box</cell><cell>2</cell><cell>14</cell><cell>2</cell><cell>14</cell><cell>0</cell><cell>16</cell><cell>0</cell><cell>16</cell><cell>0</cell><cell>16</cell></row><row><cell>Ball</cell><cell>3</cell><cell>7</cell><cell>7</cell><cell>3</cell><cell>0</cell><cell>10</cell><cell>1</cell><cell>9</cell><cell>10</cell><cell>0</cell></row><row><cell>Cylinder</cell><cell>14</cell><cell>0</cell><cell>5</cell><cell>9</cell><cell>10</cell><cell>4</cell><cell>9</cell><cell>5</cell><cell>14</cell><cell>0</cell></row><row><cell>Cup</cell><cell>11</cell><cell>0</cell><cell>1</cell><cell>10</cell><cell>0</cell><cell>11</cell><cell>0</cell><cell>11</cell><cell>11</cell><cell>0</cell></row><row><cell>Tool</cell><cell>5</cell><cell>0</cell><cell>5</cell><cell>0</cell><cell>5</cell><cell>0</cell><cell>0</cell><cell>5</cell><cell>5</cell><cell>0</cell></row><row><cell>Plate</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>4</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>The set of behaviors applicable for each object. A:Applicable; NA: Not-Applicable</figDesc><table><row><cell>Push</cell><cell>Move</cell><cell></cell><cell></cell></row><row><cell cols="2">(Left, Right (Left, Right Forward, Forward,</cell><cell>Drop Grasp Shake</cell><cell>Knock down</cell><cell>Throw</cell></row><row><cell>Backward</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>The visual, audio, haptic and proprioceptive features extracted from the interactions of the robot.</figDesc><table><row><cell>Feature Type</cell><cell>Feature</cell><cell>Position</cell></row><row><cell></cell><cell>Position:(x, y, z)</cell><cell>1-3</cell></row><row><cell></cell><cell cols="2">Object dimensions:(width, height, depth) 4-6</cell></row><row><cell>Visual (ev)</cell><cell>Normal zenith histogram bins</cell><cell>7-26</cell></row><row><cell></cell><cell>Normal azimuth histogram bins</cell><cell>27-46</cell></row><row><cell></cell><cell>Shape index histogram bins</cell><cell>47-66</cell></row><row><cell>Audio (ea)</cell><cell>13 bins of MFCC (max -min)</cell><cell>67-79</cell></row><row><cell></cell><cell>Change for index finger</cell><cell>80</cell></row><row><cell></cell><cell>Min values for index finger</cell><cell>81</cell></row><row><cell>Haptic (eh)</cell><cell>Max values for index finger</cell><cell>82</cell></row><row><cell></cell><cell>Mean for index finger</cell><cell>83</cell></row><row><cell></cell><cell>Variance for index finger</cell><cell>84</cell></row><row><cell></cell><cell>Standard deviation for index finger</cell><cell>85</cell></row><row><cell></cell><cell>Change for index finger</cell><cell>86</cell></row><row><cell></cell><cell>Min values for index finger</cell><cell>87</cell></row><row><cell cols="2">Proprioceptive (ep) Max values for index finger</cell><cell>88</cell></row><row><cell></cell><cell>Mean for index finger</cell><cell>89</cell></row><row><cell></cell><cell>Variance for index finger</cell><cell>90</cell></row><row><cell></cell><cell>Standard deviation for index finger</cell><cell>91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Used contexts and their prevalent concepts.</figDesc><table><row><cell></cell><cell>Kitchen</cell><cell></cell><cell></cell><cell>Playroom</cell><cell></cell><cell cols="2">Workshop</cell></row><row><cell>cup</cell><cell>short</cell><cell>thin</cell><cell>ball</cell><cell cols="2">edgy silent</cell><cell>tool</cell><cell>edgy</cell><cell>tall</cell></row><row><cell>plate</cell><cell cols="2">hard thick</cell><cell>box</cell><cell>soft</cell><cell cols="3">thick cylinder hard</cell><cell>thin</cell></row><row><cell cols="2">round silent</cell><cell></cell><cell cols="2">round noisy</cell><cell></cell><cell>round</cell><cell cols="2">silent thick</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V :</head><label>V</label><figDesc>The correspondence between the LDA terms and the notation used in this workLDAOur Notation document d ∈ D a single scene (i.e., the set of active concepts in the scene) corpus D all encountered scenes word w i ∈ W an active concept c act in the concept webs (can be a noun, adjective, or verb:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI :</head><label>VI</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VIII :</head><label>VIII</label><figDesc>Sample extracted prototypes for noun, adjective, and verb concepts.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that "verb concepts" do not have to correspond to the behavior set in a 1-1 manner: A verb concept can be associated with multiple behaviors, for instance, provided that all of these behaviors produce the same effect<ref type="bibr" target="#b47">[48]</ref>, although this is not the case in this study.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>Each trial is a random one, due to the probabilistic selection of the reversed concepts, with probability equal to the momentarily utilized noise probability parameter.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>Assuming we do not want to, for instance, shake a full cup.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We would like to thank <rs type="person">Angelo Cangelosi</rs>, <rs type="person">Anna Borghi</rs> and <rs type="person">Honghai Liu</rs> for fruitful discussions on integrating context into cognitive systems. For the experiments, we acknowledge the use of the facilities provided by the the <rs type="institution">Modeling and Simulation Center of METU (MODSIMMER)</rs>. This work is funded by the <rs type="funder">Scientific and Technological Research Council of Turkey (T ÜB İTAK)</rs> through project no <rs type="grantNumber">111E287</rs>, and partially supported by the <rs type="funder">Marie Curie International Outgoing</rs> Fellowship titled "<rs type="projectName">Towards Better Robot Manipulation: Improvement through Interaction</rs>" (<rs type="grantNumber">FP7-PEOPLE-2013-IOF-628854</rs>) awarded to Erol S ¸ahin.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bbWQZAu">
					<idno type="grant-number">111E287</idno>
				</org>
				<org type="funded-project" xml:id="_keUkCBg">
					<idno type="grant-number">FP7-PEOPLE-2013-IOF-628854</idno>
					<orgName type="project" subtype="full">Towards Better Robot Manipulation: Improvement through Interaction</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>We use prototypes to represent the noun (N), adjective (A) and verb (V) concepts, the extraction of which is illustrated in Figure <ref type="figure">6(a)</ref>. The noun and adjective concepts are related to the object entities, while verb concepts are related to the changes induced on the objects by the behaviors. Therefore, the prototypes of the noun and adjective concepts are obtained from the entity feature vectors e, while the verb concept prototypes are obtained from effect feature vectors f. Each object in the training set is labeled beforehand by supervision to denote the concepts it is associated with: Each training object is strictly labeled with 1 noun concept (out of 6) and 5 adjective concepts (one from each of the 5 dichotomic pairs). In addition, every applicable behavior is applied to each training object, and the interactions are labeled with strictly 1 verb concept.</p><p>During training, the entity and effect feature vectors are collected from the training objects, and divided according to the labeled concepts. For each concept, every feature is assessed in terms of its contribution to the concept: If the feature has a highly positive contribution to the concept, it is indicated with a '+' in the concept prototype. '-' denotes a negative contribution, and '*' denotes inconsistent contribution. These contributions are decided by clustering the features, using Robust Growing Neural Gas (RGNG) clustering algorithm <ref type="bibr">[88]</ref>, in a two dimensional space of means and variances: The mean axis denotes the amount of the contribution, while the variance axis denotes the consistency. Features with positive mean and low variance are marked with '+'; negative mean and low variance with '-'; and high variance with '*'. Of special interest are the features marked with '*'s, which effectively distinguishes irrelevant features, that can be disregarded from comparisons regarding the concept.</p><p>Prototypes for the verb concepts are extracted in a similar manner, except that (1) they are calculated over the effect features f, and (2) they include a '0' character for features that are unaffected by the behavior.</p><p>Eventually, we obtain 29 prototypes in total; 6 for nouns, 10 for adjectives, and 13 for verbs. The prototypes of the noun and adjective concepts are of length 91, the same with the length of an entity feature vector e, containing 66 visual, 13 audio, 6 haptic and 6 proprioceptive features. The prototypes of the verb concepts are composed of 66 characters, and denote visual features only. The prototypes used in this study are shown in Table <ref type="table">VIII</ref>.</p><p>When a new object is encountered, its entity feature vector e is compared against the noun and adjective prototypes. Similarly, if a behavior has been applied, the effect feature vector f is compared against the verb concept prototypes to recognize the behavior. This comparison consists of finding the concepts that minimize the Euclidean distance between the object's feature vector and the concept mean vector (Equation <ref type="formula">1</ref>). The irrelevant features of each concept, marked with '*' in the concept prototype, are excluded from this calculation. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning and using context on a humanoid robot using Latent Dirichlet Allocation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Celikkanat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Orhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pugeault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sahin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="201" to="207" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The situated nature of concepts</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Barsalou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American journal of psychology</title>
		<imprint>
			<biblScope unit="page" from="349" to="384" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A probabilistic concept web on a humanoid robot</title>
		<author>
			<persName><forename type="first">H</forename><surname>Celikkanat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Orhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Autonomous Mental Development</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="92" to="106" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Barwise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perry</surname></persName>
		</author>
		<title level="m">Situation and Attitudes</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">From discourse to logic: Introduction to modeltheoretic semantics of natural language, formal logic and discourse representation theory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Reyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Kluwer Academic</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Talking about quantities in space: Vague quantifiers, context and similarity</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Coventry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cangelosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Newstead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bugmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Cognition</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="221" to="241" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Scripts, plans, goals and understanding</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Abelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<publisher>Lawrence Erlbaum</publisher>
			<pubPlace>Hillsdale, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Intelligence without representation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="139" to="159" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Artificial intelligence, logic and formalizing common sense</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Philosophical logic and artificial intelligence</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual objects in context</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="617" to="629" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contextual priming for object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="169" to="191" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully generated scripted dialogue for embodied agents</title>
		<author>
			<persName><forename type="first">K</forename><surname>Van Deemter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krenn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Piwek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Klesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schröder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Art. Intel</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1219" to="1244" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Steps toward formalizing context</title>
		<author>
			<persName><forename type="first">V</forename><surname>Akman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Surav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">55</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From here to human-level ai</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="1174" to="1182" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simulation, situated conceptualization, and prediction</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Barsalou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="issue">1521</biblScope>
			<biblScope unit="page" from="1281" to="1289" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The potentiation of grasp types during visual object categorization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual cognition</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="769" to="800" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How action and context priming influence categorization: a developmental study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kalénine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bonthoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Borghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brit. J. of Dev. Psych</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="717" to="730" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scene and position specificity in visual memory for objects</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hollingworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Two forms of scene memory guide visual search: Memory for scene context and memory for the binding of target object to scene location</title>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="273" to="291" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The pairedobject affordance effect</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Riddoch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">812</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Manipulating objects and telling words: a study on concrete and abstract words acquisition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Flumini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cimatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scorolli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Context and hand posture modulate the neural dynamics of tool-object perception</title>
		<author>
			<persName><forename type="first">N</forename><surname>Natraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mizelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Flumini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Borghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Wheaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="506" to="519" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Robotics: State of the art and future challenges</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Bekey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Imperial College Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Establishing human situation awareness using a multi-modal operator control unit in an urban search &amp; rescue humanrobot team</title>
		<author>
			<persName><forename type="first">B</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Kruijff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Smets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mioch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Groenewegen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="229" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Aging Friendly Technology for Health and Independence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gregoriades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Obadan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Papadopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
	<note>A robotic system for home security enhancement</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dangerous situation awareness support system for elderly people with dementia</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tsuruma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kanai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kunifuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Human Computer Interaction</title>
		<imprint>
			<publisher>ACTA Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="62" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Contextually guided semantic labeling and search for three-dimensional point clouds</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="page">0278364912461538</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PR2 looking at things: Ensemble learning for unstructured information processing with markov logic networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nyga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Balint-Benczedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tell me Dave: Context-sensitive grounding of natural language to manipulation instructions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics Science and Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>RSS 2014</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiple-agent perspectives in reasoning about situations for context-aware pervasive computing systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Padovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Loke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="729" to="742" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Context assessment strategies for ubiquitous robots</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mastrogiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sgorbissa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zaccaria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSJ ICRA</title>
		<imprint>
			<biblScope unit="page" from="2717" to="2722" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">When is scene identification just texture recognition?</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Renninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="2301" to="2311" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning pre-attentive driving behaviour from holistic visual features</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pugeault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="154" to="167" />
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robot steering with spectral image information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ackerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Robotics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="247" to="251" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="2929" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploiting hierarchical context on a large database of object categories</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Why is real-world visual object recognition hard?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comp. Bio</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Employing Latent Dirichlet Allocation for fraud detection in telecommunications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Girolami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1727" to="1734" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Identifying functional miRNA-mRNA regulatory modules with correspondence Latent Dirichlet Allocation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tsykin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Goodall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="3105" to="3111" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Grounding of word meanings in multimodal concepts using LDA</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Iwahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSJ IROS</title>
		<imprint>
			<biblScope unit="page" from="3943" to="3948" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Grounding of word meanings in Latent Dirichlet Allocation-based multimodal concepts</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Iwahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Robotics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2189" to="2206" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Online learning of concepts and words using multimodal LDA and Hierarchical Pitman-Yor Language Model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nagasaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Iwahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSJ IROS</title>
		<imprint>
			<biblScope unit="page" from="1623" to="1630" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Automatic drive annotation via multimodal latent topic model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Takenaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nagasaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Taniguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="page" from="2744" to="2749" />
			<date type="published" when="2013-11">Nov 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The icub humanoid robot: an open platform for research in embodied cognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Metta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sandini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vernon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Natale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th workshop on performance metrics for intelligent systems</title>
		<meeting>the 8th workshop on performance metrics for intelligent systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="50" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Surface shape and curvature scales</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="557" to="564" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Verb concepts from affordances</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yürüten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Borghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sahin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interaction Studies</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Natural categories</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Rosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="350" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Conceptual spaces: The geometry of thought</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gärdenfors</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Co-learning nouns and adjectives</title>
		<author>
			<persName><forename type="first">G</forename><surname>Orhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Olgunsoylu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sahin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Third Joint International Conference on Development and Learning and Epigenetic Robotics</title>
		<imprint>
			<publisher>ICDL-EpiRob</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Toward an ecological theory of concepts</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gabora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aerts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological Psychology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="116" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A model of probabilistic category learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Kruschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1083</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mixture models of categorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rosseel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="178" to="210" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Markov random fields and their applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kindermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Snell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>American Mathematical Society</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Providence, RI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Ubigraph: Free dynamic graph visualization software</title>
		<author>
			<persName><forename type="first">T</forename><surname>Veldhuizen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A comparative study of energy minimization methods for markov random fields</title>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2006</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="16" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Stable fixed points of loopy belief propagation are minima of the bethe free energy</title>
		<author>
			<persName><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. of the Nat. Acad. of Sci</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">Suppl 1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A collapsed variational Bayesian inference algorithm for Latent Dirichlet Allocation allocation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1353" to="1360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Online inference of topics with Latent Dirichlet Allocation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Some basic principles of developmental robotics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stoytchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="122" to="130" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Autonomous Mental Development</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Antoniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The annals of statistics</title>
		<imprint>
			<date type="published" when="1974">1974</date>
			<biblScope unit="page" from="1152" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Infinite latent feature models and the Indian buffet process</title>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Bayesian density estimation and inference using mixtures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Escobar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the american statistical association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">430</biblScope>
			<biblScope unit="page" from="577" to="588" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Hierarchical dirichlet processes</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the american statistical association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">476</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Online variational inference for the hierarchical dirichlet process</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="752" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Semantic activation in action planning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Lindemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Van Schie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bekkering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">633</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Action semantics: a unifying conceptual framework for the selective use of multimodal and modality-specific object knowledge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Van Elk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Schie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bekkering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics of life reviews</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Grasping objects by their handles: a necessary interaction between cognition and action</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Creem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Proffitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">218</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">The developmental psychology of planning: Why, how, and when do we plan</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Scholnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">What&apos;s in a story: An approach to comprehension and instruction</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Trabasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in instructional psychology</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Glaser</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="213" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Working memory capacity</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cowan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The role of left prefrontal cortex in language and memory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gabrieli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Poldrack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Desmond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. of the Nat. Acad. of Sci</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="906" to="913" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Concept, knowledge, and thought</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Oden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="203" to="227" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Knowledge, concepts and categories</title>
		<author>
			<persName><forename type="first">U</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="43" to="92" />
		</imprint>
	</monogr>
	<note>Concepts and similarity</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Concepts of supervenience</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy and Phenomenological Research</title>
		<imprint>
			<biblScope unit="page" from="153" to="176" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">The symbolic species: the co-evolution of language and the human brain</title>
		<author>
			<persName><forename type="first">T</forename><surname>Deacon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Linguistic and nonlinguistic turn direction concepts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Montello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spatial information theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="354" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A conceptual model of wayfinding using multiple levels of abstraction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Timpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Volta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Egenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theories and methods of spatio-temporal reasoning in geographic space</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="348" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Knowledge, concepts, and categories</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hampton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="133" to="159" />
		</imprint>
	</monogr>
	<note>Conceptual combination</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">They saw a game; a case study</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Hastorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cantril</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Abnormal and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">129</biblScope>
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Complexity results for planning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bylander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="274" to="279" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Planning for conjunctive goals</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chapman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="333" to="377" />
		</imprint>
	</monogr>
	<note>Artificial intelligence</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Ai planning: Systems and techniques</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Siegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Daehler</surname></persName>
		</author>
		<title level="m">Across the great divide: Bridging the gap between understanding of toddlers</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
