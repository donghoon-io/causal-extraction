<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Embedded Topics in the Stochastic Block Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Rémi</forename><surname>Boutin</surname></persName>
							<email>remi.boutin.stat@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratoire MAP5</orgName>
								<orgName type="laboratory" key="lab2">UMR 8145</orgName>
								<orgName type="institution" key="instit1">Université Paris Cité</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><surname>Bouveyron</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;azur</orgName>
								<orgName type="institution" key="instit2">INRIA</orgName>
								<orgName type="institution" key="instit3">MAASAI Team</orgName>
								<address>
									<settlement>Sophia-Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Latouche</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Laboratoire MAP5</orgName>
								<orgName type="laboratory" key="lab2">UMR 8145</orgName>
								<orgName type="institution" key="instit1">Université Paris Cité</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">Laboratoire LMBP</orgName>
								<orgName type="laboratory" key="lab2">UMR 6620</orgName>
								<orgName type="institution" key="instit1">Université Clermont Auvergne</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Aubière</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Embedded Topics in the Stochastic Block Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s11222-023-10265-9</idno>
					<note type="submission">Submitted on 25 Jul 2023 Distributed under a Creative Commons Attribution 4.0 International License</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T18:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph clustering</term>
					<term>topic modelling</term>
					<term>variational inference</term>
					<term>generative model</term>
					<term>probabilistic model</term>
					<term>embedded topic model</term>
					<term>stochastic block model</term>
				</keywords>
			</textClass>
			<abstract xml:lang="fr">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many real life interactions induce the exchange of texts, as in co-authorship networks, social networks or emails for instance. Since the storage capacity keeps increasing, networks with textual data on the edges become even more frequent. In order to make such networks, called communication networks, intelligible to humans, it is of great interest to gather information about the texts exchanged between the nodes and to summarise the connectivity structure. While those two questions have been studied independently, the work we propose aims at bridging the gap between the two by modelling the joint distribution of texts and edges. To the best of our knowledge, the interest on making the two disciplines of topic modelling, when texts are present on the edges, and model-based graph clustering meets is recent and the methods that have been proposed only rely on the frequency of word within the documents without incorporating semantic meaning. In this paper, we propose to take advantage of pre-trained word embeddings in the topicmodel as presented in <ref type="bibr" target="#b12">Dieng et al. (2020)</ref> in order to incorporate semantic meaning of the words and to obtain topic-meaningful clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Both the topic modelling methods and the graph clustering techniques have first emerged as deterministic optimisation problems to progressively incorporate uncertainty which led to many developments in the statistical literature. The next part provides a brief summary of the advancements in those domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Probabilistic models for topic modelling</head><p>The statistical analysis of topics has emerged in the late 90s with <ref type="bibr" target="#b36">Papadimitriou et al. (1998)</ref>, developing statistical results for the latent semantic indexing (LSI), first proposed by <ref type="bibr" target="#b11">Deerwester et al. (1990)</ref>. LSI relies on a spectral analysis of the "term frequencyinverse document frequency" and successfully captures synonymy between words. To overcome the lack of probabilistic foundations of LSI, <ref type="bibr" target="#b19">Hofmann (1999)</ref> introduced the probabilistic latent semantic index (pLSI) which models each word distribution as a mixture model such that each mixture component corresponds to a "topic". The topic membership of each word is modelled by a multinomial random variable in pLSI. Even though the topic membership of the words depends on the document, a major drawback of this model is the absence of model at the document level. This was overcome by <ref type="bibr" target="#b4">Blei et al. (2003)</ref> with the latent Dirichlet allocation (LDA) which for each document uses a Dirichlet random variable to model the proportion of each topic. However, the Dirichlet distribution makes the topics almost uncorrelated and does not directly model correlation. <ref type="bibr" target="#b3">Blei &amp; Lafferty (2006)</ref> then proposed to use a normal-logistic prior instead of a Dirichlet prior on the topic proportion to directly model the correlations. All these models require to derive the equations for any new generative model. In <ref type="bibr" target="#b43">Srivastava &amp; Sutton (2017)</ref>, they bridged the gap between topic modelling and autoencoders, taking full advantage of gradient descent for those models. Nevertheless, all the former approaches do not incorporate semantic meaning to the words. Indeed, since the model is only based on the document term-frequency matrix, they loose the information provided by the order of the words. In the embedded topic model (ETM), <ref type="bibr" target="#b12">Dieng et al. (2020)</ref> used the strength of word embeddings, such as the continuous bag of words (CBOW) or skipgram <ref type="bibr" target="#b34">(Mikolov et al., 2013)</ref> as a part of the decoder of a variational autoencoder (VAE). The topics are also embedded into the same vector space which allows to easily measure similarities between words and topics. The optimisation is done using gradient descent, as proposed in <ref type="bibr" target="#b38">Rezende et al. (2014)</ref> or <ref type="bibr" target="#b24">Kingma &amp; Welling (2014)</ref>. For a review of the former methods relying exclusively on the document term frequency matrix, the reader may refer to <ref type="bibr" target="#b44">Vayansky &amp; Kumar (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Probabilistic models for graph analysis</head><p>Statistical network analysis first started with random graph theory, initiated by <ref type="bibr" target="#b13">Erdos et al. (1960)</ref>. They studied probabilistic properties of graphs with binary connections, and a unique probability for any connection to exist. However, real life datasets do not show such regularity. Therefore, more complex and realistic graph structures have been considered. Here, a structure designates a partition of the nodes such that nodes in a cluster present a homogeneous connectivity pattern. For example, a community is a group of nodes highly connected one to another but with few connections to the rest of the graph. If the graph is only composed of communities, reordering the adjacency matrix by group would output a block matrix. Another direction emerged with <ref type="bibr" target="#b14">Fienberg &amp; Wasserman (1981)</ref> who first introduced a probabilistic model that assumes that the probability for two nodes to be connected only depends on the group to which they belong to and applied it to Sampson's monastery dataset <ref type="bibr" target="#b41">(Sampson, 1969)</ref>. Introducing a latent representation of the nodes then became popular thanks to the latent position cluster model <ref type="bibr" target="#b18">(Handcock et al., 2007)</ref> or the stochastic block model (SBM) <ref type="bibr" target="#b46">(Wang &amp; Wong, 1987;</ref><ref type="bibr" target="#b35">Nowicki &amp; Snijders, 2001)</ref>. Many extensions have been developed to incorporate valued edges, as in <ref type="bibr" target="#b31">Mariadassou et al. (2010)</ref>, as well as categorical edges in <ref type="bibr" target="#b20">Jernite et al. (2014)</ref> or to add prior information in <ref type="bibr" target="#b47">Zanghi et al. (2010)</ref>. Some developments also focused on looking for overlapping clusters <ref type="bibr" target="#b0">(Airoldi et al., 2008;</ref><ref type="bibr" target="#b25">Latouche et al., 2011)</ref> as well as dynamic networks <ref type="bibr" target="#b32">(Matias &amp; Miele, 2017;</ref><ref type="bibr" target="#b49">Zreik et al., 2017;</ref><ref type="bibr" target="#b7">Corneli et al., 2016)</ref>. The inference of SBM-based model is often done either using Markov chain monte carlo (MCMC), variational expectation maximisation (VEM) as in <ref type="bibr" target="#b10">Daudin et al. (2008)</ref> or variational Bayes expectation maximisation (VBEM) as in <ref type="bibr" target="#b26">Latouche et al. (2012)</ref>. The classification can either be deduced from the latent variable distribution or be incorporated in the optimisation strategy with a hard clustering, for instance using the classification variational expectation maximisation (CVEM) algorithm <ref type="bibr" target="#b5">(Bouveyron et al., 2018)</ref>. The choice of the number of cluster K can either be done through a model selection criterion <ref type="bibr" target="#b10">(Daudin et al., 2008;</ref><ref type="bibr" target="#b26">Latouche et al., 2012)</ref>, through a greedy search <ref type="bibr" target="#b8">(Côme &amp; Latouche, 2015)</ref> or through a non parametric schemes <ref type="bibr" target="#b22">(Kemp et al., 2006)</ref>.</p><p>Fore more insights about SBM developments, see <ref type="bibr" target="#b29">Lee &amp; Wilkinson (2019)</ref>. For reviews on statistical network modelling, we also relate to <ref type="bibr" target="#b16">Goldenberg et al. (2010)</ref> and <ref type="bibr" target="#b33">Matias &amp; Robin (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Probabilistic models for the joint analysis of texts and networks</head><p>The rise of data combining networks with texts, such as emails, social networks or co-authors articles led to developing methods using both the network and the textual information. In that regard, <ref type="bibr" target="#b48">Zhou et al. (2006)</ref> proposed the community-user topic model (CUT). This model relies on the author-topic model (AT) <ref type="bibr" target="#b39">(Rosen-Zvi et al., 2004)</ref> and adds a latent variable to the bayesian hierarchical model for modelling the communities.</p><p>Two versions are proposed in the paper, CUT1 hypothesises that a community is entirely defined as a group of users while CUT2 makes the assumption that a community is defined as a set of topics. This model is inferred using a Gibbs sampler to approximate the joint distribution of the communities, topics and users. Eventually, the communityauthor-recipient-topic (CART) model introduced in Pathak et al. ( <ref type="formula">2008</ref>) makes use of communities both at the document generation level and at the author and recipient generation level which corresponds to the network generation. However, the high number of parameters combined with the inference based on a Gibbs sampler does not allow to scale this model to large datasets. The topic-link LDA presented in <ref type="bibr" target="#b30">Liu et al. (2009)</ref> also offers a joint-analysis of texts and links in a unified framework by conditioning the generation of a link on both the topics within the documents and the community of authors. The inference relies on a variational EM approach which allows to scale to large datasets but this method only deals with undirected networks. Finally, the topic-user-community models (TUCM) was introduced in <ref type="bibr" target="#b40">Sachan et al. (2012)</ref> and was able to discover topic-meaningful communities. The main feature of this model was its capacity to incorporate different types of interactions, well-suited for social networks applications <ref type="bibr">(tweets, retweets, messages, comments, ...)</ref>. The inference relies on Gibbs sampling which can be limiting when dealing with large datasets. The stochastic topic block model (STBM) presented in <ref type="bibr" target="#b5">Bouveyron et al. (2018)</ref> was the first model to handle the simultaneous clustering of nodes and edges while keeping the inference tractable to large dataset thanks to a variational classification EM based inference. This model was extended in <ref type="bibr" target="#b2">Bergé et al. (2019)</ref> for the simultaneous clustering of rows (observations) and columns (variables). It was also adapted for dynamic networks in <ref type="bibr" target="#b6">Corneli et al. (2019)</ref>.</p><p>Unfortunately, those models only rely on word counts and cannot use the position of words within a sentence or any form of context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Our contribution</head><p>In this paper, we propose a new methodology called the embedded topics in the stochastic block model (ETSBM), to look for node partitions incorporating the connectivity patterns as well as the topics exchanged between the nodes. We will reserve the term community to groups of nodes that are densely connected together but poorly connected to the rest of the graph. In the block model literature, the term cluster denotes a group a nodes that share a similar connectivity pattern which goes beyond the concept of community. For instance, contrary to communities, a star pattern is defined by two clusters with low intra-connection and large inter-connection probabilities <ref type="bibr" target="#b26">(Latouche et al., 2012)</ref>. Such pattern is particularly common in social networks. This type of cluster cannot be retrieved by community detection methods. In this paper, we will also assume that the nodes of a same cluster share a similar use of topics proportions. To find clusters complying with this definition, (i) we propose a generative model assuming that each node belongs to a cluster and that the probability of connection between two nodes, as well as the topic proportions of a document, only depend on the clusters of the corresponding nodes. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the necessity to combine graph clustering and topic modelling in order to distinguish all four clusters and to obtain more meaningful topics for each cluster. (ii) To model the topics exchanged between the nodes, the documents are encoded with a deep neural network to benefit from their flexibility. (iii) The decoder is made of word and topic embeddings, as in <ref type="bibr" target="#b12">Dieng et al. (2020)</ref>. (iv) In this work, the documents are aggregated at the cluster level, into Q 2 meta-documents with Q the number of clusters. The meta-documents are obtained by weighting each document with the cluster membership probabilities of the corresponding nodes. In particular, our inference strategy is able to directly optimise the construction of the meta-documents through the inference procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph obtained by SBM</head><p>Graph obtained by SBM and ETM Graph obtained by ETSBM The colours of the edges indicate the most-used topic in the corresponding documents. Note that SBM alone does not provide edge information. Thus, the left network only has a single edge colour. On the left hand side, SBM clustering results uncover 3 clusters. Again, in the middle, SBM is used and uncovers 3 clusters of nodes. ETM edge information is added to the network through the 3 edge colours green, grey and blue. On the right hand side, ETSBM clustering results uncover 4 clusters. The cluster coloured in green, in the middle of the figure, is split into two cluster on the right hand side, the green one and the red one, each discussing of a different topic, the blue and grey topic respectively. The clusters of nodes of the figure on the right-hand side are coherent both in terms of topology and topics of discussion contrary to the figure in the middle.</p><p>Organisation of the paper. The embedded topics for the stochastic block model (ETSBM) is presented in Section 3. The inference and the model selection are presented in Section 4. Eventually, the model is evaluated against state of the art algorithms on synthetic data and we present results for a real word example build from tweets during the last French presidential election in Sections 5 and 6, respectively. Section 7 presents some concluding remarks and further work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The ETSBM Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background and notations</head><p>In this work, we focus on data represented by a directed graph G = {V, E}, such that V = {1, . . . , M } denotes the set of nodes and E := {(i, j) : i, j ∈ {1, . . . , M }, i ⇝ j} the set of edges, where i ⇝ j indicates that i is connected to j. The connections, or edges, are represented by a binary matrix A ∈ M M ×M ({0, 1}) such that i is connected to j, or (i, j) ∈ E, if and only if A ij = 1. In the applications we consider, this implies that node i sent textual information to j such as one or a series of emails for instance. These texts</p><formula xml:id="formula_0">are denoted W ij = {W 1 ij , . . . , W D ij ij } with D ij</formula><p>the number of documents sent from i to j and are gathered in the collection</p><formula xml:id="formula_1">W = {W ij , (i, j) ∈ E}. Each document d in W ij is a collection of words of size N d ij , i.e W d ij = {w d1 ij , . . . , w dN d ij ij</formula><p>}. The size of the vocabulary is denoted V and the words are identified by their index in the vocabulary: each word w is in {1, . . . , V }. Finally, only graphs without self loops are considered in this work, therefore A ii = 0 for all i ∈ V. Notice that all the present work can be extended to undirected networks using W ij = W ji for all pairs (i, j) such that A ij = A ji = 1. The directed case is more adequate to messages sent from i to j while the undirected case is better suited for co-authorships networks for instance.</p><p>The notation M d×p (F) will be used to denote the matrix space with matrix of dimension d × p and coefficients in F while the notation M d (N, ω) will be used to denote the multinomial distribution with parameters N ∈ N and ω ∈ ∆ d-1 where</p><formula xml:id="formula_2">∆ d-1 =: x ∈ R d : ∀i ∈ {1, . . . , d}, x i ≥ 0, d i=1</formula><p>x i = 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Modelling the interactions</head><p>In this work, we assume that each node belongs to a single cluster. Moreover, we assume that the connexion probability between two nodes only depends on the cluster memberships. Indeed, let Y i denotes the cluster membership of node i for any i ∈ {1, . . . , M }.</p><p>All Y i are assumed to follow a multinomial distribution and to be independent and identically distributed (i.i.d ), given the cluster proportions γ ∈ ∆ Q-1 , lying in the simplex</p><formula xml:id="formula_3">of dimension Q, Y i | γ i.i.d ∼ M Q (1, γ).</formula><p>Thus, each node i is associated with cluster q with probability γ q . Then, we define the cluster membership matrix Y by stacking the node cluster membership vectors</p><formula xml:id="formula_4">(Y i ) i together such that Y = (Y 1 • • • Y M ) ⊤ ∈ M M ×Q ({0, 1}). The probability of Y is given by p(Y | γ) = M i=1 Q q=1 γ Y iq q .</formula><p>(1)</p><p>Besides, the connections between nodes are supposed to be independent given their cluster memberships. Moreover, if nodes i and j are respectively in clusters q and r, an edge is assumed to be present with probability π qr ,</p><formula xml:id="formula_5">A ij | Y iq Y jr = 1, π qr i.i.d ∼ B(π qr ),<label>(2)</label></formula><p>where B(µ) denotes the Bernoulli distribution with probability µ. Thus, given the cluster memberships of the nodes Y and the probability matrix π, the probability of all node connections is given by</p><formula xml:id="formula_6">p(A | Y, π) = M i̸ =j Q q,r π A ij qr (1 -π qr ) (1-A ij ) Y iq Y jr .<label>(3)</label></formula><p>Eventually, the joint-probability of the adjacency matrix A, and the cluster memberships vector Y , is obtained by multiplying Equations ( <ref type="formula">1</ref>) and (3),</p><formula xml:id="formula_7">p(A, Y | π, γ) = p(A | Y, π)p(Y | γ).<label>(4)</label></formula><p>Combining Equations ( <ref type="formula">1</ref>), (3), and (4), we retrieve the SBM distribution <ref type="bibr" target="#b10">(Daudin et al., 2008)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Modelling the texts</head><p>Our approach extends ETM to capture information of groups of texts. Essentially, texts are assumed to be generated according to a mixture of topics with latent topic vectors only depending on node clusters. More precisely, a text sent from node i in cluster q to node j in cluster r is assumed to have a logistic-normal topic proportion vector θ qr = (θ qr1 , . . . , θ qrK ) ⊤ ∈ ∆ K-1 , with the number of topics K fixed beforehand.</p><p>It is obtained by applying the softmax function to a Gaussian random vector δ qr ,</p><formula xml:id="formula_8">δ qr ∼ N (0 K , I K ), θ qr = softmax(δ qr ), where softmax(x) = K k=1 e x k -1 (e x 1 , . . . , e x K ) ⊤ .</formula><p>In the rest of this paper, the notation θ = (θ qr ) 1≤q,r≤Q is used to refer to the topic proportions while δ = (δ qr ) 1≤q,r≤Q will refer to the sampling of the random variable. If two nodes i and j are connected and if they are respectively in cluster q and r, the words in document W ij are assumed to be i.i.d. Indeed, the n-th word of the d-th documents is assumed to be distributed according a mixture of topics conditionally on the node clusters,</p><formula xml:id="formula_9">W dn ij | Y iq Y jr A ij = 1, θ qr , α, ρ ∼ M V (1, θ ⊤ qr β),<label>(5)</label></formula><p>where the matrix Therefore, the probability of texts can be computed as follow:</p><formula xml:id="formula_10">β = (β 1 • • • β K ) ⊤ ∈ M K×V (R)</formula><formula xml:id="formula_11">p(W | Y, A, θ, α, ρ) = M i̸ =j D ij d=1 p(W ij | Y i , Y j , A ij = 1, θ, α, ρ) = M i̸ =j D ij d=1 N d ij n=1 Q q,r V v=1 K k=1 θ qrk β kv W dnv ij A ij Y iq Y jr = Q q,r V v=1 K k=1 θ qrk β kv W v qr . (<label>6</label></formula><formula xml:id="formula_12">)</formula><p>The number of time the word v of the dictionary is used in texts sent from cluster q to cluster r is denoted</p><formula xml:id="formula_13">W v qr = M i̸ =j D ij d=1 N d ij n=1 W dnv ij A ij Y iq Y jr .</formula><p>Here, W qr = (W 1 qr , . . . , W V qr ) ⊤ ∈ N V shall be designated as meta-document (q, r). Moreover, we shall use the bag of words notations such that for any connected pair of nodes (i, j) ∈ E, sent from i to j. The model is represented in Figure <ref type="figure" target="#fig_2">2</ref>.</p><formula xml:id="formula_14">W ij = (W 1 ij , . . . , W V ij ) ⊤ ∈ N V with for any v ∈ {1, . . . , V }, W v ij represents</formula><p>3.4. Distribution of the model and links with SBM and ETM.</p><p>Given a cluster configuration Y , the joint probability of the model is obtained using Equations ( <ref type="formula" target="#formula_6">3</ref>) and ( <ref type="formula" target="#formula_11">6</ref>)</p><formula xml:id="formula_15">p(A, W | Y, α, ρ) = p(W | Y, A, α, ρ)p(A | Y, π).<label>(7)</label></formula><p>At this point, we emphasise that meta-documents between pairs of clusters of nodes are constructed using the cluster memberships Y and the node connections A. Assuming that the cluster membership Y is available as well as all the network information holded by π and γ, the model we propose would simply correspond to ETM applied on the meta-documents (W qr ) 1≤q,r,≤Q , computed with the available Y .</p><p>On the other hand, if no texts are exchanged between nodes or the texts are not available, the distribution would reduce to the second term of Equation <ref type="formula" target="#formula_15">7</ref>. In that case, the conditional distribution of a standard SBM <ref type="bibr" target="#b9">(Daudin et al., 2006)</ref> is recovered. It is also worth noticing that if a Dirichlet prior is assumed on the topic proportion instead of a logistic-normal, and no factorisation in a embedded latent space is considered, the model corresponds to STBM. By construction, ETSBM generalises SBM and ETM to incorporate both textual data and network information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Inference</head><p>This section presents the Bayesian framework considered for inference. It also describes the variational-bayes EM algorithm used to maximise the integrated joint likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Bayesian framework for the graph modelling part</head><p>First, a Dirichlet distribution is assumed as a prior distribution on the proportions γ of nodes in each cluster,</p><formula xml:id="formula_16">γ ∼ Dir Q (γ 0 ). (<label>8</label></formula><formula xml:id="formula_17">)</formula><p>where γ 0 is set to (1, . . . , 1) ∈ R Q , which corresponds to a uniform prior on the simplex.</p><p>Moreover, each coefficient of the probability matrix π ∈ M Q×Q (R), is assumed to be sampled from from a Beta distribution, such that for any pair (q, r) ∈ {1, . . . , Q} 2 ,</p><formula xml:id="formula_18">π qr i.i.d ∼ Beta(a, b).</formula><p>In particular, a and b are set to 1. Thus, the Beta prior corresponds to a Uniform distribution between 0 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Variational inference</head><p>Eventually, the integrated joint log-likelihood is given by:</p><formula xml:id="formula_19">log p(A, W | α, ρ) = log Y δ γ π p(A, W, Y, π, γ, δ | α, ρ)dπdδdγ .<label>(9)</label></formula><p>Unfortunately, this quantity is intractable since it requires computing it for the Q M configurations of Y , which is naturally computationally too demanding. Moreover, the integral with respect to δ is not tractable either because of the softmax function. Thus, it cannot be optimised directly. However, it is possible to overcome this issue using a variational-bayes expectation-maximisation algorithm (VBEM) <ref type="bibr" target="#b1">Attias (1999)</ref>. This comes handy as it makes the inference scalable to large datasets.</p><p>The variational approach consists in splitting Equation ( <ref type="formula" target="#formula_19">9</ref>) in two terms using a surrogate distribution on Y, π, γ and δ, denoted R(Y, π, γ, δ).</p><p>Proposition 4.1. Denoting R(•), a distribution on Y, π, γ and δ, the integrated joint log-likelihood can be decomposed as follow:</p><formula xml:id="formula_20">log p(A, W | α, ρ) = L (R(•); α, ρ) + KL(R(•)||p(Y, π, γ, δ | A, W, α, ρ)),</formula><p>where</p><formula xml:id="formula_21">L (R(•); α, ρ) = Y π,γ,θ R(Y, π, γ, δ) log p(A, W, Y, π, γ, δ | α, ρ) R(Y, π, γ, δ) dπdδdγ.</formula><p>Proof. The proof is provided in Appendix A.</p><p>To make L (R(•); α, ρ) tractable, we use the following mean-field assumption :</p><formula xml:id="formula_22">R(Y, π, γ, δ) = R(Y )R(π)R(γ)R (δ) . (<label>10</label></formula><formula xml:id="formula_23">)</formula><p>Following the optimality results of <ref type="bibr" target="#b26">Latouche et al. (2012)</ref>, we impose the following variational distributions:</p><formula xml:id="formula_24">R(Y ) = M i=1 R(Y i ) = M i=1 M Q (Y i ; 1, τ i ), R(π) = Q q,r=1 R(π qr ) = Q q,r=1</formula><p>Beta(π qr ; πqr1 , πqr2 ),</p><formula xml:id="formula_25">R(γ) = Dir Q (γ; γ).<label>(11)</label></formula><p>Each vector τ i is of size Q and encodes the (approximate) posterior probabilities for node i to be in each cluster. Given τ = (τ i ) i , the set of posterior cluster membership probabilities, for any pair (q, r) the corresponding expected meta-document can be computed as</p><formula xml:id="formula_26">Wqr = i̸ =j τ iq τ jr W ij .<label>(12)</label></formula><p>By construction, the v-th element of vector Wqr is the expected pseudo count of word v for all documents sent from nodes in cluster q to nodes in cluster r. Finally, the variational distribution on latent topic proportions is assumed to be:</p><formula xml:id="formula_27">R(δ) = Q q,r=1 R(δ qr ) = Q q,r=1 N δ qr ; µ qr (τ, ν), diag(σ 2 qr (τ, ν)) ,<label>(13)</label></formula><p>with (µ qr (τ, ν), σ qr (τ, ν)) ⊤ = f ( W norm qr (τ ); ν) the output of a parametric function, typically a (deep) neural network, with parameters denoted ν. Hereafter, the ETM en-coder will be used as the function f parametrised by ν. The normalised expected meta-</p><formula xml:id="formula_28">documents W norm qr (τ ) = V v=1</formula><p>W v qr (τ ) -1 Wqr (τ ) ∈ R V are then given to the encoder which outputs the mean and variance vectors (µ qr (τ, ν), σ qr (τ, ν)) ⊤ of the posterior distribution. Our inference strategy is inspired by <ref type="bibr" target="#b12">Dieng et al. (2020)</ref> and finds its roots in the original work of <ref type="bibr" target="#b24">Kingma &amp; Welling (2014)</ref> for classical data. However, as we shall see, a critical property of our methodology is that the (approximate) posterior allocation probabilities τ will change through the updates and so are the inputs of the encoder.</p><p>In all experiments we carried out, we used a 3-layer architecture with 800 units for the hidden layers, as originally proposed in <ref type="bibr" target="#b12">Dieng et al. (2020)</ref>. In order not to increase the number of parameters ν linearly with the number of pairs of groups, amortised inference is used as advocated in <ref type="bibr" target="#b15">Gershman &amp; Goodman (2014)</ref> or <ref type="bibr" target="#b24">Kingma &amp; Welling (2014)</ref>.</p><p>Proposition 4.2. Using the assumptions describes in Equations ( <ref type="formula" target="#formula_22">10</ref>), ( <ref type="formula" target="#formula_25">11</ref>) and (13), the ELBO, which is a functional of the variational distribution, reduces to a function of the variational parameters and can be split in two terms associated with the network and with the texts respectively:</p><formula xml:id="formula_29">L (R(•); α, ρ) = L (τ, π1 , π2 , γ, ν; α, ρ) (14) = L net (τ, π1 , π2 , γ; α, ρ) + L texts (τ, ν; α, ρ),<label>(15)</label></formula><p>where π1 = (π qr1 ) qr , π2 = (π qr2 ) qr .</p><p>Proof. The proof and the exact value of the ELBO is detailed in Appendix A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Optimisation and Algorithm</head><p>We now aim at maximising the ELBO with respect to the variational parameters π, γ, τ and ν and to the parameters ρ and α. On the one hand, following Latouche et al.</p><p>(2012), the variational parameters π and γ only depend on τ and are updated as follow:</p><formula xml:id="formula_30">γq = γ 0q + M i=1 τ iq πqr1 = π 0 qr1 + M i̸ =j τ iq τ jr X ij , πqr2 = π 0 qr2 + M i̸ =j τ iq τ jr (1 -X ij ). (<label>16</label></formula><formula xml:id="formula_31">)</formula><p>On the other hand, ν, as well as ρ and α are optimised by a stochastic gradient descent algorithm using Pytorch automatic differentiation <ref type="bibr" target="#b37">(Paszke et al., 2019)</ref> and the Adam optimiser <ref type="bibr" target="#b23">(Kingma &amp; Ba, 2014)</ref> with a learning rate of 10 -4 . Once both parts are done, we only need to update τ using the already up-to-date parameters. To do so, we switch from τ lying on the simplex ∆ Q-1 to the unconstrained space R Q-1 using for any i ∈ V and q ∈ {1, . . . , Q -1}:</p><formula xml:id="formula_32">ξ iq = ln(τ iq ) -ln(τ iQ ).</formula><p>We then use the automatic differentiation and the Adam optimiser with a learning rate of 0.55 to maximise the ELBO with respect to ξ. It is worth emphasising that the ELBO is optimised over the whole set of allocation probability vectors τ = (τ i ) i contrary to STBM which looks for a hard allocation of nodes to clusters, one allocation being optimised at a time, all the others being fixed. Moreover, by optimising the entry of the encoder through τ , thus looking for an optimal allocation of documents to pairs of clusters, the moves in τ aim at uncovering the optimal direction in the posterior distribution in (θ qr ) qr maximising the ELBO. In that regard, ETSBM has links with the quasi branching bound algorithm of <ref type="bibr" target="#b21">Jouvin et al. (2021)</ref> for document clustering. Considering a unique core for illustration, on an Intel(R) Core(TM) i7-10875H 2.30 GHz CPU and a Nvidia GeForce RTX 2080 Super 8 Go GPU, it takes about 15 seconds to analyse a dataset with 100 nodes and 1 000 documents. Moreover, studying a dataset with more than 200 000 documents, characterising all the connections between 1500 nodes, is done in approximately 6 minutes. In practice, we emphasise that the running time can be reduced even more by considering extensive parallelisation as well as stochastic variational inference strategies adapted for networks as in <ref type="bibr" target="#b17">Gopalan &amp; Blei (2013)</ref>. The Python implementation of the complete methodology we propose is available at <ref type="url" target="https://plmlab.math.cnrs.fr/rboutin/etsbmpackage">https://plmlab.math.cnrs.fr/rboutin/etsbm package</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Model selection</head><p>Finally, the selection of the number of cluster Q is performed using the ELBO. It is useful to remind that the aim of the model is to select the number of clusters providing the more meaning. Therefore, relying on <ref type="bibr" target="#b26">Latouche et al. (2012)</ref>, we take advantage of the Bayesian framework that automatically penalises the complexity of the model with respect to Q. The best number of cluster Q is then selected by estimating the parameters for models with different number of cluster Q and keeping the one with the highest ELBO.</p><p>Our experiment Section 2 confirms that this procedure provides a relevant model selection criterion. In this paper, the number of topics K is not selected. Indeed, we choose to keep a high K as advocated in <ref type="bibr" target="#b12">Dieng et al. (2020)</ref>. In practice, once the inference of the topics is done, a classical approach consists in focusing the interpretation on the results associated with the most frequent topics. As we shall see, in the experiment section, provided that the value of K chosen is large enough, the proposed procedure provides an accurate estimate of Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Numerical experiments</head><p>In this section, a series of experiments is presented to assess the proposed methodology.</p><p>First, three scenarii used for benchmarking are described. Second, an illustration of the results provided by ETSBM on a simulated dataset from one of the scenarii is given.</p><p>Then, results from experiments to evaluate the model selection criterion on the three scenarii considered are brought. Moreover, various strategies to initialise ETSBM are compared. Finally, an extensive set of experiments on the three scenarii with three levels of difficulty is carried out to evaluate the clustering performances of ETSBM against competitive algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Simulation setup</head><p>The networks with textual edges are generated following three scenarii A, B, C, as originally introduced in <ref type="bibr" target="#b5">Bouveyron et al. (2018)</ref>.</p><p>Sampling networks with textual edges.</p><p>• Scenario A is composed of three communities, each defining a cluster, and four topics. By definition, a community is defined such that more connections are present between nodes of the same community. For each cluster, a specific topic is employed to sample all the documents associated with the corresponding intra-cluster connections. Besides, an extra topic is considered to model documents exchanged between nodes from different clusters. Thus, by construction, the clustering structure can be retrieved either using the network or the texts only.</p><p>• Scenario B is made of a single community and three topics. Thus, all nodes connect with the same probability. Then, the community is split into two clusters with their respective topics. An extra topic is used to model documents exchanged between the two clusters. Therefore, in such a scenario, the network itself is not sufficient to find the two clusters but the documents are.</p><p>• Scenario C is composed of three communities and three topics. Two of the communities are associated with their respective topics, say t 1 and t 2 . Moreover, following the previous scenario, the third community is split in two clusters, one being associated with topic t 1 and the other with t 2 . Thus, considering both texts and topology, each network is actually made of four node clusters. Fundamentally, both textual data and the network itself are necessary to uncover the clusters. This scenario will be of major interest in this experiment section since it allows to ensure that the two sources of information are correctly used to retrieve partitions.</p><p>The edges holding the documents are constructed by sampling words from four BBC articles, focusing each on a given topic. The first topic deals with the UK monarchy, the second with cancer treatments, and the third with the political landscape in the UK. The last topic deals with astronomy. In the general setting, for all scenarii, the average text length for the documents is set to 150 words. The parameters used to sample data from the three scenarii are given in Table <ref type="table" target="#tab_0">1</ref>. Moreover, three examples of networks generated from A, B and C are presented in Figure <ref type="figure" target="#fig_3">3</ref>.</p><p>Clustering performance evaluation. The main criterion used in the following to evaluate the clustering performances of the different strategies is the adjusted random index (ARI).</p><p>ARI measures how close two partitions are from one another. The closer ARI is to 1, the better the results are. A random cluster assignment leads to an ARI of 0, while a perfect retrieval of the cluster memberships gives an ARI of 1.</p><p>Different levels of difficulties. To evaluate ETSBM against state of the art STBM in Sections 5.3 and 5.5, two levels of difficulty are introduced. The first one, named Hard 1, makes it particularly hard to distinguish connectivity patterns by using an intra-cluster connectivity probability of 0.2. In Table <ref type="table" target="#tab_0">1</ref>, it corresponds to ϵ = 0.2 instead of 0.01. The second one, named Hard 2, introduces difficulty on the text part by using smaller texts of 110 words on average instead of 150 and by adding noise. In our case, this translates</p><formula xml:id="formula_33">Scenario A Scenario B Scenario C Q (clusters) 3 2 4 K (topics) 4 3 3 Communities 3 1 3 π qr (connection probabilities) η = 0.25, ϵ = 0.01    η ϵ ϵ ϵ η ϵ ϵ ϵ η    η η η η       η ϵ ϵ ϵ ϵ η ϵ ϵ ϵ ϵ η η ϵ ϵ η η      </formula><p>Topics between pairs of clusters (q, r)  into fixing:</p><formula xml:id="formula_34">   t 1 t 4 t 4 t 4 t 2 t 4 t 4 t 4 t 3    t 1 t 3 t 3 t 2       t 1 t 3 t 3 t 3 t 3 t 2 t 3 t 3 t 3 t 3 t 1 t 3 t 3 t 3 t 3 t 2       Sufficient information</formula><formula xml:id="formula_35">θ qr = (1 -ζ)θ ⋆ qr + ζ * 1 K , . . . , 1 K ⊤ ,<label>(17)</label></formula><p>with ζ = 0.7. Thus, for each pair of clusters (q, r), the texts are sampled according to a mixture between a multinomial distribution with probability 1 on the corresponding topic and a uniform distribution over all topics considered. Finally, the intra-cluster connection probability is decreased from 0.2 to η = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">An introductory example</head><p>A first glimpse at the ETSBM results on a single network simulated with Scenario C is presented here. In Figure <ref type="figure" target="#fig_4">4</ref>, the evolution of the ELBO and ARI values are monitored at each iteration of the inference of ETSM applied on this single simulated network. As we can see, both the ELBO and the ARI increase after each iteration. In particular, starting from the clustering initialisation with an ARI value of 0.62, the algorithm converges to a value of 1, characterising a perfect cluster recovery. This figure illustrates the ability of the methodology proposed to retrieve the true node partition, by combining the textual and network data.</p><p>In addition, Figure <ref type="figure" target="#fig_6">5</ref> provides representations for the expected posterior estimates π and γ computed as follows πqr = πqr1 /(π qr1 + πqr2 ) and γq = γq /( Q r=1 γr ). We emphasise that the matrix characterises the connexion probabilities between clusters with a 10 -2 rounding. It matches the expected connectivity structure described in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Eventually, the topics learnt as well as the clustering results on the network are presented in Figure <ref type="figure" target="#fig_7">6</ref>. In the network representation, the node colours correspond to the cluster memberships while the edge colours indicate the most used topic in the corresponding documents. Moreover, for each topic t k with k ∈ {1, 2, 3}, the 10 words with the highest probabilities, according to the corresponding topic vector β k , are displayed.</p><p>The three topics presented are well-separated and can be identified as the topics dealing  respectively with astronomy, the political landscape in the UK, and the UK monarchy, as expected. In addition, four node clusters have been retrieved and the edge topics, or colours, match the description of the Scenario C setup. To conclude, ETSBM successfully render both the network topology and the edge topics.</p><p>Finally, Figure <ref type="figure">7</ref> provides a high level representation of the results. On the one hand, the "meta-nodes" represent ETSBM clusters and their size is proportional to the number of nodes assigned to the corresponding clusters. Moreover, the "meta-node" colours are consistent with the colours in Figure <ref type="figure" target="#fig_7">6</ref>. On the other hand, the edges represent the meta-documents. We recall that they correspond to the expected posterior estimate of a document for a given pair of clusters. The edge colours correspond to the most used topic within the meta-document. The edge widths are determined by the posterior probabilities of connections between pairs of clusters. This figure underlines ETSBM capability to produce intelligible and accurate data summary. We emphasise that graphs with thousands of edges, that sometimes cannot be represented because of memory issues, are here able to be summarised in easy-to-read meta-graphs.</p><p>To conclude, this introductory example showed the ETSBM capacity to render meaningful summaries by combining both network and text information. It is worth reminding that, since it comes from Scenario C, those results could not have been retrieved with models handling only network or texts as SBM, LDA or ETM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Effect of the initialisation</head><p>This experiment aims to evaluate the impact of the initialisation on the final performance of our methodology. The networks are generated according to the Hard 2 difficulty, to easily visualise the differences between the tested configurations. Moreover, the experiment is performed on Scenario C to ensure both the network and textual data are used. Three different initialisations are compared: clusters may be randomly assigned to the nodes (random), or initial clusters can be determined by a K-Means algorithm fitted on the adjacency matrix A. Finally, the dissimilarity procedure proposed in Bouveyron et al. ( <ref type="formula">2018</ref>) is evaluated as the last initialisation strategy (dissimilarity). It uses both network and textual information to build a similarity matrix based on the topics discussed between nodes. Then, a K-means algorithm is performed on this similarity matrix to find a cluster allocation for each node. This initialisation strategy requires to provide the topic proportion of each edge. Thus, ETM is trained on the texts and the estimated topic proportions (θ ij ) (i,j)∈E are used for the dissimilarity initialisation. Figure <ref type="figure" target="#fig_8">8</ref> presents the ARI results with, for each initialisation strategy, a boxplot of the raw initialisation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETSBM clustering results represented with a meta-graph</head><p>Figure <ref type="figure">7</ref>: Meta representation of ETSBM results. On the one hand, the clusters are represented by the node colours, the node widths are proportional to the expected posterior estimate of the cluster proportions, and their colours correspond to the same cluster colours as in the network in Figure <ref type="figure" target="#fig_7">6</ref>. On the other hand, the edges are coloured as the most used topic within the meta-document and the widths are proportional to the posterior probabilities of connections between clusters.</p><p>and of ETSBM clustering.</p><p>While the random initialisation is close to 0 for ARI, both the K-means and the dissimilarity initialisation fluctuates in terms of ARI, with no clear advantage for one of the two strategies. However, ETSBM provides much better results with the dissimilarity initialisation than with K-means. It is also worth noticing that the gap between the random and K-Means initialisations has largely been closed by ETSBM algorithm. One possibility is that the model suffers the same flaws as SBM, which is for the ELBO to fall into local minimum. It is possible that the use of texts in the dissimilarity limits this effect. Therefore, we will only use the dissimilarity initialisation in the rest of the paper as it provides the best results in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Model selection</head><p>This experiment aims to assess the efficiency of the model selection criterion, presented in Section 4.4. Let us remind that we do not aim at selecting the number of topics K since it is handled afterwards. As a consequence, the model selection criterion is evaluated for different values of K to ensure that the performances remain high, in all cases. For each scenario, 50 networks are sampled following the setup described in Section 5.1. For each network, ETSBM parameters are estimated taking the best initialisation out of 10. Table <ref type="table" target="#tab_1">2</ref> presents the percentage of time a number Q is selected using the strategy proposed in Section 4.4 over the 50 networks, for each K value. It is worth noticing that the right model is selected more than 75% of the time, except for the Scenario B with K = 5, slightly bellow with 68%. In addition, as advocated before, for K = 10, the right model is selected more than 80% of the time in each scenario. This experiment illustrates the capacity of the model selection criterion to retrieve the number of clusters. Moreover, keeping a high value of K is confirmed to be compatible with an efficient cluster number selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Benchmark study</head><p>To end this section, ETSBM is evaluated against state of the art clustering algorithms for STBM. We recall that STBM is currently the only algorithm capable of simultaneously analysing the texts on the edges as well as the node connections to cluster the nodes. In  order to provide baselines, we also give the results obtained with SBM as well as a spectral clustering algorithm (SC) presented in <ref type="bibr" target="#b42">Shi &amp; Malik (2000)</ref>; Von <ref type="bibr" target="#b45">Luxburg (2007)</ref>, with a radial basis function as a kernel and a normalised symmetric Lagrangian. Those methods are evaluated on the three levels of difficulty presented in Section 5.1. Besides, results for LDA as well as ETM for text clustering are also provided. For each level of difficulty and each scenario, Table <ref type="table" target="#tab_2">3</ref> displays the mean and the standard deviation of the ARI values obtained over 50 graphs. Both the node and edge clusters ARI are provided but we recall that the main interest of this work concerns the node clustering performances. In the Easy and Hard 1 settings, the ARI is always 1, which indicates that the true partitions are successfully retrieved by ETSBM and STBM. On the contrarty, SBM and SC are not able to distinguish clusters in Scenario B since all nodes connect one another with the same probability. Identically, in Scenario C, SBM and SC alone cannot differentiate the nodes highly connected but discussing of different topics. For instance, in the Easy case, this translates into an ARI of 0.01 and 0.69 respectively for SBM, and 0.00 and 0.63 respectively for SC. In the Hard 2 setting, ETSBM node clustering significantly outperforms STBM. In particular in Scenario C, Hard 2, ETSBM results reach an ARI of 0.91 against 0.63 for STBM. Even though it is not the main focus of this work, the edge ARI is always higher than 0.84, which is satisfactory, and is competitive when not higher than STBM. These significant gaps in the noisy settings highlight ETSBM clustering improvement upon STBM. To conclude, our experiments strongly indicates that ETSBM node clustering performances are either the same or significantly better than STBM. In this section, we now consider the analysis of a real dataset. We start by describing the context of the study. The dataset is then presented and the results obtained with ETSBM are given. To complete this study, the results obtained with SBM and ETM employed independently are also provided. Finally, a comparison of these results with the ones obtained with ETSBM is performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Context</head><p>This section presents a use case on a Twitter dataset dealing with the French pres- and two authors of this article <ref type="bibr" target="#b27">(Laurent, 2022)</ref>. Newspapers such as Le Monde may be interested in having a good understanding of the global dynamics on social media during an electoral period, in order to understand the interest of the public opinion. Thus, interpretable topics and meaningful clusters may help them getting a grasp on the core factors interesting the elector. During the last 50 years, French political landscape has been split between two main parties, the left-democrat, mainly represented by the socialist party, and the right-liberal, represented by Les Républicains (formerly UMP). A shift occurred in 2017 when a three-way split between the far-left political families, the centrists, or liberals, and the far-right emerged. This analysis aims at capturing the major topics discussed prior to the election. In addition, we want to understand the way those topics shape user groups interactions. However, this study does not aim at making any form of prediction about the election.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Dataset construction and method</head><p>In the collected data, each node represent a Twitter account. An account i is connected to j if the former retweeted the later or if i "mentioned" j with an "@account name" in a tweet. The text on the edges are the tweet themselves. Our database has been created by saving any tweet talking about one of the twelve candidates. If several tweets appear from i to j, the edge (i, j) holds all those tweets stack together. We only keep edges with text length greater than 100 characters. Then, a lemmatisation procedure is used to reduce the vocabulary size. The "stopwords", defined as non-informative words such as "and" or "it", are withdrawn, as well as numeric characters and words with a length inferior to 3 characters. In the end, we keep the largest connected component of this graph. Our dataset holds 2, 730 nodes and 403, 768 edges. This means that the graph is sparse at 94.58%. We emphasise that this level of sparsity is quite high and makes the data analysis particularly challenging. The number of topics is set to K = 20. Also, for each Q value, the model is trained for 10 different initialisations and the best result among those 10, ELBO wise, is kept. Then, the number of clusters is selected using our model selection criterion. Figure <ref type="figure" target="#fig_11">9</ref> shows that the most appropriate model according to our criterion corresponds to a number of clusters Q = 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results</head><p>The meta-graph presented in Figure <ref type="figure" target="#fig_13">10a</ref> is a high-level representation of the network.</p><p>The "meta-nodes" correspond to ETSBM clusters and the edges to the meta-documents as defined in Equation ( <ref type="formula" target="#formula_26">12</ref>). A translation of the top words is provided in Appendix B.12. It is interesting to note the two types of clusters uncovered. In particular, Cluster 5 is composed of central accounts such as French politicians and their communication teams, for instance Jean-Luc Mélenchon, Guillaume Peltier, En Marche #avecvous, les</p><p>Républicains or Eléonore Lhéritier. Some popular French media such as BFMTV, Le Figaro, Valeurs actuelles, franceinfo are also in this cluster. On average, the accounts in this cluster have been retweeted or mentioned 299 times against 12 times for the whole network. This cluster does not correspond to a political trend but to accounts with a high level of interactions with the rest of the graph. Despite the small size of this cluster, composed of 25 nodes, ETSBM is able to detect it and to render its central function as a relay of information to other parts of the graph. This is stressed by Topic 1, the main topic discussed within Cluster 5. It regards the election as a democratic process:</p><p>"round", "vote", "power", "president", "first" which we assume stands for "first round".</p><p>This core cluster is retweeted differently by the four other clusters which on the contrary hold clear political trends. Cluster 2 and Cluster 3 are interested in Jean-Luc Mélenchon (Topic 2) and left parties in general (Topic 4) but they seem to differ in terms of function.</p><p>Cluster 2 clearly relays information about Jean-Luc Mélenchon and is interacting with Cluster 4, interested in Eric Zemmour. On the contrary, Cluster 3 seems to only relegate contents without being retweeted. Eventually, Cluster 4, interested in Eric Zemmour (Topic 5), appears to relegate contents from the central accounts as well as sharing many of its own content. This dynamic differs from Cluster 1 interested in Emmanuel Macron (Topic 3), which mainly retransmits informations without many self interactions. To conclude, the three-way split of the French political landscape is rightfully captured.</p><p>ETSBM is also able to detect subtleties such as a split within the left-wing, with the orange cluster interested only in Jean-Luc Mélenchon and the biggest one exchanging about different left-political front runners, Jean-Luc Mélenchon, Yannick Jadot, Fabien</p><p>Roussel and Anne Hidalgo. ETSBM combines the connection information, for instance all clusters are connected to Cluster 5, and the topics information, for instance Cluster 2 and Cluster 3 should be separated, to provide relevant insights about the information organisation within the social network. This level of detail is promising and highlights how ETSBM gives a better comprehension of the complex dataset at our disposal.    Comparison with ETSBM results. The topics in Figure <ref type="figure" target="#fig_15">11</ref> do not provide much information to understand the content of the connections in the network. In particular, Topic 2, which is general and not specific, is the most used topic in the meta-network. This can be explained by the independence between the construction of the clusters and of the content of the tweets. Therefore, the meta-documents exchanged between clusters have no reason to be specific or to share a common topic. As a result, the Topic 2 emerges as the most used topic between clusters. Compared to ETSBM results, the connections are not informative and the topics exchanged are too general to be considered for interpretation. We emphasise that among the 20 topics estimated by ETM, some are very informative but do not emerge in the meta-graph, backing the claim that the clusters are not meaningful. In addition, the number of clusters selected by the ICL (8), is higher than the number of clusters selected by ETSBM (5). Having a low number of clusters can help make the results easier to understand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Comparison with SBM and ETM fitted independently</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and discussion</head><p>The embedded topics for the stochastic block model (ETSBM) is well suited to simultaneously find meaningful node and edge clusters. In addition, ETSBM provides an intelligible high-level representation of the graph. It can be used both on directed and undirected graphs and is suited for large datasets thanks to the variational inference.</p><p>The numerical experiments showed that the ELBO is a relevant model selection criterion to estimate the number of node clusters Q in this Bayesian framework. Moreover, this criterion keeps provide a good estimate of Q for a high number of topics K. In the end, a use case on a Twitter dataset proved the usefulness of the method. ETSBM clustering results were both meaningful and humanly intelligible. Further work may be directed in the study of theoretical foundations of the model selection criterion proposed. Adding temporal information concerning the connectivity patterns and the topics modelling could also contribute to obtain useful information on the data.</p><p>The Kullback-Leibler divergence between two Gaussian variables has a close form and is easy to compute. All the terms can be computed except for the expectation of T δqr ij that can be approximated using a Monte-Carlo estimator, by drawing S samples for each pair (q, r), such that:</p><p>ϵ s ∼ N (0, I K ), δ s qr = µ qr (τ, ν) + σ qr (τ, ν) ⊙ ϵ s , θ s qr = softmax(δ s qr ).</p><p>with ⊙ denoting the Hadamard product. Thus, for each pair of nodes (i, j) and pair of clusters (q, r), the estimate is given by:</p><formula xml:id="formula_36">T qr ij = S -1 S s=1 T δ s qr ij .</formula><p>Plugging T qr ij in the Equation (A.1) gives the final estimator of the ELBO.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of results on a simulated network with the use of SBM on the left, SBM and ETM in the middle and ETSBM on the right. The colours of the nodes indicate the cluster of the vertices.The colours of the edges indicate the most-used topic in the corresponding documents. Note that SBM alone does not provide edge information. Thus, the left network only has a single edge colour. On the left hand side, SBM clustering results uncover 3 clusters. Again, in the middle, SBM is used and uncovers 3 clusters of nodes. ETM edge information is added to the network through the 3 edge colours green, grey and blue. On the right hand side, ETSBM clustering results uncover 4 clusters. The cluster coloured in green, in the middle of the figure, is split into two cluster on the right hand side, the green one and the red one, each discussing of a different topic, the blue and grey topic respectively. The clusters of nodes of the figure on the right-hand side are coherent both in terms of topology and topics of discussion contrary to the figure in the middle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>corresponds to the distribution over the vocabulary for each topic such that β k = softmax ρ ⊤ α k for any k ∈ {1, . . . , K}.The matrix ρ ∈ M L×V (R) corresponds to the matrix of the vocabulary embedded into an L-dimensional vector space, and α = (α 1 • • • α K ) ∈ M L×K (R) the matrix of topics represented into the same vector space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graphical representation of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of each scenario is presented. The node colours denote the cluster memberships and the edge colours denote the most-used topic within the corresponding documents. The Scenarii A, B and C are composed of 3, 1 and 3 communities respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Evolution of ETSBM ELBO and ARI (y-axis) at each iteration (x-axis) on the Scenario C after an initialisation with the K-means algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: On the left hand side, the expected posterior estimate of the connectivity matrix π provided by ETSBM. On the right hand side, the expected posterior estimate of the cluster proportions γ. The graph was generated following Scenario C.</figDesc><graphic coords="20,108.17,99.66,183.41,183.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: On the left hand side, the top 10 words of each topic according to ETSBM results. Thus, for each topic t k with k ∈ {1, 2, 3}, the 10 words with the highest probability values, according to the topic vector β k , are displayed. On the right hand side, ETSBM clustering result is illustrated. The node colours indicate the node clusters while the edge colours correspond to the most used topic within the document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: This figure displays the boxplots of the initialisation ARI (the boxplot without stripe) and of ETSBM clustering ARI with the same initialisation (the boxplot with stripes). This experiment was performed on 50 networks generated following Scenario C in the Hard 2 setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>idential election of 2022. The election resulted in Emmanuel Macron being re-elected as President of France. The objective is to use ETSBM to capture the global trends on Twitter before the first round of the French presidential election in April 2022. The network has been constructed using tweets collected by the Linkfluence, a Meltwater company, during a collaboration between journalists of the French newspaper Le Monde</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: After running ETSBM with different number of clusters Q, the ELBO suggest to keep five clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Meta-network obtained with ETSBM. Each node corresponds to a cluster and the node widths are proportional to the posterior cluster proportions. On the other hand, the edges are coloured as the most used topics within the meta-documents and the widths are proportional to the posterior probabilities of connections between clusters. The most important words of the topics presented in the meta-graph above for ETSBM. A translation is provided in FigureB.12 of the appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: ETSBM results on the Twitter dataset for Q = 5 clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Meta-network estimated with SBM. Each node corresponds to a cluster and the node widths are proportional to the cluster proportions. On the other hand, the edges are coloured as the most used topics of the documents exchanged between the pairs of clusters found by SBM alone. Such topics are obtained by applying ETM alone. The widths of the edges are proportional to the probabilities of connections between clusters. Meta-topics estimated with ETM on the Twitter dataset. A translation is provided in FigureB.13 of the appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: SBM and ETM results on the Twitter dataset for Q = 8 clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure B. 12</head><label>12</label><figDesc>Figure B.12 provides a translation of topics found by ETSBM on the real dataset and appearing in the meta-network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure B. 13</head><label>13</label><figDesc>Figure B.13 provides a translation of topics found by ETM on the real dataset and appearing in the meta-network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Detail of the three simulation scenarii to evaluate our model.</figDesc><table><row><cell>to uncover</cell><cell>Network</cell><cell>Topics</cell><cell>Network &amp; Topics</cell></row><row><cell>the clusters</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>This table presents the percentage of time a number of clusters have been selected on 50 simulated networks. The experiment is repeated for different values of K, and for Scenario A, B and C. For instance, in Scenario A with K = 3, the model with Q = 3 clusters was selected in 90% of cases.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Benchmark of our model against STBM, SBM, SC and LDA. When a model does not provide an information, a line is displayed instead of the result. For instance, SBM does not provides edge information.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Scenario A</cell><cell cols="2">Scenario B</cell><cell cols="2">Scenario C</cell></row><row><cell></cell><cell></cell><cell>Node ARI</cell><cell>Edge ARI</cell><cell>Node ARI</cell><cell>Edge ARI</cell><cell>Node ARI</cell><cell>Edge ARI</cell></row><row><cell></cell><cell cols="4">ETSBM 1.00 ± 0.00 0.99 ± 0.03 1.00 ± 0.00</cell><cell cols="3">1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00</cell></row><row><cell>Easy</cell><cell>STBM SBM</cell><cell>0.98 ± 0.04 1.00 ± 0.00</cell><cell>0.98 ± 0.04 --</cell><cell>1.00 ± 0.00 0.01 ± 0.01</cell><cell cols="3">1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 --0.69 ± 0.07 --</cell></row><row><cell></cell><cell>SC</cell><cell>0.97 ± 0.07</cell><cell>--</cell><cell>0.00 ± 0.01</cell><cell>--</cell><cell>0.63 ± 0.11</cell><cell>--</cell></row><row><cell></cell><cell>LDA</cell><cell>--</cell><cell>0.97 ± 0.06</cell><cell>--</cell><cell>1.00 ± 0.00</cell><cell>--</cell><cell>1.00 ± 0.00</cell></row><row><cell></cell><cell>ETM</cell><cell>--</cell><cell>0.96 ± 0.14</cell><cell>--</cell><cell>1.00 ± 0.00</cell><cell>--</cell><cell>1.00 ± 0.00</cell></row><row><cell></cell><cell cols="3">ETSBM 1.00 ± 0.00 0.95 ± 0.03</cell><cell>1.00 ± 0.00</cell><cell cols="2">1.00 ± 0.00 1.00 ± 0.00</cell><cell>0.97 ± 0.04</cell></row><row><cell>Hard 1</cell><cell>STBM SBM SC</cell><cell>1.00 ± 0.00 0.01 ± 0.01 0.00 ± 0.02</cell><cell>0.90 ± 0.13 ----</cell><cell>1.00 ± 0.00 0.01 ± 0.01 -0.00 ± 0.01</cell><cell cols="3">1.00 ± 0.00 1.00 ± 0.00 0.98 ± 0.03 --0.01 ± 0.01 -----0.00 ± 0.01 --</cell></row><row><cell></cell><cell>LDA</cell><cell>--</cell><cell>0.90 ± 0.17</cell><cell>--</cell><cell>1.00 ± 0.00</cell><cell>--</cell><cell>0.99 ± 0.01</cell></row><row><cell></cell><cell>ETM</cell><cell>--</cell><cell>0.93 ± 0.07</cell><cell>--</cell><cell>1.00 ± 0.00</cell><cell>--</cell><cell>0.98 ± 0.03</cell></row><row><cell></cell><cell>ETSBM</cell><cell cols="3">0.98 ± 0.06 0.83 ± 0.07 1.00 ± 0.00</cell><cell>0.86 ± 0.03</cell><cell cols="2">0.91 ± 0.12 0.84 ± 0.12</cell></row><row><cell>Hard 2</cell><cell>STBM SBM SC</cell><cell>0.75 ± 0.27 0.96 ± 0.05 0.98 ± 0.08</cell><cell>0.82 ± 0.22 ----</cell><cell>1.00 ± 0.00 0.00 ± 0.00 -0.00 ± 0.01</cell><cell>1.00 ± 0.00 ----</cell><cell>0.63 ± 0.19 0.63 ± 0.11 0.60 ± 0.11</cell><cell>0.77 ± 0.15 ----</cell></row><row><cell></cell><cell>LDA</cell><cell>--</cell><cell>0.77 ± 0.09</cell><cell>--</cell><cell>0.88 ± 0.02</cell><cell>--</cell><cell>0.84 ± 0.04</cell></row><row><cell></cell><cell>ETM</cell><cell>--</cell><cell>0.83 ± 0.08</cell><cell>--</cell><cell>0.85 ± 0.03</cell><cell>--</cell><cell>0.86 ± 0.04</cell></row><row><cell></cell><cell cols="7">6. Real World example: analysing the French presidential election with a</cell></row><row><cell></cell><cell cols="2">Twitter dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Inference</head><p>Proof of Proposition 4.1. The ELBO can be decomposed as follow:</p><p>KL(N (µ qr (τ, ν), σ qr (τ, ν))||N (0, I))</p><p>where,</p><p>and θ qr = µ qr (τ, ν) + σ qr (τ, ν)ϵ, ϵ ∼ N (0 K , I K ).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixed membership stochastic blockmodels</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="page" from="1981" to="2014" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A variational baysian framework for graphical models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Attias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="209" to="215" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The latent topic block model for the co-clustering of textual interaction data</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Bergé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bouveyron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Corneli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Latouche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="247" to="270" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Correlated topic models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">147</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The stochastic topic block model for the clustering of vertices in networks with textual edges</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bouveyron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Latouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zreik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="11" to="31" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The dynamic stochastic topic block model for dynamic networks with textual edges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Corneli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bouveyron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Latouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="677" to="695" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Block modelling in dynamic networks with non-homogeneous poisson processes and exact icl</title>
		<author>
			<persName><forename type="first">M</forename><surname>Corneli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Latouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Network Analysis and Mining</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model selection and clustering in stochastic block models based on the exact integrated complete data likelihood</title>
		<author>
			<persName><forename type="first">E</forename><surname>Côme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Latouche</surname></persName>
		</author>
		<idno type="DOI">10.1177/1471082X15577017</idno>
	</analytic>
	<monogr>
		<title level="j">Statistical Modelling</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="564" to="589" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A mixture model for random graphs</title>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Daudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robin</surname></persName>
		</author>
		<idno>RR-5840 INRIA</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Research Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A mixture model for random graphs</title>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Daudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="173" to="183" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Topic modeling in embedding spaces</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="439" to="453" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the evolution of random graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Erdos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publ. Math. Inst. Hung. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="17" to="60" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Categorical data analysis of single sociometric relations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological methodology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="156" to="192" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Amortized inference in probabilistic reasoning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual meeting of the cognitive science society</title>
		<meeting>the annual meeting of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Goldenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
		<title level="m">A survey of statistical network models. Foundations and Trends® in Machine Learning</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="129" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient discovery of overlapping communities in massive networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="14534" to="14539" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Model-based clustering for social networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Handcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tantrum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series A (Statistics in Society)</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page" from="301" to="354" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The random subgraph model for the analysis of an ecclesiastical network in merovingian gaul</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Latouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bouveyron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lamassé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="377" to="405" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Greedy clustering of count data through a mixture of multinomial pca</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jouvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Latouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bouveyron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bataillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Livartowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning systems of concepts with an infinite relational model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Overlapping stochastic block models with application to the french political blogosphere</title>
		<author>
			<persName><forename type="first">P</forename><surname>Latouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Birmelé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ambroise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="page" from="309" to="336" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variational bayesian inference and complexity control for stochastic block models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Latouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Birmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ambroise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Modelling</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="93" to="115" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Comment la gauche sociale-démocrate a perdu la bataille des réseaux sociaux</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laurent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Le Monde</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><surname>Url</surname></persName>
		</author>
		<ptr target="https://www.lemonde.fr/politique/article/2022/03/31/comment-la-gauche-sociale-democrate-a-perdu-la-bataille-des-reseaux-sociaux_6119986_823448.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A review of stochastic block models and extensions for graph clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Wilkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Network Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Topic-link lda: joint models of topic and author community</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gryc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="665" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Uncovering latent structure in valued graphs: a variational approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mariadassou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="715" to="742" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Statistical clustering of temporal networks through a dynamic stochastic block model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Miele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="1119" to="1141" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling heterogeneity in random graphs through latent space models: a selective review</title>
		<author>
			<persName><forename type="first">C</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESAIM: Proceedings and Surveys</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="55" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Estimation and prediction for stochastic blockstructures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nowicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A B</forename><surname>Snijders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical association</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="1077" to="1087" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Latent semantic indexing: A probabilistic analysis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>ACM press</publisher>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008">2019. 2008</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
	<note>Social topic models for community extraction</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The author-topic model for authors and documents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence UAI &apos;04</title>
		<meeting>the 20th Conference on Uncertainty in Artificial Intelligence UAI &apos;04</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Using content and interactions for discovering communities in social networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Contractor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Faruquie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Subramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st international conference on World Wide Web</title>
		<meeting>the 21st international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="331" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Crisis in a cloister</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Sampson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<pubPlace>Ithaca</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis Ph. D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Autoencoding variational inference for topic models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A review of topic modeling methods</title>
		<author>
			<persName><forename type="first">I</forename><surname>Vayansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">101582</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels for directed graphs</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y C</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="8" to="19" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Clustering based on random graph model embedding vertex features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zanghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ambroise</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2010.01.026</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="830" to="836" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Probabilistic models for discovering e-communities</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Manavoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th international conference on World Wide Web</title>
		<meeting>the 15th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The dynamic random subgraph model for the clustering of evolving networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zreik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Latouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bouveyron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="501" to="533" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
