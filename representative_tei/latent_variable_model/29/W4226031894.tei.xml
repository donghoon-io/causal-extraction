<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pupil Correlates of Decision Variables in Mice Playing a Competitive Mixed-Strategy Game</title>
				<funder>
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder ref="#_GZswPDq #_ts2tz65 #_wEuDUC4">
					<orgName type="full">National Institute of Mental Health</orgName>
				</funder>
				<funder>
					<orgName type="full">Kavli Institute for Neuroscience Postdoctoral Fellowship</orgName>
				</funder>
				<funder ref="#_WkcB7nc">
					<orgName type="full">NIH Training</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-02-15">February 15, 2022.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hongli</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Interdepartmental Neuroscience Program</orgName>
								<orgName type="institution">Yale University School of Medicine</orgName>
								<address>
									<postCode>06511</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heather</forename><forename type="middle">K</forename><surname>Ortega</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Interdepartmental Neuroscience Program</orgName>
								<orgName type="institution">Yale University School of Medicine</orgName>
								<address>
									<postCode>06511</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huriye</forename><surname>Atilgan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychiatry</orgName>
								<orgName type="institution">Yale University School of Medicine</orgName>
								<address>
									<postCode>06511</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="middle">E</forename><surname>Murphy</surname></persName>
							<idno type="ORCID">0000-0003-1035-9992</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychiatry</orgName>
								<orgName type="institution">Yale University School of Medicine</orgName>
								<address>
									<postCode>06511</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kwan</surname></persName>
							<email>alex.kwan@yale.edu</email>
							<idno type="ORCID">0000-0003-2169-1667</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychiatry</orgName>
								<orgName type="institution">Yale University School of Medicine</orgName>
								<address>
									<postCode>06511</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Neuroscience</orgName>
								<orgName type="institution">Yale University School of Medicine</orgName>
								<address>
									<postCode>06511</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pupil Correlates of Decision Variables in Mice Playing a Competitive Mixed-Strategy Game</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-15">February 15, 2022.</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1523/ENEURO.0457-21.2022</idno>
					<note type="submission">Received October 28, 2021; accepted January 2, 2022; First</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T18:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>action selection</term>
					<term>arousal</term>
					<term>decision-making</term>
					<term>decisions</term>
					<term>mouse</term>
					<term>value</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choice</head><p>Outcome Left Right Choice Outcome RPE</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pupil</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Significance Statement</head><p>Many studies of decision-making rely on tasks where a pure strategy exists, in which there is always an optimal action for a certain task condition. However, in real-world competitive situations when multiple decision makers are involved, a mixed strategy involving probabilistic actions may be superior. In this study, we investigated whether head-fixed mice can play a competitive game known as "matching pennies" against a computer opponent. We analyzed the choice behavior using reinforcement learning algorithms, revealed distinct strategies, and characterized pupil dynamics. The results provide convincing evidence that mice can engage in a competitive game and link the pupil-related arousal to the process of value updating during play.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Animals learn from the outcomes of their past actions. The decision-making process can be cast in the framework of reinforcement learning <ref type="bibr" target="#b50">(Sutton and Barto, 1998)</ref>, which provides a quantitative approach to characterize how animals choose among multiple options based on prior experience. This approach, when applied to rodents and combined with powerful molecular, genetic, electrophysiological, and imaging methods, has yielded novel insights into the neural circuits involved in reward-based learning <ref type="bibr" target="#b22">(Ito and Doya, 2009;</ref><ref type="bibr" target="#b48">Sul et al., 2010</ref><ref type="bibr" target="#b49">Sul et al., , 2011;;</ref><ref type="bibr" target="#b51">Tai et al., 2012;</ref><ref type="bibr" target="#b4">Bari et al., 2019;</ref><ref type="bibr" target="#b17">Groman et al., 2019;</ref><ref type="bibr" target="#b20">Hattori et al., 2019)</ref>. To date, most studies in rodents relied on tasks involving a pure strategy, where there is always a particular action that is optimal in a given situation. However, pure strategies are not always available. For instance, in a two-player game (e.g., rock-paper-scissors) where the outcome depends on both the animal's and an opponent's choices, pure strategies are inadequate because an opponent can predict tendencies and exploit to win. Instead, the animal should adopt a mixed strategy, in which two or more pure strategies are chosen probabilistically. Analyses of behavior under competitive pressure fall into the purview of game theory <ref type="bibr" target="#b8">(Camerer, 2003)</ref>, and provide a unique window into the social and adaptive aspects of decision-making <ref type="bibr" target="#b27">(Lee, 2008)</ref>.</p><p>Research in humans and non-human primates has identified numerous brain regions contributing to decision-making during two-player games. For example, a functional imaging study in humans showed widespread representation of reward signals in the brain during multiple types of games against computerized opponents <ref type="bibr" target="#b56">(Vickery et al., 2011)</ref>. Signals related to choices and the mentalization of an opponent's actions were localized to several brain regions including the prefrontal cortex <ref type="bibr" target="#b18">(Hampton et al., 2008;</ref><ref type="bibr" target="#b56">Vickery et al., 2011)</ref>. Electrophysiological recordings in macaque monkeys demonstrated spiking activity patterns that suggest neural representations of various decision variables in prefrontal cortical regions, lateral intraparietal cortex, and amygdala <ref type="bibr" target="#b5">(Barraclough et al., 2004;</ref><ref type="bibr">Seo et al., 2007</ref><ref type="bibr">Seo et al., , 2009;;</ref><ref type="bibr" target="#b10">Chang et al., 2013;</ref><ref type="bibr" target="#b19">Haroush and Williams, 2015;</ref><ref type="bibr" target="#b12">Dal Monte et al., 2020)</ref>. However, there have only been a few reports of rodents engaging in twoplayer games <ref type="bibr" target="#b52">(Tervo et al., 2014;</ref><ref type="bibr" target="#b58">Wood et al., 2016)</ref>, and the associated neural correlates are less clear.</p><p>The neuromodulator norepinephrine (NE) may have an important role for performance during two-player games. Prior literature has linked central noradrenergic tone to behavioral flexibility <ref type="bibr" target="#b2">(Aston-Jones and Cohen, 2005;</ref><ref type="bibr" target="#b7">Bouret and Sara, 2005)</ref>. In one pioneering study, Tervo and colleagues taught rats to play a matching pennies game with a computer opponent <ref type="bibr" target="#b52">(Tervo et al., 2014)</ref>. By manipulating neural activity in locus coeruleus (LC), they showed that elevating central NE tone can suppress firing in the anterior cingulate cortex, which in turn reduces the influence of reinforcement history and promotes stochastic behavior. A different way to study neuromodulatory tone is to measure pupil size, which is often treated as a readout of NE levels in the neocortex <ref type="bibr" target="#b16">(Gilzenrat et al., 2010;</ref><ref type="bibr" target="#b24">Joshi et al., 2016;</ref><ref type="bibr" target="#b37">Reimer et al., 2016)</ref>. Studies of pupillary dynamics during tasks further support the idea that NE is important for flexible decisions. For instance, the baseline pupil size is shown to correlate with biases in the explore-exploit trade-off and attentional set shifting <ref type="bibr" target="#b23">(Jepma and Nieuwenhuis, 2011;</ref><ref type="bibr" target="#b35">Pajkossy et al., 2017)</ref>. The trial-by-trial transient change in pupil size is reported to associate with many task-relevant variables including upcoming choice, expected outcome, values of the choices, and uncertainties <ref type="bibr" target="#b21">(Hess and Polt, 1960;</ref><ref type="bibr" target="#b36">Qiyuan et al., 1985;</ref><ref type="bibr" target="#b13">de Gee et al., 2014;</ref><ref type="bibr" target="#b54">Van Slooten et al., 2018)</ref>. Given these prior results, it seems likely that pupil fluctuations can be leveraged to study the neuromodulatory mechanisms underlying adaptive action selection during competitive two-player games.</p><p>We have two goals for the current study. First, we want to know whether head-fixed mice can compete proficiently in a two-player game. Second, we want to characterize pupil fluctuations to gain insights into the role of neuromodulation in decision-making under competitive pressure. To this end, we designed a behavioral paradigm for a head-fixed mouse to play iterative matching pennies against a computer-controlled virtual opponent. We found that mice can perform at a level close to the optimal reward rate by exhibiting choice behavior consistent with a mix of reinforcement learning and choice perseveration. We underscored the unique choice behavior during matching pennies, by comparing with performance in the more widely used twoarmed bandit task. Finally, we measured within-trial changes in pupil size, and showed that the transient pupillary responses are associated with the choice, outcome, and latent variables for value updating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Animals</head><p>All animal procedures were conducted in accordance with procedures approved by the Institutional Animal Care and Use Committee at Yale University. Adult male and female C57BL/6J mice (postnatal day 56 or older; #000664, The Jackson Laboratory) were used for all experiments. Mice were housed in groups of three to five animals with 12/12 h light/dark cycle control (lights off at 7 P.M.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Surgical procedures</head><p>Anesthesia was induced with 2% isoflurane in oxygen before the surgery. The isoflurane was lowered to 1-1.2% during the surgical procedures. The mouse was placed on a water-circulating heating pad (TP-700, Gaymar Stryker) in a stereotaxic frame (David Kopf Instruments). After injecting carprofen (5 mg/kg, s.c.; #024751, Butler Animal Health) and dexamethasone (3 mg/kg, s.c.; Dexaject SP, #002459, Henry Shein Animal Health), the scalp was removed to expose the skull. A custom-made stainless-steel head plate (eMachineShop) was glued onto the skull with MetaBond (C&amp;B, Parkell). Carprofen (5 mg/kg, s.c.) was injected each day for the following 3 d. Mice were given 7 d to recover from the surgery before any behavioral training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavioral setup</head><p>The training apparatus was based on a previous design from our prior studies <ref type="bibr" target="#b45">(Siniscalchi et al., 2016</ref><ref type="bibr" target="#b46">(Siniscalchi et al., , 2019))</ref>. Detailed instruction to construct the apparatus is available at <ref type="url" target="https://github.com/Kwan-Lab/behavioral-rigs">https://github.com/Kwan-Lab/behavioral-rigs</ref>.</p><p>The mouse with a head plate implant was head-fixed to a stainless-steel holder (eMachineShop). The animal sat inside an acrylic tube (8486K433; McMaster-Carr), which limited gross movements though allowed postural adjustments. A lick port with two lick spouts was positioned in front of the subject. The spouts were constructed with blunted 20gauge stainless-steel needles. Contact with the lick spout, which was how the animal indicated its choices, was detected through wires that were soldered onto the spout and a battery-powered lick detection electronic circuit. Output signals from the circuit were sent to a computer via a data acquisition unit (USB-201, Measurement Computing) and logged by the Presentation software (Neurobehavioral Systems). Water delivery from the lick spouts was controlled independently for each spout by two solenoid fluid valves (MB202-V-A-3-0-L-204; Gems Sensors &amp; Controls). The amount of water was calibrated to ;4 ml per pulse by adjusting the duration of the electrical pulse sent by the Presentation software via a second data acquisition unit (USB-201, Measurement Computing). Two speakers (S120, Logitech) were placed in front of the mouse to play the sound cue. The whole setup was placed inside an audiovisual cart with walls lined with soundproof acoustic foams (5692T49, McMaster-Carr). A monochrome camera (GigE G3-GM11-M1920, Dalsa) with a 55-mm telecentric lens (TEC-55, Computar) was aimed at the right eye. Video was acquired at 20 Hz. A dimmable, white light source (LT-T6, Aukey) was used to provide ambient light, such that the baseline pupil size was moderate and fluctuations around the baseline was detectable. The computer running the Presentation software would send TTL pulses to a computer controlling the camera through a USB data acquisition device (USB-201; Measurement Computing). The cameraconnected computer would run a custom script written in MATLAB 2019b (MathWorks) that logged the timing of the TTL pulses so that the behavioral log files generated by the Presentation software could be aligned to the video recordings. In a small subset of experiments, we captured videos from both left and right eyes by mounting two identical camera systems on both sides of the animal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavioral training, matching pennies</head><p>All of the procedures for initial shaping as well as the final matching pennies task were written using the scripting language in the Presentation software. The animals were fluidrestricted. Water was provided during the one behavioral session daily. On the days when the animals were not trained (typically 1 d a week), a water bottle was placed in the home cage for 5 min of ad libitum consumption. All animals underwent two shaping phases before training. For phase 1 (2 d), the animals were habituated to the apparatus. They may lick either spout. A water reward would be delivered for every lick in the corresponding spout, as long as a minimum of 1 s has occurred since the last reward. The session would terminate after the animals acquired 100 rewards. For phase 2 (approximately four weeks), the animals were introduced to the trial structure and learned to suppress impulsive licks. At the start of each trial, a 5-kHz sound cue lasting for 0.2 s was played. From the onset of the sound cue, the mouse had a window of 2 s to make a response by licking either of the spouts. The 2-s response window was chosen because naive animals have yet to learn the trial timing and a more lenient response window helps them acquire the task faster. Moreover, we note that once the animal chooses with the first tongue lick, then the trial progresses, so the duration of the response window does not affect the trial timing as long as the mouse chooses before the end of the response window. If a lick was detected during the response window, a water reward would be delivered in the corresponding spout and there was a fixed 3-s period for consumption following the lick. If no lick was detected, the fixed 3-s consumption window would still be presented following the end of the response window. From the end of the consumption window, an intertrial interval (ITI) began. The duration of the ITI in seconds was drawn from a truncated exponential distribution with l = 1/3 and boundaries of 1 and 5. If the animal emitted one or more licks during the ITI, then additional time drawn again from the same distribution would be appended to the ITI. If the mouse licked again during the appended time, yet another additional time would be appended, up to a total of five draws including the initial draw. When the ITI ended, a new trial would begin. This trial timing was the same as what would be used for matching pennies. Particularly, the goal for the shaping was to habituate and introduce lick suppression. Although the mouse could theoretically get water from either spout, the animal tended to favor heavily one spout during the shaping procedures. The animal would advance to playing the matching pennies game when the average number of ITI draws per trial was lower than 1.2 for three consecutive sessions.</p><p>For matching pennies, the mouse played against a virtual opponent in the form of a computer agent. At the start of each trial, the agent made a choice (left or right). If the mouse selected the same choice as the computer, a water reward would be delivered in the corresponding spout. Otherwise, no reward was presented. Importantly, the computer agent was designed to provide competitive pressure by acting according to prediction of the animal's choices. Specifically, it was programmed to be the same as "algorithm 2" <ref type="bibr" target="#b5">(Barraclough et al., 2004;</ref><ref type="bibr" target="#b28">Lee et al., 2004)</ref> or "competitor 1" <ref type="bibr" target="#b52">(Tervo et al., 2014)</ref> in previous studies. Briefly, the agent had a record of the mouse's entire choice and reward history within the current session. The agent calculated the conditional probabilities that the animal would choose left given sequences of the preceding N choices (N = 0-4) and sequences of preceding N choice-outcome combinations (N = 1-4). The binomial test was used to test each of the nine conditional probabilities against the null hypothesis that the mouse would choose left with a probability of 0.5. If none of the null hypothesis was rejected, the agent would randomly choose either target with equal probabilities. If one or more hypotheses were rejected, the agent would generate the counter choice with the statistically significant conditional probability that was farther away from 0.5. A session would terminate automatically when no response was logged for 10 consecutive trials. When an animal reached a 40% reward rate for three consecutive sessions (approximately four weeks), then its performance was considered stable and the subsequent sessions were included in the following analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavioral training, two-armed bandit</head><p>We used the same training apparatus and the same scripting language in the Presentation software to program the two-armed bandit task, which had the same trial timing as matching pennies. The shaping procedures relied on similar tactics, but the details were different, involving three shaping phases before training. For phase 0 (1 d), the experimenter manually delivered 50 water rewards through each port (for a total of 100 rewards) and monitored for consistent licking. A 5-kHz sound cue lasting for 0.2 s was played at the same time as water delivery. If the animal was not licking to consume the water rewards, the experimenter used a blunted syringe to guide the animal to lick the spout. For phase 1 (1 d), the animal was introduced to the trial structure and learned to suppress impulsive licks. At the start of each trial, A 5-kHz sound cue lasting for 0.2 s was played. From the onset of the sound cue, the mouse had a window of 5 s to make a response by licking either of the spouts. If a lick was detected during the response window, a water reward would be delivered from the corresponding spout and there was a fixed 3-s period for consumption following the lick. If no lick was detected, no reward was given during the consumption period. Next, an ITI began. The duration of the ITI in seconds was drawn from a truncated exponential distribution with l = 1/3 and boundaries of 1 and 5. If the animal emitted one or more licks during the ITI, then additional time drawn again from the same truncated exponential distribution would be appended to the ITI. If the mouse licked again during the appended time, yet another additional time would be appended, up to a total of five draws including the initial draw. When the ITI ended, a new trial would begin. The session ends after the animal accumulates 100 rewards. In particular, the goal for phase 1 was to habituate and introduce lick suppression. For phase 2, the animal continued to lick following the cue for a water reward in a similar fashion to phase 1. However, the response window was shortened to 2 s, meaning the animal had only 2 s to lick following the cue to receive reward. The trial timing with shortened response window was the same as what would be used for the two-armed bandit task. Moreover, the animal must alternate sides (choose left if the last reward came from a right lick, and vice versa). They would not be rewarded for choosing the same spout repeatedly to discourage the development of a side bias. The animal could proceed to the final task if they achieved 200 rewards in a single session.</p><p>For the two-armed bandit task, the two options (left and right) were associated with different reward probabilities. In each trial, when an animal chose an option, a reward was delivered stochastically based on the assigned reward probability. In our implementation, there were two sets of reward probabilities: 0.7:0.1 and 0.1:0.7. A set of reward probabilities was maintained across a block of trials. Within a block, once the mouse had chosen the option with high reward probability (hit trials) for 10 times, then on any given trial there was a probability of 1/11 that the reward probabilities would change. Thus, the number of trials in a block after ten hits followed a geometric distribution with m= 11. There were no explicit cues to indicate a block switch, therefore the animal had to infer the current situation through experience. A session would terminate automatically when no response was logged for 20 consecutive trials. Animals are considered proficient when they chose the spout with higher reward probability on at least 50% of trials on three consecutive sessions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprocessing of behavioral data</head><p>A total of 115 sessions from 13 mice were included in the study. For matching pennies, data came from 81 sessions from five mice (five males). For two-armed bandit, data came from 34 sessions from eight mice, including 26 sessions from four mice (three males, one female) with single-pupil recording and 8 sessions from the other four mice (four males) with double-pupil recording. All of the sessions contained both behavioral and pupillometry data. For matching pennies, we used all 81 sessions for behavioral analysis, and 67 sessions for pupil-related analysis, because 14 sessions were excluded later because of inaccurate pupil labeling (see Materials and Methods, preprocessing of pupillometry data). For behavior, the log file saved by the Presentation software contained timestamps for all events that occurred during a session. Analyses of the behavioral data were done in MATLAB. For matching pennies, toward the end of each session, the animals tended to select the same option for around 30 trials before ceasing to respond. To avoid these repetitive trials in the analyses, for each session, the running threechoice entropy (see below) of a 30-trial window was calculated, and the MATLAB function ischange was used to fit with a piecewise linear function. The trial when the fitted function fell below a value of 1 was identified as the "last trial," and all subsequent trials were discarded for the analysis. In cases if the performance recovered after the detected last trial to a value .1, or if the fitted function did not fall below a value of 1 and no "last trial" was detected, the entire session was used for analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of behavioral data, entropy</head><p>To quantify the randomness in the animals' choices, the three-choice entropy of the choice sequence is calculated by the following:</p><formula xml:id="formula_0">Entropy ¼ À X k p k log 2 p k ;<label>(1)</label></formula><p>where p k is the frequency of occurrence of a three-choice pattern in a session. Because there were two options to choose from, there were 2 3 = 8 potential patterns possible. The maximum value for entropy is 3 bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of behavioral data, computational models</head><p>To quantify the choice behavior, we considered five computational models. The primary model used in the paper is a Q-learning with forgetting model plus choice kernels (FQ_RPE_CK; <ref type="bibr" target="#b25">Katahira, 2015;</ref><ref type="bibr" target="#b57">Wilson and Collins, 2019)</ref>. On trial n, for a choice c n that leads to an outcome r n , the action value Q i n associated with an action i is updated by the following:</p><formula xml:id="formula_1">Q i n11 ¼ Q i n 1 aðr n À Q i n Þ if c n ¼ i 1 À a ð Þ Q i n if c n 6 ¼ i ; &amp; (2)</formula><p>where a is the learning rate, and the forgetting rate for the unchosen action. In our task, there are two options, so i 2 L; R f g. For the outcome, r n = 1 for reward, 0 for no reward. Moreover, to capture the animal's tendency to make decisions based purely on previous choices, there are choice kernels K i n updated by the following:</p><formula xml:id="formula_2">K i n11 ¼ K i n 1 a K 1 À K i n À Á if c n ¼ i 1 À a K ð ÞK i n if c n 6 ¼ i ; &amp; (3)</formula><p>where a K is the learning rate of the choice kernel. For action selection, the probability to choose action i on trial n is given by a softmax function:</p><formula xml:id="formula_3">Pðc n ¼ iÞ ¼ exp b Q i n 1 b K K i n À Á X j exp b Q j n 1b K K j n À Á ;<label>(4)</label></formula><p>where b and b K are the inverse temperature parameters for action values and choice kernels, respectively.</p><p>We compared the FQ_RPE_CK model against four other models. For the win-stay-lose-switch model (WSLS), the probability to choose action i on trial n is given by the following:</p><formula xml:id="formula_4">Pðc n ¼ iÞ ¼ p ifr nÀ1 ¼ 1 1 À p if r nÀ1 ¼ 0 ; &amp; (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where p is the probability that the animal followed the WSLS strategy.</p><p>For the Q-learning model (Q_RPE), the action value is updated by the following:</p><formula xml:id="formula_6">Q i n11 ¼ Q i n 1aðr n À Q i n Þ if c n ¼ i Q i n if c n 6 ¼ i ; &amp;<label>(6)</label></formula><p>and the probability to choose action i at trial n is then given by the following:</p><formula xml:id="formula_7">Pðc n ¼ iÞ ¼ exp b Q i n À Á X j exp b Q j n À Á :<label>(7)</label></formula><p>For the forgetting Q-learning model (FQ_RPE), the action values are updated by Equation <ref type="formula">2</ref>, and the probability to choose action i on trial n is given by Equation <ref type="formula" target="#formula_7">7</ref>.</p><p>For the differential Q-learning model (DQ_RPE; <ref type="bibr" target="#b9">Cazé and van der Meer, 2013;</ref><ref type="bibr" target="#b26">Katahira, 2018)</ref>, the action value is updated by the following:</p><formula xml:id="formula_8">Q i n11 ¼ Q i n 1 a R 1 À Q i n À Á if c n ¼ i; r n ¼ 1 Q i n 1 a U 1 À Q i n À Á if c n ¼ i; r n ¼ 0 Q i n if c n 6 ¼ i ; 8 &lt; : (8)</formula><p>where a R and a U are the learning rates for rewarded and unrewarded trials, respectively. The probability to choose action i on trial n is given by Equation <ref type="formula" target="#formula_7">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of behavioral data, model fitting and comparison</head><p>To fit the computational models to the behavioral data, for each subject, the sequence of choices and outcomes were concatenated across sessions. Each model was fitted to the data using a maximum-likelihood algorithm implemented with the fmincon function in MATLAB, with the constraints 0 a; a K ; a R ; a U 1, 0,b ; b K , and 0 p 1. These fits also yielded latent decision variables such as action values and choice kernels that would be used for the subsequent multiple linear regression analyses. For model comparison, we calculated the Bayesian information criterion (BIC) for each of the model fits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprocessing of pupillometry data</head><p>To extract the coordinates of the pupil from the video recordings, we used DeepLabCut (DLC) 2.0 <ref type="bibr" target="#b31">(Mathis et al., 2018;</ref><ref type="bibr" target="#b34">Nath et al., 2019)</ref>, ran on Jupyter Notebook on Google's cloud server with 1 vCPU and 3.75 GB RAM, under Ubuntu 16.04. A small subset of the video frames was manually analyzed, with the experimenter annotating five labels, including the central, uppermost, leftmost, lowermost, and rightmost points of the pupil. The annotated frames were fed to DLC to train a deep neural network, which analyzed the remainder of the video to produce the five labels. From the labels, the pupil diameter was computed by taking the distance between the leftmost and rightmost labels. We did not use the other labels, because the estimates of the lowermost points were unstable, sometimes jumping in consecutive frames because of interference from the lower eyelid. Some sessions were excluded (14 out of 81 sessions in matching pennies), because of inaccurate detection of labels by DLC based on quality check via visual inspection. The inaccurate detection arose sometimes because of interference from whisker, eyelid, or poor video quality.</p><p>The pupil diameter signal was further processed through a 4-Hz lowpass filter with the MATLAB function lowpass. Then any frames with outliers that were .3 scaled median absolute deviation (MAD) from the median were deleted using the MATLAB function isoutlier. Using a 10-min moving window to account for drift over a session, the signal was converted to z score. Finally, we calculated the pupil response for each trial by subtracting the instantaneous z score from À1 to 5 s relative to cue onset by the baseline z score, which was defined as the mean z score from À2 to À1 s relative to cue onset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of pupil data, multiple linear regression</head><p>To determine how pupil responses may relate to choices and outcomes, we used multiple linear regression:</p><formula xml:id="formula_9">z t ð Þ ¼ b 0 1 b 1 c n11 1 b 2 r n11 1 b 3 c n11 r n11 1 b 4 c n 1 b 5 r n 1 b 6 c n r n 1 b 7 c nÀ1 1 b 8 r nÀ1 1 b 9 c nÀ1 r nÀ1 1b 10 c nÀ2 1 b 11 r nÀ2 1 b 12 c nÀ2 r nÀ2 1 b 13 r MA n 1 b 14 r Cum: n 1 « ðtÞ ;<label>(9)</label></formula><p>where z t ð Þ is the z-scored pupil response at time t in trial n, c n11 ; c n ; c nÀ1 ; c nÀ2 are the choices made on the next trial, the current trial, the previous trial, and the trial before the previous trial, respectively, r n11 ; r n ; r nÀ1 ; r nÀ2 are the outcomes for the next trial, the current trial, the previous trial, and the trial before the previous trial, respectively, b 0 ,..., b 14 are the regression coefficients, and « ðtÞ is the error term. Choices were dummy-coded as 0 for left responses and 1 for right responses. Outcomes were dummy-coded as 0 for no-reward and 1 for reward. For the last two predictors in Equation <ref type="formula" target="#formula_9">9</ref>, r MA n is the average reward over the previous 20 trials, given by the following equation:</p><formula xml:id="formula_10">r MA n ¼ X 19 i¼0 r nÀi 20 :<label>(10)</label></formula><p>The term r Cum: n indicates the normalized cumulative reward during the session, calculated by the following:</p><formula xml:id="formula_11">r Cum: n ¼ X n i¼1 r i X N i¼1 r i ; (<label>11</label></formula><formula xml:id="formula_12">)</formula><p>where n denotes the current trial number and N is the total number of trials in the session.</p><p>To determine how pupil responses may relate to latent decision variables for action selection, we used multiple linear regression:</p><formula xml:id="formula_13">z t ð Þ ¼ b 0 1 b 1 c n 1 b 2 r n 1 b 3 c n r n 1b 4 c nÀ1 1 b 5 r nÀ1 1 b 6 c nÀ1 r nÀ1 1 b 7 ðQ L n À Q R n Þ 1 b 8 Q chosen n 1b 9 ðK L n À K R n Þ 1 b 10 K chosen n 1 b 11 r MA n 1 b 12 r Cum: n 1 « ðtÞ ;<label>(12)</label></formula><p>where Q L n and Q R n denote the action values of the left and right choices in trial n, respectively, Q chosen n is the value of the action chosen in trial n, K L n and K R n are the choice kernels of the left and right choices in trial n, respectively, K chosen n is the choice kernel of the action chosen in trial n.</p><p>To determine how pupil responses may relate to latent decision variables for value updating, we used multiple linear regression, adapting the equation from <ref type="bibr" target="#b48">Sul et al. (2010)</ref>:</p><formula xml:id="formula_14">z t ð Þ ¼ b 0 1 b 1 c n 1 b 2 c nÀ1 1 b 3 r nÀ1 1b 4 ðQ L n À Q R n Þ 1 b 5 ðr n À Q chosen n Þ 1b 6 ðK L n À K R n Þ 1 b 7 ð1 À K chosen n Þ 1 b 8 r MA n 1 b 9 r Cum: n 1 « ðtÞ ; (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>where r n À Q chosen n is the reward prediction error (RPE), and 1 À K chosen n is the error term used to update the choice kernels, or choice kernel error (CKE).</p><p>For each session, the regression coefficients were determined by fitting the equations to data using the MATLAB function fitlm. The fit was done in 100-ms time bins that span from À3 to 5 s relative to cue onset, using mean pupil response within the time bins. To summarize the results, for each predictor, we calculated the proportion of sessions in which the regression coefficient was different from zero (p , 0.01). To determine whether the proportion was significantly different from chance, we performed a x 2 test against the null hypothesis that there was a 1% probability that a given predictor was mischaracterized as significant by chance in a single session. The analysis was performed with a personal computer (Intel i7-6700K CPU, 16 GB RAM). It was run under Windows 10 Education.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code accessibility</head><p>The data and code described in the paper is freely available online at <ref type="url" target="https://github.com/Kwan-Lab/wang2022">https://github.com/Kwan-Lab/wang2022</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mice played matching pennies against a computer opponent</head><p>We trained head-fixed mice to play matching pennies against a computer opponent (Fig. <ref type="figure" target="#fig_0">1A</ref>). In this iterative version of matching pennies, each trial the mouse and the computer would choose left or right. The mouse received a water reward only if the actions matched. The corresponding payoff matrix is shown in Figure <ref type="figure" target="#fig_0">1B</ref>. This game was challenging for the mouse because the computer opponent had access to the complete history of choices and rewards over the session, and was programmed to predict the mouse's next choice to make the counter action [see Materials and Methods; same as "algorithm 2" in <ref type="bibr" target="#b28">Lee et al. (2004)</ref> and "competitor 1" in <ref type="bibr" target="#b52">Tervo et al. (2014)</ref>]. Figure <ref type="figure" target="#fig_0">1C</ref> shows the trial structure. A 0.2-s, 5-kHz sound cue indicated the start of the trial. Within a 2-s response window, the mouse could make a directional tongue lick to the left or right spout to indicate its choice. Based on the animal's and computer's choices and the payoff matrix, the animal might receive a water reward after the response. To minimize precue licks, the ITI was drawn from a truncated exponential distribution and would extend if the mouse could not suppress licking (see Materials and Methods). Mice were trained daily and, on average, took approximately four weeks to follow the trial timing to suppress precue licking, and then another four weeks playing matching pennies to achieve a performance threshold of 40% reward rate for three consecutive sessions. All the data and analyses presented in the paper came from sessions after the threshold was attained. The dataset for matching pennies included 81 behavioral sessions with concurrent pupil measurements (n = 5 mice). For matching pennies, the Nash equilibrium indicates that rational players should choose left and right with equal probabilities, which would yield a long-run reward rate of 50%. In an example session, plotting the choices and rewards for a mouse and the choices for the computer showed that the animal indeed exhibited a great degree of stochasticity in its choice pattern (Fig. <ref type="figure" target="#fig_0">1D</ref>). This could be seen more clearly by looking at the cumulative occurrences of various plausible three-choice sequences (Fig. <ref type="figure" target="#fig_0">1E</ref>). Although there was sometimes a slight preference for certain patterns, such as for rightright-right early in this example session, throughout this session the animal continually employed different choice sequences. On average, animals performed 513 6 13 trials per session (mean 6 SEM; Fig. <ref type="figure" target="#fig_0">1F</ref>). The entropy for three-choice sequences, a measure of the stochasticity in choices, was 2.87 6 0.02 bits, close to the theoretical upper bound of 3 bits. Consequently, the computer was only mildly effective at predicting the mouse's choices, and mice earned an average reward rate of 44.0 6 0.5%. We note that although the reward rate compared favorably with the optimal reward rate of 50%, the difference was statistically significant (p = 1.4 Â 10 À20 , one-sample one-tailed t test). For the ITI, 98.4% of the trials fell within the truncated exponential distribution used to produce the random durations, and only 1.6% of the trials forming a small tail reflecting the extended duration triggered by a precue impulsive lick (Fig. <ref type="figure" target="#fig_0">1G</ref>). Response times were stereotypical and similar for the left and right spouts (Fig. <ref type="figure" target="#fig_0">1H</ref>). These ITI and response time analyses demonstrate effective precue lick suppression and that the animal makes the first physical indication of its choice after cue onset. Altogether, the results show that head-fixed mice can play matching pennies against a computer opponent. Given that humans and macaques likewise play matching pennies imperfectly and not at Nash equilibrium <ref type="bibr" target="#b15">(Erev and Roth, 1998;</ref><ref type="bibr" target="#b28">Lee et al., 2004</ref>), here we found that reward rate was decent but also suboptimal for mice, suggesting that the animals had certain residual tendencies that were exploited by the computer opponent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Animals' behavior was captured by a hybrid model with reinforcement learning and choice kernels</head><p>What is the strategy that characterizes the tendencies in the animal's behavior? Previous studies in macaque monkeys found that reinforcement learning can account in part for the animals' behavior in matching pennies <ref type="bibr" target="#b28">(Lee et al., 2004;</ref><ref type="bibr">Seo et al., 2007)</ref>. We therefore compared between a range of strategies: WSLS, and reinforcement learning algorithms including Q-learning (Q_RPE), differential Q-learning (DQ_RPE), and forgetting Q-learning (FQ_RPE; <ref type="bibr" target="#b22">Ito and Doya, 2009</ref>; see Materials and Methods). Fitting each model to the behavioral data, the BIC values indicated that FQ_RPE was most consistent with the choice behavior of the mice. To further improve the model, we noted that mice exhibited serial choice dependence and sometimes favored picking the same choice in successive trials (e.g., Fig. <ref type="figure" target="#fig_0">1E</ref>). To capture perseverative behavior, we added choice kernels <ref type="bibr" target="#b57">(Wilson and Collins, 2019)</ref> to create the FQ_RPE_CK algorithm. Figure <ref type="figure" target="#fig_1">2A</ref> illustrates graphically the FQ_RPE_CK scheme (henceforth called the "hybrid model"), with reinforcement learning through FQ_RPE and perseveration through choice kernels. Note that both the action values and choice kernels were updated on a trial-by-trial basis, and each had its own learning rate (a and a K ) and inverse temperature parameters (b and b K ; see Materials and Methods). A comparison of all five models indicated that the hybrid model provided the most accurate fit to the behavioral data (Fig. <ref type="figure" target="#fig_1">2B</ref>). Figure <ref type="figure" target="#fig_1">2C</ref> shows a representative session in which we plotted the animal's choices and outcomes along with the latent variables, including action values and choice kernels, estimated by the hybrid model. The model-estimated probability of choosing left (Fig. <ref type="figure" target="#fig_1">2C</ref>, black line) tracked the actual choice pattern (Fig. <ref type="figure" target="#fig_1">2C</ref>, gray line). Specifically, the magnitude of the choice kernel indicates the animal's tendency to choose a previously selected choice. In other words, the animal is more likely to select the action associated with a higher choice kernel value in a given trial, because of a pure choice effect independent from reinforcement. Animals switched the preference side frequently within sessions (34.7 6 1.9 times/session), indicating that although animals exhibit preservative behavior, they are not simply biased for one side, but instead switches their biases frequently as part of a strategy to counter the computer opponent during the matching pennies game. Taken together, these analyses indicate that the mouse's behavior during matching pennies can be quantified using a hybrid model including reinforcement learning and choice kernels. For the remainder of the analyses, we will quantify the animal's strategy using the hybrid model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning strategy in matching pennies in contrast to two-armed bandit task</head><p>To gain further insights into how the mouse competes against a computer opponent, we compared performance between matching pennies and the two-armed bandit task, which is a popular, non-competitive paradigm for assaying reward-based learning in rodents. In our implementation of the two-armed bandit, trials were organized into blocks, and each block was associated with one of two sets of reward probabilities (Fig. <ref type="figure" target="#fig_2">3A</ref>, top panel). The block switched randomly (see Materials and Methods) so that the mouse could not predict the switch with certainty and must infer based on past choices and outcomes. Moreover, the timing of each trial was design to be exactly the same between matching pennies and two-armed bandit (Fig. <ref type="figure" target="#fig_0">1C</ref>). Figure <ref type="figure" target="#fig_2">3A</ref>, bottom panel, presents an example session of a mouse engaging in two-armed bandit, showing that the mouse readily adapting its actions in response to the reversing reward probabilities. The same hybrid FQ_RPE_CK model could be used to quantify the behavior, with the estimated choice probability tracking the mouse's choice pattern (Extended Data Fig. <ref type="figure" target="#fig_2">3-1</ref>). Overall, mice performed the task well, averaging 702 6 26 trials per session with a 43.5 6 1.1% reward rate (n = 26 sessions from 4 mice; Fig. <ref type="figure" target="#fig_2">3B</ref>). Model comparison based on BIC indicated that the hybrid model outperformed WSLS and other Q-learning-based algorithms (Fig. <ref type="figure" target="#fig_2">3C</ref>).</p><p>A head-to-head comparison of the fitted learning parameters highlights the distinct modes of operation employed by the mice for matching pennies versus two-armed bandit (Fig. <ref type="figure" target="#fig_2">3D-I</ref>). In matching pennies, the choice kernel was weighed more strongly than reinforcement learning during action selection [b K /(b 1 b K ) = 0.76 6 0.04; Fig. <ref type="figure" target="#fig_2">3D</ref>], although the sum of inverse temperatures is relatively low, indicating high level of exploration (b 1 b K = 2.15 6 0.07). The ratio of learning rates suggests that the choice kernels were updated slower than the action values (a K /a = 0.22 6 0.04), which is illustrated by the example in Figure <ref type="figure" target="#fig_1">2C</ref>, where the action values fluctuate more rapidly than the choice kernels. In two-armed bandit, the fitted learning parameters were significantly different. Namely, choice kernels had less weight [b K /(b 1 b K ) = 0.29 6 0.03; Fig. <ref type="figure" target="#fig_2">3G</ref>, p = 0.02, two-sided Wilcoxon rank-sum test] and inverse temperature was higher (b 1 b K = 4.50 6 0.40; p = 0.02), indicating that the animals relied more on reinforcement feedback to guide decisions and had a lower tendency to explore.</p><p>These disparate sets of fitted model parameters for the two tasks led to learning in different regimes, which could be visualized by looking at the weighted sum of actionvalue and choice-kernel differences, the crucial parameter for the softmax function for action selection each trial. Here, on most matching pennies trials, animals decided with a weighted sum close to zero, i.e., with near equal probabilities of choosing left and right, making it difficult for the computer opponent to predict their choice (Fig. <ref type="figure" target="#fig_2">3E</ref>). By contrast, performance during two-armed bandit involved weighted sums lying at more extreme values (Fig. <ref type="figure" target="#fig_2">3H</ref>). This difference is consistent with the animal spending considerable number of trials exploiting the high-reward-probability side in a block, and only needed to adapt around a block switch. In both cases, the Finally, to determine how varying the balance between reinforcement learning and choice kernel would affect performance, we simulated choice behavior in the two tasks using a computer agent, varying b K /(b 1 b K ) while fixing a K /a and b 1 b K . For matching pennies, the simulations revealed that the reward rate was relatively stable if the computer agent used mostly reinforcement learning or a hybrid strategy (Fig. <ref type="figure" target="#fig_2">3F</ref>). However, if the computer agent based its actions exclusively on choice kernels, the performance declined precipitously. It was intriguingly to see the b K /(b 1 b K ) estimated from behavioral data lied around the threshold between these two conditions, indicating that the animals might have settled on a hybrid strategy that balanced a trade-off between reward and effort. For two-armed bandit, reward rate is maximized if the agent uses only reinforcement learning to guide decisions. However, we find that the animals have residual tendencies captured by choice kernel and therefore lie away from optimal performance (Fig. <ref type="figure" target="#fig_2">3I</ref>). Collectively, the direct comparison across tasks illustrates the utility of matching pennies for exploring a regime in reinforcement learning that is distinct from the more widely used twoarmed bandit paradigm.   <ref type="formula" target="#formula_9">9</ref>, that was fit to the pupil response in each 100-ms time bin. E, The fraction of sessions with significant regression coefficient for choice in the next trial c n11 , choice in the current trial c n , choice in the previous trial c n-1 , and choice in the trial before the previous trial c n-2 . Red shading indicates the p-value from the x 2 test, without correcting for multiple comparison. F, Same as E for trial outcomes. G, Same as E for the interactions of choice and outcome. H, Same as E for recent reward rate, calculated as a moving average over last 20 trials, and the cumulative reward from start of session to current trial. I, The mean regression coefficients of several predictors: choice of the current trial (c n ), reward of the current trial (r n ). Shading indicates the 95% confidence interval estimated by bootstrap. See also Extended Data Figure <ref type="figure" target="#fig_3">4-1</ref>.</p><formula xml:id="formula_16">Q L Q R K L K R ∆Q ∆K softmax choice outcome RPE CKE B A C 0 0.5 1 Action values Q L Q R Trial K L K R<label>0</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The pupil response contained choice-related and outcome-related signals during matching pennies</head><p>While the mouse played matching pennies, a camera was positioned to capture a video of the right eye (Fig. <ref type="figure" target="#fig_3">4A</ref>). We used the DeepLabCut toolbox <ref type="bibr" target="#b31">(Mathis et al., 2018;</ref><ref type="bibr" target="#b34">Nath et al., 2019)</ref> to extract pupil size from the videos (see Materials and Methods). Figure <ref type="figure" target="#fig_3">4B</ref> shows an example frame with manually selected and computergenerated labels. To quantify the quality of the automatic labeling, we calculated the deviations between the manually selected and automatically estimated labels for the uppermost, lowermost, leftmost, rightmost, and central points of the mouse's pupil (Extended Data Fig. <ref type="figure" target="#fig_3">4-1A,B</ref>). The mean values of the deviations were close to zero, demonstrating that the estimates had little bias. To give an intuition into the pupil size fluctuation observed, for one session, we plotted the time course of the pupil diameter after z score normalization (Extended Data Fig. <ref type="figure" target="#fig_3">4-1C</ref>). When aligned to select trial types, there were obvious task-related transients in the pupil size (Extended Data Fig. <ref type="figure" target="#fig_3">4-1D</ref>). In this study, we were interested in the relation between pupil fluctuations and decision-related variables on a trial-by-trial basis, therefore we calculated the pupil response for each trial, which was defined as the pupil size in z score minus the precue baseline z score (Fig. <ref type="figure" target="#fig_3">4C</ref>).</p><p>To characterize the factors that drive pupil responses during matching pennies, we used multiple linear regression. For each session, we fitted a regression model to determine the relation between the pupil responses and the choices, outcomes, reinforcers (choice-outcome interactions), the recent reward rate, and the cumulative reward (Fig. <ref type="figure" target="#fig_3">4D</ref>). Specifically, the choices, outcomes, and interactions included terms for the next trial, the current  <ref type="formula" target="#formula_13">12</ref>. B, The fraction of sessions with significant regression coefficient for the choice c n , outcome r n , and the interaction c n x r n in the current trial. Red shading indicates the p-value from the x 2 test, without correcting for multiple comparison. C, Same as B for the choice, outcome, and interaction in the previous trial. D, Same as B for the difference in action values (Q L n À Q R n ), the action value of the chosen action (Q chosen n ), and the difference in the choice kernel (K L n À K R n ). E, Same as B for choice kernel of the chosen action (K chosen n ), moving-average reward rate ( r MA n ), and cumulative reward (r Cum:</p><formula xml:id="formula_17">n ).</formula><p>trial, the last trial, and the trial before last to capture the potential persistent effects of these variables on neural correlates <ref type="bibr">(Seo and Lee, 2007;</ref><ref type="bibr" target="#b48">Sul et al., 2010;</ref><ref type="bibr" target="#b4">Bari et al., 2019;</ref><ref type="bibr" target="#b46">Siniscalchi et al., 2019)</ref>. The analyses revealed that pupil responses were modulated by choices, outcomes, and reinforcers during matching pennies (Fig. <ref type="figure" target="#fig_3">4E-G</ref>). For a significant fraction of sessions, we detected a change in pupil size signaling the upcoming choice well before the trial would start (c n11 ; Fig. <ref type="figure" target="#fig_3">4E</ref>). The early choice-related signal suggested that the animal was planning and preparing for the upcoming action before the cue. The choice-related signal ceased abruptly at around the time of the cue onset for the next trial (c n-1 ; Fig. <ref type="figure" target="#fig_3">4E</ref>). Meanwhile, outcome-related and reinforcer-related signals in the pupil responses emerged after the potential reward would be delivered and persisted for the next two trials (r n-1 , r n-2 , c n-1 * r n-1 , c n-2 * r n-2 ; Fig. <ref type="figure" target="#fig_3">4F,</ref><ref type="figure">G</ref>). The pupil responses were also influenced by the cumulative reward (r Cum n ; Fig. <ref type="figure" target="#fig_3">4H</ref>), which related to the number of trials performed, and therefore might reflect the motivational state of the animal. To examine whether these factors dilate or constrict the pupil, we plotted the average coefficients of the current choice and outcome. Although choice consistently influenced the pupil responses, the mean amplitude of the effect was more muted than outcome. Specifically, the presence of a reward led a large phasic dilation of the pupil (r n ; Fig. <ref type="figure" target="#fig_3">4I</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pupil response was modulated by latent variables related to value updating, but not action selection</head><p>Previously we showed that the animals' behavior can be captured by the hybrid FQ_RPE_CK model, therefore we next asked whether pupil responses may accompany changes in select latent decision variables. To this end, we built additional multiple linear regression models using latent variables relevant for action selection or value updating. For action selection, we considered action-value difference, chosen value, choice-kernel difference, and chosen choice kernel (Fig. <ref type="figure" target="#fig_4">5A</ref>). Consistent with our prior observation, we found significant choice-related and outcome-related signals (Fig. <ref type="figure" target="#fig_4">5B,</ref><ref type="figure">C</ref>); however, there was no reliable, sustained modulation of the pupil responses by the latent variables used for action selection (Q <ref type="figure" target="#fig_4">5D,</ref><ref type="figure">E</ref>). For value updating, we tested latent variables including RPE and CKE in addition to actionvalue difference and choice-kernel difference (Fig. <ref type="figure" target="#fig_5">6A</ref>). In a significant fraction of sessions, pupil responses were modulated by the RPE, and to a lesser extent CKE and choice-kernel difference (Fig. <ref type="figure" target="#fig_5">6B-D</ref>). Note that the RPE is calculated as the difference of current reward and action value of the chosen choice, thus correlated with the current outcome. To distinguish them, we grouped the trials according to the sign of RPE, then run multiple linear regressions on groups with positive or negative RPE values separately. In this analysis, linear regression with outcome is trivial, since positive RPE trials are always rewarded, while negative RPE trials are always trials without reward. Characteristically, mean coefficient was positive for both ). The results were obtained by fitting Equation 12 as we did in Figure <ref type="figure" target="#fig_4">5</ref> but onto pupil responses from two-armed bandit task. C, Same as A but for the RPE and the CKE. The results were obtained by fitting Equation 13 as we did in Figure <ref type="figure" target="#fig_5">6</ref> but onto pupil responses from two-armed bandit task. trials involving positive or negative RPE (Fig. <ref type="figure" target="#fig_5">6E</ref>). In other words, positive RPE led to phasic dilation of the pupil, whereas negative RPE was associated with transient reduction in pupil size. Overall, these analyses show that transient change in pupil diameter is modulated by latent variables used in value updating, namely, the RPE.</p><formula xml:id="formula_18">L n À Q R n , K L n À K R n ; Fig.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similar pupil correlates for decision-related variables during two-armed bandit task</head><p>The results presented so far indicated that for mice playing the matching pennies game, the transient pupil dilations were modulated by choice, outcome, and latent decision variables relevant for value updating. To determine whether comparable pupil responses occur for two-armed bandit, we made recordings during the task and applied the same multiple linear regression analyses. We found that the choice-dependent and outcome-dependent signals were present in a significant fraction of sessions (Fig. <ref type="figure" target="#fig_6">7A</ref>). The latent variables for action selection were largely absent (Fig. <ref type="figure" target="#fig_6">7B</ref>). The value updating variables, especially the RPE and again to a lesser degree the CKE, were associated with transient pupil responses (Fig. <ref type="figure" target="#fig_6">7C</ref>). Therefore, our results show that factors that influenced pupil responses in matching pennies, choice, outcome, and value updating variables were also contributors to the pupil responses during the twoarmed bandit task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correlated fluctuations of left and right pupils during the two-armed bandit task</head><p>One intriguing result is that pupil response was influenced by choice, i.e., whether the animal was choosing left or right. Is this a genuine choice-related signal? Some previous studies show that pupil dilations can predict the upcoming choice of human subjects including the choice timing and the selection of one out of five digits <ref type="bibr" target="#b14">(Einhauser et al., 2010)</ref>, and the decision of a yes/no question (de Gee et al., 2014). However, another possibility is that when the animal made a choice, the tongue lick movement could be associated with facial movements leading to spurious detection of pupil size changes. To clarify the issue, we positioned two cameras to record both eyes simultaneously during the two-armed bandit task (Fig. <ref type="figure" target="#fig_7">8A</ref>). For each eye, we applied multiple linear regression (Fig. <ref type="figure" target="#fig_3">4D</ref>) to analyze the influences of choices and outcomes on pupil responses. We reasoned that if the choice-related signal was a movement artifact, then the aberrant signal should differ across eyes and across animals, and therefore the coefficients extracted from left and right eye would be uncorrelated. By contrast, if the choice-related signal was related to the animal's internal decision, we would expect consistent dilation responses in both eyes. We analyzed the coefficients for the current choice between 3 and 5 s from cue onset when the pupil responses were largest (Fig. <ref type="figure" target="#fig_7">8B</ref>). The choice-related signal for the left pupil were correlated with that for the right pupil in every session (r = 0.83, p = 9 Â 10 À43 ). The positive correlation coefficient indicated that the pupil size changes are symmetric in the two pupils. As a positive control, we plotted the coefficients of the current outcome in the same time window, which was expected not to be lateralized and indeed showed a positive correlation coefficient (r = 0.90, p = 0; Fig. <ref type="figure" target="#fig_7">8C</ref>). Taken together, these results suggested that the effect of choice on transient pupil response could not be explained by simple movement artifact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The present study has two main findings. First, we demonstrated that head-fixed mice can play a two-player competitive game against a computer opponent. Their tendencies in the matching pennies game can be described by a computational model incorporating reinforcement learning and choice kernels. Second, we showed that transient pupil responses of the animals were associated with observable variables such as choices and outcomes, as well as latent variables relevant for value updating, but not action selection. We note that majority of the mice used for this study were male (12 males, 1 female), therefore the results may not generalize to female mice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance in the matching pennies game</head><p>Iterative matching pennies is a classic competitive game. Subjects playing the game tend to deviate from the Nash equilibrium. For example, human players attempting to generate random choices would switch too often <ref type="bibr" target="#b8">(Camerer, 2003)</ref>. Macaque monkeys and chimpanzees also showed deviation from the optimal strategy <ref type="bibr">(Lee et al., 2004;</ref><ref type="bibr" target="#b47">Soltani et al., 2006;</ref><ref type="bibr">Seo and Lee, 2009</ref>   <ref type="bibr">., 2009;</ref><ref type="bibr" target="#b30">Martin et al., 2014)</ref>. Rats were shown to counter-predict the opponent's choice first and switch to a more random behavior when facing a strong competitor <ref type="bibr" target="#b52">(Tervo et al., 2014)</ref>. Moreover, pigeons playing the matching pennies game exhibit a similar divergence from the optimal play as the humans do <ref type="bibr" target="#b38">(Sanabria and Thrailkill, 2009)</ref>. Here, we showed that head-fixed mouse can play matching pennies at a high level, albeit unsurprisingly also gaining rewards at below optimal rates. Demonstrating that mice can be studied using competitive game paradigms opens new avenues for studying neural circuitry for reward learning in animal models of neuropsychiatric disorders <ref type="bibr" target="#b6">(Barthas et al., 2020;</ref><ref type="bibr" target="#b29">Liao and Kwan, 2021)</ref>.</p><formula xml:id="formula_19">; Seo et A B C -2 -1 0 1 2 c(n) coefficients, left pupil -2 -1 0 1 2 c(n) coefficients, right pupil -2 -1 0 1 2 r(n) coefficients, left pupil r(n) coefficients,</formula><p>Although initial characterizations of the tendencies for sub-optimal play have relied on standard learning algorithms <ref type="bibr" target="#b15">(Erev and Roth, 1998;</ref><ref type="bibr" target="#b28">Lee et al., 2004)</ref>, recent studies have reported deviations from the predictions of reinforcement learning <ref type="bibr" target="#b18">(Hampton et al., 2008;</ref><ref type="bibr" target="#b44">Seo et al., 2014;</ref><ref type="bibr" target="#b52">Tervo et al., 2014)</ref>. In this study, we employed a hybrid model, which combines Q-learning with choice kernels, also known as the choice-autocorrelation factor <ref type="bibr" target="#b25">(Katahira, 2015;</ref><ref type="bibr" target="#b57">Wilson and Collins, 2019)</ref>. Specifically, choice kernels were included to capture serial choice dependency, which is commonly observed in humans and animals performing various tasks <ref type="bibr" target="#b1">(Akaishi et al., 2014;</ref><ref type="bibr" target="#b0">Abrahamyan et al., 2016)</ref>. The hybrid model indeed fit the behavior better than variations of Q-learning algorithms (Fig. <ref type="figure" target="#fig_1">2B</ref>). We want to highlight the numerical simulations where the reward rate was examined as a function of relative inverse temperature (Fig. <ref type="figure" target="#fig_2">3F</ref>). The animal's performance lied at a regime where further increase in reliance on choice kernel would deteriorate sharply the reward rate. The result hints at the possibility that the animal may be maximizing a reward-effort trade-off, because repeating the same choice is likely to be less effortful and indeed is the strategy taken by the animal often near the end of a session.</p><p>Many studies of flexible decision-making in rodents have relied on two-armed bandit tasks with a blockbased design <ref type="bibr" target="#b22">(Ito and Doya, 2009;</ref><ref type="bibr" target="#b48">Sul et al., 2010</ref><ref type="bibr" target="#b49">Sul et al., , 2011;;</ref><ref type="bibr" target="#b51">Tai et al., 2012;</ref><ref type="bibr" target="#b4">Bari et al., 2019;</ref><ref type="bibr" target="#b17">Groman et al., 2019;</ref><ref type="bibr" target="#b20">Hattori et al., 2019)</ref>. This paradigm has many merits, but also a few shortcomings. First, the length of the blocks is a key parameter that defines the volatility of the environment, but is typically manually set by the experimenter. By contrast, matching pennies refers to a payoff matrix for gameplay, but contain no other experimenter-defined parameters. Second, within a block, it is advantageous for the animals to continually exploit the high-rewardprobability option, therefore overtraining in the two-armed bandit task may lead to strong serial choice dependencies. To the contrary, in matching pennies, the computer opponent is designed to detect such dependencies and exert competitive pressure on the animal, therefore the animal is encouraged to always diversify its choice patterns during the session. Consequently, two-player games such as matching pennies are elegant and simple in design and allow for investigation of neural mechanisms underlying flexible decision-making under a regime that is quite different from multiarmed bandit tasks (Fig. <ref type="figure" target="#fig_2">3D-I</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pupil responses and potential neuromodulatory mechanisms</head><p>Pupil fluctuation is an indicator of the arousal state of an animal, and likely associates with the levels of various neuromodulators in the forebrain <ref type="bibr" target="#b32">(McGinley et al., 2015)</ref>. The relationship between pupil size and NE is supported by prior results, which showed reliable tracking of pupil fluctuations to activity of noradrenergic axons in the neocortex <ref type="bibr" target="#b37">(Reimer et al., 2016)</ref>. Furthermore, studies of the LC, the main source of NE for the forebrain, demonstrated a correlation between single unit firing in LC and pupil diameter <ref type="bibr" target="#b2">(Aston-Jones and Cohen, 2005;</ref><ref type="bibr" target="#b59">Yang et al., 2021)</ref>, and pupil dilation triggered by electrical microstimulation of the LC <ref type="bibr" target="#b24">(Joshi et al., 2016)</ref>. Several studies have linked pupil dynamics and activity in LC to choice behavior, such as in the consolidation of the previous choices <ref type="bibr" target="#b11">(Clayton et al., 2004;</ref><ref type="bibr" target="#b14">Einhauser et al., 2010)</ref> or the shaping of upcoming actions <ref type="bibr" target="#b13">(de Gee et al., 2014)</ref>. However, these studies were based on visual perceptual tasks, which is different from our task design where the cue is auditory and carries no relevant information except for trial timing. Our results hint at a potential role for noradrenergic signaling in postdecisional value updating, because the pupil response was correlated with the RPE and weakly associated with CKE. This would agree with work that have showed the importance for rewards in LC activity and pupil dilation in various behavioral settings <ref type="bibr" target="#b39">(Sara and Segal, 1991;</ref><ref type="bibr" target="#b3">Aston-Jones et al., 1997;</ref><ref type="bibr" target="#b55">Varazzani et al., 2015)</ref>. Furthermore, there is strong evidence linking pupil changes to errors and adaptations in decision tasks involving trial-based or block-based inference <ref type="bibr" target="#b33">(Nassar et al., 2012;</ref><ref type="bibr" target="#b53">Urai et al., 2017)</ref>, highlighting the role of pupil-linked systems to control the influence of incoming data to guide future decisions.</p><p>Overall, the current study lays the groundwork for studying reward-based learning in mice using competitive games. The two-player matching pennies game, which we showed the mouse can play against a computer opponent, may potentially be extended to a mouse competing against another mouse in the future. The paradigm may therefore provide a quantitative framework for evaluating social decision-making in mice. The findings of significant pupil correlates to the major decision-related variables during matching pennies provide clues to the neuromodulatory mechanisms in mice. The current results open avenues for future research into the role of neuromodulators in mediating the adaptive and social aspects of decisionmaking in mice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Performance of head-fixed mice in a matching pennies game. A, A schematic illustration of the competitive game. Headfixed mouse makes a left or right choice by licking the spouts. A computer monitors the mouse's past choices and outcomes, generating its own left or right choice every trial. B, The payoff matrix of the game. The mouse receives a water reward if it chooses the same choice as the computer does in the same trial. C, Trial timing: the mouse waits for a go cue, licks a spout to indicate its response, and the outcome is delivered immediately. A random ITI follows the outcome. D, An example session. The reward rate for this session was 52.1%. Top, The mouse's choices and outcomes. Bottom, The computer's choices. Blue and red bars indicate right (R) and left choices (L), respectively. Black bars indicate rewards. E, Cumulative number of different three-choice patterns detected as the mouse progressed in the session shown in D. F, Summary from 81 sessions. Left, The average trials performed each session is 513 6 13. Middle, The average entropy of the three-choice sequences is 2.87 6 0.02. Right, The average reward rate is 44.0 6 0.5%. G, The histogram of the ITI durations for all trials. ITI would range from 4 to 8 s long if the mouse did not lick to trigger additions to the ITI. H, The response times for trials in which the mouse chose left (left) or right (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Computational modeling of the animals' behavior during matching pennies. A, Schematic of the hybrid FQ_RPE_CK model. Q L ; Q R denote the action values for left and right choices; K L ; K R denote the choice kernels for left and right choices. The agent chooses based on a weighted sum of action-value and choice-kernel differences (DQ, DK). The chosen action is used to compute the CKE, which is used to update the choice kernels. The outcome is used to calculate the RPE, which is used to update the action values. B, Model comparison using BIC. C, An example of the time course of the latent variables and predicted behavior. Top, Long red and blue bars indicate rewarded left and right choices; short red and blue bars indicate unrewarded left and right choices. Gray line shows the observed probability to choose left, smoothed by a Gaussian kernel. Black line shows the probability to choose left predicted by the hybrid model. Middle, The action values of left (red) and right (blue) choices estimated by the hybrid model. Bottom, The choice kernels of left (red) and right (blue) choices estimated by the hybrid model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparison between matching pennies and two-armed bandit: behavioral results, model fitting, and computer simulations. A, A schematic diagram of the two-armed bandit task. Top, Head-fixed mouse makes a left or right choice by licking the spouts. The trials are separated into different blocks based on the high (0.7) or low (0.1) reward probability assigned to each of the two choices. Middle, The assigned reward probabilities for the left (red) and right (blue) choices in an example session. Bottom, The choices and outcomes in the same session. The reward rate was 52.8%. Blue and red bars indicate right and left choices. Black bars indicate rewards. B, Summary from 26 sessions. Top, The average trials performed each session is 702 6 26. Bottom, The average reward rate is 43.5 6 1.1%. C, Model comparison using BIC. D, Learning parameters extracted from fitting the hybrid model to the matching pennies data. Left, The relative weight of choice kernel, b K /(b 1 b K ) = 0.75 6 0.04. Middle, Sum of inverse temperature, b 1 b K = 2.15 6 0.07. Right, Relative learning rate of the choice kernel. a K /a = 0.22 6 0.04. E, Psychometric curve based on the whole dataset. Black histograms indicate the distribution of trials according to the weighted sum of the difference of action values (DQ) and the difference of choice kernels (DK). Dashed purple line shows the predicted probability to choose left according to the softmax equation used by the hybrid model; purple dots show the observed probability to choose left. F, The performance of a computational agent playing the game, where b K /(b 1 b K ) was varied, while the learning rates and (b 1 b K ) were fixed and set to be the median of the fitted values based on animal data. The solid and open dots indicate the median b K /(b 1 b K ) value fitted based on animal data. G-I, Same as D-F for the two-armed bandit task. See also Extended Data Figure 3-1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Effects of choices and outcomes on pupil responses during matching pennies. A, A schematic illustration of the pupillometry set up. B, An example still frame from a video showing both human labeling (dot) and DLC labeling (cross) for five labels. Scale bar: 50 pixels. C, The pupil response at any time during a trial (À1-5 s from the cue) is the z score at the corresponding time subtracted by the baseline, which is the mean z score between À2 and À1 s before the cue onset. D, A schematic diagram of the multiple linear regression model, i.e., Equation 9, that was fit to the pupil response in each 100-ms time bin. E, The fraction of sessions with significant regression coefficient for choice in the next trial c n11 , choice in the current trial c n , choice in the previous trial c n-1 , and choice in the trial before the previous trial c n-2 . Red shading indicates the p-value from the x 2 test, without correcting for multiple comparison. F, Same as E for trial outcomes. G, Same as E for the interactions of choice and outcome. H, Same as E for recent reward rate, calculated as a moving average over last 20 trials, and the cumulative reward from start of session to current trial. I, The mean regression coefficients of several predictors: choice of the current trial (c n ), reward of the current trial (r n ). Shading indicates the 95% confidence interval estimated by bootstrap. See also Extended Data Figure 4-1.</figDesc><graphic coords="11,82.49,67.98,420.00,531.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Effects of decision variables for action selection on pupil responses during matching pennies. A, A schematic diagram of the multiple linear regression model, i.e., Equation12. B, The fraction of sessions with significant regression coefficient for the choice c n , outcome r n , and the interaction c n x r n in the current trial. Red shading indicates the p-value from the x 2 test, without correcting for multiple comparison. C, Same as B for the choice, outcome, and interaction in the previous trial. D, Same as B for the difference in action values (Q L n À Q R n ), the action value of the chosen action (Q chosen</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Effects of decision variables for value updating on pupil responses during matching pennies. A, A schematic diagram of the multiple linear regression model, i.e., Equation 13. B, The fraction of sessions with significant regression coefficient for choice in the current trial c n , choice in the previous trial c n-1 , and outcome in the previous trial r n-1 . Red shading indicates the p-value from the x 2 test, without correcting for multiple comparison. C, Same as B for difference in action values (Q L n À Q R n ), the RPE, and the difference in choice kernels (K L n À K R n ). D, Same as B for the choice kernel error (CKE), moving-average reward rate ð r MA n Þ, and cumulative reward ðr Cum: n Þ. E, The mean regression coefficients for RPE (quantified separately for trials with positive or negative RPE) and CKE. Shading indicates the 95% confidence interval estimated by bootstrap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Multiple linear regression analyses of factors influencing pupil responses during two-armed bandit. A, The fraction of sessions with significant coefficient for choice in the next trial c n11 , choice in the current trial c n , choice in the previous trial c n-1 , outcome in the next trial r n11 , outcome in the current trial r n , and outcome in the previous trial r n-1 . The results were obtained by fitting Equation 9 as we did in Figure 4 but onto pupil responses from two-armed bandit task. Red shading indicates the p-value from the x 2 test, without correcting for multiple comparison. B, Same as A but for the difference in action values (Q L n À Q R n ), the action value of the chosen action (Q chosen n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure8. Two-pupil recordings during the bandit task. A, Schematics of the two-pupil recordings setup. Two cameras were placed in front of the two pupils with the same angle while the mouse was performing the task. B, Scatter plot of the linear regression coefficients of the current choice within the 3-5 s from the cue time. The linear regression is the same as shown in Figure4. x-axis: coefficients of the left pupil; y-axis: coefficients of the right pupil. Different colors represent different subjects. Each dot represents a 0.1-s interval within the 3-to 5-s period. The black line shows the diagonal when coefficients of the left pupil equal to that of the right pupil. C, Same as B for current outcome.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>al</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>March/April 2022, 9(2) ENEURO.0457-21.2022   eNeuro.org</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported by <rs type="funder">National Institutes of Health (NIH)</rs>/<rs type="funder">National Institute of Mental Health</rs> Grants <rs type="grantNumber">R01MH112750</rs>, <rs type="grantNumber">R01MH121848</rs>, and <rs type="grantNumber">R21MH118596</rs> (to A.C.K.); <rs type="grantName">China Scholarship Council-Yale World Scholars Fellowship</rs> (H.W.); <rs type="grantName">Gruber Science Fellowship</rs> (H.K.O.); the <rs type="funder">NIH Training</rs> Grant <rs type="grantNumber">T32NS007224</rs> (to H.K.O.); and <rs type="funder">Kavli Institute for Neuroscience Postdoctoral Fellowship</rs> (H.A.). Acknowledgements: We thank <rs type="person">Daeyeol Lee</rs> and <rs type="person">Hyojung Seo</rs> for helping with programming the matching pennies game, <rs type="person">Michael Siniscalchi</rs> for help with behavioral training, and <rs type="person">Matthew McGinley</rs> for advice on the pupil recording setup.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GZswPDq">
					<idno type="grant-number">R01MH112750</idno>
				</org>
				<org type="funding" xml:id="_ts2tz65">
					<idno type="grant-number">R01MH121848</idno>
				</org>
				<org type="funding" xml:id="_wEuDUC4">
					<idno type="grant-number">R21MH118596</idno>
					<orgName type="grant-name">China Scholarship Council-Yale World Scholars Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_WkcB7nc">
					<idno type="grant-number">T32NS007224</idno>
					<orgName type="grant-name">Gruber Science Fellowship</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptable history biases in human perceptual decisions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abrahamyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Dakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carandini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Natl Acad Sci</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="3548" to="E3557" />
			<date type="published" when="2016">2016</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Autonomous mechanism of internal choice estimate underlies decision inertia</title>
		<author>
			<persName><forename type="first">R</forename><surname>Akaishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Umeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nagase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="195" to="206" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An integrative theory of locus coeruleus-norepinephrine function: adaptive gain and optimal performance</title>
		<author>
			<persName><forename type="first">G</forename><surname>Aston-Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu Rev Neurosci</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="403" to="450" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conditioned responses of monkey locus coeruleus neurons anticipate acquisition of discriminative behavior in a vigilance task</title>
		<author>
			<persName><forename type="first">G</forename><surname>Aston-Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rajkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kubiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="697" to="715" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stable representations of decision variables for flexible behavior</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubin</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Cressy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="922" to="933" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>e7</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Prefrontal cortex and decision making in a mixed-strategy game</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Barraclough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="404" to="410" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cumulative effects of social stress on rewardguided actions and prefrontal cortical activity</title>
		<author>
			<persName><forename type="first">F</forename><surname>Barthas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Mineur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Picciotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol Psychiatry</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="541" to="553" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Network reset: a simplified overarching theory of locus coeruleus noradrenaline function</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bouret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Sara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Neurosci</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="574" to="582" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Behavioral game theory: experiments in strategic interaction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Camerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive properties of differential learning rates for positive and negative outcomes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Cazé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Van Der Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol Cybern</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="711" to="719" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neuronal reference frames for social decisions in primate frontal cortex</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gariépy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="243" to="250" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Phasic activation of monkey locus ceruleus neurons by simple decisions in a forced-choice task</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Clayton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rajkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aston-Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="9914" to="9920" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Specialized medial prefrontal-amygdala coordination in other-regarding decision preference</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dal Monte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ccj</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Fagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swc</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="565" to="574" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Decision-related pupil dilation reflects upcoming choice and individual bias</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>De Gee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Knapen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Donner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Natl Acad Sci</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="618" to="E625" />
			<date type="published" when="2014">2014</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pupil dilation betrays the timing of decisions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Einhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Hum Neurosci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predicting how people play games: reinforcement learning in experimental games with unique, mixed strategy equilibria</title>
		<author>
			<persName><forename type="first">I</forename><surname>Erev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am Econ Rev</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="848" to="881" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pupil diameter tracks changes in control state predicted by the adaptive gain theory of locus coeruleus function</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Gilzenrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nieuwenhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jepma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn Affect Behav Neurosci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="252" to="269" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Orbitofrontal circuits control multiple reinforcement-learning processes</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Groman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Keistler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Keip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hammarlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Dileone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pittenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="734" to="746" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural correlates of mentalizing-related computations during strategic interactions in humans</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Hampton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bossaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Natl Acad Sci</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="6741" to="6746" />
			<date type="published" when="2008">2008</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neuronal prediction of opponent&apos;s behavior during cooperative social interchange in primates</title>
		<author>
			<persName><forename type="first">K</forename><surname>Haroush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="1233" to="1245" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Areaspecificity and plasticity of history-dependent value coding during learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Danskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Babic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mlynaryk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Komiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="1858" to="1872" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>e15</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pupil size as related to interest value of visual stimuli</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Polt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="349" to="350" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Validation of decision-making models and analysis of decision variables in the rat basal ganglia</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="9861" to="9874" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pupil diameter predicts changes in the exploration-exploitation trade-off: evidence for the adaptive gain theory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jepma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nieuwenhuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Cogn Neurosci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1587" to="1596" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relationships between pupil diameter and neuronal activity in the locus coeruleus, colliculi, and cingulate cortex</title>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Kalwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Gold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="221" to="234" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The relation between reinforcement learning parameters and the influence of reinforcement history on choice behavior</title>
		<author>
			<persName><forename type="first">K</forename><surname>Katahira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Math Psychol</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="59" to="69" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The statistical structures of reinforcement learning with asymmetric value updates</title>
		<author>
			<persName><forename type="first">K</forename><surname>Katahira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Math Psychol</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="31" to="45" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Game theory and neural basis of social decision making</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="404" to="409" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reinforcement learning and decision making in monkeys during a competitive game</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Mcgreevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Barraclough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Res Cogn Brain Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="45" to="58" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Applying Reinforcement Learning to Rodent Stress Research</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chronic Stress (Thousand Oaks)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">2470547020984732</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Chimpanzee choice rates in competitive games match equilibrium game theory predictions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bossaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matsuzawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Camerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5182</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mamidanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Cury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1281" to="1289" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Waking state: rapid variations modulate neural and behavioral responses</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcginley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Batista-Brito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zagha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Cadwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Cardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1143" to="1161" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rational regulation of learning dynamics by pupil-linked arousal systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Rumsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Heasly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Gold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1040" to="1046" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using DeepLabCut for 3D markerless pose estimation across species and behaviors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mathis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Protoc</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2152" to="2176" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tonic noradrenergic activity modulates explorative behavior and attentional set shifting: evidence from pupillometry and gaze pattern analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pajkossy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Á</forename><surname>Szo †llo †si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Demeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Racsmány</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1839" to="1854" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The pupil and stimulus probability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiyuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Richer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Wagoner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beatty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="530" to="534" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pupil fluctuations track rapid changes in adrenergic and cholinergic activity in cortex</title>
		<author>
			<persName><forename type="first">J</forename><surname>Reimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcginley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rodenkirch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Tolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Commun</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13289</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pigeons (Columba livia) approach Nash equilibrium in experimental Matching Pennies competitions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Thrailkill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Exp Anal Behav</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="169" to="183" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Plasticity of sensory responses of locus coeruleus neurons in the behaving rat: implications for cognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Sara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Segal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prog Brain Res</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="571" to="585" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Temporal filtering of reward signals in the dorsal anterior cingulate cortex during a mixed-strategy game</title>
		<author>
			<persName><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="8366" to="8377" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Behavioral and neural changes after gains and losses of conditioned reinforcers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3627" to="3641" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dynamic signals related to choices and outcomes in the dorsolateral prefrontal cortex</title>
		<author>
			<persName><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Barraclough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cereb Cortex</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="110" to="117" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>Suppl 1</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lateral intraparietal cortex and reinforcement learning during a mixed-strategy game</title>
		<author>
			<persName><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Barraclough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7278" to="7289" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural correlates of strategic reasoning during competitive games</title>
		<author>
			<persName><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="page" from="340" to="343" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fast and slow transitions in frontal ensemble activity during flexible sensorimotor behavior</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Phoumthipphavong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1234" to="1242" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Enhanced population coding for rewarded choices in the medial frontal cortex of the mouse</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cereb Cortex</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4090" to="4106" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural mechanism for stochastic behaviour during a competitive game</title>
		<author>
			<persName><forename type="first">A</forename><surname>Soltani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1075" to="1090" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Distinct roles of rodent orbitofrontal and medial prefrontal cortex in decision making</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Sul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="449" to="460" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Role of rodent secondary motor cortex in value-based action selection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Sul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1202" to="1208" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Reinforcement learning: an introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Transient stimulation of distinct subpopulations of striatal neurons mimics changes in action value</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Benavidez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bonci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1281" to="1289" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Behavioral variability through stochastic choice and its gating by anterior cingulate cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Tervo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Proskurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vollmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Karpova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="21" to="32" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pupil-linked arousal is driven by decision uncertainty and alters serial choice bias</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Urai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Donner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Commun</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14637</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">How pupil responses track value-based decision-making during and after reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Slooten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jahfari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Knapen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Theeuwes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1006632</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Noradrenaline and dopamine neurons in the reward/effort trade-off: a direct electrophysiological comparison in behaving monkeys</title>
		<author>
			<persName><forename type="first">C</forename><surname>Varazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>San-Galli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gilardeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bouret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="7866" to="7877" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ubiquity and specificity of reinforcement signals throughout the human brain</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Vickery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="166" to="177" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Ten simple rules for the computational modeling of behavioral data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Elife</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">49547</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Cooperation in rats playing the iterated prisoner&apos;s dilemma game</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anim Behav</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Locus coeruleus spiking differently correlates with S1 cortex activity and pupil diameter in a tactile detection task</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Elife</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">64327</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
