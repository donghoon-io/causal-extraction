<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disentangling and Learning Robust Representations with Natural Clustering</title>
				<funder ref="#_gGbkdxN">
					<orgName type="full">Government of Aragon</orgName>
				</funder>
				<funder ref="#_rRfAqSz">
					<orgName type="full">European Social Fund</orgName>
				</funder>
				<funder>
					<orgName type="full">Spanish Ministry of Economy and Competitiveness</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-11-05">5 Nov 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Javier</forename><surname>Antorán</surname></persName>
							<email>javier.a.es@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Aragón Institute for Engineering Research (I3A)</orgName>
								<orgName type="laboratory">ViVoLab</orgName>
								<orgName type="institution">University of Zaragoza Zaragoza</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antonio</forename><surname>Miguel</surname></persName>
							<email>amiguel@unizar.es</email>
							<affiliation key="aff1">
								<orgName type="department">Aragón Institute for Engineering Research (I3A)</orgName>
								<orgName type="laboratory">ViVoLab</orgName>
								<orgName type="institution">University of Zaragoza Zaragoza</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Disentangling and Learning Robust Representations with Natural Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-11-05">5 Nov 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1901.09415v3[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T18:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Representation Learning</term>
					<term>Dimensionality reduction</term>
					<term>Disentangling</term>
					<term>Natural Clustering</term>
					<term>Variational Autoencoders</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning representations that disentangle the underlying factors of variability in data is an intuitive way to achieve generalization in deep models. In this work, we address the scenario where generative factors present a multimodal distribution due to the existence of class distinction in the data. We propose N-VAE, a model which is capable of separating factors of variation which are exclusive to certain classes from factors that are shared among classes. This model implements an explicitly compositional latent variable structure by defining a class-conditioned latent space and a shared latent space. We show its usefulness for detecting and disentangling class-dependent generative factors as well as its capacity to generate artificial samples which contain characteristics unseen in the training data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Disentangled representations, defined as those where each latent variable is responsible for only one generative factor in the data while being relatively invariant to other factors <ref type="bibr" target="#b0">[1]</ref>, allow for more easily interpretable models, <ref type="bibr" target="#b1">[2]</ref>, and facilitate generalization to previously unseen combinations of features, <ref type="bibr" target="#b2">[3]</ref>. They also generalize to new domains that share underlying factors of variation, <ref type="bibr" target="#b3">[4]</ref>.</p><p>Typically, variational autoencoder (VAE) <ref type="bibr" target="#b4">[5]</ref> based disentanglement methods impose an isotropic Gaussian prior on their latent space. However, any dataset that can be separated into classes will present some form of multimodality. Despite the unimodal prior, in these scenarios, the inferred posterior is often multimodal. This creates dependencies between latent dimensions and reduces reconstruction accuracy, <ref type="bibr" target="#b5">[6]</ref>.</p><p>Moreover, there can exist underlying factors of variation in data that are exclusive to certain classes. An example of this phenomenon is found in the MNIST dataset. The number seven can be drawn with a horizontal line crossing its stem or without it. The number two can have a flat base or one with a loop. We show how these class-exclusive factors are disentangled by our proposed model in fig. <ref type="figure" target="#fig_0">1</ref>. Other factors of variability, such as stroke thickness or azimuth, are shared among all classes, as can be seen in fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>The natural clustering prior <ref type="bibr" target="#b0">[1]</ref> states that probability mass concentrates on a set of low dimensional manifolds. Each Our main contributions are the following: 1) We formulate a lower bound on the joint log-likelihood of inputs and class labels. It can be optimized approximately by simply adding a classification objective to the standard VAE cost function.</p><p>2) We propose N-VAE, a model that is capable of capturing both generative factors which are class-dependent and ones that are shared among classes. To the best of our knowledge, we are the first to disentangle these two types of factors. 3) We show how N-VAE can be used for detection and disentanglement of mode-dependent factors of variation. We also evaluate the expressivity<ref type="foot" target="#foot_0">foot_0</ref> of N-VAE relative to other classconditioned generative models by comparing the usefulness of their generated samples for dataset augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head><p>We first give a brief overview of the variational autoencoder, as N-VAE builds upon this model. We then give a recap of some recent techniques for disentangling in VAEs. We show that, despite the popularity of these models, they are not well </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Variational Autoencoder</head><p>The VAE approximates the intractable posterior over a set of latent variables p(z|x) with an approximate distribution q(z|x). D KL (q(z|x) p(z|x)) is minimized by optimizing the Evidence Lower Bound (ELBO) L(x) ≤ log p(x):</p><formula xml:id="formula_0">L(x) = -D KL (q(z|x) p(z)) Regularization cost + E q(z|x) [log p(x|z)] Reconstruction cost (1)</formula><p>L(x) becomes equal to log p(x) when q(z|x) = p(z|x). The VAE framework parametrises both p(x|z) and q(z|x) with neural networks. Optimization is performed through gradient descent, using the reparameterisation trick, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>The encoder, q φ (z|x), is an approximate inference model which parametrises a distribution over the latent variables z. A Gaussian with diagonal covariance is usually selected: N (z; µ(x; φ), σ(x; φ)•I). The decoder, p θ (x|z), is a generative model that reconstructs the input x from the latent variables z. The prior over the latent variables is usually chosen to be p(z) = N (z; 0, I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Regularization-based approaches to disentangled VAEs</head><p>The authors of <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref> propose β-VAE. This model reweights the ELBO by multiplying the regularization cost by β &gt; 1. This pushes the approximate posterior to be closer to the factorial prior. In <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>, the ELBO's regularization cost is decomposed into the mutual information between inputs and latent variables and the total correlation of the latent space, <ref type="bibr" target="#b12">[13]</ref>. The authors propose different modifications to the regularization term of the VAE objective, all of which aim to minimize the total correlation without decreasing the mutual information. In <ref type="bibr" target="#b13">[14]</ref>, an approach that penalizes the L2 distance between the second order moments of the aggregate approximate posterior and factorial prior is proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The problem with disentangling multimodal factors</head><p>In fig. <ref type="figure" target="#fig_2">3</ref>, we show that the distribution of latent variables inferred by β-VAE <ref type="bibr" target="#b7">[8]</ref> on the MNIST test set is multimodal. Roughly, one mode belongs to each digit. When traversing any given dimension of the latent space, we are likely to and N-VAE. For N-VAE, the plot shows the shared latent variables zs. Latent spaces are chosen to be two-dimensional for ease of viewing. Below each distribution, we show latent traversals for both model's azimuth latent variable.</p><p>pass through low probability regions, generating poor quality images, and eventually switch digits.</p><p>In models with unimodal priors, such as the ones proposed in <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, multimodally distributed explanatory factors tend to occupy multiple dimensions of the latent space, becoming entangled with other attributes. We posit that expressing mode-dependent generative factors, such as the ones shown in fig. <ref type="figure" target="#fig_0">1</ref>, can result in a multimodal approximate posterior and thus, a high cost under the ELBO's KL term. These factors are often only learned if their impact on the reconstruction accuracy is significant enough to mitigate the large KL cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>Let {x (i) , y (i) } N i=1 be a dataset consisting of N i.i.d observations of some variable x (i) ∈ R D and its class label be y (i) ∈ {1, ..., L}. We propose a generative model which describes x (i) as being generated by a set of latent variables z (i) and a class label y (i) . The class probabilities are given by π (i) . Unlike <ref type="bibr" target="#b14">[15]</ref>, we treat π (i) as a continuous latent variable over which we perform inference. We will omit the superscript (i) in order to maintain notation uncluttered in the rest of this paper. The joint distribution of our variables can be factorized as: p(x, y, z, π) = p(x|y, z)p(y|π)p(π)p(z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A Lower Bound on the Joint Log-Likelihood</head><p>We formulate a variational lower bound on log p(x, y): log p(x, y) ≥ L(x, y) = E q(z,π|x,y) [-log q(z, π|x, y)</p><formula xml:id="formula_1">+ log p(x, y, z, π)]<label>(2)</label></formula><p>This expression can be expanded and rewritten as eq. ( <ref type="formula" target="#formula_2">3</ref>). See appendix A. for a complete derivation.</p><p>L(x, y) = E q(z|x) [log p(x|y, z)] -D KL (q(z|x) p(z))</p><p>-D KL (q(π|x) p(π|y)) + log p(y)</p><p>E q(z|x) [log p(x|y, z)] is a reconstruction objective for our data vector x given z and y. D KL (q(π|x) p(π|y)) penalizes the approximate distribution of π given x for differing from the true posterior of π given the class label y. This item can be interpreted as a classification objective. Its value will go to 0 as q(π|x) approaches a one-hot vector with π y = 1. It is the most important difference between our lower bound and that of a standard VAE. log p(y) is constant with respect to our model's parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proposed Model: N-VAE</head><p>We place a zero mean isotropic unit variance Gaussian prior over z: p(z) = N (z; 0, I). We model the conditional over class labels with a categorical distribution: p(y|π) = Cat(y; π). Its parameters, π, follow a symmetric Dirichlet distribution, as we assume all classes to be equally probable:</p><formula xml:id="formula_3">p(π) = Dir(π; α p • 1 L ).</formula><p>As the Dirichlet distribution is conjugate to the categorical, the posterior will also be a Dirichlet distribution: p(π|y) = Dir(π; α p • 1 L + c y ). Note that c y is a one-hot representation of the class label y.</p><p>We use an isotropic Gaussian distribution for q(z|x) and a Dirichlet for q(π|x). The neural networks that parametrise both of these approximate posteriors share parameters φ. q φ (π|x) = Dir(π; α q •α(x; φ)) (4) q φ (z|x) = N (z; µ(x; φ), σ(x; φ)•I)</p><p>We define z as the concatenation of two vectors:</p><formula xml:id="formula_5">z = [z c , z s ] .</formula><p>The class-dependent variables, z c , are the concatenation of a set of latent vectors, each of which is associated with a different class:</p><formula xml:id="formula_6">z c = [z c1 , z c2 , ..., z cL ]</formula><p>. We want z c to explain intraclass factors of variation. We refer to z s as shared latent variables. They will express generative factors that are common to multiple classes. This allows for an efficient use of training samples, as these factors do not need to be learned separately for each input mode. For a given class y, we compute the variable 2 The input, x, is regenerated from [z cy , z s ] . The one-hot class label masks out the class-dependent latent variables for all non-relevant classes. It also introduces a class-dependent bias in the decoder's first activation layer when c y is multiplied with its corresponding weight vector. The effects of this term are discussed in appendix B. The decoder is parametrized by θ, taking the form p θ (x|y, z) = f (x; y, z, θ). The complete graphical model is displayed in fig. <ref type="figure" target="#fig_4">4</ref>. N-VAE is closely related to <ref type="bibr" target="#b15">[16]</ref>, where domain dependent, class dependent and shared information are captured in separate latent spaces. 2 We refer to the element-wise product with broadcasting by . vec(•) denotes the vectorization of a matrix into a column vector.  </p><formula xml:id="formula_7">z cy = vec(c y [1 L , [z c1 , z c2 , ..., z cL ] ]).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Resulting objective function and optimization</head><p>N-VAE's encoder is both a feature extractor and a classifier. For two Dirichlet distributions, D KL (q φ (π|x) p(π|y)) can be upper bounded by the inferred class label's negative log likelihood: -log(q φ (y|x))+k, with k being a constant. Using this approximation and dropping all constants with respect to model parameters, we optimize eq. ( <ref type="formula" target="#formula_2">3</ref>) as:</p><formula xml:id="formula_8">L obj (x, y) = E q φ (z|x) [log p θ (x|y, z)] -D KL (q φ (z|x) p(z)) + log(q φ (y|x))<label>(6)</label></formula><p>The KL divergence between the approximate posterior over z and its prior distribution is calculated for all dimensions of z = [z c , z s ] . However, not all dimensions of z c are used to reconstruct every input. Most of them are masked out.</p><p>The encoder learns to match the prior distribution for all dimensions of z c which are not assigned to the correct class, making them non-informative. In other words, the encoder learns to model class-specific factors of variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We consider three datasets which are separated into discrete classes: MNIST, Fashion-MNIST, and the cropped Extended Yale Face Dataset B. We preprocess the latter by only keeping images with illumination angles smaller than 100°and scaling images to 64×64, <ref type="bibr" target="#b16">[17]</ref>.</p><p>The training objective is reweighed in order to discourage z c from learning factors of variation that are common to multiple classes:</p><formula xml:id="formula_9">L βc = E q φ (z|x) [log p θ (x|y, z)] -D KL (q φ (z s |x) p(z)) -β c D KL (q φ (z c |x) p(z)) + log(q φ (y|x))<label>(7)</label></formula><p>We set β c = 2. However, we find that N-VAE is relatively insensitive to the value of this parameter as long as β c &gt; 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detecting class-dependent factors of variation</head><p>In MNIST, there exist both class-exclusive and shared generative factors. However, in Fashion-MNIST, most of the continuous factors of variation seem to be intra-class. Classes such as sandals and t-shirts only share the factor of colour intensity. In Yale Ext B, the only continuous factors of variation are the elevation and azimuth of image illumination.</p><p>N-VAE's KL divergence values for z s and z c act as detectors for class-dependent and shared information, fig. <ref type="figure" target="#fig_5">5</ref>. The class-dependent variables carry the most information   for Fashion-MNIST. For Yale Ext B, they are practically uninformative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Disentangling generative factors</head><p>Latent traversals for Fashion-MNIST's class-dependent variables are shown in fig. <ref type="figure" target="#fig_6">6</ref>. In fig. <ref type="figure" target="#fig_7">7</ref>, we show shared latent space traversals for the same dataset. The only generative factors which are shared among all classes seem to be color intensity and object width. In fig. <ref type="figure" target="#fig_8">8</ref>, we show shared latent space traversals for Yale Ext B. We obtain all latent traversals by sampling values of z s uniformly from the range (-3, 3).</p><p>We do not provide class-dependent latent space traversals for Yale Ext B, as z c learns to be uninformative for this dataset. This is reflected in the values of D KL (q φ (z c |x) p(z)) from fig. <ref type="figure" target="#fig_5">5</ref>. Changing z c does not affect reconstructions.</p><p>By explicitly modelling multimodally distributed generative factors through class-dependent latent variables, the shared latent space is allowed to better match the factorial prior. Consequently, N-VAE is able to learn disentangled representa- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Augmenting Datasets with Generated Samples</head><p>We train classifiers with different proportions of original and artificial (generated) samples and evaluate them using the original test sets. With this method, we not only test the discriminability of our generations but also how much of the original datasets' variability they capture. A more diverse set of generated images will better constrain the classifiers' parameters, resulting in higher accuracy.</p><p>We compare N-VAE with the conditional VAE (CVAE), <ref type="bibr" target="#b17">[18]</ref>, the conditional GAN (CGAN), <ref type="bibr" target="#b18">[19]</ref> and the auxiliary classifier GAN (AC-GAN), <ref type="bibr" target="#b19">[20]</ref>. All of these models use class labels during training and allow for class-conditional sample generation. Latent variables are sampled from N (z; 0, σ•I). A larger σ allows sampling from regions of latent space which are less likely under the trainset. A large enough value should allow for the extrapolation of learned generative factors, producing more extreme characteristics or new combinations of features. Our intention is to show that generative models generalize better when an appropriate latent space structure is used, not to propose a new data augmentation scheme. On MNIST, including artificial samples has a regularization effect for most generative models as is shown in fig. <ref type="figure" target="#fig_9">9 a</ref>); Seeing more plausible data makes it more difficult for the classifiers to overfit to specific training samples. However, N-VAE is the only model for which there is a consistent decrease in classification error as σ increases. It is able to produce feature configurations that complement those of the original training set. Fashion-MNIST is a more complex dataset which is harder to explain for generative models. In plot b), we see that using samples generated with N-VAE decreases the baseline classifier's performance the least on this dataset.</p><p>We observe a significant performance decrease when training classifiers exclusively on artificial data in plots c) and d) from fig. 9. On MNIST, the conditional GAN produces the most useful samples for σ = 1, N-VAE is a close second. On Fashion-MNIST, N-VAE provides the best results. In all scenarios, N-VAE performs best as σ increases.</p><p>V. CONCLUSION N-VAE explains data in terms of shared latent variables and class-dependent latent variables. We have shown the utility of this type of model for identifying and disentangling classdependent generative factors in data. In our data augmentation experiment, N-VAE is the only model that provides more informative samples for classification as σ increases. This suggests that N-VAE is able to produce samples with new combinations of features, showing its capacity for generalization.</p><p>A: DERIVATION OF EQ. (2) AND EQ. (3) First, we show that our objective is a lower bound on the joint log-likelihood log p(x, y). Our starting point is the KL divergence between the true and approximate posteriors over z and π, as this is the quantity which we wish to minimise. D KL (q(z, π|x, y) p(z, π|x, y)) = E q(z,π|x,y) [log q(z, π|x, y) -log p(z, π|x, y)] = E q(z,π|x,y) [log q(z, π|x, y) -log p(x, y, z, π)] + log p(x, y) = -L(x, y) + log p(x, y)</p><p>Using the non-negativity of the KL divergence, we can write: L(x, y) ≤ log p(x, y).</p><p>We make the following modelling choices:</p><p>• x is conditionally independent of π given y; p(x|z, y, π) = p(x|z, y). • Our inference model q only takes x as an input. Therefore, q(π|x, y) = q(z|x) and q(π|x, y) = q(π|x). We can rewrite eq. ( <ref type="formula" target="#formula_1">2</ref>) in the following manner: log p(x, y) ≥ L(x, y) = E q(z,π|x,y) [-log q(z, π|x, y) + log p(x, y, z, π)] = E q(z,π|x,y) [-log q(z, π|x, y) + log(p(x|y, π, z)p(z)p(π|y)p(y))] = E q(z,π|x,y) [log p(x|y, π, z)] -E q(z,π|x,y) [log q(z|x, y) -log p(z)] -E q(z,π|x,y) [log q(π|x, y) -log p(π|y)] + E q(z,π|x,y) [log p(y)] = E q(z|x) [log p(x|y, z)] -D KL (q(z|x) p(z)) -D KL (q(π|x) p(π|y)) + log p(y)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B: THE BIAS TERM IN THE DECODER'S INPUT</head><p>z cy is obtained as vec(c y [1 L , [z c1 , z c2 , ..., z cL ] ]). In this expression, we can see that z cy is composed of the latent vectors predicted by the encoder, of which all but one is masked out, and a one-hot vector which results from c y 1 L = c y . The latter explicitly informs the decoder about the class identity of the sample to be reconstructed. When multiplied with the corresponding weight layer, the one-hot term adds a class-dependent bias term b y to the decoder's first activation layer.</p><p>The most likely value under the prior p(z c ) = N (z c ; 0, I) is 0. However, when we mask out latent variables, we set most of them to this value. This makes masked latent variables indistinguishable from samples from p(z c ). Without b y , the encoder is forced to predict distributions which place a lot of mass on large values of z c in order to communicate the class of x to the decoder. In fig. <ref type="figure" target="#fig_0">10</ref>, we show how the KL divergence grows for class-dependent latent variables when we remove c y 1 L and thus, the bias, from z cy . The explicit encoding of discrete information allows us to describe the data's class-dependent generative factors with distributions that more closely resemble the prior. It allows for smooth interpolation of representations in the class-conditioned latent space around zero. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. MNIST class-dependent latent traversals produced by N-VAE. Generative factors which are exclusive to each mode of the input data distribution are disentangled from factors which are shared across classes.</figDesc><graphic coords="1,305.70,221.51,263.61,88.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Traversals of N-VAE's shared latent space in the range (-3, 3) when trained on the MNIST dataset.</figDesc><graphic coords="2,82.38,50.54,447.24,108.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Latent variables inferred from MNIST's test set with β-VAE (β = 5)and N-VAE. For N-VAE, the plot shows the shared latent variables zs. Latent spaces are chosen to be two-dimensional for ease of viewing. Below each distribution, we show latent traversals for both model's azimuth latent variable.</figDesc><graphic coords="2,311.98,203.10,251.06,111.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>model. b) Train-time decoder model. c) Test-time decoder model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Graphical model of N-VAE. The model is trained using the true class labels y. At test time, y is inferred from x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. KL cost for each of N-VAE's latent dimensions for each of the datasets under consideration. zcy refers to the KL cost of the class-dependent latent variable corresponding to the correct class.</figDesc><graphic coords="4,74.67,50.54,462.67,106.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Fashion-MNIST class-dependent latent traversals produced by N-VAE by sampling zc uniformly from (-3, 3).</figDesc><graphic coords="4,42.69,203.78,263.61,96.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Shared latent traversals produced by N-VAE when trained on Fashion-MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Shared latent traversals produced by N-VAE when trained on Yale Ext B. We show these for five classes out of 28.</figDesc><graphic coords="4,48.96,343.03,251.06,97.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. Mean and standard deviation of the classification error obtained when training with data from each generative model. All experiments have been repeated three times. We show results for probabilities of substituting original datapoints with artificial ones of 0.4 (a value we found to perform well) and 1. We vary σ from 1 to 2. 'Baseline' refers to the classifier trained without artificial data.</figDesc><graphic coords="5,48.96,50.54,514.07,101.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 . 4 Fig. 11 .</head><label>10411</label><figDesc>Fig. 10. Mean value of each dimension of D KL (q φ (z ci |x) p(z)) for each class of the MNIST test set. The encoder learns to classify samples implicitly by only making the correct dimensions of zc informative. The addition of a class-dependent bias term to the decoder allows for the distribution of the class-dependent latent variables to be closer to the Gaussian prior.</figDesc><graphic coords="6,337.08,121.52,200.84,335.48" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We understand expressivity as the number of possible input configurations that are explained by a model,<ref type="bibr" target="#b0">[1]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>This work has been supported by the <rs type="funder">Spanish Ministry of Economy and Competitiveness</rs> and the <rs type="funder">European Social Fund</rs> through the project <rs type="grantNumber">TIN2017-85854-C4-1-R</rs>, by the <rs type="funder">Government of Aragon</rs> (Reference Group T36 17R) and co-financed with <rs type="programName">Feder 2014-2020 "Building Europe from Aragon</rs>."</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rRfAqSz">
					<idno type="grant-number">TIN2017-85854-C4-1-R</idno>
				</org>
				<org type="funding" xml:id="_gGbkdxN">
					<orgName type="program" subtype="full">Feder 2014-2020 &quot;Building Europe from Aragon</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scan: Learning hierarchical compositional visual concepts</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sonnerat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosnjak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structured disentangled representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Esmaeili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bozkurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Van De Meent</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Research</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sugiyama</surname></persName>
		</editor>
		<meeting>Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019-04-18">18 Apr 2019</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="2525" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Darla: Improving zeroshot transfer in reinforcement learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vae with a VampPrior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>PMLR, 09-11</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Perez-Cruz</surname></persName>
		</editor>
		<meeting>the Twenty-First International Conference on Artificial Intelligence and Statistics<address><addrLine>Lanzarote, Canary Islands</addrLine></address></meeting>
		<imprint>
			<publisher>Playa Blanca</publisher>
			<date type="published" when="2018-04">Apr 2018</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="1214" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Jebara</surname></persName>
		</editor>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun 2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Understanding disentangling in β-vae</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Disentangling by factorising</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07-15">10-15 Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2649" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Auto-encoding total correlation explanation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brekelmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1157" to="1166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31, S. Bengio</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2610" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A note on the formation of concept and of association by information-theoretical correlation analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="291" to="296" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Variational inference of disentangled latent concepts from unlabelled observations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran</publisher>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Diva: Domain invariant variational autoencoders</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Acquiring linear subspaces for face recognition under variable lighting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="684" to="698" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno>abs/1411.1784</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno>PMLR, 06-11</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017-08">Aug 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
