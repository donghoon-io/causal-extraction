<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">INFOVAEGAN : LEARNING JOINT INTERPRETABLE REPRESENTATIONS BY INFORMATION MAXIMIZATION AND MAXIMUM LIKELIHOOD</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fei</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<postCode>YO10 5GH</postCode>
									<settlement>York</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adrian</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<postCode>YO10 5GH</postCode>
									<settlement>York</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">INFOVAEGAN : LEARNING JOINT INTERPRETABLE REPRESENTATIONS BY INFORMATION MAXIMIZATION AND MAXIMUM LIKELIHOOD</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hybrid VAE-GAN generative models</term>
					<term>Disentangled representations</term>
					<term>Mutual information</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning disentangled and interpretable representations is an important step towards accomplishing comprehensive data representations on the manifold. In this paper, we propose a novel representation learning algorithm which combines the inference abilities of Variational Autoencoders (VAE) with the generalization capability of Generative Adversarial Networks (GAN). The proposed model, called InfoVAE-GAN, consists of three networks : Encoder, Generator and Discriminator. InfoVAEGAN aims to jointly learn discrete and continuous interpretable representations in an unsupervised manner by using two different data-free log-likelihood functions onto the variables sampled from the generator's distribution. We propose a two-stage algorithm for optimizing the inference network separately from the generator training. Moreover, we enforce the learning of interpretable representations through the maximization of the mutual information between the existing latent variables and those created through generative and inference processes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Unsupervised disentangled representation learning is a challenging task in any machine learning application. Most studies consider disentangled representation to be a data decomposition into sets of statistically and syntactically independent variables. Such data sets are assumed to be semantically distinct and to represent different categories of data characteristics. Learning disentangled representations that may capture semantic meaningful information can allow to explicitly edit images and is useful for a variety of tasks <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b3">3]</ref>. Enabling disentangled representations can overcome overfitting during the training, leading to better generalization in models, <ref type="bibr" target="#b4">[4]</ref>.</p><p>One of the most popular generative models is the Variational Autencoder <ref type="bibr" target="#b5">[5]</ref>, which implements a mapping between the data and an estimated latent space. The VAE's loss function maximizes the lower bound on the marginal log-likelihood of the data, while accurately reconstructing the data from the mapping of the latent space using the Kullback-Leibler (KL) divergence. Learning interpretable and disentangled representations have been considered in β-VAE <ref type="bibr" target="#b6">[6]</ref> by setting a large penalty on the KL divergence term in order to encourage the independence between latent variables. On the other hand β-VAE sacrifices the quality of data reconstruction when inducing disentangled representations, <ref type="bibr" target="#b7">[7]</ref>. β-TCVAE model introduced the usage of the total correlation (TC) penalty, which is a measure of multivariate mutual independence. The TC penalty was used in various VAE frameworks <ref type="bibr">[8]</ref> for inducing disentangled representations. However, TC is biased and is zero only if estimated on the whole dataset, <ref type="bibr" target="#b9">[9]</ref>. Meanwhile, reducing the bias to zero is impossible for a large-scale dataset. The drawback of VAE based approaches is that they generally produce blurred and unclear images when compared to Generative Adversarial Networks (GANs) <ref type="bibr" target="#b10">[10]</ref>. Few research efforts have been devoted to use GANs for disentangled representations <ref type="bibr" target="#b11">[11]</ref>, and with mixed results.</p><p>This research study has the following contributions : 1) A novel two-stage training algorithm where the inference model is estimated separately from the generator. 2) A data-free log-likelihood optimization approach able to learn an accurate inference model from a GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND AND RELATED WORKS</head><p>Variational autoencoder (VAE). VAEs <ref type="bibr" target="#b5">[5]</ref> aim to maximize a lower bound to the marginal log-likelihood of the data :</p><formula xml:id="formula_0">L(φ, θ) =E q θ (z|x) [log p φ (x|z)] -D KL (q θ (z|x)||p(z)) ≤ log p(x)<label>(1)</label></formula><p>where x and z are the input data and the corresponding latent variables, and the conditional distributions q θ (z|x) and p φ (x|z), are implemented by the Encoder and Decoder networks, of parameters θ and φ, respectively. These networks are trained using the Stochastic Gradient Descent (SGD) algorithm.</p><p>Generative adversarial networks (GAN). GANs also consist of two network components : Generator and Discriminator which are trained for playing a Minimax game, defined by the following loss:</p><formula xml:id="formula_1">min G max D V (D, G) = E x∼ pd(x) [log D(x)] + E z∼ p(z) [log[1 -D(G(z))]].<label>(2)</label></formula><p>While the discriminator network is trained to distinguish between real and fake data, the generator aims to produce more realistic data that can fool the discriminator. GANs are challenging to control and may generate unexpected results. Hybrid models. Hybrid models attempt to address the drawbacks of GANs and VAEs, by combining their architectures. These models usually have three components: an Encoder for mapping data into the latent space, a Generator to recover data from the latent space, and a Discriminator to distinguish real from fake data. Adversarial learning can be performed in the data space, latent space <ref type="bibr" target="#b2">[2]</ref>, or on their joint spaces.</p><p>Lately, the likelihood estimation as a regularization term was shown to stabilize adversarial distribution matching <ref type="bibr" target="#b3">[3]</ref>. However, these methods only focus on improving the generation capability and do not design suitable objective functions for inducing disentangled representations. Our paper is the first to propose an appropriate objective function for training a hybrid VAE-GAN method for learning both continuous and discrete disentangled representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE INFOVAEGAN MODEL</head><p>The proposed InfoVAEGAN model is made up of three networks: Encoder, Generator and Discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generation from prior distributions</head><p>Let x ∈ R d represent the observed random variables sampled from the empirical data distribution P x . One of the goals of our model is to train the Generator to approximate the true data distribution P x . Let us assume three underlying generative factors z, c, d, corresponding to random, continuous and discrete variables, which are sampled from three independent prior distributions z</p><formula xml:id="formula_2">∼ N (I z , Σ z ), c ∼ N (I c , Σ c ), d ∼ Cat(k = K, p = 1/K),</formula><p>where Cat denotes the Categorical distribution and N is he Gaussian distribution. Let us consider that the data generated x ′ is produced by a generator G ψ (z, d, c), implemented by a neural network with trainable parameters ψ, and P G to represent the distribution of data generated by G. The generation process is defined as: <ref type="figure">,</ref><ref type="figure">d,</ref><ref type="figure">c</ref>).</p><formula xml:id="formula_3">d ∼ p(d), z ∼ p(z), c ∼ p(c), x ∼ p ψ (x|z</formula><p>For the Discriminator network we use the Earth-mover distance, as in the Wasserstein GAN (WGAN) model <ref type="bibr" target="#b10">[10]</ref>, which is defined as the optimal path of transporting information mass from the generator distribution P G to the data distribution P x . By considering the Kantorovich-Rubinstein duality <ref type="bibr" target="#b12">[12]</ref>, the optimal transport adversarial learning is defined as: min</p><formula xml:id="formula_4">G max D∈Θ E x∼Px [D(x)] -E x ′ ∼P G D(x ′ )]<label>(3)</label></formula><p>where Θ represents a set of 1-Lipschitz functions. We introduce a gradient penalty term <ref type="bibr" target="#b13">[13]</ref>, to enforce the Lipschitz constraint, resulting in:</p><formula xml:id="formula_5">min G max D E x∼Px [D(x)] -E x ′ ∼P G [D(x ′ )] + λE x∼Px [( ∇ xD(x) 2 -1) 2 ],<label>(4)</label></formula><p>where P x is defined as sampling uniformly along straight lines between pairs of data sampled from P x and P G .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data-free log-likelihood optimization</head><p>In this section, we introduce two data-free log-likelihood optimization functions, which are used to learn the disentangled latent representations z and u = (d, c), respectively. Instead of maximizing the sample log-likelihood, as commonly used in the VAE framework <ref type="bibr" target="#b5">[5]</ref>, we optimize the log-likelihood function by deriving a lower bound over the data samples drawn from the generator distribution. </p><formula xml:id="formula_6">Definition 1 Let x ′ ∼ G(z,</formula><formula xml:id="formula_7">(x ′ , z, d, c) = p ψ (x ′ |z, d, c)p(d, c)p(z).</formula><p>Then, the log-likelihood of p ψ (x ′ ) is defined as:</p><formula xml:id="formula_8">log p ψ (x ′ ) = log p ψ (x ′ |d, c, z) p(d, c) p(z) dd dc dz.</formula><p>(5) This expression is intractable and can be rewritten by considering its Evidence Lower Bound (ELBO), as :</p><formula xml:id="formula_9">log p ψ (x ′ ) ≥ E q ω,ξ (d,c,z|x ′ ) log p ψ (x ′ , d, c, z) q ξ (z|x ′ )q ω (d, c|x ′ )</formula><p>.</p><p>(6) The scheme for optimizing both q ω (d, c|x ′ ) and q ξ (z|x ′ ), without updating the Generator, is very efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">THE THEORETICAL FRAMEWORK</head><p>In existing hybrid methods, the inference model and the generator network are trained jointly by using a single objective function. However, in the proposed InfoVAEGAN model we have independent optimization procedures for the inference and generation. This choice has many advantages. For instance, the training of the inference model implemented by the Encoder, does not interfere with the optimization of the Generator, which results in a stable training procedure. When the Generator approximates the true data distribution exactly, we can derive more accurate inference models. Aligning two joint distributions by using adversarial learning would also be harder to achieve than matching two single distributions individually. Unlike in InfoGAN <ref type="bibr" target="#b11">[11]</ref>, the proposed model has a full inference mechanism, which enables the inference of both meaningful and nuisance latent representations, benefiting many down-stream tasks such as data reconstructions and interpolations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 1 For a given inference model, we can estimate the testing data log-likelihood :</head><formula xml:id="formula_10">log p ψ (x t ) ≥ E q ω,ξ (d,c,z|xt) [log p ψ (d, c, z|x t )] -D KL (q ω (d|x t )||p(d)) -D KL (q ω (c|x t )||p(c)) -D KL (q ξ (z|x t )||p(z)) = L(ψ, ξ, ω; x t ) (7)</formula><p>where x t represent testing data. The model implementing p ψ (x t ) combines the two inference models and a Generator.</p><p>Proof 1 We combine the two inference models for continuous and discrete variables, and a Generator into a single model:</p><formula xml:id="formula_11">log p ψ (x t ) = p ψ (x t |d, c, z)q ω (d, c|x t )q ξ (z|x t ) (8)</formula><p>Then we define the model log-likelihood as :</p><formula xml:id="formula_12">log p ψ (x t ) = log E q ω,ξ (d,c,z|xt) p ψ (x t , d, c, z) q ω,ξ (d, c, z|x t (9)</formula><p>According to the Jensen inequality, we have :  </p><formula xml:id="formula_13">log p ψ (x t ) ≥ E q ω,ξ (d,c,z|xt) log p ψ (x t , d, c, z) q ω,ξ (d, c, z|x t ) = E q ω,ξ (d,c,z|xt) log p ψ (d, c, z|x t )p(d)p(c)p(z) q ω (d|x t )q ω (c|x t )q ξ (z|x t ) = E q ω,ξ (d,c,z|xt) [log p ψ (d, c, z|x t )] -D KL (q ω (d|x t )||p(d)) -D KL (q ω (c|x t )||p(c)) -D KL (q ξ (z|x t )||p(z)).<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">MUTUAL INFORMATION MAXIMIZATION FOR INTERPRETABLE REPRESENTATIONS</head><p>In the proposed InfoVAEGAN model, we transfer the underlying characteristic information of continuous and discrete latent variables during the decoder-generation process by using the Mutual Information (MI) maximization. Let us denote the joint latent variables by u = (d, c), while we want to maximize the MI between the joint latent variable u and the decoder output, I(u, G(z, u)). According to the research study from <ref type="bibr" target="#b14">[14]</ref> it is difficult to optimize the mutual information directly, given that it needs to access the information represented by the true posterior p(u|x). In order to address this problem, we define an auxiliary distribution W (u|x) to approximate the true posterior and then derive a lower bound on the mutual information, expressed by using the marginal entropy H(u), and the conditional entropy H(u|G(z, u)) :</p><formula xml:id="formula_14">I(u, G(z, u)) = H(u) -H(u|G(z, u)) = G(z, u)p(u|x) log p(u|x) W (u|x) dxdu + G(z, u)p(u|x) log W (u|x)dxdu + H(u) = E x∼G(z,u) D KL [p(u|x)||W (u|x)] + E x∼G(z,u) [E u∼p(u,x) [log W (u|x)]] + H(u) E x∼G(z,u) [E u∼p(u,x) [log W (u|x)]] + H(u) = L M I (11)</formula><p>where the auxiliary distribution W (u|x) is implemented by the Encoder. In practice, we sample a pair of latent variables d, c from q ω (d, c|x). We estimate the mutual information by means of the lower bound L M I , from <ref type="bibr" target="#b11">(11)</ref>, while the last term H(u) represents the marginal entropy of the latent variables.</p><p>The graph structure of the InfoVAEGAN is shown in Fig. <ref type="figure" target="#fig_1">1</ref>, where q ω (d|x) and q ω (c|x) are implemented by the same network except for the last layer which is different for the inference of each latent variable. The inference network, representing q ξ (z|x), is implemented by a neural network with trainable parameters ξ, as it can be seen in the lower part of the left side of Fig. <ref type="figure" target="#fig_1">1b</ref>. The Generator is shown in Fig. <ref type="figure" target="#fig_1">1a</ref>. Fig. <ref type="figure">2</ref>. Reconstruction results on each row: real images, reconstructions by ALI <ref type="bibr" target="#b15">[15]</ref>, InfoGAN <ref type="bibr" target="#b11">[11]</ref> and InfoVAEGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTAL RESULTS</head><p>In the following we evaluate the performance of InfoVAE-GAN on the MNIST dataset <ref type="bibr" target="#b16">[16]</ref>, representing images of handwritten digits. In order to learn the discrete latent variable which captures different styles of handwritten digits we use a categorical vector sampled from Cat(K = 10, p = 0.1) and two continuous variables, sampled from the uniform distribution U (-1, 1), as latent variables. The reconstruction results for the images from MNIST, shown in the first row from Fig. <ref type="figure">2</ref>, by ALI <ref type="bibr" target="#b15">[15]</ref>, InfoGAN <ref type="bibr" target="#b11">[11]</ref>, and InfoVAEGAN, are provided in the following rows of images, respectively. For the proposed InfoVAEGAN approach, the discrete latent variables are sampled from the Gumble-softmax distribution, while the continuous latent variables are sampled from the Gaussian distribution, whose mean and diagonal covariance are parameterized by the Encoder. From these results it can be observed that InfoVAEGAN provides better digit image reconstructions than InfoGAN or ALI.   We modify the continuous codes c 1 , c 2 within the range [-1, 1] and fix the other latent variables. The generative results for MNIST dataset are shown in Figures <ref type="figure" target="#fig_4">4a</ref> and<ref type="figure" target="#fig_4">4c</ref> for InfoVAEGAN, while for InfoGAN are provided in Figures <ref type="figure" target="#fig_4">4b</ref> and<ref type="figure" target="#fig_4">4d</ref>, when modifying c 1 and c 2 . It can be observed that by varying the latent codes in InfoVAEGAN, we generate images showing meaningful characteristics such as rotations or a variety of handwriting styles. We also consider a 10-dimensional vector for the discrete and continuous latent variables in order to model underlying changing factors in the CelebA dataset <ref type="bibr" target="#b17">[17]</ref>. We change a single latent variable in the images generated by InfoVAEGAN while fixing the others. The results shown in Figures <ref type="figure" target="#fig_2">3a-d</ref> indicate variations in face image representations such as bangs, glasses, hair colour and in smiling.</p><p>The results when using InfoVAEGAN in unsupervised classification on the MNIST dataset, when compared with other methods, are provided in Table <ref type="table" target="#tab_1">1</ref>. Most unsupervised learning methods adopt mixture deep learning models (K represents the number of components) requiring significantly more parameters. We can observe that InfoVAEGAN achieves higher accuracy than InfoGAN <ref type="bibr" target="#b11">[11]</ref>, and other models.</p><p>We investigate the disentanglement ability of the proposed approach by using the metric from <ref type="bibr" target="#b7">[7]</ref> and the dataset dSprites <ref type="bibr" target="#b25">[25]</ref>. The results are reported in Table <ref type="table" target="#tab_2">2</ref>, where all other results are cited from <ref type="bibr" target="#b18">[18]</ref>. The proposed approach achieves a competitive disentanglement score when compared with the  <ref type="bibr" target="#b18">[18]</ref> 10 0.69 current state of the art. We also use the Fréchet Inception Distance (FID) <ref type="bibr" target="#b26">[26]</ref> to evaluate the quality of the generated images when considering the CelebA dataset in Fig. <ref type="figure" target="#fig_2">3e</ref>, where InfoVAEGAN-MI denotes that the proposed approach does not use the mutual information (MI) loss. These results show that the proposed approach can balance well the disentanglement ability and image generation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>In this paper, we introduce InfoVAEGAN, a new deep learning approach for learning jointly discrete and continuous interpretable representations. InfoVAEGAN optimizes separately the inference model and the generator providing advantages over other hybrid methods. The proposed approach is a good tool to provide inference mechanisms when considering any generative GAN model without the need of any real data. In addition, InfoVAEGAN can generate high-quality interpretable data variations which can successfully be used for disentangled and interpretable representation learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Generator. (b) Inference models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Unsupervised learning structures in generative models, where c and d are continuous and discrete variables, while z represents Gaussian noise.</figDesc><graphic coords="4,135.07,406.95,158.11,93.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>We change a single latent variable in the latent space from -1 to 1 while fixing all other latent variables for CelebA dataset in (a)-(d). FID evaluation when using CelebA database for training is provided in (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) InfoVAEGAN changing c 1 . (b) InfoGAN changing c 1 . (c) InfoVAEGAN changing c 2 . (d) InfoGAN changing c 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Generation results when changing the continuous variables c 1 and c 2 from -1 to 1.</figDesc><graphic coords="5,57.78,287.86,114.80,57.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Unsupervised classification results for M runs.</figDesc><table><row><cell></cell><cell cols="2">MNIST</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">K M Mean Best</cell></row><row><cell>InfoVAEGAN</cell><cell>1</cell><cell cols="3">4 95.42 96.15</cell></row><row><cell>JointVAE [18]</cell><cell>1</cell><cell cols="3">4 71.53 87.32</cell></row><row><cell>SubGAN [19]</cell><cell cols="2">20 1</cell><cell>/</cell><cell>90.81</cell></row><row><cell>InfoGAN [11]</cell><cell>1</cell><cell>1</cell><cell>/</cell><cell>93.35</cell></row><row><cell>GMVAE [20]</cell><cell cols="2">30 1</cell><cell>/</cell><cell>89.27</cell></row><row><cell>GMVAE [20]</cell><cell cols="2">16 1</cell><cell>/</cell><cell>87.82</cell></row><row><cell>AAE [21]</cell><cell cols="2">16 1</cell><cell>/</cell><cell>90.45</cell></row><row><cell>CatGAN [22]</cell><cell cols="2">30 1</cell><cell>/</cell><cell>95.73</cell></row><row><cell>DEC [23]</cell><cell cols="2">10 1</cell><cell>/</cell><cell>84.30</cell></row><row><cell cols="3">PixelGAN [24] 30 1</cell><cell>/</cell><cell>94.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Disentanglement evaluation on the dSprites.</figDesc><table><row><cell>Methods</cell><cell>M Score</cell></row><row><cell cols="2">InfoVAEGAN 10 0.79</cell></row><row><cell>Beta-VAE [6]</cell><cell>10 0.73</cell></row><row><cell cols="2">FactorVAE [7] 10 0.82</cell></row><row><cell>JointVAE</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep mixture generative autoencoders</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning joint latent representations based on information maximization</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">567</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="216" to="236" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning latent representations across multiple data domains using lifelong vaegan</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12365</biblScope>
			<biblScope unit="page" from="777" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00410</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. of Learning Representation (ICLR)</title>
		<meeting>Int. Conf. of Learning Representation (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">β-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learning Representations</title>
		<meeting>Int. Conf. on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Disentangling by factorising</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<idno>PMLR 80</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
		<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2649" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Auto-encoding total correlation explanation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brekelmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName><surname>Galstyan</surname></persName>
		</author>
		<idno>PMLR 89</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Conf. on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>of Int. Conf. on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1157" to="1166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Information constraints on auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Regier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yosef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv in Neur Inf Proc Sys (NIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6117" to="6128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno>PMLR 70</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
		<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Inf. Proc. Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<title level="m">Optimal transport: Old and New</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Inf. Proc. Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lifelong learning of interpretable image representations</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Image Processing Theory, Tools and Applications (IPTA)</title>
		<meeting>Int. Conf. on Image essing Theory, Tools and Applications (IPTA)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarially learned inference</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learning Rep. (ICLR)</title>
		<meeting>Int. Conf. on Learning Rep. (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recog</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<meeting>of IEEE Int. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning disentangled joint continuous and discrete representations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Inf. Proc. Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="710" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sub-GAN: An unsupervised generative model via subspaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11215</biblScope>
			<biblScope unit="page" from="698" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep unsupervised clustering with Gaussian mixture variational autoencoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dilokthanakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Salimbeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02648</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adversarial autoencoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLRworkshop</title>
		<meeting>ICLRworkshop</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised and semi-supervised learning with categorical generative adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06390</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learning Representations (ICLR)</title>
		<meeting>Int. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06335</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
		<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PixelGAN autoencoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv in Neur Inf Proc Sys (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1972" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://github.com/deepmind/dsprites-dataset/" />
		<title level="m">dSprites: Disentanglement testing Sprites dataset</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Adv. in Neural Inf. Proc. Syst</title>
		<imprint>
			<biblScope unit="page" from="6626" to="6637" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
