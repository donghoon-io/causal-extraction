<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">REDUCED-ORDER MODELING OF UNSTEADY FLUID FLOW USING NEURAL NETWORK ENSEMBLES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-08-08">8 Aug 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Rakesh</forename><surname>Halder</surname></persName>
							<email>rhalder@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Aerospace Engineering</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Autodesk Research</orgName>
								<address>
									<postCode>M5G 1M1</postCode>
									<settlement>Toronto</settlement>
									<region>ON CAN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammadmehdi</forename><surname>Ataei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Autodesk Research</orgName>
								<address>
									<postCode>M5G 1M1</postCode>
									<settlement>Toronto</settlement>
									<region>ON CAN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hesam</forename><surname>Salehipour</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Autodesk Research</orgName>
								<address>
									<postCode>M5G 1M1</postCode>
									<settlement>Toronto</settlement>
									<region>ON CAN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Krzysztof</forename><surname>Fidkowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Aerospace Engineering</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Maki</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Naval Architecture and Marine Engineering</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">REDUCED-ORDER MODELING OF UNSTEADY FLUID FLOW USING NEURAL NETWORK ENSEMBLES</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-08-08">8 Aug 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2402.05372v2[physics.flu-dyn]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The use of deep learning has become increasingly popular in reduced-order models (ROMs) to obtain low-dimensional representations of full-order models. Convolutional autoencoders (CAEs) are often used to this end as they are adept at handling data that are spatially distributed, including solutions to partial differential equations. When applied to unsteady physics problems, ROMs also require a model for time-series prediction of the low-dimensional latent variables. Long short-term memory (LSTM) networks, a type of recurrent neural network useful for modeling sequential data, are frequently employed in data-driven ROMs for autoregressive time-series prediction. When making predictions at unseen design points over long time horizons, error propagation is a frequently encountered issue, where errors made early on can compound over time and lead to large inaccuracies. In this work, we propose using bagging, a commonly used ensemble learning technique, to develop a fully data-driven ROM framework referred to as the CAE-eLSTM ROM that uses CAEs for spatial reconstruction of the full-order model and LSTM ensembles for time-series prediction. When applied to two unsteady fluid dynamics problems, our results show that the presented framework effectively reduces error propagation and leads to more accurate time-series prediction of latent variables at unseen points.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Physics-based numerical simulation has become an indispensable tool in engineering and scientific applications, allowing for accurate representations of complex physical phenomena. Physical simulations are formulated as sets of governing equations, typically in the form of parameterized partial differential equations (PDEs) that are discretized over a computational domain. Simulations often involve a set of design parameters µ that govern aspects such as boundary conditions, the geometry of the computational domain, and physical properties. In tasks like design optimization, where numerous simulations are run for different designs, it is important to achieve high accuracy, or fidelity. However, high-fidelity simulation comes at a large computational cost, and this can lead to a bottleneck in the design optimization process.</p><p>Employing reduced-order models (ROMs) is a prevalent strategy to mitigate this high computational cost. <ref type="bibr" target="#b0">1</ref> By utilizing a limited set of training data from computed high-fidelity simulations, ROMs construct a low-dimensional surrogate model that can be evaluated in real-time and provide accurate full-order model (FOM) approximations at unseen designs within the parameter space covered by the training data. ROMs significantly reduce the degrees of freedom of the FOM through the use of a low-dimensional embedding that can be effectively mapped back to the full-order state, leading to a substantial decrease in computational cost. ROMs consist of two stages: an offline stage, which is computationally intensive, involving the computation of high-fidelity solutions by solving the FOM to generate data snapshots and train the low-dimensional surrogate model, and an online stage, where the model is used to approximate solutions at desired points. Running the FOM requires numerically solving the governing equations over the computational domain, which comes at a much larger computational cost than the ROM, which represents the FOM using a small number of variables.</p><p>Most ROMs utilize the proper orthogonal decomposition, <ref type="bibr" target="#b1">2</ref> a linear method that uses the singular value decomposition (SVD) to obtain a low-rank subspace consisting of a number of linearly independent basis vectors. A linear combination of these basis vectors is computed using a set of expansion coefficients to approximate full-order states within the solution space. The POD basis vectors are interpretable, allowing for visualization of dominant physical features. Although POD is widely used, it encounters difficulties when applied to highly nonlinear problems, often requiring a large number of basis vectors to provide reasonable accuracy. <ref type="bibr" target="#b2">3</ref> More recently, machine learning and artificial intelligence (AI) methods have been used to provide nonlinear mappings between the low-dimensional embedding and high-fidelity solution space. In particular, deep learning <ref type="bibr" target="#b3">4</ref> approaches have been used to develop ROMs that provide efficient nonlinear manifolds of physical systems. Convolutional autoencoders (CAEs), a type of artificial neural network, have been used to build ROMs and shown to outperform POD-based methods . <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6</ref> Artificial neural networks that use convolutional layers efficiently learn structures and patterns present in spatially distributed data, including the solutions to PDEs. Autoencoders consist of two individual neural networks: an encoder, which maps high-dimensional inputs to a low-dimensional latent space, and a decoder, which maps the low-dimensional latent space to a reconstruction of the high-dimensional input. Although autoencoders can effectively learn nonlinear relationships, they lack interpretability. Recent works have used variational autoencoders <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8</ref> to incorporate interpretable nonlinear modes into ROMs, although this results in lower reconstruction accuracy when compared to vanilla autoencoders. Another approach for simulating systems governed by PDEs at a reduced cost is the use of neural operators, <ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10</ref> which implicitly learn the governing equations using deep neural networks. However, this approach typically requires very large amounts of training data to cover parameter spaces effectively, limiting their use in design optimization.</p><p>ROMs are categorized as either non-intrusive or projection-based; non-intrusive ROMs use a fully data-driven approach, while projection-based ROMs incorporate the governing equations to solve a low-dimensional version of the FOM. While projection-based ROMs can provide better accuracy and more physically consistent results, they incur a larger computational cost and often exhibit stability issues 11 that inhibit convergence. Additionally, they are generally not portable between different solvers.</p><p>ROMs are developed for both steady and unsteady physics problems. The first approach involves computing a single point estimate of the low-dimensional embedding, while the latter involves making predictions over a prescribed time horizon. Unsteady non-intrusive ROMs combine either POD or CAE for spatial reconstruction of full-order states with a model used to make time-series predictions of the low-dimensional embeddings. Deep learning methods are popular for this as well, including long short-term memory (LSTM) neural networks <ref type="bibr" target="#b11">12</ref> and transformer neural networks. <ref type="bibr" target="#b7">8</ref> Both LSTMs and transformers are powerful models for handling data that are sequential in nature such as time-series data. Transformers utilize an architecture that is much more complex than the one LSTMs use, which can lead to better performance for complex time-series problems and other tasks such as developing large language models. <ref type="bibr" target="#b12">13</ref> However, this comes at an increased cost for both training and inference when compared to LSTMs, which use a significantly lower number of parameters. The use of LSTMs for time-series forecasting is well-established, <ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15</ref> making them a popular choice.</p><p>When making time-series predictions at unseen data sets over a long time horizon, error propagation is a common issue. Errors made in early predictions can accumulate and compound over time, leading to substantial inaccuracies. Unseen data sets are particularly suspect to this, as the model may not account for shifting data patterns. Additionally, there is no feedback from previously seen data to correct errors. This phenomenon is commonly observed in data-driven computational fluid dynamics (CFD) applications. <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17</ref> As a result, most studies in the literature focus on applying unsteady non-intrusive ROMs to single-parameter problems, <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18</ref> where the ROM is both trained on and used for a single design. We have identified two previously published studies that combine CAEs and LSTMs for non-intrusive ROMs and apply them to unseen designs. <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20</ref> The work by Maulik et al. does not mention the error propagation issue, while the work by Hasegawa et al. presents results of low accuracy. A recent paper <ref type="bibr" target="#b20">21</ref> by Jeon et al. introduced a hybrid AI-CFD method using flow residuals to address the issue of error propagation. However, this method is intrusive and requires the ROM to have access to the CFD solver for computing the residuals, which leads to a more computationally intensive online stage.</p><p>In this work, we propose the use of ensemble learning, <ref type="bibr" target="#b21">22</ref> a machine learning technique for improving the stability and lowering the variance of predictive models. Ensemble learning involves combining multiple base models, referred to as weak learners, to create a composite model that offers greater accuracy. To this end, we employ bootstrap aggregating (bagging) as the ensemble learning method for the temporal portion of the ROM, which involves training the weak learners on subsets of the dataset chosen randomly through sampling with replacement. The fully data-driven framework is referred to as the CAE-eLSTM ROM, and our results show that using ensembles leads to significantly improved stability and predictive performance when applied to two unsteady, incompressible, laminar fluid dynamics problems using different CFD solvers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In this section, we give an overview of both CAEs and LSTMs and how they are combined to develop an unsteady ROM using bagging as an ensemble learning method. CAEs are used to both provide a low-dimensional latent representation of the solution space and provide spatial reconstructions of full-order states. A temporal forecasting model of the latent variables is also required when using unsteady ROMs. LSTMs, a type of recurrent neural network, are used in this work. Although we do not provide direct comparisons to POD-based ROMs, a brief introduction to POD is given in this section. Comparisons of reconstruction errors using POD and CAE are given in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Proper Orthogonal Decomposition</head><p>Using training data obtained from a set of n solution snapshots calculated at chosen design points in the parameter space, a snapshot matrix, X ∈ R N ×n , can be assembled.</p><formula xml:id="formula_0">X ∈ R N ×n = [x 1 , x 2 , • • • , x n ] = [x(µ 1 ), x(µ 2 ), • • • , x(µ n )].<label>(1)</label></formula><p>A subspace V associated with X exists such that V = span(X). It assumed that V provides a good approximation of the solutions for µ within the parameter space if there is a sufficient variety of training data in</p><formula xml:id="formula_1">X. A rank k set of orthonormal basis vectors [ψ 1 , ψ 2 , • • • , ψ k ] ∈ R N ,</formula><p>where k ≪ N , is associated with V such that each solution x i in X can be reconstructed as a linear combination of the basis vectors</p><formula xml:id="formula_2">x i ≈ a i 1 ψ 1 + a i 2 ψ 2 • • • + a i k ψ k ,<label>(2)</label></formula><p>where a i are the low-dimensional expansion coefficients. The truncated singular value decomposition (SVD) of X decomposes the matrix into two orthonormal matrices U ∈ R N ×n and V ∈ R n×n , and a diagonal matrix Σ ∈ R n×n such that X = U ΣV T .</p><p>(3) U contains a set of n left singular vectors that span the column space of X, V contains a set of n right singular vectors that span the row space of X, and diag(Σ</p><formula xml:id="formula_3">) ∈ R n = [σ 1 , σ 2 , • • • , σ n ] contains the singular values in decreasing order, σ 1 ≥ • • • ≥ σ n ≥ 0. The first k vectors of U form the POD basis, Ψ ∈ R N ×k = [ψ 1 , ψ 2 , • • • , ψ k ].</formula><p>The singular values are a measure of the amount of information represented by each vector. Often, the singular values decay rapidly, and only the first k singular vectors are chosen to form the POD basis to preserve the most important features of the solution space. The relative information content E of the subspace is used in practice to choose a value of k,</p><formula xml:id="formula_4">E(k) = k j=1 σ 2 j n j=1 σ 2 j ,<label>(4)</label></formula><p>and k is chosen such that E(k) ≥ γ, where γ ∈ [0,1] is usually set to a value γ ≥ 0.95. <ref type="bibr" target="#b22">23</ref> Approximations of full-order solutions at unseen design parameters x(µ * ) are obtained as</p><formula xml:id="formula_5">x(µ * ) ≈ Ψa * = a * 1 ψ 1 + a * 2 ψ 2 • • • + a * k ψ k .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Convolutional Autoencoders</head><p>Convolutional neural networks were initially developed for computer vision applications, where they have been shown to vastly outperform traditional statistical image processing techniques. <ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25</ref> They have also demonstrated high predictive performance when used in ROMs, <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26</ref> highlighting their versatility. Autoencoders are a type of feedforward neural network that aim to accurately reconstruct inputs in the output layer, g : x → x where x ≈ x. Autoencoders are composed of two individual feedforward neural networks. The encoder g enc : R N → R k where k ≪ N maps a high-dimensional input x into the low-dimensional latent space a, and the decoder g dec : R k → R N maps the latent variables back to an approximation of the high-dimensional input x. The combination of the two results in</p><formula xml:id="formula_6">g : x = g dec • g enc (x).<label>(6)</label></formula><p>Deep neural networks provide nonlinear function maps through the use of activation functions. During training, the neural network parameters are tuned using backpropagation, <ref type="bibr" target="#b26">27</ref> an algorithm utilizing automatic differentiation that aims to minimize a differentiable loss function that measures the discrepancy between the computed and true outputs. This allows for discovering highly complex and abstract functional relationships that do not follow any strong model assumptions. The number of trainable parameters in vanilla artificial neural networks grows large with the number of hidden layers and neurons, and can lead to a very large computational training cost.</p><p>Convolutional neural networks effectively implement parameter sharing to limit the total number of trainable parameters in the network, where rather than weight combinations existing for each pair of neurons between layers, multiple neurons share a single weight. Convolutional layers use a number of filters that convolve over the input data, with each filter having its own set of weights. Pooling layers are also used in convolutional networks to summarize the features in input layers through operations including averaging and maximization. The input layer to a convolutional neural network is composed of a number of channels, with each representing a different state. In images, this is generally the levels of red, green, and blue present in each pixel, while for physical simulation data the states can represent components of the normalized velocity, pressure, density, etc. Once an autoencoder is sufficiently trained and g(x) ≈ x for all inputs over the training dataset X, the corresponding latent variables a can be passed to the decoder g dec (a) to obtain accurate approximations x for all data in X. High-dimensional data existing outside of the training set x * may also be well-approximated if an accurate approximation of the latent variables a * is obtained. Figure <ref type="figure" target="#fig_0">1</ref> shows an example of the encoder section of a convolutional autoencoder, which is composed of convolutional, pooling, and fully connected layers. A similar architecture in reverse would form the decoder. The input and output layers of CAEs are often structured as two-dimensional states within each channel when used for two-dimensional physical simulations. To accomodate this structure, training data needs to be reshaped before being input into the network through the use of a reshape operator R,</p><formula xml:id="formula_7">R : R N ×nc → R ny×nx×nc ,<label>(7)</label></formula><p>where n y refers to the number of data points in the vertical direction and n x the number of data points in the horizontal direction. The reshape operator is applied to each separate state that occupies the n c input channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Long Short-Term memory (LSTM) Neural Networks</head><p>Traditional recurrent neural networks are well-suited for handling sequential data; however, they have difficulties learning and capturing long-range dependencies. A primary reason for this is the vanishing gradient problem, <ref type="bibr" target="#b27">28</ref> which occurs when gradients shrink significantly as they are back-propagated through time, which can cause the model to forget early sequence information. LSTMs were introduced <ref type="bibr" target="#b28">29</ref> to address this issue by using a more complex network architecture that includes a series of gates which effectively control the flow of information. An individual LSTM cell consists of a cell state c t which acts as an internal memory, and a hidden state h t , which serves as an output. </p><formula xml:id="formula_8">f t = σ(F f (a t )) i t = σ(F i (a t )) o t = σ(F o (a t )) ct = tanh(F c (a t )) c t = f t ⊙ c t-1 + i t ⊙ ct h t = o t ⊙ tanh(c t ),<label>(8)</label></formula><p>where σ is the sigmoid function, tanh is the hyperbolic tangent function, and ⊙ is the Hadamard product. F is a linear function of the weights W a ∈ R n h and W h ∈ R n h and biases b ∈ R n h , where n h is the number of neurons in the hidden layer of each LSTM cell, and is given as</p><formula xml:id="formula_9">F = W a a t + W h h t-1 + b.<label>(9)</label></formula><p>A unique set of weights and biases belongs to each gate or cell state. When using LSTMs for time-series prediction, an autoregressive prediction is used, often referred to as the sliding window approach. For a given input sequence containing multivariate data for w timesteps, the next step in the sequence is predicted. When making the next prediction, the current prediction is incorporated into the input sequence by removing the first element and shifting the rest to the prior position. Eventually, the model inputs will consist of only previously computed predictions, which makes model performance sensitive to error propagation. The input sequence length, or window size w, is an important hyperparameter to consider when training LSTMs. Although LSTMs are designed to effectively learn long-range dependencies, using an input sequence length that is too long can lead to the inclusion of outdated and irrelevant information. On the other hand, using an input sequence length that is too small can ignore important long-range information. The optimal choice is highly problem-dependent; similar problems found in the literature can be consulted, or methods such as cross-validation can be used to find an optimal value. A diagram of a single-layer LSTM making a prediction one timestep ahead is shown in Figure <ref type="figure" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Ensemble Learning</head><p>While LSTMs are powerful models for time-series prediction, the quality of predictions over long time horizons is highly sensitive to the weights and biases of the model. Additionally, the total number of weights and biases is usually very large, which makes finding an optimal configuration very difficult, even when using state of the art optimization algorithms.</p><p>To mitigate the issue of high model variance, ensemble learning is a commonly used approach. By leveraging multiple base models that exhibit high variance, ensemble methods significantly reduce errors and improve robustness. Boosting and bootstrap aggregating (bagging) are the two main types of ensemble learning methods. Boosting 30 trains individual models sequentially, where each subsequent weak learner focuses on correcting prediction errors from the previous ones. While boosting is widely used and offers good performance, the base models must be trained iteratively, which incurs a large computational cost, especially in the context of neural networks.</p><p>Bagging <ref type="bibr" target="#b30">31</ref> is an algorithm that consists of two stages: bootstrapping and aggregation. Bootstrapping is a resampling technique where multiple random subsets of the data set, chosen through sampling with replacement, are constructed. The subsets of the data typically contain the same number of data points as the original data set. This approach leads to individual data points being present multiple times in the individual subsets. An example of bootstrapping is shown in Figure <ref type="figure">3</ref>, where the bootstrapped data sets contain more or less instances of the original data points. Multiple base models are trained individually on each of the bootstrapped data sets, which can be done in parallel. The aggregation stage creates an ensemble model by taking an average of the predictions given by each weak learner. Given m weak learners f i , the aggregate prediction f is given as</p><formula xml:id="formula_10">f = 1 m m i f i . (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>Bagging reduces the issue of error propagation by averaging the errors of multiple weak learners, which will become increasingly small as the number of learners grows, leading to much lower variance. Bagging also offers a considerable gain in robustness and generalization by leveraging the diversity of the weak learners to increase the capability of handling varying patterns outside of the training data. As an increasing number of weak learners are added, the reduction in variance plateaus, leading to diminishing returns in performance gains. After a certain point, the computational cost of adding more weak learners outweighs the marginal improvement in predictive performance. As the complexity of the problem increases, the number of weak learners required to make accurate predictions also generally increases. While theoretical considerations can be made to analyze the trade-offs between bias, variance, and covariance to determine the optimal number of weak learners, this number is difficult to choose a priori. A reasonable choice is based on both the problem's complexity and similar problems found in the literature, which are not yet present in our case and are established in this work. Figure <ref type="figure">3</ref>: An example of bootstrapping, where random subsets of the original dataset are chosen through sampling with replacement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">CAE-eLSTM ROM</head><p>This section describes the non-intrusive ROM framework combining convolutional autoencoders for spatial reconstruction and LSTM ensembles utilizing bagging for temporal prediction, which we will refer to as a CAE-eLSTM ROM. The offline stage is computationally intensive, requiring multiple solves of the FOM in addition to training multiple neural networks. First, solutions to the FOM for designs µ in the training set of design parameters U train ∈ R n×p are obtained for all timesteps and are assembled into a matrix X. Next, a convolutional autoencoder with latent dimension k is trained on X for a sufficient number of epochs such that inputs are accurately reconstructed. Then, the expansion coefficients for the training data A train are computed by feeding the full-order states from X through the encoder g enc . Sequences of length w are then generated from A train and m LSTMs using the same set of initial weights and biases are trained on individual data subsets chosen randomly through sampling with replacement. PyTorch, <ref type="bibr" target="#b31">32</ref> an open source deep learning framework available for Python, is used to implement the autoencoder and LSTM using default weight and bias initialization settings.</p><p>A schematic describing the online stage of the ROM is shown in Figure <ref type="figure" target="#fig_4">4</ref>. The online stage involves executing the ROM to compute approximate solutions at a point µ * . First, the FOM is run for T i timesteps, as an initial sequence of latent variables is required for the LSTM ensemble. The computed full-order states are fed through the encoder to obtain the initial sequence of latent variables A * Ti ∈ R k×w . The FOM must be run for at least T i ≥ w timesteps, and potentially longer depending on the how useful simulation data from the initial timesteps are for the ROM. The latent variables for the rest of the prediction horizon are calculated autoregressively using the LSTM ensemble by taking an average of the predictions from the individual weak learners. After the predicted latent variables are found for each timestep t, they are fed through the decoder to obtain approximate full-order fields. The online and offline stages are outlined in Algorithm 1.</p><p>When using a ROM with a single LSTM as in previous works, <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20</ref> the LSTM is trained on the entire time-series dataset and is used alone for time-series prediction. When compared to these works, the main difference in our presented ROM framework is the use of an ensemble model for time-series prediction. Although this results in an increased offline training cost, using an ensemble allows for better accuracy and stability over long prediction horizons. While the use of an ensemble LSTM is simple, it can greatly improve ROM performance without requiring additional training data. Compute high-fidelity solutions for µ ∈ U train by solving FOM and assemble into X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Train convolutional autoencoder with latent dimension k on X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Calculate expansion coefficients for training data A train = g enc (X).</p><p>5: Run FOM for T i timesteps at µ * and compute the expansion coefficients</p><formula xml:id="formula_12">Train m LSTMs E = [LSTM 1 (A), LSTM 2 (A), • • • LSTM m (A)]</formula><formula xml:id="formula_13">A * Ti = g enc [X * T1 , X * T2 , • • • X * Ti ] .</formula><p>3:</p><formula xml:id="formula_14">for t ∈ {T i , T i+1 , . . . , T -1} do 4:</formula><p>Compute a * t+1 = E(A * t ) by using an average of the m individual LSTM predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Incorporate a * t+1 into the current window A * t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>end for 7:</p><p>Predict full-order fields X * = g dec (A * ) 8:</p><p>return X * 9: end function 3 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The two test cases used to demonstrate the performance of the CAE-eLSTM ROM are a lid-driven cavity and the flow over a cylinder. Both cases use two-dimensional computational domains. The lid-driven cavity case consists of three design parameters controlling the geometry of the domain, while the cylinder case consists of two design parameters controlling geometric and physical properties of the simulation. Different CFD solvers are used for each case; this is done to illustrate the portability of the ROM and allow for the use of a structured grid for the cylinder case. Latin hypercube sampling (LHS), <ref type="bibr" target="#b32">33</ref> a popular statistical method for generating samples from a multi-dimensional parameter space is used to generate sets of design parameters that are split into training and test sets. LHS aims to maximize the distance and minimize the correlation amongst samples produced by using uniform sampling. The ROM is used to predict the vertical and horizontal components of the velocity. The training data for each velocity component is scaled to a range [0,1] using min-max scaling before input into the CAE. Similarly, the CAE latent variables are also scaled using min-max scaling before being used for the LSTM. Data normalization improves performance and allows the optimizer to learn optimal network parameters at a much faster rate. <ref type="bibr" target="#b33">34</ref> Both the CAE and LSTM are trained using the mean squared error loss function. An NVIDIA DGX system, consisting of eight NVIDIA A100 GPUs, is used to train the CAE and LSTMs, run the cylinder simulations, and for ROM inference. The lid-driven cavity simulations are run on a local workstation using a single CPU core. </p><formula xml:id="formula_15">ϵ s = 1 T -T i T t=Ti+1 ∥x t -xt ∥ 2 ∥x t ∥ 2 .<label>(11)</label></formula><p>The errors are then averaged over each seed to give a single error metric</p><formula xml:id="formula_16">ε ε = 1 N s Ns s=1 ϵ s . (<label>12</label></formula><formula xml:id="formula_17">)</formula><p>We are also interested in how sensitive the error is to the initial weights and biases of the model. The sample standard deviation σ s of ϵ s is used as a measure of this,</p><formula xml:id="formula_18">σ s = std(ϵ s ).<label>(13)</label></formula><p>A lower standard deviation in the error term indicates that the model performance over the prediction horizon does not vary much with the initial weights and biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Lid-Driven Cavity</head><p>The first test case is a geometrically parameterized two-dimensional lid-driven cavity flow, a popular benchmark problem for CFD solvers. Unsteady, incompressible, laminar flow is simulated by solving the Navier-Stokes equations, given as</p><formula xml:id="formula_19">S - → V • d⃗ n dS = 0, (<label>14</label></formula><formula xml:id="formula_20">) Ω ∂ - → V ∂t dΩ + S - → V - → V • d⃗ n dS + Ω ∇p dΩ - S ν(∇ - → V + ∇ - → V T ) • d⃗ n dS = 0,<label>(15)</label></formula><p>Figure <ref type="figure">5</ref>: Schematics describing the lid-driven cavity problem.</p><p>where -→ V = [u, v] is the velocity vector, Ω is the fluid domain, S is the face-area vector, ⃗ n is the outward-pointing normal, ν is the kinematic viscosity, and p is the pressure. OpenFOAM, <ref type="bibr" target="#b34">35</ref> an open-source toolbox for multiphysics simulation is used. The boundary conditions and design parameters are shown in Figure <ref type="figure">5</ref>. On each edge Γ i , i ∈ [1, 2, 3, 4] of the computational domain Ω, the pressure gradient ∇p is set to 0. No slip conditions exist on all of the edges except the top, where u = 1, v = 0. The reference pressure is set to p = 0 at the bottom left corner. The first design parameter µ 1 controls the horizontal length, the second µ 2 controls the slanting length, while the third µ 3 controls the slanting angle. The Reynolds number Re is set to 400, and is related to the kinematic viscosity as</p><formula xml:id="formula_21">Re = 400 = max(µ 1 , µ 2 ) ν(µ) . (<label>16</label></formula><formula xml:id="formula_22">)</formula><p>The bounds for the design parameters are given as</p><formula xml:id="formula_23">µ 1 ∈ [1.2, 1.8], µ 2 ∈ [1.2, 1.8], µ 3 ∈ [- π 6 , π<label>6</label></formula><p>].</p><p>The computational mesh consists of 128 × 128 cells distributed uniformly in the x and y directions. A single cell exists in the spanwise direction z, resulting in N = 16384. The initial condition is the solution to steady, incompressible, laminar flow at Re = 20 which is computed using the standard OpenFOAM solver simpleFoam. Unsteady flow is simulated using the OpenFOAM solver icoFoam for T = 5 seconds with data being saved every 0.025 seconds, leading to 200 time snapshots for a single simulation. Both solvers utilize the finite volume method (FVM). Figure <ref type="figure">6</ref> shows contours of u and v at T = 5 at three different sets of design parameters. A vortex that varies in shape, size, and location with the design parameters is shown for both velocity components. The variation of the flow with µ is highly nonlinear, making this a difficult prediction problem for ROMs. 100 sets of design parameters are generated and randomly split into 90 training samples and 10 test points, which are given in Table <ref type="table" target="#tab_1">I</ref>. The CAE-eLSTM ROM uses m = 64 bagged LSTMs and a window size w = 20. These parameters were chosen through a trial-and-error process with the goal of maximizing predictive accuracy while trying to keep the computational cost of training the LSTM ensemble relatively low. This was done by directly evaluating the performance on the test set, while in practice methods like cross-validation would be used. Initial guesses for the window size were based on similar problems found in the literature. An initial number of approximately 50 weak learners was used, which was increased in increments of 8 until the performance gains plateaued. The CAE latent dimension is set to k = 4; below this value, the CAE reconstruction errors were found to be worse, and increasing k offered no improvement. The LSTM architecture consists of two hidden layers consisting of 50 neurons each and a dropout 36 rate of 0.  Figure <ref type="figure" target="#fig_7">7</ref> shows the trajectories of the latent variables computed using ensemble and single LSTMs at the test point µ * = [1.299, 1.689, -0.0367] using five different seeds which control the initial weights and biases of the LSTM. It is shown that for all of the latent variables, using LSTM ensembles leads to higher prediction accuracy and significantly lower variance. When using a single LSTM, the different predictions quickly diverge from each other and the ground truth. Due to their high capacity to learn, neural networks often exhibit high variance. Since neural networks contain a large number of parameters, the effects of error propagation between different trained models are heightened due to relatively large differences in autoregressive predictions being fed back into the model. In contrast, the ensemble model is much less sensitive to the initial seed, and the predicted trajectories do not differ much. The second latent variable exhibits diverging trajectories when using the ensemble model, but the effect is much less pronounced than when using a single LSTM. Figure <ref type="figure" target="#fig_8">8</ref> shows contours of u and v at the test point as well as absolute ROM errors averaged over the last 10% of the simulation (t ∈ [4.5, 5] seconds) for a single seed. The errors are considerably larger when using a single    </p><formula xml:id="formula_24">Test Case Index ε, u (Ensemble) ε, u (Single) ε, v (Ensemble) ε, v<label>(Single)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">2D Cylinder</head><p>The next test case involves two-dimensional incompressible, unsteady, laminar flow over a cylinder between two solid walls. Eventually, the lid-driven cavity flow from the previous test case reaches steady-state and does not exhibit long-term transient behavior that is commonly found in fluid dynamics problems. Laminar flow over a cylinder is a well-studied problem in fluid dynamics, with both experimental and computational results present in the literature. <ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39</ref> Unsteady cylinder flow is characterized by the presence of vortices that separate from the surface and form in the wake. This distinctive pattern is known as the Von Kármán vortex street, where alternating vortices of a regular pattern are shed downstream of the cylinder. The unsteady Navier-Stokes equations found in equations 14 and 15 are solved using XLB, <ref type="bibr" target="#b39">40</ref> a Lattice Boltzmann method <ref type="bibr" target="#b40">41</ref> (LBM) library utilizing the JAX framework <ref type="bibr" target="#b41">42</ref> available for Python, which allows for effective scaling across multiple CPUs, GPUs, and distributed multi-GPU systems. The cited works can be referred to for an overview of the Lattice Boltzmann method and its implementation in XLB.</p><p>No-slip boundary conditions are applied to the cylinder's surface and top and bottom walls. A Poiseuille flow profile is used for the inlet velocity. Extrapolation outflow boundary conditions are used for the outlet to allow the fluid flow to exit the domain freely. The computational domain measures 1536 × 512 voxels that are uniformly spaced. The cylinder is centered at x c , y c = [160, 256] (zero-based indexing is used). Simulation results are down-sampled onto a grid that measures 384 × 128 before being used for the ROM, resulting in N = 49152, as the original domain's large size leads to a very large training cost as well as memory usage. Two design parameters are used, the diameter d of the cylinder and the Reynolds number Re. The bounds of the design parameters are given as</p><formula xml:id="formula_25">µ 1 = d ∈ [48, 68], µ 2 = Re ∈ [120, 240].</formula><p>The diameter d is set to an integer quantity by rounding to the nearest whole number. The wake structure behind the cylinder undergoes instabilities <ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44</ref> at a critical reynolds number of approximately Re c = 180, where the vortices transition to becoming turbulent. As a result, the prescribed range given for Re in the parameter space includes a variety of physical regimes. Additionally, both Re and d control the size and periodicity of the shed vortices, making this a difficult prediction problem for ROMs. The freestream velocity in the x-direction is set to u ∞ = 0.001 in non-dimensional units. The simulation is run for T = 750000 timesteps with data being saved every 1000 steps, leading to 750 snapshots for a single simulation. In LBM simulations, time and space are often non-dimensionalized by ∆t and ∆x to arrive at dimensionless values of ∆x * = ∆t * = 1. However, one needs to ensure that ∆t/∆x ≪ 1 to avoid compressibility errors. In these runs we picked ∆t/∆x = 0.001. The initial condition for a simulation of a given diameter d is the solution to steady flow at Re = 20. 50 sets of design parameters are generated and split into 45 training samples and 5 test points, shown in Figure <ref type="figure" target="#fig_9">9</ref>. Table V also lists the test point design parameters. The ROM uses m = 96 bagged LSTMs and a window size of w = 30, which were again chosen through a trial-and-error process to maximize accuracy while keeping the number of weak learners to a minimum. The CAE latent dimension is set to k = 10. Similar to the lid-driven cavity case, values below k = 10 resulted in higher reconstruction errors and increasing the latent dimension further offers no improvement. A bidirectional LSTM architecture <ref type="bibr" target="#b44">45</ref> is used. Bidirectional recurrent neural networks process sequential data in both forward and backward directions, allowing the model to learn both past and future context. For problems that are characterized by regularly repeating patterns such as a vortex street, bidirectional LSTMs can better learn the cyclic nature of the pattern by leveraging context in both directions. The network consists of three hidden layers with 36 neurons each and a dropout rate of 0.15. The output layer again contains a sigmoid activation function, and the Adam optimizer with the same initial learning rate η = 5 × 10 -4 and weight decay of λ = 1 × 10 -6 is used for both the LSTM and CAE. Again, the CAE is trained for 200 epochs while an individual LSTM is trained for 250 epochs. The CAE architecture is given in Appendix B. At the test points, the FOM is run for T i = 300000 seconds (300 snapshots), or 40% of the total simulation time. This value is required to be high as the flow exhibits highly oscillatory behavior initially, leading to very noisy latent variables that cannot be used for model training. As a result, latent variable sequences are generated starting at the 200th snapshot, and A train does not contain time-series data of latent variables before this point.  Figure <ref type="figure" target="#fig_2">10</ref> shows the latent variable trajectories for the first four latent variables at the test point µ * = [51, 142.2]. For each latent variable, the predictions given by single LSTMs are initially similar, but eventually diverge in terms of both the amplitude and frequency of the latent variables, leading to large inaccuracies. Using LSTM ensembles greatly reduces this effect, and the resultant latent variable trajectories follow the ground truth closely and do not differ greatly in amplitude nor frequency. The regular pattern of the Kármán vortex street is well-predicted given a small amount of initial latent variable history.</p><p>Figure <ref type="figure" target="#fig_10">11</ref> shows snapshots of u and v at T = 750000 as well as ROM errors averaged over the last 10% of the simulation for a single seed. The errors are significantly lower throughout the computational domain when using LSTM ensembles, which are greatest in the wake of the cylinder and immediately downstream of it.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This work presents a fully data-driven ROM framework for time-dependent physics simulations using convolutional autoencoders and LSTM ensembles. The combined framework, referred to as the CAE-eLSTM ROM, is used to mitigate the issue of error propagation when performing autoregressive time-series prediction at unseen data sets, a common problem in data-driven CFD applications. The ROM obtains a low-dimensional solution manifold using convolutional autoencoders, a type of artificial neural network useful for reconstructing spatially distributed data. Using the encoder section of the autoencoder, the low-dimensional latent variables of the training data are used to generate multivariate time-series sequences that are used to train LSTMs, a type of recurrent neural network useful for modeling sequential data. Bagging, a popular ensemble learning technique, is used to train multiple base LSTMs on subsets of the training sequences sampled randomly with replacement. Ensemble learning is chosen as it is a useful tool for improving the stability and accuracy of machine learning methods. At unseen test points, the full-order model is run to obtain an initial sequence of latent variables through the encoder and autoregressive time-series prediction is used to predict the latent variables over a prescribed time horizon. The predicted latent variables are then passed through the decoder to obtain spatial reconstructions of the FOM at different points in time.</p><p>When applied to two incompressible, unsteady, laminar fluid dynamics problems, the proposed ROM exhibits high levels of accuracy when compared to a ROM using a single LSTM model. The use of LSTM ensembles significantly diminishes the error propagation issue, with latent variable trajectories from different weight initializations showing low levels of variance. Error metrics show that using the ensemble ROM leads to better predictive performance at all test points. Furthermore, the fully data-driven nature of the ROM allows it to be applied to two different CFD solvers. Although the cost associated with training ensemble neural networks is high, bagging allows for models to be trained in parallel, and multi-GPU architectures can be used to significantly reduce the cost. The addition of an ensemble model for time-series predictions, while simple, greatly improves the performance of the ROM and does not require additional training data. A limitation of the presented framework is its significantly increased offline training cost. While this can be alleviated to a great degree by training weak learners in parallel, multi-GPU platforms are expensive and may not be readily available for practitioners. Additionally, the use of convolutional autoencoders limits the application of the ROM to structured grids unless a mesh interpolation method is used. Future work will focus on further developing the method to apply it to three-dimensional geometries, problems exhibiting turbulent flow, and more complex geometries involving unstructured meshes. Problems consisting of time-series datasets that exhibit non-smooth variations over long periods of time will be of particular interest, as they are more representative of real-world applications and can reap greater benefits from ensemble learning. Computationally efficient optimal hyperparameter selection methods will also be explored to lower the offline costs of training the ROM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the encoder of a convolutional autoencoder (CAE) consisting of convolutional, pooling, and fully connected layers.</figDesc><graphic coords="4,83.70,228.15,444.60,97.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Diagram of a single-layer LSTM neural network making a prediction one timestep ahead.</figDesc><graphic coords="5,165.60,287.38,280.80,279.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Offline and online stages of CAE-eLSTM ROM 1: function CAE_ ELSTM_ OFFLINE(U train , k, m, w) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>using a window size of w on sequences sampled randomly with replacement from A train . 6: return (g enc , g dec , E) 7: end function 1: function CAE_ ELSTM_ ONLINE(µ * , g enc , g dec , E) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Schematic of the online stage of the CAE-eLSTM ROM.</figDesc><graphic coords="8,83.70,72.00,444.60,267.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>1. The output layer contains a sigmoid activation function so the outputs are constrained to a range of [0, 1]. The CAE architecture is given in Appendix B. The Adam optimizer 37 is used to train both the CAE and LSTM, with an initial learning rate of η = 5 × 10 -4 and weight decay of λ = 1 × 10 -6 . Dropout and weight decay are used to prevent overfitting. The CAE is trained for a total of 200 epochs, while an individual LSTM is trained for 250 epochs. At the test points, the FOM is simulated for T i = 0.75 seconds (30 snapshots), or 15% of the total time horizon. The final 20 of these snapshots are used as the initial input sequence for the LSTM ensemble.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(a) µ = ( 1 . 5 , 3 √ 3 , -π 6 ) 3 √ 3 , π 6 ) 3 √ 3 , -π 6 ) 3 √ 3 , π 6 )Figure 6 :</head><label>153363363363366</label><figDesc>Figure 6: Snapshots at T = 5 of u (top) and v (bottom) for the lid-driven cavity problem at three different sets of design parameters.</figDesc><graphic coords="10,76.28,410.58,164.22,126.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparison of latent variable trajectories for the lid-driven cavity case at µ * = [1.299, 1.689, -0.0367].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Time snapshot of u (top) and v (bottom) at T = 5 and errors averaged over the last 10% of the simulation at µ * = [1.299, 1.689, -0.0367].</figDesc><graphic coords="13,128.01,382.10,117.00,160.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Training and test design parameters for the 2D cylinder case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Time snapshots of u and v at T = 750000 and errors averaged over the last 10% of the simulation at µ * = [51, 142.2].</figDesc><graphic coords="18,75.43,414.84,229.31,105.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,83.70,371.23,444.61,181.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The cell state is responsible for selectively retaining or forgetting information from previous steps, while the hidden state represents the model's cumulative output. LSTM cells contain a forget gate f t , input gate i t , and output gate o t . Forget gates determine what information from previous states should be forgotten. Input gates decide what new pieces of information should be stored in the current state. Output gates use the current and previous cell states to determine what information is retained in the model. Using this architecture, the model is able to retain information that is relevant for long-range sequential dependencies and discard information that becomes irrelevant over time, making LSTMs a popular choice for sequential modeling. For a given input a t ∈ R k , the LSTM equations are given as</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I :</head><label>I</label><figDesc>Test case design parameters for the lid-driven cavity problem.LSTM throughout the computational domain for both u and v. Table II lists the seed-averaged relative errors ε in u and v for the test points. The ensemble method offers better performance in predicting both fields at all of the test points, usually by wide margins. Table III lists the standard deviation of these errors; at all of the test points, the standard deviation is also smaller. As there is a lower spread amongst the different seeds in the time-averaged error metrics, this shows that using the ensemble method leads to much greater stability and reliability in predictions. Table IV lists the computational costs associated with neural network training, CFD simulation, and ROM inference. The given CFD wall time is the portion of the simulation over the prediction time horizon; using the ROM for inference over this period of time offers a speed-up of approximately 9.4x over CFD. The ROM inference costs include autoregressive prediction using the ensemble LSTM and obtaining full-order approximations using the decoder. The CAE is much more expensive to train than a single LSTM as it has a larger number of trainable parameters. While a multi-GPU architecture was used to train the LSTMs in parallel, the cost to train a single LSTM is given.</figDesc><table><row><cell>Latent Variable Magnitude</cell><cell>0.8 0.6 0.4 0.2 0.0 0.2 0.4</cell><cell></cell><cell></cell><cell cols="9">Test Case Index µ 1 1 1.473 1.701 0.288 µ 2 µ 3 2 1.659 1.653 -0.372 3 1.593 1.611 0.455 4 1.209 1.443 0.371 5 1.299 1.689 -0.0367 6 1.371 1.311 -0.351 7 1.443 1.617 -0.487 8 1.232 1.335 -0.330 9 1.719 1.785 -0.455 CAE Latent Variable #1 CAE Latent Variable #1 Ground Truth Ensemble LSTM #1 Ensemble LSTM #2 Ensemble LSTM #3 Ensemble LSTM #4 Ensemble LSTM #5 1.00 0.75 0.50 0.25 0.00 0.25 Latent Variable Magnitude</cell><cell>Ground Truth Single LSTM #1 Single LSTM #2 Single LSTM #3 Single LSTM #4 Single LSTM #5</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell cols="6">1.281 1.449 -0.340 1.25</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>25</cell><cell>50</cell><cell cols="2">75 Time Snapshot 100 125 150 175 200</cell><cell></cell><cell></cell><cell>0</cell><cell>25</cell><cell>50</cell><cell>75 Time Snapshot 100 125 150 175 200</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CAE Latent Variable #2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CAE Latent Variable #2</cell></row><row><cell>Latent Variable Magnitude</cell><cell>0.8 1.0 1.2 1.4</cell><cell>0</cell><cell>25</cell><cell>50</cell><cell cols="2">75 Time Snapshot 100 125 150 175 200 Ground Truth Ensemble LSTM #1 Ensemble LSTM #2 Ensemble LSTM #3 Ensemble LSTM #4 Ensemble LSTM #5</cell><cell>Latent Variable Magnitude</cell><cell>1.4 0.4 0.6 0.8 1.0 1.2</cell><cell>0</cell><cell cols="2">25 Ground Truth 50 Single LSTM #1 Single LSTM #2 Single LSTM #3 Single LSTM #4 Single LSTM #5</cell><cell>75 Time Snapshot 100 125 150 175 200</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CAE Latent Variable #3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CAE Latent Variable #3</cell></row><row><cell></cell><cell>1.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.25</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Latent Variable Magnitude</cell><cell>0.25 0.00 0.25 0.50 0.75 1.00</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ground Truth Ensemble LSTM #1 Ensemble LSTM #2 Ensemble LSTM #3 Ensemble LSTM #4 Ensemble LSTM #5</cell><cell>Latent Variable Magnitude</cell><cell>0.25 0.00 0.25 0.50 0.75 1.00</cell><cell></cell><cell></cell><cell></cell><cell>Ground Truth Single LSTM #1 Single LSTM #2 Single LSTM #3 Single LSTM #4 Single LSTM #5</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>25</cell><cell>50</cell><cell cols="2">75 Time Snapshot 100 125 150 175 200</cell><cell></cell><cell></cell><cell>0</cell><cell>25</cell><cell>50</cell><cell>75 Time Snapshot 100 125 150 175 200</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CAE Latent Variable #4</cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell cols="2">CAE Latent Variable #4</cell></row><row><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Latent Variable Magnitude</cell><cell>2.5 2.0 1.5 1.0 0.5</cell><cell>0</cell><cell>25</cell><cell>50</cell><cell cols="2">75 Time Snapshot 100 125 150 175 200 Ground Truth Ensemble LSTM #1 Ensemble LSTM #2 Ensemble LSTM #3 Ensemble LSTM #4 Ensemble LSTM #5</cell><cell>Latent Variable Magnitude</cell><cell>2.5 2.0 1.5 1.0 0.5</cell><cell>0</cell><cell>25</cell><cell>50</cell><cell>75 Time Snapshot 100 125 150 175 200 Ground Truth Single LSTM #1 Single LSTM #2 Single LSTM #3 Single LSTM #4 Single LSTM #5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table II :</head><label>II</label><figDesc>Average relative errors in u and v for the lid-driven cavity case. Boldface denotes the lowest values.Test Case Index σ s , u (Ensemble) σ s , u (Single) σ s , v (Ensemble) σ s , v (Single)</figDesc><table><row><cell>1</cell><cell>0.02421</cell><cell>0.07119</cell><cell>0.02514</cell><cell>0.07204</cell></row><row><cell>2</cell><cell>0.01958</cell><cell>0.04199</cell><cell>0.02174</cell><cell>0.03870</cell></row><row><cell>3</cell><cell>0.02622</cell><cell>0.05282</cell><cell>0.02868</cell><cell>0.06296</cell></row><row><cell>4</cell><cell>0.03640</cell><cell>0.06575</cell><cell>0.03924</cell><cell>0.06742</cell></row><row><cell>5</cell><cell>0.02442</cell><cell>0.06589</cell><cell>0.01996</cell><cell>0.05131</cell></row><row><cell>6</cell><cell>0.04177</cell><cell>0.04939</cell><cell>0.03964</cell><cell>0.04733</cell></row><row><cell>7</cell><cell>0.02445</cell><cell>0.03771</cell><cell>0.02789</cell><cell>0.03915</cell></row><row><cell>8</cell><cell>0.04559</cell><cell>0.05304</cell><cell>0.04251</cell><cell>0.05194</cell></row><row><cell>9</cell><cell>0.03313</cell><cell>0.04678</cell><cell>0.03891</cell><cell>0.04900</cell></row><row><cell>10</cell><cell>0.02877</cell><cell>0.04291</cell><cell>0.02717</cell><cell>0.04079</cell></row><row><cell>1</cell><cell>0.005225</cell><cell>0.02819</cell><cell>0.004470</cell><cell>0.03051</cell></row><row><cell>2</cell><cell>0.001637</cell><cell>0.009202</cell><cell>0.001766</cell><cell>0.01267</cell></row><row><cell>3</cell><cell>0.004603</cell><cell>0.01885</cell><cell>0.004040</cell><cell>0.02263</cell></row><row><cell>4</cell><cell>0.008210</cell><cell>0.02007</cell><cell>0.006915</cell><cell>0.01172</cell></row><row><cell>5</cell><cell>0.003980</cell><cell>0.01121</cell><cell>0.004222</cell><cell>0.007739</cell></row><row><cell>6</cell><cell>0.003244</cell><cell>0.01555</cell><cell>0.002547</cell><cell>0.01726</cell></row><row><cell>7</cell><cell>0.002077</cell><cell>0.007614</cell><cell>0.002466</cell><cell>0.006886</cell></row><row><cell>8</cell><cell>0.002393</cell><cell>0.02031</cell><cell>0.002030</cell><cell>0.02041</cell></row><row><cell>9</cell><cell>0.002515</cell><cell>0.007371</cell><cell>0.002550</cell><cell>0.01324</cell></row><row><cell>10</cell><cell>0.002467</cell><cell>0.01627</cell><cell>0.001759</cell><cell>0.01493</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table III :</head><label>III</label><figDesc>Standard deviation in relative errors of u and v for the lid-driven cavity case. Boldface denotes the lowest values.</figDesc><table><row><cell>Task</cell><cell>Wall Time (s)</cell></row><row><cell>CAE</cell><cell>749</cell></row><row><cell>Single LSTM</cell><cell>81</cell></row><row><cell>CFD (1 CPU)</cell><cell>136</cell></row><row><cell cols="2">ROM Inference 14.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table IV :</head><label>IV</label><figDesc>Computational costs associated with the lid-driven cavity problem.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table V :</head><label>V</label><figDesc>Test case design parameters for the 2D cylinder case.</figDesc><table><row><cell>Latent Variable Magnitude</cell><cell>0.2 0.0 0.2 0.4 0.6 0.8</cell><cell></cell><cell cols="2">Ground Truth Ensemble LSTM #1 Ensemble LSTM #2 Ensemble LSTM #3 Ensemble LSTM #4 Ensemble LSTM #5</cell><cell cols="2">CAE Latent Variable #1</cell><cell cols="6">0.2 0.0 0.2 0.4 0.6 0.8 Test Case Index d 1.0 1 53 231.6 Re Ground Truth Single LSTM #1 Single LSTM #2 Single LSTM #3 2 51 142.2 Single LSTM #4 Single LSTM #5 3 65 154.8 4 57 171.0 Latent Variable Magnitude 5 61 225.6</cell><cell></cell><cell cols="2">CAE Latent Variable #1</cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300 Time Snapshot 400</cell><cell>500</cell><cell>600</cell><cell>700</cell><cell></cell><cell></cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300 Time Snapshot 400</cell><cell>500</cell><cell>600</cell><cell>700</cell></row><row><cell>Latent Variable Magnitude</cell><cell>0.2 0.0 0.2 0.4 0.6 0.8</cell><cell></cell><cell cols="2">Ground Truth Ensemble LSTM #1 Ensemble LSTM #2 Ensemble LSTM #3 Ensemble LSTM #4 Ensemble LSTM #5</cell><cell cols="2">CAE Latent Variable #2</cell><cell></cell><cell></cell><cell>Latent Variable Magnitude</cell><cell>0.25 0.00 0.25 0.50 0.75 1.00</cell><cell></cell><cell cols="2">Ground Truth Single LSTM #1 Single LSTM #2 Single LSTM #3 Single LSTM #4 Single LSTM #5</cell><cell cols="2">CAE Latent Variable #2</cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300 Time Snapshot 400</cell><cell>500</cell><cell>600</cell><cell>700</cell><cell></cell><cell></cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300 Time Snapshot 400</cell><cell>500</cell><cell>600</cell><cell>700</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CAE Latent Variable #3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CAE Latent Variable #3</cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Latent Variable Magnitude</cell><cell>0.1 0.0 0.1 0.2 0.3 0.4</cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300 Time Snapshot 400</cell><cell>500</cell><cell cols="2">600 Ground Truth 700 Ensemble LSTM #1 Ensemble LSTM #2 Ensemble LSTM #3 Ensemble LSTM #4 Ensemble LSTM #5</cell><cell>Latent Variable Magnitude</cell><cell>0.1 0.0 0.1 0.2 0.3 0.4</cell><cell>0</cell><cell>100 Ground Truth Single LSTM #1 Single LSTM #2 Single LSTM #3 Single LSTM #4 Single LSTM #5</cell><cell>200</cell><cell>300 Time Snapshot 400</cell><cell>500</cell><cell>600</cell><cell>700</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CAE Latent Variable #4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CAE Latent Variable #4</cell><cell></cell></row><row><cell>Latent Variable Magnitude</cell><cell>0.2 0.0 0.2 0.4 0.6 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ground Truth Ensemble LSTM #1 Ensemble LSTM #2 Ensemble LSTM #3 Ensemble LSTM #4 Ensemble LSTM #5</cell><cell>Latent Variable Magnitude</cell><cell>0.2 0.0 0.2 0.4 0.6 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ground Truth Single LSTM #1 Single LSTM #2 Single LSTM #3 Single LSTM #4 Single LSTM #5</cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300 Time Snapshot 400</cell><cell>500</cell><cell>600</cell><cell>700</cell><cell></cell><cell>0.6</cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300 Time Snapshot 400</cell><cell>500</cell><cell>600</cell><cell>700</cell></row></table><note><p>Figure 10: Comparison of latent variable trajectories for the 2D cylinder case at µ * = [51, 142.2].</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table VI :</head><label>VI</label><figDesc>Table VI lists the seed-averaged relative errors in u and v at the test points. The ensemble ROM again offers better performance in predicting both fields at all of the test points. Table VII lists the standard deviation of these errors, which are again lower at all of the test points. At the fourth test case index (µ * = [57, 171.0]), the errors and standard deviations are similar. This test point lies in the middle of the parameter space and is well-surrounded by training samples; as a result, the advantage gained using an ensemble method may be marginal. Table VIII lists the computational costs associated with the problem. Using the ROM for inference offers a speed-up of approximately 27x over CFD (again, the given CFD and ROM inference costs are for the portion of the simulation over the prediction time horizon). Due to the increased domain size and amount of training data, the CAE and LSTM are more expensive to train when compared to the lid-driven cavity case. Average relative errors in u and v for the 2D cylinder case. Boldface denotes the lowest values.Test Case Index σ s , u (Ensemble) σ s , u (Single) σ s , v (Ensemble) σ s , v (Single)</figDesc><table><row><cell>1</cell><cell>0.04324</cell><cell>0.07975</cell><cell>0.3379</cell><cell>0.6162</cell></row><row><cell>2</cell><cell>0.03463</cell><cell>0.05838</cell><cell>0.2971</cell><cell>0.4871</cell></row><row><cell>3</cell><cell>0.02380</cell><cell>0.05195</cell><cell>0.1399</cell><cell>0.3315</cell></row><row><cell>4</cell><cell>0.05446</cell><cell>0.06636</cell><cell>0.3995</cell><cell>0.4900</cell></row><row><cell>5</cell><cell>0.03655</cell><cell>0.06029</cell><cell>0.2269</cell><cell>0.4059</cell></row><row><cell>1</cell><cell>0.01321</cell><cell>0.04184</cell><cell>0.1138</cell><cell>0.3372</cell></row><row><cell>2</cell><cell>0.007607</cell><cell>0.03304</cell><cell>0.07005</cell><cell>0.2720</cell></row><row><cell>3</cell><cell>0.004610</cell><cell>0.01587</cell><cell>0.03493</cell><cell>0.1136</cell></row><row><cell>4</cell><cell>0.01521</cell><cell>0.02026</cell><cell>0.1186</cell><cell>0.1536</cell></row><row><cell>5</cell><cell>0.007528</cell><cell>0.03398</cell><cell>0.05904</cell><cell>0.2684</cell></row></table><note><p>Test Case Index ε, u (Ensemble) ε, u (Single) ε, v (Ensemble) ε, v (Single)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table VII :</head><label>VII</label><figDesc>Standard deviation in relative errors of u and v for the 2D cylinder case. Boldface denotes the lowest values.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table VIII :</head><label>VIII</label><figDesc>Computational costs associated with the 2D cylinder case.</figDesc><table><row><cell>Task</cell><cell>Wall Time (s)</cell></row><row><cell>CAE</cell><cell>2933</cell></row><row><cell>Single LSTM</cell><cell>140</cell></row><row><cell cols="2">CFD (2 GPUs) 1305</cell></row><row><cell cols="2">ROM Inference 48.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table X :</head><label>X</label><figDesc>Number of Filters Kernel Size Activation Function Size of Output Convolutional autoencoder architecture for the 2D cylinder case.</figDesc><table><row><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell>384 × 128 × 2</cell></row><row><cell>Convolutional</cell><cell>8</cell><cell>5 × 5</cell><cell>Leaky ReLU</cell><cell>192 × 64 × 8</cell></row><row><cell>Batch Norm</cell><cell></cell><cell></cell><cell></cell><cell>192 × 64 × 8</cell></row><row><cell>Convolutional</cell><cell>16</cell><cell>5 × 5</cell><cell>Leaky ReLU</cell><cell>96 × 32 × 16</cell></row><row><cell>Batch Norm</cell><cell></cell><cell></cell><cell></cell><cell>96 × 32 × 16</cell></row><row><cell>Convolutional</cell><cell>32</cell><cell>3 × 3</cell><cell>Leaky ReLU</cell><cell>96 × 32 × 32</cell></row><row><cell>Pool-Norm</cell><cell></cell><cell>2 × 2</cell><cell></cell><cell>48 × 16 × 32</cell></row><row><cell>Convolutional</cell><cell>64</cell><cell>3 × 3</cell><cell>Leaky ReLU</cell><cell>48 × 16 × 64</cell></row><row><cell>Pool-Norm</cell><cell></cell><cell>2 × 2</cell><cell></cell><cell>24 × 8 × 64</cell></row><row><cell>Reshape</cell><cell></cell><cell></cell><cell></cell><cell>12288</cell></row><row><cell>Fully Connected</cell><cell></cell><cell></cell><cell>Leaky ReLU</cell><cell>128</cell></row><row><cell>Batch Norm</cell><cell></cell><cell></cell><cell></cell><cell>128</cell></row><row><cell>Fully Connected (Latent Space)</cell><cell></cell><cell></cell><cell></cell><cell>10</cell></row><row><cell>Fully Connected</cell><cell></cell><cell></cell><cell>Leaky ReLU</cell><cell>128</cell></row><row><cell>Batch Norm</cell><cell></cell><cell></cell><cell></cell><cell>128</cell></row><row><cell>Fully Connected</cell><cell></cell><cell></cell><cell>Leaky ReLU</cell><cell>12288</cell></row><row><cell>Batch Norm</cell><cell></cell><cell></cell><cell></cell><cell>12288</cell></row><row><cell>Reshape</cell><cell></cell><cell></cell><cell></cell><cell>24 × 8 × 64</cell></row><row><cell>Convolutional Transpose</cell><cell>32</cell><cell>3 × 3</cell><cell>Leaky ReLU</cell><cell>48 × 16 × 32</cell></row><row><cell>Batch Norm</cell><cell></cell><cell></cell><cell></cell><cell>48 × 16 × 32</cell></row><row><cell>Convolutional Transpose</cell><cell>16</cell><cell>3 × 3</cell><cell>Leaky ReLU</cell><cell>96 × 32 × 16</cell></row><row><cell>Batch Norm</cell><cell></cell><cell></cell><cell></cell><cell>96 × 32 × 16</cell></row><row><cell>Convolutional Transpose</cell><cell>8</cell><cell>5 × 5</cell><cell>Leaky ReLU</cell><cell>192 × 64 × 8</cell></row><row><cell>Batch Norm</cell><cell></cell><cell></cell><cell></cell><cell>192 × 64 × 8</cell></row><row><cell>Convolutional Transpose</cell><cell>2</cell><cell>5 × 5</cell><cell>Sigmoid</cell><cell>384 × 128 × 2</cell></row></table></figure>
		</body>
		<back>

			
			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The presented research was conducted during the author's internship at Autodesk Research.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Availability Statement</head><p>The data that support the findings of this study are available from the corresponding author upon reasonable request.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Declarations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest</head><p>The authors have no conflicts to disclose. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head><note type="other">Rakesh</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Reconstruction Errors</head><p>Reconstruction error comparisons between CAE and POD in u and v are given for both test cases. The same test points from the results section are used. Figure <ref type="figure">13</ref> shows the reconstruction errors between CAE (k = 4) and POD at k = 4 and k = 25 for the lid-driven cavity case at µ * = [1.299, 1.689, -0.0367]. For the same latent dimension, the CAE reconstruction errors are significantly smaller than those from POD throughout the domain in both u and v. For k = 25, reconstruction errors from POD are similar to that of CAE. Figure <ref type="figure">12</ref> shows reconstruction errors for the cylinder case at µ * = [51, 142.2]. Again, it is shown that for the same latent dimension, the reconstruction error from a CAE is much smaller than from POD. At k = 50, the error contours are comparable. Using POD would require the LSTM model to handle a very large multivariate time-series problem, which would degrade the accuracy of predictions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Convolutional autoencoder architectures</head><p>The convolutional autoencoder architectures for the lid-driven cavity and 2D cylinder cases are given in Tables IX and X respectively. Both architectures are similar and use convolutional layers in the encoder section to progressively reduce the number of pixels or grow the number of filters in subsequent layers. Convolutional layers are usually followed max-pooling layers that have batch normalization <ref type="bibr" target="#b45">46</ref> applied to them (referred to as pool-norm layers). Batch normalization normalizes the input of each layer over a mini-batch, reducing internal covariate shift, leading to more efficient gradient flow during backpropagation. Fully connected layers are present in both the encoder and decoder. Although their inclusion leads to a larger number of network parameters, reconstruction accuracies are significantly improved. The decoder consists of convolutional transpose layers that are used to progressively increase the number of pixels. The output layer uses a sigmoid activation function to constrain the range of outputs to [0,1]. The leaky ReLU activation function is used for convolutional and fully connected layers, and is given as,</p><p>A value of α = 0.25 is used for all leaky ReLU activation functions. The activation function is often used over the standard ReLU activation function (α = 0). For inputs less than 0, the ReLU activation function returns a gradient of zero, effectively rendering certain neurons inactive and inhibiting gradient flow, referred to as the dying ReLU problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reduced-order modeling: new approaches for computational physics</title>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>David J Lucia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><forename type="middle">A</forename><surname>Beran</surname></persName>
		</author>
		<author>
			<persName><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in aerospace sciences</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="51" to="117" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The proper orthogonal decomposition in the analysis of turbulent flows</title>
		<author>
			<persName><surname>Berkooz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><surname>Lumley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Fluid Mechanics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="539" to="575" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Non-intrusive reduced order modeling of nonlinear problems using neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Hesthaven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ubbiali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="page" from="55" to="78" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders</title>
		<author>
			<persName><forename type="first">Kookjin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Carlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">404</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Non-intrusive reduced-order modeling using convolutional autoencoders</title>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Halder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krzysztof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Fidkowski</surname></persName>
		</author>
		<author>
			<persName><surname>Maki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal for Numerical Methods in Engineering</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="5369" to="5390" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards extraction of orthogonal and parsimonious non-linear modes from turbulent flows</title>
		<author>
			<persName><forename type="first">Hamidreza</forename><surname>Eivazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soledad</forename><surname>Le Clainche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Hoyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Vinuesa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page">117038</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">β-variational autoencoders and transformers for reduced-order modelling of fluid flows</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Solera-Rico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Sanmiguel Vila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdulrahman</forename><surname>Almashjary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Dawson</surname></persName>
		</author>
		<author>
			<persName><surname>Vinuesa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning the solution operator of parametric partial differential equations with physics-informed DeepONets</title>
		<author>
			<persName><forename type="first">Sifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paris</forename><surname>Perdikaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">40</biblScope>
			<biblScope unit="page">8605</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Physics-informed neural operator for learning partial differential equations</title>
		<author>
			<persName><forename type="first">Zongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Kovachki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burigede</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM/JMS Journal of Data Science</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the stability of projection-based model order reduction for convection-dominated laminar and turbulent flows</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Grimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charbel</forename><surname>Farhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Youkilis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">419</biblScope>
			<biblScope unit="page">109681</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adil Rasheed, and Traian Iliescu. Nonintrusive reduced order modeling framework for quasigeostrophic turbulence</title>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Sk M Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Pawar</surname></persName>
		</author>
		<author>
			<persName><surname>San</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">53306</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zican</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<title level="m">A survey of large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural network models for time series forecasts</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><surname>Remus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1082" to="1092" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Data-driven prediction of unsteady flow over a circular cylinder using deep learning</title>
		<author>
			<persName><forename type="first">Sangseung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Fluid Mechanics</title>
		<imprint>
			<biblScope unit="volume">879</biblScope>
			<biblScope unit="page" from="217" to="254" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhancing computational fluid dynamics with machine learning</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Vinuesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Brunton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Computational Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="358" to="366" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weibing Feng, Fangxing Fang, and Christopher Pain. A non-intrusive reduced order model with transformer neural network and its application</title>
		<author>
			<persName><forename type="first">Pin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics of Fluids</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reduced-order modeling of advection-dominated systems with recurrent neural networks and convolutional autoencoders</title>
		<author>
			<persName><forename type="first">Romit</forename><surname>Maulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bethany</forename><surname>Lusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanna</forename><surname>Balaprakash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics of Fluids</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Machine-learning-based reduced-order modeling for unsteady flows around bluff bodies of various shapes</title>
		<author>
			<persName><forename type="first">Kazuto</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Fukami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Murata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koji</forename><surname>Fukagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical and Computational Fluid Dynamics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="367" to="383" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Residual-based physics-informed transfer learning: A hybrid method for accelerating long-term cfd simulations via deep learning</title>
		<author>
			<persName><forename type="first">Joongoo</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhyeong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Vinuesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Joong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Heat and Mass Transfer</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page">124900</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ensemble learning: A survey</title>
		<author>
			<persName><forename type="first">Omer</forename><surname>Sagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Rokach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1249</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reduced-order modeling of vehicle aerodynamics via proper orthogonal decomposition</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Mrosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Othmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Radespiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SAE International Journal of Passenger Cars -Mechanical Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="236" />
			<date type="published" when="2019-10">oct 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-level convolutional autoencoder networks for parametric prediction of spatio-temporal dynamics</title>
		<author>
			<persName><forename type="first">Jiayang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Duraisamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods in Applied Mechanics and Engineering</title>
		<imprint>
			<biblScope unit="volume">372</biblScope>
			<biblScope unit="page">113379</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning Representations by Back-propagating Errors</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A short introduction to boosting</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal-Japanese Society For Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1612</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An efficient algorithm for constructing optimal design of computer experiments</title>
		<author>
			<persName><forename type="first">Ruichen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agus</forename><surname>Sudjianto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="268" to="287" />
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Importance of input data normalization for the application of neural networks to complex industrial problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sevilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Nuclear Science</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1464" to="1468" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A tensorial approach to computational continuum mechanics using object-oriented techniques</title>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Henry G Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrvoje</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christer</forename><surname>Jasak</surname></persName>
		</author>
		<author>
			<persName><surname>Fureby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in physics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="620" to="631" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno>arxiv:1412.6980 Comment</idno>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference for Learning Representations</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2015</date>
		</imprint>
	</monogr>
	<note>Published as a conference paper at the</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Experiments on the flow past a circular cylinder at low reynolds numbers</title>
		<author>
			<persName><surname>David J Tritton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Fluid Mechanics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="547" to="567" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Numerical simulation of a cylinder in uniform flow: application of a virtual boundary method</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eileen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saiki</surname></persName>
		</author>
		<author>
			<persName><surname>Biringen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational physics</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="450" to="465" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">XLB: A differentiable massively parallel lattice Boltzmann library in Python</title>
		<author>
			<persName><forename type="first">Mohammadmehdi</forename><surname>Ataei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hesam</forename><surname>Salehipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Physics Communications</title>
		<imprint>
			<biblScope unit="volume">300</biblScope>
			<biblScope unit="page">109187</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lattice Boltzmann method for fluid flows</title>
		<author>
			<persName><forename type="first">Shiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Gary D Doolen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of fluid mechanics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="329" to="364" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skye</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Flow around circular cylinders</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Gerrard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">fundamentals. by M. M. Zdravkovich. oxford science publications</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="375" to="378" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
	<note>Journal of Fluid Mechanics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Numerical simulation of laminar flow past a circular cylinder</title>
		<author>
			<persName><surname>Bn Rajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sekhar</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName><surname>Majumdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematical Modelling</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1228" to="1247" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuldip K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
