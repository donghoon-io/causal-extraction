<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bayesian inference for logistic models using Pólya-Gamma latent variables</title>
				<funder ref="#_5dxG3Ef">
					<orgName type="full">U.S. National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2013-07-22">22 Jul 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicholas</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Chicago</orgName>
								<orgName type="institution" key="instit2">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">G</forename><surname>Scott</surname></persName>
							<email>james.scott@mccombs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Chicago</orgName>
								<orgName type="institution" key="instit2">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jesse</forename><surname>Windle</surname></persName>
							<email>jwindle@ices.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Chicago</orgName>
								<orgName type="institution" key="instit2">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bayesian inference for logistic models using Pólya-Gamma latent variables</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-07-22">22 Jul 2013</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1205.0310v3[stat.ME]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new data-augmentation strategy for fully Bayesian inference in models with binomial likelihoods. The approach appeals to a new class of Pólya-Gamma distributions, which are constructed in detail. A variety of examples are presented to show the versatility of the method, including logistic regression, negative binomial regression, nonlinear mixed-effects models, and spatial models for count data. In each case, our data-augmentation strategy leads to simple, effective methods for posterior inference that: (1) circumvent the need for analytic approximations, numerical integration, or Metropolis-Hastings; and (2) outperform other known data-augmentation strategies, both in ease of use and in computational efficiency. All methods, including an efficient sampler for the Pólya-Gamma distribution, are implemented in the R package BayesLogit.</p><p>In the technical supplement appended to the end of the paper, we provide further details regarding the generation of Pólya-Gamma random variables; the empirical benchmarks reported in the main manuscript; and the extension of the basic dataaugmentation framework to contingency tables and multinomial outcomes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Bayesian inference for the logistic regression model has long been recognized as a hard problem, due to the analytically inconvenient form of the model's likelihood function. By comparison, Bayesian inference for the probit model is much easier, owing to the simple latent-variable method of <ref type="bibr" target="#b0">Albert and Chib (1993)</ref> for posterior sampling.</p><p>In the two decades since the work of <ref type="bibr" target="#b0">Albert and Chib (1993)</ref> on the probit model, there have been many attempts to apply the same missing-data strategy to the logit model (e.g. <ref type="bibr" target="#b26">Holmes and Held, 2006;</ref><ref type="bibr" target="#b15">Frühwirth-Schnatter and Frühwirth, 2010;</ref><ref type="bibr" target="#b25">Gramacy and Polson, 2012)</ref>. The results have been mixed. Certainly many of these approaches have been used successfully in applied work. Yet they all involve data-augmentation algorithms that are either approximate, or are significantly more complicated than the Albert/Chib method, as they involve multiple layers of latent variables. Perhaps as a result, the Bayesian treatment of the logit model has not seen widespread adoption by non-statisticians in the way that, for example, the Bayesian probit model is used extensively in both political science and market research (e.g. <ref type="bibr" target="#b36">Rossi et al., 2005;</ref><ref type="bibr" target="#b27">Jackman, 2009)</ref>. The lack of a standard computational approach also makes it more difficult to use the logit link in the kind of complex hierarchical models that have become routine in Bayesian statistics.</p><p>In this paper, we present a new data-augmentation algorithm for Bayesian logistic regression. Although our method involves a different missing-data mechanism from that of <ref type="bibr" target="#b0">Albert and Chib (1993)</ref>, it is nonetheless a direct analogue of their construction, in that it is both exact and simple. Moreover, because our method works for any binomial likelihood parametrized by log odds, it leads to an equally painless Bayesian treatment of the negative-binomial model for overdispersed count data.</p><p>This approach appeals to a new family of Pólya-Gamma distributions, described briefly here and constructed in detail in Section 2. Definition 1. A random variable X has a Pólya-Gamma distribution with parameters b &gt; 0 and c ∈ R, denoted X ∼ PG(b, c), if</p><formula xml:id="formula_0">X D = 1 2π 2 ∞ k=1 g k (k -1/2) 2 + c 2 /(4π 2 ) ,<label>(1)</label></formula><p>where the g k ∼ Ga(b, 1) are independent gamma random variables, and where D = indicates equality in distribution.</p><p>Our main result (Theorem 1, below) is that binomial likelihoods parametrized by logodds can be represented as mixtures of Gaussians with respect to a Pólya-Gamma distribution. The fundamental integral identity at the heart of our approach is that, for b &gt; 0, (e ψ ) a (1 + e ψ ) b = 2 -b e κψ ∞ 0 e -ωψ 2 /2 p(ω) dω ,</p><p>where κ = ab/2 and ω ∼ PG(b, 0). When ψ = x T β is a linear function of predictors, the integrand is the kernel of a Gaussian likelihood in β. Moreover, as we will show below, the implied conditional distribution for ω, given ψ, is also a Pólya-Gamma distribution. This suggests a simple strategy for Gibbs sampling across a wide class of binomial models: Gaussian draws for the main parameters, and Pólya-Gamma draws for a single layer of latent variables. The success of this strategy depends upon the existence of a simple, effective way to simulate Pólya-Gamma random variables. The sum-of-gammas representation in Formula (1) initially seems daunting, and suggests only a naïve finite approximation. But we describe a fast, exact Pólya-Gamma simulation method that avoids the difficulties that can result from truncating an infinite sum. The method, which is implemented in the R package 1. In simple logit models with abundant data and no hierarchical structure, the Pólya-Gamma method is a close second to the independence Metropolis-Hastings (MH) sampler, as long as the MH proposal distribution is chosen carefully.</p><p>2. In virtually all other cases, the Pólya-Gamma method is most efficient.</p><p>The one exception we have encountered to the second claim is the case of a negative-binomial regression model with many counts per observation, and with no hierarchical structure in the prior. Here, the effective sample size of the Pólya-Gamma method remains the best, but its effective sampling rate suffers. As we describe below, this happens because our present method for sampling PG(n, c) is to sum n independent draws from PG(1, c); with large counts, this becomes a bottleneck. In such cases, the method of <ref type="bibr" target="#b16">Frühwirth-Schnatter et al. (2009)</ref> provides a fast approximation, at the cost of introducing a more complex latent-variable structure. This caveat notwithstanding, the Pólya-Gamma scheme offers real advantages, both in speed and simplicity, across a wide variety of structured Bayesian models for binary and count data. In general, the more complex the model, and the more time that one must spend sampling its main parameters, the larger will be the efficiency advantage of the new method. The difference is especially large for the Gaussian-process spatial models we consider below, which require expensive matrix operations. We have also made progress in improving the speed of the Pólya-Gamma sampler for large shape parameters, beyond the method described in Section 4. These modifications lead to better performance in negativebinomial models with large counts. They are detailed in <ref type="bibr">Windle et al. (2013b)</ref>, and have been incorporated into the latest version of our R package <ref type="bibr">(Windle et al., 2013a)</ref>.</p><p>Furthermore, in a recent paper based on an early technical report of our method, <ref type="bibr" target="#b8">Choi and Hobert (2013)</ref> have proven that the Pólya-Gamma Gibbs sampler for Bayesian logistic regression is uniformly ergodic. This result has important practical consequences; most notably, it guarantees the existence of a central limit theorem for Monte Carlo averages of posterior draws. We are aware of no similar result for any other MCMC-based approach to the Bayesian logit model. Together with the numerical evidence we present here, this provides a strong reason to favor the routine use of the Pólya-Gamma method.</p><p>The paper proceeds as follows. The Pólya-Gamma distribution is constructed in Section 2, and used to derive a data-augmentation scheme for binomial likelihoods in Section 3. Section 4 describes a method for simulating from the Pólya-Gamma distribution, which we have implemented as a stand-alone sampler in the BayesLogit R package. Section 5 presents the results of an extensive benchmarking study comparing the efficiency of our method to other data-augmentation schemes. Section 6 concludes with a discussion of some open issues related to our proposal. Many further details of the sampling algorithm and our empirical study of its efficiency are deferred to a technical supplement.</p><p>2 The Pólya-Gamma distribution 2.1 The case PG(b, 0)</p><p>The key step in our approach is the construction of the Pólya-Gamma distribution. We now describe this new family, deferring our method for simulating PG random variates to Section 4.</p><p>The Pólya-Gamma family of distributions, denoted PG(b, c), is a subset of the class of infinite convolutions of gamma distributions. We first focus on the PG(1, 0) case, which is a carefully chosen element of the class of infinite convolutions of exponentials, also know as Pólya distributions <ref type="bibr" target="#b2">(Barndorff-Nielsen et al., 1982)</ref>. The PG(1, 0) distribution has Laplace transform E{exp(-ωt)} = cosh -1 ( t/2). Using this as a starting point, one may define the random variable ω ∼ P G(b, 0), b &gt; 0, as the infinite convolution of gamma distributions (hence the name Pólya-Gamma) that has Laplace transform</p><formula xml:id="formula_2">E{exp(-ωt)} = t i=1 1 + t 2π 2 (k -1/2) 2 -b = 1 cosh b ( t/2) .<label>(3)</label></formula><p>The last equality is a consequence of the Weierstrass factorization theorem. By inverting the Laplace transform, one finds that if ω ∼ PG(b, 0), then it is equal in distribution to an infinite sum of gammas:</p><formula xml:id="formula_3">ω D = 1 2π 2 ∞ k=1 g k (k -1/2) 2 ,</formula><p>where the g k ∼ Ga(b, 1) are mutually independent.</p><p>The PG(b, 0) class of distributions is closely related to a subset of distributions that are surveyed by <ref type="bibr" target="#b4">Biane et al. (2001)</ref>. This family of distributions, which we denote by J * (b), b &gt; 0, has close connections with the Jacobi Theta and Riemann Zeta functions, and with Brownian excursions. Its Laplace transform is</p><formula xml:id="formula_4">E{e -tJ * (b) } = cosh -b ( √ 2t) ,<label>(4)</label></formula><p>implying that PG(b, 0)</p><formula xml:id="formula_5">D = J * (b)/4.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The general PG(b, c) class</head><p>The general PG(b, c) class arises through an exponential tilting of the PG(b, 0) density, much in the same way that a Gaussian likelihood combines with a Gamma prior for a precision. Specifically, a PG(b, c) random variable has the probability density function</p><formula xml:id="formula_6">p(ω | b, c) = exp -c 2 2 ω p(ω | b, 0) E ω exp(-c 2 2 ω) ,<label>(5)</label></formula><p>where p(ω | b, 0) is the density of a PG(b, 0) random variable. The expectation in the denominator is taken with respect to the PG(b, 0) distribution; it is thus cosh -b (c/2) by ( <ref type="formula" target="#formula_2">3</ref>), ensuring that p(ω | b, c) is a valid density.</p><p>The Laplace transform of a PG(b, c) distribution may be calculated by appealing to the Weierstrass factorization theorem again:</p><formula xml:id="formula_7">E ω {exp (-ωt)} = cosh b c 2 cosh b c 2 /2+t 2 (6) = ∞ k=1   1 + c 2 /2 2(k-1/2) 2 π 2 1 + c 2 /2+t 2(k-1/2) 2 π 2   b = ∞ k=1 (1 + d -1 k t) -b , where d k = 2 k - 1 2 2 π 2 + c 2 /2 .</formula><p>Each term in the product is recognizable as the Laplace transform of a gamma distribution. We can therefore write a PG(b, c) as an infinite convolution of gamma distributions,</p><formula xml:id="formula_8">ω D = ∞ k=1 Ga(b, 1) d k = 1 2π 2 ∞ k=1 Ga(b, 1) (k -1 2 ) 2 + c 2 /(4π 2 )</formula><p>, which is the form given in Definition 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Further properties</head><p>The density of a Pólya-Gamma random variable can be expressed as an alternating-sign sum of inverse-Gaussian densities. This fact plays a crucial role in our method for simulating Pólya-Gamma draws. From the characterization of J * (b) density given by <ref type="bibr" target="#b4">Biane et al. (2001)</ref>, we know that the P G(b, 0) distribution has density</p><formula xml:id="formula_9">f (x | b, 0) = 2 b-1 Γ(b) ∞ n=0 (-1) n Γ(n + b) Γ(n + 1) (2n + b) √ 2πx 3 e -(2n+b) 2 8x</formula><p>.</p><p>The density of P G(b, z) distribution is then computed by an exponential tilt and a renormalization:</p><formula xml:id="formula_10">f (x | b, c) = {cosh b (c/2)} 2 b-1 Γ(b) ∞ n=0 (-1) n Γ(n + b) Γ(n + 1) (2n + b) √ 2πx 3 e -(2n+b) 2 8x -c 2 2 x .</formula><p>Notice that the normalizing constant is known directly from the Laplace transform of a PG(b, 0) random variable.</p><p>A further useful fact is that all finite moments of a Pólya-Gamma random variable are available in closed form. In particular, the expectation may be calculated directly. This allows the Pólya-Gamma scheme to be used in EM algorithms, where the latent ω's will form a set of complete-data sufficient statistics for the main parameter. We arrive at this result by appealing to the Laplace transform of ω ∼ PG(b, c). Differentiating (6) with respect to t, negating, and evaluating at zero yields</p><formula xml:id="formula_11">E(ω) = b 2c tanh(c/2) = b 2c e c -1 1 + e c .</formula><p>Lastly, the Pólya-Gamma class is closed under convolution for random variates with the same scale (tilting) parameter. If</p><formula xml:id="formula_12">ω 1 ∼ PG(b 1 , z) and ω 2 ∼ PG(b 2 , z) are independent, then ω 1 + ω 2 ∼ P G(b 1 + b 2 , z</formula><p>). This follows from the Laplace transform. We will employ this property later when constructing a Pólya-Gamma sampler.</p><p>3 The data-augmentation strategy</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Main result</head><p>The Pólya-Gamma family has been carefully constructed to yield a simple Gibbs sampler for the Bayesian logistic-regression model. The two differences from the <ref type="bibr" target="#b0">Albert and Chib (1993)</ref> method for probit regression are that the posterior distribution is a scale mixture, rather than location mixture, of Gaussians; and that Albert and Chib's truncated normals are replaced by Pólya-Gamma latent variables.</p><p>To fix notation: let y i be the number of successes, n i the number of trials, and x i = (x i1 , . . . , x ip ) the vector of regressors for observation i ∈ {1, . . . , N }. Let y i ∼ Binom(n i , 1/{1+ e -ψ i }), where ψ i = x T i β are the log odds of success. Finally, let β have a Gaussian prior, β ∼ N(b, B). To sample from the posterior distribution using the Pólya-Gamma method, simply iterate two steps:</p><formula xml:id="formula_13">(ω i | β) ∼ PG(n i , x T i β) (β | y, ω) ∼ N(m ω , V ω ) , where V ω = (X T ΩX + B -1 ) -1 m ω = V ω (X T κ + B -1 b) ,</formula><p>where κ = (y 1 -n 1 /2, . . . , y N -n N /2), and Ω is the diagonal matrix of ω i 's.</p><p>We now derive this sampler, beginning with a careful statement and proof of the integral identity mentioned in the introduction.</p><p>Theorem 1. Let p(ω) denote the density of the random variable ω ∼ PG(b, 0), b &gt; 0. Then the following integral identity holds for all a ∈ R:</p><formula xml:id="formula_14">(e ψ ) a (1 + e ψ ) b = 2 -b e κψ ∞ 0 e -ωψ 2 /2 p(ω) dω ,<label>(7)</label></formula><p>where κ = ab/2. Moreover, the conditional distribution</p><formula xml:id="formula_15">p(ω | ψ) = e -ωψ 2 /2 p(ω) ∞ 0 e -ωψ 2 /2 p(ω) dω ,</formula><p>which arises in treating the integrand in ( <ref type="formula" target="#formula_14">7</ref>) as an unnormalized joint density in (ψ, ω), is also in the Pólya-Gamma class: (ω | ψ) ∼ PG(b, ψ).</p><p>Proof. Appealing to (3), we may write the lefthand side of ( <ref type="formula" target="#formula_14">7</ref>) as</p><formula xml:id="formula_16">(e ψ ) a (1 + e ψ ) b = 2 -b exp{κψ} cosh b (ψ/2) = 2 -b e κψ E ω {exp(-ωψ 2 /2} ,</formula><p>where the expectation is taken with respect to ω ∼ PG(b, 0), and where κ = ab/2.</p><p>Turn now to the conditional distribution</p><formula xml:id="formula_17">p(ω | ψ) = e -ωψ 2 /2 p(ω) ∞ 0 e -ωψ 2 /2 p(ω) dω</formula><p>, where p(ω) is the density of the prior, PG(b, 0). This is of the same form as (5), with ψ = c. Therefore (ω | ψ) ∼ PG(b, ψ).</p><p>To derive our Gibbs sampler, we appeal to Theorem 1 and write the likelihood contribution of observation i as</p><formula xml:id="formula_18">L i (β) = {exp(x T i β)} y i 1 + exp(x T i β) ∝ exp(κ i x T i β) ∞ 0 exp{-ω i (x T i β) 2 /2} p(ω i | n i , 0) ,</formula><p>where κ i = y i -n i /2, and where p(ω i | n i , 0) is the density of a Pólya-Gamma random variable with parameters (n i , 0). Combining the terms from all n data points gives the following expression for the con-ditional posterior of β, given ω = (ω 1 , . . . , ω N ):</p><formula xml:id="formula_19">p(β | ω, y) ∝ p(β) N i=1 L i (β | ω i ) = p(β) N i=1 exp κ i x T i β -ω i (x T i β) 2 /2 ∝ p(β) N i=1 exp ω i 2 (x T i β -κ i /ω i ) 2 ∝ p(β) exp - 1 2 (z -Xβ) T Ω(z -Xβ) ,</formula><p>where z = (κ 1 /ω 1 , . . . , κ n /ω N ), and where Ω = diag(ω 1 , . . . , ω N ). This is a conditionally Gaussian likelihood in β, with working responses z, design matrix X, and diagonal covariance matrix Ω -1 . Since the prior p(β) is Gaussian, a simple linear-model calculation leads to the Gibbs sampler defined above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Existing data-augmentation schemes</head><p>A comparison with the methods of <ref type="bibr" target="#b26">Holmes and Held (2006)</ref> and Frühwirth-Schnatter and Frühwirth (2010) clarifies how the Pólya-Gamma method differs from previous attempts at data augmentation. Both of these methods attempt to replicate the missing-data mechanism of <ref type="bibr" target="#b0">Albert and Chib (1993)</ref>, where the outcomes y i are assumed to be thresholded versions of an underlying continuous quantity z i . For simplicity, we assume that n i = 1 for all observations, and that y i is either 0 or 1. Let</p><formula xml:id="formula_20">y i = 1 , z i ≥ 0 0 , z i &lt; 0 z i = x T i β + i , i ∼ Lo(1) ,<label>(8)</label></formula><p>where i ∼ Lo(1) has a standard logistic distribution. Upon marginalizing over the z i , often called the latent utilities, the original binomial likelihood is recovered. Although (8) would initially seem to be a direct parallel with <ref type="bibr" target="#b0">Albert and Chib (1993)</ref>, it does not lead to an easy method for sampling from the posterior distribution of β. This creates additional complications compared to the probit case. The standard approach has been to add another layer of auxiliary variables to handle the logistic error model on the latent-utility scale. One strategy is to represent the logistic distribution as a normal-scale mixture <ref type="bibr" target="#b26">(Holmes and Held, 2006)</ref>:</p><formula xml:id="formula_21">( i | φ i ) ∼ N(0, φ i ) φ i = (2λ 2 i ) , λ i ∼ KS(1) ,</formula><p>where λ i has a Kolmogorov-Smirnov distribution <ref type="bibr" target="#b1">(Andrews and Mallows, 1974)</ref>. Alternatively, one may approximate the logistic error term as a discrete mixture of normals</p><formula xml:id="formula_22">β y i i φ i i = 1, . . . , n β y i i = 1, . . . , n ω i Figure 1:</formula><p>Directed acyclic graphs depicting two latent-variable constructions for the logistic-regression model: the difference of random-utility model of <ref type="bibr" target="#b26">Holmes and Held (2006)</ref> and Frühwirth-Schnatter and Frühwirth (2010), on the left; versus our direct dataaugmentation scheme, on the right.</p><p>(Frühwirth-Schnatter and Frühwirth, 2010):</p><formula xml:id="formula_23">( i | φ i ) ∼ N(0, φ i ) φ i ∼ K k=1 w k δ φ (k) ,</formula><p>where δ φ indicates a Dirac measure at φ. The weights w k and the points φ (k) in the discrete mixture are fixed for a given choice of K so that the Kullback-Leibler divergence from the true distribution of the random utilities is minimized. Frühwirth-Schnatter and Frühwirth (2010) find that the choice of K = 10 leads to a good approximation, and list the optimal weights and variances for this choice.</p><p>In both cases, posterior sampling can be done in two blocks, sampling the complete conditional of β in one block and sampling the joint complete conditional of both layers of auxiliary variables in the second block. The discrete mixture of normals is an approximation, but it outperforms the scale mixture of normals in terms of effective sampling rate, as it is much faster.</p><p>One may also arrive at the hierarchy above by manipulating the random utility-derivation of <ref type="bibr" target="#b30">McFadden (1974)</ref>; this involves the difference of random utilities, or "dRUM," using the term of <ref type="bibr" target="#b15">Frühwirth-Schnatter and Frühwirth (2010)</ref>. The dRUM representation is superior to the random utility approach explored in <ref type="bibr" target="#b14">Frühwirth-Schnatter and Frühwirth (2007)</ref>. Further work by <ref type="bibr" target="#b19">Fussl et al. (2013)</ref> improves the approach for binomial logistic models. In this extension, one must use a table of different weights and variances representing different normal mixtures, to approximate a finite collection of type-III logistic distributions, and interpolate within this table to approximate the entire family.</p><p>Both <ref type="bibr" target="#b0">Albert and</ref><ref type="bibr" target="#b0">Chib (1993) and</ref><ref type="bibr" target="#b31">O'Brien and</ref><ref type="bibr" target="#b31">Dunson (2004)</ref> suggest another approximation: namely, the use of a Student-t link function as a close substitute for the logistic link. But this also introduces a second layer of latent variables, in that the Student-t error model for z i is represented as a scale mixture of normals.</p><p>Our data-augmentation scheme differs from each of these approaches in several ways. First, it does not appeal directly to the random-utility interpretation of the logit model. Instead, it represents the logistic CDF as a mixture with respect to an infinite convolution of gammas. Second, the method is exact, in the sense of making draws from the correct joint posterior distribution, rather than an approximation to the posterior that arises out of an approximation to the link function. Third, like the <ref type="bibr" target="#b0">Albert and Chib (1993)</ref> method, it requires only a single layer of latent variables.</p><p>A similar approach to ours is that of <ref type="bibr" target="#b25">Gramacy and Polson (2012)</ref>, who propose a latentvariable representation of a powered-up version of the logit likelihood (c.f. <ref type="bibr" target="#b34">Polson and Scott, 2013)</ref>. This representation is useful for obtaining classical penalized-likelihood estimates via simulation, but for the ordinary logit model it leads to an improper mixing distribution for the latent variable. This requires modifications of the basic approach that make simulation difficult in the general logit case. As our experiments show, the method does not seem to be competitive on speed grounds with the Pólya-Gamma representation, which results in a proper mixing distribution for all common choices of a i , b i in (2).</p><p>For negative-binomial regression, Frühwirth-Schnatter et al. ( <ref type="formula">2009</ref>) employ the discretemixture/table-interpolation approach, like that used by <ref type="bibr" target="#b19">Fussl et al. (2013)</ref>, to produce a tractable data augmentation scheme. In some instances, the Pólya-Gamma approach outperforms this method; in others, it does not. The reasons for this discrepancy can be explained by examining the inner workings of our Pólya-Gamma sampler, discussed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mixed model example</head><p>We have introduced the Pólya-Gamma method in the context of a binary logit model. We do this with the understanding that, when data are abundant, the Metropolis-Hastings algorithm with independent proposals will be efficient, as asymptotic theory suggests that a normal approximation to the posterior distribution will become very accurate as data accumulate. This is well understood among Bayesian practitioners (e.g. <ref type="bibr" target="#b7">Carlin, 1992;</ref><ref type="bibr" target="#b22">Gelman et al., 2004)</ref>.</p><p>But the real advantage of data augmentation, and the Pólya-Gamma technique in particular, is that it becomes easy to construct and fit more complicated models. For instance, the Pólya-Gamma method trivially accommodates mixed models, factor models, and models with a spatial or dynamic structure. For most problems in this class, good Metropolis-Hastings samplers are difficult to design, and at the very least will require ad-hoc tuning to yield good performance.</p><p>Several relevant examples are considered in Section 5. But as an initial illustration of the point, we fit a binomial logistic mixed model using the data on contraceptive use among Bangladeshi women provided by the R package mlmRev <ref type="bibr" target="#b3">(Bates et al., 2011)</ref>. The data comes from a Bangladeshi survey whose predictors include a woman's age, the number of children at the time of the survey, whether the woman lives in an urban or rural area, and a more specific geographic identifier based upon the district in which the woman resides. Some districts have few observations and district 54 has no observations; thus, a mixed model is necessary if one wants to include this effect. The response identifies contraceptive use. We</p><formula xml:id="formula_24">District Random Effect -1.5 -1.0 -0.5 0.0 0.5 1.0 D34 D14 D16 D56 D46 D43 D39 D48 D30 D37 D41 D50 D31 D3 D58 D42 D35 D47 D20 D25 D4 D52 D40 D13 D19 D8 D5 D51 D26 D21 D2 D15 D18 D33 D12 D36 D29 D53 D17 D45 D7 D23 D38 D6 D49 D9 D44 D28 D55 D22 D60 D32 D10 D57 D59 D27 D24 D61 D1 D11</formula><p>Figure <ref type="figure">2</ref>: Marginal posterior distribution of random intercepts for each district found in a Bangladeshi contraception survey. For 10,000 samples after 2,000 burn-in, median ESS=8168 and median ESR=59.88 for the PG method. Grey/white bars: 90%/50% posterior credible intervals. Black dots: posterior means.</p><p>fit the mixed model</p><formula xml:id="formula_25">y ij ∼ Binom(1, p ij ) , p ij = e ψ ij 1 + e ψ ij , ψ ij = m + δ j + x ij β, δ j ∼ N (0, 1/φ), m ∼ N (0, κ 2 /φ),</formula><p>where i and j correspond to the ith observation from the jth district. The fixed effect β is given a N (0, 100I) prior while the precision parameter φ is given Ga(1, 1) prior. We take κ → ∞ to recover an improper prior for the global intercept m. Figure <ref type="figure">2</ref> shows the box plots of the posterior draws of the random intercepts m + δ j . If one does not shrink these random intercepts to a global mean using a mixed model, then several take on unrealistic values due to the unbalanced design.</p><p>We emphasize that there are many ways to model this data, and that we do not intend our analysis to be taken as definitive. It is merely a proof of concept, showing how various aspects of Bayesian hierarchical modeling-in this case, models with both fixed and random effects-can be combined routinely with binomial likelihoods using the Pólya-Gamma scheme. Together these changes require just a few lines of code and a few extra seconds of runtime compared to the non-hierarchical logit model. A posterior draw of 2,000 samples for this data set takes 26.1 seconds for a binomial logistic regression, versus 27.3 seconds for a binomial logistic mixed model. As seen in the negative binomial examples below, one may also painlessly incorporate a more complex prior structure using the Pólya-Gamma technique. For instance, if given information about the geographic location of each district, one could place spatial process prior upon the random offsets {δ j }.</p><p>4 Simulating Pólya-Gamma random variables 4.1 The PG(1,z) sampler All our developments thus far require an efficient method for sampling Pólya-Gamma random variates. In this section, we derive such a method, which is implemented in the R package BayesLogit. We focus chiefly on simulating PG(1,z) efficiently, as this is most relevant to the binary logit model.</p><p>First, observe that one may sample Pólya-Gamma random variables naïvely (and approximately) using the sum-of-gammas representation in Equation ( <ref type="formula" target="#formula_0">1</ref>). But this is slow, and involves the potentially dangerous step of truncating an infinite sum.</p><p>We therefore construct an alternate, exact method by extending the approach of Devroye (2009) for simulating J * (1) from ( <ref type="formula" target="#formula_4">4</ref>). The distribution J * (1) is related to the Jacobi theta function, so we call J * (1) the Jacobi distribution. One may define an exponentially tilted Jacobi distribution J * (1, z) via the density</p><formula xml:id="formula_26">f (x | z) = cosh(z) e -xz 2 /2 f (x) ,<label>(9)</label></formula><p>where f (x) is the density of J * (1). The P G(1, z) distribution is related to J * (1, z) through the rescaling</p><formula xml:id="formula_27">P G(1, z) = 1 4 J * (1, z/2).<label>(10)</label></formula><p>Devroye ( <ref type="formula">2009</ref>) develops an efficient J * (1, 0) sampler. Following this work, we develop an efficient sampler for an exponentially tilted J * random variate, J * (1, z). In both cases, the density of interest can be written as an infinite, alternating sum that is amenable to the series method described in Chapter IV.5 of <ref type="bibr" target="#b12">Devroye (1986)</ref>. Recall that a random variable with density f may be sampled by the accept/reject algorithm by: (1) proposing X from a density g; (2) drawing U ∼ U(0, cg(X)) where f /g ∞ ≤ c; and (3) accepting X if U ≤ f (X) and rejecting X otherwise. When f (x) = ∞ n=0 (-1) n a n (x) and the coefficients a n (x) are decreasing for all n ∈ N 0 , for fixed x in the support of f , then the partial sums,</p><formula xml:id="formula_28">S n (x) = n i=0 (-1) i a i (x), satisfy S 0 (x) &gt; S 2 (x) &gt; • • • &gt; f (x) &gt; • • • &gt; S 3 (x) &gt; S 1 (x). (<label>11</label></formula><formula xml:id="formula_29">)</formula><p>In that case, step (3) above is equivalent to accepting X if U ≤ S i (X) for some odd i, and rejecting X if U &gt; S i (X) for some even i. Moreover, the partial sums S i (X) can be calculated iteratively. Below we show that for the J * (1, z) distribution the algorithm will accept with high probability upon checking U ≤ S 1 (X).</p><p>The Jacobi density has two alternating-sum representations, ∞ n=0 (-1) n a L n (x) and ∞ n=0 (-1) n a R i (x), neither of which satisfy (11) for all x in the support of f . However, each satisfies (11) on an interval. These two intervals, respectively denoted I L and I R , satisfy</p><formula xml:id="formula_30">I L ∪ I R = (0, ∞)</formula><p>and I L ∩ I R = ∅. Thus, one may pick t &gt; 0 and define the piecewise coefficients</p><formula xml:id="formula_31">a n (x) =          π(n + 1/2) 2 πx 3/2 exp - 2(n + 1/2) 2 x , 0 &lt; x ≤ t,<label>(12)</label></formula><formula xml:id="formula_32">π(n + 1/2) exp - (n + 1/2) 2 π 2 2 x , x &gt; t,<label>(13)</label></formula><p>so that f (x) = ∞ n=0 (-1) n a n (x) satisfies the partial sum criterion (11) for x &gt; 0. Devroye shows that the best choice of t is near 0.64.</p><p>Employing (9), we now see that the J * (1, z) density can be written as an infinite, alternating sum</p><formula xml:id="formula_33">f (x|z) = ∞ n=0 (-1) n a n (x|z)</formula><p>, where</p><formula xml:id="formula_34">a n (x|z) = cosh(z) exp - z 2 x 2 a n (x) .</formula><p>This satisfies (11), as a n+1 (x|z)/a n (x|z) = a n+1 (x)/a n (x). Since a 0 (x|z) ≥ f (x|z), the first term of the series provides a natural proposal:</p><formula xml:id="formula_35">c(z) g(x|z) = π 2 cosh(z)          2 πx 3/2 exp - z 2 x 2 - 1 2x , 0 &lt; x ≤ t, exp - z 2 2 + π 2 8 x , x &gt; t.<label>(14)</label></formula><p>Examining these two kernels, one finds that X ∼ g(x|z) may be sampled from a mixture of an inverse-Gaussian and an exponential:</p><formula xml:id="formula_36">X ∼ IG(|z| -1 , 1)I (0,t] with prob. p/(p + q)</formula><p>Ex(-z 2 /2 + π 2 /8)I (t,∞) with prob. q/(p + q)</p><p>where p(z) = t 0 c(z) g(x|z)dx and q(z) = ∞ t c(z) g(x|z)dx. Note that we are implicitly suppressing the dependence of p, q, c, and g upon t.</p><p>With this proposal in hand, sampling J * (1, z) proceeds as follows:</p><p>1. Generate a proposal X ∼ g(x|z).</p><p>2. Generate U ∼ U(0, c(z)g(X|z)).</p><p>3. Iteratively calculate S n (X|z), starting at S 1 (X|z), until U ≤ S n (X|z) for an odd n or until U &gt; S n (X|z) for an even n.</p><p>4. Accept X if n is odd; return to step 1 if n is even.</p><p>To sample Y ∼ P G(1, z), draw X ∼ J * (1, z/2) and then let Y = X/4. The details of the implementation, along with pseudocode, can be found in the technical supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis of acceptance rate</head><p>This J * (1, z) sampler is very efficient. The parameter c = c(z, t) found in ( <ref type="formula" target="#formula_35">14</ref>) characterizes the average number of proposals we expect to make before accepting. Devroye shows that in the case of z = 0, one can pick t so that c(0, t) is near unity. We extend this result to non-zero tilting parameters and calculate that, on average, the J * (1, z) sampler rejects no more than 9 out of every 10,000 draws, regardless of z.</p><p>Proposition 2. Define</p><formula xml:id="formula_37">p(z, t) = t 0 π 2 cosh(z) exp - z 2 x 2 a L 0 (x)dx, q(z, t) = ∞ t π 2 cosh(z) exp - z 2 x 2 a R 0 (x)dx .</formula><p>The following facts about the Pólya-Gamma rejection sampler hold.</p><p>1. The best truncation point t * is independent of z ≥ 0.</p><p>2. For a fixed truncation point t, p(z, t) and q(z, t) are continuous, p(z, t) decreases to zero as z diverges, and q(z, t) converges to 1 as z diverges. Thus c(z, t) = p(z, t)+q(z, t) is continuous and converges to 1 as z diverges.</p><p>3. For fixed t, the average probability of accepting a draw, 1/c(z, t), is bounded below for all z. For t * , this bound to five digits is 0.99919, which is attained at z 1.378.</p><p>Proof. We consider each point in turn. Throughout, t is assumed to be in the interval of valid truncation points, I L ∩ I R .</p><p>1. We need to show that for fixed z, c(z, t) = p(z, t) + q(z, t) has a maximum in t that is independent of z. For fixed z ≥ 0, p(z, t) and q(z, t) are both differentiable in t. Thus any extrema of c will occur on the boundary of the interval I L ∩ I R , or at the critical points for which ∂c ∂t = 0; that is,</p><formula xml:id="formula_38">t ∈ I L ∩ I R , for which cosh(z) exp - z 2 2 t [a L 0 (t) -a R 0 (t)] = 0.</formula><p>The exponential term is never zero, so an interior critical point must satisfy a L 0 (t)a R 0 (t) = 0, which is independent of z. Devroye shows there is one such critical point, t * 0.64, and that it corresponds to a maximum. 2. Both p and q are integrals of recognizable kernels. Rewriting the expressions in terms of the corresponding densities and integrating yields</p><formula xml:id="formula_39">p(z, t) = cosh(z) π 2 1 y(z) exp -y(z)t , y(z) = z 2 2 + π 2 8 , and q(z, t) = (1 + e -2z )Φ IG (t|1/z, 1) ,</formula><p>where Φ IG is the cumulative distribution function of an IG(1/z, 1) distribution.</p><p>One can see that p(z, t) is eventually decreasing in z for fixed t by noting that the sign of ∂p ∂z is determined by</p><formula xml:id="formula_40">tanh(z) - z z 2 2 + π 2 8 -zt ,</formula><p>which is eventually negative. (In fact, for the t * calculated above it appears to be negative for all z ≥ 0, which we do not prove here.) Further, p(z, t) is continuous in z and converges to 0 as z diverges.</p><p>To see that q(z, t) converges to 1, consider a Brownian motion (W s ) defined on the probability space (Ω, F, P) and the subsequent Brownian motion with drift X z s = zs + W s . The stopping time T z = inf{s &gt; 0|X z s ≥ 1} is distributed as IG(1/z, 1) and P(T z &lt; t) = P(max s∈[0,t] X z s ≥ 1) . Hence P(T z &lt; t) is increasing and lim z→∞ P(T z &lt; t) = 1, ensuring that q(z, t) ∝ (1+e -2z )P(T z &lt; t) converges to 1 as z → ∞ as well. Continuity follows by considering the cumulative distribution</p><formula xml:id="formula_41">P(T z &lt; t) = Φ{(zt -1)/ √ t} -exp(2zt)Φ{(-1 -zt)/ √ t}, which is a composition of continuous functions in z.</formula><p>By the continuity and tail behavior of p and q, it follows that c(z, t) = p(z, t) + q(z, t), for fixed t, is continuous for all z and converges to 1 as z diverges. Further c(z, t) ≥ 1 since the target density and proposal density satisfy f (x|z) ≤ c(z, t)g(x|z) for all x ≥ 0. Thus, c takes on its maximum over z.</p><p>3. Since, for each t, c(z, t) is bounded above in z, we know that 1/c(z, t) is bounded below above zero. For t * , we numerically calculate that 1/c(z, t * ) attains its minimum 0.9991977 at z 1.378; thus, 1/c(z, t * ) &gt; 0.99919 suggesting that no more than 9 of every 10,000 draws are rejected on average.</p><p>Since t * is the best truncation point regardless of z, we will assume that the truncation point has been fixed at t * and suppress it from the notation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of tail probabilities</head><p>Proposition 2 tells us that the sampler rarely rejects a proposal. One possible worry, however, is that the algorithm might calculate many terms in the sum before deciding to accept or reject, and that the sampler would be slow despite rarely rejecting.</p><p>Happily, this is not the case, as we now prove. Suppose one samples X ∼ J * (1, z). Let N denote the total number of proposals made before accepting, and let L n be the number of partial sums S i (i = 1, . . . , L n ) that are calculated before deciding to accept or reject proposal n ≤ N . A variant of theorem 5.1 from <ref type="bibr" target="#b12">Devroye (1986)</ref> </p><formula xml:id="formula_42">employs Wald's equation to show that that E[ N n=1 L n ] = ∞ i=0 ∞ 0 a i (x|z)dx.</formula><p>For the worst enclosing envelope, z 1.378, E[N ] = 1.0016; that is, on average, one rarely calculates anything beyond S 1 of the first proposal. A slight alteration of this theorem gives a more precise sense of how many terms in the partial sum must be calculated. Proposition 3. When sampling X ∼ J * (1, z), the probability of deciding to accept or reject upon checking the nth partial sum</p><formula xml:id="formula_43">S n , n ≥ 1, is 1 c(z) ∞ 0 {a n-1 (x|z) -a n (x|z)} dx.</formula><p>Proof. Let L denote the number of partials sums that are calculated before accepting or rejecting the proposal. That is, a proposal, X, is generated; U is drawn from U(0, a 0 (X|z)); and L is the smallest natural number n ∈ N for which U ≤ S n if n is odd or U &gt; S n if n is even, where S n denotes S n (X|z). But since L is the smallest n for which this holds, S L-2 &lt; U ≤ S L when L is odd and S L &lt; U ≤ S L-2 when L is even. Thus, the algorithm accepts or rejects if and only if U ∈ K L (X|z) where</p><formula xml:id="formula_44">K n (x|z) = (S n-2 (x|z), S n (x|z)], odd n (S n (x|z), S n-2 (x|z)], even n. In either case, |K n (x|z)| = a n-1 (x|z) -a n (x|z). Thus P(L = n|X = x) = a n-1 (x|z) -a n (x|z) a 0 (x|z) .</formula><p>Marginalizing over x yields</p><formula xml:id="formula_45">P(L = n) = 1 c(z) ∞ 0 {a n-1 (x|z) -a n (x|z)} dx.</formula><p>Since each coefficient a n is the piecewise composition of an inverse Gaussian kernel and an exponential kernel, these integrals may be evaluated. In particular, Together with Proposition 2, this provides a strong guarantee of the efficiency of the PG(1,z) sampler.</p><formula xml:id="formula_46">a n (x|z) = cosh(z)    2e -(2n+1)z p IG (x|µ n (z), λ n ), x &lt; t π n + 1 2 1 yn(z) p E (x|y n (z)), x ≥ t , where µ n (z) = 2n+1 z , λ n = (2n + 1) 2 , y n (z) = 0.5(z 2 + (n + 1/2) 2 π 2 ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The general PG(b, z) case</head><p>To sample from the entire family of PG(b, z) distributions, we exploit the additivity of the Pólya-Gamma class. In particular, when b ∈ N, one may sample PG(b, z) by taking b i.i.d.</p><p>draws from PG(1, z) and summing them. In binomial logistic regression, one will always sample PG(b, z) using integral b. This will also be the case in negative-binomial regression if one chooses an integer over-dispersion parameter. In the technical supplement, we discuss the case of non-integral b.</p><p>The run-time of the latent-variable sampling step is therefore roughly linear in the number of total counts in the data set. For example, to sample 1 million Pólya-Gamma(1,1) random variables took 0.70 seconds on a dual-core Apple laptop, versus 0.17 seconds for the same number of Gamma random variables. By contrast, to sample 1 million PG(10,1) random variables required 6.43 seconds, and to sample 1 million PG(100,1) random variables required 60.0 seconds.</p><p>We have had some initial success in developing a faster method to simulate from the PG(n,z) distribution that does not require summing together n PG(1,z) draws, and that works for non-integer values of n. This is an active subject of research, though somewhat beyond the scope of the present paper, where we use the sum-of-PG(1,z)'s method on all our benchmark examples. A full report on the alternative simulation method for PG(n,z) may be found in <ref type="bibr">Windle et al. (2013b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We benchmarked the Pólya-Gamma method against several alternatives for logit and negativebinomial models. Our purpose is to summarize the results presented in detail in our online technical supplement, to which we refer the interested reader.</p><p>Our primary metrics of comparison are the effective sample size and the effective sampling rate, defined as the effective sample size per second of runtime. The effective sampling rate quantifies how rapidly a Markov-chain sampler can produce independent draws from the posterior distribution. Following <ref type="bibr" target="#b26">Holmes and Held (2006)</ref>, the effective sample size (ESS) for the ith parameter in the model is</p><formula xml:id="formula_47">ESS i = M/{1 + 2 k j=1 ρ i (j)},</formula><p>where M is the number of post-burn-in samples, and ρ i (j) is the jth autocorrelation of β i . We use the coda package <ref type="bibr" target="#b32">(Plummer et al., 2006)</ref>, which fits an AR model to approximate the spectral density at zero, to estimate each ESS i . All of the benchmarks are generated using R so that timings are comparable. Some R code makes external calls to C. In particular, the Pólya-Gamma method calls a C routine to sample the Pólya-Gamma random variates, just as R routines for sampling common distributions use externally compiled code. Here we report the median effective sample size across all parameters in the model. Minimum and maximum effective sample sizes are reported in the technical supplement.</p><p>Our numerical experiments support several conclusions.</p><p>In binary logit models. First, the Pólya-Gamma is more efficient than all previously proposed data-augmentation schemes. This is true both in terms of effective sample size and effective sampling rate. Table <ref type="table" target="#tab_1">1</ref> summarizes the evidence: across 6 real and 2 simulated data sets, the Pólya-Gamma method was always more efficient than the next-best data-augmentation scheme (typically by a factor of 200%-500%). This includes the approximate random-utility methods of O' <ref type="bibr">Brien and Dunson (2004)</ref> and Frühwirth-Schnatter and Frühwirth (2010), and the exact method of <ref type="bibr" target="#b25">Gramacy and Polson (2012)</ref>. Frühwirth-Schnatter and Frühwirth (2010) find that their own method beats several other competitors, including the method of <ref type="bibr" target="#b26">Holmes and Held (2006)</ref>. We find this as well, and omit these timings from our comparison. Further details can be found in Section 3 of the technical supplement.</p><p>Second, the Pólya-Gamma method always had a higher effective sample size than the two default Metropolis samplers we tried. The first was a Gaussian proposal using Laplace's approximation. The second was a multivariate t 6 proposal using Laplace's approximation to provide the centering and scale-matrix parameters, recommended by <ref type="bibr" target="#b36">Rossi et al. (2005)</ref> and implemented in the R package bayesm <ref type="bibr" target="#b35">(Rossi, 2012)</ref>.</p><p>On 5 of the 8 data sets, the best Metropolis algorithm did have a higher effective sampling rate than the Pólya-Gamma method, due to the difference in run times. But this advantage depends crucially on the proposal distribution, where even small perturbations can lead to surprisingly large declines in performance. For example, on the Australian credit data set (labeled AC in the table), the Gaussian proposal led to a median effective sampling rate of 122 samples per second. The very similar multivariate t 6 proposal led to far more rejected proposals, and gave an effective sampling rate of only 2.6 samples per second. Diagnosing such differences for a specific problem may cost the user more time than is saved by a slightly faster sampler.</p><p>Finally, the Pólya-Gamma method truly shines when the model has a complex prior structure. In general, it is difficult to design good Metropolis samplers for these problems. For example, consider a binary logit mixed model with grouped data and a random-effects structure, where the log-odds of success for observation j in group i are and where either the α i , the β i , or both receive further hyperpriors. It is not clear that a good default Metropolis sampler is easily constructed unless there are a large number of observations per group. Table <ref type="table" target="#tab_2">2</ref> shows the results of naïvely using an independence Metropolis sampler based on the Laplace approximation to the full joint posterior. For a synthetic data set with a balanced design of 100 observations per group, the Pólya-Gamma method is slightly better. For the two real data sets with highly unbalanced designs, it is much better. Of course, it is certainly possible to design and tune better Metropolis-Hastings samplers for mixed models; see, for example, <ref type="bibr" target="#b20">Gamerman (1997)</ref>. We simply point out that what works well in the simplest case need not work well in a slightly more complicated case. The advantages of the Pólya-Gamma method are that it requires no tuning, is simple to implement, is uniformly ergodic <ref type="bibr" target="#b8">(Choi and Hobert, 2013)</ref>, and gives optimal or near-optimal performance across a range of cases.</p><formula xml:id="formula_48">ψ ij = α i + x ij β i ,</formula><p>In negative-binomial models. The Pólya-Gamma method consistently yields the best effective sample sizes in negative-binomial regression. However, its effective sampling rate suffers when working with a large counts or a non-integral over-dispersion parameter. Cur-rently, our Pólya-Gamma sampler can draw from PG(b, ψ) quickly when b = 1, but not for general, integral b: to sample from PG(b, ψ) when b ∈ N, we take b independent samples of PG(1, ψ) and sum them. Thus in negative-binomial models, one must sample at least N i=1 y i Pólya-Gamma random variates, where y i is the ith response, at every MCMC iteration. When the number of counts is relatively high, this becomes a burden. (The sampling method described in <ref type="bibr">Windle et al. (2013b)</ref> leads to better performance, but describing the alternative method is beyond the subject of this paper.)</p><p>The columns labeled Sim1 and Sim2 of Table <ref type="table" target="#tab_3">3</ref> show results for data simulated from a negative-binomial model with 400 observations and 3 regressors. (See the technical supplement for details.) In the first case (Sim1), the intercept is chosen so that the average outcome is a count of 8 (3244 total counts). Given the small average count size, the Pólya-Gamma method has a superior effective sampling rate compared to the approximate method of Frühwirth-Schnatter et al. ( <ref type="formula">2009</ref>), the next-best choice. In the second case (Sim2), the average outcome is a count of 24 (9593 total counts). Here the Frühwirth-Schnatter et al. algorithm finishes more quickly, and therefore has a better effective sampling rate. In both cases we restrict the sampler to integer over-dispersion parameters.</p><p>As before, the Pólya-Gamma method starts to shine when working with more complicated hierarchical models that devote proportionally less time to sampling the auxiliary variables. For instance, consider a spatial model where we observe counts y 1 , . . . , y n at locations x 1 , . . . , x n , respectively. It is natural to model the log rate parameter as a Gaussian process:</p><formula xml:id="formula_49">y i ∼ N B(n, 1/{1 + e -ψ i }) , ψ ∼ GP (0, K) ,</formula><p>where ψ = (ψ 1 , . . . , ψ n ) T and K is constructed by evaluating a covariance kernel at the locations x i . For example, under the squared-exponential kernel, we have</p><formula xml:id="formula_50">K ij = κ + exp d(x i , x j ) 2 2 2 ,</formula><p>with characteristic length scale , nugget κ, and distance function d (in our examples, Euclidean distance).</p><p>Using either the Pólya-Gamma or the Frühwirth-Schnatter et al. ( <ref type="formula">2009</ref>) techniques, one arrives at a multivariate Gaussian conditional for ψ whose covariance matrix involves latent variables. Producing a random variate from this distribution is expensive, as one must calculate the Cholesky decomposition of a relatively large matrix at each iteration. Therefore, the overall sampler spends relatively less time drawing auxiliary variables. Since the Pólya-Gamma method leads to a higher effective sample size, it wastes fewer of the expensive draws for the main parameter.</p><p>The columns labeled GP1 and GP2 of Table <ref type="table" target="#tab_3">3</ref> show two such examples. In the first synthetic data set, 256 equally spaced x points were used to generate a draw for ψ from a Gaussian process with length scale = 0.1 and nugget κ = 0.0. The average count was ȳ = 35.7, or 9137 total counts (roughly the same as in the second regression example, Sim2). In the second synthetic data set, we simulated ψ from a Gaussian process over 1000 x points, with length scale = 0.1 and a nugget = 0.0001. This yielded 22,720 total counts. In both cases, the Pólya-Gamma method led to a more efficient sampler-by a factor of 3 for the smaller problem, and 5 for the larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We have shown that Bayesian inference for logistic models can be implemented using a data augmentation scheme based on the novel class of Pólya-Gamma distributions. This leads to simple Gibbs-sampling algorithms for posterior computation that exploit standard normal linear-model theory, and that are notably simpler than previous schemes. We have also constructed an accept/reject sampler for the new family, with strong guarantees of efficiency (Propositions 2 and 3).</p><p>The evidence suggests that our data-augmentation scheme is the best current method for fitting complex Bayesian hierarchical models with binomial likelihoods. It also opens the door for exact Bayesian treatments of many modern-day machine-learning classification methods based on mixtures of logits (e.g. <ref type="bibr" target="#b37">Salakhutdinov et al., 2007;</ref><ref type="bibr" target="#b5">Blei and Lafferty, 2007)</ref>. Applying the Pólya-Gamma mixture framework to such problems is currently an active area of research.</p><p>Moreover, posterior updating via exponential tilting is a quite general situation that arises in Bayesian inference incorporating latent variables. In our case, the posterior distribution of ω that arises under normal pseudo-data with precision ω and a PG(b, 0) prior is precisely an exponentially titled PG(b, 0) random variable. This led to our characterization of the general PG(b, c) class. An interesting fact is that we were able to identify the conditional posterior for the latent variable strictly using its moment-generating function, without ever appealing to Bayes' rule for density functions. This follows the Lévy-penalty framework of <ref type="bibr" target="#b33">Polson and Scott (2012)</ref> and relates to work by <ref type="bibr" target="#b10">Ciesielski and Taylor (1962)</ref> on the sojourn times of Brownian motion. There may be many other situations where the same idea is applicable.</p><p>Our benchmarks have relied upon serial computation. However, one may trivially parallelize a vectorized Pólya-Gamma draw on a multicore CPU. Devising such a sampler for a graphical-processing unit (GPU) is less straightforward, but potentially more fruitful. The massively parallel nature of GPUs offer a solution to the sluggishness found when sampling PG(n, z) variables for large, integral n, which was the largest source of inefficiency with the negative-binomial results presented earlier. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Technical Supplement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1 Details of Pólya-Gamma sampling algorithm</head><p>Algorithm 1 shows pseudo-code for sampling the Pólya-Gamma(1, z) distribution. Recall from the main manuscript that one may pick t &gt; 0 and define the piecewise coefficients</p><formula xml:id="formula_51">a n (x) =          π(n + 1/2) 2 πx 3/2 exp - 2(n + 1/2) 2 x 0 &lt; x ≤ t,<label>(15)</label></formula><formula xml:id="formula_52">π(n + 1/2) exp - (n + 1/2) 2 π 2 2 x x &gt; t,<label>(16)</label></formula><p>so that f (x) = ∞ n=0 (-1) n a n (x) satisfies the partial sum criterion for x &gt; 0. To complete the analysis of the Pólya-Gamma sampler, we specify our method for sampling truncated inverse Gaussian random variables, IG(1/z, 1)I (0,t] . When z is small the inverse Gaussian distribution is approximately inverse χ 2 1 , motivating an accept-reject algorithm. When z is large, most of the inverse Gaussian distribution's mass will be below the truncation point t, motivating a rejection algorithm. Thus, we take a two pronged approach.</p><p>When 1/z &gt; t we generate a truncated inverse-Gaussian random variate using acceptreject sampling using the proposal distribution (1/χ 2 1 )I (t,∞) . The proposal X is generated following <ref type="bibr" target="#b13">Devroye (2009)</ref>. Considering the ratio of the kernels, one finds that P (accept|X = x) = exp(-xz 2 /2). Since z &lt; 1/t and X &lt; t we may compute a lower bound on the average rate of acceptance:</p><formula xml:id="formula_53">E exp -z 2 2 X ≥ exp -1 2t = 0.61 .</formula><p>See algorithm (2) for pseudocode. When 1/z ≤ t, we generate a truncated inverse-Gaussian random variate using rejec-For each data set, we run 10 MCMC simulations with 12,000 samples each, discarding the first 2,000 as burn-in, thereby leaving 10 batches of 10,000 samples. The effective sample size for each regression coefficient is calculated using the coda <ref type="bibr" target="#b32">(Plummer et al., 2006)</ref> package and averaged across the 10 batches. The component-wise minimum, median, and maximum of the (average) effective sample sizes are reported to summarize the results. A similar calculation is performed to calculate minimum, median, and maximum effective sampling rates (ESR). The effective sampling rate is the ratio of the effective sample size to the time taken to produce the sample. Thus, the effective sampling rates are normalized by the time taken to produce the 10,000 samples, disregarding the time taken for initialization, preprocessing, and burn-in. When discussing the various methods the primary metric we refer to is the median effective sampling rate, following the example of <ref type="bibr" target="#b15">Frühwirth-Schnatter and Frühwirth (2010)</ref>.</p><p>All of these experiments are carried out using R 2.15.1 on an Ubuntu machine with 8GB or RAM and an Intel Core i5 quad core processor. The number of cores is a potentially important factor as some libraries, including those that perform the matrix operations in R, may take advantage of multiple cores. The C code that we have written does not use parallelism.</p><p>In the sections that follow, each table reports the following metrics:</p><p>• the execution time of each method in seconds;</p><p>• the acceptance rate (relevant for the Metropolis samplers);</p><p>• the minimum, median, and maximum effective sample sizes (ESS) across all fixed or random effects; and • the minimum, median, and maximum effective sampling rates (ESR) across all fixed or random effects, defined as the effective sample size per second of runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3 Benchmarks: binary logistic regression S3.1 Data Sets</head><p>Nodal: part of the boot R package <ref type="bibr" target="#b6">(Canty and Ripley, 2012)</ref>. The response indicates if cancer has spread from the prostate to surrounding lymph nodes. There are 53 observations and 5 binary predictors. Pima Indian: There are 768 observations and 8 continuous predictors. It is noted on the UCI website<ref type="foot" target="#foot_0">foot_0</ref> that there are many predictor values coded as 0, though the physical measurement should be non-zero. We have removed all of those entries to generate a data set with 392 observations. The marginal mean incidence of diabetes is roughly 0.33 before and after removing these data points. Heart: The response represents either an absence or presence of heart disease.<ref type="foot" target="#foot_1">foot_1</ref> There are 270 observations and 13 attributes, of which 6 are categorical or binary and 1 is ordinal. The ordinal covariate has been stratified by dummy variables. Australian Credit: The response represents either accepting or rejecting a credit card application. <ref type="foot" target="#foot_2">3</ref> The meaning of each predictor was removed to protect the propriety of the original data. There are 690 observations and 14 attributes, of which 8 are categorical or binary. There were 37 observations with missing attribute values. These missing values were replaced by the mode of the attribute in the case of categorical data and the mean of the attribute for continuous data. This dataset is linearly separable and results in some divergent regression coefficients, which are kept in check by the prior. German Credit 1 and 2: The response represents either a good or bad credit risk. <ref type="foot" target="#foot_3">4</ref>There are 1000 observations and 20 attributes, including both continuous and categorical data. We benchmark two scenarios. In the first, the ordinal covariates have been given integer values and have not been stratified by dummy variables, yielding a total of 24 numeric predictors. In the second, the ordinal data has been stratified by dummy variables, yielding a total of 48 predictors. Synthetic 1: Simulated data with 150 outcomes and 10 predictors. The design points were chosen to be orthogonal. The data are included as a supplemental file. Synthetic 2: Simulated data with 500 outcomes and 20 predictors. The design points were simulated from a Gaussian factor model, to yield pronounced patterns of collinearity.</p><p>The data are included as a supplemental file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3.2 Methods</head><p>All of these routines are implemented in R, though some of them make calls to C. In particular, the independence Metropolis samplers do not make use of any non-standard calls to C, though their implementations have very little R overhead in terms of function calls. The Pólya-Gamma method calls a C routine to sample the Pólya-Gamma random variates, but otherwise only uses R. As a check upon our independence Metropolis sampler we include the independence Metropolis sampler of <ref type="bibr" target="#b36">Rossi et al. (2005)</ref>, which may be found in the bayesm package <ref type="bibr" target="#b35">(Rossi, 2012)</ref>. Their sampler uses a t 6 proposal, while ours uses a normal proposal. The suite of routines in the binomlogit package <ref type="bibr" target="#b17">(Fussl, 2012)</ref> implement the techniques discussed in <ref type="bibr" target="#b19">Fussl et al. (2013)</ref>. One routine provided by the binomlogit package coincides with the technique described in Frühwirth-Schnatter and Frühwirth (2010) for the case of binary logistic regression. A separate routine implements the latter and uses a single call to C. Gramacy and Polson's R package, reglogit, also calls external C code <ref type="bibr" target="#b24">(Gramacy, 2012)</ref>.</p><p>For every data set the regression coefficient was given a diffuse N (0, 0.01I) prior, except when using Gramacy and Polson's method, in which case it was given a exp( i |β i /100|) prior per the specifications of the reglogit package. The following is a short description of each method along with its abbreviated name. PG: The Pólya-Gamma method described previously. FS: Frühwirth-Schnatter and Frühwirth (2010) follow <ref type="bibr" target="#b26">Holmes and Held (2006)</ref> and use the representation</p><formula xml:id="formula_54">y i = 1{z i &gt; 0} , z i = x i β + i , i ∼ Lo , (<label>17</label></formula><formula xml:id="formula_55">)</formula><p>where Lo is the standard logistic distribution (c.f. Albert and Chib, 1993, for the probit case). They approximate p( i ) using a discrete mixture of normals. IndMH: Independence Metropolis with a normal proposal using the posterior mode and the Hessian at the mode for the mean and precision matrix. RAM: after Rossi, Allenby, and McCulloch. An independence Metropolis with a t 6 proposal from the R package bayesm <ref type="bibr" target="#b35">(Rossi, 2012)</ref>. Calculate the posterior mode and the Hessian at the mode to pick the mean and scale matrix of the proposal.  <ref type="bibr" target="#b15">Schnatter and Frühwirth (2010)</ref>. A convenient representation is found that relies on a discrete mixture of normals approximation for posterior inference that works for binomial logistic regression. From the R package binomlogit <ref type="bibr" target="#b17">(Fussl, 2012)</ref>. dRUMIndMH: Similar to dRUMAuxMix, but instead of using a discrete mixture of normals, use a single normal to approximate the error term and correct using Metropolis-Hastings. From the R package binomlogit. IndivdRUMIndMH: This is the same as dRUMIndMH, but specific to binary logistic regression. From the R package binomlogit. dRUMHAM: Identical to dRUMAuxMix, but now use a discrete mixture of normals approximation in which the number of components to mix over is determined by y i /n i .</p><p>From the R package binomlogit. GP: after <ref type="bibr" target="#b25">Gramacy and Polson (2012)</ref>. Another data augmentation scheme with only a single layer of latents. This routine uses a double exponential prior, which is hardcoded in the R package reglogit <ref type="bibr" target="#b24">(Gramacy, 2012)</ref>. We set the scale of this prior to agree with the scale of the normal prior we used in all other cases above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3.3 Results</head><p>The results are shown in Tables <ref type="table" target="#tab_11">4 through 11</ref>. As mentioned previously, these are averaged over 10 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S4 Benchmarks: logit mixed models</head><p>A major advantage of data augmentation, and hence the Pólya-Gamma technique, is that it is easily adapted to more complicated models. We consider three examples of logistic mixed model whose intercepts are random effects, in which case the log odds for observation j from     </p><formula xml:id="formula_56">ψ ij = α i + x ij β α i ∼ N (m, 1/φ) m ∼ N (0, κ 2 /φ) φ ∼ Ga(1, 1) β ∼ N (0, 100I) .<label>(18)</label></formula><p>An extra step is easily added to the Pólya-Gamma Gibbs sampler to estimate (α, β, m) and φ. We use the following three data sets to benchmark the Pólya-Gamma method.</p><p>Synthetic: A synthetically generated dataset with 5 groups, 100 observations within each group, and a single fixed effect.</p><p>Polls: Voting data from a Presidential campaign <ref type="bibr" target="#b21">(Gelman and Hill, 2006)</ref>. The response indicates a vote for or against former President George W. Bush. There are 49 groups corresponding to states. Some states have very few observations, requiring a model that shrinks coefficients towards a global mean to get reasonable estimates. A single fixed effect for the race of the respondent is included, although it would be trivial to include other covariates. Entries with missing data were deleted to yield a total of 2015 observations.</p><p>Xerop: The Xerop data set from the epicalc R package <ref type="bibr" target="#b9">(Chongsuvivatwong, 2012)</ref>. Indonesian children were observed to examine the causes of respiratory infections; of specific interest is whether vitamin A deficiencies cause such illness. Multiple observations of each individual were made. The data is grouped by individual id yielding a total of 275 random intercepts. A total of 5 fixed effects are included in the modelage, sex, height, stunted growth, and season-corresponding to an 8 dimensional regression coefficient after expanding the season covariate using dummy variables.</p><p>Table <ref type="table" target="#tab_13">12</ref> summarizes the results, which suggest that the Pólya-Gamma method is a sensible default choice for fitting nonlinear mixed-effect models.</p><p>While an independence Metropolis sampler usually works well for binary logistic regression, it does not work well for the mixed models we consider. For instance, in the polls data set, at least two heuristics that suggest the Laplace approximation will be a poor proposal.</p><p>Synthetic: N = 500, Pa = 5, P b = 1, samp=10,000, burn=2,000, thin=1  Notice that the second and third benchmarks are thinned every 10 samples to produce a total of 10,000 posterior draws. Even after thinning, the effective sample size for each is low compared to the PG method. The effective samples sizes are taken for the collection (α, β, m) and do not include φ.</p><p>First, the posterior mode does not coincide with the posterior mean. Second, the Hessian at the mode is nearly singular. Its smallest eigenvalue, in absolute terms, corresponds to an eigenvector that points predominantly in the direction of φ. Thus, there is a great deal of uncertainty in the posterior mode of φ. If we iteratively solve for the MLE by starting at the posterior mean, or if we start at the posterior mode for all the coordinates except φ, which we initialize at the posterior mean of φ, then we arrive at the same end point. This suggests that the behavior we observe is not due to a poor choice of initial value or a poor stopping rule. The first image in Figure <ref type="figure" target="#fig_1">S4</ref> shows that the difference between the posterior mode and posterior mean is, by far, greatest in the φ coordinate. The second image in Figure <ref type="figure" target="#fig_1">S4</ref> provides one example of the lack of curvature in φ at the mode. If one plots φ against the other coordinates, then one sees a similar, though often less extreme, picture. In general, large values of φ are found at the tip of an isosceles triangular whose base runs parallel to the coordinate that is not φ. While the upper tip of the triangle may posses the most likely posterior values, the rest of the posterior does not fall away quick enough to make that a likely posterior random variate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5 Benchmarks: negative-binomial models</head><p>We simulated two synthetic data sets with N = 400 data points using the model</p><formula xml:id="formula_57">y i ∼ N B(mean = µ i , d) , log µ i = α + x i β</formula><p>where β ∈ R 3 . Both data sets are included as supplements. The parameter d is estimated using a random-walk Metropolis-Hastings step over the integers. (Neither the Pólya-Gamma method nor the R package by <ref type="bibr" target="#b17">Fussl (2012)</ref> are set up to work efficiently with non-integer    values of this parameter.) The model with fewer counts corresponds to α = 2, while the model with more counts corresponds to α = 3. This produced a sample mean of roughly 8 in the former case and 24 in the latter.</p><p>Table <ref type="table" target="#tab_15">13</ref> shows the results for both simulated data sets. Notice that the Pólya-Gamma method has superior effective sample size in both cases, but a lower effective sampling rate in the second case. This is caused by the bottleneck of summing n copies of a PG(1, z) variable to draw a PG(n, z) variable. As mentioned in the main manuscript, it is an open challenge to create an efficient Pólya-Gamma sampler for arbitrary n, which would make it the best choice in both cases.</p><p>One reaches a different conclusion when working with more complicated models that devote proportionally less time to sampling the auxiliary variables. Specifically, consider the model</p><formula xml:id="formula_58">y i ∼ N B(mean = µ(x i ), d) , log µ ∼ GP (0, K) ,</formula><p>where K is the square exponential covariance kernel,</p><formula xml:id="formula_59">K(x 1 , x 2 ) = κ + exp x 1 -x 2 2 2 2 ,</formula><p>with characteristic length scale and nugget κ. Using either the Pólya-Gamma or <ref type="bibr" target="#b16">Frühwirth-Schnatter et al. (2009)</ref> data augmentation techniques, one arrives at a complete conditional for υ = log µ that is equivalent to the posterior (υ|z) derived using pseudo-data {z i } generated by</p><formula xml:id="formula_60">z i = υ(x i ) + i , i ∼ N (0, V i )</formula><p>where V i is a function of the ith auxiliary variable. Since the prior for υ is a Gaussian process one may use conjugate formulas to sample the complete conditional of υ. But producing a random variate from this distribution is expensive as one must calculate the Cholesky decomposition of a relatively large matrix at each iteration. Consequently, the relative time spent sampling the auxiliary variables in each model decreases, making the Pólya-Gamma method competitive, and sometimes better, than the method of Frühwirth-Schnatter et al. We provide two such examples in Table ( <ref type="formula" target="#formula_35">14</ref>). In the first synthetic data set, 256 equally spaced points were used to generate a draw υ(x i ) and y i for i = 1, . . . , 256 where υ ∼ GP (0, K) and K has length scale = 0.1 and a nugget κ = 0.0. The average count value of the synthetic data set is ȳ = 35.7, yielding 9137 total counts, which is roughly the same amount as in the larger negative binomial example discussed earlier. Now, however, because proportionally more time is spent sampling the main parameter, and because the Pólya-Gamma method wastes fewer of these expensive draws, it is more efficient. In the second synthetic data set, 1000 randomly selected points were chosen to generate a draw from υ(x i ) and y i with υ ∼ GP (0, K) where K has length scale = 0.1 and a nugget κ = 0.0001. The average count value is ȳ = 22.72, yielding 22,720 total counts. The larger problem shows an even greater improvement in performance over the method of Frühwirth-Schnatter et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S6 Extensions</head><p>S6.1 2 × 2 × N tables Consider a simple example of a binary-response clinical trial conducted in each of N different centers. Let n ij be the number of patients assigned to treatment regime j in center i; and</p><p>Table <ref type="table" target="#tab_5">15</ref>: Data from a multi-center, binary-response study on topical cream effectiveness <ref type="bibr" target="#b38">(Skene and Wakefield, 1990)</ref>. let Y = {y ij } be the corresponding number of successes for i = 1, . . . , N . Table <ref type="table" target="#tab_1">1</ref> presents a data set along these lines, from <ref type="bibr" target="#b38">Skene and Wakefield (1990)</ref>. These data arise from a multicenter trial comparing the efficacy of two different topical cream preparations, labeled the treatment and the control. Let p ij denote the underlying success probability in center i for treatment j, and ψ ij the corresponding log-odds. If ψ i = (ψ i1 , ψ i2 ) T is assigned a bivariate normal prior ψ i ∼ N(µ, Σ) then the posterior for Ψ = {ψ ij } is</p><formula xml:id="formula_61">p(Ψ | Y ) ∝ N i=1</formula><p>e y i1 ψ i1 (1 + e ψ i1 ) n i1 e y i2 ψ i2 (1 + e ψ i2 ) n i2 p(ψ i1 , ψ i2 | µ, Σ) .</p><p>We apply Theorem 1 from the main paper to each term in the posterior, thereby introducing augmentation variables Ω i = diag(ω i1 , ω i2 ) for each center. This yields, after some algebra, a simple Gibbs sampler that iterates between two sets of conditional distributions:</p><formula xml:id="formula_62">(ψ i | Y, Ω i , µ, Σ) ∼ N(m i , V Ω i ) (19) (ω ij | ψ ij ) ∼ PG (n ij , ψ ij ) ,<label>where</label></formula><formula xml:id="formula_63">V -1 Ω i = Ω i + Σ -1 m i = V Ω i (κ i + Σ -1 µ) κ i = (y i1 -n i1 /2, y i2 -n i2 /2) T .</formula><p>Figure <ref type="figure">5</ref> shows the results of applying this Gibbs sampler to the data from <ref type="bibr" target="#b38">Skene and Wakefield (1990)</ref>.</p><p>In this analysis, we used a normal-Wishart prior for (µ, Σ -1 ). Hyperparameters were chosen to match Table <ref type="table">II</ref> from <ref type="bibr" target="#b38">Skene and Wakefield (1990)</ref>, who parameterize the model in terms of the prior expected values for ρ, σ 2 ψ 1 , and σ 2 ψ 2 , where</p><formula xml:id="formula_64">Σ = σ 2 ψ 1 ρ ρ σ 2 ψ 2 .</formula><p>Following <ref type="bibr" target="#b28">Leonard (1975)</ref>, we can model these probabilities using a logistic transformation. Let</p><formula xml:id="formula_65">p ijk = exp(ψ ijk ) K l=1 exp(ψ ijl )</formula><p>.</p><p>Many common prior structures will maintain conditional conjugacy using the Polya-Gamma framework outlined thus far. For example, we may assume an exchangeable matrix-normal prior at the level of treatment centers:</p><formula xml:id="formula_66">ψ i ∼ N(M, Σ R , Σ C ) ,</formula><p>where ψ i is the matrix whose (j, k) entry is ψ ijk ; M is the mean matrix; and Σ R and Σ C are row-and column-specific covariance matrices, respectively. See <ref type="bibr" target="#b11">Dawid (1981)</ref> for further details on matrix-normal theory. Note that, for identifiability, we set ψ ijK = 0, implying that Σ C is of dimension K -1. This leads to a posterior of the form</p><formula xml:id="formula_67">p(Ψ | D) = N i=1   p(ψ i ) • J j=1 K k=1 exp(ψ ijk ) K l=1 exp(ψ ijl ) y ijk   ,</formula><p>suppressing any dependence on (M, Σ R , Σ C ) for notational ease. To show that this fits within the Polya-Gamma framework, we use a similar approach to <ref type="bibr" target="#b26">Holmes and Held (2006)</ref>, rewriting each probability as where c ijk = log{ l =k exp(ψ ijl )} is implicitly a function of the other ψ ijl 's for l = k. We now fix values of i and k and examine the conditional posterior distribution for ψ i•k = (ψ i1k , . . . , ψ iJk ) , given ψ i•l for l = k:</p><formula xml:id="formula_68">p(ψ i•k | D, ψ i•(-k) ) ∝ p(ψ i•k | ψ i•(-k) ) • J j=1 e ψ ijk -c ijk 1 + e ψ ijk -c ijk y ijk 1 1 + e ψ ijk -c ijk n ij -y ijk = p(ψ i•k | ψ i•(-k) ) • J j=1 e y ijk (ψ ijk -c ijk ) (1 + e ψ ijk -c ijk ) n ij</formula><p>This is simply a multivariate version of the same bivariate form in that arises in a 2 × 2 table. Appealing to the theory of Polya-Gamma random variables outlined above, we may express this as:</p><formula xml:id="formula_69">p(ψ i•k | D, ψ i•(-k) ) ∝ p(ψ i•k | ψ i•(-k) ) • J j=1 e κ ijk [ψ ijk -c ijk ] cosh n ij ([ψ ijk -c ijk ]/2) = p(ψ i•k | ψ i•(-k) ) • J j=1 e κ ijk [ψ ijk -c ijk ] • E e -ω ijk [ψ ijk -c ijk ] 2 /2</formula><p>, where ω ijk ∼ PG(n ij , 0), j = 1, . . . , J; and κ ijk = y ijk -n ij /2. Given {ω ijk } for j = 1, . . . , J, all of these terms will combine in a single normal kernel, whose mean and covariance structure will depend heavily upon the particular choices of hyperparameters in the matrixnormal prior for ψ i . Each ω ijk term can be updated as</p><formula xml:id="formula_70">(ω ijk | ψ ijk ) ∼ PG(n ij , ψ ijk -c ijk ) ,</formula><p>leading to a simple MCMC that loops over centers and responses, drawing each vector of parameters ψ i•k (that is, for all treatments at once) conditional on the other ψ i•(-k) 's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S6.3 Multinomial logistic regression</head><p>One may extend the Pólya-Gamma method used for binary logistic regression to multinomial logistic regression. Consider the multinomial sample y i = {y ij } J j=1 that records the number of responses in each category j = 1, . . . , J and the total number of responses n i . The logistic link function for polychotomous regression stipulates that the probability of randomly drawing a single response from the jth category in the ith sample is</p><formula xml:id="formula_71">p ij = exp ψ ij J i=1 exp ψ ik</formula><p>where the log odds ψ ij is modeled by x T i β j and β J has been constrained to be zero for purposes of identification. Following <ref type="bibr" target="#b26">Holmes and Held (2006)</ref> the likelihood for β j conditional upon β -j , the matrix with column vector β j removed, is where κ ij = (y ij -n i /2). Employing the conditionally conjugate prior β j ∼ N (m 0j , V 0j ) yields a two-part update:</p><formula xml:id="formula_72">(β j | Ω j ) ∼ N (m j , V j ) (ω ij | β j ) ∼ P G(n i , η ij ) for i = 1, • • • , N,</formula><p>where</p><formula xml:id="formula_73">V -1 j = X Ω j X + V -1 0j , m j = V j X (κ j -Ω j c j ) + V -1 0j m 0j ,</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Plots of the density of the Pólya-Gamma distribution PG(b, c) for various values of b and c. Note that the horizontal and vertical axes differ in each plot.</figDesc><graphic coords="25,106.00,72.00,400.00,200.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Proceeding from left to right and top to bottom. Upper left: the posterior mode and the posterior mean of (α, β, m, φ). The mode and mean are most different in φ. Upper right: the level sets of (φ, m) of the log posterior when the other coordinates are evaluated at the posterior mode. The log posterior is very flat when moving along φ. Bottom left: the marginal posterior distribution of φ. When marginalizing, one finds that few large values of φ are likely. Bottom right: a scatter plot of posterior samples for (φ, m). Again, one sees that upon marginalizing out the other coordinates the posterior mass is concentrated at relatively small values of φ compared to its value at the posterior mode.</figDesc><graphic coords="35,112.19,336.26,192.00,192.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>p ijk = exp(ψ ijk ) l =k exp(ψ ijl ) + exp(ψ ijk ) = e ψ ijk -c ijk 1 + e ψ ijk -c ijk ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc>η ij n i -y ij where η ij = x T i β j -C ij with C ij = log k =j exp x T i β k ,which looks like the binary logistic likelihood previously discussed. Incorporating the Pólya-Gamma auxiliary variable, the likelihood becomesN i=1 e κ ij η ij e - η 2 ij 2 ω ij P G(ω ij |n i , 0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and p IG and p E are the corresponding densities. The table below shows the first several probabilities for the worst case envelope, z 1.378. Clearly P(L &gt; n) decays rapidly with n.</figDesc><table><row><cell>n</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell cols="5">P(L &gt; n) 8.023 × 10 -4 1.728 × 10 -9 8.213 × 10 -18 8.066 × 10 -29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Summary of experiments on real and simulated data for binary logistic regression. ESS: the median effective sample size for an MCMC run of 10,000 samples. ESR: the median effective sample rate, or median ESS divided by the runtime of the sampler in seconds. AC: Australian credit data set. GC1 and GC2: partial and full versions of the German credit data set. Sim1 and Sim2: simulated data with orthogonal and correlated predictors, respectively. Best RU-DA: the result of the best random-utility dataaugmentation algorithm for that data set. Best Metropolis: the result of the Metropolis algorithm with the most efficient proposal distribution among those tested. See the technical supplement for full details.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Data set</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Nodal Diab. Heart</cell><cell cols="4">AC GC1 GC2 Sim1 Sim2</cell></row><row><cell>ESS</cell><cell>Pólya-Gamma</cell><cell>4860</cell><cell>5445</cell><cell cols="5">3527 3840 5893 5748 7692 2612</cell></row><row><cell></cell><cell>Best RU-DA</cell><cell>1645</cell><cell>2071</cell><cell cols="4">621 1044 2227 2153 3031</cell><cell>574</cell></row><row><cell></cell><cell>Best Metropolis</cell><cell>3609</cell><cell>5245</cell><cell>1076</cell><cell cols="4">415 3340 1050 4115 1388</cell></row><row><cell>ESR</cell><cell>Pólya-Gamma</cell><cell>1632</cell><cell>964</cell><cell>634</cell><cell>300</cell><cell>383</cell><cell>258 2010</cell><cell>300</cell></row><row><cell></cell><cell>Best RU-DA</cell><cell>887</cell><cell>382</cell><cell>187</cell><cell>69</cell><cell>129</cell><cell>85 1042</cell><cell>59</cell></row><row><cell></cell><cell>Best Metropolis</cell><cell>2795</cell><cell>2524</cell><cell>544</cell><cell>122</cell><cell>933</cell><cell>223 2862</cell><cell>537</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of experiments on real and simulated data for binary logistic mixed models. Metropolis: the result of an independence Metropolis sampler based on the Laplace approximation. Using a t6 proposal yielded equally poor results. See the technical supplement for full details.</figDesc><table><row><cell></cell><cell cols="2">Data set</cell><cell></cell></row><row><cell></cell><cell cols="3">Synthetic Polls Xerop</cell></row><row><cell>ESS Pólya-Gamma</cell><cell cols="2">6976 9194</cell><cell>3039</cell></row><row><cell>Metropolis</cell><cell>3675</cell><cell>53</cell><cell>3</cell></row><row><cell>ESR Pólya-Gamma</cell><cell>957</cell><cell>288</cell><cell>311</cell></row><row><cell>Metropolis</cell><cell>929</cell><cell>0.36</cell><cell>0.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Summary of experiments on simulated data for negative-binomial models. Metropolis: the result of an independence Metropolis sampler based on a t6 proposal. FS09: the algorithm of<ref type="bibr" target="#b16">Frühwirth-Schnatter et al. (2009)</ref>. Sim1 and Sim2: simulated negative-binomial regression problems. GP1 and GP2: simulated Gaussian-process spatial models. The independence Metropolis algorithm is not applicable in the spatial models, where there as many parameters as observations.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Data set</cell><cell></cell></row><row><cell></cell><cell cols="3">Sim1 Sim2 GP1</cell><cell>GP2</cell></row><row><cell cols="5">Total Counts 3244 9593 9137 22732</cell></row><row><cell cols="4">ESS Pólya-Gamma 7646 3590 6309</cell><cell>6386</cell></row><row><cell>FS09</cell><cell>719</cell><cell cols="2">915 1296</cell><cell>1157</cell></row><row><cell>Metropolis</cell><cell>749</cell><cell>764</cell><cell>-</cell><cell>-</cell></row><row><cell>ESR Pólya-Gamma</cell><cell>285</cell><cell>52</cell><cell>62</cell><cell>3.16</cell></row><row><cell>FS09</cell><cell>86</cell><cell>110</cell><cell>24</cell><cell>0.62</cell></row><row><cell>Metropolis</cell><cell>73</cell><cell>87</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Nodal data: N = 53, P = 6 Method time ARate ESS.min ESS.med ESS.max ESR.min ESR.med ESR.max The method of O'Brien and Dunson (2004). Strictly speaking, this is not logistic regression; it is binary regression using a Student-t cumulative distribution function as the inverse link function. dRUMAuxMix: Work by Fussl et al. (2013) that extends the technique of Frühwirth-</figDesc><table><row><cell>PG</cell><cell>2.98</cell><cell>1.00</cell><cell>3221.12</cell><cell>4859.89</cell><cell>5571.76</cell><cell>1081.55</cell><cell>1631.96</cell><cell>1871.00</cell></row><row><cell>IndMH</cell><cell>1.76</cell><cell>0.66</cell><cell>1070.23</cell><cell>1401.89</cell><cell>1799.02</cell><cell>610.19</cell><cell>794.93</cell><cell>1024.56</cell></row><row><cell>RAM</cell><cell>1.29</cell><cell>0.64</cell><cell>3127.79</cell><cell>3609.31</cell><cell>3993.75</cell><cell>2422.49</cell><cell>2794.69</cell><cell>3090.05</cell></row><row><cell>OD</cell><cell>3.95</cell><cell>1.00</cell><cell>975.36</cell><cell>1644.66</cell><cell>1868.93</cell><cell>246.58</cell><cell>415.80</cell><cell>472.48</cell></row><row><cell>FS</cell><cell>3.49</cell><cell>1.00</cell><cell>979.56</cell><cell>1575.06</cell><cell>1902.24</cell><cell>280.38</cell><cell>450.67</cell><cell>544.38</cell></row><row><cell>dRUMAuxMix</cell><cell>2.69</cell><cell>1.00</cell><cell>1015.18</cell><cell>1613.45</cell><cell>1912.78</cell><cell>376.98</cell><cell>598.94</cell><cell>710.30</cell></row><row><cell>dRUMIndMH</cell><cell>1.41</cell><cell>0.62</cell><cell>693.34</cell><cell>1058.95</cell><cell>1330.14</cell><cell>492.45</cell><cell>751.28</cell><cell>943.66</cell></row><row><cell>IndivdRUMIndMH</cell><cell>1.30</cell><cell>0.61</cell><cell>671.76</cell><cell>1148.61</cell><cell>1339.58</cell><cell>518.79</cell><cell>886.78</cell><cell>1034.49</cell></row><row><cell>dRUMHAM</cell><cell>3.06</cell><cell>1.00</cell><cell>968.41</cell><cell>1563.88</cell><cell>1903.00</cell><cell>316.82</cell><cell>511.63</cell><cell>622.75</cell></row><row><cell>GP</cell><cell>17.86</cell><cell>1.00</cell><cell>2821.49</cell><cell>4419.37</cell><cell>5395.29</cell><cell>157.93</cell><cell>247.38</cell><cell>302.00</cell></row><row><cell>OD:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Diabetes data, N=270, P=19</figDesc><table><row><cell>Method</cell><cell cols="8">time ARate ESS.min ESS.med ESS.max ESR.min ESR.med ESR.max</cell></row><row><cell>PG</cell><cell>5.65</cell><cell>1.00</cell><cell>3255.25</cell><cell>5444.79</cell><cell>6437.16</cell><cell>576.14</cell><cell>963.65</cell><cell>1139.24</cell></row><row><cell>IndMH</cell><cell>2.21</cell><cell>0.81</cell><cell>3890.09</cell><cell>5245.16</cell><cell>5672.83</cell><cell>1759.54</cell><cell>2371.27</cell><cell>2562.59</cell></row><row><cell>RAM</cell><cell>1.93</cell><cell>0.68</cell><cell>4751.95</cell><cell>4881.63</cell><cell>5072.02</cell><cell>2456.33</cell><cell>2523.85</cell><cell>2621.98</cell></row><row><cell>OD</cell><cell>6.63</cell><cell>1.00</cell><cell>1188.00</cell><cell>2070.56</cell><cell>2541.70</cell><cell>179.27</cell><cell>312.39</cell><cell>383.49</cell></row><row><cell>FS</cell><cell>6.61</cell><cell>1.00</cell><cell>1087.40</cell><cell>1969.22</cell><cell>2428.81</cell><cell>164.39</cell><cell>297.72</cell><cell>367.18</cell></row><row><cell>dRUMAuxMix</cell><cell>6.05</cell><cell>1.00</cell><cell>1158.42</cell><cell>1998.06</cell><cell>2445.66</cell><cell>191.52</cell><cell>330.39</cell><cell>404.34</cell></row><row><cell>dRUMIndMH</cell><cell>3.82</cell><cell>0.49</cell><cell>647.20</cell><cell>1138.03</cell><cell>1338.73</cell><cell>169.41</cell><cell>297.98</cell><cell>350.43</cell></row><row><cell>IndivdRUMIndMH</cell><cell>2.91</cell><cell>0.48</cell><cell>614.57</cell><cell>1111.60</cell><cell>1281.51</cell><cell>211.33</cell><cell>382.23</cell><cell>440.63</cell></row><row><cell>dRUMHAM</cell><cell>6.98</cell><cell>1.00</cell><cell>1101.71</cell><cell>1953.60</cell><cell>2366.54</cell><cell>157.89</cell><cell>280.01</cell><cell>339.18</cell></row><row><cell>GP</cell><cell>88.11</cell><cell>1.00</cell><cell>2926.17</cell><cell>5075.60</cell><cell>5847.59</cell><cell>33.21</cell><cell>57.61</cell><cell>66.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Heart data: N = 270, P = 19</figDesc><table><row><cell>Method</cell><cell cols="8">time ARate ESS.min ESS.med ESS.max ESR.min ESR.med ESR.max</cell></row><row><cell>PG</cell><cell>5.56</cell><cell>1.00</cell><cell>2097.03</cell><cell>3526.82</cell><cell>4852.37</cell><cell>377.08</cell><cell>633.92</cell><cell>872.30</cell></row><row><cell>IndMH</cell><cell>2.24</cell><cell>0.39</cell><cell>589.64</cell><cell>744.86</cell><cell>920.85</cell><cell>263.63</cell><cell>333.19</cell><cell>413.03</cell></row><row><cell>RAM</cell><cell>1.98</cell><cell>0.30</cell><cell>862.60</cell><cell>1076.04</cell><cell>1275.22</cell><cell>436.51</cell><cell>543.95</cell><cell>645.13</cell></row><row><cell>OD</cell><cell>6.68</cell><cell>1.00</cell><cell>620.90</cell><cell>1094.27</cell><cell>1596.40</cell><cell>93.03</cell><cell>163.91</cell><cell>239.12</cell></row><row><cell>FS</cell><cell>6.50</cell><cell>1.00</cell><cell>558.95</cell><cell>1112.53</cell><cell>1573.88</cell><cell>85.92</cell><cell>171.04</cell><cell>241.96</cell></row><row><cell>dRUMAuxMix</cell><cell>5.97</cell><cell>1.00</cell><cell>604.60</cell><cell>1118.89</cell><cell>1523.84</cell><cell>101.33</cell><cell>187.49</cell><cell>255.38</cell></row><row><cell>dRUMIndMH</cell><cell>3.51</cell><cell>0.34</cell><cell>256.85</cell><cell>445.87</cell><cell>653.13</cell><cell>73.24</cell><cell>127.28</cell><cell>186.38</cell></row><row><cell>IndivdRUMIndMH</cell><cell>2.88</cell><cell>0.35</cell><cell>290.41</cell><cell>467.93</cell><cell>607.80</cell><cell>100.70</cell><cell>162.25</cell><cell>210.79</cell></row><row><cell>dRUMHAM</cell><cell>7.06</cell><cell>1.00</cell><cell>592.63</cell><cell>1133.59</cell><cell>1518.72</cell><cell>83.99</cell><cell>160.72</cell><cell>215.25</cell></row><row><cell>GP</cell><cell>65.53</cell><cell>1.00</cell><cell>1398.43</cell><cell>2807.09</cell><cell>4287.55</cell><cell>21.34</cell><cell>42.84</cell><cell>65.43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Australian Credit: N = 690, P = 35</figDesc><table><row><cell>Method</cell><cell cols="8">time ARate ESS.min ESS.med ESS.max ESR.min ESR.med ESR.max</cell></row><row><cell>PG</cell><cell>12.78</cell><cell>1.00</cell><cell>409.98</cell><cell>3841.02</cell><cell>5235.53</cell><cell>32.07</cell><cell>300.44</cell><cell>409.48</cell></row><row><cell>IndMH</cell><cell>3.42</cell><cell>0.22</cell><cell>211.48</cell><cell>414.87</cell><cell>480.02</cell><cell>61.89</cell><cell>121.53</cell><cell>140.59</cell></row><row><cell>RAM</cell><cell>3.92</cell><cell>0.00</cell><cell>8.27</cell><cell>10.08</cell><cell>26.95</cell><cell>2.11</cell><cell>2.57</cell><cell>6.87</cell></row><row><cell>OD</cell><cell>14.59</cell><cell>1.00</cell><cell>28.59</cell><cell>988.30</cell><cell>1784.77</cell><cell>1.96</cell><cell>67.73</cell><cell>122.33</cell></row><row><cell>FS</cell><cell>15.05</cell><cell>1.00</cell><cell>36.22</cell><cell>1043.69</cell><cell>1768.47</cell><cell>2.41</cell><cell>69.37</cell><cell>117.53</cell></row><row><cell>dRUMAuxMix</cell><cell>14.92</cell><cell>1.00</cell><cell>29.34</cell><cell>991.32</cell><cell>1764.40</cell><cell>1.97</cell><cell>66.44</cell><cell>118.27</cell></row><row><cell>dRUMIndMH</cell><cell>8.93</cell><cell>0.19</cell><cell>13.03</cell><cell>222.92</cell><cell>435.42</cell><cell>1.46</cell><cell>24.97</cell><cell>48.76</cell></row><row><cell>IndivdRUMIndMH</cell><cell>7.38</cell><cell>0.19</cell><cell>13.61</cell><cell>220.02</cell><cell>448.76</cell><cell>1.85</cell><cell>29.83</cell><cell>60.84</cell></row><row><cell>dRUMHAM</cell><cell>18.64</cell><cell>1.00</cell><cell>28.75</cell><cell>1040.74</cell><cell>1817.85</cell><cell>1.54</cell><cell>55.84</cell><cell>97.53</cell></row><row><cell>GP</cell><cell>162.73</cell><cell>1.00</cell><cell>95.81</cell><cell>2632.74</cell><cell>4757.04</cell><cell>0.59</cell><cell>16.18</cell><cell>29.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>German Credit 1: N = 1000, P = 25</figDesc><table><row><cell>Method</cell><cell cols="8">time ARate ESS.min ESS.med ESS.max ESR.min ESR.med ESR.max</cell></row><row><cell>PG</cell><cell>15.37</cell><cell>1.00</cell><cell>3111.71</cell><cell>5893.15</cell><cell>6462.36</cell><cell>202.45</cell><cell>383.40</cell><cell>420.44</cell></row><row><cell>IndMH</cell><cell>3.58</cell><cell>0.68</cell><cell>2332.25</cell><cell>3340.54</cell><cell>3850.71</cell><cell>651.41</cell><cell>932.96</cell><cell>1075.47</cell></row><row><cell>RAM</cell><cell>4.17</cell><cell>0.43</cell><cell>1906.23</cell><cell>2348.20</cell><cell>2478.68</cell><cell>457.11</cell><cell>563.07</cell><cell>594.30</cell></row><row><cell>OD</cell><cell>17.32</cell><cell>1.00</cell><cell>1030.53</cell><cell>2226.92</cell><cell>2637.98</cell><cell>59.51</cell><cell>128.59</cell><cell>152.33</cell></row><row><cell>FS</cell><cell>18.21</cell><cell>1.00</cell><cell>957.05</cell><cell>2154.06</cell><cell>2503.09</cell><cell>52.55</cell><cell>118.27</cell><cell>137.43</cell></row><row><cell>dRUMAuxMix</cell><cell>18.13</cell><cell>1.00</cell><cell>955.41</cell><cell>2150.59</cell><cell>2533.40</cell><cell>52.68</cell><cell>118.60</cell><cell>139.70</cell></row><row><cell>dRUMIndMH</cell><cell>10.60</cell><cell>0.29</cell><cell>360.72</cell><cell>702.89</cell><cell>809.20</cell><cell>34.03</cell><cell>66.30</cell><cell>76.33</cell></row><row><cell>IndivdRUMIndMH</cell><cell>8.35</cell><cell>0.29</cell><cell>334.83</cell><cell>693.41</cell><cell>802.33</cell><cell>40.09</cell><cell>83.04</cell><cell>96.08</cell></row><row><cell>dRUMHAM</cell><cell>22.15</cell><cell>1.00</cell><cell>958.02</cell><cell>2137.13</cell><cell>2477.10</cell><cell>43.25</cell><cell>96.48</cell><cell>111.84</cell></row><row><cell>GP</cell><cell>223.80</cell><cell>1.00</cell><cell>2588.07</cell><cell>5317.57</cell><cell>6059.81</cell><cell>11.56</cell><cell>23.76</cell><cell>27.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>German Credit 2: N = 1000, P = 49</figDesc><table><row><cell>Method</cell><cell cols="8">time ARate ESS.min ESS.med ESS.max ESR.min ESR.med ESR.max</cell></row><row><cell>PG</cell><cell>22.30</cell><cell>1.00</cell><cell>2803.23</cell><cell>5748.30</cell><cell>6774.82</cell><cell>125.69</cell><cell>257.75</cell><cell>303.76</cell></row><row><cell>IndMH</cell><cell>4.72</cell><cell>0.41</cell><cell>730.34</cell><cell>1050.29</cell><cell>1236.55</cell><cell>154.73</cell><cell>222.70</cell><cell>262.05</cell></row><row><cell>RAM</cell><cell>6.02</cell><cell>0.00</cell><cell>5.49</cell><cell>14.40</cell><cell>235.50</cell><cell>0.91</cell><cell>2.39</cell><cell>39.13</cell></row><row><cell>OD</cell><cell>25.34</cell><cell>1.00</cell><cell>717.94</cell><cell>2153.05</cell><cell>2655.86</cell><cell>28.33</cell><cell>84.96</cell><cell>104.80</cell></row><row><cell>FS</cell><cell>26.44</cell><cell>1.00</cell><cell>727.17</cell><cell>2083.48</cell><cell>2554.62</cell><cell>27.50</cell><cell>78.80</cell><cell>96.62</cell></row><row><cell>dRUMAuxMix</cell><cell>26.91</cell><cell>1.00</cell><cell>755.31</cell><cell>2093.68</cell><cell>2562.11</cell><cell>28.06</cell><cell>77.80</cell><cell>95.21</cell></row><row><cell>dRUMIndMH</cell><cell>14.66</cell><cell>0.13</cell><cell>132.74</cell><cell>291.11</cell><cell>345.12</cell><cell>9.05</cell><cell>19.86</cell><cell>23.54</cell></row><row><cell>IndivdRUMIndMH</cell><cell>12.45</cell><cell>0.13</cell><cell>136.57</cell><cell>290.13</cell><cell>345.22</cell><cell>10.97</cell><cell>23.31</cell><cell>27.73</cell></row><row><cell>dRUMHAM</cell><cell>35.99</cell><cell>1.00</cell><cell>742.04</cell><cell>2075.41</cell><cell>2579.42</cell><cell>20.62</cell><cell>57.67</cell><cell>71.67</cell></row><row><cell>GP</cell><cell>243.41</cell><cell>1.00</cell><cell>2181.84</cell><cell>5353.41</cell><cell>6315.71</cell><cell>8.96</cell><cell>21.99</cell><cell>25.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Synthetic 1, orthogonal predictors: N = 150, P = 10 Method time ARate ESS.min ESS.med ESS.max ESR.min ESR.med ESR.max</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Synthetic 2, correlated predictors: N = 500, P = 20</figDesc><table><row><cell>Method</cell><cell cols="8">time ARate ESS.min ESS.med ESS.max ESR.min ESR.med ESR.max</cell></row><row><cell>PG</cell><cell>8.70</cell><cell>1.00</cell><cell>1971.61</cell><cell>2612.10</cell><cell>2837.41</cell><cell>226.46</cell><cell>300.10</cell><cell>325.95</cell></row><row><cell>FS</cell><cell>9.85</cell><cell>1.00</cell><cell>459.59</cell><cell>585.91</cell><cell>651.05</cell><cell>46.65</cell><cell>59.48</cell><cell>66.09</cell></row><row><cell>IndMH</cell><cell>2.52</cell><cell>0.42</cell><cell>826.94</cell><cell>966.95</cell><cell>1119.81</cell><cell>327.98</cell><cell>382.96</cell><cell>443.65</cell></row><row><cell>RAM</cell><cell>2.59</cell><cell>0.34</cell><cell>1312.67</cell><cell>1387.94</cell><cell>1520.29</cell><cell>507.54</cell><cell>536.84</cell><cell>588.10</cell></row><row><cell>OD</cell><cell>9.67</cell><cell>1.00</cell><cell>428.12</cell><cell>573.75</cell><cell>652.30</cell><cell>44.28</cell><cell>59.36</cell><cell>67.48</cell></row><row><cell>dRUMIndMH</cell><cell>5.35</cell><cell>0.33</cell><cell>211.14</cell><cell>249.33</cell><cell>281.50</cell><cell>39.46</cell><cell>46.58</cell><cell>52.59</cell></row><row><cell>dRUMHAM</cell><cell>11.18</cell><cell>1.00</cell><cell>452.50</cell><cell>563.30</cell><cell>644.73</cell><cell>40.46</cell><cell>50.37</cell><cell>57.65</cell></row><row><cell>dRUMAuxMix</cell><cell>9.51</cell><cell>1.00</cell><cell>422.00</cell><cell>564.95</cell><cell>639.89</cell><cell>44.39</cell><cell>59.43</cell><cell>67.31</cell></row><row><cell>IndivdRUMIndMH</cell><cell>4.17</cell><cell>0.32</cell><cell>201.50</cell><cell>239.50</cell><cell>280.35</cell><cell>48.37</cell><cell>57.51</cell><cell>67.30</cell></row><row><cell>GP</cell><cell>114.98</cell><cell>1.00</cell><cell>748.71</cell><cell>1102.59</cell><cell>1386.08</cell><cell>6.51</cell><cell>9.59</cell><cell>12.06</cell></row><row><cell cols="2">group i, ψ ij , is modeled by</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>A set of three benchmarks for binary logistic mixed models. N denotes the number of samples, P a denotes the number of groups, and P b denotes the dimension of the fixed effects coefficient. The random effects are limited to group dependent intercepts.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Negative binomial regression. PG is the Pólya-Gamma Gibbs sampler. FS follows<ref type="bibr" target="#b16">Frühwirth-Schnatter et al. (2009)</ref>. RAM is the random walk Metropolis-Hastings sampler from the bayesm package<ref type="bibr" target="#b35">(Rossi, 2012)</ref>. α is the true intercept and y i is the ith response. Each model has three continuous predictors.</figDesc><table><row><cell></cell><cell cols="3">Gaussian process 1: ȳ = 35.7,</cell><cell cols="4">yi = 9137, N = 256, = 0.1, nugget=0.0</cell><cell></cell></row><row><cell>Method</cell><cell cols="8">time ARate ESS.min ESS.med ESS.max ESR.min ESR.med ESR.max</cell></row><row><cell>PG</cell><cell>101.89</cell><cell>1.00</cell><cell>790.55</cell><cell>6308.65</cell><cell>9798.04</cell><cell>7.76</cell><cell>61.92</cell><cell>96.19</cell></row><row><cell>FS</cell><cell>53.17</cell><cell>1.00</cell><cell>481.36</cell><cell>1296.27</cell><cell>2257.27</cell><cell>9.05</cell><cell>24.38</cell><cell>42.45</cell></row><row><cell></cell><cell cols="3">Gaussian process 2: ȳ = 22.7,</cell><cell cols="4">yi = 22732, N = 1000, = 0.1, nugget=0.0001</cell><cell></cell></row><row><cell>Method</cell><cell cols="8">time ARate ESS.min ESS.med ESS.max ESR.min ESR.med ESR.max</cell></row><row><cell>PG</cell><cell>2021.78</cell><cell>1.00</cell><cell>1966.77</cell><cell>6386.43</cell><cell>9862.54</cell><cell>0.97</cell><cell>3.16</cell><cell>4.88</cell></row><row><cell>FS</cell><cell>1867.05</cell><cell>1.00</cell><cell>270.13</cell><cell>1156.52</cell><cell>1761.70</cell><cell>0.14</cell><cell>0.62</cell><cell>0.94</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>Binomial spatial models. PG is the Pólya-Gamma Gibbs sampler. FS follows<ref type="bibr" target="#b16">Frühwirth-Schnatter et al. (2009)</ref>. N is the total number of observations and y i denotes the ith observation.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://archive.ics.uci.edu/ml/datasets/Statlog+(Heart)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://archive.ics.uci.edu/ml/datasets/Statlog+(Australian+Credit+Approval).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. The authors wish to thank <rs type="person">Hee Min Choi</rs> and <rs type="person">Jim Hobert</rs> for sharing an early draft of their paper on the uniform ergodicity of the <rs type="institution">Pólya-Gamma Gibbs sampler</rs>. They also wish to thank two anonymous referees, the associate editor, and the editor of the Journal of the <rs type="institution">American Statistical Association</rs>, whose many insights and helpful suggestions have improved the paper. The second author acknowledges the support of a <rs type="grantName">CAREER grant</rs> from the <rs type="funder">U.S. National Science Foundation</rs> (<rs type="grantNumber">DMS-1255187</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5dxG3Ef">
					<idno type="grant-number">DMS-1255187</idno>
					<orgName type="grant-name">CAREER grant</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 1 Sampling from P G(1, z)</p><p>Input: z &gt; 0.</p><p>Define: pigauss(t | µ, λ), the CDF of the inverse Gaussian distribution Define: a n (x), the piecewise-defined coefficients in (1) and (2). z ← |z|/2, t ← 0.64, K ← π 2 /8 + z 2 /2 p ← π 2K exp(-Kt) q ← 2 exp(-|z|) pigauss(t | µ = 1/z, λ = 1.0) repeat Generate U, V ∼ U(0, 1) if U &lt; p/(p + q) then (Truncated Exponential)</p><p>tion sampling. <ref type="bibr">Devroye (1986) (p. 149</ref>) describes how to sample from an inverse-Gaussian distribution using a many-to-one transformation. Sampling X in this fashion until X &lt; t yields an acceptance rate bounded below by</p><p>for all 1/z &lt; t. See Algorithm 3 for pseudocode.</p><p>Recall that when b is an integer, we draw PG(b, z) by summing b i.i.d. draws from PG(1, z). When b is not integral, the following simple approach often suffices. Write b = b + e, where b is the integral part of b, and sum a draw from PG( b , z), using the method previously described, with a draw from PG(e, z), using the finite sum-of-gammas approximation. With 200 terms in the sum, we find that the approximation is quite accurate for such small values of the first parameter, as each Ga(e, 1) term in the sum tends to be small, and the weights in the sum decay like 1/k 2 . This, in contrast, may not be the case when using the finite sum-of-gammas approximation for arbitrary b.</p><p>In <ref type="bibr">Windle et al. (2013b)</ref>, we describe a better method for handling large and/or noninteger shape parameters. This method is implemented in the BayesLogit R package <ref type="bibr">(Windle et al., 2013a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2 Benchmarks: overview</head><p>We benchmark the Pólya-Gamma method against several alternatives for binary logistic regression and negative binomial regression for count data to measure its relative performance. All of these benchmarks are empirical and hence some caution is urged. Our primary metric of comparison is the effective sampling rate, which is the effective sample size per second and which quantifies how quickly a sampler can produce independent draws from the posterior distribution. However, this metric is sensitive to numerous idiosyncrasies relating to the implementation of the routines, the language in which they are written, and the hardware on which they are run. We generate these benchmarks using R, though some of the routines make calls to external C code. The specifics of each method are discussed in further detail below. In general, we find that the Pólya-Gamma technique compares favorably to other data augmentation methods. Specifically, the Pólya-Gamma technique performs better than the methods of O' <ref type="bibr">Brien and Dunson (2004)</ref>, <ref type="bibr" target="#b24">Gramacy and</ref><ref type="bibr">Polson (2012), and</ref><ref type="bibr">Frühwirth-Schnatter and</ref><ref type="bibr" target="#b15">Frühwirth (2010)</ref>. Frühwirth-Schnatter and Frühwirth (2010) provides a detailed comparison of several methods itself. For instance, the authors find that method of <ref type="bibr" target="#b26">Holmes and Held (2006)</ref> did not beat their discrete mixture of normals. We find this as well and hence omit it from the comparisons below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Log-odds ratios in an 8-center binary-response study</head><p>Treatment Center</p><p>Treatment (95% CI) Control (95% CI)</p><p>Figure <ref type="figure">5</ref>: Posterior distributions for the log-odds ratio for each of the 8 centers in the topical-cream study from <ref type="bibr" target="#b38">Skene and Wakefield (1990)</ref>. The vertical lines are central 95% posterior credible intervals; the dots are the posterior means; and the X's are the maximumlikelihood estimates of the log-odds ratios, with no shrinkage among the treatment centers.</p><p>Note that the maximum-likelihood estimate is ψ i2 = -∞ for the control group in centers 5 and 6, as no successes were observed.</p><p>To match their choices, we use the following identity that codifies a relationship between the hyperparameters B and d, and the prior moments for marginal variances and the correlation coefficient. If Σ ∼ IW(d, B), then</p><p>In this way we are able to map from pre-specified moments to hyperparameters, ending up with d = 4 and B = 0.754 0.857 0.857 1.480 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S6.2 Higher-order tables</head><p>Now consider a multi-center, multinomial response study with more than two treatment arms. This can be modeled using hierarchy of N different two-way tables, each having the same J treatment regimes and K possible outcomes. The data D consist of triply indexed outcomes y ijk , each indicating the number of observations in center i and treatment j with outcome k. We let n ij = k y ij indicate the number of subjects assigned to have treatment j at center i.</p><p>Let P = {p ijk } denote the set of probabilities that a subject in center i with treatment j experiences outcome k, such that k p ijk = 1 for all i, j. Given these probabilities, the full likelihood is Table <ref type="table">16</ref>: "Correct" refers to the number of glass fragments for each category that were correctly identified by the Bayesian multinomial logit model. The glass identification dataset includes a type of glass, class 4, for which there are no observations. c j is the jth column of C, and Ω j = diag({ω ij } N i=1 ). One may sample the posterior distribution of (β | y) via Gibbs sampling by iterating over the above steps for j = 1, . . . , J -1.</p><p>The Pólya-Gamma method generates samples from the joint posterior distribution without appealing to analytic approximations to the posterior. This offers an important advantage when the number of observations is not significantly larger than the number of parameters.</p><p>To see this, consider sampling the joint posterior for β using a Metropolis-Hastings algorithm with an independence proposal. The likelihood in β is approximately normal, centered at the posterior mode m, and with variance V equal to the inverse of the Hessian matrix evaluated at the mode. (Both of these may be found using standard numerical routines.) Thus a natural proposal for (vec(β (t) ) | y) is vec(b) ∼ N (m, aV ) for some a ≈ 1. When data are plentiful, this method is both simple and highly efficient, and is implemented in many standard software packages (e.g. <ref type="bibr" target="#b29">Martin et al., 2011)</ref>.</p><p>But when vec(β) is high-dimensional relative to the number of observations the Hessian matrix H may be ill-conditioned, making it impossible or impractical to generate normal proposals. Multinomial logistic regression succumbs to this problem more quickly than binary logistic regression, as the number of parameters scales like the product of the number of categories and the number of predictors.</p><p>To illustrate this phenomenon, we consider glass-identification data from <ref type="bibr" target="#b23">German (1987)</ref>. This data set has J = 6 categories of glass and nine predictors describing the chemical and optical properties of the glass that one may measure in a forensics lab and use in a criminal investigation. This generates up to 50 = 10 × 5 parameters, including the intercepts and the constraint that β J = 0. These must be estimated using n = 214 observations. In this case, the Hessian H at the posterior mode is poorly conditioned when employing a vague prior, incapacitating the independent Metropolis-Hastings algorithm. Numerical experiments confirm that even when a vague prior is strong enough to produce a numerically invertible Hessian, rejection rates are prohibitively high. In contrast, the multinomial Pólya-Gamma method still produces reasonable posterior distributions in a fully automatic fashion, even with a weakly informative normal prior for each β j . Table <ref type="table">16</ref>, which shows the in-sample performance of the multinomial logit model, demonstrates the problem with the joint proposal distribution: category 6 is perfectly separable into cases and non-cases, even though the other categories are not. This is a well-known problem with maximumlikelihood estimation of logistic models. The same problem also forecloses the option of posterior sampling using methods that require a unique MLE to exist.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian analysis of binary and polychotomous response data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">422</biblScope>
			<biblScope unit="page" from="669" to="679" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scale mixtures of normal distributions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mallows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="99" to="102" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Normal variance-mean mixtures and z distributions</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">E</forename><surname>Barndorff-Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="145" to="159" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maechler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bolker</surname></persName>
		</author>
		<ptr target="http://CRAN.R-project.org/package=mlmRev" />
		<title level="m">mlmRev: Examples from Multilevel Modelling Software Review</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="0" to="1" />
		</imprint>
	</monogr>
	<note>R package version 1</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probability laws related to the Jacobi theta and Riemann zeta functions, and Brownian excursions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Biane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="435" to="465" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A correlated topic model of Science</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="35" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Canty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ripley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">boot: Bootstrap R (S-Plus) Functions</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>1.3-4 edition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meta-analysis for 2 × 2 tables: a Bayesian approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in Medicine</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="158" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The Polya-gamma Gibbs sampler for Bayesian logistic regression is uniformly ergodic</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Hobert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>University of Florida</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Chongsuvivatwong</surname></persName>
		</author>
		<ptr target="http://CRAN.R-project.org/package=epicalc" />
		<title level="m">Epidemiological calculator</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>R package version 2.15.1.0</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">First passage times and sojourn times for Brownian motion in space and the exact Hausdorff measure of the sample path</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ciesielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="434" to="450" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Some matrix-variate distribution theory: Notational considerations and a Bayesian application</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="265" to="274" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Non-uniform random variate generation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Devroye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On exact simulation algorithms for some distributions related to Jacobi theta functions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Devroye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics &amp; Probability Letters</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="2251" to="2259" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Auxiliary mixture sampling with applications to logistic models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frühwirth-Schnatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frühwirth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics and Data Analysis</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="3509" to="3528" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Data augmentation and mcmc for binary and multinomial logit models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frühwirth-Schnatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frühwirth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Modelling and Regression Structures</title>
		<imprint>
			<publisher>Available from UT library online</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="111" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved auxiliary mixture sampling for hierarchical models of non-Gaussian data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frühwirth-Schnatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frühwirth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="479" to="492" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Fussl</surname></persName>
		</author>
		<ptr target="http://CRAN" />
		<title level="m">Efficient MCMC for Binomial Logit Models</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">R-Project</forename></persName>
		</author>
		<title level="m">R package version 1.0</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient MCMC for binomial logit models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fussl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Frühwirth-Schnatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frühwirth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Modeling and Computer Simulation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sampling from the posterior distribution in generalized linear mixed models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gamerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="57" to="68" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Data Analysis Using Regression and Multilevel/Hierarchical Models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Bayesian Data Analysis</title>
		<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Glass identification dataset</title>
		<author>
			<persName><forename type="first">B</forename><surname>German</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml/datasets/Glass+Identification" />
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">reglogit: Simulation-based Regularized Logistic Regression</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Gramacy</surname></persName>
		</author>
		<ptr target="http://CRAN.R-project.org/package=reglogit.Rpackageversion1.1" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simulation-based regularized logistic regression</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Gramacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="567" to="590" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayesian auxiliary variable models for binary and multinomial regression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Held</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="168" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Jackman</surname></persName>
		</author>
		<title level="m">Bayesian Analysis for the Social Sciences</title>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bayesian estimation methods for two-way contingency tables</title>
		<author>
			<persName><forename type="first">T</forename><surname>Leonard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society (Series B)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="37" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">MCMCpack: Markov chain Monte Carlo in r</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conditional logit analysis of qualitative choice behavior</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mcfadden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers of Econometrics</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Zarembka</surname></persName>
		</editor>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1974">1974</date>
			<biblScope unit="page" from="105" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bayesian multivariate logistic regression</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="739" to="746" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CODA: Convergence diagnosis and output analysis for MCMC</title>
		<author>
			<persName><forename type="first">M</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cowles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vines</surname></persName>
		</author>
		<ptr target="http://CRAN.R-project.org/doc/Rnews/" />
	</analytic>
	<monogr>
		<title level="j">R News</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="11" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Local shrinkage rules, Lévy processes, and regularized regression</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society (Series B)</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="287" to="311" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Data augmentation for non-Gaussian regression models using variance-mean mixtures</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="459" to="471" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rossi</surname></persName>
		</author>
		<ptr target="http://CRAN.R-project.org/package=bayesm" />
		<title level="m">Bayesian Inference for Marketing/Micro-econometrics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2" to="5" />
		</imprint>
	</monogr>
	<note>R package version</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mcculloch</surname></persName>
		</author>
		<title level="m">Bayesian statistics and marketing</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Restricted Boltzmann machines for collaborative filtering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International Conference on Machine Learning</title>
		<meeting>the 24th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical models for multi-centre binary response studies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Skene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Wakefield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in Medicine</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="919" to="929" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">BayesLogit: Bayesian logistic regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Windle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Scott</surname></persName>
		</author>
		<ptr target="http://cran.r-project.org/web/packages/BayesLogit/index.html" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>R package version 0.2-4</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Improved Pólya-gamma sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Windle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>University of Texas at Austin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
