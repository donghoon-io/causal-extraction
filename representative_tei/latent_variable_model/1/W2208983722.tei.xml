<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-conditional Latent Variable Model for Joint Facial Action Unit Detection</title>
				<funder ref="#_PPrbjBX #_pweGPXk">
					<orgName type="full">European Community 7th Framework Programme</orgName>
				</funder>
				<funder ref="#_cywPdqt">
					<orgName type="full">European Community Horizon 2020</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stefanos</forename><surname>Eleftheriadis</surname></persName>
							<email>s.eleftheriadis@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ognjen</forename><surname>Rudovic</surname></persName>
							<email>orudovic@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
							<email>m.pantic@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">EEMCS</orgName>
								<orgName type="institution" key="instit2">University of Twente</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-conditional Latent Variable Model for Joint Facial Action Unit Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel multi-conditional latent variable model for simultaneous facial feature fusion and detection of facial action units. In our approach we exploit the structure-discovery capabilities of generative models such as Gaussian processes, and the discriminative power of classifiers such as logistic function. This leads to superior performance compared to existing classifiers for the target task that exploit either the discriminative or generative property, but not both. The model learning is performed via an efficient, newly proposed Bayesian learning strategy based on Monte Carlo sampling. Consequently, the learned model is robust to data overfitting, regardless of the number of both input features and jointly estimated facial action units. Extensive qualitative and quantitative experimental evaluations are performed on three publicly available datasets (CK+, Shoulder-pain and DISFA). We show that the proposed model outperforms the state-of-theart methods for the target task on (i) feature fusion, and (ii) multiple facial action unit detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial expression is one of the most powerful channels of non-verbal communication <ref type="bibr" target="#b0">[1]</ref>. It conveys emotions, provides clues about people's personality and intentions, reveals the state of pain, weakness or hesitation, among others. Automatic analysis of facial expressions has attracted significant research attention over the past decade, due to its wide importance in various domains such as medicine, security and psychology <ref type="bibr" target="#b13">[14]</ref>. The facial action coding system (FACS) <ref type="bibr" target="#b6">[7]</ref> is the most comprehensive anatomically-based system for describing facial expressions in terms of nonoverlapping, visually detectable facial muscle activations, named action units (AUs). FACS defines 33 unique AUs, several categories of head/eye positions and other movements, which can describe every possible facial expression.</p><p>Automatic detection of AUs is a challenging task mainly due to the complexity and subtlety of human facial behav-ior, and individual differences and artifacts caused by variation in head-pose, illumination, occlusions, etc. <ref type="bibr" target="#b3">[4]</ref>. These and other sources of variation in facial expression data are typically accounted for at the (i) feature level, by finding facial features that are robust to the aforementioned artifacts, and/or (ii) model level, by capturing semantics of AUs, i.e., their co-occurrences as commonly encountered in naturalistic data. At the feature level, detection of AUs can be performed using either geometric or appearance descriptors, or both. While the geometric features (e.g., the displacement of the facial points between expressive and neutral faces <ref type="bibr" target="#b12">[13]</ref>) are more robust to illumination and pose changes, not all AUs can be detected solely from them. For example, activation of AU6 wrinkles the skin around the outer corners of the eyes and raises the cheeks, which makes it difficult to detect this AU (independently from other AUs) using the geometric features explicitly. On the other hand, appearance-based features overcome this by being able to capture transient differences in the facial texture, such as wrinkles, bulges and furrows, however, they are usually prone to overfitting. Hence, modeling both geometric and appearance features exploits the complementary properties of these two features, leading to improved AU detection.</p><p>At the model level, the goal is to improve the AU detection by modeling 'semantics' of facial behavior (e.g., in terms of AU co-occurrences). This is important because AUs rarely appear in isolation (more than 7,000 AU combinations have been observed in everyday life <ref type="bibr" target="#b22">[23]</ref>). The type of the AU co-occurrences depends largely on the context in which the facial expression is displayed, e.g., due to latent variables such as emotions (e.g., AU12 and AU6 in the case of happiness, and AU4 and AU7 in the case of fear). Furthermore, co-occurring AUs can be non-additive, in the case of one AU masks another, or a new and distinct set of appearances can be created <ref type="bibr" target="#b6">[7]</ref>. For instance, AU4 (brow lowerer) has a different appearance when occurring together with AU1 (inner brow raise) than alone. When AU1,4 cooccur, the brows are drawn together and are raised due to the action of AU1; the brows are lowered otherwise. This, in turn, significantly affects the appearance of the target AUs.</p><p>Figure <ref type="figure">1</ref>. The proposed MC-LVM. The geometrical and appearance input features, y (1) and y (2) , are first projected onto the shared manifold X. The fusion is attained via GP conditionals, p(y (1) |x) and p(y (2) |x), that generate the inputs. Classification is performed on the manifold via simultaneously learned logistic functions p(z (c) |x) for multiple AU detection. The subspace is regularized using constraints imposed on both latent positions and output classifiers, encoding local and global dependencies among the AUs.</p><p>Most of existing approaches to AU detection model each AU independently, using either a single feature set <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>, or by combining multiple feature sets through feature concatenation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, or multiple-kernel learning (MKL) <ref type="bibr" target="#b23">[24]</ref>. Other methods treat different combinations of AUs as new independent classes <ref type="bibr" target="#b15">[16]</ref>; yet, this is impractical given the number of possible combinations. On the other hand, methods that do attempt to model the AU co-occurrences (e.g., <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">35]</ref>) fail to perform efficient fusion of different types of facial features. To the best of our knowledge, the only methods that attempt both are <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b34">34]</ref>. However, neither of these methods can perform simultaneous feature fusion and modeling of a large number of AUs.</p><p>To this end, we propose a Multi-Conditional Latent Variable Model (MC-LVM) that performs jointly the fusion of different facial features and detection of multiple AUs. Instead of performing the AU detection in the original feature space, as done in existing works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b36">36]</ref>, the MC-LVM attains the fusion by learning a low-dimensional subspace (i) shared across different feature sets, learned via the framework of Gaussian processes (GPs) <ref type="bibr" target="#b20">[21]</ref>, and (ii) constrained by the local dependencies among multiple AUs, encoded by means of string kernels <ref type="bibr" target="#b20">[21]</ref>, and the global dependencies, encoded via the AU co-occurrence structure. The key to our approach is the proposed definition of the multiconditional likelihood function that combines both the generative and discriminative properties of probabilistic models. In contrast to existing subspace learning methods for multi-output (e.g., <ref type="bibr" target="#b29">[30]</ref>), the MC-LVM learns a discriminative subspace for multiple AU detection that is endowed with the generative property of GPs, which turns out to be an efficient regularizer during the parameter learning. To further improve the robustness of the parameter estimation, a Bayesian learning of the subspace is facilitated through Monte Carlo (MC) sampling, and the Expectation-Maximization (EM)-like algorithm is proposed. As a result, the training of the MC-LVM can be performed with a large number of AUs, without seriously affecting its computational load. During inference, multiple AU detection is performed through the learned subspace that best generate the input features. This is attained via the learned back mappings to the shared space, and does not require any additional optimization. As evidenced by our results, the resulting model achieves superior performance compared to existing methods for multiple AU detection, and other methods for feature fusion and multi-label classification. The outline of the proposed approach is given in Fig. <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Facial AU detection</head><p>The majority of the existing works attempt to recognize AUs or certain AU combinations independently <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref>. While the former ignores the dependencies among AUs, the latter is a prohibitively large space of possible combinations. To the best of our knowledge, there are only few works that perform joint AU detection. <ref type="bibr" target="#b25">[26]</ref> proposed a generative framework based on dynamic Bayesian networks (DBN) to model the semantics of different AUs. A downside of this model, is that it lacks discriminative properties of what models. In contrast, the models in <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">34]</ref> are defined in a fully discriminative manner. Specifically, <ref type="bibr" target="#b36">[36]</ref> first learns the logistic classifiers for multiple AUs using the notion of multitask feature learning, and then uses a pre-trained BN to refine the predictions. This independent modeling could result in inconsistent dependencies across inputs/outputs, and produce contradictory predictions. <ref type="bibr" target="#b35">[35]</ref> tries to learn independent logistic classifiers by first selecting a sparse subset of facial patches which are more relevant to each AU. Yet, the fusion task is not addressed, while the AU-dependencies are regarded only between predefined pairs. <ref type="bibr" target="#b28">[29]</ref> employed the restricted Boltzman machine (RBM) to overcome the pair-wise AU modeling limitation of DBN. Discrete latent variables account for the dependencies among the outputs, which are directly connected to the image features. Since the latent variables are not connected to the feature space, they cannot model correlations between the inputs, hence, concatenation is used for the fusion task. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">34]</ref> combine multi-task learning with MKL to jointly learn different AU classifiers. The authors introduce l p -norm regularization to the MKL, in order to fuse multiple features with different kernels <ref type="bibr" target="#b34">[34]</ref>, and account for the AU-dependencies <ref type="bibr" target="#b32">[33]</ref>. Yet, <ref type="bibr" target="#b34">[34]</ref> can deal only with subsets of AUs in its output due to its learning complexity, while in <ref type="bibr" target="#b32">[33]</ref> the relations among the AUs are captured by predefined latent variables.</p><p>Our approach significantly differs from the above works, since the fusion of the features is performed in a continuous latent space. The latter can also efficiently model relations among large number of outputs, without the requirement to a priori define groups of AUs as done in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35]</ref>. The learning of the output dependencies is performed simultaneously to the fusion task by combining both generative and discriminative learning within a single model. This has not been addressed before in models for multiple AU detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-label Classification</head><p>Various approaches for multi-label classification (MLC) exist in the literature. For an extensive overview please see <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b24">25]</ref>. Baseline methods include <ref type="bibr" target="#b31">[32]</ref>, which extends the k-nearest neighbor (kNN) classifier to the multi-label scenario, and <ref type="bibr" target="#b30">[31]</ref>, which derives the back-propagation algorithm of the neural networks for the MLC. MLC is also highly related to multi-task learning techniques, that capture dependencies among multiple outputs through parameter sharing <ref type="bibr" target="#b8">[9]</ref>. More sophisticated algorithms learn a latent variable model of task specific parameters within a probabilistic framework <ref type="bibr" target="#b29">[30]</ref>. However, none of these methods perform simultaneous feature fusion and MLC.</p><p>To mitigate the limitations of the above methods, recent works in the GP context <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b5">6]</ref> try to combine multitask learning and feature fusion via subspace learning. <ref type="bibr" target="#b27">[28]</ref> jointly optimizes latent variables in order to reconstruct the input data, and account for multiple tasks in the output. A downside of this method is that the latent space is directly optimized using the ML strategy, which in the case of large number of data can overfit. To ameliorate this, <ref type="bibr" target="#b5">[6]</ref> proposed learning of the space in a fully Bayesian framework using variational inference to integrate out the latent space.</p><p>Contrary to <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b5">6]</ref>, MC-LVM employs multi-conditional learning strategies to re-weight the generative and discriminative conditionals, in order to unravel a more suitable subspace for joint feature fusion and MLC. In our Bayesian approach, the latent space is approximated via efficient MC sampling, where the conditional models determine the importance of each sample. More importantly, the inference step is efficiently facilitated via the learned projection mappings to the manifold. This overcomes the requirement of <ref type="bibr" target="#b5">[6]</ref> to learn another approximation to the posterior of the test inputs. Finally, note that such an approach has not been applied before on the task of multiple AU detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-conditional Latent Variable Model (MC-LVM)</head><p>Let us denote the training set as</p><formula xml:id="formula_0">D = {Y, Z}. Y = [y 1 , . . . , y i , . . . , y N ] T is comprised of N instances of multi-variate inputs stored in y i = {y (1) i , . . . y (v) i , . . . y (V )</formula><p>i }, where y (v) i ∈ R Dv . These represent different types of corresponding facial features or observation spaces. Furthermore, Z = [z 1 , . . . , z i , . . . , z N ] T are multiple binary labels, with z i ∈ {-1, +1} C encoding C (co-occurring) outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Definition</head><p>We aim to learn a model that simultaneously combines different inputs and detects activations of multiple outputs. We assume the existence of a latent space X = [x 1 , . . . , x i , . . . , x N ] T , where x i ∈ R q , q ≪ D, jointly generates y i and z i . For notational simplicity, in what follows, we set the number of input spaces to V = 2. Then, the joint distribution p(y, z) can formally be written down as marginalization over the latent space x as:</p><formula xml:id="formula_1">p(y, z) = p(y (1) |x)p(y (2) |x)p(z|x)p(x)dx,<label>(1)</label></formula><p>where we exploited the property of conditional independence, i.e., {y (1) , y (2) , z} are independent given x. For the non-linear conditional models, which we propose in Sec.3.2, the integral in Eq.( <ref type="formula" target="#formula_1">1</ref>) cannot be computed analytically. To this end, we numerically approximate the marginal likelihood using MC sampling</p><formula xml:id="formula_2">p(y, z) ≈ 1 S S s=1 p(y (1) |x s )p(y (2) |x s )p(z|x s ),<label>(2)</label></formula><p>where the samples x s , s = 1, . . . , S are drawn from p(x), which is defined in Sec.3.2. Using the Bayes' rule, we can derive the posterior of the model as:</p><p>p(x|y (1) , y (2) , z) = p(z|x)p(y (1) , y (2) |x)p(x)</p><p>1 S S s=1 p(y (1) , y (2) |x s )p(z|x s )</p><p>.</p><p>(</p><formula xml:id="formula_3">)<label>3</label></formula><p>We can now calculate the above probability for all pairs of training data i and MC latent samples s, to obtain the membership probabilities p(s, i) = p(x s |y</p><p>i , y</p><p>i , z i ). This gives rise to the expectation of the latent points:</p><formula xml:id="formula_6">x i = E{x|y (1) i , y (2) i , z i } = S s=1 p(s, i)x s .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Conditional Models</head><p>The choice of conditional models p(y (v) |x), v = 1, 2, and p(z|x), as well as the sampling distribution p(x), in Eq.( <ref type="formula" target="#formula_3">3</ref>) critically affect the representational capacity of the space, and thus, the model's performance. Effectively, this boils down to learning conditional models that provide: (i) generative mappings from latent space to the inputs (x → y (v) , v = 1, 2), (ii) projection mappings from the inputs to latent space (y (v) → x), and (iii), discriminative mappings from latent space to multiple binary outputs (x → z). Generative mappings. Different probabilistic models such as Gaussian models <ref type="bibr" target="#b2">[3]</ref> or naive Bayes models <ref type="bibr" target="#b18">[19]</ref> can be employed to recover the generative mappings. However, these parametric models are limited in their ability to recover non-linear mappings from the latent space to highdimensional input features, and the other way around. To this end, we exploit the framework of GP <ref type="bibr" target="#b20">[21]</ref>, which allows us to model arbitrary data structures via suitable choice of a kernel function. We briefly describe GPs below.</p><p>Given a collection of latent points X and corresponding outputs, e.g., Y (v) , we seek to find mapping f : X → Y (v) . By placing a GP prior over f , we can integrate it out <ref type="bibr" target="#b20">[21]</ref>. Then, the marginal distribution over the outputs is:</p><formula xml:id="formula_7">p(Y (v) |X, θ Y (v) ) = 1 (2π) N Dv |K Y (v) | Dv exp(- 1 2 tr((K Y (v) ) -1 Y (v) (Y (v) ) T )),<label>(5)</label></formula><p>where K Y (v) is N × N kernel matrix, obtained by applying the covariance function k(x, x ′ ) to elements of X, and it is assumed to be shared across the dimensions of Y (v) . The covariance function is usually chosen as the sum of the radial basis function (RBF) kernel, bias and noise terms</p><formula xml:id="formula_8">k(x, x ′ ) = θ 1 exp(- θ 2 2 x -x ′ 2 ) + θ 3 + δ x,x ′ θ 4 ,<label>(6)</label></formula><p>where δ x,x ′ is the Kronecker delta function, and θ Y (v) = (θ 1 , θ 2 , θ 3 , θ 4 ) are the kernel hyperparameters. The parameter learning is performed by gradient-based minimization of <ref type="bibr" target="#b20">[21]</ref>. Then, conditional probability for new inputs x * has the Gaussian form p(y</p><formula xml:id="formula_9">-log(p(Y (v) |X, θ Y (v) )) w.r.t. θ Y (v)</formula><formula xml:id="formula_10">(v) * |x * , X, Y (v) ) = N (µ y (v) * , σ y (v) * )<label>(7)</label></formula><formula xml:id="formula_11">µ y (v) * = k T * (K Y (v) + σ 2 I) -1 Y (v)<label>(8)</label></formula><formula xml:id="formula_12">σ y (v) * = k * * + k T * (K Y (v) + σ 2 I) -1 k * + σ 2 . (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>The kernel values k * and k * * are computed by applying Eq.( <ref type="formula" target="#formula_8">6</ref>) to (X, x * ) and (x * , x * ), respectively, and σ is the noise on the outputs. We use the conditional model in Eq.( <ref type="formula" target="#formula_10">7</ref>) to represent p(y (v) |x), v = 1, 2, in Eq.(3).</p><p>Projection mappings and sampling. To model the sampling distribution p(x), the simplest choice is to assume a Gaussian prior over the latent points x. However, sampling from such an uninformative prior, would give rise to latent representations that do not exploit the true nature of the input data. To ameliorate this, we define the sampling distribution so that it constraints the samples x s by conditioning them on the inputs, i.e., p(x) = p(x|y (1) , y (2) ). This is motivated by the notion of back-constraints in <ref type="bibr" target="#b11">[12]</ref>, where this type of conditional distribution is used to learn the mappings from input to latent space, and also ensures that distances between the outputs (in our case, {y (1) , y (2) }) are preserved in the manifold. We learn the conditional model for p(x) using GPs, as done for the generative mappings. The use of GP in the projection mappings allows us to easily combine multiple features within its kernel matrix as</p><formula xml:id="formula_14">K X = K (1) X + K<label>(2)</label></formula><p>X , corresponding to the sum of the kernel functions defined on y (1) and y (2) , respectively. The resulting conditional model p(x * |y (1) * , y</p><p>(2) * ), which is the Gaussian distribution as in Eq.( <ref type="formula" target="#formula_10">7</ref>), is used for sampling. Discriminative mappings. Since we are interested in detection of activations of multiple AUs, we use the logistic function <ref type="bibr" target="#b20">[21]</ref> to model p(z|x). By assuming conditional independence given x, we can factorize this conditional as: In the case of multi-class outputs (e.g., when modeling AU intensities), the class conditional in Eq.( <ref type="formula" target="#formula_16">11</ref>) should be modeled with multiple logistic functions.</p><formula xml:id="formula_15">p(z|x, W) = p(z (1) |x, w 1 ) . . . p(z (C) |x, w C ),<label>(10)</label></formula><formula xml:id="formula_16">p(z (c) |x, w c ) = 1 1 + e -x T wcz (c) , c = 1, . . . , C,<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Output Relational Constraints</head><p>Due to the possible large number of outputs, the topology of the latent space need to be constrained in order to avoid the model focusing on unimportant variation in the data. We need, also, to encourage the model to produce similar predictions for likely co-occurring outputs (e.g., AU6+12), and dissimilar for some rarely co-occurring (e.g., AU12 and AU17). Below we describe the construction of appropriate constraints based on the output relations, and how these are incorporated into the MC-LVM as additional regularizers. Topological constraints. Herein, we define constraints that encode co-occurrences of the output labels using the notion of the graph Laplacian matrix <ref type="bibr" target="#b4">[5]</ref>. The latter is defined as L = D -S, where S is a N × N similarity matrix, and D is a diagonal matrix with D ii = j S ij . We define S in a supervised fashion by measuring the similarity between the output label vectors using string kernels <ref type="bibr" target="#b20">[21]</ref> as:</p><formula xml:id="formula_17">S(x, x ′ ) = l∈A z T l,x z l,x ′ , (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>where A is the set of all possible 2 C combinations of sublabels l for a given latent position, and z l,x denotes the number of times l appears in labels z of x. Note that by accounting for all sub-labels, we measure the similarity of the outputs based on all possible groups of AUs, and not only on pairs. Then, using the expectation of the latent positions from Eq. ( <ref type="formula" target="#formula_6">4</ref>), we arrive at the Laplacian regularization term:</p><formula xml:id="formula_19">C = tr(X T LX) = N i,j S s=1 S t=1</formula><p>L ij p(s, i)p(t, j)x T s x t .</p><p>(13) Eq. ( <ref type="formula">13</ref>) incurs higher penalty if latent projections of cooccurring AUs are distant in the latent space. Global relational constraint. In order for the MC-LVM to fully benefit from the above topological constraint, it is important to ensure that the model will produce similar predictions for frequently co-occurring AUs. For this, we introduce the global relational regularizer as:</p><formula xml:id="formula_20">R = P T z P z -Z T 0 Z 0 2 F ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_21">P z = [p(z 1 |x 1 ), . . . , p(z N |x N )]</formula><p>T are the predictions from Eq.( <ref type="formula" target="#formula_16">11</ref>) for each x i from Eq.( <ref type="formula" target="#formula_6">4</ref>), and Z 0 1 is the true label set. Thus, the regularizer in Eq.( <ref type="formula" target="#formula_20">14</ref>), incurs a high penalty if correlated outputs have dissimilar predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning and Inference</head><p>The objective function of our model is the sum of the complete data log-likelihood of the (weighted) joint distribution in Eq.( <ref type="formula" target="#formula_2">2</ref>) penalized by the constraints in Eq. <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b13">14</ref>)</p><formula xml:id="formula_22">L(Θ) = N i=1 log S s=1</formula><p>p(y (1) , y (2) |xs)</p><formula xml:id="formula_23">p g,i 1-α p(z|xs) p d,i α -λC C-λRR,<label>(15)</label></formula><p>where Θ = {θ Y (v) , W}. Note that, in contrast to the standard ML optimization, we use the parameter α ∈ [0, 1] to find an optimal balance between the generative (p g,i ) and discriminative (p d,i ) components, as commonly used in multi-conditional models <ref type="bibr" target="#b18">[19]</ref>. The generative component has the key role to unravel the latent space of the fused features, while the discriminative component regularizes the manifold by inducing to the space information regarding the outputs' relations. By finding optimal α, we restructure the joint likelihood by allowing the model to concentrate its modeling power on a conditional distribution of interest.</p><p>To optimize the objective in Eq.( <ref type="formula" target="#formula_23">15</ref>), we propose an EMbased approach for parameter learning. In the E-step, we find the expectation of the complete-data log-likelihood in Eq.( <ref type="formula" target="#formula_23">15</ref>) under the posterior in Eq.( <ref type="formula" target="#formula_3">3</ref>), which is given by</p><formula xml:id="formula_24">Q(Θ, Θ (old) ) = N i=1 S s=1 p(s, i) log p 1-α g,i p α d,i ,<label>(16)</label></formula><p>1 The subscript 0 indicates the negative class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 MC-LVM: Learning and Inference</head><p>Learning Inputs: D = (Y (v) , Z), v = 1, . . . , V Initialize X using PCA. repeat Stage 1 Learn p(x) = p(x|y (1) , y (2) ) by training the specified GP. Draw S latent variables xs from p(x) Stage 2 E-step: Use the current estimate of the parameters Θ (old)  to compute the membership probabilities in Eq. ( <ref type="formula" target="#formula_3">3</ref>). M-step: Update Θ by maximizing Eq. ( <ref type="formula" target="#formula_25">17</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 3</head><p>Update the latent space using Eq. ( <ref type="formula" target="#formula_6">4</ref>) until convergence of Eq. ( <ref type="formula" target="#formula_25">17</ref>) Outputs: X, Θ Inference Inputs: y (1)   * , y (2) *</p><p>Step 1: Find the projection x * to the latent space using Eq. ( <ref type="formula" target="#formula_11">8</ref>).</p><p>Step 2: Apply the logistic functions from Eq. ( <ref type="formula" target="#formula_16">11</ref>) to the obtained embedding to compute the outputs z * .</p><p>Output: z * where the membership probabilities, p(s, i), are computed with Θ (old) . In the M-step, we find Θ (new) by optimizing</p><formula xml:id="formula_25">Θ (new) = arg max Θ Q(Θ, Θ (old) ) -λ C C -λ R R,<label>(17)</label></formula><p>w.r.t. Θ using the conjugate gradient method <ref type="bibr" target="#b20">[21]</ref>.</p><p>The full training of the model is split into two stages, where in each stage we compute p(x|y (1) , y (2) ) and p(y (1) , y (2) , z|x) alternatively. First, we initialize the latent coordinates X, using a dimensionality reduction method, e.g., PCA. Then, we learn the sampling distribution p(x|y (1) , y (2) ) by training a GP on the projection mappings, as explained in Sec.3.2, and collect S samples from the GP posterior. During the second stage, we employ the EM algorithm described above to learn the parameters Θ. Note that the constraints C and R implicitly depend on the posterior, which is a function of the current Θ, hence, we need to compute their derivatives w.r.t to Θ. Eq.( <ref type="formula" target="#formula_25">17</ref>) can be optimized jointly <ref type="bibr" target="#b2">[3]</ref> or separately <ref type="bibr" target="#b9">[10]</ref> without violating the EM-optimization scheme, since the updates from the penalty terms do not affect the computation of the expectation. After the M-step we refine our original estimate of the latent space X, using Eq.( <ref type="formula" target="#formula_6">4</ref>). We iterate between stage 1 and 2 until convergence of the objective function in Eq. <ref type="bibr" target="#b16">(17)</ref>. Inference: Inference in MC-LVM is straightforward. The test data y (1) * , y</p><p>(2) * , are first projected onto the manifold using Eq.( <ref type="formula" target="#formula_10">7</ref>). In the second step, the activation of each output is detected by applying the classifiers from Eq.( <ref type="formula" target="#formula_16">11</ref>) to the obtained latent position. All this is summarized in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate the proposed model on three publicly available datasets: Extended Cohn-Kanade (CK+) <ref type="bibr" target="#b12">[13]</ref>, UNBC-McMaster Shoulder Pain Expression Archive (Shoulderpain) <ref type="bibr" target="#b14">[15]</ref>, and Denver Intensity of Spontaneous Facial Actions (DISFA) <ref type="bibr" target="#b17">[18]</ref>. These are benchmark datasets of posed (CK+), and spontaneous (Shoulder-pain, DISFA) data, containing a large number of FACS coded AUs. Specifically, CK+ contains 593 video recordings of 123 subjects displaying posed facial expressions in frontal views. The Shoulderpain dataset contains video recordings of 25 patients suffering from chronic shoulder pain while performing a range of arm motion tests. Each frame is coded in terms of AU intensity on a six-point ordinal scale. DISFA contains video recordings of 27 subjects while watching YouTube videos. Again, each frame is coded in terms of the AU intensity on a six-point ordinal scale. For both DISFA and Shoulder-pain we treated each AU with intensity larger than zero as active. Fig. <ref type="figure" target="#fig_1">2</ref> depicts the AU relations, and the distribution of the AU activations for the data used from each dataset. Features: From images in each dataset, 49 fiducial facial points were extracted using the 2D Active Appearance Model <ref type="bibr" target="#b16">[17]</ref>. Based on these points, we registered the images to a reference face (average for each dataset) using an affine transformation. As input to our model, we used both geometric features, i.e., the registered facial points (feature set I), and appearance features, i.e., Local Binary Patterns (LBP) histograms <ref type="bibr" target="#b19">[20]</ref> (feature set II) extracted around each facial point from regions of 32×32 pixels. We chose these features as they showed good performance in variety of AU recognition tasks <ref type="bibr" target="#b23">[24]</ref>. To reduce the dimensionality of the extracted features we applied PCA, retaining 95% of the energy. This resulted in approximately 20D (geometric) and 40D (appearance) feature vectors, for each dataset. Evaluation procedure. We evaluate MC-LVM on a subset of highly correlated AUs, i.e., AUs <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17)</ref> for CK+, AUs <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17)</ref> for DISFA and AUs <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr">43)</ref>, that according to the Prkachin and Solomon formula <ref type="bibr" target="#b14">[15]</ref>, are associated with pain. We report F1 score as the performance measure. In all our experiments, we performed 5 fold subject independent cross validation. Models compared. We compare the proposed MC-LVM to GP methods with different learning strategy. Specifically, we compare to the manifold relevance determination (MRD) <ref type="bibr" target="#b5">[6]</ref>, which uses the variational approximation, and the discriminative shared GP latent variable model (DS-GPVLM) <ref type="bibr" target="#b7">[8]</ref> and multi-task latent GP (MT-LGP) <ref type="bibr" target="#b27">[28]</ref>, which perform exact ML learning. We also compare to the multi-label backpropagation and kNN (k=1), i.e. the BPMLL <ref type="bibr" target="#b30">[31]</ref> and ML-KNN <ref type="bibr" target="#b31">[32]</ref>. Lastly, we compare to the state-of-the-art methods for multiple AU detection: hierarchical RBM (HRBM) <ref type="bibr" target="#b28">[29]</ref>, l p -regularized multitask MKL (l p -MTMKL) <ref type="bibr" target="#b34">[34]</ref> and joint patch multi-label learning (JPML) <ref type="bibr" target="#b35">[35]</ref>. For the single input methods, we concatenated the two feature sets. For the kernelbased methods, we used the RBF kernel (in l p -MTMKL we also used the polynomial kernel). Due to the high learning complexity of l p -MTMKL, we followed the training scheme in <ref type="bibr" target="#b34">[34]</ref> where AUs were split into groups:</p><formula xml:id="formula_26">{{AU 1, AU 2, AU 4}, {AU 6, AU 7, AU 12}, {AU 15, AU 17}}</formula><p>for CK+, the same groups (without AU7) for DISFA, and {AU 4, AU 43, AU 7}, {AU 6, AU 9, AU 10} for Shoulder-pain. The parameters of each method were tuned as described in the corresponding papers. For the MC-LVM, optimal α and λ C , λ R parameters, as well as the size of the latent space (set to 8D) were found via a validation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Results</head><p>Fig. <ref type="figure" target="#fig_2">3</ref> (left) shows the convergence of the learning criterion in MC-LVM as a function of the used samples during training on the CK+ dataset. We see that for small number of samples, the model does not converge to a minimum. This is expected, since with few samples (100 -500) the posterior in Eq.( <ref type="formula" target="#formula_3">3</ref>) cannot be approximated well. By increasing the number of samples to 1000, the model converges, and does not change considerably after that. Thus, we fixed the number of samples to 1000. Fig. <ref type="figure" target="#fig_2">3</ref> (right) shows the effect of changing α on the discriminative power of the model, for all three datasets. We observe that the model prefers a weighted conditional distribution, than fully selecting the generative/discriminative component. The optimal value of α is 0.6 for the posed, and 0.8 for the spontaneous data. This difference is because in the case of the naturalistic data (DISFA, Shoulder-pain), the model puts less focus on explaining the unnecessary (for the AU detection) variations (e.g., head pose) of the input features. Therefore, the influence of the generative component is lower (higher α) than in the case of the posed expressions from CK+. In Fig. <ref type="figure" target="#fig_3">4</ref> (left) we see the effect of the introduced relational constraints on the model's performance, on all three datasets. At first we observe that when no regularization is used (λ C , λ R = 0), MC-LVM achieves the lowest performance for both posed and spontaneous data. By including the topological constraint (λ C = 0), MC-LVM unravels a better representation of the data in the manifold, which results in higher F1 scores. Finally, with the addition of the global relational constraint (λ C , λ R = 0) MC-LVM achieves the highest scores. Note that the difference is more pronounced in data from DISFA and Shoulder-pain, which evidences the importance of modeling the global relations for the detection of spontaneous (more subtle) AUs. We continue by evaluating the effectiveness of the proposed MC-LVM on the feature fusion task. To this end, we learn the MC-LVM in single, and multi-input settings. Fig. <ref type="figure" target="#fig_3">4</ref> (right) shows the average performance of the model on all three datasets, for the different feature combinations. In the single input case, we observe that, on average, geometric features (I) outperform the appearance features (II) (apart from DISFA where features (I) suffer from large variations in head pose) in the task of multiple AU detection. This is because, by concatenating the histograms obtained from each patch, the local information of the data is lost, and thus, the model obtains lower scores. However, when both inputs are used, MC-LVM can unravel a shared latent space with fused information from the global geometrical descriptors and the local patch-related histograms. This results in the highest F1 score, with significant improvement on the spontaneous data of DISFA and Shoulder-pain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model Comparisons on Posed Data</head><p>We next compare the proposed MC-LVM to several state-of-the-art methods on the posed data from CK+. We first inspect the performance of MC-LVM and the GPrelated methods on the target task. From Table <ref type="table" target="#tab_0">1</ref>, the MLbased methods, i.e., the MT-LGP <ref type="bibr" target="#b27">[28]</ref> and DS-GPLVM <ref type="bibr" target="#b7">[8]</ref>, achieve similar performance on average and per AU, since they are based on the same learning scheme. On the other hand, MRD <ref type="bibr" target="#b5">[6]</ref>, uses a variational distribution to approximate a manifold shared across multiple inputs and outputs,  without any constraints over the latent variables. By contrast, the combination of the approximate learning with the relational constraints used in the proposed MC-LVM results in a significant increase in the performance. We partly attribute this to the explicit modeling of AU co-occurrences through the introduced constraints, as well as the multiconditional learning using the proposed sampling scheme. The importance of the latter is further evidenced in the performance of the single output instance of MC-LVM, which for the case of the posed data it achieves comparable scores to the multi-output. Finally, the state-of-the-art models for joint AU detection, i.e. the HRBM and l p -MTMKL, improve the detection of specific AUs (AU1,6). Yet, they achieve lower results compared to the proposed MC-LVM. HRBM cannot handle simultaneously the fusion of the concatenated features and the modeling of the AU dependencies using binary latent variables. l p -MTMKL, due to its modeling complexity, it is trained on subsets of AUs (as mentioned above) which affects its ability to capture all AU relations. More importantly, in contrast to MC-LVM, these two models lack the generative component, which, evidently, acts as a powerful regularizer. The results of JPML were obtained from <ref type="bibr" target="#b35">[35]</ref>, thus, they are not directly comparable to the other models. Yet, we report its performance as a reference to the state-of-the-art. The baseline models, BPMLL and ML-KNN, report the lower average scores.</p><p>To demonstrate the model's scalability when dealing with large number of outputs, we compare the proposed approach to the state-of-the-art HRBM for joint AU detection on all 17 AUs from CK+ (l p -MTMKL cannot be evaluated on this experiment due to its learning complexity). As we can see in Table <ref type="table" target="#tab_1">2</ref>, modeling of the remaining (less frequent) AUs affects the overall performance of both MC-LVM and HRBM, which suffer a drop of 8.6% and 7.6%, respectively. However, MC-LVM outperforms HRBM on 14 out of 17 AUs, which demonstrates the ability of the former to better model the relations among AUs, even in a larger scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Model Comparisons on Spontaneous Data</head><p>We further investigate the models' performance on spontaneous data from DISFA and Shoulder-pain. We focus here on the best performing methods from Table <ref type="table" target="#tab_0">1</ref>. From Table <ref type="table" target="#tab_2">3</ref>, we observe a significant drop in the performance of all methods on both datasets. This evidences the difficulty of the task of AU detection in realistic environments, and demonstrates the difference between posed and spontaneous expressions. We also observe from Figure <ref type="figure" target="#fig_1">2</ref> that the distribution of the activated AUs is more imbalanced than the posed dataset. This imposes an additional challenge since data for certain AUs (e.g., AU2,15 for DISFA, and AU9,10 for Shoulder-pain) are limited compared to others, and thus, the models need to give more emphasis on the AU co-occurrences for their detection. Hence, the single output MC-LVM reports low scores for the aforementioned AUs in both datasets. On the other hand, with limited data the modeling of the global AU relations is even harder task. HRBM is adversely affected by this issue, and it performs close to the single output model. l p -MTMKL, reports even lower results (especially in Shoulder-pain), due to not modeling global relations. MT-LGP fails to model explicitly the relations between AUs, resulting in low scores. On the other hand, it is evidenced that the proposed MC-LVM is more robust to the data imbalance, and can better discover the AU relations, which in turn gives the best average scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusions</head><p>We proposed a novel multi-conditional latent variable model that exploits successfully the non-parametric probabilistic framework of GPs to perform multi-conditional sub-space learning for efficient feature fusion and joint AU detection. By assuming conditional independence given the subspace of AUs, MC-LVM allows each feature set to be described via feature-specific GPs, resulting in more accurate fusion in the manifold, and hence, more discriminative features for the detection task. More importantly, the newly introduced multi-conditional objective allows the generative and discriminative costs of the model to act in concert -the generative component has the key role to unravel the shared subspace of different feature sets, while the discriminative component endows the subspace with the relational information about the output labels. Consequently, this enables MC-LVM to learn the structure of a discriminative subspace that is optimized for multiple AU detection, while being effectively regularized by the generative component. We demonstrated the effectiveness of these properties on three publicly available datasets by showing that the proposed model outperforms the existing works for multiple AU detection, and several methods for feature fusion and multi-label learning. Finally, we showed that the proposed MC-LVM scales well with a large number of AUs, without significant increase in its computational complexity.</p><p>As evidenced by our experiments, the proposed joint inference improves detection of most AUs and the overall performance. Yet, sometimes this results in decreased detection performance on other AUs, when compared to single output AU detectors. It would be interesting to investigate how the subsets of strongly correlated AUs could efficiently be detangled by learning subset-specific subspaces within the proposed framework. Also, automatic balancing of the conditional distributions in the model is another direction to pursue. These are going to be the focus of our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>where W = [w 1 , . . . , w C ] ∈ R q×C contains the weight vectors of the individual functions. During inference, if p(z (c) * |x * ) &gt; 0.5, the c-th output is active, i.e., z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The global AU relations (in terms of correlation coefficients) (upper row), and the distribution of the AU activations within the used datasets (lower row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The cost function of the MC-LVM for different number of samples used to estimate the posterior (left), and average F1 score for multiple AU detection as a function of the regularization parameter α (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Average F1 score on all three datasets for different settings of the proposed MC-LVM. The effect of the relational constraints (left), and the feature fusion (right) to the joint AU detection task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>F1 score for joint AU detection on CK+ dataset.</figDesc><table><row><cell cols="4">Methods (I+II) AU1 AU2 AU4 AU6 AU7 AU12 AU15 AU17 Avg.</cell></row><row><cell>MC-LVM</cell><cell cols="3">84.39 86.55 81.60 68.42 61.67 88.48 82.54 87.40 80.14</cell></row><row><cell cols="4">MC-LVM (SO) 86.06 88.37 82.93 70.80 57.27 87.16 73.26 85.57 78.93</cell></row><row><cell>MRD [6]</cell><cell cols="3">80.72 79.18 69.93 69.81 53.24 77.83 65.70 85.20 72.70</cell></row><row><cell>MT-LGP [28]</cell><cell cols="3">89.12 83.70 79.79 67.16 60.89 80.53 64.63 85.97 76.47</cell></row><row><cell cols="4">DS-GPLVM [8] 87.41 81.78 79.70 68.48 63.29 81.04 60.33 84.29 76.17</cell></row><row><cell>HRBM [29]</cell><cell cols="3">87.62 84.00 74.10 62.90 50.74 82.38 66.06 84.56 74.04</cell></row><row><cell cols="4">lp-MTMKL [34] 87.50 85.50 51.43 72.65 58.82 85.95 74.21 75.44 73.93</cell></row><row><cell>BPMLL [31]</cell><cell cols="3">75.41 84.31 64.85 69.14 64.34 83.98 69.50 76.25 73.47</cell></row><row><cell cols="4">ML-KNN [32] 76.83 84.34 63.28 67.23 53.19 82.88 65.88 78.71 71.54</cell></row><row><cell>JPML  *  [35]</cell><cell>91.2 96.5</cell><cell>-</cell><cell>75.6 50.9 80.4 76.8 80.1 78.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>F1 score for joint AU detection (all 17) on CK+ dataset. Comparison to state-of-the-art. .96 79.16 73.47 72.80 57.52 87.94 31.11 87.60 76.40 86.76 70.27 67.27 51.02 91.81 21.05 91.14 71.45 HRBM [29] 86.86 85.47 72.58 72.04 61.74 54.47 85.91 26.51 72.65 72.53 81.66 47.46 56.64 35.29 92.57 37.61 87.65 66.45</figDesc><table><row><cell cols="2">Methods (I+II) AU1 AU2 AU4 AU5 AU6 AU7 AU9 AU11 AU12 AU15 AU17 AU20 AU23 AU24 AU25 AU26 AU27 Avg.</cell></row><row><cell>MC-LVM</cell><cell>82.49 86</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>F1 score for joint AU detection on DISFA and Shoulder-pain datasets. MTMKL [34] 42.21 45.81 47.18 62.79 76.33 34.47 41.40 50.03 37.69 97.75 70.08 33.28 41.79 44.03 54.10</figDesc><table><row><cell>Methods (I+II)</cell><cell>DISFA dataset AU1 AU2 AU4 AU6 AU12 AU15 AU17 Avg. AU4 AU6 AU7 AU9 AU10 AU43 Avg. Shoulder-pain dataset</cell></row><row><cell>MC-LVM</cell><cell>58.55 62.99 72.85 52.32 84.74 49.44 48.63 61.36 47.20 97.75 67.88 37.13 58.23 72.51 63.45</cell></row><row><cell cols="2">MC-LVM (SO) 35.50 52.68 70.99 54.67 82.58 37.11 47.76 54.47 57.76 95.57 63.59 34.54 49.93 64.49 60.98</cell></row><row><cell>MT-LGP [28]</cell><cell>41.44 36.84 61.19 45.98 49.78 40.12 43.01 45.48 50.42 50.48 63.52 33.38 61.62 61.00 53.40</cell></row><row><cell>HRBM [29]</cell><cell>39.67 55.92 61.56 54.01 79.16 38.72 38.82 52.55 47.20 93.93 63.67 29.80 52.39 69.54 59.42</cell></row><row><cell>lp-</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work has been funded by the <rs type="funder">European Community Horizon 2020</rs> [<rs type="grantNumber">H2020/2014-2020</rs>] under grant agreement no. <rs type="grantNumber">645094</rs> (SEWA). The work by <rs type="person">S. Eleftheriadis</rs> is further supported by the <rs type="funder">European Community 7th Framework Programme</rs> [FP7/2007-2013] under grant agreement no. <rs type="grantNumber">611153</rs> (TERESA).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_cywPdqt">
					<idno type="grant-number">H2020/2014-2020</idno>
				</org>
				<org type="funding" xml:id="_PPrbjBX">
					<idno type="grant-number">645094</idno>
				</org>
				<org type="funding" xml:id="_pweGPXk">
					<idno type="grant-number">611153</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Thin slices of expressive behavior as predictors of interpersonal consequences: A metaanalysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ambady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">256</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognizing facial expression: machine learning and application to spontaneous behavior</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lainscsek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="568" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supervised spectral latent variable models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conf. on AISTATS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Selective transfer machine for personalized facial action unit detection</title>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D L</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3515" to="3522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Spectral graph theory</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Manifold relevance determination</title>
		<author>
			<persName><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Facial action coding system</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>A Human Face</publisher>
			<pubPlace>Salt Lake City, UT</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative shared gaussian processes for multiview and view-invariant facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eleftheriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="189" to="204" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Regularized multi-task learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Laplacian regularized gaussian mixture model for data clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1406" to="1418" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A dynamic texturebased approach to recognition of facial actions and their temporal models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Koelstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1940" to="1954" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Local distance preservation in the gp-lvm through back constraints</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Candela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on CVPR&apos;W</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatically detecting pain in video through facial action units</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Howlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Prkachin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on SMCB</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="664" to="674" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Part B</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Painful data: The UNBC-McMaster shoulder pain expression archive database</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Prkachin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE Int&apos;l Conf. on AFGR</title>
		<imprint>
			<biblScope unit="page" from="57" to="64" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Facial action unit recognition with sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Veon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conf. on AFGR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="336" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Active appearance models revisited</title>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="164" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Disfa: A spontaneous facial action intensity database</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TAC</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiconditional learning: Generative/discriminative training for clustering and classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Druck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">433</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT press</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kernel conditional ordinal random fields for temporal segmentation of facial action units</title>
		<author>
			<persName><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;W12</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="260" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Handbook of methods in nonverbal behavior research</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Facial action recognition combining heterogeneous features via multikernel learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Senechal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Salam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Seguier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Prevost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on SMCB</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="993" to="1005" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Part B</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A literature survey on algorithms for multilabel learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sorower</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Corvallis</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Oregon State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Facial action unit recognition by exploiting their dynamic and semantic relationships</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1683" to="1699" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-label classification: An overview</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJDWM</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Transferring nonlinear representations using gaussian processes with a shared latent space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>MIT-CSAIL- TR-08-020</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Capturing global semantic relationships for facial action unit recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Flexible latent variable models for multi-task learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="221" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multilabel neural networks with applications to functional genomics and text categorization</title>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1338" to="1351" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ml-knn: A lazy learning approach to multi-label learning</title>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2038" to="2048" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Simultaneous detection of multiple facial action units via hierarchical task structure learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m">IEEE ICPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1863" to="1868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An lp-norm MTMKL framework for simultaneous detection of multiple facial action units</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE WACV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1104" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint patch and multi-label learning for facial action unit detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2207" to="2216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multiple-facial action unit recognition by shared feature learning and semantic relation modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1663" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
