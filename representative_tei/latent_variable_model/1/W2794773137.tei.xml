<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Intervention and Identifiability in Latent Variable Modelling</title>
				<funder>
					<orgName type="full">Leverhulme Trust</orgName>
				</funder>
				<funder>
					<orgName type="full">UK Arts and Humanities Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2018-03-30">30 March 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jan-Willem</forename><surname>Romeijn</surname></persName>
							<email>j.w.romeijn@rug.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Philosophy</orgName>
								<orgName type="institution">University of Groningen</orgName>
								<address>
									<addrLine>Oude Boteringestraat 52</addrLine>
									<postCode>9712 GL</postCode>
									<settlement>Groningen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jon</forename><surname>Williamson</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Philosophy Cornwallis North West</orgName>
								<orgName type="institution">University of Kent</orgName>
								<address>
									<postCode>CT2 7NF</postCode>
									<settlement>Canterbury</settlement>
									<region>Kent</region>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Intervention and Identifiability in Latent Variable Modelling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-03-30">30 March 2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s11023-018-9460-y</idno>
					<note type="submission">Received: 6 February 2017 / Accepted: 22 February 2018 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Interventions</term>
					<term>Statistical inference</term>
					<term>Identifiability</term>
					<term>Latent variable modelling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the use of interventions for resolving a problem of unidentified statistical models. The leading examples are from latent variable modelling, an influential statistical tool in the social sciences. We first explain the problem of statistical identifiability and contrast it with the identifiability of causal models. We then draw a parallel between the latent variable models and Bayesian networks with hidden nodes. This allows us to clarify the use of interventions for dealing with unidentified statistical models. We end by discussing the philosophical and methodological import of our result.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A statistical model may include hypotheses that have identical likelihood functions over the entire sample space. This is the problem of statistical identifiability: several statistical hypotheses fit the data equally well, hence we cannot identify the best one by data alone. So-called unidentified models exhibit a form of underdetermination, though not the radical form that often features in arguments against scientific realism. The standard response to underdetermination is to look for theoretical criteria, such as simplicity or explanatory force, that help us choose between the rivals. In factor analytic models, for example, one might use criteria pertaining to the variation among the estimations of the statistical parameters to force a unique solution of the estimation of factor loadings.</p><p>In this paper we investigate a particular solution to the problem of statistical identifiability in the context of causal modelling. Given the context, let us stress that the statistical identifiability problem must not be confused with the problem of identifying so-called causal effects (cf. Pearl 2000, chapter 3). The latter concerns the determination of how a system responds to interventions, i.e., determining causal structure. Statistical identifiability is different because it does not involve uncertainty about causal structure. Instead it concerns the determination of statistical parameters within a model whose causal structure is fully specified. It occurs when the statistical hypotheses under consideration say the very same things about what observations to expect, i.e., they have exactly the same likelihood functions and thus perform equally well on the observed data.</p><p>That said, the solution that we investigate does rely on the causal interpretation of the statistical models. In fact, the solution assumes that certain aspects of the causal model are known, and therefore that the problem of causal identifiability has to some extent been resolved. It trades on the fact that the otherwise identical statistical hypotheses need not be equivalent in a causal sense. We can consider specific changes to the setup of the study, i.e., specific interventions, such that the hypotheses get different likelihood functions over the additional results. The hypotheses are then told apart by their differing causal content. For this solution to work, we need to presume that we have already determined how the system behaves after intervention.</p><p>Our solution to statistical identifiability conveys two messages. The first is philosophical: we want to bring to the fore an important and, to our mind, undervalued aspect of scientific confirmation, namely the use of intervention data. We believe that insights from the philosophy of experiment (e.g. Hacking 1980;  Gooding 1990) can come to fruition in confirmation theory and we hope to make a modest start with that. A further message is methodological: we hope to contribute to a better understanding of the benefits of interventions and stimulate the uptake of statistical tools for modeling interventions in social science. Despite the availability of statistical theories and methodological tools for exploiting intervention data, scientists are often not aware of their potential. Moreover, insofar as there is awareness, this mostly concentrates on the identification of causal effects or the use of intervention data for determining causal structure (e.g., Spirtes et al. 2001;  Eberhardt et al. 2010; Hyttinen et al. 2012; Silva and Scheines 2003). This paper suggests a different use of intervention data.</p><p>We present our argument in the setting of latent variable modelling, a statistical modelling tool from the social sciences that remains understudied in the philosophy of science, with one or two exceptions. Johnson (2014) offers a wonderful overview of the philosophical import of factor analysis in connection to the problem of underdetermination. Interestingly, although our papers target different problems and were written independently, they reach similar general conclusions. Factor analysis makes another appearance in Haig (2005) and Schurz (2008), namely as a model for abductive inference, and thus as a tool for generating and selecting theory. In this paper we take a different perspective. We employ exploratory factor analysis as an illustration of a more general problem concerning statistical unidentifiability, and we focus on the role of interventions in resolving it.</p><p>The paper is set up in the following way. In Sect. 2 we introduce statistical identifiability abstractly and in Sect. 3 we make these problems concrete for latent class analysis and factor analysis. We show in Sect. 4 that latent variable modelling is for our purposes identical to estimating parameters in a Bayesian network with hidden nodes. Just as is the case with causal Bayesian networks, data obtained after intervention can be used to identify features of models in factor analysis. In particular, we argue in Sect. 4.3 that intervention data can, under the right conditions, be used to resolve problems of statistical identifiability. In Sect. 5, finally, we briefly suggest how this model for intervention may prove useful to the philosophy of science in general.</p><p>We see the topic of this paper as an opportunity for a fruitful interaction between philosophers of science and social science methodologists. Our own expertise is first and foremost in the former: we mostly consider identifiability problems and causal models from an abstract point of view. Social science methodologists, on the other hand, regularly encounter such problems in practice. We believe that insights from the applications can shed valuable light on the theoretical problem. Similarly, we hope that our more theoretical insights will be of use to the methodologists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Unidentified Models</head><p>In what follows we characterize the problem of unidentified statistical models, and make it precise for latent class analysis (LCA), a well-known statistical technique in, e.g., psychometrics. LCA is a close cousin to factor analysis (FA). LCA and FA are both routinely used to interpret psychological test data, and working psychologists face the problem that the data often do not allow for a complete determination of the underlying classes or factors. This presents psychological science with its own version of the philosophical problem of underdetermination (cf. Johnson 2014).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Identifiability in Statistics</head><p>Here we illustrate the concept of statistical identifiability using some toy examples. A more realistic setting will be introduced in Sect. 2.2.</p><p>Consider a simple statistical problem, in which we estimate the chances of events in independent and identical trials, e.g., results in psychological tests. An observation at time t is denoted by the assignment of a value to a binary variable Q t , with possible values failing and passing the test. We denote a sequence of t observations or test results by means of the variable S t . For example, if</p><formula xml:id="formula_0">S t ¼ 010. . .1, then Q 1 ¼ 0, Q 2 ¼ 1,</formula><p>and so on. The hypothesis H h says that the chance of observing Q tþ1 ¼ 1 is h irrespective of which sequence of outcomes S t precedes it.</p><formula xml:id="formula_1">PðQ tþ1 ¼ 1jH h ; S t Þ ¼ h<label>ð1Þ</label></formula><p>for every S t and for each trial Q tþ1 , an expression involving what is often called the likelihood function of H h . <ref type="bibr">1</ref> The chance h of the event Q tþ1 ¼ 1 may be any value in [0, 1], so we have a whole continuum of hypotheses H h gathered in what we call a statistical model, denoted H. On the basis of some sequence of events S t , we can provide an estimate of h. We can do so either by defining a prior PðH h Þ and then computing a posterior by Bayesian conditioning, or by defining an estimator function over the event space, typically the observed relative frequency:</p><formula xml:id="formula_2">ĥðS t Þ ¼ 1 t X t i¼1 Q i ;</formula><p>The above estimation problem is completely unproblematic. The observations have a different bearing on each of the hypotheses in the model, i.e. each member of the set of hypotheses. If there is indeed a true hypothesis in the set, then according to well-known convergence theorems (cf. Earman 1992, pp. 141-149), the probability of assigning a probability 1 to this hypothesis will tend to one. In the limit, we can therefore almost always, in the technical sense of this expression, tell the statistical hypotheses apart.<ref type="foot" target="#foot_2">foot_2</ref> This situation is different if we take a slightly different set of statistical hypotheses G n , characterized as follows:</p><formula xml:id="formula_3">PðQ tþ1 ¼ 1jG n ; S t Þ ¼ n 2 ; n 2 ½À 1; 1:</formula><p>This set of hypotheses covers the same set of possibilities, only they are doubly labelled. The hypotheses G n and G Àn are indistinguishable, because they both assign exactly the same probability to all the observations:</p><formula xml:id="formula_4">PðQ tþ1 ¼ 1jG n \ S t Þ ¼ PðQ tþ1 ¼ 1jG Àn \ S t Þ.</formula><p>In such a case, we speak of an unidentifiable model. Notice that this situation is much like having a single equation with two unknowns, for instance x þ y ¼ 1 with x; y 2 ½0; 1. We cannot find a unique solution for x and y, rather we have a whole collection of solutions. To force uniqueness, we need a further equation, e.g., x À y ¼ 0. Unidentifiable models are in a sense underdetermined by the observations. Importantly, this kind of statistical underdetermination is not of the kind most feared by scientific realists, because there may well be experiments or additional observations that would allow one to disentangle the statistical hypotheses. This paper shows how additional experiments can achieve this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Latent Variable Models</head><p>The above example of statistical underdetermination is rather contrived: no reason is given for distinguishing between the regions n [ 0 and n\0. However, there are cases in which it makes perfect sense to introduce distinctions between hypotheses that do not differ in their likelihood functions. This subsection is devoted to presenting one of these cases, involving a so-called latent class model. The exposition is partly borrowed from [omitted for purpose of blind review].</p><p>A latent variable model posits hidden, or latent, random variables on the basis of an analysis of the correlational structure of observed, or manifest, random variables. Examples are latent class models, which are discussed below, and factor models, in which latent and manifest variables are continuous. <ref type="bibr">3</ref> Suppose that in some experiment we observe (continuously or discretely varying) levels of fear F and loathing L in a number of individuals who are represented via the index i, and we find a positive correlation between these two variables,</p><formula xml:id="formula_5">PðF i ; L i Þ [ PðF i ÞPðL i Þ:</formula><p>One way of accounting for the correlation is by positing a statistical model over the variables in which fear and loathing may be related directly.</p><p>We may feel that it is neither the loathing that instills fear in people, nor the fear that invites loathing. Instead we might think that both feelings are correlated because of a latent characteristic of the individuals, namely a depression from they might be suffering. Conditional on the level of the depression, denoted D i , fear and loathing might be uncorrelated:</p><formula xml:id="formula_6">PðD i ; F i ; L i Þ ¼ PðD i ÞPðF i jD i ÞPðL i jD i Þ:</formula><p>In the case in which all the variables vary continuously, we speak of a factor model. We then say that the depression is the common factor to the observable, or manifest, variables of fear and loathing, and the correlations between the depression variable and the levels of fear and loathing we call the factor loadings.</p><p>Latent variable models come in several shapes and sizes, subdivided according to whether the manifest and latent variables are categorical or continuous. In what follows we discuss one of the most straightforward applications of such models, in which both the manifest and latent variables are binary: latent class analysis. Our reason is that we are making a conceptual point about interventions and underdetermination. For this purpose the simplest format of factor analysis suffices.</p><p>To illustrate the latent class analysis, say that the depression is either present in subject i, D i ¼ 1, or absent, D i ¼ 0, and similarly for fear and loathing. We assume that over time the variables are independent and identically distributed. That is, for i 6 ¼ i 0 the variable D i is independent of D i 0 , F i 0 and L i 0 , and similarly for F i and L i .</p><p>Out of the possible probabilistic dependencies among F i , L i and D i , we confine ourselves to</p><formula xml:id="formula_7">PðF i ¼ 1jD i ¼ jÞ ¼ / j ;<label>ð2Þ</label></formula><formula xml:id="formula_8">PðL i ¼ 1jD i ¼ jÞ ¼ k j ;<label>ð3Þ</label></formula><p>for j ¼ 0; 1, a conditional version of the Bernoulli model of Eq. (1). Similarly for the variable D i ,</p><formula xml:id="formula_9">PðD i ¼ 1Þ ¼ d<label>ð4Þ</label></formula><p>The probability over the variables D i , L i and F i is thus given by five Bernoulli distributions, each characterized independently by a single chance parameter.</p><p>There may be experimental conditions in which the latent class that enhances or reduces fear and loathing is observable, e.g., when the individuals all take a drug E which reduces fear and loathing. But the depression variable D in our example is latent: it cannot be observed directly. Although the causal or mechanistic underpinning is unknown, we might nevertheless posit such a variable. Exploratory factor analysis is a technique for arriving at such common factors in a systematic way, in cases where the variables aer continuous. When given a set of correlations among manifest variables, it produces a statistical model of latent common factors that accounts for exactly these correlations. <ref type="bibr">4</ref> Perhaps unsurprisingly, latent variable models suffer from problems of identifiability. They posit the theoretical structure of unobservable common causes, over and above the observed correlations between observable variables. There will generally be many latent variable models, and accordingly many different causal structures, that fit the data. This is the problem of causal identifiability alluded to earlier. However, even if all modeling choices have been made and if the list of salient variables and their causal structure have been fixed, either by assumption or by background knowledge, the problem of statistical underdetermination may appear. In what follows we focus specifically on this restricted identification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Unidentifiability of Latent Variable Models</head><p>We now show that the model of Eqs. ( <ref type="formula" target="#formula_7">2</ref>), ( <ref type="formula" target="#formula_8">3</ref>) and ( <ref type="formula" target="#formula_9">4</ref>) cannot be identified by the data.</p><p>Focus on the dimensions of this model. We count 5 parameters, namely d, and / j and k j for j ¼ 0; 1. On the other hand, we have the binary observations F i and L i that can be used to determine these parameters. But because we are using Bernoulli hypotheses, only the observed relative frequencies of the possible combinations of F i and L i matter. And because we have 4 possible combinations of F i and L i , whose relative frequencies must add up to 1, we have 3 frequencies to determine the 5 parameters in the model. After having used the observations in the determination of the parameters, therefore, we still have 2 degrees of freedom left. Hence the values of the parameters in the model cannot be determined by the observation data uniquely.</p><p>We can state this problem in more detail by looking at the likelihoods for the observations of possible combinations of F i and L i . We write h ¼ hd; / 0 ; / 1 ; k 0 ; k 1 i.</p><p>For the likelihoods we write</p><formula xml:id="formula_10">PðF i ¼ 0; L i ¼ 1jH h Þ ¼ h 01 ¼ dð1 À / 1 Þk 1 þ ð1 À dÞð1 À / 0 Þk 0 ; PðF i ¼ 1; L i ¼ 0jH h Þ ¼ h 10 ¼ d/ 1 ð1 À k 1 Þ þ ð1 À dÞ/ 0 ð1 À k 0 Þ; PðF i ¼ 1; L i ¼ 1jH h Þ ¼ h 11 ¼ d/ 1 k 1 þ ð1 À dÞ/ 0 k 0 ;<label>ð5Þ</label></formula><p>where we omitted mention of the other individuals S iÀ1 . The fourth likelihood,</p><formula xml:id="formula_11">PðF i ¼ 0; L i ¼ 0jH h Þ,</formula><p>can be derived from these expressions. The salient point is that the system of equations resulting from filling in particular values for the above likelihoods has infinitely many solutions in terms of the components of h: for any value of the likelihoods, the space of solutions in h has 2 dimensions. The statistical model is thus unidentifiable.</p><p>Let us briefly elaborate on the unidentifiability of the model. It means that the likelihood function over the model does not have a unique maximum, and so that the maximum-likelihood estimator does not point to a uniquely best hypothesis. In fact there are infinitely many hypotheses compatible with the data. Say that we observe the following relative frequencies:</p><formula xml:id="formula_12">r 11 ¼ 1 t X t i¼1 F i L i ; r 10 ¼ 1 t X t i¼1 F i ð1 À L i Þ; r 01 ¼ 1 t X t i¼1 ð1 À F i ÞL i :<label>ð6Þ</label></formula><p>The likelihood PðS t jH h Þ is maximal if the observed relative frequencies r jk match the corresponding likelihoods h jk for all j and k:</p><formula xml:id="formula_13">h jk ¼ r jk :<label>ð7Þ</label></formula><p>But as said, there are infinitely many hypotheses H h that have these particular values for the likelihoods. Consequently, there is no unique hypothesis H h that has maximal overall likelihood PðS t jH h Þ.</p><p>For future reference we note that, by means of the likelihoods given in Eqs.</p><p>(5), we can determine a posterior probability for the hypotheses in the model, PðH h jS t Þ. And from the posterior distribution over the hypotheses we can generate the expectation value of the parameter h of the model H, according to</p><formula xml:id="formula_14">E½h ¼ Z H h PðH h jS t Þ dh:<label>ð8Þ</label></formula><p>Here h runs over ½0; 1 5 because the model is spanned by five independent chances. Like the posterior, the estimations will suffer from the fact that the hypotheses cannot be told apart: they will depend on the prior probability over the hypotheses. Of course, this is usually the case in a Bayesian analysis. What is troublesome is that no amount of additional data can eliminate this dependence of the estimations on the prior.</p><p>One reaction is to downplay the identifiability problem and say that it only concerns the values of these abstract parameters and not the empirical consequences. But because the estimations and expectations are not fully determined, the nature of the latent variable underlying the manifest variables is not determined either: it is not clear what causal role it plays. Different values for the parameters / j and k j entail different systematic relations between depression, fear and loathing, and ultimately this reflects back on our understanding of the posited notion of depression itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Identifiability in Multivariate Linear Regression</head><p>The foregoing mostly concerned a latent class model, and such models are a lot simpler than the models of factor analysis. In this section we argue that the problem outlined above also shows up there. Furthermore, we will note that in factor analysis there are actually two statistical identifiability problems. The first is made more concrete in the first subsection. It presents an analogous problem to that described in Sect. 2.3. The second type is briefly mentioned in the second subsection, mostly because it has been hotly debated in psychological methodology, but also because the present paper can offer a specific angle on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Rotation Problem</head><p>In factor analysis the variables are not binary but continuous, the probabilistic relations between the variables are linear regressions with normal errors, and the latent variable is assumed to be governed by some continuous distribution as well.</p><p>In our example we may write F i ¼ f for the event that the level of fear is f 2 R, and similarly for depression D i ¼ d. Then the relation between F i and D i , for example, is</p><formula xml:id="formula_15">PðF i ¼ f jD i ¼ dÞ ¼ Nðk F d; r F Þ ð<label>9Þ</label></formula><p>in which Nðkx; rÞ is a normal distribution over the values f of F i . So the relation between the variables D i and F i is characterized by a richer family of distributions, parameterized by a regression parameter k F and an error of size r F . Despite these differences, the same kind of statistical identifiability problems occur. Note that we can extend factor models like the one above to include any number of common factors. However, once a model includes more than one common factor, we find that the factor loadings are not completely determined. Suppose, for example, that we analyze fear F, loathing L, and sleeplessness S in terms of two common factors, one of them depression D, and the other the latent variable C. Every individual is supposed to occupy a specific position in the C Â D surface. We might feel that a more natural way of understanding the surface of latent variables is by labeling the states in this surface differently, for example by introducing variables A and B, both of which are linear combinations of C and D. The factors in a model may be linearly combined or, in more spatial terms, rotated to form any new pair of factors. <ref type="bibr">5</ref> The problem with this is that any rotation of factors, e.g., from fC; Dg to some fA; Bg, will perform equally well on the estimation criterion, be it maximum likelihood, generalized least squares, or similar, as long as we can adapt the factor loadings and perhaps the correlations among the factors accordingly. This problem is known as the problem of the rotation of factor scores. Neither the estimation criteria-often maximum likelihood-nor Bayesian methods of incorporating the data lead to a single best hypothesis in the factor model. The result is rather a collection of such hypotheses that all fit optimally. That is, the factor model is not identifiable.</p><p>A standard reaction to the rotation problem is to adopt further theoretical criteria that can constrain the latent variables. For example, it may be considered desirable to have maximal variation among the regression coefficients which, intuitively, comes down to coupling each latent variable with a distinct subset of manifest variables. <ref type="bibr">6</ref> The thing to note is that, from the point of view of statistics, the choice for how to parameterize the space of latent variables is underdetermined: we cannot decide between these parameterizations on the basis of the observations alone.</p><p>In this paper we will not elaborate the mathematical details of identifiability problems in these more complicated models. For present purposes, it suffices to use the simpler factor model of Eqs. ( <ref type="formula" target="#formula_7">2</ref>) to (4). The crucial characteristic in all of what follows is that there are latent variables explaining the correlational structure among the manifest variables, and that these structures are not fully determined by the correlations among the manifest variables. Admittedly, this paper thereby falls short of providing practical guidelines for dealing with the rotation problem, but we hope that our suggestions about a means to remedy it are valuable in their own right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Factor Score Indeterminacy</head><p>There is another problem with factor analysis that can be framed as an identifiability problem, and that has received considerable attention within statistical psychology. <ref type="bibr">7</ref> Say that we have rotated the factors to meet the theoretical criterion of our choice. Can we then reconstruct the latent variable itself, that is, can we provide a labeling in which each individual, i.e., each assignment of values to the observable variables, is assigned a determinate expected latent score? Sadly, the classical statistical answer here is negative. We still have to deal with the so-called indeterminacy of factor scores, meaning that there is a variety of ways in which we can organize the allocation of the individuals on the latent scores, all of them perfectly consistent with the estimations. <ref type="bibr">8</ref> The type of unidentifiability presented by factor score indeterminacy depends on what we take to be the statistical inference underlying factor analysis. In the context of this paper, we take the factor analysis to specify a complete probability assignment over the latent and manifest variables, including a prior probability over the latent variables. As explained in Bartholomew and Knott (1999), factor score indeterminacy is thereby eliminated, as long as there are sufficiently many manifest variables that are related to the latent variables according to distributions of a suitable, namely exponential, form. In this paper we will therefore ignore most of the discussion on factor score indeterminacy.</p><p>There is one point at which the problem of factor score indeterminacy enters the present discussion. We will show in what follows that intervention data can also be used to choose among a class of priors. But as indicated, the problem of choosing a prior probability is related to the problem of factor score indeterminacy. Therefore the use of intervention data, which resolves the identifiability problem discussed above, provides a new perspective on the problem of the indeterminacy of factor scores as well. We will return to this idea in Sect. 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Interventions to Resolve Identifiability</head><p>In the foregoing we have shown that latent variable models suffer from identifiability problems. We now explain these problems by revealing analogous problems in the estimation of parameters in Bayesian networks. This leads us to consider a specific solution, namely by means of intervention data. We first introduce Bayesian networks in Sect. 4.1, then the notion of intervention in Sect. 4.2, and finally its use in identifying latent variable models in Sect. 4.3. To the best of our knowledge, this solution to the problem of statistical identifiability has not yet been offered in the literature. The fact that the solution is not worked out in full generality here is hopefully compensated for by the fact that it offers a new insight into the use of intervention data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Bayesian Networks and Factor Analysis</head><p>In general, a Bayesian network consists of a directed acyclic graph on a finite set of variables fD; F; L; E. . .g together with the probability distributions of each variable conditional on its parents in the graph, e.g., PðE j Par E Þ. The graph is related to the probability distribution over the variables by an assumption known as the Markov Condition: each variable is probabilistically independent of its non-descendants in the graph, conditional on its parents, e.g., E ⊥ ⊥ NonDesc E | Par E; see Pearl (2000). Under this assumption the network suffices to determine the joint probability distribution over the variables, via the identity:</p><formula xml:id="formula_16">PðD; F; L; . . .Þ ¼ PðD j Par D Þ Â PðF j Par F Þ Â PðL j Par L Þ Â . . .<label>ð10Þ</label></formula><p>The probability of any assignment of values to the variables on the left hand side of this equation can be computed by filling in these valuations on the right hand side.</p><p>It is well-known that Bayesian networks and latent variable modeling are closely related. In fact, the introduction of the latent class models for the binary variables fF; L; Dg was already an introduction to a specific class of Bayesian networks. To ease exposition, suppose that there are no inter-subject dependencies and that the same probability assignment describes all subjects,</p><formula xml:id="formula_17">PðD i ; F i ; L i Þ ¼ PðD i 0 ; F i 0 ; L i 0 Þ;<label>ð11Þ</label></formula><p>so that we can omit the subscripts i. For each subject, the factor analysis determines a probability function P(F, L, D) that satisfies a specific symmetry: conditional on the latent depression D there is no correlation between the manifest fear F and loathing L, PðD; F; LÞ ¼ PðDÞPðFjDÞPðLjDÞ:</p><p>On this basis we can build a network, with the variables F, L and D as nodes. The probability function determined by factor analysis can thus be represented in a Bayesian network whose graph is depicted in Fig. <ref type="figure">1</ref>.</p><p>There are also differences between the theory of Bayesian networks and that of latent variable models. For one, the latter entails a rather specific network structure: there are hidden parent nodes, observable child nodes, there are typically fewer parents than children, and any child can be connected to any parent. On the other hand, applications of the former are usually restricted to probability functions over finite or at least countable domains. Nodes with continuous domains are not that commonly discussed, although they have been studied in the context of structural equation models, for example by Pearl (2000), and, from the side of latent variable modeling, by von Eye and Clogg (1994). A related difference is that in most applications of factor analysis, the probability functions that are considered are restricted to normal distributions over latent nodes, and to linear regressions with normal errors between latent and observable nodes. Applications of Bayesian networks are typically, but not necessarily, restricted to Bernoulli distributions.</p><p>In this section we approach latent variable modelling from the angle of Bayesian networks, using the framework for inference over Bayesian networks presented in Romeijn et al. (2009). Hence the statistical underdetermination presented in F D L Fig. <ref type="figure">1</ref> The graphical structure representing the independence relations in a factor analysis of depression, fear and loathing Sect. 2.3 is framed as a problem to do with determining the posterior probability distribution over the parameters that characterize the Bayesian network of Fig. <ref type="figure">1</ref>. The aim is to resolve this statistical underdetermination by means of intervention data. In order to do this, we first introduce interventions in the context of Bayesian networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Interventions</head><p>A causally interpreted Bayesian network, or causal net for short, is a Bayesian network where the graph is interpreted as a causal graph. That is, each arrow in the graph is interpreted as denoting a direct causal relationship from the parent variable to the child variable. Under this interpretation, the Markov Condition is called the Causal Markov Condition. It says that each variable is probabilistically independent of its non-effects conditional on its direct causes. It is often assumed that the Causal Markov Condition is bound to hold if the graph in the net is correct and is closed under common causes (i.e., any common causes of variables in the net are also included in the net). While there are situations in which the Causal Markov Condition is implausible, it can nevertheless be justified as a default assumption (Williamson 2005), and we shall take it for granted here.</p><p>Causal nets are helpful for predicting the effects of interventions. When an experimenter intervenes to fix the value of a target variable, she interrupts the normal course of affairs and sets the variable exogenously. The usual mechanisms by which the target variable is determined are thereby replaced by new mechanisms; these new mechanisms allow the experimenter to fix a value of the variable. An 'ideal' or 'divine' intervention is one in which the intervention only changes the intended target variable, without changing other variables under consideration and without changing other causal relationships under consideration. By means of Eq. ( <ref type="formula" target="#formula_16">10</ref>) we can determine the probability P 0 that some variable F takes value 0 after an ideal intervention has been performed that sets D to 1, say. Note that the causal net determines two different probability distributions, P before and P 0 after intervention. While P and P 0 will coincide in the probability assignments to nondescendants of D, and also in the probability assignments conditional on D, the unconditional probabilities for the variables downstream from D will be different.</p><p>Not all interventions are ideal. Other forms of intervention are ham-fisted in that they change the values of several variables at once, or non-modular in that they change other causal relationships, or parametric in the sense that they change the conditional probability distribution of the target variable without deterministically fixing its value. One subspecies of parametric intervention, which we shall refer to as a stochastic intervention, is central to our concerns: an intervention in which one sets the probability of the target variable to a new value P 0 ðD ¼ 1Þ ¼ d 0 while leaving the rest of the network intact. In other words, the causal net is transformed by eliminating arrows into the target D, setting its unconditional distribution to P 0 ðD ¼ 1Þ ¼ d 0 , and then determining the new probabilities for other variables.<ref type="foot" target="#foot_9">foot_9</ref> </p><p>Generally, interventions can help with identifiability problems in two ways. First, they can help with the identifiability of causal effects, as alluded to at the start of this paper. If more than one causal structure is compatible with evidence or if the specific relation between two variables is not known, then one can intervene, collect more evidence, and use this new evidence to decide over the matter. To take the example presented in the foregoing, suppose variables F, L and D are all measured, and that the resulting data shows that F and L are probabilistically independent conditional on D, written F ⊥ ⊥ L | D . This evidence is compatible with the causal graph of Fig. <ref type="figure">1</ref>, but equally with Figs. <ref type="figure" target="#fig_0">2</ref> and<ref type="figure">3</ref>. The evidence can be used to fill in the conditional probability distributions on these causal models, but cannot decide between them. An intervention can decide between them, however. If, after intervening to change the distribution of D, the distribution of F and L are changed, then that favours Fig. <ref type="figure">1</ref>. Otherwise if only the distribution of L is changed after intervention, then Fig. <ref type="figure" target="#fig_0">2</ref> is supported, and if only the distribution of F is changed then Fig. <ref type="figure">3 is supported</ref>.</p><p>By contrast, the point of this paper is that interventions can be used to make a statistical model with a given causal structure identifiable. Suppose that the causal structure is known and that data is collected which helps to estimate the probability distributions of some variables conditional on their parents, but which does not determine conditional distributions that attach to other variables. By carrying out an intervention, an experimenter changes the conditional distribution of one variable without changing the distributions of other variables. The data obtained after the intervention can then be used in conjunction with the old data to further constrain the values of the underdetermined distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Interventions and Model Identifiability</head><p>In this section we show how interventions, in a wide reading of this term, can be used to resolve the statistical identifiability problem for latent class models, introduced in Sect. 2.3 with the example on depression, fear, and loathing.</p><p>Let us briefly explain the general idea. We need to assume that the latent variable model is more than a convenient way of representing the probability functions involved. The arrows in the model need to be interpreted causally, that is, the latent variables must be taken as the causes of the observed variables. With this causal assumption in place, an intervention on the subjects will indeed change the distribution over the latent variables of the subjects. Importantly, in the application of interventions that we are currently considering it is not required that we have detailed knowledge of how the intervention has influenced the target variable, as long as we know that this change is not an effect of other variables in the model. Note, in particular, that a stochastic intervention is taken to be modular: the probabilistic relations between the latent and the manifest variables does not change as a result of the intervention. As explained in the foregoing, after an intervention we obtain an entirely new estimation problem for the parameters in the Bayesian network. However, we assume that the parameters associated with the relations between latent and manifest variables do not change: the values of / i and k i are not affected. In the following we show that, depending on the model, the data obtained after an intervention of this type can be used to select a unique best estimate for the parameter values in the latent variable model.</p><p>Consider again the model characterized by Eqs. ( <ref type="formula" target="#formula_7">2</ref>) to ( <ref type="formula" target="#formula_9">4</ref>), ( <ref type="formula" target="#formula_17">11</ref>) and ( <ref type="formula" target="#formula_18">12</ref>). As explained in the foregoing, an intervention is an exogenous change to the probability assignment. In this particular case, some change is made to the node D, e.g., all the subjects are given a treatment intended to change the probability for depression. We thus change the probability of depression, PðD i ¼ 1Þ ¼ d, to a new value,</p><formula xml:id="formula_19">P 0 ðD i ¼ 1Þ ¼ d 0 ;</formula><p>which-we assume-is less than than d. The relation of the depression variable to the variables of fear and loathing, given by P 0 ðF i ¼ 1jD i ¼ jÞ ¼ / j and P 0 ðL i ¼ 1jD i ¼ jÞ ¼ k j , is not changed by the intervention: the treatment changes the probability for depression but not how depression, whether absent or present, affects feelings of fear and loathing.</p><p>It is important to stress that the intervention under consideration covers a wider class than what is usually taken as an intervention in the literature of Bayesian networks. We do not need to suppose that the details of the exogenous change to the probability of depression is known but merely that it has particular qualitative characteristics, e.g., that d 0 \d. Moreover, we need not even suppose that we only target the depression variable D. Any ham-fisted intervention that makes an exogenous change to other variables that are not causes of the observables under consideration, in addition to the change on the latent variable under consideration, is suitable as an intervention. This means that the solution of the statistical identifiability problem considered here may also work in the context of a so-called 'natural experiment'.</p><p>After the intervention, or exogenous change to the system, we record the observations S 0 t in the same set of t individuals. By analogy to Eq. ( <ref type="formula" target="#formula_12">6</ref>), we observe the numbers of the occurrences in the new sequence of observations S 0 t ,</p><formula xml:id="formula_20">r 0 11 ¼ 1 t X t i¼1 F i L i ; . . .:</formula><p>So r 0 jk are the relative frequencies of the variables F and L as observed after the intervention. They present three further constraints on the parameters of the latent variable model.</p><p>To get the point across quickly, we focus again on the dimensions of the model. This time we count a number of 6 parameters, namely d, / j and k j for j ¼ 0; 1, and finally d 0 . On the other hand, we have a richer set of observations that can be used to determine these parameters. Specifically, we have 3 observed relative frequencies of f j i ^lk i before intervention, and 3 of them after intervention, so 6 in total. Whereas previously we had two degrees of freedom left after the incorporation of the data, we can now fill in all the parameter values of the factor model.</p><p>Let us make this more precise. As before, we have the likelihoods of Eqs. ( <ref type="formula" target="#formula_10">5</ref>). But to these expressions we now add the likelihoods of the hypotheses after the intervention:</p><formula xml:id="formula_21">P 0 ðF i ¼ 0; L i ¼ 1jH h Þ ¼ h 0 01 ¼ d 0 ð1 À / 1 Þk 1 þ ð1 À d 0 Þð1 À / 0 Þk 0 ; P 0 ðF i ¼ 1; L i ¼ 0jH h Þ ¼ h 0 10 ¼ d 0 / 1 ð1 À k 1 Þ þ ð1 À d 0 Þ/ 0 ð1 À k 0 Þ; P 0 ðF i ¼ 1; L i ¼ 1jH h Þ ¼ h 0 11 ¼ d 0 / 1 k 1 þ ð1 À d 0 Þ/ 0 k 0 :<label>ð13Þ</label></formula><p>The system of equations that results from equating likelihoods and observed relative frequencies before and after intervention is</p><formula xml:id="formula_22">h jk ¼ r jk and h 0 jk ¼ r 0 jk<label>ð14Þ</label></formula><p>for all j and k. Each of these constrains the parameters in h and h 0 in a particular way.</p><p>The ''Appendix'' to this paper shows that if this system of equations has a solution, then the solution is unique up to a transformation of the two values for D. Solutions thus come in mirror-image pairs, differing in the interpretation of the values for the variable D or, in other words, differing in whether the intervention has beneficial or adverse effects on the probability of being depressed. On the assumption that the treatment reduces the probability for depression, every hypothesis H h in the model is associated with a unique set of values for the likelihoods h jk and h 0 jk . The conclusion is that if the data are generated by a chance process specified by a hypothesis H h , then we can identify this hypothesis, in the same way as we were able to identify the true H h in the model of Eq. ( <ref type="formula" target="#formula_1">1</ref>).</p><p>Note that this does not hold for the entire range of possible values for the observed frequencies. For extremal values there is still an infinity of solutions. Moreover, certain combinations of frequencies simply do not match with any of the statistical hypotheses within the model. In those cases the intervention data overdetermine the latent variable model, i.e., it fails to fit all the correlations. We must then look for a richer statistical model. It would be rather natural to incorporate this aspect of scientific reasoning into our account, by describing how statistical models are adapted when intervention data yield a bad fit. The idea is that the overdetermination due to intervention may lead to controlled and formally specified changes in the model, and that this may lead to a formal account of theory change. However, such an account is beyond the scope of the current paper.</p><p>The main conclusion for now is that intervention data can indeed be used to resolve the identifiability problem introduced in Sect. 2.1. If there are parameter values matching the relative frequencies exactly, then on the assumption that the treatment is beneficial, these values are unique: the likelihood function has a unique maximum after the normal and the intervention data are incorporated. While we have only shown this for a simple example, it is readily seen, and briefly considered in the ''Appendix'', that the example generalizes. The example serves as a proof of principle and supports the central idea of this paper, which is that interventions can help to resolve statistical identifiability problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Philosophical and Practical Implications</head><p>We now discuss the philosophical and practical implications of the approach of this paper. After that we briefly revisit the indeterminacy of factor scores and suggest how intervention data can be used to resolve this indeterminacy, at least in the form it takes within a Bayesian statistical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Interventions Replace Theoretical Criteria</head><p>Our paper suggests a novel way to use intervention data, namely to resolve statistical identifiability problems. Where we had otherwise to use a theoretical criterion to choose among the equally well fitting alternative hypotheses, we can now make this choice on the basis of additional data, obtained after intervention. One might say that within statistics the identifiability problem has fuzzy edges: it can be resolved by an appeal to theoretical criteria, as routinely done for the rotation problem in factor analysis, but it can also be resolved by extending the realm of observations to include intervention data.</p><p>It is worth reiterating that we do not need to know anything about the exact impact of the intervention. That is, we do not need to know the exact value of d 0 . It suffices that we have changed the probability of the latent variable. Clearly, this is not to say that the use of intervention data requires no assumptions whatsoever. As indicated in the foregoing, the new data can only be taken as pertaining to the same parameters if we assume that the causal structure of the latent and observed variables is, at least roughly, correct. More specifically, we need to assume that the probabilistic relations between the latent and the observed variables, expressed in / i and k i , remain invariant under intervention. So in order to employ the intervention data for a resolution of the identifiability problem, we have to make particular causal assumptions. In a sense these modeling assumptions help us to get more out of the data than would otherwise be possible. <ref type="bibr">10</ref> We think that this resolution by causal assumptions and further empirical data is preferable to a resolution that employs a theoretical criterion only. This may be interesting for philosophers concerned with the interplay between theory and empirical fact in confirmation relations. Additionally, the result may help to put latent variable modelling on a firmer footing-in particular factor analysis, which has long been regarded by some as somewhat speculative (Furfey and Daly 1937). Finally, the use of interventions to resolve identifiability problems in factor analysis may be of practical interest. The rotation problem is a live one for designers of clinical and personality tests: how do we relate clusters of test items to specific personality traits? And what traits should we distinguish in the first place? Our suggestion would be that intervention data may help constrain the latent structure behind psychometric tests, thereby providing a clearer view of what the tests are measuring. 11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Interventions and the Indeterminacy of Factor Scores</head><p>We briefly remark on the problem of the indeterminacy of factor scores, as discussed in Sect. 3.2. Insofar as there is a problem with factor scores in the Bayesian treatment, intervention data can play an interesting role.</p><p>Recall that the expected value E½h, given in Eq. ( <ref type="formula" target="#formula_13">7</ref>), depends on the posterior probability over the parameter PðH h jS t Þ, and that this posterior depends on the prior probability PðH h Þ. As shown by Bartholomew and Knott (1999), the indeterminacy of factor scores in classical factor analysis derives directly from the fact that a prior probability is not provided. And because in a Bayesian treatment such a prior is assumed, we can say that Bayesian factor analysis is not affected by factor score indeterminacy. However, the prior is assumed, not derived, so a classical statistician may well ask for a motivation of the prior probability assignment.</p><p>Following the ideas set out above, the prior probability may be determined by means of intervention data. Instead of choosing a single prior, we might consider a range of priors over the parameter values, labeled by q say. We thereby increase the dimension of the parameter space by one. But we might know from a different study that the chance of being depressed after the treatment d 0 has some particular value, or is functionally related to the chance on depression before treatment. This reduces the number of parameters by one again, because d 0 is then fixed, or every d 0 is coupled to a unique value d. The net effect is that we can again estimate all the parameters, namely d, / j and k j for j ¼ 0; 1, and finally the second-order parameter q. <ref type="bibr">12</ref> In other words, just as we can estimate the effects of an intervention, d 0 , we can estimate the prior probability assignment that best suits the factor model. Of course, this is just a simple example. We have not said anything about the more realistic continuous case, in which we typically assume a normal distribution over the continuous variable D i as prior. Moreover, it is unrealistic to suppose that there is a clear and deterministic relation between the parameters governing the distribution 11 It is a topic of ongoing debate whether latent variables have to be taken as real in some sense (cf. Borsboom et al. 2003). This debate is relevant to our concerns, but not sufficiently to motivate an indepth discussion here. We need not adopt a realist or an instrumentalist view on latent variables to appreciate the point that theoretical criteria, formulated in terms of such latents, can be replaced by causal assumptions and intervention data. Similarly, the insightful discussion in Weinberger (2015) on latents and ideal interventions is relevant but not crucial: our points do not hinge on the interventions on latents being ideal. over the variables D i before and after the intervention. Nevertheless, we suggest that the analysis presented here illuminates how intervention data can be of use in dealing with the rightful heir of the problem of factor score indeterminacy in Bayesian factor analysis, namely the problem of how to choose a prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we have investigated the use of interventions for the problem of statistical identifiability: if two hypotheses have exactly the same likelihoods for all the possible observations, then how do we choose between them? While an answer to this question often invokes theoretical criteria such as simplicity and explanatory considerations, we have provided a partial answer in terms of empirical criteria. The idea is to use the background theory that generates the hypotheses, namely the causal structure. This theory provides us with a recipe for how to deal with interventions. Together with some assumptions on the causal structure of the latent and observed variables, the intervention data enable us to tell the statistically equivalent hypotheses apart.</p><p>We illustrated the identifiability problem by means of a latent class model. That is, we showed how interventions can be framed in terms of alterations to such a model, and how the intervention data can then be employed. In this paper we have not developed the same ideas for the more practical setting of factor analysis with normal distributions over continuous variables. But we believe that the problems identified in discrete Bayesian networks is in all the relevant respects similar to the rotation problem in the continuous setting, and we suggest that future work can resolve this problem of rotation by appealing to intervention data. On the other hand we realize that there is still a long way to go from the theoretical considerations in this work to the practical concerns of psychometricians. <ref type="bibr">13</ref> We will mention one specific theme for future research. We suggested that, relative to a given causal structure that links latent and observable variables, intervention data can also guide extensions of the statistical model. The rough idea is that the specifics of the misfit between model and intervention data will suggest how the latent structure might be adapted to repair the fit. Model selection techniques and further considerations of complexity or conservativity might then determine which of these adaptations is most appropriate. The methods and algorithms for putting this idea to work have yet to be determined, but we think that there are many potential applications of the idea. A tool for guiding extensions of statistical models can be of use to experimental scientists, but also to computer scientists working on the automated search of network structures.</p><p>Such applications lie within the realm of statistical methodology. However, there may be a further application of these ideas within the philosophy of science. The confirmatory practice of scientists has received a lot of attention from formally oriented philosophers of science, often with the aim of explaining or rationalizing science, or of providing scientists with norms that guide the inference from data to theory. Experimental practice, on the other hand, has not been subject to the same scrutiny by formal modelers. It has been the subject of science and technology studies, but not of formal philosophy of science. We believe formal philosophy of science will have interesting things to say about experimentation because the tools to describe interventions in mathematical terms are available. We hope that with the present study, we are contributing to the development of such a formal philosophy of experiment.</p><formula xml:id="formula_23">d ¼ f À / 0 / 1 À / 0 ¼ l À k 0 k 1 À k 0 ; d 0 ¼ f 0 À / 0 / 1 À / 0 ¼ l 0 À k 0 k 1 À k 0 :<label>ð16Þ</label></formula><p>The intuitive meaning is that f and f 0 must both sit in between / 0 and / 1 , as determined by d and d 0 , and that the relative positions of f and f 0 within this interval must be matched by the relative positions of l and l 0 in between k 0 and k 1 . In terms of freedom in the parameter space, there are thus two degrees of freedom left. If, for example, we fix / 0 and / 1 by hand, the values for k 0 and k 1 as well as the values for d and d 0 follow. We now determine these two constraints by a further set of two equations. Consider the expressions for fear and loathing occurring together:</p><formula xml:id="formula_24">h 11 ¼ d/ 1 k 1 þ ð1 À dÞ/ 0 k 0 c;<label>ð17Þ</label></formula><formula xml:id="formula_25">h 0 11 ¼ d 0 / 1 k 1 þ ð1 À d 0 Þ/ 0 k 0 c 0 :<label>ð18Þ</label></formula><p>We abbreviate the frequencies of them occurring together as c and c 0 . We can now substitute terms appearing in Eqs. (15) into Eq. ( <ref type="formula" target="#formula_24">17</ref>). With some reformulation these substitutions lead to</p><formula xml:id="formula_26">k 0 / 1 ¼ f k 0 þ l/ 1 À c;<label>ð19Þ</label></formula><formula xml:id="formula_27">k 1 / 0 ¼ f k 1 þ l/ 0 À c:<label>ð20Þ</label></formula><p>We can derive the analogous expressions for the parameters by using the frequency after intervention c 0 , now substituting terms from Eqs. (15) into Eq. ( <ref type="formula" target="#formula_25">18</ref>). Combining Eqs. ( <ref type="formula" target="#formula_26">19</ref>) and ( <ref type="formula" target="#formula_27">20</ref>) with the analogous expressions involving c 0 , we obtain</p><formula xml:id="formula_28">k 0 ¼ l 0 À l f À f 0 / 1 À c 0 À c f À f 0 ;<label>ð21Þ</label></formula><formula xml:id="formula_29">k 1 ¼ l 0 À l f À f 0 / 0 À c 0 À c f À f 0 :<label>ð22Þ</label></formula><p>Together with the constraints of Eq. ( <ref type="formula" target="#formula_23">16</ref>) these two linear relations between the k's and /'s are sufficient for determining all the values of the parameters.</p><p>To solve the equations we fill in the expression for k 0 of Eq. ( <ref type="formula" target="#formula_28">21</ref>) into Eq. ( <ref type="formula" target="#formula_26">19</ref>), thereby obtaining a quadratic equation for / 1 :</p><formula xml:id="formula_30">l 0 À l f À f 0 / 1 À c 0 À c f À f 0 / 1 ¼ f l 0 À l f À f 0 / 1 À c 0 À c f À f 0 þ l/ 1 À b:</formula><p>A parallel expression for / 0 can be obtained by filling in k 1 of Eq. ( <ref type="formula" target="#formula_29">22</ref>) into Eq. ( <ref type="formula" target="#formula_27">20</ref>), but if soluble within the domain [0, 1], this expression will yield the same two solutions. Once we choose either of the two solutions for / 1 , the parameter / 0 takes on the other value. And once we have solved for / 1 and chosen whether it obtains the higher or the lower of the two values, we thereby fix the values of all the other parameters. Swapping around the two solutions will effectively swap around the ordering among d and d 0 , according to the expressions above.</p><p>With respect to the interpretation of depression, fear, loathing, and treatment, the normal case will have f 0 \f , l 0 \l and c 0 \c so that k 0 \k 1 , / 0 \/ 1 , and d 0 \d. A further investigation of the space of solutions can be undertaken by identifying of each point in the space of frequencies whether or not the constraints can all be met. However, for present purposes the abstract characterization suffices, alongside the remark that the space of solutions is non-empty.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 A</head><label>2</label><figDesc>Fig. 2 A chain of fear F causing depression D, which causes loathing L</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J.-W. Romeijn, J. Williamson</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>We follow the Bayesian idea that hypotheses H h can serve as arguments of the probability function. Further conventions are that equations, like Q tþ1 ¼ 1, can appear as arguments of a probability function, and that expressions like S t function as variables.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Any infinitely long sequence of results is in principle consistent with any of the hypotheses H h , and in that sense we are encountering an underdetermination problem in the estimation. However, here we will not consider this type of identifiability.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>See Lawley and Maxwell (1971)  for a classical statistical overview,<ref type="bibr" target="#b19">Mulaik (1985)</ref> for a philosophically-minded discussion, and<ref type="bibr" target="#b0">Bartholomew and Knott (1999)</ref> for a very insightful introduction from a Bayesian perspective. All these treatises introduce exploratory factor analysis as well as the much less problematic statistical tool of confirmatory factor analysis. In this paper we concentrate on the former, and simply call it factor analysis.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>See<ref type="bibr" target="#b0">Bartholomew and Knott (1999)</ref> for a general introduction. Seeing that exploratory factor analysis generates a structure that explains the observed correlations, it is rather natural that<ref type="bibr" target="#b10">Haig (2005)</ref> and<ref type="bibr" target="#b23">Schurz (2008)</ref> present it as a formal model of abduction.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>This is a coordinate transformation in the space of latent variables, characterizing it in terms of different bases.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>This criterion is known as ''varimax''; see, e.g.,<ref type="bibr" target="#b16">Lawley and Maxwell (1971)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7"><p>See<ref type="bibr" target="#b26">Steiger (1979)</ref> for some historical context,<ref type="bibr" target="#b17">Maraun (1996)</ref> for a philosophical evaluation,<ref type="bibr" target="#b18">McDonald (1974)</ref> for an excellent classical statistical discussion, and<ref type="bibr" target="#b0">Bartholomew and Knott (1999)</ref> for a Bayesian account of it.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_8"><p>There are some restrictions to this allocation. For example, as worked out in<ref type="bibr" target="#b5">Ellis and Junker (1997)</ref>, if we let the number of manifest variables increase and assume that there is a single latent variable that is tail-measurable in terms of these manifest variables, then the factor scores are determined up to a monotonic transformation.252J.-W. Romeijn, J. Williamson</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_9"><p>See Korb et al. (2004)  and<ref type="bibr" target="#b4">Eberhardt and Scheines (2007)</ref> for further discussion of kinds of intervention.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_10"><p>We thank an anonymous reviewer for suggesting this formulation to us.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p>In the statistical literature, the idea that we can confirm or disconfirm probability distributions over statistical parameters has become known as hierarchical Bayesian modelling. See, for instance, Chapter 5 of<ref type="bibr" target="#b7">Gelman et al. (2013)</ref>, and the philosophical appraisals in<ref type="bibr" target="#b12">Henderson et al. (2010)</ref> and<ref type="bibr" target="#b21">Romeijn (2013)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_12"><p>In many cases psychometricians will have more pressing concerns than the exact identifiability of the parameters. See, for example,<ref type="bibr" target="#b11">Hayduk and Littvay (2012)</ref>, who argue that it is often preferable to accept some uncertainty in the determination of the model. In their view it is better to use the few best indicators, and direct further observation efforts towards developing more sophisticated theoretical models, so as to bring mediating and confounding variables into view. However, this does not take away from our point that intervention data may be highly informative.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_13"><p>With the aid of the solver in Mathematica, we have also investigated this space numerically. Special thanks go to David Atkinson for providing help with this, and for initially presenting us with an alternative, more elegant proof of uniqueness.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="262" xml:id="foot_14"><p>J.-W. Romeijn, J. Williamson</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements We would like to thank <rs type="person">David Atkinson</rs> for leading the way to analytic solutions for the problem of uniqueness and members of the PCCP research seminar in Groningen, especially <rs type="person">Leah Henderson</rs>, for providing detailed comments on an earlier version of the manuscript. Material from this paper was presented at numerous venues. We thank audiences in <rs type="institution">Munich, Pittsburgh, Utrecht, Tilburg, Toronto, and Groningen</rs> for their comments. <rs type="person">Jon Williamson</rs>'s work on this paper was supported by a grant from the <rs type="funder">UK Arts and Humanities Research Council</rs> for the project Evaluating evidence in medicine and by a grant from the <rs type="funder">Leverhulme Trust</rs> for the project Grading evidence of mechanisms in physics and biology.</p><p>Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ref type="url" target="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ref>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This appendix substantiates the claim that if the system of Eqs. ( <ref type="formula">14</ref>) has a solution, then this solution is unique. We are only dealing with the specific example of this paper and do not generalize the result. The generalization will bring rather cumbersome algebraic expressions and, we believe, little added insight. The reader may glean the strategy for an analytical investigation of the solution space, and an associated proof strategy for the general case, from what follows. <ref type="bibr">14</ref> We first combine the expressions of Eqs. ( <ref type="formula">5</ref>) and ( <ref type="formula">13</ref>) to obtain</p><p>where f ¼ r 10 þ r 11 and l ¼ r 01 þ r 11 are the frequencies of fear and loathing observed before intervention, and f 0 and l 0 are those frequencies after intervention.</p><p>We can now solve for d as well as d 0 by combining Eqs. (15), thus deriving the first four constraints on the parameters:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Latent variable models and factor analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Bartholomew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Knott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The theoretical status of latent variables</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mellenbergh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Heerden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="219" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bayes or bust</title>
		<author>
			<persName><forename type="first">J</forename><surname>Earman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge (MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining experiments to discover linear cyclic models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="185" to="192" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interventions and causal inference</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="981" to="995" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tail-measurability in monotone latent variable models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Junker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="523" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A criticism of factor analysis as a technique of social research</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Furfey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Daly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Sociological Review</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="178" to="186" />
			<date type="published" when="1937">1937</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Chapman &amp; Hall/CRC Texts in statistical science</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Bayesian data analysis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Experiment and the making of meaning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gooding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Hacking</surname></persName>
		</author>
		<title level="m">Representing and intervening</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An abductive theory of scientific method</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Haig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="371" to="388" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Should researchers use single indicators, best indicators, or multiple indicators in structural equation models?</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hayduk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Littvay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Medical Research Methodology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">159</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The structure and dynamics of scientific theories: A hierarchical bayesian perspective</title>
		<author>
			<persName><forename type="first">L</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Woodward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="172" to="200" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Causal discovery for linear cyclic models with latent variables</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="3387" to="3439" />
			<date type="published" when="2012-11">2012. Nov</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Realism and uncertainty of unobservable common causes in factor analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nou ˆs</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="329" to="355" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Varieties of causal intervention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Korb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Axnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Pacific rim international conference on AI</title>
		<meeting>the Pacific rim international conference on AI<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Factor analysis as a statistical method</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Lawley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Maxwell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Butterworths</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Metaphor taken as math: Indeterminancy in the factor analysis model</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Maraun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="517" to="538" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The measurement of factor indeterminacy</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="203" to="222" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Factor analysis and psychometrika: Major developments</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mulaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="23" to="33" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Abducted by Bayesians</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Romeijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Logic</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="430" to="439" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Logical relations in a statistical problem</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Romeijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haenni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of foundations of the formal sciences VI</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Loewe</surname></persName>
		</editor>
		<meeting>foundations of the formal sciences VI<address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>College Publications</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Common cause abduction and the formation of theoretical concepts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schurz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">TPD preprints</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning measurement models for unobserved variables</title>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th conference on uncertainty in artificial intelligence</title>
		<meeting>the 18th conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="543" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, prediction, and search</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Factor indeterminacy in the 1930s and the 1970s some interesting parallels</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Steiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="157" to="167" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Latent variables analysis: Applications for developmental research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Von Eye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Clogg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Sage</publisher>
			<pubPlace>Thousand Oaks (CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">If intelligence is a cause, it is a within-subjects cause</title>
		<author>
			<persName><forename type="first">N</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory and Psychology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="361" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bayesian nets and causality: Philosophical and computational foundations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Williamson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
