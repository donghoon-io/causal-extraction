<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminative conditional restricted Boltzmann machine for discrete choice and latent variable modelling *</title>
				<funder ref="#_BMKYPfQ">
					<orgName type="full">Fonds de recherche du Québec -Nature et technologies (FRQ-NT)</orgName>
				</funder>
				<funder>
					<orgName type="full">Ryerson University</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2017-06-05">June 5, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Melvin</forename><surname>Wong</surname></persName>
							<email>melvin.wong@ryerson.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Civil Engineering</orgName>
								<orgName type="laboratory">Laboratory of Innovations in Transportation (LITrans)</orgName>
								<orgName type="institution">Ryerson University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bilal</forename><surname>Farooq</surname></persName>
							<email>bilal.farooq@ryerson.ca</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Civil Engineering</orgName>
								<orgName type="laboratory">Laboratory of Innovations in Transportation (LITrans)</orgName>
								<orgName type="institution">Ryerson University</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guillaume-Alexandre</forename><surname>Bilodeau</surname></persName>
							<email>guillaume-alexandre.bilodeau@polymtl.ca</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer and Software</orgName>
								<orgName type="laboratory">Laboratoire d&apos;Interprétation et de Traitement d&apos;Images et Vidéo (LITIV)</orgName>
								<address>
									<addrLine>Polytechnique Montréal</addrLine>
									<settlement>Engi- neering Montréal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discriminative conditional restricted Boltzmann machine for discrete choice and latent variable modelling *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-06-05">June 5, 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1706.00505v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional methods of estimating latent behaviour generally use attitudinal questions which are subjective and these survey questions may not always be available. We hypothesize that an alternative approach can be used for latent variable estimation through an undirected graphical models. For instance, non-parametric artificial neural networks. In this study, we explore the use of generative non-parametric modelling methods to estimate latent variables from prior choice distribution without the conventional use of measurement indicators. A restricted Boltzmann machine is used to represent latent behaviour factors by analyzing the relationship information between the observed choices and explanatory variables. The algorithm is adapted for latent behaviour analysis in discrete choice scenario and we use a graphical approach to evaluate and understand the semantic meaning from estimated parameter vector values. We illustrate our methodology on a financial instrument choice dataset and perform statistical analysis on parameter sensitivity and stability. Our findings show that through non-parametric statistical tests, we can extract useful latent information on the behaviour of latent constructs through machine learning methods and present strong and significant influence on the choice process. Furthermore, our modelling framework shows robustness in input variability through sampling and validation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Complex theories of decision-making processes provide the basis of latent behaviour representation in statistical models focusing on the use of psychometric data such as choice perception and attitudinal questions. Although they can provide important insights into choice processes and underlying heterogeneity, studies have shown the limited flexibility and benefits of statistical latent behaviour models, i.e. Integrated Choice and Latent Variable (ICLV) models <ref type="bibr" target="#b0">(Chorus and Kroesen, 2014;</ref><ref type="bibr" target="#b1">Vij and Walker, 2016)</ref>. Two disadvantages are known in ICLV models: first, datasets are required to have attitudinal responses, for instance, likert scale questions in product choice surveys. Second, model mis-specification may occur when latent variable model equations are poorly defined and attitudinal questions are subjective and would change over time.</p><p>The objective of this study is to use of machine learning (ML) methods to analyze the underlying latent behaviour in choice models based on a set of synthetic ML considerations and hyperparameters without explicitly using attitudinal or perception attributes. A growing body of behavioural research focuses on patterns and clusters of behaviour characteristics including latent attitudes and choice perceptions. Yet, comparing with specific advanced choice modelling strategies such as ICLV models, our knowledge of the prevalence and consequences of latent behaviour in choice model still remains limited <ref type="bibr" target="#b1">(Vij and Walker, 2016)</ref>. Studies of hidden representations using neural network models may give us more nuanced and potentially new perspectives of latent variables on discrete choice experiments and choice behaviour theory <ref type="bibr" target="#b2">(Rungie et al., 2012)</ref>. Given the many possible latent variable combinations, it is necessary to use advanced ML techniques to segment population into groups with similar attitudinal profiles. For this study, we have chosen to use restricted Boltzmann machines (RBM). RBM is a non-parametric generative modelling approach that seeks to find latent representations within a homogeneous group by hypothesizing that posterior outputs can be explained with a reduced number of hidden units <ref type="bibr" target="#b3">(Le Roux and Bengio, 2008)</ref>. In addition, identifying common latent representation may enable policy makers to better understand the sensitivity and stability of latent behaviour models in surveyed and revealed preference data. We decouple the latent behaviour model underlying the data distribution by estimation on a financial instrument choice behaviour dataset without the need for subjective measurement indicators. The proposed method does not predefine a semantic meaning for each latent variable. Instead, we define a restricted Boltzmann machine to learn the latent relationships and approximate the posterior probability.</p><p>We show in our findings that a RBM modelling approach is able to characterize latent variables with semantic meaning without additional psychometric data. The parameters estimated through our RBM model presents strong and significant influence in the choice process. Furthermore, sensitivity analysis have shown that this approach is robust to input data variance and use of generated latent variables improves sampling stability.</p><p>The remainder of the paper is organized as follows: in Section 2, we provide a background literature review on latent behaviour models. Section 3 describes the conditional RBM modelling approach and model training methodology, given only observed variables without attitudinal questions. Section 4 explains the data and the experiment procedure. Section 5 presents the results and performance tests. Section 6 analyzes the model sensitivity and stability. Finally, section 7 discuss the conclusions and future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Current practice in choice modelling is targeted at drawing conclusion on the mechanism of the stochastic model and not so much about the nature of the data itself. This leads to simple assumptions of data relevance and statistical properties of explanatory variables <ref type="bibr" target="#b4">(Burnham and Anderson, 2003)</ref>. A number of parametric and non-parametric modelling methods are available. Parametric models are regression based and random utility maximization structural models. Examples of non-parametric methods include latent class and variable models. Other statistical models include k-means or hierarchical clustering. These nonparametric methods are often criticized for being too descriptive, theoretical, may result in inconsistent estimates and often not possible to make generalizations <ref type="bibr" target="#b5">(Ben-Akiva and Bierlaire, 1999;</ref><ref type="bibr" target="#b6">Atasoy et al., 2013;</ref><ref type="bibr" target="#b7">Bhat and Dubey, 2014)</ref>. Analyzing data through the statistical properties is generally applied for extracting information about the evolution of the responses associated with stochastic input variables rather than having good prediction capabilities. On the other hand, algorithmic modelling approaches such as artificial neural networks (ANN), decision trees, clustering and factor analysis are based on the ability to predict future responses accurately given future input variables within a 'black-box' framework <ref type="bibr" target="#b8">(Breiman et al., 2001)</ref>. Econometric choice models can be estimated by using both parametric and non-parametric methods that incorporate machine learning algorithms into discrete choice analysis to learn mappings from latent variables to posterior distribution <ref type="bibr">(Eric et al., 2008)</ref>.</p><p>A number of different approaches which implements the use of attitudinal variables have been used in existing literature <ref type="bibr" target="#b10">(Ashok et al., 2002;</ref><ref type="bibr" target="#b11">Morey et al., 2006;</ref><ref type="bibr" target="#b12">Hackbarth and Madlener, 2013)</ref>. The first approach relies on a top-down modelling framework which makes prior assumptions that individuals are divided into multiple market segments and each segment has its own utility function of underlying attributes. In the most generic form, these assumptions are based on multiple sources of unobserved heterogeneity influencing decisions, e.g. inter-and intra-class variance and 'agent effect' <ref type="bibr">(Yazdizadeh et al., 2017)</ref>. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the Latent Class and ICLV model framework which shows the process of deriving latent classes or variables and how it integrates into the structural choice model.</p><p>The Latent Class model (LCM) is one such form which assumes a discrete distribution among market segments <ref type="bibr" target="#b14">(Hess and Daly, 2014)</ref>. LCM derive clusters using a probabilistic model that describes the distribution of the data. Based on this assumption, similarities within a heterogeneous population are identified through assignment of latent class probabilities. Individuals in the same class share a common joint probability distribution among the observed variables. Under assumption of class independence, the utility is generated with a prior hypothesis from several sub-populations, and each sub-population is modelled separately. The resulting classes are often meaningful and easily interpretable. The unobserved heterogeneity in the population is captured by the latent classes, each of which is associated with different utility vector in the sub-model (Fig. <ref type="figure" target="#fig_0">1a</ref>). Another similar class of top-down models are finite mixture models, e.g. Mixed Logit, which allows the parameters to vary with a variance component and that behaviour is dependent on the observable attributes and on the latent heterogeneity which varies with the unobserved factors <ref type="bibr" target="#b15">(Hensher and Greene, 2003)</ref>.</p><p>The use of attitudes and perception latent variables are also particularly interesting and popular in past work <ref type="bibr" target="#b16">(Glerum et al., 2014;</ref><ref type="bibr" target="#b6">Atasoy et al., 2013)</ref>. Choice models with measurement indicator functions treat correlated indicators into multiple latent variables. This factor analysis method is similar to principal component analysis where the latent variables are used as principal components <ref type="bibr" target="#b16">(Glerum et al., 2014)</ref>. This approach involves the analysis of relationship between indicators and the choice model. Within this domain, there is the sequential and simultaneous estimation process. Sequential approach first estimates a measurement model which derives the relationship between latent variables and indicators. Then, a choice model is estimated, integrating over the distribution of the latent variables. The main disadvantage of this approach is that the parameters may contain measurement errors from the indicator function that were not taken into account during the choice model.</p><p>To solve this issue, another approach uses simultaneous estimation of structural and measurement model, which includes the latent variable in the choice model framework. This is so called the Integrated Choice and Latent Variable (ICLV) model (Fig. <ref type="figure" target="#fig_0">1b</ref>). The ICLV model explicitly uses information from measurement indicators and explanatory variables to derive latent constructs. This combined structural model framework has led to many interesting results, e.g. environmental attitudes in rail travel <ref type="bibr" target="#b17">(Hess et al., 2013)</ref>, image, stress and safety attitudes towards cycling <ref type="bibr" target="#b18">(Maldonado-Hinarejos et al., 2014)</ref>, and social attitudes towards electric cars <ref type="bibr" target="#b19">(Kim et al., 2014)</ref>. However, the simultaneous approach still relies on a separate measurement model (latent variable model) that describes the relationship to indicators. Despite the direct benefits of the ICLV model combining factor analysis with traditional discrete choice models, the only advantage to using such an approach is when attitudinal measurement indicators are expected to be available to the modeller and the observed explanatory variables are weak predictors of the choice model <ref type="bibr" target="#b1">(Vij and Walker, 2016)</ref>. Even when measurement indicators are available, they may not provide any further information that directly influence the choice than through explanatory variables <ref type="bibr" target="#b0">(Chorus and Kroesen, 2014)</ref>. Consequently, misspecification and other measurement errors may occur, when the criteria is not associated with the choice model. Without measurement indicators to guide selection of latent variables, we can alternatively use ML for latent variables through data mining. This can be implemented through generative modelling methods used in ML. Generative modelling in ML is a class of models which uses unlabelled data to generate latent features. Generative models learn the underlying choice distribution p(y) and the latent inference p(h|y), where h is the latent variable. Followed by implementing a Bayesian network that represents a probabilistic conditional relationship between random variables and dependencies to derive the posterior distribution of y given h using p(y|h) = p(h|y)p(y) p(h)</p><p>. Efficient algorithms which perform ML and inference such as RBMs can be used in this method. The denominator is given by p(h) = y p(h|y = 1) indicating choice y is chosen. The rapid advancement of machine learning research have led to the development of efficient semi-supervised training algorithms such as the conditional restricted Boltzmann machine (C-RBM) <ref type="bibr">(Salakhutdinov et al., 2007;</ref><ref type="bibr" target="#b21">Larochelle and Bengio, 2008)</ref>, a hybrid discriminative-generative model, capable of simultaneously estimating a latent variable model using a priori choice distribution with an latent inference model (see Fig. <ref type="figure" target="#fig_1">2</ref>).</p><p>To date, econometric and machine learning models are often studied for its contrasting purposes in decision forecasting by behavioural researchers <ref type="bibr" target="#b8">(Breiman et al., 2001)</ref>. Econometric models are based on the classical decision theory that individual's decisions can be modelled rationally based on utility maximization. These models assume that the population will adhere to the strict formulation of the choice model, but may not always represent the true decisions. Generative modelling based approach uses clustering and factor analysis developed through algorithmic modelling of the data. Associations between decision factors can be classified in this method, obtaining latent information without explicit definition of latent constructs <ref type="bibr">(Poucin et al., 2016)</ref>. Thus, machine learning algorithms such as ANN that decouple latent information from 'true' distribution generally outperform traditional regression based models in multidimensional problems <ref type="bibr" target="#b23">(Ahmed et al., 2010)</ref>. Recent works on latent behaviour modelling on choice analysis agree on the potential of improving behaviour models with machine learning. Examples include combining machine learning to improve complex psychological models <ref type="bibr" target="#b24">(Rosenfeld et al., 2012)</ref>, representing the phenomena of similarity, attraction and compromise in choice models <ref type="bibr" target="#b25">(Osogami and Otsuka, 2014)</ref> and inference of priorities and attitudinal characteristics <ref type="bibr" target="#b26">(Aggarwal, 2016)</ref>.</p><p>Despite the many benefits, interpretation of results are still extremely difficult due to the complexity and number of parameters in ML analysis. As a result, ML models are not often used for general purpose behaviour understanding, but created exclusively for a specific purpose for prediction accuracy. Still, machine learning research is a rapidly growing field at the intersection of statistical analysis and information science to find patterns in complex data <ref type="bibr" target="#b27">(Donoho, 2015)</ref>. Furthermore, with the emphasis on applications and theoretical studies in today's massive data driven industry, improving analytical techniques with ML is very relevant, although structural modelling, statistical and probability theory will remain the cornerstone of discrete choice analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The basis of latent class and latent variable models</head><p>The latent class model shown in Figure <ref type="figure" target="#fig_0">1</ref> is a simple top-down model that imparts generalization properties to the choice model that predefines a discrete number of classes, allowing the parameters to vary with an fixed distribution. Formally, the LCM choice probability can be expressed as:</p><formula xml:id="formula_0">P (y) = n P (s n )P (y|x, s n ) (1)</formula><p>where S = [s 1 , s 2 , ..., s n ] are the set of classes and P (s n ) is the probability that an individual belongs to class s. P (y|x, s n ) is the conditional probability of choice y selected given the class s n and input variable x.</p><p>The ICLV model extends the choice model by describing how perceptions and attitudes affect real choices as well as using separate indicators to estimate latent variables <ref type="bibr" target="#b28">(Ben-Akiva et al., 2002)</ref>. Latent variables can be classified as either attitudinal (individual characteristics) or perceived (personal beliefs towards responses) <ref type="bibr" target="#b5">(Ben-Akiva and Bierlaire, 1999)</ref>. The latent variable model (measurement model) forms a sub-part of the structural framework which captures the relationship between the latent variables and indicators and the observed explanatory variables which influence the latent variables. This specification can be used to identify more useful parameters and predict accurate decision outcomes when there is a lack of strong significant correlation between explanatory variables and choice outcomes. The functions of the structural and measurement model can be explained in four equations <ref type="bibr" target="#b1">(Vij and Walker, 2016)</ref>:</p><formula xml:id="formula_1">x * = Ax + ν</formula><p>(2)</p><formula xml:id="formula_2">I * = Dx * + η (3) u = Bx + Gx * +<label>(4)</label></formula><formula xml:id="formula_3">y i = 1 if u i &gt; u i for i ∈ {1, ..., I} 0 otherwise (5)</formula><p>where u i is the utility of selecting alternative i. A represents the relationship between input explanatory variables x and latent variables x * , D represents the relationship between x and the indicator output I * . B and G represents the model parameters with respect to the observed and latent variables. ν, η and are the stochastic error terms of the model, assumed to be mutually independent and Gumbel distributed. In a generative model, parameters are shared between G and D that simply defines the joint distribution of p(y, h), i.e. G = D (Fig. <ref type="figure" target="#fig_1">2</ref>). The re-use of a shared parameter vector differentiates the RBM model from the structural equation formulation of the ICLV model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Modelling through generative machine learning methods</head><p>In generative machine learning models, hidden units h are the learned features (see Fig. <ref type="figure" target="#fig_1">2</ref>) which performs non-redundant generalization of the data to reduce high dimensional input data (Hinton and Salakhutdinov,   <ref type="bibr">2006)</ref>. Intuitively, in terms of econometric analysis, hidden units are latent variables that depend on some observed data, for instance, socio-economic attributes such as weather or price information or direct choices such as location and choice of purchase. We can construct a generative model as a function of these dependent and independent variables. In the case of factor analysis approach, a common process is to perform feature extraction based on statistical hypothesis testing to determine if the values of the two classes are distinct, for example, using Support Vector Machines (SVMs) or Principal Component Analysis (PCA) to learn low-dimensional classes by capturing only significant statistical variances in the data <ref type="bibr">(Poucin et al., 2016;</ref><ref type="bibr">Wong et al., 2016)</ref>. The learned classes (or clusters) can then be introduced directly into the model via parameterization. In generative modelling approach, we use the priors directly to learn the distribution of the hidden units. In this process we extract latent information directly from the observed choice data instead of using measurement functions which may be prone to errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Balancing model inference and accuracy</head><p>One common problem that researchers face when constructing latent behaviour models is specifying of the optimal size of latent factors <ref type="bibr" target="#b31">(Vermunt and Magidson, 2002)</ref>. Since the hypothesis on the number of latent size cannot be tested directly, typical statistical evaluation methods such as AIC and BIC are used to guide class selection <ref type="bibr" target="#b31">(Vermunt and Magidson, 2002)</ref>, in the case of ICLV models, through predefinition of measurement functions <ref type="bibr" target="#b2">(Rungie et al., 2012)</ref>. However, since the number of latent factors determines the ability of the model to represent the various heterogeneity in the data, it is likely that as we increase h, the choice model become more efficient in capturing complex behaviour effects from individual and latent attributes. On the other hand, if we increase the number of latent segments, the number of parameters will also increase at an exponential rate <ref type="bibr" target="#b31">(Vermunt and Magidson, 2002)</ref>. Therefore, we may gain model accuracy but we would lose model interpretability.</p><p>The trade-off between inference and accuracy is a challenge when dealing with complex data <ref type="bibr" target="#b8">(Breiman et al., 2001)</ref>. If the goal of latent behaviour modelling is to leverage on data to understand underlying statistical problems, we have to incorporate implicit modelling methods in addition to describing explicit structural utility formulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we provide a brief overview on restricted Boltzmann machines and how it can be used to generate prior over the choice distributions. We refer readers to <ref type="bibr" target="#b32">Goodfellow et al. (2016)</ref> for background and details on generative models and deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Restricted Boltzmann machines</head><p>A restricted Boltzmann machine (RBM) is an energy-based undirected graphical model that extends from a Markov Random Field distribution by including hidden variables <ref type="bibr">(Salakhutdinov et al., 2007)</ref>. It is a single layer artificial neural network with no internal layer connections. The model has stochastic visible variables y ∈ {0, 1} I and stochastic hidden variables h ∈ {0, 1} J . The joint configuration (y, h) of visible and hidden variables is given by the Hopfield energy <ref type="bibr" target="#b33">(Hinton et al., 1984)</ref>:</p><formula xml:id="formula_4">Energy(y, h) = - i∈vis y i c i - j∈hid h j d j - i,j h j D ij y i ,<label>(6)</label></formula><p>where d j and c i represent the vector biases (constants) for the hidden and visible vectors respectively. D ij is the matrix of parameters representing an undirected connection between the hidden and visible variables. We can express the Boltzmann distribution as an energy model with energy function F (y):</p><formula xml:id="formula_5">p(y, h) = 1 Z exp(-F (y)),<label>(7)</label></formula><p>where the partition function Z = i,j exp(-Energy(y, h)) is the normalization function over all possible vector combinations. F (y) is defined as the free energy F (y) = -ln h exp(-Energy(y, h)) and further simplified to</p><formula xml:id="formula_6">F (y) = -y i c i - j∈hid ln(1 + exp(D .,j y + d j )).<label>(8)</label></formula><p>The probability of assigning a visible vector y is given by the sum of all possible hidden vector states:</p><formula xml:id="formula_7">p(y) = 1 Z h exp(-F (y)).<label>(9)</label></formula><p>The RBM model is used to learn aspects of an unknown probability distribution based on samples from that distribution. Given some observation, the RBM makes updates to the model weights such that the model best represent the distribution of the observation. To generate data with this method, it is necessary to compute the log likelihood gradient for all visible and hidden units. Hinton introduced a fast greedy algorithm to learn model parameters efficiently using Contrastive Divergence (CD) method that starts a sampling chain (Gibbs sampling) from real data points instead of random initialization <ref type="bibr" target="#b34">(Hinton, 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model estimation and inference</head><p>The probability that the RBM network learns a training sample can be raised by adjusting the weights to lower the energy of that training sample and raise the energy of other non-training samples. In order to minimize the negative log likelihood of the probability distribution p(y), we take its gradient derivative of the log probability of a training vector with respect to the model parameters as follows:</p><formula xml:id="formula_8">∂ log p(y) ∂θ = y i h j train -y i h j model = φ + -φ -,<label>(10)</label></formula><p>where the components in the angle brackets corresponds to the expectations under the specified distribution. The first and second terms are the positive φ + and negative φ -phases respectively. This function updates the model parameters using a simple learning rule with a learning rate Φ: ∆θ = Φ( y i h j train -y i h j model ).</p><p>(11)</p><p>The updates for parameters θ = {D ij , d j , c i } can be performed using simple stochastic gradient descent at each iteration of t:</p><formula xml:id="formula_9">θ t = θ t-1 -∆θ. (<label>12</label></formula><formula xml:id="formula_10">)</formula><p>To obtain a sample of a hidden unit from y i h j train , we take a random training sample y and sample the state in the hidden layer is given by the following function:</p><formula xml:id="formula_11">p(h j = 1|y) = e d j + i D ij y i 1 + e d j + i W ij y i = σ(d j + i D ij y i ),<label>(13)</label></formula><p>where σ(x) = e x /(1 + e x ). Similarly, we can obtain a visible state, given a vector of sampled hidden units, via a logistic function:</p><formula xml:id="formula_12">p(y i |h) = e c i + j D ij h j i e c i + j D i j h j . (<label>14</label></formula><formula xml:id="formula_13">)</formula><p>Since weights are shared between D and G and they define the distributions of p(y), p(h), p(y, h), p(y|h) and p(h|y), we can express the posterior distribution as p(y) = h p(h)p(y|h) <ref type="bibr" target="#b35">(Ng and Jordan, 2002)</ref>. Due to its bidirectional structure, this framework possesses good generalization capabilities. The visible layer represents the data (in the case of choice modelling, data represent selected choices), and the hidden layer represents the capacity of the model as class distributions.</p><p>The model can be inferred from y i h j model can be done by setting the states of the visible variables to a training sample and then the states of the hidden variables are computed using Eq. 13. Once a "state" is chosen for the hidden variables, a "reconstruction" phase produces a new vector ỹ with a probability given by Eq. 14, and the gradient update rule is given by: ∆θ = Φ( y i h j train -y i h j reconstruction ).</p><p>(15)</p><p>We approximate the gradient function by using a CD Gibbs sampler minimizing the divergence between the expected and estimated probability distribution, known as the Kullback-Leibler (KL) divergence <ref type="bibr" target="#b36">(Hinton, 2002)</ref>. A divergence ratio of 0 indicates that the estimated distribution is totally similar. The training algorithm that runs for a total number of N chain steps is initialized from a fixed point from the data distribution and then averaged across all examples (Carreira-Perpinan and Hinton, 2005).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modelling approach</head><p>In this paper, the proposed method uses a conditional RBM (C-RBM) training algorithm to include inputoutput connections that allows for discriminative learning <ref type="bibr" target="#b38">(Mnih et al., 2012)</ref>. C-RBM expands the model to include "context input variables", i.e. p(y|x, h). k input explanatory variables are introduced as context variables so that they can be used to influence the latent variables, even though Eq.14 does not reconstruct these explanatory variables. This influence is represented by a weight matrix B ik . The intuition is that for each latent variable, it acts as a function of the observed choice y, conditional on x (see Fig. <ref type="figure" target="#fig_1">2</ref>). In the choice prediction stage, a vector of new input samples x generate latent variables h. Conditional on the explanatory and latent variables, a probability function describing the choice behaviour is given as:</p><formula xml:id="formula_14">p(y i |h, x) = e k B ik x k + j D ij h j +c i i e k B i k x k + j D i j h j +c i . (<label>16</label></formula><formula xml:id="formula_15">)</formula><p>Likewise, sampling of the hidden state is extended to incorporate x:  where the update parameters are θ = {D ij , B ik , A jk , d j , c i }. During the reconstruction phase, the condition probability (Eq. 16) is equivalent to a MNL model with latent variables (where h and x represents the latent and observed variables respectively). Good latent variables h best capture information along the orthogonal direction where choices y and observed inputs x vary the most. The training and choice estimation phase is illustrated in Fig. <ref type="figure" target="#fig_2">3</ref> and<ref type="figure" target="#fig_3">4</ref>. In the positive phase, parameter vectors are adjusted decided by the learning rate σ to learn the transformed latent representation of the training set. In the negative phase, the latent variables are "clamped" or realized and the parameter vectors are adjusted again by reconstructing the observed variables. Referring to Fig. <ref type="figure" target="#fig_1">2</ref>, the multinomial (MNL) model estimates the conditional parameter vector B and bias vector c, while the C-RBM model includes vectors D, A and d.</p><formula xml:id="formula_16">p(h j = 1|y) = σ(d j + i D ij y i + k A jk x k ),<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data</head><p>In this section, we develop a financial product choice scenario with explanatory variables using the C-RBM model. The latent variables representing the latent attitudinal variables is simultaneously estimated in conjunction with the interaction with choice model. First, we construct a structured choice subset from a financial product transaction dataset from the Kaggle database 1 . The data shows a monthly basis record of each financial product purchase by customers of Santander. The time span of the data is from January 2015 to June 2016. Next, we reduced the complexity of the dataset by removing transaction data which contain multiple product choices. To ensure consistency, inputs were scaled and normalized. Overall, the constructed dataset has a total of 13 alternatives (product choice) and 20 explanatory variables. Table <ref type="table" target="#tab_0">1</ref> lists the alternatives and distribution across the dataset. Given the above conditions, a total of 253,803 valid responses were recorded representing the total population sample with 13 available choices. A descriptive list of mean and standard deviation values of the explanatory variables are shown in Table <ref type="table" target="#tab_1">2</ref>. The experimental question is straightforward: "Given a set of examples with explanatory variables, what product is the individual most likely to purchase in the given month?" In a typical situation, the decision maker chooses an alternative that yields the maximum utility, making an inference about the behaviour of the decision maker using the predictive model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Method for assessing C-RBM model performance</head><p>We can estimate the weights for the latent inference model B ik and D ij by optimizing the lower bound of the KL-divergence using gradient backpropagation. Intuitively D ij represents the parameters for the explanatory variables and B ik represents the parameters for the latent variables. We selected models with 2, 4, 16 and 32 latent variables to observe the effects of increasing model complexity. One disadvantage of this step is that it results in a large number of estimated parameters: (N params ∈ R (I×J)+(K×I)+(K×J)+K+I ). With J = 4, we ended up with 409 parameters. To counteract overfitting due to this problem, we trained on 70% of our data and validate the model on the other 30% with a 2-fold cross-validation to verify generalization. When the validation error stops decreasing, the optimal estimation is reached <ref type="bibr" target="#b32">(Goodfellow et al., 2016)</ref>. A baseline comparison is set up using a standard multinomial logistic regression model with all explanatory variable and compared to the discriminative C-RBM modelling approach, followed by comparing the log-likelihood, ρ 2 model fit and predictive accuracy across all data models. The criteria for measuring performance of a categorical based model include: ρ 2 model fit and prediction error. The ρ 2 fit denotes the predictive ability between the trained model and a model without covariates. In the prediction error evaluation, the elements in the diagonal cells of a confusion matrix over the total number of examples denotes the accuracy of the model in predicting the correct choice and the error is</p><formula xml:id="formula_17">Error valid = 1 - i P (y pred = 1|x, h, y i = 1). (<label>18</label></formula><formula xml:id="formula_18">)</formula><p>y i is the actual choice and Error valid is the sum of all the error probabilities for correct assessment for each choice. We fit the model on the training set and evaluate on the validation set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We compare the different models based on their generalization performance on the test set. A total of 76,141 observations were used in the test. For the purpose of this study, we tested on both normalized and non-normalized data and found that both data produce similar result. Model estimation and validation were performed with Theano ML Python libraries<ref type="foot" target="#foot_1">foot_1</ref> . Optimization parameters used were stochastic gradient descent (SGD) on mini-batches of 64 samples for 400 epochs with input normalization. We used an adaptive momentum based learning rate of with initial rate of 1e -3 <ref type="bibr">(Hinton et al., 2006)</ref>. Training time was approximately 30 minutes for each model including validation running on a Intel Core i5 workstation. At the given time, computational demand may not be significant to justify the small number of hidden units, however, speed could become a more important consideration when model estimation and validation increase in data size or using very large parameter vectors with higher dimensionality. The statistical results of the model comparison across the same validation set is shown in Table <ref type="table" target="#tab_2">3</ref>. We found that additional latent information about the relationship between explanatory variables and observed decisions was useful and increases model accuracy. Bayesian Information Criterion (BIC) values indicate that 8 hidden units may be the optimal number of latent variables and higher BIC values above 8 hidden units might suggest overfitting. However, when generating semantic class meanings, a smaller number of latent variables may be simpler, therefore, in our example, we use only 2 latent variables for analysis.</p><p>To evaluate the efficiency of the models, we used a Hinton diagram <ref type="bibr" target="#b40">(Bremner et al., 1994)</ref> to analyze the parameter strengths between independent and dependent variables. We plot the parameter values and significance with choice on the y-axis and independent variables on the x-axis <ref type="bibr" target="#b40">(Bremner et al., 1994)</ref>. A Hinton diagram is often used in model analysis where the dimensionality of the model is high and provides a simple visual way of analyzing each vector. Figs. 5 through 9 shows the parameter estimates of the  The signs and value of each parameter corresponds to the size and colour of the patches in the matrix, with white and black representing positive and negative signs respectively. Statistical significance (t-test) of each parameter is calculated using θ √ σ , where σ is the inverse of the Hessian of the log likelihood with sample size adjustment with respect to the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Characteristics of latent variables</head><p>We can characterize each hidden unit with the explained significance and strengths represented by the weights D . D is a parameter vector that indicates the linear contribution of each latent variable and a constant d, such that each alternative can be described as a utility function of latent variables: y = Dh + d.</p><p>For example, C-RBM-2 latent variable hidden1 is characterized by individuals who are of working age, non-EU foreign citizens with non-VIP status and does not own any special accounts. We can therefore infer this latent variable that indicates a 'savings driven attitude' (see Fig <ref type="figure">6b</ref>). From the model results, population with such characteristics have a positive preference of purchasing a payroll product and a low motivation of purchasing a (credit/debit) card product as indicated in Fig 6b . Likewise in latent variable hidden2, it is represented by older, loyal customers who are VIP and have held various account types over their lifetime. This latent variable can be inferred to as 'self-reliance attitude' and are indication of the population who are less likely to purchase long term deposits, funds, securities and card products. The C-RBM with latent variables outperforms the MNL model, however, the performance increase from increasing the number of latent variables past 4 LV, is small. This would suggest that the upper bound of latent representative capacity is reached with just a small number of latent variables. Using 2 or 4 latent variables would be sufficient for significant improvement over a MNL structure.</p><p>From the presented results, it is clear that the C-RBM models differ significantly from the MNL model in terms of parameters which are strong and significant. This result seems to be broad-based in the sense that it is not dictated by the number of hidden units and signifies that the observed distribution has some latent factors that can be explored. However, we should mention that the training parameter initialization may have a small random effect on the model. Note that in the parameter plots, the signs and strength contribution to the choice model differ from model to model which may indicate that model training may be stuck at a local optima. This also suggest that the hidden and observed layer have different scale <ref type="bibr">(Glorot and Bengio, 2010)</ref>. What is suggested in <ref type="bibr">He et al. (2015)</ref> is to increase learning rate to improve convergence, but that would result in overgeneralization and loss of expressive power in the hidden units. We posit a We performed 2-fold cross-validation analysis and determined that the residual from model fit is not significant, therefore the model is robust to changes in input data -this is further confirmed with a sensitivity analysis presented in the following section. In the parameter plots, we can see the values and signs correspond to the strength of each variable. For instance, the parameters for Guarantees choice are not significant, since the distribution is very low (0.002%). The latent models show similar results. For C-RBM with 2 and 4 hidden units, almost all of the parameters are significant, except for income, employee, savings, derivada and junior variables. This can be attributed to the small mean values (and high deviation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Sensitivity of parameter estimates</head><p>The versatility and effectiveness of parameter estimates are determined by a sensitivity analysis of the model output. Methods of sensitivity analysis include variance based estimator, sampling based and differential analysis <ref type="bibr">(Helton et al., 1991;</ref><ref type="bibr" target="#b44">Saltelli et al., 2000</ref><ref type="bibr" target="#b45">Saltelli et al., , 2010))</ref>."Sensitive" parameters are those whose uncertainty contributes substantially to the test results <ref type="bibr">(Helton et al., 1991;</ref><ref type="bibr" target="#b46">Hamby, 1994)</ref>. The model is sensitive to input parameters in the variability associated with the input variable resulting in a large output variability. Sensitivity ranking sorts the input parameters by the amount of influence it has on the model output and the disagreement between rankings measures the parameter sensitivity to changes to the input <ref type="bibr" target="#b46">(Hamby, 1994)</ref>.</p><p>We first define a list of parameters used in the model by their standard errors calculated over the full dataset. In large dataset sensitivity analysis, a key concern is the computational cost needed to complete the analysis, hence we use a sampling based approach as a cheap estimator to the output % difference of the parameter minimum and maximum value. Random sampling (e.g. simple random sample, Monte Carlo, etc.) generates distributions of input and output to assess model uncertainties <ref type="bibr">(Helton et al., 1991)</ref>. Analyzing the sampling effects can provide information of the overal model performance since parameter sensitivity depends on all parameters which the model is sensitive and therefore the importance of each parameter <ref type="bibr" target="#b46">(Hamby, 1994)</ref>.</p><p>Consider that the C-RBM model is represented by y = f (x, h), where x and h are the input vectors of observed and latent variables respectively and y is the model output. We suppose that the model f (•) is a complex, highly non-linear function such that we cannot completely define the way the C-RBM model responds to changes in input variables. Also, h is dependent on x through a submodel previously shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The analysis involves independently and randomly generated sample with size n S = 0.1n (10% random sample draw), where n = 76, 141 is the total number of observations. The model performance is considered by sampling stability of variable parameters. Sensitivities are also assessed for size of hidden units used in generating the C-RBM models and indicates what number of latent variables (hyperparameter) is required for model identifiability. Since the model was applied using a multinomial logit approach instead of a conditional logit, this resulted in a very large number of parameters. Thus the effect of relative changes to the number of distributed parameters gives the range of variance across each explanatory variable and number of hidden units used. Table <ref type="table" target="#tab_3">4</ref> shows the effects of sampling on the sensitivity and stability of the model observed parameters on the theoretical values and size of latent variables. Notice that the relative difference in standard error between the full and sampled model decreases when number of latent variables increases. This shows that the C-RBM model with high synthetic latent variables are robust to changes to input values through sampling. Additionally, the parameter sensitivity rank across variables also becomes more consistent and therefore,we show that RBM models are efficient in obtaining good latent variables with low generalization error. The significant decrease in standard error difference from 2 LV to 4 LV may indicate that the number of latent variables used in the models has a lower bound on the generalization error, which implies that we need careful consideration on h for obtaining efficient but yet accurate exact values of β without losing model interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This study analyzes alternative means of latent behaviour modelling in the absence of attitudinal indicators. In ICLV models, specialized surveys have to be constructed with attitudinal questions to model latent effects on the decisions. While it has been one of the more popular method in discrete choice analysis, there are several disadvantages to it. First, attitudinal questions are subjective and the behaviours are subjected to changes over time. Next, existing datasets that have no attitudinal questions cannot leverage on the ICLV model, thus latent effects cannot be utilized. We explore generative modelling of the choice distribution to uncover latent variables using machine learning methods, without measurement indicators. We hypothesized that latent effects can be obtained not only from attitudinal questions, but also from the posterior choice distribution. In effect, we are modelling latent components that fits the real choice distribution rather than achieving good statistics on subjective models. For example, there could possibly be some mean behaviour that dictates a more probable influence on purchases given some latent variables.</p><p>For this method to be effective, certain conditions have to be present: First, difficultly to get a good discriminative prediction result using only the provided explanatory variables. In this scenario, the C-RBM models were able to learn good latent variable representation and improve the model fit and prediction accuracy while providing latent variable inferrability. Next, when the data lacks attitudinal survey data, this method can find latent effects without the use of subjective measurement indicators.</p><p>The current limitations of this study are the absence of choice dynamics or explanatory variable dynamics, i.e. changes over time or multiple choices for the same individual was not considered, but can be brought in.</p><p>The underlying RBM is capable of dynamics. We hypothesize that this may improve the model significantly, but we are still looking for ways to incorporate dynamics into our C-RBM model. In recent studies, we have seen dynamic frameworks such as recurrent neural networks used in modelling temporal data <ref type="bibr" target="#b47">(Taylor et al., 2007;</ref><ref type="bibr" target="#b38">Mnih et al., 2012)</ref>. Finally, it is worth noting that as the number of latent variable increases, the number of estimated parameters increases exponentially. This will pose problems in large datasets and the ability to reduce dimensionality will give a significant benefit to efficient use of model parameters. In our observation, performing cross-validation or model selection with lowest validation error is a justifiable method to prevent overfitting using all the parameters. In the future, we would also look at the possibility of introducing deep learning architecture to choice modelling by stacking RBMs <ref type="bibr">(Otsuka and Osogami, 2016)</ref>.</p><p>While ICLV model are optimized to predict the effects of latent constructs on the choice model using measurement indicators to guide latent parameters selection, this method uses observed decisions as an influence source for optimizing latent variables through machine learning. This is not to say that we do not agree with using measurement indicators which may often be subjective and may raise mis-specification problems and when explanatory variables are poor predictors, ICLV models can improve latent effects on choice models <ref type="bibr" target="#b1">(Vij and Walker, 2016)</ref>. However, latent effects may not only be present in attitudes and perceptions, but also in the direct observation of choices. Our current work explores the use of posterior choice distribution for latent behaviour modelling. Generative modelling in DCA is inspired by state-of-theart machine learning algorithms that performs unsupervised feature extraction from unlabelled data used in classification problems <ref type="bibr" target="#b33">(Hinton et al., 1984)</ref>. In circumstances when attitudinal variables are not available, we have a strong reason to believe that the generation of latent factors are important and effective in building a discrete choice model.</p><p>A future study that would be of interest is to extend this method to datasets with attitudinal questions and survey. For example, inter-city rail survey <ref type="bibr">(Sobhani and Farooq, 2017)</ref>, and perform an analysis on both RBM and ICLV methods to obtain the generalization error of attitudinal survey models. A comparative study would provide a foundation for analysis of various latent behaviour models through graphical and algorithmic methods and provide guidance not only in selecting the appropriate latent variables, but also direct research effect to more promising directions.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Classical structural framework for (a) latent class model and (b) integrated choice and latent variables model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Framework for a C-RBM choice model conditional on explanatory variables and choice distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: C-RBM (a) positive φ+ and (b) negative φ-phases during semi-supervised discriminative training. Weights (connections) are learned to reduce reconstruction ỹ error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: During the choice prediction phase, (a) latent variables are sampled using explanatory variables, and (b) the choice model is estimated with variables x and h.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>completed training stage of the different models. The Hinton matrix shows the influence of each independent variable on each alternative or latent variable. Statistically significant (&gt;95% confidence bound) parameters are highlighted in blue. The values along the x-axis are normalized with zero mean and unit variance. The 13 financial product choices are listed on the y-axis. The estimated parameters and bias of the C-RBM prediction model B, D and c are projected onto the Hinton diagram (Figs. 6a, 7a, 8a and 9a) while parameters A and d representing the parameters and bias for the latent variable with respect to the alternatives shown in Figs. 6b, 7b, 8b and 9b. c and d are the constants with respect to the observed and hidden layer respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: (a) C-RBM model with 16 latent variables. (b) Latent variable relationship parameters. White: +ve values, Black: -ve values, Blue: &gt;95% significant</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Choices (y)    </figDesc><table><row><cell cols="2">Choice index Name</cell><cell>Total sample distrib.</cell></row><row><cell>1</cell><cell>Guarantees</cell><cell>0.002%</cell></row><row><cell>2</cell><cell>Short-term deposits</cell><cell>0.83%</cell></row><row><cell>3</cell><cell cols="2">Medium-term deposits 0.07%</cell></row><row><cell>4</cell><cell>Long-term deposits</cell><cell>3.79%</cell></row><row><cell>5</cell><cell>Funds</cell><cell>0.98%</cell></row><row><cell>6</cell><cell>Mortgage</cell><cell>0.02%</cell></row><row><cell>7</cell><cell>Pensions</cell><cell>0.15%</cell></row><row><cell>8</cell><cell>Loans</cell><cell>0.035%</cell></row><row><cell>9</cell><cell>Taxes</cell><cell>2.68%</cell></row><row><cell>10</cell><cell>Cards</cell><cell>21.93%</cell></row><row><cell>11</cell><cell>Securities</cell><cell>1.42%</cell></row><row><cell>12</cell><cell>Payroll</cell><cell>22.04%</cell></row><row><cell>13</cell><cell>Direct debit</cell><cell>46.05%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Explanatory variable descriptive statistics (x)    </figDesc><table><row><cell cols="2">Explanatory variable Description</cell><cell>mean</cell><cell>std. dev.</cell></row><row><cell>age</cell><cell>Customer age</cell><cell>42.9</cell><cell>13.0</cell></row><row><cell>loyalty</cell><cell>Customer seniority (in years)</cell><cell>8.03</cell><cell>6.0</cell></row><row><cell>income</cell><cell>Customer income (e)</cell><cell cols="2">141,838 262,748</cell></row><row><cell>sex</cell><cell>Customer sex (1=male)</cell><cell>0.387</cell><cell>0.487</cell></row><row><cell>employee</cell><cell>Employee index, 1 if employee</cell><cell>0.0006</cell><cell>0.024</cell></row><row><cell>active</cell><cell>Active customer index</cell><cell>0.95</cell><cell>0.199</cell></row><row><cell>new_cust</cell><cell>1 if customer loyalty &lt; 6 mo.</cell><cell>0.045</cell><cell>0.207</cell></row><row><cell>resident</cell><cell>Resident index (Spain)</cell><cell>0.999</cell><cell>0.007</cell></row><row><cell>foreigner</cell><cell>Foreign citizen index</cell><cell>0.045</cell><cell>0.21</cell></row><row><cell>european</cell><cell>EU citizen index</cell><cell>0.995</cell><cell>0.006</cell></row><row><cell>vip</cell><cell>VIP customer index</cell><cell>0.116</cell><cell>0.32</cell></row><row><cell>savings</cell><cell>Savings Account type</cell><cell>0.0002</cell><cell>0.012</cell></row><row><cell>current</cell><cell>Current Account type</cell><cell>0.572</cell><cell>0.495</cell></row><row><cell>derivada</cell><cell>Derivada Account type</cell><cell>0.0009</cell><cell>0.03</cell></row><row><cell>payroll_acc</cell><cell>Payroll Account type</cell><cell>0.416</cell><cell>0.493</cell></row><row><cell>junior</cell><cell>Junior Account type</cell><cell>0.0001</cell><cell>0.0098</cell></row><row><cell>masparti</cell><cell>Mas Particular Account type</cell><cell>0.017</cell><cell>0.128</cell></row><row><cell>particular</cell><cell>Particular Account type</cell><cell>0.168</cell><cell>0.373</cell></row><row><cell>partiplus</cell><cell>Particular Plus Account type</cell><cell>0.113</cell><cell>0.316</cell></row><row><cell>e_acc</cell><cell>e-Account type</cell><cell>0.255</cell><cell>0.436</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Model training results</figDesc><table><row><cell cols="5">Model latent variables Validation error log-likelihood ρ 2</cell><cell cols="2">no. of params BIC</cell></row><row><cell>MNL</cell><cell>(baseline)</cell><cell>0.4454</cell><cell>-206808</cell><cell cols="2">0.546 273</cell><cell>416915</cell></row><row><cell cols="2">CRBM J = 2</cell><cell>0.4360</cell><cell>-203558</cell><cell cols="2">0.553 341</cell><cell>411237</cell></row><row><cell></cell><cell>J = 4</cell><cell>0.4338</cell><cell>-202066</cell><cell cols="2">0.556 409</cell><cell>409075</cell></row><row><cell></cell><cell>J = 8</cell><cell>0.4323</cell><cell>-200846</cell><cell cols="2">0.559 545</cell><cell>408279</cell></row><row><cell></cell><cell>J = 16</cell><cell>0.4318</cell><cell>-200223</cell><cell cols="2">0.560 817</cell><cell>410321</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Parameter sensitivity rank and standard error difference for estimated parameters B for sampling-</figDesc><table><row><cell cols="2">based sensitivity analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">RBM 2 LV</cell><cell></cell><cell cols="2">C-RBM 4 LV</cell><cell></cell><cell cols="2">C-RBM 8 LV</cell><cell>C-RBM 16 LV</cell></row><row><cell>sample size</cell><cell cols="2">n n s</cell><cell></cell><cell cols="2">n n s</cell><cell></cell><cell cols="2">n n s</cell><cell>n n s</cell></row><row><cell></cell><cell></cell><cell></cell><cell>std. err.</cell><cell></cell><cell></cell><cell>std. err.</cell><cell></cell><cell></cell><cell>std. err.</cell><cell>std. err.</cell></row><row><cell>parameter</cell><cell cols="2">rank</cell><cell>diff.</cell><cell cols="2">rank</cell><cell>diff.</cell><cell cols="2">rank</cell><cell>diff.</cell><cell>rank</cell><cell>diff.</cell></row><row><cell>βage</cell><cell cols="3">15 15 49.30</cell><cell cols="3">15 12 0.52</cell><cell cols="3">11 11 0.99</cell><cell>11 12 0.64</cell></row><row><cell>βloyalty</cell><cell cols="3">18 14 59.36</cell><cell cols="3">14 15 0.38</cell><cell cols="3">15 15 0.82</cell><cell>15 17 0.48</cell></row><row><cell>βincome</cell><cell>3</cell><cell cols="2">3 3712.99</cell><cell>3</cell><cell cols="2">3 26.67</cell><cell>3</cell><cell cols="2">3 43.00</cell><cell>3</cell><cell>2 35.82</cell></row><row><cell>βsex</cell><cell cols="3">12 13 67.51</cell><cell cols="3">13 14 0.41</cell><cell cols="3">14 13 0.91</cell><cell>14 15 0.52</cell></row><row><cell>βemployee</cell><cell>5</cell><cell cols="2">2 4267.79</cell><cell>2</cell><cell cols="2">4 13.74</cell><cell>5</cell><cell cols="2">5 21.27</cell><cell>4</cell><cell>4 33.74</cell></row><row><cell>βactive</cell><cell cols="3">21 16 47.92</cell><cell cols="3">16 19 0.20</cell><cell cols="3">19 19 0.34</cell><cell>19 19 0.26</cell></row><row><cell>βnew_cust</cell><cell cols="3">6 12 53.93</cell><cell cols="3">12 7 1.49</cell><cell>8</cell><cell cols="2">8 1.34</cell><cell>8</cell><cell>9 0.91</cell></row><row><cell>βresident</cell><cell cols="3">16 20 16.61</cell><cell cols="3">20 20 0.19</cell><cell cols="3">20 20 0.31</cell><cell>20 20 0.23</cell></row><row><cell>βforeigner</cell><cell cols="3">8 17 29.15</cell><cell cols="3">17 8 1.43</cell><cell cols="3">9 10 0.76</cell><cell>7</cell><cell>7 1.35</cell></row><row><cell>βeuropean</cell><cell cols="3">17 20 16.62</cell><cell cols="3">20 20 0.19</cell><cell cols="3">21 20 0.31</cell><cell>21 20 0.23</cell></row><row><cell>βvip</cell><cell cols="3">20 10 122.66</cell><cell cols="3">10 16 0.33</cell><cell cols="3">16 12 0.99</cell><cell>16 13 0.68</cell></row><row><cell>βsavings</cell><cell>1</cell><cell cols="3">1 34177.13 1</cell><cell cols="2">1 258.41</cell><cell>2</cell><cell cols="2">1 255.12</cell><cell>2</cell><cell>1 181.81</cell></row><row><cell>βcurrent</cell><cell cols="3">7 11 64.19</cell><cell cols="3">11 13 0.41</cell><cell cols="3">12 18 0.38</cell><cell>12 16 0.39</cell></row><row><cell>βderivada</cell><cell>4</cell><cell cols="2">4 3112.38</cell><cell>4</cell><cell cols="2">5 4.70</cell><cell>4</cell><cell cols="2">4 19.67</cell><cell>5</cell><cell>5 2.82</cell></row><row><cell cols="4">βpayroll_acc 9 18 24.91</cell><cell cols="3">18 18 0.29</cell><cell cols="3">18 17 0.52</cell><cell>18 18 0.41</cell></row><row><cell>βjunior</cell><cell>2</cell><cell cols="2">5 1759.26</cell><cell>5</cell><cell cols="2">2 58.29</cell><cell>1</cell><cell cols="2">2 45.32</cell><cell>1</cell><cell>3 22.43</cell></row><row><cell>βmasparti</cell><cell cols="3">11 7 185.94</cell><cell>7</cell><cell cols="2">9 1.41</cell><cell>7</cell><cell cols="2">6 4.99</cell><cell>9</cell><cell>6 2.29</cell></row><row><cell>βparticular</cell><cell cols="3">14 8 166.56</cell><cell cols="3">8 11 0.61</cell><cell cols="3">13 14 0.83</cell><cell>13 14 0.53</cell></row><row><cell>βpartiplus</cell><cell cols="3">10 6 189.75</cell><cell cols="3">6 10 0.65</cell><cell cols="3">10 9 1.51</cell><cell>10 10 0.86</cell></row><row><cell>βe_acc</cell><cell cols="3">19 9 159.38</cell><cell cols="3">9 17 0.33</cell><cell cols="3">17 16 0.82</cell><cell>17 11 0.91</cell></row><row><cell>bias</cell><cell cols="3">13 19 19.07</cell><cell cols="3">19 6 3.17</cell><cell>6</cell><cell cols="2">7 3.35</cell><cell>6</cell><cell>8 0.48</cell></row><row><cell cols="10">middle-of-the-road solution should have adequate model accuracy and generalization over a large population.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Figure 5: MNL model parameters. White: +ve values, Black: -ve values, Blue: &gt;95% significant Figure 7: (a) C-RBM model with 4 latent variables. (b) Latent variable relationship parameters. White: +ve values, Black: -ve values, Blue: &gt;95% significant</figDesc><table><row><cell>guarantees short term deposits medium term deposits long term deposits funds mortgage pensions loans taxes cards securities payroll direct debit</cell><cell cols="2">age loyalty income sex employee active new_cust partiplus particular guarantees C-RBM-4 model short term deposits medium term deposits long term deposits funds mortgage pensions loans taxes cards securities payroll direct debit european (a) foreigner resident new_cust active employee sex income loyalty age vip savings current derivada payroll_acc junior masparti e_acc hidden1 hidden2 hidden3 hidden4 constant</cell><cell cols="16">resident foreigner european vip savings current derivada payroll_acc hidden16 hidden15 hidden14 hidden13 hidden12 hidden11 hidden10 hidden9 hidden8 hidden7 hidden6 hidden5 hidden4 hidden3 hidden2 hidden1 MNL model hidden1 hidden2 hidden3 hidden4 age loyalty income junior masparti particular partiplus e_acc C-RBM-4 latent variables (b) constant sex employee active new_cust resident foreigner european vip savings current derivada payroll_acc C-RBM-16 latent variables</cell><cell cols="2">junior</cell><cell cols="2">masparti</cell><cell>particular</cell><cell>partiplus</cell><cell>e_acc</cell><cell>constant</cell></row><row><cell>guarantees mortgage pensions loans taxes cards securities payroll direct debit funds long term deposits medium term deposits short term deposits</cell><cell>(a)</cell><cell>C-RBM-2 model</cell><cell>hidden8 hidden6 hidden7 hidden5 hidden2 hidden4 hidden1 hidden2 hidden3 hidden1</cell><cell>age</cell><cell cols="18">loyalty age income loyalty sex income employee sex active employee active new_cust C-RBM-2 latent variables resident foreigner european vip savings current derivada payroll_acc C-RBM-8 latent variables new_cust resident foreigner european vip savings current derivada payroll_acc junior junior masparti masparti particular partiplus particular e_acc partiplus constant e_acc</cell><cell>constant</cell></row><row><cell></cell><cell cols="2">age loyalty income sex employee active new_cust resident foreigner european vip savings current derivada payroll_acc junior masparti particular partiplus e_acc hidden1 hidden2 constant</cell><cell></cell><cell>age</cell><cell>loyalty</cell><cell>income</cell><cell>sex</cell><cell>employee</cell><cell>active</cell><cell>new_cust</cell><cell>resident</cell><cell>foreigner</cell><cell>european</cell><cell>vip</cell><cell>savings</cell><cell>current</cell><cell>derivada</cell><cell cols="2">payroll_acc</cell><cell cols="2">junior</cell><cell>masparti</cell><cell>particular</cell><cell>partiplus</cell><cell>e_acc</cell><cell>constant</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) (b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Figure 6: (a) C-RBM model with 2 latent variables. (b) Latent variable relationship parameters. White: +ve values, Black: -ve values, Blue: &gt;95% significant Figure 8: (a) C-RBM model with 8 latent variables. (b) Latent variable relationship parameters. White: +ve values, Black: -ve values, Blue: &gt;95% significant</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Dataset: https://www.kaggle.com/c/santander-product-recommendation/data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Theano Python library: http://github.com/Theano/Theano</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research is in part funded by <rs type="funder">Ryerson University</rs> PhD Fellowship and by <rs type="funder">Fonds de recherche du Québec -Nature et technologies (FRQ-NT)</rs> with team grant No. <rs type="grantNumber">2016-PR-189250</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BMKYPfQ">
					<idno type="grant-number">2016-PR-189250</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the (im-) possibility of deriving transport policy implications from hybrid choice models</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Chorus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kroesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transport Policy</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="217" to="222" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How, when and why integrated choice and latent variable models are latently useful</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="192" to="217" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent variables in discrete choice experiments</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Rungie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Coote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Louviere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Choice Modelling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="145" to="156" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representational power of restricted boltzmann machines and deep belief networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1631" to="1649" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Model selection and multimodel inference: a practical information-theoretic approach</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Burnham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Discrete choice methods and their applications to short term travel decisions, in: Handbook of transportation science</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ben-Akiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bierlaire</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="5" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attitudes towards mode choice in switzerland</title>
		<author>
			<persName><forename type="first">B</forename><surname>Atasoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Glerum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bierlaire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">disP-The Planning Review</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="101" to="117" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new estimation approach to integrate latent psychological constructs in choice modeling</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="68" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical modeling: The two cultures</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="199" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Active preference learning with discrete choice data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extending discrete choice models to incorporate attitudinal and other latent variables</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of marketing research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="31" to="46" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using angler characteristics and attitudinal data to identify environmental preference classes: a latent-class model</title>
		<author>
			<persName><forename type="first">E</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Breffle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmental and Resource Economics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="91" to="115" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Consumer preferences for alternative fuel vehicles: A discrete choice analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hackbarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Madlener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part D: Transport and Environment</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A generic form for capturing unobserved heterogeneity in discrete choice modeling: Application to neighborhood location choice</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yazdizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Farooq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rezaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transportation Research Board 96th Annual Meeting</title>
		<imprint>
			<biblScope unit="page" from="17" to="05144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Handbook of choice modelling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Daly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Edward Elgar Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The mixed logit model: the state of practice</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hensher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="133" to="176" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Forecasting the demand for electric vehicles: Accounting for attitudes and perceptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Glerum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stankovikj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thémans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bierlaire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Science</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accommodating underlying pro-environmental attitudes in a rail travel context: application of a latent variable latent class specification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jopson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part D: Transport and Environment</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="42" to="48" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploring the role of individual attitudes and perceptions in predicting the demand for cycling: a hybrid choice modelling approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Maldonado-Hinarejos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Polak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1287" to="1304" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Expanding scope of hybrid choice models allowing for mixture of social influences and latent attitudes: Application to intended purchase of electric cars</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rasouli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Timmermans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation research part A: policy and practice</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="71" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Restricted boltzmann machines for collaborative filtering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Classification using discriminative restricted boltzmann machines</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="536" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pedestrian activity pattern mining in wifi-network connection data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Poucin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Farooq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transportation Research Board 95th Annual Meeting</title>
		<imprint>
			<biblScope unit="page" from="16" to="5846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An empirical comparison of machine learning models for time series forecasting</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Atiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Gayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>El-Shishiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometric Reviews</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="594" to="621" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Combining psychological models with machine learning to better predict people&apos;s decisions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zuckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Azaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kraus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Restricted boltzmann machines modeling human choice</title>
		<author>
			<persName><forename type="first">T</forename><surname>Osogami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Otsuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="73" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On learning of choice models with interactive attributes</title>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2697" to="2708" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">50 years of data science</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Tukey Centennial Workshop</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hybrid choice models: progress and challenges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ben-Akiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcfadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Train</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bierlaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bolduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boersch-Supan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brownstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Bunch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="163" to="175" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Next direction route choice model for cyclist using panel data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Farooq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-A</forename><surname>Bilodeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">st Annual Conference of Canadian Transportation Research Forum</title>
		<imprint>
			<biblScope unit="page">51</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Latent class cluster analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Vermunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Magidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied latent class analysis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="89" to="106" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Learning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Boltzmann machines: Constraint satisfaction networks that learn</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ackley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie-Mellon University, Department of Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A practical guide to training restricted boltzmann machines</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">926</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="841" to="848" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">On contrastive divergence learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>AISTATS</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1202.3748</idno>
		<title level="m">Conditional restricted boltzmann machines for structured output prediction</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hinton diagrams: Viewing connection strengths in neural networks</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Bremner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gotts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Denham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="215" to="218" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<publisher>Aistats</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Sensitivity analysis techniques and results for performance assessment at the waste isolation pilot plant</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Helton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Mccurley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Rudeen</surname></persName>
		</author>
		<imprint>
			<pubPlace>Albuquerque, NM (USA)</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Sandia National Labs.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Saltelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensitivity analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2000">2000</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Variance based sensitivity analysis of model output. design and estimator for the total sensitivity index</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saltelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Annoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Azzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Campolongo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ratto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tarantola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Physics Communications</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page" from="259" to="270" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A review of techniques for parameter sensitivity analysis of environmental models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hamby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmental monitoring and assessment</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="135" to="154" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">1345</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A deep choice model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Otsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Osogami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<biblScope unit="page" from="850" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Innovative intercity transport mode: Application of choice preference integrated with attributes nonattendance and value learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Farooq</surname></persName>
		</author>
		<imprint>
			<publisher>Operational Research Societies, Québéc City</publisher>
		</imprint>
	</monogr>
	<note>21st International Federation of</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
