<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalized Network Psychometrics: Combining Network and Latent Variable Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2017-09-13">September 13, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Sacha</forename><surname>Epskamp</surname></persName>
							<email>sacha.epskamp@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychological Methods</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mijke</forename><forename type="middle">T</forename><surname>Rhemtulla</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Davis: Department of Psychology</orgName>
								<orgName type="institution">University of California</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Denny</forename><surname>Borsboom</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychological Methods</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generalized Network Psychometrics: Combining Network and Latent Variable Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-09-13">September 13, 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1605.09288v4[math.ST]</idno>
					<note type="submission">Manuscript accepted for publication in Psychometrika. Manuscript submitted to &quot;Psychometrika</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Make friends easily</term>
					<term>E5: Take charge</term>
					<term>Neuroticism N1: Get angry easily</term>
					<term>N2: Get irritated easily</term>
					<term>N3: Have frequent mood swings</term>
					<term>N4: Often feel blue</term>
					<term>N5: Panic easily Make friends easily</term>
					<term>E5: Take charge</term>
					<term>Neuroticism N1: Get angry easily</term>
					<term>N2: Get irritated easily</term>
					<term>N3: Have frequent mood swings</term>
					<term>N4: Often feel blue</term>
					<term>N5: Panic easily</term>
					<term>Openness O1: Am full of ideas</term>
					<term>O2: Avoid difficult reading material</term>
					<term>O3: Carry the conversation to a higher level</term>
					<term>O4: Spend time reflecting on things</term>
					<term>O5: Will not probe deeply into a subject Make friends easily</term>
					<term>E5: Take charge</term>
					<term>Neuroticism N1: Get angry easily</term>
					<term>N2: Get irritated easily</term>
					<term>N3: Have frequent mood swings</term>
					<term>N4: Often feel blue</term>
					<term>N5: Panic easily</term>
					<term>Openness O1: Am full of ideas</term>
					<term>O2: Avoid difficult reading material</term>
					<term>O3: Carry the conversation to a higher level</term>
					<term>O4: Spend time reflecting on things</term>
					<term>O5: Will not probe deeply into a subject</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce the network model as a formal psychometric model, conceptualizing the covariance between psychometric indicators as resulting from pairwise interactions between observable variables in a network structure. This contrasts with standard psychometric models, in which the covariance between test items arises from the influence of one or more common latent variables. Here, we present two generalizations of the network model that encompass latent variable structures, establishing network modeling as parts of the more general framework of Structural Equation Modeling (SEM). In the first generalization, we model the covariance structure of latent variables as a network. We term this framework Latent Network Modeling (LNM) and show that, with LNM, a unique structure of conditional independence relationships between latent variables can be obtained in an explorative manner. In the second generalization, the residual variance-covariance structure of indicators is modeled as a network. We term this generalization Residual Network Modeling (RNM) and show that, within this framework, identifiable models can be obtained in which local independence is structurally violated. These generalizations allow for a general modeling framework that can be used to fit, and compare, SEM models, network models, and the RNM and LNM generalizations. This methodology has been implemented in the freeto-use software package lvnet, which contains confirmatory model testing as well as two exploratory search algorithms: stepwise search algorithms for low-dimensional datasets and penalized maximum likelihood estimation for larger datasets. We show in simulation studies that these search algorithms performs adequately in identifying the structure of the relevant residual or latent networks. We further demonstrate the utility of these generalizations in an empirical example on a personality inventory dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Recent years have seen an emergence of network modeling in psychometrics <ref type="bibr">(Borsboom, 2008;</ref><ref type="bibr">Schmittmann et al., 2013)</ref>, with applications in clinical psychology (e.g., <ref type="bibr" target="#b53">van Borkulo et al. 2015;</ref><ref type="bibr" target="#b44">McNally et al. 2015;</ref><ref type="bibr" target="#b19">Fried et al. 2015)</ref>, psychiatry (e.g., <ref type="bibr">Isvoranu et al. 2016b,a)</ref>, health sciences (e.g., <ref type="bibr">Kossakowski et al. 2015)</ref>, social psychology (e.g., <ref type="bibr" target="#b12">Dalege et al. 2016;</ref><ref type="bibr">Cramer et al. 2012)</ref>, and other fields (see for a review of recent literature <ref type="bibr" target="#b20">Fried and van Borkulo 2016)</ref>. This line of literature stems from the network perspective of psychology <ref type="bibr">(Cramer et al., 2010)</ref>, in which noticeable macroscopic behavior-the co-occurrence of aspects of psychology such as cognitive abilities, psychopathological symptoms, or a set of behaviors-is hypothesized to not be due to the influence of unobserved common causes, such as general intelligence, psychopathological disorders, or personality traits, but rather to emergent behavior in a network of interacting psychological, sociological, and biological components. Network models are used to gain insight into this potentially high-dimensional interplay <ref type="bibr" target="#b17">(Epskamp et al., 2016)</ref>. In practice, network models can be used as a sparse representation of the joint distribution of observed indicators, and as such these models show great promise in psychometrics by providing a perspective that complements latent variable modeling. Network modeling highlights variance that is unique to pairs of variables, whereas latent variable modeling focuses on variance that is shared across all variables <ref type="bibr" target="#b9">(Costantini et al., 2015)</ref>. As a result, network modeling and latent variable modeling can complement-rather than exclude-one-another.</p><p>In this paper, we introduce the reader to this field of network psychometrics <ref type="bibr">(Epskamp et al., ress)</ref> and formalize the network model for multivariate normal data, the Gaussian Graphical Model (GGM; <ref type="bibr">Lauritzen 1996)</ref>, as a formal psychometric model. We contrast the GGM to the Structural Equation Model (SEM; <ref type="bibr">Wright 1921;</ref><ref type="bibr">Kaplan 2000)</ref> and show that the GGM can be seen as another way to approach modeling covariance structures as is typically done in psychometrics. In particular, rather than modeling the covariance matrix, the GGM models the inverse of a covariance matrix. The GGM and SEM are thus very closely related: every GGM model and every SEM model imply a constrained covariance structure. We make use of this relationship to show that, through a reparameterization of the SEM model, the GGM model can be obtained in two different ways: first, as a network structure that relates a number of latent variables to each other, and second, as a network between residuals that remain given a fitted latent variable model. As such, the GGM can be modeled and estimated in SEM, which allows for network modeling of psychometric data to be carried out in a framework familiar to psychometricians and methodologists. In addition, this allows for one to assess the fit of a GGM, compare GGMs to one-another and compare a GGM to a SEM model.</p><p>However, the combination of GGM and SEM allows for more than fitting network models. As we will show, the strength of one framework can help overcome shortcomings of the other framework. In particular, SEM falls short in that exploratory estimation is complicated and there is a strong reliance on local independence, whereas the GGM falls short in that it assumes no latent variables. In this paper, we introduce network models for latent covariances and for residual covariances as two distinct generalized frameworks of both the SEM and GGM. The first framework, Latent Network Modeling (LNM), formulates a network among latent variables. This framework allows researchers to exploratively estimate conditional independence relationships between latent variables through model search algorithms; this estimation is difficult in the SEM framework due to the presence of equivalent models <ref type="bibr">(MacCallum et al., 1993)</ref>. The second framework, which we denote Residual Network Modeling (RNM), formulates a network structure on the residuals of a SEM model. With this framework, researchers can circumvent critical assumptions of both SEM and the GGM: SEM typically relies on the assumption of local independence, whereas network modeling typically relies on the assumption that the covariance structure among a set of the items is not due to latent variables at all. The RNM framework allows researchers to estimate SEM models without the assumption of local independence (all residuals can be correlated, albeit due to a constrained structure on the inverse residual covariance matrix) as well as to estimate a network structure, while taking into account the fact that the covariance between items may be partly due to latent factors.</p><p>While the powerful combination of SEM and GGM allows for confirmative testing of network structures both with and without latent variables, we recognize that few researchers have yet formulated strict confirmatory hypotheses in the relatively new field of network psychometrics. Often, researchers are more interested in exploratively searching a plausible network structure. To this end, we present two exploratory search algorithms. The first is a step-wise model search algorithm that adds and removes edges of a network as long as fit is improved, and the second uses penalized maximum likelihood estimation <ref type="bibr">(Tibshirani, 1996)</ref> to estimate a sparse model. We evaluate the performance of these search methods in four simulation studies. Finally, the proposed methods have been implemented in a free-to-use R package, lvnet, which we illustrate in an empirical example on personality inventory items <ref type="bibr" target="#b48">(Revelle, 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling Multivariate Gaussian Data</head><p>Let y y y be the response vector of a random subject on P items<ref type="foot" target="#foot_0">foot_0</ref> . We assume y y y is centered and follows a multivariate Gaussian density: y y y ∼ N P (0 0 0, Σ Σ Σ) ,</p><p>In which Σ Σ Σ is a P × P variance-covariance matrix, estimated by some model-implied Σ Σ Σ. Estimating Σ Σ Σ is often done through some form of maximum likelihood estimation. If we measure N independent samples of y y y we can formulate the N × P matrix Y Y Y containing realization y y y i as its ith row. Let S S S represent the sample variance-covariance matrix of Y Y Y :</p><formula xml:id="formula_0">S S S = 1 N -1 Y Y Y Y Y Y .</formula><p>In maximum likelihood estimation, we use S S S to compute and minimize -2 times the loglikelihood function to find Σ Σ Σ <ref type="bibr">(Lawley, 1940;</ref><ref type="bibr">Jöreskog, 1967;</ref><ref type="bibr">Jacobucci et al., 2016)</ref>:</p><formula xml:id="formula_1">min Σ Σ Σ</formula><p>log det Σ Σ Σ + Trace S S S Σ Σ Σ -1 -log det Ŝ S S -P .</p><p>(1)</p><p>To optimize this expression, Σ Σ Σ should be estimated as closely as possible to S S S and perfect fit is obtained if Σ Σ Σ = S S S. A properly identified model with the same number of parameters (K) used to form Σ Σ Σ as there are unique elements in S S S (P (P + 1)/2 parameters) will lead to Σ Σ Σ = S S S and therefore a saturated model. The goal of modeling multivariate Gaussian data is to obtain some model for Σ Σ Σ with positive degrees of freedom, K &lt; P (P + 1)/2, in which Σ Σ Σ resembles S S S closely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structural Equation Modeling</head><p>In Confirmatory Factor Analysis (CFA), Y Y Y is typically assumed to be a causal linear effect of a set of M centered latent variables, η η η, and independent residuals or error, ε ε ε:</p><formula xml:id="formula_2">y y y = Λ Λ Λη η η + ε ε ε.</formula><p>Here, Λ Λ Λ represents a P × M matrix of factor loadings. This model implies the following model for Σ Σ Σ:</p><formula xml:id="formula_3">Σ Σ Σ = Λ Λ ΛΨ Ψ ΨΛ Λ Λ + Θ Θ Θ,<label>(2)</label></formula><p>in which Ψ Ψ Ψ = Var (η η η) and Θ Θ Θ = Var (ε ε ε). In Structural Equation Modeling (SEM), Var (η η η) can further be modeled by adding structural linear relations between the latent variables<ref type="foot" target="#foot_1">foot_1</ref> :  <ref type="bibr">(Wright, 1934)</ref>. The Θ Θ Θ matrix is, like Σ Σ Σ and S S S, a P × P matrix; if Θ Θ Θ is fully estimated-contains no restricted elements-then Θ Θ Θ alone constitutes a saturated model. Therefore, to make either (2) or (3) identifiable, Θ Θ Θ must be strongly restricted. Typically, Θ Θ Θ is set to be diagonal, a restriction often termed local independence <ref type="bibr">(Lord et al., 1968;</ref><ref type="bibr">Holland and Rosenbaum, 1986)</ref> because indicators are independent of each other after conditioning on the set of latent variables. To improve fit, select off-diagonal elements of Θ Θ Θ can be estimated, but systematic violations of local independence-many nonzero elements in Θ Θ Θ-are not possible as that will quickly make (2) and (3) saturated or even over-identified. More precisely, Θ Θ Θ can not be fully-populated-some elements of Θ Θ Θ must be set to equal zero-when latent variables are used. An element of Θ Θ Θ being fixed to zero indicates that two variables are locally independent after conditioning on the set of latent variables. As such, local independence is a critical assumption in both CFA and SEM; if local independence is systematically violated, CFA and SEM will never result in correct models.</p><formula xml:id="formula_4">η η η = B B Bη η η + ζ ζ ζ,</formula><p>The assumption of local independence has led to critiques of the factor model and its usage in psychology; local independence appears to be frequently violated due to direct causal effects, semantic overlap, or reciprocal interactions between putative indicators of a latent variable <ref type="bibr">(Borsboom, 2008;</ref><ref type="bibr">Cramer et al., 2010;</ref><ref type="bibr">Borsboom et al., 2011;</ref><ref type="bibr">Cramer et al., 2012;</ref><ref type="bibr">Schmittmann et al., 2013)</ref>. In psychopathology research, local independence of symptoms given a person's level of a latent mental disorder has been questioned <ref type="bibr">(Borsboom and Cramer, 2013)</ref>. For example, three problems associated with depression are "fatigue", "concentration problems" and "rumination". It is plausible that a person who suffers from fatigue will also concentrate more poorly, as a direct result of being fatigued and regardless of his or her level of depression. Similarly, rumination might lead to poor concentration. In another example, <ref type="bibr">Kossakowski et al. (2015)</ref> describe the often-used SF-36 questionnaire <ref type="bibr">(Ware Jr and Sherbourne, 1992)</ref> designed to measure health related quality of life. The SF-36 contains items such as "can you walk for more than one kilometer" and "can you walk a few hundred meters". Clearly, these items can never be locally independent after conditioning on any latent trait, as one item (the ability to walk a few hundred meters) is a prerequisite for the other (walking more than a kilometer). In typical applications, the excessive covariance between items of this type is typically left unmodeled, and treated instead by combining items into a subscale or total score that is subsequently subjected to factor analysis; of course, however, this is tantamount to ignoring the relevant psychometric problem rather than solving it.</p><p>Given the many theoretically expected violations of local independence in psychometric applications, many elements of Θ Θ Θ in both ( <ref type="formula" target="#formula_3">2</ref>) and (3) should ordinarily be freely estimated. Especially when violations of local independence are expected to be due to causal effects of partial overlap, residual correlations should not be constrained to zero; in addition, a chain of causal relationships between indicators can lead to all residuals to become correlated. Thus, even when latent factors cause much of the covariation between measured items, fitting a latent variable model that involves local independence may not fully account for correlation structure between measured items. Of course, in practice, many psychometricians are aware of this problem, which is typically addressed by freeing up correlations between residuals to improve model fit. However, this is usually done in an ad-hoc fashion, on the basis of inspection of modification indices and freeing up error covariances one by one, which is post hoc, suboptimal, and involves an uncontrolled journey through the model space. As a result, it is often difficult to impossible to tell how exactly authors arrived at their final reported models. As we will show later in this paper, this process can be optimized and systematized using network models to connect residuals on top of a latent variable structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Modeling</head><p>Recent authors have suggested that the potential presence of causal relationships between measured variables may allow the explanation of the covariance structure without the need to invoke any latent variables <ref type="bibr">(Borsboom, 2008;</ref><ref type="bibr">Cramer et al., 2010;</ref><ref type="bibr">Borsboom et al., 2011;</ref><ref type="bibr">Schmittmann et al., 2013)</ref>. The interactions between indicators can instead be modeled as a network, in which indicators are represented as nodes that are connected by edges representing pairwise interactions. Such interactions indicate the presence of covariances that cannot be explained by any other variable in the model and can represent-possibly reciprocal-causal relationships. Estimating a network structure on psychometric data is termed network psychometrics <ref type="bibr">(Epskamp et al., ress)</ref>. Such a network of interacting compo-</p><formula xml:id="formula_5">Y 1 Y 2 Y 3 Figure 1 .</formula><p>Example of a pairwise Markov Random Field model. Edges in this model indicate pairwise interactions, and are drawn using undirected edges to distinguish from (bidirectional) covariances. Rather than a model for marginal associations (such as a network indicating covariances), this is a model for conditional associations. The network above encodes that Y 1 and Y 3 are independent after conditioning on Y 2 . Such a model allows all three variables to correlate while retaining one degree of freedom (the model only has two parameters) nents can generate data that fit factor models well, as is commonly the case in psychology. <ref type="bibr">van der Maas et al. (2006)</ref> showed that the positive manifold of intelligence-which is commonly explained with the general factor for intelligence, g-can emerge from a network of mutually benefiting cognitive abilities. <ref type="bibr">Borsboom et al. (2011)</ref> showed that a network of psychopathological symptoms, in which disorders are modeled as clusters of symptoms, could explain comorbidity between disorders. Furthermore, <ref type="bibr">Epskamp et al. (ress)</ref> showed that the Ising model for ferromagnetism <ref type="bibr">(Ising, 1925)</ref>, which models magnetism as a network of particles, is equivalent to multidimensional item response theory <ref type="bibr" target="#b101">(Reckase, 2009)</ref>.</p><p>In network psychometrics, psychometric data are modeled through directed or undirected networks. Directed networks are equivalent to path analysis models. For modeling undirected networks, pairwise Markov Random Fields <ref type="bibr">(Lauritzen, 1996;</ref><ref type="bibr">Murphy, 2012)</ref> are used. In these models, each variable is represented by a node, and nodes are connected by a set of edges. If two nodes, y j and y k , are not connected by an edge, then this means they are independent after conditioning on the set of all other nodes, y y y -(j,k) . Whenever two nodes cannot be rendered independent conditional on the other nodes in the system, they are said to feature in a pairwise interaction, which is represented by an undirected edge-an edge with no arrows-to contrast such an effect from covariances typically represented in the SEM literature with bidirectional edges. Figure <ref type="figure" target="#fig_11">1</ref> represents such a network model, in which nodes y 1 and y 3 are independent after conditioning on node y 2 . Such a model can readily arise from direct interactions between the nodes. For example, this conditional independence structure would emerge if y 2 is a common cause of y 1 and y 3 , or if y 2 is the mediator in a causal path between y 1 and y 3 . In general, it is important to note that pairwise interactions are not mere correlations; two variables may be strongly correlated but unconnected (e.g., when both are caused by another variable in the system) and they may be uncorrelated but strongly connected in the network (e.g., when they have a common effect in the system). For instance, in the present example the model does not indicate that y 1 and y 3 are uncorrelated, but merely indicates that any correlation between y 1 and y 3 is due to their mutual interaction with y 2 ; a network model in which either directly or indirectly connected paths exist between all pairs of nodes typically implies a fully populated (no zero elements) variance-covariance matrix.</p><p>In the case of multivariate Gaussian data, this model is termed the Gaussian Graphical Model (GGM; <ref type="bibr">Lauritzen 1996)</ref>. Here, the partial correlation coefficient is sufficient to test the degree of conditional independence of two variables after conditioning on all other variables; if the partial correlation coefficient is zero, there is conditional independence and hence no edge in the network. As such, partial correlation coefficients can directly be used in the network as edge weights; the strength of connection between two nodes<ref type="foot" target="#foot_2">foot_2</ref> . Such a network is typically encoded in a symmetrical and real valued p × p weight matrix, Ω Ω Ω, in which element ω jk represents the edge weight between node j and node k:</p><formula xml:id="formula_6">Cor y j , y k | y y y -(j,k) = ω jk = ω kj .</formula><p>The partial correlation coefficients can be directly obtained from the inverse of variancecovariance matrix Σ Σ Σ, also termed the precision matrix K K K <ref type="bibr">(Lauritzen, 1996)</ref>:</p><formula xml:id="formula_7">Cor y j , y k | y y y -(j,k) = - κ jk √ κ kk √ κ jj .</formula><p>Thus, element κ jk of the precision matrix is proportional to to the partial correlation coefficient of variables y j and y k after conditioning on all other variables. Since this process simply involves standardizing the precision matrix, we propose the following model<ref type="foot" target="#foot_3">foot_3</ref> :</p><formula xml:id="formula_8">Σ Σ Σ = K K K -1 = ∆ ∆ ∆ (I I I -Ω Ω Ω) -1 ∆ ∆ ∆,<label>(4)</label></formula><p>in which ∆ ∆ ∆ is a diagonal matrix with δ jj = κ -1 2 jj and Ω Ω Ω has zeroes on the diagonal. This model allows for confirmative testing of the GGM structures on psychometric data. Furthermore, the model can be compared to a saturated model (fully populated off-diagonal values of Ω Ω Ω) and the independence model (Ω Ω Ω = O O O), allowing one to obtain χ 2 fit statistics as well as fit indices such as the RMSEA <ref type="bibr">(Browne and Cudeck, 1992)</ref> and CFI <ref type="bibr">(Bentler, 1990)</ref>. Such methods of assessing model fit have not yet been used in network psychometrics.</p><p>Similar to CFA and SEM, the GGM relies on a critical assumption; namely, that covariances between observed variables are not caused by any latent or unobserved variable. If we estimate a GGM in a case where, in fact, a latent factor model was the true data generating structure, then generally we would expect the GGM to be saturated-i.e., there would be no missing edges in the GGM <ref type="bibr" target="#b7">(Chandrasekaran et al., 2012)</ref>. A missing edge in the GGM indicates the presence of conditional independence between two indicators given all other indicators; we do not expect indicators to become independent given subsets of other indicators (see also <ref type="bibr">Ellis and Junker 1997;</ref><ref type="bibr">Holland and Rosenbaum 1986)</ref>. Again, this critical assumption might not be plausible. While variables such as "Am indifferent to the feelings of others" and "Inquire about others' well-being" quite probably interact with each other, it might be far-fetched to assume that no unobserved variable, such as a personality trait, in part also causes some of the variance in responses on these items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalizing Factor Analysis and Network Modeling</head><p>We propose two generalizations of both SEM and the GGM that both allow the modeling of network structures in SEM. In the first generalization, we adopt the CFA<ref type="foot" target="#foot_4">foot_4</ref> decomposition in (2) and model the variance-covariance matrix of latent variables as a GGM:</p><formula xml:id="formula_9">Ψ Ψ Ψ = ∆ ∆ ∆ Ψ Ψ Ψ (I I I -Ω Ω Ω Ψ Ψ Ψ ) -1 ∆ ∆ ∆ Ψ Ψ Ψ .</formula><p>This framework can be seen as modeling conditional independencies between latent variables not by directed effects (as in SEM) but as an undirected network. As such, we term this framework latent network modeling (LNM).</p><p>In the second generalization, we adopt the SEM decomposition of the variancecovariance matrix in (3) and allow the residual variance-covariance matrix Θ Θ Θ to be modeled as a GGM:</p><formula xml:id="formula_10">Θ Θ Θ = ∆ ∆ ∆ Θ Θ Θ (I I I -Ω Ω Ω Θ Θ Θ ) -1 ∆ ∆ ∆ Θ Θ Θ .</formula><p>Because this framework conceptualizes associations between residuals as pairwise interactions, rather than correlations, we term this framework Residual Network Modeling (RNM). Using this framework allows-as will be described below-for a powerful way of fitting a confirmatory factor structure even though local independence is systematically violated and all residuals are correlated.</p><p>Figure <ref type="figure" target="#fig_13">2</ref> shows four different examples of possible models that are attainable under the SEM, LNM and RNM frameworks. Panel (a) shows a typical SEM model in which one latent variable functions as a common cause of two others. Panel (b) shows a network model which can be estimated using both the RNM and the LNM frameworks. Panel (c) shows a completely equivalent LNM model to the SEM model of Panel (a) in which the direction of effect between latent variables is not modeled. Finally, panel (d) shows a model in which three exogenous latent variables underlie a set of indicators of which the residuals form a network. The remainder of this section will describe RNM and LNM in more detail and will outline the class of situations in which using these models is advantageous over CFA or SEM.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Network Modeling</head><p>The LNM framework models the latent variance-covariance matrix of a CFA model as a GGM:</p><formula xml:id="formula_11">Σ Σ Σ = Λ Λ Λ∆ ∆ ∆ Ψ Ψ Ψ (I I I -Ω Ω Ω Ψ Ψ Ψ ) -1 ∆ ∆ ∆ Ψ Ψ Ψ Λ Λ Λ + Θ Θ Θ. (<label>5</label></formula><formula xml:id="formula_12">)</formula><p>This allows researchers to model conditional independence relationships between latent variables without making the implicit assumptions of directionality or acyclicness. In SEM, B B B is typically modeled as a directed acyclic graph (DAG), meaning that elements of B B B can be represented by directed edges, and following along the path of these edges it is not possible to return to any node (latent variable). The edges in such a DAG can be interpreted as causal, and in general they imply a specific set of conditional independence relationships between the nodes <ref type="bibr">(Pearl, 2000)</ref>.</p><p>While modeling conditional independence relationships between latent variables as a DAG is a powerful tool for testing strictly confirmatory hypotheses, it is less useful for more exploratory estimation. Though there have been recent advances in exploratory estimation of DAGs within an SEM framework (e.g., <ref type="bibr">Gates and Molenaar 2012;</ref><ref type="bibr">Rosa et al. 2012</ref>), many equivalent DAGs can imply the same conditional independence relationships, and thus fit the data equally well even though their causal interpretation can be strikingly different <ref type="bibr">(MacCallum et al., 1993)</ref>. Furthermore, the assumption that the generating model is acyclic-which, in practice, often is made on purely pragmatic grounds to identify a model-is problematic in that much psychological behavior can be assumed to have at least some cyclic and complex behavior and feedback <ref type="bibr">(Schmittmann et al., 2013)</ref>. Thus, the true conditional independence relationships in a dataset can lead to many equivalent compositions of B B B, and possibly none of them are the true model.</p><p>In psychometrics and SEM the GGM representation has not been very prominent, even though it has some manifest benefits over the attempt to identify DAGs directly. For example, by modeling conditional independence relationships between latent variables as a GGM, many relationships can be modeled in a simpler way as compared to a DAG. In addition, in the GGM each set of conditional independence relations only corresponds to one model: there are no equivalent GGMs with the same nodes. Figure <ref type="figure" target="#fig_15">3</ref> shows a comparison of several conditional independence relations that can be modeled equivalently or not by using a GGM or by using a DAG. Panel (a) and Panel (b) show two DAGs that represent the same conditional independence relations, y 1 ⊥ ⊥ y 3 | y 2 , which can both be represented by the same GGM shown in Panel (e) and Panel (f). There are some conditional independence relations that a GGM cannot represent in the same number of parameters as a DAG; Panel (c) shows a collider structure that cannot be exactly represented by a GGM (the best fitting GGM would feature three edges instead of two). On the other hand, there are also conditional independence relationships that a GGM can represent and a DAG cannot; the cycle of Panel (h) cannot be represented by a DAG. Further equivalences and differences between GGMs and DAGs are beyond the scope of this paper, but haven been well described in the literature (e.g., chapter 3 of <ref type="bibr">Lauritzen 1996;</ref><ref type="bibr">Koller and Friedman 2009;</ref><ref type="bibr" target="#b33">Kolaczyk 2009)</ref>. In sum, the GGM offers a natural middle ground between zero-order correlations and DAGs: every set of zero-order correlations implies exactly one GGM, and every DAG implies exactly one GGM. In a sense, the road from correlations to DAGs (including hierarchical factor models) thus always must pass through the realm of GGMs, which acts as a bridge</p><formula xml:id="formula_13">Y 1 Y 2 Y 3 (a) Y 1 Y 2 Y 3 (b) Y 1 Y 2 Y 3 (c)</formula><p>No equivalent GGM with the same # of parameters</p><formula xml:id="formula_14">(d) Y 1 Y 2 Y 3 (e) Y 1 Y 2 Y 3 (f) No equivalent GGM with the same # of parameters (g) Y 1 Y 2 Y 3 Y 4 (h)</formula><p>Figure <ref type="figure" target="#fig_15">3</ref> . Equivalent models between directed acyclic graphs (DAG; top) and Gaussian Graphical Models (GGM; bottom). Each column of graphs show two models that are equivalent. Panels (a), (b), (e) and (f) all represent the same conditional independence structure: Y 1 and Y 3 are independent after conditioning on Y 2 . Panel (c) represents that Y 1 and Y 3 are marginally independent even though they both are correlated with Y 2 , a structure that cannot be represented in a GGM with only two edges. Panel (h) shows that Y 1 and Y 3 are independent after conditioning on the set Y 2 and Y 4 , which cannot be represented with a DAG of four edges.</p><p>between the correlational and causal worlds.</p><p>Because there are no equivalent undirected models possible, LNM offers a powerful tool for exploratory estimation of relationships between latent variables. For example, suppose one encounters data generated by the SEM model in Figure <ref type="figure" target="#fig_13">2</ref>, Panel (a). Without prior theory on the relations between latent variables, exploratory estimation on this dataset would lead to three completely equivalent models: the one shown in Figure <ref type="figure" target="#fig_13">2</ref>, Panel (c) and two models in which the common cause instead is the middle node in a causal chain. As the number of latent variables increases, the potential number of equivalent models that encode the same conditional independence relationships grows without bound. The LNM model in Panel (c) of Figure <ref type="figure" target="#fig_13">2</ref> portrays the same conditional independence relationship as the SEM model in Panel (a) of Figure <ref type="figure" target="#fig_13">2</ref>, while having no equivalent model. Exploratory estimation could easily find this model, and portrays the retrieved relationship in a clear and unambiguous way.</p><p>A final benefit of using LNM models is that they allow network analysts to construct a network while taking measurement error into account. So far, networks have been constructed based on single indicators only and no attempt has been made to remediate measurement error. By forming a network on graspable small concepts measured by a few indicators, the LNM framework can be used to control for measurement error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual Network Modeling</head><p>In the RNM framework the residual structure of SEM is modeled as a GGM:</p><formula xml:id="formula_15">Σ Σ Σ = Λ Λ Λ (I I I -B B B) -1 Ψ Ψ Ψ (I I I -B B B) -1 Λ Λ Λ + ∆ ∆ ∆ Θ Θ Θ (I I I -Ω Ω Ω Θ Θ Θ ) -1 ∆ ∆ ∆ Θ Θ Θ . (<label>6</label></formula><formula xml:id="formula_16">)</formula><p>This modeling framework conceptualizes latent variable and network modeling as two sides of the same coin, and offers immediate benefits to both. In latent variable modeling, RNM allows for the estimation of a factor structure (possibly including structural relationships between the latent variables), while having no uncorrelated errors and thus no local independence. The error-correlations, however, are still highly structured due to the residual network structure. This can be seen as a compromise between the ideas of network analysis and factor modeling; while we agree that local independence is plausibly violated in many psychometric tests, we think the assumption of no underlying latent traits and therefore a sparse GGM may often be too strict. For network modeling, RNM allows a researcher to estimate a sparse network structure while taking into account that some of the covariation between items was caused by a set of latent variables. Not taking this into account would lead to a saturated model <ref type="bibr" target="#b7">(Chandrasekaran et al., 2012)</ref>, whereas the residual network structure can be sparse.</p><p>To avoid confusion between residual correlations, we will denote edges in Ω Ω Ω Θ Θ Θ residual interactions. Residual interactions can be understood as pairwise linear effects, possibly due to some causal influence or partial overlap between indicators that is left after controlling for the latent structure. Consider again the indicators for agreeableness "Am indifferent to the feelings of others" and "Inquire about others' well-being". It seems clear that we would not expect these indicators to be locally independent after conditioning on agreeableness; being indifferent to the feelings of others will cause one to not inquire about other's wellbeing. Thus, we could expect these indicators to feature a residual interaction; some degree of correlation between these indicators is expected to remain, even after conditioning on the latent variable and all other indicators in the model.</p><p>The RNM framework in particular offers a new way of improving the fit of confirmatory factor models. In contrast to increasingly popular methods such as exploratory SEM (ESEM; <ref type="bibr">Marsh et al. 2014)</ref> or LASSO regularized SEM models <ref type="bibr">(Jacobucci et al., 2016)</ref>, the RNM framework improves the fit by adding residual interactions rather than allowing for more cross-loadings. The factor structure is kept exactly intact as specified in the confirmatory model. Importantly, therefore, the interpretation of the latent factor does not change. This can be highly valuable in the presence of a strong theory on the latent variables structure underlying a dataset even in the presence of violations of local independence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploratory Network Estimation</head><p>Both the LNM and RNM modeling frameworks allow for confirmative testing of network structures. Confirmatory estimation is straightforward and similar to estimating SEM models, with the exception that instead of modeling Ψ Ψ Ψ or Θ Θ Θ now the latent network Ω Ω Ω Ψ Ψ Ψ or Ω Ω Ω Θ Θ Θ is modeled. Furthermore, both modeling frameworks allow for the confirmatory fit of a network model. In LNM, a confirmatory network structure can be tested by setting Λ Λ Λ = I I I and Θ Θ Θ = O O O; in RNM, a confirmatory network model can be tested by omitting any latent variables. We have developed the R package lvnet<ref type="foot" target="#foot_5">foot_5</ref> , which utilizes OpenMx <ref type="bibr" target="#b46">(Neale et al., 2016)</ref> for confirmative testing of RNM and LNM models (as well as a combination of the two). The lvnet function can be used for this purpose by specifying the fixed and the free elements of model matrices. The package returns model fit indices (e.g., the RMSEA, CFI and χ 2 value), parameter estimates, and allows for model comparison tests.</p><p>Often the network structure, either at the residual or the latent level, is unknown and needs to be estimated. To this end, the package includes two exploratory search algorithms described below: step-wise model search and penalized maximum likelihood estimation. For both model frameworks and both search algorithms, we present simulation studies to investigate the performance of these procedures. As is typical in simulation studies investigating the performance of network estimation techniques, we investigated the sensitivity and specificity <ref type="bibr">(van Borkulo et al., 2014)</ref>. These measures investigate the estimated edges versus the edges in the true model, with a 'positive' indicating an estimated edge and a 'negative' indicating an edge that is estimated to be zero. Sensitivity, also termed the true positive rate, gives the ratio of the number of true edges that were detected in the estimation versus the total number of edges in the true model: sensitivity = # true positives # true positives + # of false negatives Specificity, also termed the true negative rate, gives the ratio of true missing edges detected in the estimation versus the total number of absent edges in the true model: specificity = # true negatives # true negatives + # false positives</p><p>The specificity can be seen as a function of the number of false positives: a high specificity indicates that there were not many edges detected to be nonzero that are zero in the true model. To favor degrees of freedom, model sparsity and interpretability, specificity should be high all-around-estimation techniques should not result in many false positives-whereas sensitivity should increase as a function of the sample size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulating Gaussian Graphical models</head><p>In all simulation studies reported here, networks were constructed in the same way as done by <ref type="bibr">Yin and Li (2011)</ref> in order to obtain a positive definite inverse-covariance matrix K K K. First, a network structure was generated without weights. Next, weights were drawn randomly from a uniform distribution between 0.5 and 1, and made negative with 50% probability. The diagonal elements of K K K were then set to 1.5 times the sum of all absolute values in the corresponding row, or 1 if this sum was zero. Next, all values in each row were divided by the diagonal value, ensuring that the diagonal values become 1. Finally, the matrix was made symmetric by averaging the lower and upper triangular elements. In the chain graphs used in the following simulations, this algorithm created networks in which the non-zero partial correlations had a mean of 0.33 and a standard deviation of 0.04.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stepwise Model Search</head><p>In exploratory search, we are interested in recovering the network structure of either Ω Ω Ω Ψ Ψ Ψ in LNM or Ω Ω Ω Θ Θ Θ in RNM. This can be done through a step-wise model search, either based on χ 2 difference tests (Algorithm 1) or on minimization of some information criterion (Algorithm 2) such as the Akaike information criterion (AIC), Bayesian information criterion (BIC) or the extended Bayesian information criterion (EBIC; <ref type="bibr">Chen and Chen 2008)</ref> which is now often used in network estimation <ref type="bibr">(van Borkulo et al., 2014;</ref><ref type="bibr">Foygel and Drton, 2010)</ref>. In LNM, removing edges from Ω Ω Ω Ψ Ψ Ψ cannot improve the fit beyond that of an already fitting CFA model. Hence, model search for Ω Ω Ω Ψ Ψ Ψ should start at a fully populated initial setup for Ω Ω Ω Ψ Ψ Ψ . In RNM, on the other hand, a densely populated Ω Ω Ω Θ Θ Θ would lead to an over-identified model, and hence the step-wise model search should start at an empty network Ω Ω Ω Θ Θ Θ = O O O. The function lvnetSearch in the lvnet package can be used for both search algorithms. We performed a simulation study to assess the performance of the above mentioned step-wise search algorithms in LNM models. Figure <ref type="figure" target="#fig_2">4</ref> shows the LNM model under which we simulated data. In this model, four latent factors with three indicators each were connected in a latent network. The latent network was a chain network, leading all latent variables to be correlated according to a structure that cannot be represented in SEM. Factor loadings and residual variances were set to 1, and the network weights were simulated as described in the section "Simulating Gaussian Graphical models". The simulation study followed a 5 × 4 design: the sample size was varied between 50, 100, 250, 500 and 1 000 to represent typical sample sizes in psychological research, and the stepwise evaluation criterion was either χ 2 difference testing, AIC, BIC or EBIC (using a tuning parameter of 0.5). Each condition was simulated 1 000 times, resulting in 20 000 total simulated datasets.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows the results of the simulation study. Data is represented in standard boxplots <ref type="bibr">(McGill et al., 1978)</ref>: the box shows the 25th, 50th (median) 75th quantiles, the whiskers range from the largest values in 1.5 times the inter-quantile range (75th -25th quantile) and points indicate outliers outside that range. In each condition, we investigated the sensitivity and specificity. The top panel shows that sensitivity improves with sample size, with AIC performing best and EBIC worst. From sample sizes of 500 and higher all estimation criterion performed well in retrieving the edges. The bottom panel shows that specificity is generally very high, with EBIC performing best and AIC worst. These results indicate that the step-wise procedure is conservative and prefers simpler models to more complex models; missing edges are adequately detected but present edges in the true model might go unnoticed except in larger samples. With sample sizes over 500, all four estimation AIC Chi-square BIC EBIC q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 AIC Chi-square BIC EBIC q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00  Figure <ref type="figure" target="#fig_7">7</ref> shows the model under which data were simulated: two latent variables with 5 indicators each. The residual network was constructed to be a chain graph linking a residual of an indicator of one latent variable to two indicators of the other latent variable. This structure cannot be represented by a DAG and causes all residuals to be connected, so that Θ Θ Θ is fully populated. Factor loadings and residual variances were set to 1, the factor covariance was set to 0.25, and the network weights were simulated as described in the section "Simulating Gaussian Graphical models".</p><p>The simulation study followed a 5×4 design; sample size was again varied between 50, 100, 250, 500 and 1 000, and models were estimated using either χ 2 significance testing, AIC, BIC or EBIC. Factor loadings and factor variances were set to 1 and the factor correlation was set to 0.25. The weights in Ω Ω Ω Θ Θ Θ were chosen as described in the section "Simulating Gaussian Graphical models". Each condition was simulated 1, 000 times, leading to 20 000 total datasets.</p><p>Figure <ref type="figure" target="#fig_7">7</ref> shows the results of the simulation study. The top panel shows that sensitivity increases with sample size and performs best when using AIC as the criterion. BIC performed comparably in sensitivity to χ 2 testing and EBIC performed the worst. The bottom panel shows that specificity was very high for all sample sizes and all criteria, with EBIC performing best and AIC worst. These results indicate that the number of false pos-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AIC</head><p>Chi-square BIC EBIC q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 AIC Chi-square BIC EBIC q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 Each condition was replicated 1 000 times, leading to 20 000 total simulated datasets. High sensitivity indicates that the method is able to detect edges in the true model, and high specificity indicates that the method does not detect edges that are zero in the true model.</p><p>itives is very low and that the method is on average well capable of discovering true edges for sample size larger than 250. In sum, all four criteria perform well with EBIC erring on the side of caution and AIC erring on the side of discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LASSO Regularization</head><p>While the step-wise model selection algorithms perform well in retrieving the correct network structure, they are very slow when the number of nodes in the network increases (e.g., more than 10 nodes). This is particularly important in the context of RNM, in which the number of indicators can be larger than 10 even in small models. A popular method for fast estimation of high-dimensional network structures is by applying the least absolute shrinkage and selection operator (LASSO; <ref type="bibr">Tibshirani 1996)</ref>. LASSO regularization has also recently been introduced in the SEM literature <ref type="bibr">(Jacobucci et al., 2016)</ref> as a method for obtaining sparser structures of Λ Λ Λ and B B B. In the LASSO, instead of optimizing the likelihood function as described in (1), a penalized likelihood is optimized <ref type="bibr">(Jacobucci et al., 2016)</ref>:</p><formula xml:id="formula_17">min Σ Σ Σ log det Σ Σ Σ + Trace S S S Σ Σ Σ -1 -log det Ŝ S S -P + νPenalty , (<label>7</label></formula><formula xml:id="formula_18">)</formula><p>in which ν denotes a tuning parameter controlling the level of penalization. The penalty here is taken to be the sum of absolute parameters:</p><formula xml:id="formula_19">Penalty = &lt;i,j&gt; |ω ij |,</formula><p>in which ω ij denotes an element from either Ω Ω Ω Ψ Ψ Ψ or Ω Ω Ω Θ Θ Θ . Other penalty functions may be used as well-such as summing the squares of parameter estimates (ridge regression; Hoerl and Kennard 1970) or combining both absolute and squared values (elastic net; Zou and Hastie 2005)-but these are not currently implemented in lvnet. The benefit of the LASSO is that it returns models that perform better in cross-validation. In addition, the LASSO yields sparse models in which many relationships are estimated to be zero. </p><formula xml:id="formula_20">(Ω Ω Ω Θ Θ Θ , Ω Ω Ω Ψ Ψ Ψ , Θ Θ Θ, Ψ Ψ Ψ, Λ Λ Λ or B B B</formula><p>) and a given value for the tuning parameter ν. The optimizer used in lvnet does not return exact zeroes. To circumvent this issue, any absolute parameter below some small value (by default = 0.0001) is treated as zero in counting the number of parameters and degrees of freedom <ref type="bibr" target="#b61">(Zou et al., 2007)</ref>. The lvnetLasso function implements the search algorithm described in Algorithm 3 to automatically choose an appropriate tuning parameter, use that for model selection and rerun the model to obtain a comparable fit to non-regularized models. In this algorithm, a sequence of tuning parameters is tested, which is set by default to a logarithmically spaced sequence of 20 values between 0.01 and 1.</p><p>Simulation Study 3: Latent Network Modeling. We studied the performance of LASSO penalization in estimating the latent network structure in a similar simulation study to the study of the step-wise procedure described above. Data were simulated under a similar model to the one shown in Figure <ref type="figure" target="#fig_2">4</ref>, except that now 8 latent variables were used leading to a total of 24 observed variables. All parameter values were the same as in simulation study 1. The simulation followed a 5 × 3 design. Sample size was varied between 100, 250, 500, 1 000 and 2 500, and for each sample size 1 000 datasets were simulated leading to a total of 5 000 generated datasets. On these datasets the best model was selected using either AIC, BIC or EBIC, leading to 15 000 total replications. In each replication, sensitivity and specificity were computed. Figure <ref type="figure" target="#fig_8">8</ref> shows that AIC had a relatively poor specificity all-around, but a high sensitivity. EBIC performed well with sample sizes of 500 and higher.</p><p>Simulation Study 4: Residual Network Modeling. To assess the performance of LASSO in estimating the residual network structure we simulated data as in Figure <ref type="figure" target="#fig_5">6</ref>, except that in this case four latent variables were used, each with 5 indicators, the residuals of which were linked via a chain graph. All parameter values were the same as in simulation study 2. The design was the same as in simulation study 3, leading to 5 000 generated datasets on which AIC, BIC or EBIC were used to select the best model. While Figure <ref type="figure" target="#fig_22">9</ref> shows good performance of the LASSO in retrieving the residual network structure and similar results as before: AIC performs the worst in specificity and EBIC the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empirical Example: Personality Inventory</head><p>In this section, we demonstrate LNM and RNM models by confirmatively testing a model and exploratively searching a residual and a latent network structure. We use the lvnet package, which can be installed in R as follows: &gt; install.packages("lvnet") &gt; library("lvnet")</p><p>To exemplify the method, we will use a dataset from the psych package <ref type="bibr" target="#b48">(Revelle, 2010)</ref> on the Big 5 personality traits <ref type="bibr">(Benet-Martinez and John, 1998;</ref><ref type="bibr">Digman, 1989;</ref><ref type="bibr" target="#b23">Goldberg, 1990</ref><ref type="bibr" target="#b22">Goldberg, , 1993;;</ref><ref type="bibr">McCrae and Costa, 1997)</ref>. This dataset consists of 2800 observations of 25 items designed to measure the 5 central personality traits with 5 items per trait. We estimated the CFA model on the BFI dataset. Next, we used LASSO estimation to the RNM model using 100 different tuning parameters and using EBIC as criterion to maximize specificity and search for a sparse residual network. The fully correlated covariance matrix of latent variables is equivalent to a fully connected latent network structure. Thus, after fitting a RNM model, we can again apply LASSO to estimate a latent network in the resulting model, which we abbreviate here to an RNM+LNM model. The R code used for this analysis can be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AIC</head><p>BIC EBIC q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 Sensitivity (true positive rate) AIC BIC EBIC q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00  <ref type="figure" target="#fig_2">4</ref> was used except now with 4 latent variables leading to 24 observed variables. For each sample size 1 000 datasets were generated, leading to 5 000 total simulated datasets on which AIC, BIC or EBIC was used to select the best model. High sensitivity indicates that the method is able to detect edges in the true model, and high specificity indicates that the method does not detect edges that are zero in the true model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AIC BIC EBIC</head><p>q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 Sensitivity (true positive rate) AIC BIC EBIC q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00  q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Residual network Figure <ref type="figure" target="#fig_23">10</ref> . Visualization of the factor structure and latent network (left) and the residual network (right) of the BFI personality dataset from the psych package in R. LASSO estimation with 100 different tuning parameters in combination with EBIC model selection was used to first estimate the residual network structure and following the latent network structure.</p><p>Table <ref type="table" target="#tab_6">1</ref> shows the fit of the three models. The CFA model fits poorly. The RNM model has substantively improved fit and resulted in good fit indices overall. The estimated RNM+LNM model showed that 5 edges could be removed from the latent network after taking residual interactions into account. Figure <ref type="figure" target="#fig_23">10</ref> shows the factor structure and residual network of the final RNM+LNM model. It can be seen that Agreeableness is now only connected to extraversion: after taking into account someone's level of extraversion agreeableness is independent of the other three personality traits. Extraversion is the most central node in this network and the only trait that is directly linked to all other traits. The residual network shows many meaningful connections. While seemingly densely connected, this network only has 30% of all possible edges in a network of that size, leading the model to have 176 degrees of freedom. The corresponding residual covariance structure is fully populated with no zero elements.</p><p>It should be noted that the procedures used in this example are highly explorative. While Table <ref type="table" target="#tab_6">1</ref> shows that the RNM fits better than the CFA model, the CFA model is solely based on theory whereas the RNM model was found through high-dimensional model search. As a result, to substantially interpret the structure found in Figure <ref type="figure" target="#fig_23">10</ref> it should first be replicated in independent samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper we introduced a formal psychometric model for network modeling of multivariate normal data. We contrasted this model with latent variable models as commonly used in CFA and SEM. Furthermore, using the CFA and SEM frameworks, we proposed two generalizations of the network model to encompass latent variable structures within the network paradigm. In the first generalization, LNM, we construct a network among the latent variables, whereas in the second generalization, RNM, a network is formed among residuals of indicators. Both frameworks offer powerful benefits over both latent variable and network modeling. From the perspective of latent variable modeling, the LNM framework allows one to exploratively search for conditional independence relationships between latent variables without the need for prior theory, and the RNM framework allows one to model latent common causes without assuming local independence. From the perspective of network modeling, the LNM framework allows one to model network structures while taking measurement error into account, and the RNM framework allows one to estimate a network structure, even when all nodes are in part caused by unobserved or latent variables. In addition, both frameworks allow for network models to be fitted and compared to SEM models. The discussed methodology has been implemented in the freely available R package lvnet.</p><p>Simulation studies showed that step-wise search and penalized maximum likelihood estimation of the residual or latent network structures resulted in high specificity all aroundthe methods did not result often in false positives-and rapidly increasing sensitivity as a function of the sample size; the higher the sample size, the more true edges were detected in the algorithm. These numbers are comparable to state-of-the-art network estimation techniques in sample and model sizes that are plausible in psychological settings <ref type="bibr">(van Borkulo et al., 2014)</ref>. In all four simulation studies, using AIC as the model selection criterion led to the best sensitivity and using EBIC led to the best specificity. However, it is important to note that the choice of a particular information criterion cannot be argued by these numbers alone, and depends on the relative importance one assigns to the side of discovery (optimizing sensitivity) or the side of caution (optimizing specificity; <ref type="bibr">Dziak et al. 2012</ref>). Furthermore, it should be noted that these simulation results are specific for the particular model setup and sample sizes used; results might be different for other kinds of models or sample size ranges.</p><p>In addition to the LNM and RNM frameworks, other combinations of CFA, SEM and network modeling are possible as well. For example, a framework can be constructed which contains both a latent and a residual network (as shown in our empirical example), or directed regression paths as in the SEM model can be added to the LNM model. While these models are all estimable in the lvnet software, in the current paper we chose to focus on the distinct benefits that modeling a residual or latent network presents. Thus, in this paper, we only described the modeling of multivariate normal data. More advanced models are possible, but not yet implemented in the lvnet software. In the case of binary variables, the appropriate model to use is the Ising model, which has been shown to be equivalent to multivariate item response models <ref type="bibr">(Epskamp et al., ress;</ref><ref type="bibr" target="#b41">Marsman et al., 2015)</ref>. Future research could aim at constructing Ising models among binary latent variables in latent class analysis, or constructing residual networks in models with binary indicators. Finally, the expressions optimized in Equations ( <ref type="formula" target="#formula_22">1</ref>) and ( <ref type="formula" target="#formula_17">7</ref>) are based on summary statistics and therefore only truly applicable to complete data. With incomplete data, the appropriate estimation method is to use full-information maximum likelihood (FIML; <ref type="bibr">Arbuckle et al. 1996)</ref>; however, FIML has not yet been implemented in the lvnet software.</p><p>In our view, the presented modeling framework is a versatile and promising addition to the spectrum of psychometric models. The GGM, which has a central place in this modeling framework, acts as a natural interface between correlation and causality, and we think this representation should receive more attention in psychometrics. From the point of view afforded by the current paper, the typical attempt to determine directed SEMs from correlation structures in fact appears somewhat haphazard in psychology, a historical accident in a field that has been prematurely directed to hypothesis testing at the expense of systematic exploration. Perhaps, psychometrics as a field should consider taking a step back to focus on the consistent identification of GGMs, instead of wanting to jump to the causal conclusion immediately. In this regard, the fact that GGMs do not have equivalent models would appear to be a major benefit, as they allow us to focus on charting connections between variables systematically, without being forced to adhere to one particular causal interpretation or another. In addition, because the GGM does not specify the nature or direction of interactions between variables, it appears a natural model for research situations where no temporal information or experimental interventions are present, so that associations may arise for a multitude of reasons: the GGM can be consistently interpreted regardless of whether associations arise from direct causal relations, reciprocal causation, latent common causes, semantic overlap between items, or homeostatic couplings of parameters. While this can be seen as a downside of the GGM-the lack of directionality leads to fewer falsifiable hypotheses-it can also be a major asset in a field like psychology, where strong causal theory is sparse and the identification of DAGs often appears a bridge too far.</p><p>framework Latent Network Modeling (LNM) and show that, with LNM, a unique structure of conditional independence relationships between latent variables can be obtained in an explorative manner. In the second generalization, the residual variance-covariance structure of indicators is modeled as a network. We term this generalization Residual Network Modeling (RNM) and show that, within this framework, identifiable models can be obtained in which local independence is structurally violated. Both generalizations have been implemented in the free-to-use software package lvnet, which contains confirmatory model testing as well as two exploratory search algorithms: stepwise search algorithms for low-dimensional datasets and penalized maximum likelihood estimation for larger datasets. We show in simulation studies that these search algorithms performs adequately in identifying the structure of the relevant residual or latent networks. We further demonstrate the utility of these generalizations in an empirical example on a personality inventory dataset. Key words: networks, structural equation modeling, simulation study</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Recent years have seen an emergence of network modeling in psychometrics <ref type="bibr">(Borsboom, 2008;</ref><ref type="bibr">Borsboom et al., 2011;</ref><ref type="bibr">Cramer et al., 2012;</ref><ref type="bibr">Schmittmann et al., 2013)</ref>. In this paper, we introduce the reader to this field of network psychometrics <ref type="bibr">(Epskamp et al., in press</ref>) and formalize the network model for multivariate normal data, the Gaussian Graphical Model (GGM; Lauritzen 1996), as a formal psychometric model. We contrast the GGM to the Structural Equation Model (SEM; <ref type="bibr">Wright 1921;</ref><ref type="bibr">Kaplan 2000)</ref> and show that the GGM can be seen as another way to approach modeling covariance structures as is typically done in psychometrics. In particular, rather than modeling the covariance matrix, the GGM models the inverse of a covariance matrix. The GGM and SEM are thus very closely related: every GGM model and every SEM model imply a constrained covariance structure. We make use of this relationship to show that, through a reparameterization of the SEM model, the GGM model can be obtained in two different ways: first, as a network structure that relates a number of latent variables to each other, and second, as a network between residuals that remain given a fitted latent variable model. As such, the GGM can be modeled and estimated in SEM, which allows for network modeling of psychometric data to be carried out in a framework familiar to psychometricians and methodologists.</p><p>However, the combination of GGM and SEM allows for more than fitting network models. As we will show, the strength of one framework can help the other framework and vice versa. In this paper, we introduce network models for latent covariances and for residual covariances as two distinct generalized frameworks of both the SEM and GGM. The first framework, Latent Network Modeling (LNM), formulates a network among latent variables. This framework allows researchers to exploratively estimate conditional independence relationships between latent variables through model search algorithms; this estimation is difficult in the SEM framework due to the presence of equivalent models <ref type="bibr">(MacCallum et al., 1993)</ref>. The second framework, which we denote Residual Network Modeling (RNM), formulates a network structure on the residuals of a SEM model. With this framework, researchers can circumvent critical assumptions of both SEM and the GGM: SEM typically relies on the assumption of local independence, whereas network modeling typically relies on the assumption that the covariance structure among a set of the items is not due to latent variables at all. The RNM framework allows researchers to estimate SEM models without the assumption of local independence (all residuals can be correlated, albeit due to a constrained structure on the inverse residual covariance matrix) as well as to estimate a network structure, while taking into account the fact that the covariance between items may be partly due to latent factors.</p><p>While the powerful combination of SEM and GGM allows for confirmative testing of network structures both with and without latent variables, we recognize that few researchers have yet formulated strict confirmatory hypotheses in the relatively new field of network psychometrics. Often, researchers are more interested in exploratively searching a plausible network structure. To this end, we present two exploratory search algorithms. The first is a step-wise model search algorithm that adds and removes edges of a network as long as fit is improved, and the second uses penalized maximum likelihood estimation <ref type="bibr">(Tibshirani, 1996)</ref> to estimate a sparse model. We evaluate the performance of these search methods in four simulation studies. Finally, the proposed methods have been implemented in a free-to-use R package, lvnet, which we illustrate in an empirical example on personality inventory items <ref type="bibr" target="#b102">(Revelle, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling Multivariate Gaussian Data</head><p>Let y y y be the response vector of a random subject on P items 1 . We assume y y y is centered and follows a multivariate Gaussian density: y y y ∼ N P (0 0 0, Σ Σ Σ) , 1 Throughout this paper, vectors will be represented with lowercase boldfaced letters and matrices will be denoted by capital boldfaced letters. Roman letters will be used to denote observed variables and parameters (such as the number of nodes) and Greek letters will be used to denote latent variables and parameters that need to be estimated. The subscript i will be used to denote the realized response vector of subject i and omission of this subscript will be used to denote the response of a random subject.</p><p>In which Σ Σ Σ is a P × P variance-covariance matrix, estimated by some model-implied Σ Σ Σ.</p><p>Estimating Σ Σ Σ is often done through some form of maximum likelihood estimation. If we measure N independent samples of y y y we can formulate the N × P matrix Y Y Y containing realization y y y i as its ith row. Let S S S represent the sample variance-covariance matrix of Y Y Y :</p><formula xml:id="formula_21">S S S = 1 N -1 Y Y Y Y Y Y .</formula><p>In maximum likelihood estimation, we use S S S to compute and minimize -2 times the the log-likelihood function to find Σ Σ Σ <ref type="bibr">(Lawley, 1940;</ref><ref type="bibr">Jöreskog, 1967;</ref><ref type="bibr">Jacobucci et al., 2016)</ref>:</p><formula xml:id="formula_22">min Σ Σ Σ log det Σ Σ Σ + Trace S S S Σ Σ Σ -1 -log det Ŝ S S -P .<label>(1)</label></formula><p>To optimize this expression, Σ Σ Σ should be estimated as closely as possible to S S S and perfect fit is obtained if Σ Σ Σ = S S S. A properly identified model with the same number of parameters (K) used to form Σ Σ Σ as there are unique elements in S S S (P (P + 1)/2 parameters) will lead to Σ Σ Σ = S S S and therefore a saturated model. The goal of modeling multivariate Gaussian data is to obtain some model for Σ Σ Σ with positive degrees of freedom, K &lt; P (P + 1)/2, in which Σ Σ Σ resembles S S S closely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structural Equation Modeling</head><p>In Confirmatory Factor Analysis (CFA), Y Y Y is typically assumed to be a causal linear effect of a set of M centered latent variables, η η η, and independent residuals or error, ε ε ε:</p><formula xml:id="formula_23">y y y = Λ Λ Λη η η + ε ε ε.</formula><p>Here, Λ Λ Λ represents a P × M matrix of factor loadings. This model implies the following model for Σ Σ Σ: analysis <ref type="bibr">(Wright, 1934)</ref>.</p><formula xml:id="formula_24">Σ Σ Σ = Λ Λ ΛΨ Ψ ΨΛ Λ Λ + Θ Θ Θ,<label>(2)</label></formula><p>The Θ Θ Θ matrix is, like Σ Σ Σ and S S S, a P × P matrix; if Θ Θ Θ is fully estimated-contains no restricted elements-then Θ Θ Θ alone constitutes a saturated model. Therefore, to make either (2) or (3) identifiable, Θ Θ Θ must be strongly restricted. Typically, Θ Θ Θ is set to be diagonal, a restriction often termed local independence <ref type="bibr">(Lord et al., 1968;</ref><ref type="bibr">Holland and Rosenbaum, 1986</ref>) because indicators are independent of each other after conditioning on the set of latent variables. To improve fit, select off-diagonal elements of Θ Θ Θ can be estimated, but systematic violations of local independence-many nonzero elements in Θ Θ Θ-are not possible as that will quickly make (2) and (3) saturated or even over-identified. More precisely, Θ Θ Θ can not be fully-populated-some elements of Θ Θ Θ must be set to equal zero-when latent variables are used. An element of Θ Θ Θ being fixed to zero indicates that two variables are locally independent after conditioning on the set of latent variables. As such, local independence is a critical assumption in both CFA and SEM; if local independence is systematically violated, CFA and SEM will never result in correct models.</p><p>The assumption of local independence has led to critiques of the factor model and its usage in psychology; local independence appears to be frequently violated due to direct causal effects, semantic overlap, or reciprocal interactions between putative indicators of a latent variable <ref type="bibr">(Borsboom, 2008;</ref><ref type="bibr">Cramer et al., 2010;</ref><ref type="bibr">Borsboom et al., 2011;</ref><ref type="bibr">Cramer et al., 2012;</ref><ref type="bibr">Schmittmann et al., 2013)</ref>. For example, in psychopathology research, local independence of symptoms given a person's level of a latent mental disorder has been questioned <ref type="bibr">(Borsboom and Cramer, 2013)</ref>. For example, three problems associated with depression are "fatigue", "concentration problems" and "rumination". It is plausible that a person who suffers from fatigue will also concentrate more poorly, as a direct result of being fatigued and regardless of his or her level of depression. Similarly, rumination might lead to poor concentration. In another example, <ref type="bibr">Kossakowski et al. (2015)</ref> describe the often-used SF-36 questionnaire <ref type="bibr">(Ware Jr and Sherbourne, 1992)</ref> designed to measure health related quality of life. The SF-36 contains items such as "can you walk for more than one kilometer" and "can you walk a few hundred meters". Clearly, these items can never be locally independent after conditioning on any latent trait, as one item (the ability to walk a few hundred meters) is a prerequisite for the other (walking more than a kilometer). In typical applications, the excessive covariance between items of this type is typically left unmodeled, and treated instead by combining items into a subscale or total score that is subsequently subjected to factor analysis; of course, however, this is tantamount to ignoring the relevant psychometric problem rather than solving it.</p><p>Given the many theoretically expected violations of local independence in psychometric applications, many elements of Θ Θ Θ in both ( <ref type="formula" target="#formula_3">2</ref>) and (3) should ordinarily be freely estimated. Especially when violations of local independence are expected to be due to causal effects of partial overlap, residual correlations should not be constrained to zero; in addition, a chain of causal relationships between indicators can lead to all residuals to become correlated. Thus, even when latent factors cause much of the covariation between measured items, fitting a latent variable model that involves local independence may not fully account for correlation structure between measured items. Of course, in practice, many psychometricians are aware of this problem, which is typically addressed by freeing up correlations between residuals to improve model fit.</p><p>However, this is usually done in an ad-hoc fashion, on the basis of inspection of modification indices and freeing up error covariances one by one, which is post hoc, suboptimal, and involves an uncontrolled journey through the model space. As a result, it is often difficult to impossible to tell how exactly authors arrived at their final reported models. As we will show later in this paper, this process can be optimized and systematized using network models to connect residuals on top of a latent variable structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Modeling</head><p>Recent authors have suggested that the potential presence of causal relationships between measured variables may allow the explanation of the covariance structure without the need to invoke any latent variables <ref type="bibr">(Borsboom, 2008;</ref><ref type="bibr">Cramer et al., 2010;</ref><ref type="bibr">Borsboom et al., 2011;</ref><ref type="bibr">Schmittmann et al., 2013)</ref>. The interactions between indicators can instead be modeled as a network, in which indicators are represented as nodes that are connected by edges representing pairwise interactions. Such interactions indicate the presence of covariances that cannot be explained by any other variable in the model and can represent-possibly reciprocal-causal relationships. Estimating a network structure on psychometric data is termed network psychometrics <ref type="bibr">(Epskamp et al., in press)</ref>. Such a network of interacting components can generate data that fit factor models well, as is commonly the case in psychology. <ref type="bibr">Van der Maas et al. (2006)</ref> showed that the positive manifold of intelligence-which is commonly explained with the general factor for intelligence, g-can emerge from a network of mutually benefiting cognitive abilities. <ref type="bibr">Borsboom et al. (2011)</ref> showed that a network of psychopathological symptoms, in which disorders are modeled as clusters of symptoms, could explain comorbidity between disorders. Furthermore, <ref type="bibr">Epskamp et al. (in press)</ref> showed that the Ising model for ferromagnetism <ref type="bibr">(Ising, 1925)</ref>, which models magnetism as a network of particles, is equivalent to multidimensional item response theory <ref type="bibr" target="#b101">(Reckase, 2009)</ref>.</p><p>In network psychometrics, psychometric data are modeled through directed or undirected networks. Directed networks are equivalent to path analysis models. For modeling undirected networks, pairwise Markov Random Fields <ref type="bibr">(Lauritzen, 1996;</ref><ref type="bibr">Murphy, 2012)</ref> are used. In these models, each variable is represented by a node, and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Psychometrika Submission</head><p>September 13, 2017 10  (j,k) . Whenever two nodes cannot be rendered independent conditional on the other nodes in the system, they are said to feature in a pairwise interaction, which is represented by an undirected edge-an edge with no arrows-to contrast such an effect from covariances typically represented in the SEM literature with bidirectional edges.</p><formula xml:id="formula_25">Y 1 Y 2 Y 3</formula><p>Figure <ref type="figure" target="#fig_11">1</ref> represents such a network model, in which nodes y 1 and y 3 are independent after conditioning on node y 2 . Such a model can readily arise from direct interactions between the nodes. For example, this conditional independence structure would emerge if y 2 is a common cause of y 1 and y 3 , or if y 2 is the mediator in a causal path between y 1 and y 3 . In general, it is important to note that pairwise interactions are not mere correlations; two variables may be strongly correlated but unconnected (e.g., when both are caused by another variable in the system) and they may be uncorrelated but strongly connected in the network (e.g., when they have a common effect in the system). For instance, in the present example the model does not indicate that y 1 and y 3 are uncorrelated, but merely indicates that any correlation between y 1 and y 3 is due to their mutual interaction with y 2 ; a network model in which either directly or indirectly connected paths exist between all pairs of nodes typically implies a fully populated (no zero elements) variance-covariance matrix.</p><p>In the case of multivariate Gaussian data this model is termed the Gaussian Graphical Model (GGM; Lauritzen 1996). In the case of multivariate normality, the partial correlation coefficient is sufficient to test the degree of conditional independence of two variables after conditioning on all other variables; if the partial correlation coefficient is zero, there is conditional independence and hence no edge in the network.</p><p>As such, partial correlation coefficients can directly be used in the network as edge weights; the strength of connection between two nodes 3 . Such a network is typically encoded in a symmetrical and real valued p × p weight matrix, Ω Ω Ω, in which element ω jk represents the edge weight between node j and node k:</p><formula xml:id="formula_26">Cor y j , y k | y y y -(j,k) = ω jk = ω kj .</formula><p>The partial correlation coefficients can be directly obtained from the inverse of variance-covariance matrix Σ Σ Σ, also termed the precision matrix K K K <ref type="bibr">(Lauritzen, 1996)</ref>:</p><formula xml:id="formula_27">Cor y j , y k | y y y -(j,k) = - κ jk √ κ kk √ κ jj .</formula><p>Thus, element κ jk of the precision matrix is proportional to the partial correlation coefficient of variables y j and y k after conditioning on all other variables. Since this process simply involves standardizing the precision matrix, we propose the following</p><formula xml:id="formula_28">model 4 : Σ Σ Σ = K K K -1 = ∆ ∆ ∆ (I I I -Ω Ω Ω) -1 ∆ ∆ ∆, (<label>4</label></formula><formula xml:id="formula_29">)</formula><p>in which ∆ ∆ ∆ is a diagonal matrix with δ jj = κ  <ref type="bibr">(Browne and Cudeck, 1992)</ref> and CFI <ref type="bibr">(Bentler, 1990)</ref>. Such methods of assessing model fit have not yet been used in network psychometrics.</p><p>Similar to CFA and SEM, the GGM relies on a critical assumption; namely, that covariances between observed variables are not caused by any latent or unobserved variable. If we estimate a GGM in a case where, in fact, a latent factor model was the true data generating structure, then generally we would expect the GGM to be saturated-i.e., there would be no missing edges in the GGM <ref type="bibr" target="#b69">(Chandrasekaran et al., 2010)</ref>. A missing edge in the GGM indicates the presence of conditional independence between two indicators given all other indicators; we do not expect indicators to become independent given subsets of other indicators (see also <ref type="bibr">Ellis and Junker 1997;</ref><ref type="bibr">Holland and Rosenbaum 1986)</ref>. Again, this critical assumption might not be plausible.</p><p>While variables such as "Am indifferent to the feelings of others" and "Inquire about others' well-being" quite probably interact with each other, it might be far-fetched to assume that no unobserved variable, such as a personality trait, in part also causes some of the variance in responses on these items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalizing Factor Analysis and Network Modeling</head><p>We propose two generalizations of both SEM and the GGM that both allow the modeling of network structures in SEM. In the first generalization, we adopt the CFA<ref type="foot" target="#foot_10">foot_10</ref> decomposition in (2) and model the variance-covariance matrix of latent variables as a GGM:</p><formula xml:id="formula_30">Ψ Ψ Ψ = ∆ ∆ ∆ Ψ Ψ Ψ (I I I -Ω Ω Ω Ψ Ψ Ψ ) -1 ∆ ∆ ∆ Ψ Ψ Ψ .</formula><p>This framework can be seen as modeling conditional independencies between latent variables not by directed effects (as in SEM) but as an undirected network. As such, we term this framework latent network modeling (LNM).</p><p>In the second generalization, we adopt the SEM decomposition of the variance-covariance matrix in (3) and allow the residual variance-covariance matrix Θ Θ Θ to be modeled as a GGM:</p><formula xml:id="formula_31">Θ Θ Θ = ∆ ∆ ∆ Θ Θ Θ (I I I -Ω Ω Ω Θ Θ Θ ) -1 ∆ ∆ ∆ Θ Θ Θ .</formula><p>Because this framework conceptualizes associations between residuals as pairwise interactions, rather than correlations, we term this framework Residual Network Modeling (RNM). Using this framework allows-as will be described below-for a powerful way of fitting a confirmatory factor structure even though local independence is systematically violated and all residuals are correlated.  and LNM in more detail and will outline the class of situations in which using these models is advantageous over CFA or SEM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Network Modeling</head><p>The LNM framework models the latent variance-covariance matrix of a CFA model as a GGM:</p><formula xml:id="formula_32">Σ Σ Σ = Λ Λ Λ∆ ∆ ∆ Ψ Ψ Ψ (I I I -Ω Ω Ω Ψ Ψ Ψ ) -1 ∆ ∆ ∆ Ψ Ψ Ψ Λ Λ Λ + Θ Θ Θ. (<label>5</label></formula><formula xml:id="formula_33">)</formula><p>This allows researchers to model conditional independence relationships between latent variables without making the implicit assumptions of directionality or acyclicness. In SEM, B B B is typically modeled as a directed acyclic graph (DAG), meaning that elements of B B B can be represented by directed edges, and following along the path of these edges it is not possible to return to any node (latent variable). The edges in such a DAG can be interpreted as causal, and in general they imply a specific set of conditional independence relationships between the nodes <ref type="bibr">(Pearl, 2000)</ref>.</p><p>While modeling conditional independence relationships between latent variables as a DAG is a powerful tool for testing strictly confirmatory hypotheses, it is less useful for more exploratory estimation. Though there have been recent advances in exploratory estimation of DAGs within an SEM framework (e.g., <ref type="bibr">Gates and Molenaar 2012;</ref><ref type="bibr">Rosa et al. 2012</ref>), many equivalent DAGs can imply the same conditional independence relationships, and thus fit the data equally well even though their causal interpretation can be strikingly different <ref type="bibr">(MacCallum et al., 1993)</ref>. Furthermore, the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Psychometrika Submission</head><p>September 13, 2017 16</p><formula xml:id="formula_34">Y 1 Y 2 Y 3 (a) Y 1 Y 2 Y 3 (b) Y 1 Y 2 Y 3 (c)</formula><p>No equivalent GGM with the same # of parameters</p><formula xml:id="formula_35">(d) Y 1 Y 2 Y 3<label>(e) Y 1 Y 2 Y 3 (f)</label></formula><p>No equivalent GGM with the same # of parameters assumption that the generating model is acyclic-which, in practice, often is made on purely pragmatic grounds to identify a model-is problematic in that much psychological behavior can be assumed to have at least some cyclic and complex behavior and feedback <ref type="bibr">(Schmittmann et al., 2013)</ref>. Thus, the true conditional independence relationships in a dataset can lead to many equivalent compositions of B B B, and possibly none of them are the true model.</p><formula xml:id="formula_36">(g) Y 1 Y 2 Y 3 Y 4<label>(h)</label></formula><p>In psychometrics and SEM the GGM representation has not been very prominent, even though it has some manifest benefits over the attempt to identify DAGs directly.</p><p>For example, by modeling conditional independence relationships between latent variables as a GGM, many relationships can be modeled in a simpler way as compared to a DAG. In addition, in the GGM each set of conditional independence relations only corresponds to one model: there are no equivalent GGMs with the same nodes. Figure <ref type="figure" target="#fig_15">3</ref> shows a comparison of several conditional independence relations that can be modeled equivalently or not by using a GGM or by using a DAG. Figures <ref type="figure" target="#fig_15">3a</ref> and<ref type="figure" target="#fig_15">3b</ref> show two DAGs that represent the same conditional independence relations,</p><formula xml:id="formula_37">y 1 ⊥ ⊥ y 3 | y 2</formula><p>, which can both be represented by the same GGM shown in Figure <ref type="figure" target="#fig_15">3e</ref>.</p><p>There are some conditional independence relations that a GGM cannot represent in the same number of parameters as a DAG; Figure <ref type="figure" target="#fig_15">3c</ref> shows a collider structure that cannot be exactly represented by a GGM (the best fitting GGM would feature three edges instead of two). On the other hand, there are also conditional independence relationships that a GGM can represent and a DAG cannot; the cycle of Figure <ref type="figure" target="#fig_15">3d</ref> cannot be represented by a DAG. Further equivalences and differences between GGMs and DAGs are beyond the scope of this paper, but haven been well described in the literature (e.g., chapter 3 of Lauritzen 1996; <ref type="bibr">Koller and Friedman 2009;</ref><ref type="bibr" target="#b88">Kolaczyk and Csárdi 2014)</ref>. In sum, the GGM offers a natural middle ground between zero-order correlations and DAGs: every set of zero-order correlations implies exactly one GGM, and every DAG implies exactly one GGM. In a sense, the road from correlations to DAGs (including hierarchical factor models) thus always must pass through the realm of GGMs, which acts as a bridge between the correlational and causal worlds.</p><p>Because there are no equivalent undirected models possible, LNM offers a powerful tool for exploratory estimation of relationships between latent variables. For example, suppose one encounters data generated by the SEM model in Figure <ref type="figure" target="#fig_14">2a</ref>. Without prior theory on the relations between latent variables, exploratory estimation on this dataset would lead to three completely equivalent models: the one shown in Figure <ref type="figure" target="#fig_14">2c</ref> and two models in which the common cause instead is the middle node in a causal chain. As the number of latent variables increases, the potential number of equivalent models that encode the same conditional independence relationships grows without bound.</p><p>The LNM model in Figure <ref type="figure" target="#fig_14">2c</ref> portrays the same conditional independence relationship as the SEM model in 2a while having no equivalent model. Exploratory estimation could easily find this model, and portrays the retrieved relationship in a clear and unambiguous way.</p><p>A final benefit of using LNM models is that they allow network analysts to construct a network while taking measurement error into account. So far, networks have been constructed based on single indicators only and no attempt has been made</p><p>to remediate measurement error. By forming a network on graspable small concepts measured by a few indicators, the LNM framework can be used to control for measurement error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual Network Modeling</head><p>In the RNM framework the residual structure of SEM is modeled as a GGM:</p><formula xml:id="formula_38">Σ Σ Σ = Λ Λ Λ (I I I -B B B) -1 Ψ Ψ Ψ (I I I -B B B) -1 Λ Λ Λ + ∆ ∆ ∆ Θ Θ Θ (I I I -Ω Ω Ω Θ Θ Θ ) -1 ∆ ∆ ∆ Θ Θ Θ .<label>(6)</label></formula><p>This modeling framework conceptualizes latent variable and network modeling as two sides of the same coin, and offers immediate benefits to both. In latent variable modeling, RNM allows for the estimation of a factor structure (possibly including structural relationships between the latent variables), while having no uncorrelated errors and thus no local independence. The error-correlations, however, are still highly structured due to the residual network structure. This can be seen as a compromise between the ideas of network analysis and factor modeling; while we agree that local independence is plausibly violated in many psychometric tests, we think the assumption of no underlying latent traits and therefore a sparse GGM may often be too strict. For network modeling, RNM allows a researcher to estimate a sparse network structure while taking into account that some of the covariation between items was caused by a set of latent variables. Not taking this into account would lead to a saturated model <ref type="bibr" target="#b69">(Chandrasekaran et al., 2010)</ref>, whereas the residual network structure can be sparse.</p><p>To avoid confusion between residual correlations, we will denote edges in Ω Ω Ω Θ Θ Θ residual interactions. Residual interactions can be understood as pairwise linear effects, possibly due to some causal influence or partial overlap between indicators that is left after controlling for the latent structure. Consider again the indicators for agreeableness "Am indifferent to the feelings of others" and "Inquire about others' well-being". It seems clear that we would not expect these indicators to be locally independent after conditioning on agreeableness; being indifferent to the feelings of others will cause one to not inquire about other's well-being. Thus, we could expect these indicators to feature a residual interaction; some degree of correlation between these indicators is expected to remain, even after conditioning on the latent variable and all other indicators in the model.</p><p>The RNM framework in particular offers a new way of improving the fit of confirmatory factor models. In contrast to increasingly popular methods such as exploratory SEM (ESEM; <ref type="bibr">Marsh et al. 2014)</ref> or LASSO regularized SEM models <ref type="bibr">(Jacobucci et al., 2016)</ref>, the RNM framework improves the fit by adding residual interactions rather than allowing for more cross-loadings. The factor structure is kept exactly intact as specified in the confirmatory model. Importantly, therefore, the interpretation of the latent factor does not change. This can be highly valuable in the presence of a strong theory on the latent variables structure underlying a dataset even in the presence of violations of local independence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploratory Network Estimation</head><p>Both the LNM and RNM modeling frameworks allow for confirmative testing of network structures. Confirmatory estimation is straightforward and similar to estimating SEM models, with the exception that instead of modeling Ψ Ψ Ψ or Θ Θ Θ now the Often the network structure, either at the residual or the latent level, is unknown and needs to be estimated. To this end, the package includes two exploratory search algorithms described below: step-wise model search and penalized maximum likelihood estimation. For both model frameworks and both search algorithms, we present simulation studies to investigate the performance of these procedures. As is typical in simulation studies investigating the performance of network estimation techniques, we investigated the sensitivity and specificity <ref type="bibr">(van Borkulo et al., 2014)</ref>. These measures investigate the estimated edges versus the edges in the true model, with a 'positive'</p><formula xml:id="formula_39">latent network Ω Ω Ω Ψ Ψ Ψ or Ω Ω Ω Θ Θ Θ is modeled. Furthermore,</formula><p>indicating an estimated edge and a 'negative' indicating an edge that is estimated to be zero. Sensitivity, also termed the true positive rate, gives the ratio of the number of true edges that were detected in the estimation versus the total number of edges in the true model: sensitivity = # true positives # true positives + # of false negatives Specificity, also termed the true negative rate, gives the ratio of true missing edges <ref type="bibr">Borkulo et al., 2014;</ref><ref type="bibr">Foygel and Drton, 2010)</ref>. In LNM, removing edges from Ω Ω Ω Ψ Ψ Ψ cannot improve the fit beyond that of an already fitting CFA model. Hence, model search for Ω Ω Ω Ψ Ψ Ψ should start at a fully populated initial setup for Ω Ω Ω Ψ Ψ Ψ . In RNM, on the other hand, a densely populated Ω Ω Ω Θ Θ Θ would lead to an over-identified model, and hence We performed a simulation study to assess the performance of the above mentioned step-wise search algorithms in LNM models. Figure <ref type="figure" target="#fig_2">4</ref> shows the LNM model under which we simulated data. In this model, four latent factors with three indicators each were connected in a latent network. The latent network was a chain network, leading all latent variables to be correlated according to a structure that cannot be represented in SEM. Factor loadings and residual variances were set to 1, and the network weights were simulated as described in the section "Simulating Gaussian Graphical models". The simulation study followed a 5 × 4 design: the sample size was varied between 50, 100, 250, 500 and 1 000 to represent typical sample sizes in psychological research, and the stepwise evaluation criterion was either χ 2 difference testing, AIC, BIC or EBIC (using a tuning parameter of 0.5). Each condition was simulated 1 000 times, resulting in 20 000 total simulated datasets.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows the results of the simulation study. Data is represented in standard boxplots <ref type="bibr">(McGill et al., 1978)</ref>: the box shows the 25th, 50th (median) 75th quantiles, the whiskers range from the largest values in 1.5 times the inter-quantile range (75th -25th quantile) and points indicate outliers outside that range. In each condition, we AIC Chi-square BIC EBIC q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 AIC Chi-square BIC EBIC q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 We conducted a second simulation study to assess the performance of step-wise model selection in RNM models. Figure <ref type="figure" target="#fig_7">7</ref> shows the model under which data were simulated: two latent variables with 5 indicators each. The residual network was constructed to be a chain graph linking a residual of an indicator of one latent variable to two indicators of the other latent variable. This structure cannot be represented by a DAG and causes all residuals to be connected, so that Θ Θ Θ is fully populated. Factor loadings and residual variances were set to 1, the factor covariance was set to 0.25, and the network weights were simulated as described in the section "Simulating Gaussian Graphical models".</p><p>The simulation study followed a 5 × 4 design; sample size was again varied between 50, 100, 250, 500 and 1 000, and models were estimated using either χ 2 significance testing, AIC, BIC or EBIC. Factor loadings and factor variances were set to 1 and the factor correlation was set to 0.25. The weights in Ω Ω Ω Θ Θ Θ were chosen as described in the section "Simulating Gaussian Graphical models". Each condition was simulated 1, 000 times, leading to 20 000 total datasets. AIC Chi-square BIC EBIC q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 AIC Chi-square BIC EBIC q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 the worst. The bottom panel shows that specificity was very high for all sample sizes and all criteria, with EBIC performing best and AIC worst. These results indicate that the number of false positives is very low and that the method is on average well capable of discovering true edges for sample size larger than 250. In sum, all four criteria perform well with EBIC erring on the side of caution and AIC erring on the side of discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LASSO regularization</head><p>While the step-wise model selection algorithms perform well in retrieving the correct network structure, they are very slow when the number of nodes in the network increases (e.g., more than 10 nodes). This is particularly important in the context of RNM, in which the number of indicators can be larger than 10 even in small models. A popular method for fast estimation of high-dimensional network structures is by applying the least absolute shrinkage and selection operator (LASSO; <ref type="bibr">Tibshirani 1996)</ref>.</p><p>LASSO regularization has also recently been introduced in the SEM literature <ref type="bibr">(Jacobucci et al., 2016)</ref> as a method for obtaining sparser structures of Λ Λ Λ and B B B. In the LASSO, instead of optimizing the likelihood function as described in (1), a penalized likelihood is optimized <ref type="bibr">(Jacobucci et al., 2016)</ref>:</p><formula xml:id="formula_40">min Σ Σ Σ log det Σ Σ Σ + Trace S S S Σ Σ Σ -1 -log det Ŝ S S -P + νPenalty ,<label>(7)</label></formula><p>in which ν denotes a tuning parameter controlling the level of penalization. The penalty here is taken to be the sum of absolute parameters:</p><formula xml:id="formula_41">Penalty = &lt;i,j&gt; |ω ij |, in which ω ij denotes an element from either Ω Ω Ω Ψ Ψ Ψ or Ω Ω Ω Θ Θ Θ .</formula><p>Other penalty functions may be used as well-such as summing the squares of parameter estimates (ridge regression;</p><p>Hoerl and Kennard 1970) or combining both absolute and squared values (elastic net;</p><p>Zou and Hastie 2005)-but these are not currently implemented in lvnet. The benefit of the LASSO is that it returns models that perform better in cross-validation. In addition, the LASSO yields sparse models in which many relationships are estimated to be zero. The lvnet function allows for LASSO regularization for a given model matrix</p><formula xml:id="formula_42">(Ω Ω Ω Θ Θ Θ , Ω Ω Ω Ψ Ψ Ψ , Θ Θ Θ, Ψ Ψ Ψ, Λ Λ Λ or B B B</formula><p>) and a given value for the tuning parameter ν. The optimizer used in lvnet does not return exact zeroes. To circumvent this issue, any absolute parameter below some small value (by default = 0.0001) is treated as zero in counting the number of parameters and degrees of freedom <ref type="bibr" target="#b61">(Zou et al., 2007)</ref>. The lvnetLasso function implements the search algorithm described in Algorithm 3 to automatically choose an appropriate tuning parameter, use that for model selection and rerun the model to obtain a comparable fit to non-regularized models. In this algorithm, a sequence of tuning parameters is tested, which is set by default to a logorithmically spaced sequence of 20 values between 0.01 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulation Study 3: Latent Network Modeling</head><p>We studied the performance of LASSO penalization in estimating the latent network structure in a similar simulation study to the study of the step-wise procedure described above. Data were simulated under a similar model to the one shown in Figure <ref type="figure" target="#fig_2">4</ref>, except that now 8 latent variables were used leading to a total of 24 observed variables. All parameter values were the same as in simulation study 1. The simulation followed a 5 × 3 design. Sample size was varied between 100, 250, 500, 1 000 and 2 500, and for each sample size 1 000 datasets were simulated leading to a total of 5 000 generated datasets. On these datasets the best model was selected using either AIC, BIC or EBIC, leading to 15 000 total replications. In each replication, sensitivity and specificity were computed. Figure <ref type="figure" target="#fig_8">8</ref> shows that AIC had a relatively poor specificity all-around, but a high sensitivity. EBIC performed well with sample sizes of 500 and higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulation Study 4: Residual Network Modeling</head><p>To assess the performance of LASSO in estimating the residual network structure we simulated data as in Figure <ref type="figure" target="#fig_5">6</ref>, except that in this case four latent variables were used, each with 5 indicators, the residuals of which were linked via a chain graph. All parameter values were the same as in simulation study 2. The design was the same as in simulation study 3, leading to 5 000 generated datasets on which AIC, BIC or EBIC were used to select the best model. While Figure <ref type="figure" target="#fig_22">9</ref> shows good performance of the LASSO in retrieving the residual network structure and similar results as before: AIC performs the worst in specificity and EBIC the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empirical Example: Personality Inventory</head><p>In this section, we demonstrate LNM and RNM models by confirmative testing a model and exploratively searching a residual and a latent network structure. We use the lvnet package, which can be installed directly from Github using the devtools package in R:</p><p>&gt; library("devtools") &gt; install_github("sachaepskamp/lvnet") &gt; library("lvnet")</p><p>To exemplify the method, we will use a dataset from the psych package <ref type="bibr" target="#b102">(Revelle, 2015)</ref> on the Big 5 personality traits <ref type="bibr">(Benet-Martinez and John, 1998;</ref><ref type="bibr">Digman, 1989;</ref><ref type="bibr"></ref> AIC BIC EBIC q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 Sensitivity (true positive rate) AIC BIC EBIC q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 was used except now with 4 latent variables leading to 24 observed variables. For each sample size 1 000 datasets were generated, leading to 5 000 total simulated datasets on which AIC, BIC or EBIC was used to select the best model. High sensitivity indicates that the method is able to detect edges in the true model, and high specificity indicates that the method does not detect edges that are zero in the true model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AIC BIC EBIC</head><p>q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 Sensitivity (true positive rate) AIC BIC EBIC q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.00 that the method is able to detect edges in the true model, and high specificity indicates that the method does not detect edges that are zero in the true model. <ref type="bibr" target="#b80">Goldberg, 2001</ref><ref type="bibr" target="#b79">Goldberg, , 1998;;</ref><ref type="bibr">McCrae and Costa, 1997)</ref>. This dataset consists of 2800 observations of 25 items designed to measure the 5 central personality traits with 5 items per trait. We estimated the CFA model on the BFI dataset. Next, we used LASSO estimation to the RNM model using 100 different tuning parameters and using EBIC as criterion to maximize specificity and search for a sparse residual network. The fully correlated covariance matrix of latent variables is equivalent to a fully connected latent network structure. Table <ref type="table" target="#tab_6">1</ref> shows the fit of the three models. The CFA model fits poorly. The RNM model has substantively improved fit and resulted in good fit indices overall. The estimated RNM+LNM showed that 5 edges could be removed from the latent network after taking residual interactions into account. Figure <ref type="figure" target="#fig_23">10</ref> shows the factor structure and residual network of the final RNM+LNM model. It can be seen that Agreeableness is now only connected to extraversion: after taking into account someone's level of extraversion agreeableness is independent of the other three personality traits.</p><p>Extraversion is the most central node in this network and the only trait that is directly linked to all other traits. The residual network shows many meaningful connections.</p><p>While seemingly densely connected, this network only has 30% of all possible edges in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper we introduced a formal psychometric model for network modeling of multivariate normal data. We contrasted this model with latent variable models as commonly used in CFA and SEM. Furthermore, using the CFA and SEM frameworks, we proposed two generalizations of the network model to encompass latent variable structures within the network paradigm. In the first generalization, LNM, we construct a network among the latent variables, whereas in the second generalization, RNM, a network is formed among residuals of indicators. Both frameworks offer powerful benefits over both latent variable and network modeling. From the perspective of latent variable modeling, the LNM framework allows one to exploratively search for conditional independence relationships between latent variables without the need for prior theory, and the RNM framework allows one to model latent common causes without assuming local independence. From the perspective of network modeling, the LNM framework allows one to model network structures while taking measurement error into account, and the RNM framework allows one to estimate a network structure, even when all nodes are in part caused by unobserved or latent variables.</p><p>Both frameworks have been implemented in the freely available R package lvnet.</p><p>Simulation studies showed that step-wise search and penalized maximum likelihood estimation of the residual or latent network structures resulted in high specificity all around-the methods did not result often in false positives-and rapidly increasing sensitivity as a function of the sample size; the higher the sample size, the more true edges were detected in the algorithm. These numbers are comparable to state-of-the-art network estimation techniques in sample and model sizes that are plausible in psychological settings <ref type="bibr">(van Borkulo et al., 2014)</ref>. In all four simulation studies, using AIC as the model selection criterion led to the best sensitivity and using EBIC led to the best specificity. However, it is important to note that the choice of a particular information criterion cannot be argued by these numbers alone, and depends on the relative importance one assigns to the side of discovery (optimizing sensitivity)</p><p>or the side of caution (optimizing specificity; <ref type="bibr">Dziak et al. 2012</ref>). Furthermore, it should be noted that these simulation results are specific for the particular model setup and sample sizes used; results might be different for other kinds of models or sample size ranges.</p><p>In addition to the LNM and RNM frameworks, other combinations of CFA, SEM and network modeling are possible as well. For example, a framework can be constructed which contains both a latent and a residual network (as shown in our empirical example), or directed regression paths as in the SEM model can be added to the LNM model. While these models are all estimable in the lvnet software, in the current paper we chose to focus on the distinct benefits that modeling a residual or latent network presents. Thus, in this paper, we only described the modeling of multivariate normal data. More advanced models are possible, but not yet implemented in the lvnet software. In the case of binary variables, the appropriate model to use is the Ising model, which has been shown to be equivalent to multivariate item response models (see <ref type="bibr">Epskamp et al. in press)</ref>. Future research could aim at constructing Ising models among binary latent variables in latent class analysis, or constructing residual networks in models with binary indicators. Finally, the expressions optimized in Equations ( <ref type="formula" target="#formula_22">1</ref>) and ( <ref type="formula" target="#formula_17">7</ref>) are based on summary statistics and therefore only truly applicable to complete data. With incomplete data, the appropriate estimation method is to use full-information maximum likelihood (FIML; <ref type="bibr">Arbuckle et al. 1996)</ref>; however, FIML has not yet been implemented in the lvnet software.</p><p>In our view, the presented modelling framework is a versatile and promising addition to the spectrum of psychometric models. The GGM, which has a central place in this modelling framework, acts as a natural interface between correlation and causality, and we think this representation should receive more attention in psychometrics. From the point of view afforded by the current paper, the typical attempt to determine directed SEMs from correlation structures in fact appears somewhat haphazard in psychology, a historical accident in a field that has been prematurely directed to hypothesis testing at the expense of systematic exploration.</p><p>Perhaps, psychometrics as a field should consider taking a step back to focus on the consistent identification of GGMs, instead of wanting to jump to the causal conclusion immediately. In this regard, the fact that GGMs do not have equivalent models would appear to be a major benefit, as they allow us to focus on charting connections between variables systematically, without being forced to adhere to one particular causal interpretation or another. In addition, because the GGM does not specify the nature or direction of interactions between variables, it appears a natural model for research situations where no temporal information or experimental interventions are present, so that associations may arise for a multitude of reasons: the GGM can be consistently interpreted regardless of whether associations arise from direct causal relations, reciprocal causation, latent common causes, semantic overlap between items, or homeostatic couplings of parameters. This appears to be a major asset in a field like psychology, where strong causal theory is sparse and the identification of DAGs often appears a bridge too far.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>in which ζ ζ ζ is a vector of residuals and B B B is an M × M matrix of regression coefficients. Now, Σ Σ Σ can be more extensively modeled as: Σ Σ Σ = Λ Λ Λ (I I I -B B B) -1 Ψ Ψ Ψ (I I I -B B B) -1 Λ Λ Λ + Θ Θ Θ, (3) in which now Ψ Ψ Ψ = Var (ζ ζ ζ). This framework can be used to model direct causal effects between observed variables by setting Λ Λ Λ = I I I and Θ Θ Θ = O O O, which is often called path analysis</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4 . Model used in simulation study 1: step-wise model search in latent network modeling. Four latent variables were each measured by three items. Latent variables covary due to the structure of a latent Gaussian graphical model in which edges indicate partial correlation coefficients. This model has the form of a chain graph, which cannot be represented in a structural equation model. Factor loadings, residual variances and latent variances were set to 1 and the latent partial correlations had an average of 0.33 with a standard deviation of 0.04.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. Simulation results of simulation study 1: step-wise model search in latent network modeling. Each condition was replicated 1 000 times, leading to 20 000 total simulated datasets. High sensitivity indicates that the method is able to detect edges in the true model, and high specificity indicates that the method does not detect edges that are zero in the true model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure6. Model used in simulation study 2: step-wise model search in residual network modeling. Two latent variables were each measured by five items; a Gaussian graphical model, in which edges indicate partial correlation coefficients, leads to all residuals to be correlated due to a chain graph between residuals, which cannot be represented in a structural equation model. Factor loadings, residual variances and latent variances were set to 1, the factor covariance was set to 0.25 and the latent partial correlations had an average of 0.33 with a standard deviation of 0.04.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure7. Simulation results of simulation study 2: step-wise model search in residual network modeling. Each condition was replicated 1 000 times, leading to 20 000 total simulated datasets. High sensitivity indicates that the method is able to detect edges in the true model, and high specificity indicates that the method does not detect edges that are zero in the true model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Figure8. Simulation results of simulation study 3: model selection via penalized maximum likelihood estimation in latent network modeling. The same model as in Figure4was used except now with 4 latent variables leading to 24 observed variables. For each sample size 1 000 datasets were generated, leading to 5 000 total simulated datasets on which AIC, BIC or EBIC was used to select the best model. High sensitivity indicates that the method is able to detect edges in the true model, and high specificity indicates that the method does not detect edges that are zero in the true model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>in which Ψ Ψ Ψ = Var (η η η) and Θ Θ Θ = Var (ε ε ε). In Structural Equation Modeling (SEM), Var (η η η) can further be modeled by adding structural linear relations between the latent variables 2 : η η η = B B Bη η η + ζ ζ ζ, in which ζ ζ ζ is a vector of residuals and B B B is an M × M matrix of regression coefficients.Now, ΣΣ Σ can be more extensively modeled as:Σ Σ Σ = Λ Λ Λ (I I I -B B B) -1 Ψ Ψ Ψ (I I I -B B B) -1 Λ Λ Λ + Θ Θ Θ,(3)in which now Ψ Ψ Ψ = Var (ζ ζ ζ). This framework can be used to model direct causal effects between observed variables by setting Λ Λ Λ = I I I and Θ Θ Θ = O O O, which is often called path</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of a pairwise Markov Random Field model. Edges in this model indicate pairwise interactions, and are drawn using undirected edges to distinguish from (bidirectional) covariances. Rather than a model for marginal associations (such as a network indicating covariances), this is a model for conditional associations. The network above encodes that Y 1 and Y 3 are independent after conditioning on Y 2 . Such a model allows all three variables to correlate while retaining one degree of freedom (the model only has two parameters)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Ω Ω has zeroes on the diagonal. This model allows for confirmative testing of the GGM structures on psychometric data. Furthermore, the model can be compared to a saturated model (fully populated off-diagonal values of Ω Ω Ω) and the independence model (Ω Ω Ω = O O O), allowing one to obtain χ 2 fit statistics as well as fit indices such as the RMSEA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of possible models under four different modeling frameworks. Circular nodes indicate latent variables, square nodes indicate manifest variables and gray nodes indicate residuals. Directed edges indicate factor loadings or regression parameters and undirected edges indicate pairwise interactions. Note that such undirected edges do not indicate covariances, which are typically denoted with bidirectional edges.Replacing covariances with interactions is where the network models differ from typical SEM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 2</head><label>2</label><figDesc>Figure 2 shows four different examples of possible models that are attainable under the SEM, LNM and RNM frameworks. Panel 2a shows a typical SEM model in which one latent variable functions as a common cause of two others. Panel 2b shows a network model that can be estimated using both the RNM and the LNM frameworks. Panel 2c shows a completely equivalent LNM model to the SEM model of Panel 2a in which the direction of effect between latent variables is not modeled. Finally, panel 2d shows a model in which three exogenous latent variables underlie a set of indicators of which the residuals form a network. The remainder of this section will describe RNM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Equivalent models between directed acyclic graphs (DAG; top) and Gaussian Graphical Models (GGM; bottom). Each column of graphs show two models that are equivalent. Panels (a), (b), (e) and (f) all represent the same conditional independence structure: Y 1 and Y 3 are independent after conditioning on Y 2 . Panel (c) represents that Y 1 and Y 3 are marginally independent even though they both are correlated with Y 2 , a structure that cannot be represented in a GGM with only two edges. Panel (h) shows that Y 1 and Y 3 are independent after conditioning on the set Y 2 and Y 4 , which cannot be represented with a DAG of four edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>both modeling frameworks allow for the confirmatory fit of a network model. In LNM, a confirmatory network structure can be tested by setting Λ Λ Λ = I I I and Θ Θ Θ = O O O; in RNM, a confirmatory network model can be tested by omitting any latent variables. We have developed the R package lvnet 6 , which utilizes OpenMx (Neale et al., 2015) for confirmative testing of RNM and LNM models (as well as a combination of the two). The lvnet function can be used for this purpose by specifying the fixed and the free elements of model matrices. The package returns model fit indices (e.g., the RMSEA, CFI and χ 2 value), parameter estimates, and allows for model comparison tests.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>the step-wise model search should start at an empty network Ω Ω Ω Θ Θ Θ = O O O. The function lvnetSearch in the lvnet package can be used for both search algorithms.Algorithm 1 Stepwise network estimation by χ 2 difference testing. Start with initial setup for Ω Ω Ω repeat for all Unique elements of Ω Ω Ω do Remove edge if present or add edge if absent Fit model with changed edge end for if Adding an edge significantly improves fit (α = 0.05) then Add edge that improves fit the most else if Removing an edge does not significantly worsen fit (α = 0.05) then Remove edge that worsens fit the least end if until No added edge significantly improves fit and removing any edge significantly worsens fit Algorithm 2 Stepwise network estimation by AIC, BIC or EBIC optimization. Start with initial setup for Ω Ω Ω repeat for all Unique elements of Ω Ω Ω do Remove edge if present or add edge if absent Fit model with changed edge end for if Any changed edge improved AIC, BIC or EBIC then Change edge that improved AIC, BIC or EBIC the most end if until No changed edge improves AIC, BIC or EBIC Simulation Study 1: Latent Network Modeling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Model used in simulation study 1: step-wise model search in latent network modeling. Four latent variables were each measured by three items. Latent variables covary due to the structure of a latent Gaussian graphical model in which edges indicate partial correlation coefficients. This model has the form of a chain graph, which cannot be represented in a structural equation model. Factor loadings, residual variances and latent variances were set to 1 and the latent partial correlations had an average of 0.33 with a standard deviation of 0.04.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 7 Figure 6 :</head><label>76</label><figDesc>Figure7shows the results of the simulation study. The top panel shows that sensitivity increases with sample size and performs best when using AIC as the criterion. BIC performed comparably in sensitivity to χ 2 testing and EBIC performed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Simulation results of simulation study 2: step-wise model search in residual network modeling. Each condition was replicated 1 000 times, leading to 20 000 total simulated datasets. High sensitivity indicates that the method is able to detect edges in the true model, and high specificity indicates that the method does not detect edges that are zero in the true model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Simulation results of simulation study 3: model selection via penalized maximum likelihood estimation in latent network modeling. The same model as in Figure 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Simulation results of simulation study 4: model selection via penalized maximum likelihood estimation in residual network modeling. The same model as in Figure 6 was used except now with 4 latent variables leading to 20 observed variables. For each sample size 1 000 datasets were generated, leading to 5 000 total simulated datasets on which AIC, BIC or EBIC was used to select the best model. High sensitivity indicates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Visualization of the factor structure and latent network (left) and the residual network (right) of the BFI personality dataset from the psych package in R. LASSO estimation with 100 different tuning parameters in combination with EBIC model selection was used to first estimate the residual network structure and following the latent network structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 2 . Examples of possible models under four different modeling frameworks. Circular nodes indicate latent variables, square nodes indicate manifest variables and gray nodes indicate residuals. Directed edges indicate factor loadings or regression parameters and undirected edges indicate pairwise interactions. Note that such undirected edges do not indicate covariances, which are typically denoted with bidirectional edges. Replacing covariances with interactions is where the network models differ from typical SEM.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>else if Removing an</head><label></label><figDesc>Any changed edge improved AIC, BIC or EBIC then Change edge that improved AIC, BIC or EBIC the most end if until No changed edge improves AIC, BIC or EBIC</figDesc><table><row><cell>Algorithm 1 Stepwise network estimation by χ 2 difference testing. Start with initial setup for Ω Ω Ω repeat for all Unique elements of Ω Ω Ω do Remove edge if present or add edge if absent Fit model with changed edge end for if Adding an edge significantly improves fit (α = 0.05) then Add edge that improves fit the most</cell></row></table><note><p>edge does not significantly worsen fit (α = 0.05) then Remove edge that worsens fit the least end if until No added edge significantly improves fit and removing any edge significantly worsens fit Algorithm 2 Stepwise network estimation by AIC, BIC or EBIC optimization. Start with initial setup for Ω Ω Ω repeat for all Unique elements of Ω Ω Ω do Remove edge if present or add edge if absent Fit model with changed edge end for if</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Simulation Study 1: Latent Network Modeling.</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1</head><label>1</label><figDesc>Figure9. Simulation results of simulation study 4: model selection via penalized maximum likelihood estimation in residual network modeling. The same model as in Figure6was used except now with 4 latent variables leading to 20 observed variables. For each sample size 1 000 datasets were generated, leading to 5 000 total simulated datasets on which AIC, BIC or EBIC was used to select the best model. High sensitivity indicates that the method is able to detect edges in the true model, and high specificity indicates that the method does not detect edges that are zero in the true model. Fit measures for three models estimated on the BFI dataset in the psych R package. CFA is the correlated five-factor model. RNM is the same model as the CFA model with a residual network. RNM+LNM denotes the same model as the RNM model in which edges of the latent network have been removed.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Specificity (true negative rate)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Specificity</cell><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>100</cell><cell>250</cell><cell>500 1000 2500</cell><cell>100</cell><cell>250</cell><cell>500 1000 2500</cell><cell>100</cell><cell>250</cell><cell>500 1000 2500</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Sample Size</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>High sensitivity indicates that the method is able to detect edges in the true model, and high specificity indicates that the method does not detect edges that are zero in the true model. investigated the sensitivity and specificity. The top panel shows that sensitivity improves with sample size, with AIC performing best and EBIC worst. From sample sizes of 500 and higher all estimation criterion performed well in retrieving the edges.</figDesc><table><row><cell>0.25 0.50 0.75 1.00 Figure 5: Simulation results of simulation study 1: step-wise model search in latent 50 100 250 500 1000 50 100 250 500 1000 50 100 250 500 1000 50 100 250 500 1000 Sample Size Specificity Specificity (true negative rate) network modeling. Each condition was replicated 1 000 times, leading to 20 000 total best and AIC worst. These results indicate that the step-wise procedure is conservative and prefers simpler models to more complex models; missing edges are adequately detected but present edges in the true model might go unnoticed except in larger samples. With sample sizes over 500, all four estimation methods show both a high sensitivity and specificity. simulated datasets. The bottom panel shows that specificity is generally very high, with EBIC performing Simulation Study 2: Residual Network Modeling</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Algorithm 3 LASSO estimation for exploratory network search.for all Sequence of tuning parameters ν 1 , ν 2 , . . . do</figDesc><table><row><cell>Estimate LASSO regularized model using given tuning parameter</cell></row><row><cell>Count the number of parameters for which the absolute estimate is larger than</cell></row><row><cell>Determine information criterion AIC or BIC given fit and number of parameters</cell></row><row><cell>end for</cell></row><row><cell>Select model with best AIC, BIC or EBIC</cell></row><row><cell>Refit this model without LASSO in which absolute parameters smaller than are fixed</cell></row><row><cell>to zero</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 1 :</head><label>1</label><figDesc>Thus, after fitting a RNM model, we can again apply LASSO to estimate a latent network in the resulting model, which we term an RNM+LNM model. The R code used for this analysis can be found in the supplementary materials. Fit measures for three models estimated on the BFI dataset in the psych R package. CFA is the correlated five-factor model, RNM the same model but with a residual network, and RNM+LNM the RNM model in which edges of the latent network been removed.</figDesc><table><row><cell>df</cell><cell>χ 2</cell><cell>AIC</cell><cell>BIC</cell><cell cols="2">EBIC RMSEA TLI CFI</cell></row><row><cell cols="5">CFA 265 4713.94 183233.67 183589.91 184542.39</cell><cell>0.08 0.75 0.78</cell></row><row><cell>RNM 172</cell><cell cols="4">806.63 179510.97 180419.39 182848.22</cell><cell>0.04 0.94 0.97</cell></row><row><cell>RNM+LNM 176</cell><cell cols="4">843.18 179539.52 180424.19 182789.53</cell><cell>0.04 0.94 0.97</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Throughout this paper, vectors will be represented with lowercase boldfaced letters and matrices will be denoted by capital boldfaced letters. Roman letters will be used to denote observed variables and parameters (such as the number of nodes) and Greek letters will be used to denote latent variables and parameters that need to be estimated. The subscript i will be used to denote the realized response vector of subject i and omission of this subscript will be used to denote the response of a random subject.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We make use here of the convenient all-y notation and do not distinguish between exogenous and endogenous latent variables(Hayduk, 1987).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>A saturated GGM is also called a partial correlation network because it contains the sample partial correlation coefficients as edge weights.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>To our knowledge, the GGM has not yet been framed in this form. We chose this form because it allows for clear modeling and interpretation of the network parameters.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>We use the CFA framework instead of the SEM framework here as the main application of this framework is in exploratively estimating relationships between latent variables.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Developmental version: http://www.github.com/sachaepskamp/lvnet; stable version: https://cran.r-project.org/package=lvnet</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_6"><p>We make use here of the convenient all-y notation and do not distinguish between exogenous and endogenous latent variables(Hayduk, 1987).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_7"><p>A saturated GGM is also called a partial correlation network because it contains the sample partial correlation coefficients as edge weights.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_8"><p>September 13, 2017</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_9"><p>To our knowledge, the GGM has not yet been framed in this form. We chose this form because it allows for clear modeling and interpretation of the network parameters.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_10"><p>We use the CFA framework instead of the SEM framework here as the main application of this framework is in exploratively estimating relationships between latent variables.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_11"><p>github.com/sachaepskamp/lvnet</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>detected in the estimation versus the total number of absent edges in the true model: specificity = # true negatives # true negatives + # false positives</p><p>The specificity can be seen as a function of the number of false positives: a high specificity indicates that there were not many edges detected to be nonzero that are zero in the true model. To favor degrees of freedom, model sparsity and interpretability, specificity should be high all-around-estimation techniques should not result in many false positives-whereas sensitivity should increase as a function of the sample size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulating Gaussian Graphical models</head><p>In all simulation studies reported here, networks were constructed in the same way as done by <ref type="bibr">Yin and Li (2011)</ref> in order to obtain a positive definite inverse-covariance matrix K K K. First, a network structure was generated without weights. Next, weights were drawn randomly from a uniform distribution between 0.5 and 1, and made negative with 50% probability. The diagonal elements of K K K were then set to 1.5 times the sum of all absolute values in the corresponding row, or 1 if this sum was zero.</p><p>Next, all values in each row were divided by the diagonal value, ensuring that the diagonal values become 1. Finally, the matrix was made symmetric by averaging the lower and upper triangular elements. In the chain graphs used in the following simulations, this algorithm created networks in which the non-zero partial correlations had a mean of 0.33 and a standard deviation of 0.04.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stepwise Model Search</head><p>In exploratory search, we are interested in recovering the network structure of q q q q q q q q q q q q q q q q q q q q q q q q q Agreeableness A1: Am indifferent to the feelings of others. A2: Inquire about others' well-being. A3: Know how to comfort others. A4: Love children. A5: Make people feel at ease. Conscientiousness C1: Am exacting in my work. C2: Continue until everything is perfect. C3: Do things according to a plan. C4: Do things in a half-way manner. C5: Waste my time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extraversion</head><p>E1: Don't talk a lot. E2: Find it difficult to approach others. E3: Know how to captivate people. E4: Make friends easily. E5: Take charge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Openness</head><p>O1: Am full of ideas. O2: Avoid difficult reading material. O3: Carry the conversation to a higher level. O4: Spend time reflecting on things. O5: Will not probe deeply into a subject. q q q q q q q q q q q q q q q q q q q q q q q q q Zou, H. and Hastie, T. <ref type="bibr">(2005)</ref>. Regularization and variable selection via the elastic net.</p><p>Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301-320. <ref type="bibr" target="#b61">Zou, H., Hastie, T., Tibshirani, R., et al. (2007)</ref>. On the degrees of freedom of the lasso. The Annals of Statistics, 35(5):2173-2192.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Full information estimation in the presence of incomplete data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Arbuckle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Marcoulides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schumacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced structural equation modeling: Issues and techniques</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="243" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Los Cinco Grandes across Cultures and Ethnic Groups: Multitrait Multimethod Analyses of the Big Five in Spanish and English</title>
		<author>
			<persName><forename type="first">V</forename><surname>Benet-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="729" to="750" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Comparative fit indexes in structural models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Bentler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="238" to="346" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Psychometric Perspectives on Diagnostic Systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of clinical psychology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1089" to="1108" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Network analysis: An integrative approach to the structure of psychopathology</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O J</forename><surname>Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Clinical Psychology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="91" to="121" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The small world of psychopathology</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O J</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Schmittmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Waldorp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">27407</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Alternative ways of assessing model fit</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cudeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological Methods &amp; Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="230" to="258" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Latent variable graphical model selection via convex optimization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1935" to="1967" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>with discussion</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extended bayesian information criteria for model selection with large model spaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="759" to="771" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">State of the aRt personality research: A tutorial on network analysis of personality data in R</title>
		<author>
			<persName><forename type="first">G</forename><surname>Costantini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perugini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mõttus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Waldorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Research in Personality</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="13" to="29" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dimensions of normal personality as networks in search of equilibrium: You can&apos;t like parties if you don&apos;t like people</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O J</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sluis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Noordhof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wichers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Geschwind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Aggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Kendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Personality</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="414" to="431" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comorbidity: A Network Perspective</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O J</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Waldorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Der Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="137" to="150" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Toward a formalized account of attitudes: The Causal Attitude Network (CAN) model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dalege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Van Harreveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Conner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L J</forename><surname>Van Der Maas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="22" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Five Robust Trait Dimensions: Development, Stability, and Utility</title>
		<author>
			<persName><forename type="first">J</forename><surname>Digman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="214" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sensitivity and specificity of information criteria. The Methodology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Coffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Lanza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>Center and Department of Statistics, Penn State, The Pennsylvania State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tail-measurability in monotone latent variable models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Junker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="523" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Maris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Waldorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<title level="m">Handbook of Psychometrics</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Irwing</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Hughes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Booth</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
	<note>Network psychometrics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Waldorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mõttus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04156</idno>
		<title level="m">Discovering psychological dynamics in time-series data</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Extended Bayesian information criteria for Gaussian graphical models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Foygel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2020" to="2028" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From loss to loneliness: The relationship between bereavement and depressive symptoms</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bockting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amshoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tuerlinckx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stroebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of abnormal psychology</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="256" to="265" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Mental disorders as networks of problems: a review of recent insights</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Borkulo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Group search algorithm recovers effective connectivity maps for individuals in homogeneous and heterogeneous samples</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Gates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Molenaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="310" to="319" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Structure of Phenotypic Personality Traits</title>
		<author>
			<persName><forename type="first">L</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="34" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An alternative &quot;description of personality&quot;: the big-five factor structure</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1216" to="1229" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Structural equation modeling with LISREL: Essentials and advances</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hayduk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Johns Hopkins University Press</publisher>
			<pubPlace>Baltimore, MD, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ridge regression: Biased estimation for nonorthogonal problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hoerl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Kennard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional association and unidimensionality in monotone latent variable models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Rosenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1523" to="1543" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Beitrag zur theorie des ferromagnetismus</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ising</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zeitschrift für Physik A Hadrons and Nuclei</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="253" to="258" />
			<date type="published" when="1925">1925</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Network Approach to Environmental Impact in Psychotic Disorders: Brief Theoretical Framework</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Isvoranu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Os</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guloksuz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Schizophrenia Bulletin</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="870" to="873" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Isvoranu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Van Borkulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Boyette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T W</forename><surname>Wigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Vinkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><surname>Investigators</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Network Approach to Psychosis: Pathways between Childhood Trauma and Psychotic Symptoms</title>
		<imprint>
			<date type="published" when="2016-05-10">2016. May 10, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regularized structural equation modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jacobucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="566" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A general approach to confirmatory maximum likelihood factor analysis</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Jöreskog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETS Research Bulletin Series</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="1967">1967. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Structural equation modeling: Foundations and extensions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kaplan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Sage, Thousand Oaks, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Statistical analysis of network data</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Kolaczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: Principles and techniques</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The application of a network approach to health-related quality of life (HRQoL): Introducing a new method for assessing hrqol in healthy adults and cancer patient</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Kossakowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kieffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Van Borkulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rhemtulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quality of Life Research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="781" to="792" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<title level="m">Graphical models</title>
		<meeting><address><addrLine>Oxford, UK.</addrLine></address></meeting>
		<imprint>
			<publisher>Clarendon Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">VI.-the estimation of factor loadings by the method of maximum likelihood</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Lawley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of Edinburgh</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="64" to="82" />
			<date type="published" when="1940">1940</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Statistical theories of mental test scores</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Novick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birnbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Oxford, UK.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The problem of equivalent models in applications of covariance structure analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Maccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Wegener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Uchino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Fabrigar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="199" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploratory structural equation modeling: An integration of the best features of exploratory and confirmatory factor analysis</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Marsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Clinical Psychology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="85" to="110" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bayesian inference for low-rank ising networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Maris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bechger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">9050</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Personality trait structure as a human universal</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Mccrae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="509" to="516" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Variations of box plots</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mcgill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mental disorders as causal systems a network approach to posttraumatic stress disorder</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mcnally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Robinaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Deserno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Psychological Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="836" to="849" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Machine learning: A probabilistic perspective</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>MIT press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Openmx 2.0: Extended structural equation and statistical modeling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Neale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Pritikin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zahery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Brick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Estabrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Boker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="535" to="549" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning, and Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">psych: Procedures for Psychological, Psychometric, and Personality Research (R package version 1</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Reckase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Revelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2010</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="0" to="93" />
			<pubPlace>New York, NY, USA; Evanston, Illinois</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Northwestern University</orgName>
		</respStmt>
	</monogr>
	<note>Multidimensional item response theory</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Post-hoc selection of dynamic causal models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Penny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience methods</title>
		<imprint>
			<biblScope unit="volume">208</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="78" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deconstructing the construct: A network perspective on psychological phenomena</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Schmittmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O J</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Waldorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Kievit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Ideas in Psychology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="53" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Van Borkulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Blanken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Boschloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Schoevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Waldorp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A new method for constructing networks from binary data</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Association of symptom network structure with the course of depression</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Van Borkulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Boschloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W J H</forename><surname>Penninx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Waldorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Schoevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Psychiatry</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1219" to="1226" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A dynamical model of general intelligence: The positive manifold of intelligence by mutualism</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Van Der Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Grasman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Huizenga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Raijmakers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="842" to="861" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">The MOS 36-item short-form health survey</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Ware</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Sherbourne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>SF-36</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Conceptual framework and item selection</title>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Care</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="473" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Correlation and causation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of agricultural research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="557" to="585" />
			<date type="published" when="1921">1921</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The method of path coefficients</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="161" to="215" />
			<date type="published" when="1934">1934</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A sparse conditional gaussian graphical model for analysis of genetical genomics data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2630" to="2650" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">On the &quot;degrees of freedom&quot; of the lasso</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2173" to="2192" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Full information estimation in the presence of incomplete data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Arbuckle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Marcoulides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schumacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced structural equation modeling: Issues and techniques</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="243" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Los Cinco Grandes across cultures and ethnic groups: Multitrait multimethod analyses of the Big Five in Spanish and English</title>
		<author>
			<persName><forename type="first">V</forename><surname>Benet-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="729" to="750" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Comparative fit indexes in structural models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Bentler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="238" to="346" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Psychometric perspectives on diagnostic systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of clinical psychology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1089" to="1108" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Network analysis: an integrative approach to the structure of psychopathology</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O J</forename><surname>Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of clinical psychology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="91" to="121" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The small world of psychopathology</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O J</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Schmittmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Waldorp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">27407</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Alternative ways of assessing model fit</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cudeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological Methods &amp; Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="230" to="258" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Latent variable graphical model selection via convex optimization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Parrilo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1610" to="1613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Extended bayesian information criteria for model selection with large model spaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="759" to="771" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Dimensions of normal personality as networks in search of equilibrium: You can&apos;t like parties if you don&apos;t like people</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O J</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sluis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Noordhof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wichers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Geschwind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Aggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Kendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Personality</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="414" to="431" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Comorbidity: a network perspective</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O J</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Waldorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L J</forename><surname>Van Der Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="137" to="150" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Five robust trait dimensions: Development, stability, and utility</title>
		<author>
			<persName><forename type="first">J</forename><surname>Digman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="214" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Sensitivity and specificity of information criteria. The Methodology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Coffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Lanza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>Center and Department of Statistics, Penn State, The Pennsylvania State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Tail-measurability in monotone latent variable models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Junker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="523" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Maris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Waldorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<title level="m">Handbook of Psychometrics</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Irwing</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Hughes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Booth</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
	<note>Network psychometrics</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Extended Bayesian information criteria for Gaussian graphical models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Foygel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="604" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Group search algorithm recovers effective connectivity maps for individuals in homogeneous and heterogeneous samples</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Gates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Molenaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="310" to="319" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">The structure of phenotypic personality traits. Personality: critical concepts in psychology</title>
		<author>
			<persName><forename type="first">L</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="26" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">An alternative&quot; description of personality&quot;: The Big-Five factor structure. The Science of Mental Health: Personality and personality disorder</title>
		<author>
			<persName><forename type="first">L</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Structural equation modeling with LISREL: Essentials and advances</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hayduk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Johns Hopkins University Press</publisher>
			<pubPlace>Baltimore, MD, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Ridge regression: Biased estimation for nonorthogonal problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hoerl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Kennard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Conditional association and unidimensionality in monotone latent variable models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Rosenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1523" to="1543" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Beitrag zur theorie des ferromagnetismus</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ising</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zeitschrift für Physik A Hadrons and Nuclei</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="253" to="258" />
			<date type="published" when="1925">1925</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Regularized structural equation modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jacobucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcardle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A general approach to confirmatory maximum likelihood factor analysis</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Jöreskog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETS Research Bulletin Series</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="1967">1967. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Structural equation modeling: Foundations and extensions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kaplan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Sage, Thousand Oaks, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Statistical analysis of network data with R</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Kolaczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Csárdi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">Probabilistic graphical models: principles and techniques</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">The application of a network approach to health-related quality of life (HRQoL): Introducing a new method for assessing hrqol in healthy adults and cancer patient</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Kossakowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kieffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Van Borkulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rhemtulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quality of Life Research</title>
		<imprint>
			<biblScope unit="page" from="962" to="9343" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Graphical models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford, UK.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Vi.the estimation of factor loadings by the method of maximum likelihood</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Lawley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of Edinburgh</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="64" to="82" />
			<date type="published" when="1940">1940</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Statistical theories of mental test scores</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Novick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birnbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Oxford, UK.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">The problem of equivalent models in applications of covariance structure analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Maccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Wegener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Uchino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Fabrigar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="199" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Exploratory structural equation modeling: An integration of the best features of exploratory and confirmatory factor analysis</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Marsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Clinical Psychology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="85" to="110" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Personality trait structure as a human universal</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mccrae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American psychologist</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="509" to="516" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Variations of box plots</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mcgill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>MIT press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Neale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Pritikin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zahery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Brick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Estabrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Boker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Openmx 2.0: Extended structural equation and statistical modeling</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Causality: models, reasoning and inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge Univ Press</publisher>
			<biblScope unit="volume">29</biblScope>
			<pubPlace>Cambridge, UK.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Reckase</surname></persName>
		</author>
		<title level="m">Multidimensional item response theory</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">W</forename><surname>Revelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<pubPlace>Evanston, Illinois</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Procedures for Psychological, Psychometric, and Personality Research. Northwestern University</orgName>
		</respStmt>
	</monogr>
	<note>R package version 1.5.8</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Post-hoc selection of dynamic causal models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Penny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience methods</title>
		<imprint>
			<biblScope unit="volume">208</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="78" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Schmittmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O J</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Waldorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Kievit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<title level="m">Deconstructing the construct: A network perspective on psychological phenomena. New ideas in psychology</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="43" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Van Borkulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Epskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Blanken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Boschloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Schoevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Waldorp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A new method for constructing networks from binary data</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5918</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">A dynamical model of general intelligence: the positive manifold of intelligence by mutualism</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L J</forename><surname>Van Der Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P P P</forename><surname>Grasman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Huizenga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E J</forename><surname>Raijmakers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="842" to="861" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">The MOS 36-item short-form health survey (SF-36): I. Conceptual framework and item selection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Ware</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Sherbourne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Care</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="473" to="483" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Correlation and causation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of agricultural research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="557" to="585" />
			<date type="published" when="1921">1921</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">The method of path coefficients</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="161" to="215" />
			<date type="published" when="1934">1934</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">A sparse conditional gaussian graphical model for analysis of genetical genomics data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of applied statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">26302650</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
