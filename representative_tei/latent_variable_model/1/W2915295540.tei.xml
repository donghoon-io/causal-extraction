<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Action Spaces for Reinforcement Learning in End-to-end Dialog Agents with Latent Variable Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-04-15">15 Apr 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
							<email>tianchez@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaige</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Action Spaces for Reinforcement Learning in End-to-end Dialog Agents with Latent Variable Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-04-15">15 Apr 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1902.08858v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Defining action spaces for conversational agents and optimizing their decision-making process with reinforcement learning is an enduring challenge. Common practice has been to use handcrafted dialog acts, or the output vocabulary, e.g. in neural encoder decoders, as the action spaces. Both have their own limitations. This paper proposes a novel latent action framework that treats the action spaces of an end-to-end dialog agent as latent variables and develops unsupervised methods in order to induce its own action space from the data. Comprehensive experiments are conducted examining both continuous and discrete action types and two different optimization methods based on stochastic variational inference. Results show that the proposed latent actions achieve superior empirical performance improvement over previous word-level policy gradient methods on both DealOrNoDeal and MultiWoz dialogs. Our detailed analysis also provides insights about various latent variable approaches for policy learning and can serve as a foundation for developing better latent actions in future research. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Optimizing dialog strategies in multi-turn dialog models is the cornerstone of building dialog systems that more efficiently solve real-world challenges, e.g. providing information <ref type="bibr" target="#b40">(Young, 2006)</ref>, winning negotiations <ref type="bibr" target="#b16">(Lewis et al., 2017)</ref>, improving engagement <ref type="bibr" target="#b17">(Li et al., 2016)</ref> etc. A classic solution employs reinforcement learning (RL) to learn a dialog policy that models the optimal action distribution conditioned on the dialog state <ref type="bibr" target="#b33">(Williams and Young, 2007)</ref>. However, since there are infinite human language possibilities, an enduring challenge has been to define what the action space is. For traditional modular systems, the action space is defined by hand-crafted semantic representations such as dialog acts and slotvalues <ref type="bibr" target="#b21">(Raux et al., 2005;</ref><ref type="bibr" target="#b2">Chen et al., 2013)</ref> and the goal is to obtain a dialog policy that chooses the best hand-crafted action at each dialog turn. But it is limited because it can only handle simple domains whose entire action space can be captured by hand-crafted representations <ref type="bibr" target="#b29">(Walker, 2000;</ref><ref type="bibr" target="#b27">Su et al., 2017)</ref>. This cripples a system's ability to handle conversations in complex domains.</p><p>Conversely, end-to-end (E2E) dialog systems have removed this limit by directly learning a response generation model conditioned on the dialog context using neural networks <ref type="bibr" target="#b28">(Vinyals and Le, 2015;</ref><ref type="bibr" target="#b26">Sordoni et al., 2015)</ref>. To apply RL to E2E systems, the action space is typically defined as the entire vocabulary; every response output word is considered to be an action selection step <ref type="bibr" target="#b17">(Li et al., 2016)</ref>, which we denote as the word-level RL. Word-level RL, however, has been shown to have several major limitations in learning dialog strategies. The foremost one is that direct application of word-level RL leads to degenerate behavior: the response decoder deviates from human language and generates utterances that are incomprehensible <ref type="bibr" target="#b16">(Lewis et al., 2017;</ref><ref type="bibr" target="#b4">Das et al., 2017;</ref><ref type="bibr" target="#b13">Kottur et al., 2017)</ref>. A second issue is that since a multi-turn dialog can easily span hundreds of words, word-level RL suffers from credit assignment over a long horizon, leading to slow and suboptimal convergence <ref type="bibr" target="#b11">(Kaelbling et al., 1996;</ref><ref type="bibr" target="#b8">He et al., 2018)</ref>.</p><p>This paper proposes Latent Action Reinforcement Learning (LaRL), a novel framework that overcomes the limitations of word-level RL for E2E dialog models, marrying the benefits of a traditional modular approach in an unsupervised manner. The key idea is to develop E2E models that can invent their own discourse-level ac-tions. These actions must be expressive enough to capture response semantics in complex domains (i.e. have the capacity to represent a large number of actions), thus decoupling the discourselevel decision-making process from natural language generation. Then any RL technique can be applied to this induced action space in the place of word-level output. We propose a flexible latent variable dialog framework and investigate several approaches to inducing latent action space from natural conversational data. We further propose (1) a novel training objective that outperforms the typical evidence lower bound used in dialog generation and (2) an attention mechanism for integrating discrete latent variables in the decoder to better model long responses.</p><p>We test this on two datasets, DealOrN-oDeal <ref type="bibr" target="#b16">(Lewis et al., 2017)</ref> and Multi-Woz <ref type="bibr" target="#b0">(Budzianowski et al., 2018)</ref>, to answer two key questions: (1) what are the advantages of LaRL over  what effective methods can induce this latent action space. Results show that LaRL is significantly more effective than word-level RL for learning dialog policies and it does not lead to incomprehensible language generation. Our models achieve 18.2% absolute improvement over the previous stateof-the-art on MultiWoz and discover novel and diverse negotiation strategies on DealOrNoDeal. Besides strong empirical improvement, our model analysis reveals novel insights, e.g. it is crucial to reduce the exposure bias in the latent action space and discrete latent actions are more suitable than continuous ones to serve as action spaces for RL dialog agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Prior RL research in modular dialog management has focused on policy optimization over hand-crafted action spaces in task-oriented domains <ref type="bibr" target="#b29">(Walker, 2000;</ref><ref type="bibr" target="#b38">Young et al., 2007)</ref>. A dialog manager is formulated as a Partially Observable Markov Decision Process (POMDP) <ref type="bibr" target="#b39">(Young et al., 2013)</ref>, where the dialog state is estimated via dialog state tracking models from the raw dialog context <ref type="bibr" target="#b15">(Lee, 2013;</ref><ref type="bibr" target="#b9">Henderson et al., 2014;</ref><ref type="bibr" target="#b22">Ren et al., 2018)</ref>. RL techniques are then used to find the optimal dialog policy <ref type="bibr" target="#b6">(Gasic and Young, 2014;</ref><ref type="bibr" target="#b27">Su et al., 2017;</ref><ref type="bibr" target="#b32">Williams et al., 2017)</ref>. Recent deep-learning modular dialog models have also explored joint optimization over dialog pol-icy and state tracking to achieve stronger performance <ref type="bibr" target="#b30">(Wen et al., 2016;</ref><ref type="bibr">Zhao and Eskenazi, 2016;</ref><ref type="bibr" target="#b18">Liu and Lane, 2017)</ref>.</p><p>A related line of work is reinforcement learning for E2E dialog systems. Due to the flexibility of encoder-decoder dialog models, prior work has applied reinforcement learning to more complex domains and achieved higher dialog-level rewards, such as open-domain chatting <ref type="bibr" target="#b17">(Li et al., 2016;</ref><ref type="bibr">Serban et al., 2017a)</ref>, negotiation <ref type="bibr" target="#b16">(Lewis et al., 2017)</ref>, visual dialogs <ref type="bibr" target="#b4">(Das et al., 2017)</ref>, grounded dialog <ref type="bibr" target="#b20">(Mordatch and Abbeel, 2017)</ref> etc. As discussed in Section 1, these methods consider the output vocabulary at every decoding step to be the action space; they suffer from limitations such as deviation from natural language and sub-optimal convergence.</p><p>Finally, research in latent variable dialog models is closely related to our work, which strives to learn meaningful latent variables for E2E dialog systems. Prior work has shown that learning with latent variables leads to benefits like diverse response decoding <ref type="bibr">(Serban et al., 2017b;</ref><ref type="bibr">Zhao et al., 2017;</ref><ref type="bibr" target="#b1">Cao and Clark, 2017)</ref>, interpretable decision-making <ref type="bibr" target="#b31">(Wen et al., 2017;</ref><ref type="bibr">Zhao et al., 2018)</ref> and zero-shot domain transfer <ref type="bibr">(Zhao and Eskenazi, 2018)</ref>. Also, driven by similar motivations of this work, prior studies have explored to utilize a coarse discrete node, either handcrafted or learned, to decouple the word generation process from dialog policy in E2E systems for better dialog policy <ref type="bibr" target="#b8">(He et al., 2018;</ref><ref type="bibr" target="#b37">Yarats and Lewis, 2017)</ref>. Our work differs from prior work for two reasons: (1) latent action in previous work is only auxiliary, small-scale and mostly learned in a supervised or semi-supervised setting. This paper focuses on unsupervised learning of latent variables and learns variables that are expressive enough to capture the entire action space by itself.</p><p>(2) to our best knowledge, our work is the first comprehensive study of the use of latent variables for RL policy optimization in dialog systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baseline Approach</head><p>E2E response generation can be treated as a conditional language generation task, which uses neural encoder-decoders <ref type="bibr" target="#b3">(Cho et al., 2014)</ref> to model the conditional distribution p(x|c) where c is the observed dialog context and x is the system's response to the context. The format of the dialog context is domain dependent. It can vary from tex- tual raw dialog history <ref type="bibr" target="#b28">(Vinyals and Le, 2015)</ref> to visual and textual context <ref type="bibr" target="#b4">(Das et al., 2017)</ref>. Training with RL usually has 2 steps: supervised pretraining and policy gradient reinforcement learning <ref type="bibr" target="#b34">(Williams and Zweig, 2016;</ref><ref type="bibr" target="#b5">Dhingra et al., 2017;</ref><ref type="bibr" target="#b17">Li et al., 2016)</ref>. Specifically, the supervised learning step maximizes the log likelihood on the training dialogs, where θ is the model parameter:</p><formula xml:id="formula_0">L SL (θ) = E x,c [log p θ (x|c)]<label>(1)</label></formula><p>Then the following RL step uses policy gradients, e.g. the REINFORCE algorithm <ref type="bibr" target="#b35">(Williams, 1992)</ref> to update the model parameters with respect to task-dependent goals. We assume that we have an environment that the dialog agent can interact with and that there is a turn-level reward r t at every turn t of the dialog. We can then write the expected discounted return under a dialog model θ as</p><formula xml:id="formula_1">J(θ) = E[ T 0 γ t r t ],</formula><p>where γ ∈ [0, 1] is the discounting factor and T is the length of the dialog. Often a baseline function b is used to reduce the variance of the policy gradient <ref type="bibr" target="#b7">(Greensmith et al., 2004)</ref>, leading to R t = T -t k=0 γ k (r t+k -b). Word-level Reinforcement Learning: as shown in Figure <ref type="figure" target="#fig_0">1</ref>, the baseline approach treats every output word as an action step and its policy gradient is:</p><formula xml:id="formula_2">∇ θ J(θ) = E θ [ T t=0 Ut j=0 R tj ∇ θ log p θ (w tj |w &lt;tj , c t )]</formula><p>(2) where U t is the number of tokens in the response at turn t and j is the word index in the response. It is evident that Eq 2 has a very large action space, i.e. |V | and a long learning horizon, i.e. T U . Prior work has found that the direct application of Eq 2 leads to divergence of the decoder. The common solution is to alternate with supervised learning with Eq 2 at a certain ratio <ref type="bibr" target="#b16">(Lewis et al., 2017)</ref>. We denote this ratio as RL:SL=A:B, which means for every A policy gradient updates, we run B supervised learning updates. We use RL:SL=off for the case where only policy gradients are used and no supervised learning is involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Latent Action Reinforcement Learning</head><p>We now describe the proposed LaRL framework. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, a latent variable z is introduced in the response generation process. The conditional distribution is factorized into p(x|c) = p(x|z)p(z|c) and the generative story is: (1) given a dialog context c we first sample a latent action z from p θe (z|c) and ( <ref type="formula">2</ref>) generate the response by sampling x based on z via p θ d (x|z), where p θe is the dialog encoder network and p θ d is the response decoder network. Given the above setup, LaRL treats the latent variable z as its action space instead of outputting words in response x. We can now apply REINFORCE in the latent action space:</p><formula xml:id="formula_3">∇ θ J(θ) = E θ [ T t=0 R t log p θ (z|c t )]<label>(3)</label></formula><p>Compared to Eq 2, LaRL differs by:</p><p>• Shortens the horizon from T U to T .</p><p>• Latent action space is designed to be lowdimensional, much smaller than V .</p><p>• The policy gradient only updates the encoder θ e and the decoder θ d stays intact.</p><p>These properties reduce the difficulties for dialog policy optimization and decouple high-level decision-making from natural language generation. The p θe are responsible for choosing the best latent action given a context c while p θ d is only responsible for transforming z into the surface-form words. Our formulation also provides a flexible framework for experimenting with various types of model learning methods. In this paper, we focus on two key aspects: the type of latent variable z and optimization methods for learning z in the supervised pre-training step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Types of Latent Actions</head><p>Two types of latent variables have been used in previous research: continuous isotropic Gaussian distribution <ref type="bibr">(Serban et al., 2017b)</ref> and multivariate categorical distribution <ref type="bibr">(Zhao et al., 2018)</ref>. These two types are both compatible with our LaRL framework and can be defined as follows:</p><p>Gaussian Latent Actions follow M dimensional multivariate Gaussian distribution with a diagonal covariance matrix, i.e. z ∼ N (µ, σ 2 I). Let the encoder p θe consist of two parts: a context encoder F, a neural network that encodes the dialog context c into a vector representation h, and a feed forward network π that projects h into µ and σ. The process is defined as follows:</p><formula xml:id="formula_4">h = F(c) (4) µ log(σ 2 ) = π(h) (5) p(x|z) = p θ d (z) z ∼ N (µ, σ 2 I)<label>(6)</label></formula><p>where the sampled z is used as the initial state of the decoder for response generation. Also we use p θ (z|c) = N (z; µ, σ 2 I) to compute the policy gradient update in Eq 3. Categorical Latent Actions are M independent K-way categorical random variables. Each z m has its own token embeddings to map latent symbols into vector space E m ∈ R K×D where m ∈ [1, M ] and D is the embedding size. Thus M latent actions can represent exponentially, K M , unique combinations, making it expressive enough to model dialog acts in complex domains. Similar to Gaussian Latent Actions, we have</p><formula xml:id="formula_5">h = F(c) (7) p(Z m |c) = softmax(π m (h)) (8) p(x|z) = p θ d (E 1:M (z 1:M )) z m ∼ p(Z m |c) (9)</formula><p>For the computing policy gradient in Eq 3, we have p θ (z|c) = M m=1 p(Z m = z m |c) Unlike Gaussian latent actions, a matrix R M ×D comes after the embedding layers E 1:M (z 1:M ), whereas the decoder's initial state is a vector of size R D . Previous work integrated this matrix with the decoder by summing over the latent embeddings, i.e. x = p θ d ( M 1 E m (z m )), denoted as Summation Fusion for later discussion <ref type="bibr">(Zhao et al., 2018)</ref>. A limitation of this method is that it could lose fine-grained order information in each latent dimension and have issues with long responses that involve multiple dialog acts. Therefore, we propose a novel method, Attention Fusion, to combine categorical latent actions with the decoder. We apply the attention mechanism <ref type="bibr" target="#b19">(Luong et al., 2015)</ref> over latent actions as the following. Let i be the step index during decoding. Then we have:</p><formula xml:id="formula_6">α mi = softmax(h T i W a E m (z m )) (10) c i = M m=1 α mi E m (z m )<label>(11)</label></formula><formula xml:id="formula_7">h i = tanh(W s h i c i ) (12) p(w i |h i , c i ) = softmax(W o h i )<label>(13)</label></formula><p>The decoder's next state is updated by h i+1 = RNN(h i , w i+1 ), h i ) and h 0 is computed via summation-fusion. Thus attention fusion lets the decoder focus on different latent dimensions at each generation step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Optimization Approaches</head><p>Full ELBO: Now given a training dataset {x, c}, our base optimization method is via stochastic variational inference by maximizing the evidence lowerbound (ELBO), a lowerbound on the data log likelihood:</p><formula xml:id="formula_8">L f ull (θ) = p q(z|x,c) (x|z)-D KL [q(z|x, c) p(z|c)]<label>(14)</label></formula><p>where q γ (z|x, c) is a neural network that is trained to approximate the posterior distribution q(z|x, c) and p(z|c) and p(x|z) are achieved by F, π and p θ d . For Gaussian latent actions, we use the reparametrization trick (Kingma and Welling, 2013) to backpropagate through Gaussian latent actions and the Gumbel-Softmax <ref type="bibr" target="#b10">(Jang et al., 2016)</ref> to backpropagate through categorical latent actions.</p><p>Lite ELBO: a major limitation is that Full ELBO can suffer from exposure bias at latent space, i.e. the decoder only sees z sampled from q(z|x, c) and never experiences z sampled from p θ (z|c), which is always used at testing time. Therefore, in this paper, we propose a simplified ELBO for encoder-decoder models with stochastic latent variables:</p><formula xml:id="formula_9">L lite (θ) = p p(z|c) (x|z) -βD KL [p(z|c)) p(z)]<label>(15)</label></formula><p>Essentially this simplified objective sets the posterior network the same as our encoder, i.e. q γ (z|x, c) = p θe (z|c), which makes the KL term in Eq 14 zero and removes the issue of exposure bias. But this leaves the latent spaces unregularized and our experiments show that if we only maximize p p(z|c) (x|z) there is overfitting.</p><p>For this, we add the additional regularization term βD KL [p(z|c)) p(z)] that encourages the posterior be similar to certain prior distributions and β is a hyper-parameter between 0 and 1. We set the p(z) for categorical latent actions to be uniform, i.e. p(z) = 1/K, and set the prior for Gaussian latent actions to be N (0, I), which we will show that are effective.</p><p>5 Experiment Settings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DealOrNoDeal Corpus and RL Setup</head><p>DealOrNoDeal is a negotiation dataset that contains 5805 dialogs based on 2236 unique scenarios <ref type="bibr" target="#b16">(Lewis et al., 2017)</ref>. We hold out 252 scenarios for testing environment and randomly sample 400 scenarios from the training set for validation. The results are evaluated from 4 perspectives: Perplexity (PPL), Reward, Agree and Diversity. PPL helps us to identify which model produces the most human-like responses, while Reward and Agree evaluate the model's negotiation strength. Diversity indicates whether the model discovers a novel discourse-level strategy or just repeats dull responses to compromise with the opponent. We closely follow the original paper and use the same reward function and baseline calculation. At last, to have a fair comparison, all the compared models shared the identical judge model and user simulator, which are a standard hierarchical encoder-decoder model trained with Maximum Likelihood Estimation (MLE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-Woz Corpus and Novel RL Setup</head><p>Multi-Woz is a slot-filling dataset that contains 10438 dialogs on 6 different domains. 8438 dialogs are for training and 1000 each are for validation and testing. Since no prior user simulator exists for this dataset, for a fair comparison with the previous state-of-the-art we focus on the Dialog-Context-to-Text Generation task proposed in <ref type="bibr" target="#b0">(Budzianowski et al., 2018)</ref>. This task assumes that the model has access to the groundtruth dialog belief state and is asked to generate the next response at every system turn in a di-alog. The results are evaluated from 3 perspectives: BLEU, Inform Rate and Success Rate. The BLEU score checks the response-level lexical similarity, while Inform and Success Rate measure whether the model gives recommendations and provides all the requested information at dialoglevel. Current state-of-the-art results struggle in this task and MLE models only achieve 60% success <ref type="bibr" target="#b0">(Budzianowski et al., 2018)</ref>. To transform this task into an RL task, we propose a novel extension to the original task as follows:</p><p>1. For each RL episode, randomly sample a dialog from the training set 2. Run the model on every system turn, and do not alter the original dialog context at every turn given the generated responses.</p><p>3. Compute Success Rate based on the generated responses in this dialog.</p><p>4. Compute policy gradient using Eq 3 and update the parameters.</p><p>This setup creates a variant RL problem that is similar to the Contextual Bandits <ref type="bibr" target="#b14">(Langford and Zhang, 2008)</ref>, where the goal is to adjust its parameters to generate responses that yield better Success Rate. Our results show that this problem is challenging and that word-level RL falls short.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Language Constrained Reward (LCR) curve for Evaluation</head><p>It is challenging to quantify the performance of RL-based neural generation systems because it is possible for a model to achieve high task reward and yet not generate human language <ref type="bibr" target="#b4">(Das et al., 2017)</ref>. Therefore, we propose a novel measure, the Language Constrained Reward (LCR) curve as an additional robust measure. The basic idea is to use an ROC-style curve to visualize the tradeoff between achieving higher reward and being faithful to human language. Specifically, at each checkpoint i over the course of RL training, we record two measures: (1) the PPL of a given model on the test data p i = PPL(θ i ) and ( <ref type="formula">2</ref>) this model's average cumulative task reward in the test environment R t i . After RL training is complete, we create a 2D plot where the x-axis is the maximum PPL allowed, and the y-axis is the best achievable reward within the PPL budget in the testing environments:</p><formula xml:id="formula_10">y = max i R t i subject to p i &lt; x (16)</formula><p>As a result, a perfect model should lie in the upper left corner whereas a model that sacrifices language quality for higher reward will lie in the lower right corner. Our results will show that the LCR curve is an informative and robust measure for model comparison.</p><p>6 Results: Latent Actions or Words?</p><p>We have created 6 different variations of latent action dialog models under our LaRL framework. the RL training step, we set RL:SL=off for all latent action models, while the baseline word-level RL models are free to tune RL:SL for best performance. For latent variable models, their perplexity is estimated via Monte Carlo p(x|c) ≈ E p(z|c) [p(x|z)p(z|c)]. For the sake of clarity, this only compares the best performing latent action models to the best performing word-level models and focuses on the differences between them. A detailed comparison of the 6 latent space configurations is addressed in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">DealOrNoDeal</head><p>The baseline system is a hierarchical recurrent encoder-decoder (HRED) model <ref type="bibr" target="#b24">(Serban et al., 2016)</ref> that is tuned to reproduce results from <ref type="bibr" target="#b16">(Lewis et al., 2017)</ref>. Word-level RL is then used to fine-tune the pre-trained model with RL:SL=4:1. On the other hand, the best performing latent action model is LiteCat. Best models are chosen based on performance on the validation environment.</p><p>The results are summarized in Table <ref type="table" target="#tab_1">2</ref> and Figure <ref type="figure" target="#fig_1">2</ref> shows the LCR curves for the baseline with the two best models plus LiteAttnCat and baseline without RL:SL. From Table <ref type="table" target="#tab_1">2</ref>, it appears that the word-level RL baseline performs better than Lite-Cat in terms of rewards. However, Figure <ref type="figure" target="#fig_1">2</ref> shows that the two LaRL models achieve strong task rewards with a much smaller performance drop in language quality (PPL), whereas the word-level model can only increase its task rewards by deviating significantly from natural language. Closer analysis shows the word-level baseline severely overfits to the user simulator. The caveat is that the word-level models have in fact discovered a loophole in the simulator by insisting on 'hat' and 'ball' several times and the user model eventually yields to agree to the deal. This is reflected in the diversity measure, which is the number of unique responses that a model uses in all 200 testing scenarios. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, after RL training, the diversity of the baseline model drops to only 5. It is surprising that the agent can achieve high reward with a well-trained HRED user simulator using only 5 unique utterances. On the contrary, LiteCat increases its response diversity after RL training from 58 to 202, suggesting that LiteCat discovers novel discourse-level strategies in order to win the negotiation instead of exploiting local loopholes in the same user simulator. Our qualitative analysis confirms this when we observe that our LiteCat model is able to use multiple strategies in negotiation, e.g. elicit preference question, request different offers, insist on key objects etc. See supplementary material for example conversations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">MultiWoz</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For</head><p>MultiWoz, we reproduce results from <ref type="bibr" target="#b0">(Budzianowski et al., 2018)</ref> as the baseline. After RL training, the best LaRL model is LiteAt-tnCat and the best word-level model is word RL:SL=off.   <ref type="bibr" target="#b0">(Budzianowski et al., 2018)</ref>. More importantly, perplexity only slightly increases from 4.05 to 5.22. On the other hand, the word-level RL's success rate also improves to 79%, but the generated responses completely deviate from natural language, increasing perplexity from 3.98 to 17.11 and dropping BLEU from 18.9 to 1.4. Figure <ref type="figure" target="#fig_3">4</ref> shows the LCR curves for MultiWoz, with a trend similar to the previous section: the word-level models can only achieve task reward improvement by sacrificing their response decoder PPL. Figure <ref type="figure" target="#fig_3">4</ref> also shows the LCR curve for the baseline trained with RL:SL=100:1, hoping that supervised learning can force the model to conform to natural language. While PPL and BLEU are indeed improved, it also limits final reward performance. The latent-level models, on the contrary, do not suffer from this tradeoff. We also observe that LiteAttnCat consistently outperforms LiteCat on MultiWoz, confirming the effectiveness of Attention Fusion for handling long dialog responses with multiple entities and dialog acts. Lastly, Table <ref type="table" target="#tab_5">4</ref> qualitatively exhibits the generation differences between the two approaches. The RL:SL=off model learns to continuously output entities to fool the evaluation script for high success rate, whereas LiteCatAttn learns to give more information while maintaining the language quality.  We compare the 6 variants of latent action models on DealOrNoDeal and MultiWoz.  shows performance of the models that are pretrained only with supervised learning. Figure <ref type="figure" target="#fig_4">5</ref> shows LCR curves for the 3 models pre-trained with L lite and fine-tuned with policy gradient reinforcement learning. The following are the main findings based on these results. L lite outperforms L f ull as a pre-train objective. Table <ref type="table" target="#tab_6">5</ref> shows that models with L f ull fall behind their Lite counterparts on PPL and BLEU. We attribute this to the exposure bias in the latent space, i.e. the decoder is not trained to consider the discrepancy between the posterior network and actual dialog policy network. Meanwhile, the full models tend to enjoy higher diversity at pre-training, which agrees with the diversity-promoting effect observed in prior research <ref type="bibr">(Zhao et al., 2017)</ref>. However, our previous discussion on Figure <ref type="figure" target="#fig_2">3</ref> shows that Lite models are able to increase their response diversity in order to win more in negotiation through RL training. This is fundamentally different from diversity in pretraining, since diversity in LaRL is optimized to improve task reward, rather than to better model the original data distribution. importance of latent space regularization. When β is 0, both LiteCat and LiteGauss reach suboptimal policies with final reward that are much smaller than the regularized versions (β = 0.01). The reason behind this is that the unregularized pretrained policy has very low entropy, which prohibits sufficient exploration in the RL stage.</p><p>Categorical latent actions outperform Gaussian latent actions. Models with discrete actions consistently outperform models with Gaussian ones. This is surprising since continuously distributed representations are a key reason for the success of deep learning in natural language processing. Our finding suggests that (1) multivariate categorical distributions are powerful enough to model complex natural dialog responses semantics, and can achieve on par results with Gaussian or non-stochastic continuous representations. (2) categorical variables are a better choice to serve as action spaces for reinforcement learning. Figure <ref type="figure" target="#fig_4">5</ref> shows that Lite(Attn)Cat easily achieves strong rewards while LiteGauss struggles to improve its reward. Also, applying REINFORCE on Gaussian latent actions is unstable and often leads to model divergence. We suspect the reason for this is the unbounded nature of continuous latent space: RL exploration in the continuous space may lead to areas in the manifold that are not covered in supervised training, which causes undefined decoder behavior given z in these unknown areas.</p><p>In conclusion, this paper proposes a latent variable action space for RL in E2E dialog agents. We present a general framework with a regularized ELBO objective and attention fusion for discrete variables. The methods are assessed on two dialog tasks and analyzed using the proposed LCR curve. Results show our models achieve superior performance and create a new state-of-the-art success rate on MultiWoz. Extensive analyses enable us to gain insight on how to properly train latent variables that can serve as the action spaces for dialog agents. This work is situated in the approach concerning practical latent variables in dialog agents, being able to create action abstraction in an unsupervised manner. We believe that our findings are a basic first step in this promising research direction. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: High-level comparison between word-level and latent-action reinforcement learning in a sample multiturn dialog. The green boxes denote the decoder network used to generate the response given the latent code z. Dashed line denotes places where policy gradients from task rewards are applied to the model.</figDesc><graphic coords="3,72.00,62.81,453.53,71.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: LCR curves on DealOrNoDeal dataset.</figDesc><graphic coords="6,314.36,259.39,204.10,143.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Response diversity and task reward learning curve over the course of RL training for both word RL:SL=4:1 (left) and LiteCat (right).</figDesc><graphic coords="7,113.10,153.77,136.06,89.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: LCR curves on the MultiWoz dataset.</figDesc><graphic coords="7,307.28,94.21,222.24,100.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: LCR curves on DealOrNoDeal and Multi-Woz. Models with L f ull are not included because their PPLs are too poor to compare to the Lite models.</figDesc><graphic coords="8,113.10,559.60,136.07,91.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>To demonstrate the advantages of LaRL, during All proposed variations of LaRL models.</figDesc><table><row><cell>Model</cell><cell>Var Type</cell><cell>Loss</cell><cell>Integration</cell></row><row><cell>Gauss</cell><cell>Gaussian</cell><cell>L f ull</cell><cell>/</cell></row><row><cell>Cat</cell><cell>Categorical</cell><cell>L f ull</cell><cell>sum</cell></row><row><cell>AttnCat</cell><cell>Categorical</cell><cell>L f ull</cell><cell>attn</cell></row><row><cell>LiteGauss</cell><cell>Gaussian</cell><cell>L lite</cell><cell>/</cell></row><row><cell>LiteCat</cell><cell>Categorical</cell><cell>L lite</cell><cell>sum</cell></row><row><cell>LiteAttnCat</cell><cell>Categorical</cell><cell>L lite</cell><cell>attn</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on DealOrNoDeal. Diversity is measured by the number of unique responses the model used in all scenarios from the test data.</figDesc><table><row><cell></cell><cell cols="3">PPL Reward Agree% Diversity</cell></row><row><cell cols="2">Baseline 5.23 3.75</cell><cell>59</cell><cell>109</cell></row><row><cell>LiteCat</cell><cell>5.35 2.65</cell><cell>41</cell><cell>58</cell></row><row><cell>Baseline</cell><cell>8.23 7.61</cell><cell>86</cell><cell>5</cell></row><row><cell>+RL</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LiteCat</cell><cell>6.14 7.27</cell><cell>87</cell><cell>202</cell></row><row><cell>+RL</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table 3 shows that LiteAttnCat is on</figDesc><table><row><cell></cell><cell cols="4">PPL BLEU Inform Success</cell></row><row><cell>Human</cell><cell>/</cell><cell>/</cell><cell>90%</cell><cell>82.3%</cell></row><row><cell>Baseline</cell><cell cols="4">3.98 18.9 71.33% 60.96%</cell></row><row><cell cols="5">LiteAttnCat 4.05 19.1 67.98% 57.36%</cell></row><row><cell>Baseline</cell><cell cols="2">17.11 1.4</cell><cell cols="2">80.5% 79.07%</cell></row><row><cell>+RL</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LiteAttnCat</cell><cell cols="4">5.22 12.8 82.78% 79.2%</cell></row><row><cell>+RL</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Main results on MultiWoz test set. RL models are chosen based on performance on the validation set.</figDesc><table><row><cell>par with the baseline in the supervised learning</cell></row><row><cell>step, showing that multivariate categorical latent</cell></row><row><cell>variables alone are powerful enough to match with</cell></row><row><cell>continuous hidden representations for modeling</cell></row><row><cell>dialog actions. For performance after RL training,</cell></row><row><cell>LiteAttnCat achieves near-human performance</cell></row><row><cell>in terms of success rate and inform rate, ob-</cell></row><row><cell>taining 18.24% absolute improvement over the</cell></row><row><cell>MLE-based state-of-the-art</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Example responses from baselines and Lite-CatAttn on MultiWoz.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>Deal</cell><cell cols="3">PPL Reward Agree% Diversity</cell></row><row><cell>Baseline</cell><cell>3.23 3.75</cell><cell>59</cell><cell>109</cell></row><row><cell>Gauss</cell><cell>110K 2.71</cell><cell>43</cell><cell>176</cell></row><row><cell>LiteGauss</cell><cell>5.35 4.48</cell><cell>65</cell><cell>91</cell></row><row><cell>Cat</cell><cell>80.41 3.9</cell><cell>62</cell><cell>115</cell></row><row><cell>AttnCat</cell><cell>118.3 3.23</cell><cell>51</cell><cell>145</cell></row><row><cell>LiteCat</cell><cell>5.35 2.67</cell><cell>41</cell><cell>58</cell></row><row><cell cols="2">LiteAttnCat 5.25 3.69</cell><cell>52</cell><cell>75</cell></row><row><cell cols="4">MultiWoz PPL BLEU Inform% Succ%</cell></row><row><cell>Baseline</cell><cell>3.98 18.9</cell><cell>71.33</cell><cell>60.96</cell></row><row><cell>Gauss</cell><cell>712.3 7.54</cell><cell>60.5</cell><cell>23.0</cell></row><row><cell>LiteGauss</cell><cell>4.06 19.3</cell><cell>56.46</cell><cell>48.06</cell></row><row><cell>Cat</cell><cell>7.07 13.7</cell><cell>54.15</cell><cell>42.04</cell></row><row><cell>AttnCat</cell><cell>12.01 12.6</cell><cell>63.9</cell><cell>45.8</cell></row><row><cell>LiteCat</cell><cell>4.10 19.1</cell><cell>61.56</cell><cell>49.15</cell></row><row><cell cols="2">LiteAttnCat 4.05 19.1</cell><cell>67.97</cell><cell>57.36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note><p>Comparison of 6 model variants with only supervised learning training.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>shows the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Best rewards in test environments on DealOrNoDeal with various β.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Tiancheng Zhao and Maxine Eskenazi. 2016. Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning. In 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue, page 1. Tiancheng Zhao and Maxine Eskenazi. 2018. Zeroshot dialog generation with cross-domain latent actions. In Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue, pages 1-10. Tiancheng Zhao, Kyusong Lee, and Maxine Eskenazi. 2018. Unsupervised discrete sentence representation learning for interpretable neural dialog generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1. Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi. 2017. Learning discourse-level diversity for neural dialog models using conditional variational autoencoders. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 654-664.</figDesc><table><row><cell>A Supplemental Material</cell></row><row><cell>A.1 Training Details</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Data and code are available at https://github. com/snakeztc/NeuralDialog-LaRL</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The following hyperparameters are used for the results on DealOrNoDeal.   <ref type="bibr" target="#b36">(Yang et al., 2016)</ref> The following hyperparameters are used for the results on MultiWoz. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 DealOrNoDeal Example Conversations</head><p>The followings are examples dialogs generated from word-level models (Table <ref type="table">9</ref>) and latent-level models (Table <ref type="table">10</ref>).  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiwoz-a largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling</title>
		<author>
			<persName><forename type="first">Paweł</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-Hsiang</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iñigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Osman Ramadan</surname></persName>
		</author>
		<author>
			<persName><surname>Gasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5016" to="5026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent variable dialogue models and their diversity</title>
		<author>
			<persName><forename type="first">Kris</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<title level="s">Short Papers</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="182" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised induction and filling of semantic slots for spoken dialogue systems using frame-semantic parsing</title>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">I</forename><surname>Rudnicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="120" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning cooperative visual dialog agents with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards end-to-end reinforcement learning of dialogue agents for information access</title>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="484" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gaussian processes for pomdp-based dialogue manager optimization</title>
		<author>
			<persName><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="40" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variance reduction techniques for gradient estimates in reinforcement learning</title>
		<author>
			<persName><forename type="first">Evan</forename><surname>Greensmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1471" to="1530" />
			<date type="published" when="2004-11">2004. Nov</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decoupling strategy and generation in negotiation dialogues</title>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anusha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2333" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Word-based dialog state tracking with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</title>
		<meeting>the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="292" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reinforcement learning: A survey</title>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Pack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaelbling</forename><surname>Michael L Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="237" to="285" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Natural language does not emerge naturallyin multi-agent dialog</title>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2962" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The epochgreedy algorithm for multi-armed bandits with side information</title>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structured discriminative model for dialog state tracking</title>
		<author>
			<persName><forename type="first">Sungjin</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIG-DIAL 2013 Conference</title>
		<meeting>the SIG-DIAL 2013 Conference</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="442" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deal or no deal? end-to-end learning of negotiation dialogues</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2443" to="2453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network model with belief tracking for task-oriented dialog</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2506" to="2510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04908</idno>
		<title level="m">Emergence of grounded compositional language in multi-agent populations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lets go public! taking a spoken dialog system to the real world</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Raux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Langner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Bohus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards universal dialogue state tracking</title>
		<author>
			<persName><forename type="first">Liliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaige</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2780" to="2786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Chinnadhurai</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesup</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Pieper</surname></persName>
		</author>
		<author>
			<persName><surname>Chandar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02349</idno>
		<title level="m">A deep reinforcement learning chatbot</title>
		<meeting><address><addrLine>Nan Rosemary Ke</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16)</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence (AAAI-16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter</title>
		<meeting>the 2015 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sample-efficient actor-critic reinforcement learning with supervised data for dialogue management</title>
		<author>
			<persName><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paweł</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="147" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A neural conversational model</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email</title>
		<author>
			<persName><forename type="first">Marilyn</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="387" to="416" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04562</idno>
		<title level="m">A networkbased end-to-end trainable task-oriented dialogue system</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Latent intention dialogue models</title>
		<author>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3732" to="3741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning</title>
		<author>
			<persName><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kavosh</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="665" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Partially observable markov decision processes for spoken dialog systems</title>
		<author>
			<persName><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="393" to="422" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Endto-end lstm-based dialog control optimized with supervised and reinforcement learning</title>
		<author>
			<persName><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01269</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Williams</forename><surname>Ronald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Hierarchical text generation and planning for strategic dialogue</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05846</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The hidden information state approach to dialog management</title>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Weilhammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">149</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Gašić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Using pomdps for dialog management</title>
		<author>
			<persName><forename type="first">Steve J</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SLT</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="8" to="13" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
