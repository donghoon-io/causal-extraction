<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tensor Decompositions for Learning Latent Variable Models</title>
				<funder ref="#_p2CQsjn">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_b7cxTFy">
					<orgName type="full">AFOSR</orgName>
				</funder>
				<funder ref="#_9HaRtJx">
					<orgName type="full">ARO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
							<email>a.anandkumar@uci.edu</email>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
							<email>djhsu@cs.columbia.edu</email>
						</author>
						<author>
							<persName><forename type="first">Matus</forename><surname>Telgarsky</surname></persName>
							<email>mtelgars@cs.ucsd.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Irvine 2200 Engineering Hall Irvine</addrLine>
									<postCode>92697</postCode>
									<settlement>Rong Ge</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Microsoft Research One Memorial Drive Cambridge</orgName>
								<address>
									<postCode>02142</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<addrLine>1214 Amsterdam Avenue</addrLine>
									<postCode>0401 10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country>#</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Statistics Rutgers University</orgName>
								<orgName type="institution">Microsoft Research One Memorial Drive Cambridge</orgName>
								<address>
									<addrLine>110 Frelinghuysen Road Piscataway</addrLine>
									<postCode>02142 08854</postCode>
									<region>MA NJ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tensor Decompositions for Learning Latent Variable Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Submitted 2/13; Revised 3/14;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Anandkumar</term>
					<term>Ge</term>
					<term>Hsu</term>
					<term>Kakade</term>
					<term>and Telgarsky latent variable models</term>
					<term>tensor decompositions</term>
					<term>mixture models</term>
					<term>topic models</term>
					<term>method of moments</term>
					<term>power method</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models-including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation-which exploits a certain tensor structure in their low-order observable moments (typically, of second-and third-order). Specifically, parameter estimation is reduced to the problem of extracting a certain (orthogonal) decomposition of a symmetric tensor derived from the moments; this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices. Although tensor decompositions are generally intractable to compute, the decomposition of these specially structured tensors can be efficiently obtained by a variety of approaches, including power iterations and maximization approaches (similar to the case of matrices). A detailed analysis of a robust tensor power method is provided, establishing an analogue of Wedin's perturbation theorem for the singular vectors of matrices. This implies a robust and computationally tractable estimation approach for several popular latent variable models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The method of moments is a classical parameter estimation technique <ref type="bibr" target="#b72">(Pearson, 1894)</ref> from statistics which has proved invaluable in a number of application domains. The basic paradigm is simple and intuitive: (i) compute certain statistics of the data-often empirical moments such as means and correlations-and (ii) find model parameters that give rise to (nearly) the same corresponding population quantities. In a number of cases, the method of moments leads to consistent estimators which can be efficiently computed; this is especially relevant in the context of latent variable models, where standard maximum likelihood approaches are typically computationally prohibitive, and heuristic methods can be unreliable and difficult to validate with high-dimensional data. Furthermore, the method of moments can be viewed as complementary to the maximum likelihood approach; simply taking a single step of Newton-Raphson on the likelihood function starting from the moment based estimator <ref type="bibr" target="#b59">(Le Cam, 1986)</ref> often leads to the best of both worlds: a computationally efficient estimator that is (asymptotically) statistically optimal.</p><p>The primary difficulty in learning latent variable models is that the latent (hidden) state of the data is not directly observed; rather only observed variables correlated with the hidden state are observed. As such, it is not evident the method of moments should fare any better than maximum likelihood in terms of computational performance: matching the model parameters to the observed moments may involve solving computationally intractable systems of multivariate polynomial equations. Fortunately, for many classes of latent variable models, there is rich structure in low-order moments (typically second-and third-order) which allow for this inverse moment problem to be solved efficiently <ref type="bibr" target="#b23">(Cattell, 1944;</ref><ref type="bibr" target="#b18">Cardoso, 1991;</ref><ref type="bibr" target="#b24">Chang, 1996;</ref><ref type="bibr" target="#b66">Mossel and Roch, 2006;</ref><ref type="bibr">Hsu et al., 2012b;</ref><ref type="bibr">Anandkumar et al., 2012c,a;</ref><ref type="bibr" target="#b46">Hsu and Kakade, 2013)</ref>. What is more is that these decomposition problems are often amenable to simple and efficient iterative methods, such as gradient descent and the power iteration method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>In this work, we observe that a number of important and well-studied latent variable models-including Gaussian mixture models, hidden Markov models, and Latent Dirichlet allocation-share a certain structure in their low-order moments, and this permits certain tensor decomposition approaches to parameter estimation. In particular, this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices.</p><p>While much of this (or similar) structure was implicit in several previous works <ref type="bibr" target="#b24">(Chang, 1996;</ref><ref type="bibr" target="#b66">Mossel and Roch, 2006;</ref><ref type="bibr">Hsu et al., 2012b;</ref><ref type="bibr">Anandkumar et al., 2012c,a;</ref><ref type="bibr" target="#b46">Hsu and Kakade, 2013)</ref>, here we make the decomposition explicit under a unified framework. Specifically, we express the observable moments as sums of rank-one terms, and reduce the parameter estimation task to the problem of extracting a symmetric orthogonal decomposition of a symmetric tensor derived from these observable moments. The problem can then be solved by a variety of approaches, including fixed-point and variational methods.</p><p>One approach for obtaining the orthogonal decomposition is the tensor power method of <ref type="bibr">Lathauwer et al. (2000, Remark 3</ref>). We provide a convergence analysis of this method for orthogonally decomposable symmetric tensors, as well as a detailed perturbation analysis for a robust (and a computationally tractable) variant <ref type="bibr">(Theorem 5.1)</ref>. This perturbation analysis can be viewed as an analogue of Wedin's perturbation theorem for singular vectors of matrices <ref type="bibr" target="#b84">(Wedin, 1972)</ref>, providing a bound on the error of the recovered decomposition in terms of the operator norm of the tensor perturbation. This analysis is subtle in at least two ways. First, unlike for matrices (where every matrix has a singular value decomposition), an orthogonal decomposition need not exist for the perturbed tensor. Our robust variant uses random restarts and deflation to extract an approximate decomposition in a computationally tractable manner. Second, the analysis of the deflation steps is non-trivial; a naïve argument would entail error accumulation in each deflation step, which we show can in fact be avoided. When this method is applied for parameter estimation in latent variable models previously discussed, improved sample complexity bounds (over previous work) can be obtained using this perturbation analysis.</p><p>Finally, we also address computational issues that arise when applying the tensor decomposition approaches to estimating latent variable models. Specifically, we show that the basic operations of simple iterative approaches (such as the tensor power method) can be efficiently executed in time linear in the dimension of the observations and the size of the training data. For instance, in a topic modeling application, the proposed methods require time linear in the number of words in the vocabulary and in the number of non-zero entries of the term-document matrix. The combination of this computational efficiency and the robustness of the tensor decomposition techniques makes the overall framework a promising approach to parameter estimation for latent variable models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>The connection between tensor decompositions and latent variable models has a long history across many scientific and mathematical disciplines. We review some of the key works that are most closely related to ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.1">Tensor Decompositions</head><p>The role of tensor decompositions in the context of latent variable models dates back to early uses in psychometrics <ref type="bibr" target="#b23">(Cattell, 1944)</ref>. These ideas later gained popularity in chemometrics, and more recently in numerous science and engineering disciplines, including neuroscience, phylogenetics, signal processing, data mining, and computer vision. A thorough survey of these techniques and applications is given by <ref type="bibr" target="#b55">Kolda and Bader (2009)</ref>. Below, we discuss a few specific connections to two applications in machine learning and statistics, independent component analysis and latent variable models (between which there is also significant overlap).</p><p>Tensor decompositions have been used in signal processing and computational neuroscience for blind source separation and independent component analysis (ICA) <ref type="bibr" target="#b28">(Comon and Jutten, 2010)</ref>. Here, statistically independent non-Gaussian sources are linearly mixed in the observed signal, and the goal is to recover the mixing matrix (and ultimately, the original source signals). A typical solution is to locate projections of the observed signals that correspond to local extrema of the so-called "contrast functions" which distinguish Gaussian variables from non-Gaussian variables. This method can be effectively implemented using fast descent algorithms <ref type="bibr" target="#b49">(Hyvarinen, 1999)</ref>. When using the excess kurtosis (i.e., fourth-order cumulant) as the contrast function, this method reduces to a generalization of the power method for symmetric tensors <ref type="bibr" target="#b58">(Lathauwer et al., 2000;</ref><ref type="bibr" target="#b85">Zhang and Golub, 2001;</ref><ref type="bibr" target="#b54">Kofidis and Regalia, 2002)</ref>. This case is particularly important, since all local extrema of the kurtosis objective correspond to the true sources (under the assumed statistical model) <ref type="bibr" target="#b34">(Delfosse and Loubaton, 1995)</ref>; the descent methods can therefore be rigorously analyzed, and their computational and statistical complexity can be bounded <ref type="bibr" target="#b39">(Frieze et al., 1996;</ref><ref type="bibr" target="#b67">Nguyen and Regev, 2009;</ref><ref type="bibr">Arora et al., 2012b)</ref>.</p><p>Higher-order tensor decompositions have also been used to develop estimators for commonly used mixture models, hidden Markov models, and other related latent variable models, often using the the algebraic procedure of R. Jennrich (as reported in the article of <ref type="bibr" target="#b42">Harshman, 1970)</ref>, which is based on a simultaneous diagonalization of different ways of flattening a tensor to matrices. Jennrich's procedure was employed for parameter estimation of discrete Markov models by <ref type="bibr" target="#b24">Chang (1996)</ref> via pair-wise and triple-wise probability tables; and it was later used for other latent variable models such as hidden Markov models (HMMs), latent trees, Gaussian mixture models, and topic models such as latent Dirichlet allocation (LDA) by many others <ref type="bibr" target="#b66">(Mossel and Roch, 2006;</ref><ref type="bibr">Hsu et al., 2012b;</ref><ref type="bibr">Anandkumar et al., 2012c,a;</ref><ref type="bibr" target="#b46">Hsu and Kakade, 2013)</ref>. In these contexts, it is often also possible to establish strong identifiability results, without giving an explicit estimators, by invoking the non-constructive identifiability argument of <ref type="bibr" target="#b57">Kruskal (1977)</ref>-see the article by <ref type="bibr" target="#b1">Allman et al. (2009)</ref> for several examples.</p><p>Related simultaneous diagonalization approaches have also been used for blind source separation and ICA (as discussed above), and a number of efficient algorithms have been developed for this problem <ref type="bibr" target="#b17">(Bunse-Gerstner et al., 1993;</ref><ref type="bibr" target="#b21">Cardoso and Souloumiac, 1993;</ref><ref type="bibr" target="#b19">Cardoso, 1994;</ref><ref type="bibr" target="#b20">Cardoso and Comon, 1996;</ref><ref type="bibr" target="#b30">Corless et al., 1997;</ref><ref type="bibr" target="#b86">Ziehe et al., 2004)</ref>. A rather different technique that uses tensor flattening and matrix eigenvalue decomposition has been developed by <ref type="bibr" target="#b18">Cardoso (1991)</ref> and later by <ref type="bibr" target="#b33">De Lathauwer et al. (2007)</ref>. A significant advantage of this technique is that it can be used to estimate overcomplete mixtures, where the number of sources is larger than the observed dimension.</p><p>The relevance of tensor analysis to latent variable modeling has been long recognized in the field of algebraic statistics <ref type="bibr" target="#b70">(Pachter and Sturmfels, 2005)</ref>, and many works characterize the algebraic varieties corresponding to the moments of various classes of latent variable models <ref type="bibr" target="#b37">(Drton et al., 2007;</ref><ref type="bibr" target="#b82">Sturmfels and Zwiernik, 2013)</ref>. These works typically do not address computational or finite sample issues, but rather are concerned with basic questions of identifiability.</p><p>The specific tensor structure considered in the present work is the symmetric orthogonal decomposition. This decomposition expresses a tensor as a linear combination of simple tensor forms; each form is the tensor product of a vector (i.e., a rank-1 tensor), and the collection of vectors form an orthonormal basis. An important property of tensors with such decompositions is that they have eigenvectors corresponding to these basis vectors. Although the concepts of eigenvalues and eigenvectors of tensors is generally significantly more complicated than their matrix counterpart-both algebraically <ref type="bibr" target="#b73">(Qi, 2005;</ref><ref type="bibr" target="#b22">Cartwright and Sturmfels, 2013;</ref><ref type="bibr" target="#b60">Lim, 2005)</ref> and computationally <ref type="bibr" target="#b43">(Hillar and Lim, 2013;</ref><ref type="bibr" target="#b54">Kofidis and Regalia, 2002)</ref>-the special symmetric orthogonal structure we consider permits simple algorithms to efficiently and stably recover the desired decomposition. In particular, a generalization of the matrix power method to symmetric tensors, introduced by <ref type="bibr">Lathauwer et al. (2000, Remark 3</ref>) and analyzed by <ref type="bibr" target="#b54">Kofidis and Regalia (2002)</ref>, provides such a decomposition. This is in fact implied by the characterization of <ref type="bibr" target="#b85">Zhang and Golub (2001)</ref>, which shows that iteratively obtaining the best rank-1 approximation of such orthogonally decomposable tensors also yields the exact decomposition. We note that in general, obtaining such approximations for general (symmetric) tensors is NP-hard <ref type="bibr" target="#b43">(Hillar and Lim, 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.2">Latent Variable Models</head><p>This work focuses on the particular application of tensor decomposition methods to estimating latent variable models, a significant departure from many previous approaches in the machine learning and statistics literature. By far the most popular heuristic for parameter estimation for such models is the Expectation-Maximization (EM) algorithm <ref type="bibr" target="#b35">(Dempster et al., 1977;</ref><ref type="bibr" target="#b74">Redner and Walker, 1984)</ref>. Although EM has a number of merits, it may suffer from slow convergence and poor quality local optima <ref type="bibr" target="#b74">(Redner and Walker, 1984)</ref>, requiring practitioners to employ many additional heuristics to obtain good solutions. For some models such as latent trees <ref type="bibr" target="#b76">(Roch, 2006)</ref> and topic models <ref type="bibr">(Arora et al., 2012a)</ref>, maximum likelihood estimation is NP-hard, which suggests that other estimation approaches may be more attractive. More recently, algorithms from theoretical computer science and machine learning have addressed computational and sample complexity issues related to estimating certain latent variable models such as Gaussian mixture models and HMMs <ref type="bibr" target="#b31">(Dasgupta, 1999;</ref><ref type="bibr" target="#b6">Arora and Kannan, 2005;</ref><ref type="bibr" target="#b32">Dasgupta and Schulman, 2007;</ref><ref type="bibr" target="#b83">Vempala and Wang, 2004;</ref><ref type="bibr" target="#b53">Kannan et al., 2008;</ref><ref type="bibr" target="#b0">Achlioptas and McSherry, 2005;</ref><ref type="bibr" target="#b25">Chaudhuri and Rao, 2008;</ref><ref type="bibr" target="#b16">Brubaker and Vempala, 2008;</ref><ref type="bibr" target="#b52">Kalai et al., 2010;</ref><ref type="bibr" target="#b13">Belkin and Sinha, 2010;</ref><ref type="bibr" target="#b65">Moitra and Valiant, 2010;</ref><ref type="bibr" target="#b46">Hsu and Kakade, 2013;</ref><ref type="bibr" target="#b24">Chang, 1996;</ref><ref type="bibr" target="#b66">Mossel and Roch, 2006;</ref><ref type="bibr">Hsu et al., 2012b;</ref><ref type="bibr">Anandkumar et al., 2012c;</ref><ref type="bibr">Arora et al., 2012a;</ref><ref type="bibr">Anandkumar et al., 2012a)</ref>. See the works by <ref type="bibr">Anandkumar et al. (2012c)</ref> and <ref type="bibr" target="#b46">Hsu and Kakade (2013)</ref> for a discussion of these methods, together with the computational and statistical hardness barriers that they face. The present work reviews a broad range of latent variables where a mild non-degeneracy condition implies the symmetric orthogonal decomposition structure in the tensors of low-order observable moments.</p><p>Notably, another class of methods, based on subspace identification <ref type="bibr" target="#b69">(Overschee and Moor, 1996)</ref> and observable operator models/multiplicity automata <ref type="bibr" target="#b78">(Schützenberger, 1961;</ref><ref type="bibr" target="#b51">Jaeger, 2000;</ref><ref type="bibr" target="#b61">Littman et al., 2001)</ref>, have been proposed for a number of latent variable models. These methods were successfully developed for HMMs by <ref type="bibr">Hsu et al. (2012b)</ref>, and subsequently generalized and extended for a number of related sequential and tree Markov models models <ref type="bibr" target="#b79">(Siddiqi et al., 2010;</ref><ref type="bibr" target="#b10">Bailly, 2011;</ref><ref type="bibr" target="#b15">Boots et al., 2010;</ref><ref type="bibr" target="#b71">Parikh et al., 2011;</ref><ref type="bibr" target="#b77">Rodu et al., 2013;</ref><ref type="bibr">Balle et al., 2012;</ref><ref type="bibr" target="#b11">Balle and Mohri, 2012)</ref>, as well as certain classes of parse tree models <ref type="bibr" target="#b62">(Luque et al., 2012;</ref><ref type="bibr" target="#b26">Cohen et al., 2012;</ref><ref type="bibr" target="#b36">Dhillon et al., 2012)</ref>. These methods use low-order moments to learn an "operator" representation of the distribution, which can be used for density estimation and belief state updates. While finite sample bounds can be given to establish the learnability of these models <ref type="bibr">(Hsu et al., 2012b)</ref>, the algorithms do not actually give parameter estimates (e.g., of the emission or transition matrices in the case of HMMs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Organization</head><p>The rest of the paper is organized as follows. Section 2 reviews some basic definitions of tensors. Section 3 provides examples of a number of latent variable models which, after appropriate manipulations of their low order moments, share a certain natural tensor structure. Section 4 reduces the problem of parameter estimation to that of extracting a certain (symmetric orthogonal) decomposition of a tensor. We then provide a detailed analysis of a robust tensor power method and establish an analogue of Wedin's perturbation theorem for the singular vectors of matrices. The discussion in Section 6 addresses a number of practical concerns that arise when dealing with moment matrices and tensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>We introduce some tensor notations borrowed from <ref type="bibr" target="#b60">Lim (2005)</ref>. A real p-th order tensor A ∈ p i=1 R n i is a member of the tensor product of Euclidean spaces R n i , i ∈ [p]. We generally restrict to the case where</p><formula xml:id="formula_0">n 1 = n 2 = • • • = n p = n, and simply write A ∈ p R n . For a vector v ∈ R n , we use v ⊗p := v ⊗ v ⊗ • • • ⊗ v ∈ p R n to denote its p-th tensor power.</formula><p>As is the case for vectors (where p = 1) and matrices (where p = 2), we may identify a p-th order tensor with the p-way array of real numbers</p><formula xml:id="formula_1">[A i 1 ,i 2 ,...,ip : i 1 , i 2 , . . . , i p ∈ [n]], where A i 1 ,i 2 ,...,ip is the (i 1 , i 2 , . . . , i p )-th coordinate of A (with respect to a canonical basis).</formula><p>We can consider A to be a multilinear map in the following sense: for a set of matrices {V i ∈ R n×m i : i ∈ [p]}, the (i 1 , i 2 , . . . , i p )-th entry in the p-way array representation of</p><formula xml:id="formula_2">A(V 1 , V 2 , . . . , V p ) ∈ R m 1 ×m 2 ×•••×mp is [A(V 1 , V 2 , . . . , V p )] i 1 ,i 2 ,...,ip := j 1 ,j 2 ,...,jp∈[n] A j 1 ,j 2 ,...,jp [V 1 ] j 1 ,i 1 [V 2 ] j 2 ,i 2 • • • [V p ] jp,ip . Note that if A is a matrix (p = 2), then A(V 1 , V 2 ) = V 1 AV 2 .</formula><p>Similarly, for a matrix A and vector v ∈ R n , we can express Av as</p><formula xml:id="formula_3">A(I, v) = Av ∈ R n ,</formula><p>where I is the n × n identity matrix. As a final example of this notation, observe</p><formula xml:id="formula_4">A(e i 1 , e i 2 , . . . , e ip ) = A i 1 ,i 2 ,...,ip ,</formula><p>where {e 1 , e 2 , . . . , e n } is the canonical basis for R n .</p><p>Most tensors A ∈ p R n considered in this work will be symmetric (sometimes called supersymmetric), which means that their p-way array representations are invariant to permutations of the array indices: i.e., for all indices i 1 , i 2 , . . . , i p ∈ [n], A i 1 ,i 2 ,...,ip = A i π(1) ,i π(2) ,...,i π(p) for any permutation π on <ref type="bibr">[p]</ref>. It can be checked that this reduces to the usual definition of a symmetric matrix for p = 2.</p><p>The rank of a p-th order tensor A ∈ p R n is the smallest non-negative integer k such</p><formula xml:id="formula_5">that A = k j=1 u 1,j ⊗ u 2,j ⊗ • • • ⊗ u p,j for some u i,j ∈ R n , i ∈ [p], j ∈ [k],</formula><p>and the symmetric rank of a symmetric p-th order tensor A is the smallest non-negative integer k such that A = k j=1 u ⊗p j for some</p><formula xml:id="formula_6">u j ∈ R n , j ∈ [k]. 1</formula><p>The notion of rank readily reduces to the usual definition of matrix rank when p = 2, as revealed by the singular value decomposition. Similarly, for symmetric matrices, the symmetric rank is equivalent to the matrix rank as given by the spectral theorem. A decomposition into such rank-one terms is known as a canonical polyadic decomposition <ref type="bibr">(Hitchcock, 1927a,b)</ref>.</p><p>The notion of tensor (symmetric) rank is considerably more delicate than matrix (symmetric) rank. For instance, it is not clear a priori that the symmetric rank of a tensor should even be finite <ref type="bibr" target="#b29">(Comon et al., 2008)</ref>. In addition, removal of the best rank-1 approximation of a (general) tensor may increase the tensor rank of the residual <ref type="bibr" target="#b81">(Stegeman and Comon, 2010)</ref>.</p><p>Throughout, we use v = ( i v 2 i ) 1/2 to denote the Euclidean norm of a vector v, and M to denote the spectral (operator) norm of a matrix. We also use T to denote the operator norm of a tensor, which we define later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Tensor Structure in Latent Variable Models</head><p>In this section, we give several examples of latent variable models whose low-order moments can be written as symmetric tensors of low symmetric rank; some of these examples can be deduced using the techniques developed in the text by <ref type="bibr" target="#b64">McCullagh (1987)</ref>. The basic form is demonstrated in Theorem 3.1 for the first example, and the general pattern will emerge from subsequent examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Exchangeable Single Topic Models</head><p>We first consider a simple bag-of-words model for documents in which the words in the document are assumed to be exchangeable. Recall that a collection of random variables x 1 , x 2 , . . . , x are exchangeable if their joint probability distribution is invariant to permutation of the indices. The well-known De Finetti's theorem <ref type="bibr" target="#b9">(Austin, 2008)</ref> implies that such exchangeable models can be viewed as mixture models in which there is a latent variable h such that x 1 , x 2 , . . . , x are conditionally i.i.d. given h (see Figure <ref type="figure" target="#fig_0">1</ref>(a) for the corresponding graphical model) and the conditional distributions are identical at all the nodes.</p><p>In our simplified topic model for documents, the latent variable h is interpreted as the (sole) topic of a given document, and it is assumed to take only a finite number of distinct values. Let k be the number of distinct topics in the corpus, d be the number of distinct words in the vocabulary, and ≥ 3 be the number of words in each document. The generative process for a document is as follows: the document's topic is drawn according to the discrete distribution specified by the probability vector w := (w 1 , w 2 , . . . , w k ) ∈ ∆ k-1 . This is modeled as a discrete random variable h such that</p><formula xml:id="formula_7">Pr[h = j] = w j , j ∈ [k].</formula><p>Given the topic h, the document's words are drawn independently according to the discrete distribution specified by the probability vector µ h ∈ ∆ d-1 . It will be convenient to represent the words in the document by d-dimensional random vectors x 1 , x 2 , . . . , x ∈ R d . Specifically, we set</p><formula xml:id="formula_8">x t = e i if and only if the t-th word in the document is i, t ∈ [ ],</formula><p>where e 1 , e 2 , . . . e d is the standard coordinate basis for R d .</p><p>One advantage of this encoding of words is that the (cross) moments of these random vectors correspond to joint probabilities over words. For instance, observe that</p><formula xml:id="formula_9">E[x 1 ⊗ x 2 ] = 1≤i,j≤d Pr[x 1 = e i , x 2 = e j ] e i ⊗ e j = 1≤i,j≤d</formula><p>Pr[1st word = i, 2nd word = j] e i ⊗ e j , so the (i, j)-the entry of the matrix</p><formula xml:id="formula_10">E[x 1 ⊗ x 2 ] is Pr[1st word = i, 2nd word = j]. More generally, the (i 1 , i 2 , . . . , i )-th entry in the tensor E[x 1 ⊗ x 2 ⊗ • • • ⊗ x ] is Pr[1st word = i 1 , 2nd word = i 2 , . . . , -th word = i ].</formula><p>This means that estimating cross moments, say, of x 1 ⊗ x 2 ⊗ x 3 , is the same as estimating joint probabilities of the first three words over all documents. (Recall that we assume that each document has at least three words.)</p><p>The second advantage of the vector encoding of words is that the conditional expectation of x t given h = j is simply µ j , the vector of word probabilities for topic j:</p><formula xml:id="formula_11">E[x t |h = j] = d i=1 Pr[t-th word = i|h = j] e i = d i=1 [µ j ] i e i = µ j , j ∈ [k]</formula><p>(where [µ j ] i is the i-th entry in the vector µ j ). Because the words are conditionally independent given the topic, we can use this same property with conditional cross moments, say, of x 1 and x 2 :</p><formula xml:id="formula_12">E[x 1 ⊗ x 2 |h = j] = E[x 1 |h = j] ⊗ E[x 2 |h = j] = µ j ⊗ µ j , j ∈ [k].</formula><p>This and similar calculations lead one to the following theorem.</p><p>Theorem 3.1 <ref type="bibr">(Anandkumar et al., 2012c)</ref> If</p><formula xml:id="formula_13">M 2 := E[x 1 ⊗ x 2 ] M 3 := E[x 1 ⊗ x 2 ⊗ x 3 ], then M 2 = k i=1 w i µ i ⊗ µ i M 3 = k i=1 w i µ i ⊗ µ i ⊗ µ i .</formula><p>As we will see in Section 4.3, the structure of M 2 and M 3 revealed in Theorem 3.1 implies that the topic vectors µ 1 , µ 2 , . . . , µ k can be estimated by computing a certain symmetric tensor decomposition. Moreover, due to exchangeability, all triples (resp., pairs) of words in a document-and not just the first three (resp., two) words-can be used in forming M 3 (resp., M 2 ); see Section 6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Beyond Raw Moments</head><p>In the single topic model above, the raw (cross) moments of the observed words directly yield the desired symmetric tensor structure. In some other models, the raw moments do not explicitly have this form. Here, we show that the desired tensor structure can be found through various manipulations of different moments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Spherical Gaussian Mixtures: Common Covariance</head><p>We now consider a mixture of k Gaussian distributions with spherical covariances. We start with the simpler case where all of the covariances are identical; this probabilistic model is closely related to the (non-probabilistic) k-means clustering problem <ref type="bibr" target="#b63">(MacQueen, 1967)</ref>.</p><p>Let w i ∈ (0, 1) be the probability of choosing component i ∈ [k], {µ 1 , µ 2 , . . . , µ k } ⊂ R d be the component mean vectors, and σ 2 I be the common covariance matrix. An observation in this model is given by</p><formula xml:id="formula_14">x := µ h + z,</formula><p>where h is the discrete random variable with Pr[h = i] = w i for i ∈ [k] (similar to the exchangeable single topic model), and z ∼ N (0, σ 2 I) is an independent multivariate Gaussian random vector in R d with zero mean and spherical covariance σ 2 I.</p><p>The Gaussian mixture model differs from the exchangeable single topic model in the way observations are generated. In the single topic model, we observe multiple draws (words in a particular document) x 1 , x 2 , . . . , x given the same fixed h (the topic of the document). In contrast, for the Gaussian mixture model, every realization of x corresponds to a different realization of h. Theorem 3.2 <ref type="bibr" target="#b46">(Hsu and Kakade, 2013)</ref> Assume d ≥ k. The variance σ 2 is the smallest eigenvalue of the covariance matrix E</p><formula xml:id="formula_15">[x ⊗ x] -E[x] ⊗ E[x]. Furthermore, if M 2 := E[x ⊗ x] -σ 2 I M 3 := E[x ⊗ x ⊗ x] -σ 2 d i=1 E[x] ⊗ e i ⊗ e i + e i ⊗ E[x] ⊗ e i + e i ⊗ e i ⊗ E[x] , then M 2 = k i=1 w i µ i ⊗ µ i M 3 = k i=1 w i µ i ⊗ µ i ⊗ µ i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Spherical Gaussian Mixtures: Differing Covariances</head><p>The general case is where each component may have a different spherical covariance. An observation in this model is again x = µ h + z, but now z ∈ R d is a random vector whose conditional distribution given h = i (for some i ∈ [k]) is a multivariate Gaussian N (0, σ 2 i I) with zero mean and spherical covariance σ 2 i I.</p><p>Theorem 3.3 <ref type="bibr" target="#b46">(Hsu and Kakade, 2013)</ref> Assume d ≥ k. The average variance σ2 :</p><formula xml:id="formula_16">= k i=1 w i σ 2 i is the smallest eigenvalue of the covariance matrix E[x ⊗ x] -E[x] ⊗ E[x].</formula><p>Let v be any unit norm eigenvector corresponding to the eigenvalue σ2 . If</p><formula xml:id="formula_17">M 1 := E[x(v (x -E[x])) 2 ] M 2 := E[x ⊗ x] -σ2 I M 3 := E[x ⊗ x ⊗ x] - d i=1 M 1 ⊗ e i ⊗ e i + e i ⊗ M 1 ⊗ e i + e i ⊗ e i ⊗ M 1 , then M 2 = k i=1 w i µ i ⊗ µ i M 3 = k i=1 w i µ i ⊗ µ i ⊗ µ i .</formula><p>As shown by <ref type="bibr" target="#b46">Hsu and Kakade (2013)</ref>, M 1 = k i=1 w i σ 2 i µ i . Note that for the common covariance case, where σ 2 i = σ 2 , we have that M 1 = σ 2 E[x] (cf. Theorem 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Independent Component Analysis (ICA)</head><p>The standard model for ICA <ref type="bibr" target="#b27">(Comon, 1994;</ref><ref type="bibr" target="#b20">Cardoso and Comon, 1996;</ref><ref type="bibr" target="#b50">Hyvärinen and Oja, 2000;</ref><ref type="bibr" target="#b28">Comon and Jutten, 2010)</ref>, in which independent signals are linearly mixed and corrupted with Gaussian noise before being observed, is specified as follows. Let h ∈ R k be a latent random vector with independent coordinates, A ∈ R d×k the mixing matrix, and z be a multivariate Gaussian random vector. The random vectors h and z are assumed to be independent. The observed random vector is</p><formula xml:id="formula_18">x := Ah + z.</formula><p>Let µ i denote the i-th column of the mixing matrix A.</p><p>Theorem 3.4 <ref type="bibr" target="#b28">(Comon and Jutten, 2010)</ref> Define</p><formula xml:id="formula_19">M 4 := E[x ⊗ x ⊗ x ⊗ x] -T</formula><p>where T is the fourth-order tensor with</p><formula xml:id="formula_20">[T ] i 1 ,i 2 ,i 3 ,i 4 := E[x i 1 x i 2 ]E[x i 3 x i 4 ] + E[x i 1 x i 3 ]E[x i 2 x i 4 ] + E[x i 1 x i 4 ]E[x i 2 x i 3 ], 1 ≤ i 1 , i 2 , i 3 , i 4 ≤ k ( i.e., T is the fourth derivative tensor of the function v → 8 -1 E[(v x) 2 ] 2 , so M 4 is the fourth cumulant tensor). Let κ i := E[h 4 i ] -3 for each i ∈ [k]. Then M 4 = k i=1 κ i µ i ⊗ µ i ⊗ µ i ⊗ µ i .</formula><p>Note that κ i corresponds to the excess kurtosis, a measure of non-Gaussianity as κ i = 0 if h i is a standard normal random variable. Furthermore, note that A is not identifiable if h is a multivariate Gaussian.</p><p>We may derive forms similar to that of M 2 and M 3 from Theorem 3.1 using M 4 by observing that</p><formula xml:id="formula_21">M 4 (I, I, u, v) = k i=1 κ i (µ i u)(µ i v) µ i ⊗ µ i , M 4 (I, I, I, v) = k i=1 κ i (µ i v) µ i ⊗ µ i ⊗ µ i for any vectors u, v ∈ R d .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Latent Dirichlet Allocation (LDA)</head><p>An increasingly popular class of latent variable models are mixed membership models, where each datum may belong to several different latent classes simultaneously. LDA is one such model for the case of document modeling; here, each document corresponds to a mixture over topics (as opposed to just a single topic). The distribution over such topic mixtures is a Dirichlet distribution Dir(α) with parameter vector α ∈ R k ++ with strictly positive entries; its density over the probability simplex ∆</p><formula xml:id="formula_22">k-1 := {v ∈ R k : v i ∈ [0, 1]∀i ∈ [k], k i=1 v i = 1} is given by p α (h) = Γ(α 0 ) k i=1 Γ(α i ) k i=1 h α i -1 i , h ∈ ∆ k-1 where α 0 := α 1 + α 2 + • • • + α k .</formula><p>As before, the k topics are specified by probability vectors µ 1 , µ 2 , . . . , µ k ∈ ∆ d-1 . To generate a document, we first draw the topic mixture h = (h 1 , h 2 , . . . , h k ) ∼ Dir(α), and then conditioned on h, we draw words x 1 , x 2 , . . . , x independently from the discrete distribution specified by the probability vector k i=1 h i µ i (i.e., for each x t , we independently sample a topic j according to h and then sample x t according to µ j ). Again, we encode a word x t by setting x t = e i iff the t-th word in the document is i.</p><p>The parameter α 0 (the sum of the "pseudo-counts") characterizes the concentration of the distribution. As α 0 → 0, the distribution degenerates to a single topic model (i.e., the limiting density has, with probability 1, exactly one entry of h being 1 and the rest are 0). At the other extreme, if α = (c, c, . . . , c) for some scalar c &gt; 0, then as α 0 = ck → ∞, the distribution of h becomes peaked around the uniform vector (1/k, 1/k, . . . , 1/k) (furthermore, the distribution behaves like a product distribution). We are typically interested in the case where α 0 is small (e.g., a constant independent of k), whereupon h typically has only a few large entries. This corresponds to the setting where the documents are mainly comprised of just a few topics.</p><formula xml:id="formula_23">h x 1 x 2 • • • x (a) Multi-view models h 1 h 2 • • • h x 1 x 2 x (b) Hidden Markov model</formula><p>Theorem 3.5 <ref type="bibr">(Anandkumar et al., 2012a)</ref> Define</p><formula xml:id="formula_24">M 1 := E[x 1 ] M 2 := E[x 1 ⊗ x 2 ] - α 0 α 0 + 1 M 1 ⊗ M 1 M 3 := E[x 1 ⊗ x 2 ⊗ x 3 ] - α 0 α 0 + 2 E[x 1 ⊗ x 2 ⊗ M 1 ] + E[x 1 ⊗ M 1 ⊗ x 2 ] + E[M 1 ⊗ x 1 ⊗ x 2 ] + 2α 2 0 (α 0 + 2)(α 0 + 1) M 1 ⊗ M 1 ⊗ M 1 .</formula><p>Then</p><formula xml:id="formula_25">M 2 = k i=1 α i (α 0 + 1)α 0 µ i ⊗ µ i M 3 = k i=1 2α i (α 0 + 2)(α 0 + 1)α 0 µ i ⊗ µ i ⊗ µ i .</formula><p>Note that α 0 needs to be known to form M 2 and M 3 from the raw moments. This, however, is a much weaker than assuming that the entire distribution of h is known (i.e., knowledge of the whole parameter vector α).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-View Models</head><p>Multi-view models (also sometimes called naïve Bayes models) are a special class of Bayesian networks in which observed variables x 1 , x 2 , . . . , x are conditionally independent given a latent variable h. This is similar to the exchangeable single topic model, but here we do not require the conditional distributions of the x t , t ∈ [ ] to be identical. Techniques developed for this class can be used to handle a number of widely used models including hidden Markov models <ref type="bibr" target="#b66">(Mossel and Roch, 2006;</ref><ref type="bibr">Anandkumar et al., 2012c)</ref>, phylogenetic tree models <ref type="bibr" target="#b24">(Chang, 1996;</ref><ref type="bibr" target="#b66">Mossel and Roch, 2006)</ref>, certain tree mixtures <ref type="bibr">(Anandkumar et al., 2012b)</ref>, and certain probabilistic grammar models <ref type="bibr">(Hsu et al., 2012a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As before, we let h ∈ [k] be a discrete random variable with Pr</head><formula xml:id="formula_26">[h = j] = w j for all j ∈ [k]. Now consider random vectors x 1 ∈ R d 1 , x 2 ∈ R d 2 ,</formula><p>and x 3 ∈ R d 3 which are conditionally independent given h, and</p><formula xml:id="formula_27">E[x t |h = j] = µ t,j , j ∈ [k], t ∈ {1, 2, 3}</formula><p>where the µ t,j ∈ R dt are the conditional means of the x t given h = j. Thus, we allow the observations x 1 , x 2 , . . . , x to be random vectors, parameterized only by their conditional means. Importantly, these conditional distributions may be discrete, continuous, or even a mix of both.</p><p>We first note the form for the raw (cross) moments.</p><p>Proposition 3.1 We have that:</p><formula xml:id="formula_28">E[x t ⊗ x t ] = k i=1 w i µ t,i ⊗ µ t ,i , {t, t } ⊂ {1, 2, 3}, t = t E[x 1 ⊗ x 2 ⊗ x 3 ] = k i=1 w i µ 1,i ⊗ µ 2,i ⊗ µ 3,i .</formula><p>The cross moments do not possess a symmetric tensor form when the conditional distributions are different. Nevertheless, the moments can be "symmetrized" via a simple linear transformation of x 1 and x 2 (roughly speaking, this relates x 1 and x 2 to x 3 ); this leads to an expression from which the conditional means of x 3 (i.e., µ 3,1 , µ 3,2 , . . . , µ 3,k ) can be recovered. For simplicity, we assume d 1 = d 2 = d 3 = k; the general case (with d t ≥ k) is easily handled using low-rank singular value decompositions.</p><p>Theorem 3.6 <ref type="bibr">(Anandkumar et al., 2012a)</ref> Assume that {µ v,1 , µ v,2 , . . . , µ v,k } are linearly independent for each v ∈ {1, 2, 3}. Define</p><formula xml:id="formula_29">x1 := E[x 3 ⊗ x 2 ]E[x 1 ⊗ x 2 ] -1 x 1 x2 := E[x 3 ⊗ x 1 ]E[x 2 ⊗ x 1 ] -1 x 2 M 2 := E[x 1 ⊗ x2 ] M 3 := E[x 1 ⊗ x2 ⊗ x 3 ]. Then M 2 = k i=1 w i µ 3,i ⊗ µ 3,i M 3 = k i=1 w i µ 3,i ⊗ µ 3,i ⊗ µ 3,i .</formula><p>We now discuss three examples (taken mostly from <ref type="bibr">Anandkumar et al., 2012c)</ref> where the above observations can be applied. The first two concern mixtures of product distributions, and the last one is the time-homogeneous hidden Markov model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Mixtures of Axis-Aligned Gaussians and Other Product Distributions</head><p>The first example is a mixture of k product distributions in R n under a mild incoherence assumption <ref type="bibr">(Anandkumar et al., 2012c)</ref>. Here, we allow each of the k component distributions to have a different product distribution (e.g., Gaussian distribution with an axis-aligned covariance matrix), but require the matrix of component means</p><formula xml:id="formula_30">A := [µ 1 |µ 2 | • • • |µ k ] ∈ R n×k</formula><p>to satisfy a certain (very mild) incoherence condition. The role of the incoherence condition is explained below.</p><p>For a mixture of product distributions, any partitioning of the dimensions [n] into three groups creates three (possibly asymmetric) "views" which are conditionally independent once the mixture component is selected. However, recall that Theorem 3.6 requires that for each view, the k conditional means be linearly independent. In general, this may not be achievable; consider, for instance, the case µ i = e i for each i ∈ [k]. Such cases, where the component means are very aligned with the coordinate basis, are precluded by the incoherence condition.</p><p>Define coherence(A) := max i∈[n] {e i Π A e i } to be the largest diagonal entry of the orthogonal projector to the range of A, and assume A has rank k. The coherence lies between k/n and 1; it is largest when the range of A is spanned by the coordinate axes, and it is k/n when the range is spanned by a subset of the Hadamard basis of cardinality k. The incoherence condition requires, for some ε, δ ∈ (0, 1), coherence(A) ≤ (ε 2 /6)/ ln(3k/δ). Essentially, this condition ensures that the non-degeneracy of the component means is not isolated in just a few of the n dimensions. Operationally, it implies the following. Proposition 3.2 <ref type="bibr">(Anandkumar et al., 2012c)</ref> Assume A has rank k, and</p><formula xml:id="formula_31">coherence(A) ≤ ε 2 /6 ln(3k/δ)</formula><p>for some ε, δ ∈ (0, 1). With probability at least 1-δ, a random partitioning of the dimensions [n] into three groups (for each i ∈ [n], independently pick t ∈ {1, 2, 3} uniformly at random and put i in group t) has the following property. For each t ∈ {1, 2, 3} and j ∈ [k], let µ t,j be the entries of µ j put into group t, and let</p><formula xml:id="formula_32">A t := [µ t,1 |µ t,2 | • • • |µ t,k ].</formula><p>Then for each t ∈ {1, 2, 3}, A t has full column rank, and the k-th largest singular value of A t is at least</p><p>(1 -ε)/3 times that of A.</p><p>Therefore, three asymmetric views can be created by randomly partitioning the observed random vector x into x 1 , x 2 , and x 3 , such that the resulting component means for each view satisfy the conditions of Theorem 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Spherical Gaussian Mixtures, Revisited</head><p>Consider again the case of spherical Gaussian mixtures (cf. Section 3.2). As we shall see in Section 4.3, the previous techniques (based on Theorem 3.2 and Theorem 3.3) lead to estimation procedures when the dimension of x is k or greater (and when the k component means are linearly independent). We now show that when the dimension is slightly larger, say greater than 3k, a different (and simpler) technique based on the multi-view structure can be used to extract the relevant structure.</p><p>We again use a randomized reduction. Specifically, we create three views by (i) applying a random rotation to x, and then (ii) partitioning x ∈ R n into three views x1 , x2 , x3 ∈ R d for d := n/3. By the rotational invariance of the multivariate Gaussian distribution, the distribution of x after random rotation is still a mixture of spherical Gaussians (i.e., a mixture of product distributions), and thus x1 , x2 , x3 are conditionally independent given h. What remains to be checked is that, for each view t ∈ {1, 2, 3}, the matrix of conditional means of xt for each view has full column rank. This is true with probability 1 as long as the matrix of conditional means</p><formula xml:id="formula_33">A := [µ 1 |µ 2 | • • • |µ k ] ∈ R n×k has rank k and n ≥ 3k.</formula><p>To see this, observe that a random rotation in R n followed by a restriction to d coordinates is simply a random projection from R n to R d , and that a random projection of a linear subspace of dimension k to R d is almost surely injective as long as d ≥ k. Applying this observation to the range of A implies the following.</p><p>Proposition 3.3 <ref type="bibr" target="#b46">(Hsu and Kakade, 2013)</ref> Assume A has rank k and that n ≥ 3k. Let R ∈ R n×n be chosen uniformly at random among all orthogonal n × n matrices, and set</p><formula xml:id="formula_34">x := Rx ∈ R n and Ã := RA = [Rµ 1 |Rµ 2 | • • • |Rµ k ] ∈ R n×k . Partition [n] into three groups of sizes d 1 , d 2 , d 3 with d t ≥ k for each t ∈ {1, 2, 3}. Furthermore, for each t, define xt ∈ R dt (respectively, Ãt ∈ R dt×k )</formula><p>to be the subvector of x (resp., submatrix of Ã) obtained by selecting the d t entries (resp., rows) in the t-th group. Then x1 , x2 , x3 are conditionally independent given h; E[x t |h = j] = Ãt e j for each j ∈ [k] and t ∈ {1, 2, 3}; and with probability 1, the matrices Ã1 , Ã2 , Ã3 have full column rank.</p><p>It is possible to obtain a quantitative bound on the k-th largest singular value of each A t in terms of the k-th largest singular value of A (analogous to Proposition 3.2). One avenue is to show that a random rotation in fact causes Ã to have low coherence, after which we can apply Proposition 3.2. With this approach, it is sufficient to require n = O(k log k) (for constant ε and δ), which results in the k-th largest singular value of each A t being a constant fraction of the k-th largest singular value of A. We conjecture that, in fact, n ≥ c • k for some c &gt; 3 suffices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Hidden Markov Models</head><p>Our last example is the time-homogeneous HMM for sequences of vector-valued observations</p><formula xml:id="formula_35">x 1 , x 2 , . . . ∈ R d . Consider a Markov chain of discrete hidden states y 1 → y 2 → y 3 → • • • over k possible states [k]</formula><p>; given a state y t at time t, the observation x t at time t (a random vector taking values in R d ) is independent of all other observations and hidden states. See Figure <ref type="figure" target="#fig_5">1(b)</ref>.</p><p>Let π ∈ ∆ k-1 be the initial state distribution (i.e., the distribution of y 1 ), and T ∈ R k×k be the stochastic transition matrix for the hidden state Markov chain: for all times t,</p><formula xml:id="formula_36">Pr[y t+1 = i|y t = j] = T i,j , i, j ∈ [k].</formula><p>Finally, let O ∈ R d×k be the matrix whose j-th column is the conditional expectation of x t given y t = j: for all times t,</p><formula xml:id="formula_37">E[x t |y t = j] = Oe j , j ∈ [k].</formula><p>Proposition 3.4 <ref type="bibr">(Anandkumar et al., 2012c)</ref> Define h := y 2 , where y 2 is the second hidden state in the Markov chain. Then • x 1 , x 2 , x 3 are conditionally independent given h;</p><p>• the distribution of h is given by the vector w :</p><formula xml:id="formula_38">= T π ∈ ∆ k-1 ; • for all j ∈ [k], E[x 1 |h = j] = O diag(π)T diag(w) -1 e j E[x 2 |h = j] = Oe j E[x 3 |h = j] = OT e j .</formula><p>Note the matrix of conditional means of x t has full column rank, for each t ∈ {1, 2, 3}, provided that: (i) O has full column rank, (ii) T is invertible, and (iii) π and T π have positive entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Orthogonal Tensor Decompositions</head><p>We now show how recovering the µ i 's in our aforementioned problems reduces to the problem of finding a certain orthogonal tensor decomposition of a symmetric tensor. We start by reviewing the spectral decomposition of symmetric matrices, and then discuss a generalization to the higher-order tensor case. Finally, we show how orthogonal tensor decompositions can be used for estimating the latent variable models from the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Review: The Matrix Case</head><p>We first build intuition by reviewing the matrix setting, where the desired decomposition is the eigendecomposition of a symmetric rank-k matrix M = V ΛV , where</p><formula xml:id="formula_39">V = [v 1 |v 2 | • • • |v k ] ∈ R n×k</formula><p>is the matrix with orthonormal eigenvectors as columns, and Λ = diag(λ 1 , λ 2 , . . . , λ k ) ∈ R k×k is diagonal matrix of non-zero eigenvalues. In other words,</p><formula xml:id="formula_40">M = k i=1 λ i v i v i = k i=1 λ i v ⊗2 i .<label>(1)</label></formula><p>Such a decomposition is guaranteed to exist for every symmetric matrix. Recovery of the v i 's and λ i 's can be viewed at least two ways. First, each v i is fixed under the mapping u → M u, up to a scaling factor λ i :</p><formula xml:id="formula_41">M v i = k j=1 λ j (v j v i )v j = λ i v i</formula><p>as v j v i = 0 for all j = i by orthogonality. The v i 's are not necessarily the only such fixed points. For instance, with the multiplicity λ 1 = λ 2 = λ, then any linear combination of v 1 and v 2 is similarly fixed under M . However, in this case, the decomposition in (1) is not unique, as</p><formula xml:id="formula_42">λ 1 v 1 v 1 + λ 2 v 2 v 2 is equal to λ(u 1 u 1 + u 2 u 2 )</formula><p>for any pair of orthonormal vectors, u 1 and u 2 spanning the same subspace as v 1 and v 2 . Nevertheless, the decomposition is unique when λ 1 , λ 2 , . . . , λ k are distinct, whereupon the v j 's are the only directions fixed under u → M u up to non-trivial scaling.</p><p>The second view of recovery is via the variational characterization of the eigenvalues. Assume λ 1 &gt; λ 2 &gt; • • • &gt; λ k ; the case of repeated eigenvalues again leads to similar nonuniqueness as discussed above. Then the Rayleigh quotient</p><formula xml:id="formula_43">u → u M u u u</formula><p>is maximized over non-zero vectors by v 1 . Furthermore, for any s ∈ [k], the maximizer of the Rayleigh quotient, subject to being orthogonal to v 1 , v 2 , . . . , v s-1 , is v s . Another way of obtaining this second statement is to consider the deflated Rayleigh quotient u → u M -s-1 j=1 λ j v j v j u u u and observe that v s is the maximizer.</p><p>Efficient algorithms for finding these matrix decompositions are well studied <ref type="bibr">(Golub and van Loan, 1996, Section 8.2</ref>.3), and iterative power methods are one effective class of algorithms.</p><p>We remark that in our multilinear tensor notation, we may write the maps</p><formula xml:id="formula_44">u → M u and u → u M u/ u 2 2 as u → M u ≡ u → M (I, u),<label>(2)</label></formula><formula xml:id="formula_45">u → u M u u u ≡ u → M (u, u) u u .</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Tensor Case</head><p>Decomposing general tensors is a delicate issue; tensors may not even have unique decompositions. Fortunately, the orthogonal tensors that arise in the aforementioned models have a structure which permits a unique decomposition under a mild non-degeneracy condition. We focus our attention to the case p = 3, i.e., a third order tensor; the ideas extend to general p with minor modifications. An orthogonal decomposition of a symmetric tensor</p><formula xml:id="formula_46">T ∈ 3 R n is a collection of or- thonormal (unit) vectors {v 1 , v 2 , . . . , v k } together with corresponding positive scalars λ i &gt; 0 such that T = k i=1 λ i v ⊗3 i .<label>(4)</label></formula><p>Note that since we are focusing on odd-order tensors (p = 3), we have added the requirement that the λ i be positive. This convention can be followed without loss of generality since -λ i v ⊗p i = λ i (-v i ) ⊗p whenever p is odd. Also, it should be noted that orthogonal decompositions do not necessarily exist for every symmetric tensor.</p><p>In analogy to the matrix setting, we consider two ways to view this decomposition: a fixed-point characterization and a variational characterization. Related characterizations based on optimal rank-1 approximations are given by <ref type="bibr" target="#b85">Zhang and Golub (2001)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Fixed-Point Characterization</head><p>For a tensor T , consider the vector-valued map u → T (I, u, u)</p><p>(5) which is the third-order generalization of (2). This can be explicitly written as</p><formula xml:id="formula_47">T (I, u, u) = d i=1 1≤j,l≤d</formula><p>T i,j,l (e j u)(e l u)e i .</p><p>Observe that ( <ref type="formula">5</ref>) is not a linear map, which is a key difference compared to the matrix case.</p><p>An eigenvector u for a matrix M satisfies M (I, u) = λu, for some scalar λ. We say a unit vector u ∈ R n is an eigenvector of T , with corresponding eigenvalue λ ∈ R, if</p><formula xml:id="formula_48">T (I, u, u) = λu.</formula><p>(To simplify the discussion, we assume throughout that eigenvectors have unit norm; otherwise, for scaling reasons, we replace the above equation with T (I, u, u) = λ u u.) This concept was originally introduced by <ref type="bibr" target="#b60">Lim (2005)</ref> and <ref type="bibr" target="#b73">Qi (2005)</ref>. For orthogonally decomposable tensors</p><formula xml:id="formula_49">T = k i=1 λ i v ⊗3 i , T (I, u, u) = k i=1 λ i (u v i ) 2 v i .</formula><p>By the orthogonality of the v i , it is clear that</p><formula xml:id="formula_50">T (I, v i , v i ) = λ i v i for all i ∈ [k]. Therefore each (v i , λ i ) is an eigenvector/eigenvalue pair.</formula><p>There are a number of subtle differences compared to the matrix case that arise as a result of the non-linearity of (5). First, even with the multiplicity λ 1 = λ 2 = λ, a linear combination u := c 1 v 1 + c 2 v 2 may not be an eigenvector. In particular,</p><formula xml:id="formula_51">T (I, u, u) = λ 1 c 2 1 v 1 + λ 2 c 2 2 v 2 = λ(c 2 1 v 1 + c 2 2 v 2 )</formula><p>may not be a multiple of c 1 v 1 + c 2 v 2 . This indicates that the issue of repeated eigenvalues does not have the same status as in the matrix case. Second, even if all the eigenvalues are distinct, it turns out that the v i 's are not the only eigenvectors. For example, set</p><formula xml:id="formula_52">u := (1/λ 1 )v 1 + (1/λ 2 )v 2 . Then, T (I, u, u) = λ 1 (1/λ 1 ) 2 v 1 + λ 2 (1/λ 2 ) 2 v 2 = u,</formula><p>so u/ u is an eigenvector. More generally, for any subset S ⊆ [k], the vector</p><formula xml:id="formula_53">i∈S 1 λ i • v i is (proportional to) an eigenvector.</formula><p>As we now see, these additional eigenvectors can be viewed as spurious. We say a unit vector u is a robust eigenvector of T if there exists an &gt; 0 such that for all θ ∈ {u ∈ R n : u -u ≤ }, repeated iteration of the map θ → T (I, θ, θ)</p><formula xml:id="formula_54">T (I, θ, θ) ,<label>(6)</label></formula><p>starting from θ converges to u. Note that the map (6) rescales the output to have unit Euclidean norm. Robust eigenvectors are also called attracting fixed points of (6) (see, e.g., <ref type="bibr" target="#b56">Kolda and Mayo, 2011)</ref>.</p><p>The following theorem implies that if T has an orthogonal decomposition as given in (4), then the set of robust eigenvectors of T are precisely the set {v 1 , v 2 , . . . v k }, implying that the orthogonal decomposition is unique. (For even order tensors, the uniqueness is true up to sign-flips of the v i .)</p><p>Theorem 4.1 Let T have an orthogonal decomposition as given in (4).</p><p>1. The set of θ ∈ R n which do not converge to some v i under repeated iteration of (6) has measure zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The set of robust eigenvectors of</head><formula xml:id="formula_55">T is equal to {v 1 , v 2 , . . . , v k }.</formula><p>The proof of Theorem 4.1 is given in Appendix A.1, and follows readily from simple orthogonality considerations. Note that every v i in the orthogonal tensor decomposition is robust, whereas for a symmetric matrix M , for almost all initial points, the map θ → M θ M θ converges only to an eigenvector corresponding to the largest magnitude eigenvalue. Also, since the tensor order is odd, the signs of the robust eigenvectors are fixed, as each -v i is mapped to v i under (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Variational Characterization</head><p>We now discuss a variational characterization of the orthogonal decomposition. The generalized Rayleigh quotient <ref type="bibr" target="#b85">(Zhang and Golub, 2001)</ref> for a third-order tensor is</p><formula xml:id="formula_56">u → T (u, u, u) (u u) 3/2 ,</formula><p>which can be compared to (3). For an orthogonally decomposable tensor, the following theorem shows that a non-zero vector u ∈ R n is an isolated local maximizer <ref type="bibr" target="#b68">(Nocedal and Wright, 1999)</ref> of the generalized Rayleigh quotient if and only if</p><formula xml:id="formula_57">u = v i for some i ∈ [k].</formula><p>Theorem 4.2 Let T have an orthogonal decomposition as given in (4), and consider the optimization problem max u∈R n T (u, u, u) s.t. u ≤ 1. 1. The stationary points are eigenvectors of T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A stationary point u is an isolated local maximizer if and only if</head><formula xml:id="formula_58">u = v i for some i ∈ [k].</formula><p>The proof of Theorem 4.2 is given in Appendix A.2. It is similar to local optimality analysis for ICA methods using fourth-order cumulants (e.g., <ref type="bibr" target="#b34">Delfosse and Loubaton, 1995;</ref><ref type="bibr" target="#b39">Frieze et al., 1996)</ref>. Again, we see similar distinctions to the matrix case. In the matrix case, the only local maximizers of the Rayleigh quotient are the eigenvectors with the largest eigenvalue (and these maximizers take on the globally optimal value). For the case of orthogonal tensor forms, the robust eigenvectors are precisely the isolated local maximizers.</p><p>An important implication of the two characterizations is that, for orthogonally decomposable tensors T , (i) the local maximizers of the objective function u → T (u, u, u)/(u u) 3/2 correspond precisely to the vectors v i in the decomposition, and (ii) these local maximizers can be reliably identified using a simple fixed-point iteration (i.e., the tensor analogue of the matrix power method). Moreover, a second-derivative test based on T (I, I, u) can be employed to test for local optimality and rule out other stationary points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Estimation via Orthogonal Tensor Decompositions</head><p>We now demonstrate how the moment tensors obtained for various latent variable models in Section 3 can be reduced to an orthogonal form. For concreteness, we take the specific form from the exchangeable single topic model (Theorem 3.1):</p><formula xml:id="formula_59">M 2 = k i=1 w i µ i ⊗ µ i , M 3 = k i=1 w i µ i ⊗ µ i ⊗ µ i .</formula><p>(The more general case allows the weights w i in M 2 to differ in M 3 , but for simplicity we keep them the same in the following discussion.) We now show how to reduce these forms to an orthogonally decomposable tensor from which the w i and µ i can be recovered. See Appendix D for a discussion as to how previous approaches <ref type="bibr" target="#b66">(Mossel and Roch, 2006;</ref><ref type="bibr">Anandkumar et al., 2012c,a;</ref><ref type="bibr" target="#b46">Hsu and Kakade, 2013)</ref> achieved this decomposition through a certain simultaneous diagonalization method.</p><p>Throughout, we assume the following non-degeneracy condition. Observe that Condition 4.1 implies that M 2 0 is positive semidefinite and has rank k. This is often a mild condition in applications. When this condition is not met, learning is conjectured to be generally hard for both computational <ref type="bibr" target="#b66">(Mossel and Roch, 2006)</ref> and information-theoretic reasons <ref type="bibr" target="#b65">(Moitra and Valiant, 2010)</ref>. As discussed by <ref type="bibr">Hsu et al. (2012b)</ref> and <ref type="bibr" target="#b46">Hsu and Kakade (2013)</ref>, when the non-degeneracy condition does not hold, it is often possible to combine multiple observations using tensor products to increase the rank of the relevant matrices. Indeed, this observation has been rigorously formulated in very recent works of <ref type="bibr" target="#b14">Bhaskara et al. (2014)</ref> and <ref type="bibr" target="#b5">Anderson et al. (2014)</ref> using the framework of smoothed analysis <ref type="bibr" target="#b80">(Spielman and Teng, 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">The Reduction</head><p>First, let W ∈ R d×k be a linear transformation such that</p><formula xml:id="formula_60">M 2 (W, W ) = W M 2 W = I</formula><p>where I is the k × k identity matrix (i.e., W whitens M 2 ). Since M 2 0, we may for concreteness take W := U D -1/2 , where U ∈ R d×k is the matrix of orthonormal eigenvectors of M 2 , and D ∈ R k×k is the diagonal matrix of positive eigenvalues of M 2 . Let</p><formula xml:id="formula_61">μi := √ w i W µ i .</formula><p>Observe that</p><formula xml:id="formula_62">M 2 (W, W ) = k i=1 W ( √ w i µ i )( √ w i µ i ) W = k i=1 μi μ i = I, so the μi ∈ R k are orthonormal vectors. Now define M 3 := M 3 (W, W, W ) ∈ R k×k×k , so that M 3 = k i=1 w i (W µ i ) ⊗3 = k i=1 1 √ w i μ⊗3 i .</formula><p>As the following theorem shows, the orthogonal decomposition of M 3 can be obtained by identifying its robust eigenvectors, upon which the original parameters w i and µ i can be recovered. For simplicity, we only state the result in terms of robust eigenvector/eigenvalue pairs; one may also easily state everything in variational form using Theorem 4.2.</p><p>Theorem 4.3 Assume Condition 4.1 and take M 3 as defined above.</p><p>1. The set of robust eigenvectors of M 3 is equal to {μ 1 , μ2 , . . . , μk }.</p><p>2. The eigenvalue corresponding to the robust eigenvector μi of</p><formula xml:id="formula_63">M 3 is equal to 1/ √ w i , for all i ∈ [k]. 3. If B ∈ R d×k is the Moore-Penrose pseudoinverse of W , and (v, λ) is a robust eigen- vector/eigenvalue pair of M 3 , then λBv = µ i for some i ∈ [k].</formula><p>The theorem follows by combining the above discussion with the robust eigenvector characterization of Theorem 4.1. Recall that we have taken as convention that eigenvectors have unit norm, so the µ i are exactly determined from the robust eigenvector/eigenvalue pairs of M 3 (together with the pseudoinverse of W ); in particular, the scale of each µ i is correctly identified (along with the corresponding w i ). Relative to previous works on moment-based estimators for latent variable models (e.g., <ref type="bibr">Anandkumar et al., 2012c,a;</ref><ref type="bibr" target="#b46">Hsu and Kakade, 2013)</ref>, Theorem 4.3 emphasizes the role of the special tensor structure, which in turn makes transparent the applicability of methods for orthogonal tensor decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Local Maximizers of (Cross Moment) Skewness</head><p>The variational characterization provides an interesting perspective on the robust eigenvectors for these latent variable models. Consider the exchangeable single topic models (Theorem 3.1), and the objective function</p><formula xml:id="formula_64">u → E[(x 1 u)(x 2 u)(x 3 u)] E[(x 1 u)(x 2 u)] 3/2 = M 3 (u, u, u) M 2 (u, u) 3/2 .</formula><p>In this case, every local maximizer</p><formula xml:id="formula_65">u * satisfies M 2 (I, u * ) = √ w i µ i for some i ∈ [k].</formula><p>The objective function can be interpreted as the (cross moment) skewness of the random vectors x 1 , x 2 , x 3 along direction u.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Tensor Power Method</head><p>In this section, we consider the tensor power method of <ref type="bibr">Lathauwer et al. (2000, Remark 3)</ref> for orthogonal tensor decomposition. We first state a simple convergence analysis for an orthogonally decomposable tensor T .</p><p>When only an approximation T to an orthogonally decomposable tensor T is available (e.g., when empirical moments are used to estimate population moments), an orthogonal decomposition need not exist for this perturbed tensor (unlike for the case of matrices), and a more robust approach is required to extract the approximate decomposition. Here, we propose such a variant in Algorithm 1 and provide a detailed perturbation analysis. We note that alternative approaches such as simultaneous diagonalization can also be employed (see Appendix D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Convergence Analysis for Orthogonally Decomposable Tensors</head><p>The following lemma establishes the quadratic convergence of the tensor power methodi.e., repeated iteration of (6)-for extracting a single component of the orthogonal decomposition. Note that the initial vector θ 0 determines which robust eigenvector will be the convergent point. Computation of subsequent eigenvectors can be computed with deflation, i.e., by subtracting appropriate terms from T .</p><p>Lemma 5.1 Let T ∈ 3 R n have an orthogonal decomposition as given in (4). For a vector θ 0 ∈ R n , suppose that the set of numbers</p><formula xml:id="formula_66">|λ 1 v 1 θ 0 |, |λ 2 v 2 θ 0 |, . . . , |λ k v k θ 0 | has a unique largest element. Without loss of generality, say |λ 1 v 1 θ 0 | is this largest value and |λ 2 v 2 θ 0 | is the second largest value. For t = 1, 2, . . . , let θ t := T (I, θ t-1 , θ t-1 ) T (I, θ t-1 , θ t-1 ) . Then v 1 -θ t 2 ≤ 2λ 2 1 k i=2 λ -2 i • λ 2 v 2 θ 0 λ 1 v 1 θ 0 2 t+1 .</formula><p>That is, repeated iteration of (6) starting from θ 0 converges to v 1 at a quadratic rate.</p><p>To obtain all eigenvectors, we may simply proceed iteratively using deflation, executing the power method on Tj λ j v ⊗3 j after having obtained robust eigenvector / eigenvalue pairs {(v j , λ j )}. Proof Let θ 0 , θ 1 , θ 2 , . . . be the sequence given by θ 0 := θ 0 and θ t := T (I, θ t-1 , θ t-1 ) for t ≥ 1. Let c i := v i θ 0 for all i ∈ [k]. It is easy to check that (i) θ t = θ t / θ t , and (ii)</p><formula xml:id="formula_67">θ t = k i=1 λ 2 t -1 i c 2 t i v i . (Indeed, θ t+1 = k i=1 λ i (v i θ t ) 2 v i = k i=1 λ i (λ 2 t -1 i c 2 t i ) 2 v i = k i=1 λ 2 t+1 -1 i c 2 t+1 i v i .) Then 1 -(v 1 θ t ) 2 = 1 - (v 1 θ t ) 2 θ t 2 = 1 - λ 2 t+1 -2 1 c 2 t+1 1 k i=1 λ 2 t+1 -2 i c 2 t+1 i ≤ k i=2 λ 2 t+1 -2 i c 2 t+1 i k i=1 λ 2 t+1 -2 i c 2 t+1 i ≤ λ 2 1 k i=2 λ -2 i • λ 2 c 2 λ 1 c 1 2 t+1 . Since λ 1 &gt; 0, we have v 1 θ t &gt; 0 and hence v 1 -θ t 2 = 2(1 -v 1 θ t ) ≤ 2(1 -(v 1 θ t )</formula><p>2 ) as required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Perturbation Analysis of a Robust Tensor Power Method</head><p>Now we consider the case where we have an approximation T to an orthogonally decomposable tensor T . Here, a more robust approach is required to extract an approximate decomposition. We propose such an algorithm in Algorithm 1, and provide a detailed perturbation analysis. For simplicity, we assume the tensor T is of size k × k × k as per the reduction from Section 4.3. In some applications, it may be preferable to work directly with a n × n × n tensor of rank k ≤ n (as in Lemma 5.1); our results apply in that setting with little modification.</p><p>Algorithm 1 Robust tensor power method input symmetric tensor T ∈ R k×k×k , number of iterations L, N . output the estimated eigenvector/eigenvalue pair; the deflated tensor.</p><p>1: for τ = 1 to L do 2:</p><p>Draw θ (τ ) 0 uniformly at random from the unit sphere in R k .</p><p>3:</p><p>for t = 1 to N do 4:</p><p>Compute power iteration update</p><formula xml:id="formula_68">θ (τ ) t := T (I, θ (τ ) t-1 , θ<label>(τ )</label></formula><p>t-1 ) T (I, θ to obtain θ, and set λ := T ( θ, θ, θ). 9: return the estimated eigenvector/eigenvalue pair ( θ, λ); the deflated tensor Tλ θ⊗3 .</p><p>Assume that the symmetric tensor T ∈ R k×k×k is orthogonally decomposable, and that T = T + E, where the perturbation E ∈ R k×k×k is a symmetric tensor with small operator norm:</p><formula xml:id="formula_69">E := sup θ =1 |E(θ, θ, θ)|.</formula><p>In our latent variable model applications, T is the tensor formed by using empirical moments, while T is the orthogonally decomposable tensor derived from the population moments for the given model. In the context of parameter estimation (as in Section 4.3), E must account for any error amplification throughout the reduction, such as in the whitening step (see, e.g., Hsu and Kakade, 2013, for such an analysis).</p><p>The following theorem is similar to Wedin's perturbation theorem for singular vectors of matrices <ref type="bibr" target="#b84">(Wedin, 1972)</ref> in that it bounds the error of the (approximate) decomposition returned by Algorithm 1 on input T in terms of the size of the perturbation, provided that the perturbation is small enough.</p><formula xml:id="formula_70">Theorem 5.1 Let T = T + E ∈ R k×k×k , where T is a symmetric tensor with orthogonal decomposition T = k i=1 λ i v ⊗3 i</formula><p>where each λ i &gt; 0, {v 1 , v 2 , . . . , v k } is an orthonormal basis, and E is a symmetric tensor with operator norm E ≤ . Define λ min := min{λ i : i ∈ [k]}, and λ max := max{λ i : i ∈ [k]}. There exists universal constants C 1 , C 2 , C 3 &gt; 0 such that the following holds. Pick any η ∈ (0, 1), and suppose</p><formula xml:id="formula_71">≤ C 1 • λ min k , N ≥ C 2 • log(k) + log log λ max ,<label>and</label></formula><formula xml:id="formula_72">ln(L/ log 2 (k/η)) ln(k) • 1 - ln(ln(L/ log 2 (k/η))) + C 3 4 ln(L/ log 2 (k/η)) - ln(8) ln(L/ log 2 (k/η)) ≥ 1.02 1 + ln(4) ln(k) .</formula><p>(Note that the condition on L holds with L = poly(k) log(1/η).) Suppose that Algorithm 1 is iteratively called k times, where the input tensor is T in the first call, and in each subsequent call, the input tensor is the deflated tensor returned by the previous call. Let (v 1 , λ1 ), (v 2 , λ2 ), . . . , (v k , λk ) be the sequence of estimated eigenvector/eigenvalue pairs returned in these k calls. With probability at least 1 -η, there exists a permutation π on</p><formula xml:id="formula_73">[k] such that v π(j) -vj ≤ 8 /λ π(j) , |λ π(j) -λj | ≤ 5 , ∀j ∈ [k],</formula><p>and</p><formula xml:id="formula_74">T - k j=1 λj v⊗3 j ≤ 55 .</formula><p>The proof of Theorem 5.1 is given in Appendix B. One important difference from Wedin's theorem is that this is an algorithm dependent perturbation analysis, specific to Algorithm 1 (since the perturbed tensor need not have an orthogonal decomposition). Furthermore, note that Algorithm 1 uses multiple restarts to ensure (approximate) convergence-the intuition is that by restarting at multiple points, we eventually start at a point in which the initial contraction towards some eigenvector dominates the error E in our tensor. The proof shows that we find such a point with high probability within L = poly(k) trials. It should be noted that for large k, the required bound on L is very close to linear in k.</p><p>We note that it is also possible to design a variant of Algorithm 1 that instead uses a stopping criterion to determine if an iterate has (almost) converged to an eigenvector. For instance, if T (θ, θ, θ) &gt; max{ T F / √ 2r, T (I, I, θ) F /1.05}, where T F is the tensor Frobenius norm (vectorized Euclidean norm), and r is the expected rank of the unperturbed tensor (r = k -# of deflation steps), then it can be shown that θ must be close to one of the eigenvectors, provided that the perturbation is small enough. Using such a stopping criterion can reduce the number of random restarts when a good initial point is found early on. See Appendix C for details.</p><p>In general, it is possible, when run on a general symmetric tensor (e.g., T ), for the tensor power method to exhibit oscillatory behavior <ref type="bibr">(Kofidis and Regalia, 2002, Example 1)</ref>. This is not in conflict with Theorem 5.1, which effectively bounds the amplitude of these oscillations; in particular, if T = T + E is a tensor built from empirical moments, the error term E (and thus the amplitude of the oscillations) can be driven down by drawing more samples. The practical value of addressing these oscillations and perhaps stabilizing the algorithm is an interesting direction for future research <ref type="bibr" target="#b56">(Kolda and Mayo, 2011)</ref>.</p><p>A final consideration is that for specific applications, it may be possible to use domain knowledge to choose better initialization points. For instance, in the topic modeling applications (cf. Section 3.1), the eigenvectors are related to the topic word distributions, and many documents may be primarily composed of words from just single topic. Therefore, good initialization points can be derived from these single-topic documents themselves, as these points would already be close to one of the eigenvectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this section, we discuss some practical and application-oriented issues related to the tensor decomposition approach to learning latent variable models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Practical Implementation Considerations</head><p>A number of practical concerns arise when dealing with moment matrices and tensors. Below, we address two issues that are especially pertinent to topic modeling applications <ref type="bibr">(Anandkumar et al., 2012c,a)</ref> or other settings where the observations are sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Efficient Moment Representation for Exchangeable Models</head><p>In an exchangeable bag-of-words model, it is assumed that the words x 1 , x 2 , . . . , x in a document are conditionally i.i.d. given the topic h. This allows one to estimate p-th order moments using just p words per document. The estimators obtained via Theorem 3.1 (single topic model) and Theorem 3.5 (LDA) use only up to third-order moments, which suggests that each document only needs to have three words.</p><p>In practice, one should use all of the words in a document for efficient estimation of the moments. One way to do this is to average over all 3 • 3! ordered triples of words in a document of length . At first blush, this seems computationally expensive (when is large), but as it turns out, the averaging can be done implicitly, as shown by <ref type="bibr" target="#b87">Zou et al. (2013)</ref>. Let c ∈ R d be the word count vector for a document of length , so c i is the number of occurrences of word i in the document, and d i=1 c i = . Note that c is a sufficient statistic for the document. Then, the contribution of this document to the empirical third-order moment tensor is given by</p><formula xml:id="formula_75">1 3 • 1 3! • c ⊗ c ⊗ c + 2 d i=1 c i (e i ⊗ e i ⊗ e i ) - d i=1 d j=1 c i c j (e i ⊗ e i ⊗ e j ) - d i=1 d j=1 c i c j (e i ⊗ e j ⊗ e i ) - d i=1 d j=1 c i c j (e i ⊗ e j ⊗ e j ) .<label>(8)</label></formula><p>It can be checked that this quantity is equal to</p><formula xml:id="formula_76">1 3 • 1 3! • ordered word triple (x, y, z) e x ⊗ e y ⊗ e z</formula><p>where the sum is over all ordered word triples in the document. A similar expression is easily derived for the contribution of the document to the empirical second-order moment matrix:</p><formula xml:id="formula_77">1 2 • 1 2! • c ⊗ c -diag(c) .<label>(9)</label></formula><p>Note that the word count vector c is generally a sparse vector, so this representation allows for efficient multiplication by the moment matrices and tensors in time linear in the size of the document corpus (i.e., the number of non-zero entries in the term-document matrix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Dimensionality Reduction</head><p>Another serious concern regarding the use of tensor forms of moments is the need to operate on multidimensional arrays with Ω(d 3 ) values (it is typically not exactly d 3 due to symmetry). When d is large (e.g., when it is the size of the vocabulary in natural language applications), even storing a third-order tensor in memory can be prohibitive. Sparsity is one factor that alleviates this problem. Another approach is to use efficient linear dimensionality reduction. When this is combined with efficient techniques for matrix and tensor multiplication that avoid explicitly constructing the moment matrices and tensors (such as the procedure described above), it is possible to avoid any computational scaling more than linear in the dimension d and the training sample size. Consider for concreteness the tensor decomposition approach for the exchangeable single topic model as discussed in Section 4.3. Using recent techniques for randomized linear algebra computations (e.g., <ref type="bibr" target="#b41">Halko et al., 2011)</ref>, it is possible to efficiently approximate the whitening matrix W ∈ R d×k from the second-moment matrix M 2 ∈ R d×d . To do this, one first multiplies M 2 by a random matrix R ∈ R d×k for some k ≥ k, and then computes the top k singular vectors of the product M 2 R. This provides a basis U ∈ R d×k whose span is approximately the range of M 2 . From here, an approximate SVD of U M 2 U is used to compute the approximate whitening matrix W . Note that both matrix products M 2 R and U M 2 U may be performed via implicit access to M 2 by exploiting (9), so that M 2 need not be explicitly formed. With the whitening matrix W in hand, the third-moment tensor M 3 = M 3 (W, W, W ) ∈ R k×k×k can be implicitly computed via (8). For instance, the core computation in the tensor power method θ := M 3 (I, θ, θ) is performed by (i) computing η := W θ, (ii) computing η := M 3 (I, η, η), and finally (iii) computing θ := W η . Using the fact that M 3 is an empirical third-order moment tensor, these steps can be computed with O(dk + N ) operations, where N is the number of non-zero entries in the term-document matrix <ref type="bibr" target="#b87">(Zou et al., 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Computational Complexity</head><p>It is interesting to consider the computational complexity of the tensor power method in the dense setting where T ∈ R k×k×k is orthogonally decomposable but otherwise unstructured. Each iteration requires O(k 3 ) operations, and assuming at most k 1+δ random restarts for extracting each eigenvector (for some small δ &gt; 0) and O(log(k)+log log(1/ )) iterations per restart, the total running time is O(k 5+δ (log(k) + log log(1/ ))) to extract all k eigenvectors and eigenvalues.</p><p>An alternative approach to extracting the orthogonal decomposition of T is to reorganize T into a matrix M ∈ R k×k 2 by flattening two of the dimensions into one. In this case, if</p><formula xml:id="formula_78">T = k i=1 λ i v ⊗3 i , then M = k i=1 λ i v i ⊗ vec(v i ⊗ v i ).</formula><p>This reveals the singular value decomposition of M (assuming the eigenvalues λ 1 , λ 2 , . . . , λ k are distinct), and therefore can be computed with O(k 4 ) operations. Therefore it seems that the tensor power method is less efficient than a pure matrix-based approach via singular value decomposition. However, it should be noted that this matrix-based approach fails to recover the decomposition when eigenvalues are repeated, and can be unstable when the gap between eigenvalues is smallsee Appendix D for more discussion.</p><p>It is worth noting that the running times differ by roughly a factor of Θ(k 1+δ ), which can be accounted for by the random restarts. This gap can potentially be alleviated or removed by using a more clever method for initialization. Moreover, using special structure in the problem (as discussed above) can also improve the running time of the tensor power method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Sample Complexity Bounds</head><p>Previous work on using linear algebraic methods for estimating latent variable models crucially rely on matrix perturbation analysis for deriving sample complexity bounds <ref type="bibr" target="#b66">(Mossel and Roch, 2006;</ref><ref type="bibr">Hsu et al., 2012b;</ref><ref type="bibr">Anandkumar et al., 2012c,a;</ref><ref type="bibr" target="#b46">Hsu and Kakade, 2013)</ref>. The learning algorithms in these works are plug-in estimators that use empirical moments in place of the population moments, and then follow algebraic manipulations that result in the desired parameter estimates. As long as these manipulations can tolerate small perturbations of the population moments, a sample complexity bound can be obtained by exploiting the convergence of the empirical moments to the population moments via the law of large numbers. As discussed in Appendix D, these approaches do not directly lead to practical algorithms due to a certain amplification of the error (a polynomial factor of k, which is observed in practice).</p><p>Using the perturbation analysis for the tensor power method, improved sample complexity bounds can be obtained for all of the examples discussed in Section 3. The underlying analysis remains the same as in previous works (e.g., <ref type="bibr">Anandkumar et al., 2012a;</ref><ref type="bibr" target="#b46">Hsu and Kakade, 2013)</ref>, the main difference being the accuracy of the orthogonal tensor decomposition obtained via the tensor power method. Relative to the previously cited works, the sample complexity bound will be considerably improved in its dependence on the rank parameter k, as Theorem 5.1 implies that the tensor estimation error (e.g., error in estimating M 3 from Section 4.3) is not amplified by any factor explicitly depending on k (there is a requirement that the error be smaller than some factor depending on k, but this only contributes to a lower-order term in the sample complexity bound). See Appendix D for further discussion regarding the stability of the techniques from these previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Other Perspectives</head><p>The tensor power method is simply one approach for extracting the orthogonal decomposition needed in parameter estimation. The characterizations from Section 4.2 suggest that a number of fixed point and variational techniques may be possible (and Appendix D provides yet another perspective based on simultaneous diagonalization). One important consideration is that the model is often misspecified, and therefore approaches with more robust guarantees (e.g., for convergence) are desirable. Our own experience with the tensor power method (as applied to exchangeable topic modeling) is that while model misspecification does indeed affect convergence, the results can be very reasonable even after just a dozen or so iterations <ref type="bibr">(Anandkumar et al., 2012a)</ref>. Nevertheless, robustness is likely more important in other applications, and thus the stabilization approaches <ref type="bibr" target="#b54">(Kofidis and Regalia, 2002;</ref><ref type="bibr" target="#b75">Regalia and Kofidis, 2003;</ref><ref type="bibr" target="#b38">Erdogan, 2009;</ref><ref type="bibr" target="#b56">Kolda and Mayo, 2011</ref>) may be advantageous.</p><p>1. The set of θ ∈ R n which do not converge to some v i under repeated iteration of (6) has measure zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The set of robust eigenvectors of</head><formula xml:id="formula_79">T is {v 1 , v 2 , . . . , v k }.</formula><p>Proof For a random choice of θ ∈ R n (under any distribution absolutely continuous with respect to Lebesgue measure), the values |λ 1 v 1 θ|, |λ 2 v 2 θ|, . . . , |λ k v k θ| will be distinct with probability 1. Therefore, there exists a unique largest value, say |λ i v i θ| for some i ∈ [k], and by Lemma 5.1, we have convergence to v i under repeated iteration of ( <ref type="formula" target="#formula_54">6</ref>). Thus the first claim holds.</p><p>We now prove the second claim. First, we show that every v i is a robust eigenvector. Pick any i ∈ [k], and note that for a sufficiently small ball around v i , we have that for all θ in this ball, λ i v i θ is strictly greater than λ j v j θ for j ∈ [k] \ {i}. Thus by Lemma 5.1, v i is a robust eigenvector. Now we show that the v i are the only robust eigenvectors. Suppose there exists some robust eigenvector u not equal to v i for any i ∈ [k]. Then there exists a positive measure set around u such that all points in this set converge to u under repeated iteration of ( <ref type="formula" target="#formula_54">6</ref>). This contradicts the first claim. 1. The stationary points are eigenvectors of T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A stationary point u is an isolated local maximizer if and only if</head><formula xml:id="formula_80">u = v i for some i ∈ [k].</formula><p>Proof Consider the Lagrangian form of the corresponding constrained maximization problem over unit vectors u ∈ R n :</p><formula xml:id="formula_81">L(u, λ) := T (u, u, u) - 3 2 λ(u u -1). Since ∇ u L(u, λ) = ∇ u k i=1 λ i (v i u) 3 - 3 2 λ(u u -1) = 3 T (I, u, u) -λu ,</formula><p>the stationary points u ∈ R n (with u ≤ 1) satisfy</p><formula xml:id="formula_82">T (I, u, u) = λu</formula><p>for some λ ∈ R, i.e., (u, λ) is an eigenvector/eigenvalue pair of T . Now we characterize the isolated local maximizers. Observe that if u = 0 and T (I, u, u) = λu for λ &lt; 0, then T (u, u, u) &lt; 0. Therefore u = (1 -δ)u for any δ ∈ (0, 1) satisfies T (u , u , u ) = (1 -δ) 3 T (u, u, u) &gt; T (u, u, u). So such a u cannot be a local maximizer.</p><p>Moreover, if u &lt; 1 and T (I, u, u) = λu for λ &gt; 0, then u = (1 + δ)u for a small enough δ ∈ (0, 1) satisfies u ≤ 1 and T (u , u , u ) = (1 + δ) 3 T (u, u, u) &gt; T (u, u, u). Therefore a local maximizer must have T (I, u, u) = λu for some λ ≥ 0, and u = 1 whenever λ &gt; 0.</p><p>Extend {v 1 , v 2 , . . . , v k } to an orthonormal basis {v 1 , v 2 , . . . , v n } of R n . Now pick any stationary point u = n i=1 c i v i with λ := T (u, u, u) = u T (I, u, u). Then</p><formula xml:id="formula_83">λ i c 2 i = λ i (u v i ) 2 = v i T (I, u, u) = λv i u = λc i ≥ 0, i ∈ [k],</formula><p>and thus</p><formula xml:id="formula_84">∇ 2 u L(u, λ) = 6 k i=1 λ i c i v i v i -3λI = 3λ 2 i∈Ω v i v i -I</formula><p>where Ω := {i ∈ [k] : c i = 0}. This implies that for any unit vector</p><formula xml:id="formula_85">w ∈ R n , w ∇ 2 u L(u, λ)w = 3λ 2 i∈Ω (v i w) 2 -1 .</formula><p>The point u is an isolated local maximum if the above quantity is strictly negative for all unit vectors w orthogonal to u. We now consider three cases depending on the cardinality of Ω and the sign of λ.</p><p>• Case 1: |Ω| = 1 and λ &gt; 0. This means u = v i for some i ∈ [k] (as u = -v i implies λ = -λ i &lt; 0). In this case,</p><formula xml:id="formula_86">w ∇ 2 u L(u, λ)w = 3λ i (2(v i w) 2 -1) = -3λ i &lt; 0 for all w ∈ R n satisfying (u w) 2 = (v i w) 2 = 0.</formula><p>Hence u is an isolated local maximizer.</p><p>• Case 2: |Ω| ≥ 2 and λ &gt; 0. Since |Ω| ≥ 2, we may pick a strict non-empty subset S Ω and set</p><formula xml:id="formula_87">w := 1 Z 1 Z S i∈S c i v i - 1 Z S c i∈Ω\S c i v i</formula><p>where Z S := i∈S c 2 i , Z S c := i∈Ω\S c 2 i , and Z := 1/Z S + 1/Z S c . It is easy to check that w 2 = i∈Ω (v i w) 2 = 1 and u w = 0. Consider any open neighborhood U of u, and pick δ &gt; 0 small enough so that ũ :</p><formula xml:id="formula_88">= √ 1 -δ 2 u + δw is contained in U . Set u 0 := √ 1 -δ 2 u</formula><p>. By Taylor's theorem, there exists ∈ [0, δ] such that, for ū := u 0 + w, we have</p><formula xml:id="formula_89">T (ũ, ũ, ũ) = T (u 0 , u 0 , u 0 ) + ∇ u T (u, u, u) (ũ -u 0 ) u=u 0 + 1 2 (ũ -u 0 ) ∇ 2 u T (u, u, u)(ũ -u 0 ) u=ū = (1 -δ 2 ) 3/2 λ + δ(1 -δ 2 )λu w + 1 2 δ 2 w ∇ 2 u T (u, u, u)w u=ū = (1 -δ 2 ) 3/2 λ + 0 + 3δ 2 k i=1 λ i (v i (u 0 + w))(v i w) 2 = (1 -δ 2 ) 3/2 λ + 3δ 2 1 -δ 2 k i=1 λ i c i (v i w) 2 + 3δ 2 k i=1 λ i (v i w) 3 = (1 -δ 2 ) 3/2 λ + 3δ 2 1 -δ 2 λ i∈Ω (v i w) 2 + 3δ 2 k i=1 λ i (v i w) 3 = (1 -δ 2 ) 3/2 λ + 3δ 2 1 -δ 2 λ + 3δ 2 k i=1 λ i (v i w) 3 = 1 - 3 2 δ 2 + O(δ 4 ) λ + 3δ 2 1 -δ 2 λ + 3δ 2 k i=1 λ i (v i w) 3 .</formula><p>Since ≤ δ, for small enough δ, the RHS is strictly greater than λ. This implies that u is not an isolated local maximizer.</p><p>• Case 3: |Ω| = 0 or λ = 0. Note that if |Ω| = 0, then λ = 0, so we just consider λ = 0. Consider any open neighborhood U of u, and pick j ∈ [n] and δ &gt; 0 small enough so that ũ := √ 1 -δ 2 u + δv j is contained in U . Then</p><formula xml:id="formula_90">T (ũ, ũ, ũ) = (1 -δ 2 ) 3/2 T (u, u, u) + 3λ j (1 -δ 2 )δc 2 j + 3λ i 1 -δ 2 δ 2 c j + δ 3 &gt; 0 = λ</formula><p>for sufficiently small δ. Thus u is not an isolated local maximizer.</p><p>From these exhaustive cases, we conclude that a stationary point u is an isolated local maximizer if and only if</p><formula xml:id="formula_91">u = v i for some i ∈ [k].</formula><p>We are grateful to Hanzhang Hu, Drew Bagnell, and Martial Hebert for alerting us of an issue with our original statement of Theorem 4.2 and its proof, and for suggesting a simple fix. The original statement used the optimization constraint u = 1 (rather than u ≤ 1), but the characterization of the decomposition with this constraint is then only given by isolated local maximizers u with the additional constraint that T (u, u, u) &gt; 0-that is, there can be isolated local maximizers with T (u, u, u) ≤ 0 that are not vectors in the decomposition. The suggested fix of Hu, Bagnell, and Herbert is to relax to u ≤ 1, which eliminates isolated local maximizers with T (u, u, u) ≤ 0; this way, the characterization of the decomposition is simply the isolated local maximizers under the relaxed constraint.</p><p>It suffices to show that with probability at least 1/2, there is a column j</p><formula xml:id="formula_92">* ∈ [L] such that |Z 1,j * | ≥ 1 1 -γ max i∈[k]\{1} |Z i,j * |. Since max j∈[L] |Z 1,j | is a 1-Lipschitz function of L independent N (0, 1) random variables, it follows that Pr max j∈[L] |Z 1,j | -median max j∈[L] |Z 1,j | &gt; 2 ln(8) ≤ 1/4. Moreover, median max j∈[L] |Z 1,j | ≥ median max j∈[L]</formula><p>Z 1,j =: m.</p><p>Observe that the cumulative distribution function of max j∈[L] Z 1,j is given by</p><formula xml:id="formula_93">F (z) = Φ(z) L , where Φ is the standard Gaussian CDF. Since F (m) = 1/2, it follows that m = Φ -1 (2 -1/L ).</formula><p>It can be checked that</p><formula xml:id="formula_94">Φ -1 (2 -1/L ) ≥ 2 ln(L) - ln(ln(L)) + c 2 2 ln(L)</formula><p>for some absolute constant c &gt; 0. Also, let j * := arg max j∈</p><formula xml:id="formula_95">[L] |Z 1,j |. Now for each j ∈ [L], let |Z 2:k,j | := max{|Z 2,j |, |Z 3,j |, . . . , |Z k,j |}. Again, since |Z 2:k,j | is a 1-Lipschitz function of k -1 independent N (0, 1) random variables, it follows that Pr |Z 2:k,j | &gt; E |Z 2:k,j | + 2 ln(4) ≤ 1/4.</formula><p>Moreover, by a standard argument,</p><formula xml:id="formula_96">E |Z 2:k,j | ≤ 2 ln(k). Since |Z 2:k,j | is independent of |Z 1,j | for all j ∈ [L]</formula><p>, it follows that the previous two displayed inequalities also hold with j replaced by j * .</p><p>Therefore we conclude with a union bound that with probability at least 1/2,</p><formula xml:id="formula_97">|Z 1,j * | ≥ 2 ln(L) - ln(ln(L)) + c 2 2 ln(L) -2 ln(8) and |Z 2:k,j * | ≤ 2 ln(k) + 2 ln(4).</formula><p>Since L satisfies (10) by assumption, in this event, the j * -th random vector is γ-separated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Tensor Power Iterations</head><p>Recall the update rule used in the power method. Let θ t = k i=1 θ i,t v i ∈ R k be the unit vector at time t. Then</p><formula xml:id="formula_98">θ t+1 = k i=1 θ i,t+1 v i := T (I, θ t , θ t )/ T (I, θ t , θ t ) .</formula><p>In this subsection, we assume that T has the form</p><formula xml:id="formula_99">T = k i=1 λi v ⊗3 i + Ẽ (11)</formula><p>where {v 1 , v 2 , . . . , v k } is an orthonormal basis, and, without loss of generality,</p><formula xml:id="formula_100">λ1 |θ 1,t | = max i∈[k] λi |θ i,t | &gt; 0.</formula><p>Also, define</p><formula xml:id="formula_101">λmin := min{ λi : i ∈ [k], λi &gt; 0}, λmax := max{ λi : i ∈ [k]}.</formula><p>We further assume the error Ẽ is a symmetric tensor such that, for some constant p &gt; 1,</p><formula xml:id="formula_102">Ẽ(I, u, u) ≤ ˜ , ∀u ∈ S k-1 ; (12) Ẽ(I, u, u) ≤ ˜ /p, ∀u ∈ S k-1 s.t. (u v 1 ) 2 ≥ 1 -(3˜ / λ1 ) 2 . (<label>13</label></formula><formula xml:id="formula_103">)</formula><p>In the next two propositions (Propositions B.1 and B.2) and next two lemmas (Lemmas B.2 and B.3), we analyze the power method iterations using T at some arbitrary iterate θ t using only the property (12) of Ẽ. But throughout, the quantity ˜ can be replaced by ˜ /p if θ t satisfies (θ t v 1 ) 2 ≥ 1 -(3˜ / λ1 ) 2 as per property (13). Define</p><formula xml:id="formula_104">R τ := θ 2 1,τ 1 -θ 2 1,τ 1/2 , r i,τ := λ1 θ 1,τ λi |θ i,τ | , γ τ := 1 - 1 min i =1 |r i,τ | , δ τ := ˜ λ1 θ 2 1,τ , κ := λmax λ1<label>(14)</label></formula><p>for τ ∈ {t, t + 1}.</p><formula xml:id="formula_105">Proposition B.1 min i =1 |r i,t | ≥ R t κ , γ t ≥ 1 - κ R t , θ 2 1,t = R 2 t 1 + R 2 t . Proposition B.2 r i,t+1 ≥ r 2 i,t • 1 -δ t 1 + κδ t r 2 i,t = 1 -δ t 1 r 2 i,t + κδ t , i ∈ [k],<label>(15)</label></formula><formula xml:id="formula_106">R t+1 ≥ R t • 1 -δ t 1 -γ t + δ t R t ≥ 1 -δ t κ R 2 t + δ t . (<label>16</label></formula><formula xml:id="formula_107">)</formula><p>Proof Let θt+1 := T (I, θ t , θ t ), so</p><formula xml:id="formula_108">θ t+1 = θt+1 / θt+1 . Since θi,t+1 = T (v i , θ t , θ t ) = T (v i , θ t , θ t ) + E(v i , θ t , θ t ), we have θi,t+1 = λi θ 2 i,t + E(v i , θ t , θ t ), i ∈ [k].</formula><p>Using the triangle inequality and the fact</p><formula xml:id="formula_109">E(v i , θ t , θ t ) ≤ ˜ , we have θi,t+1 ≥ λi θ 2 i,t -˜ ≥ |θ i,t | • λi |θ i,t | -˜ /|θ i,t |<label>(17)</label></formula><p>and</p><formula xml:id="formula_110">| θi,t+1 | ≤ | λi θ 2 i,t | + ˜ ≤ |θ i,t | • λi |θ i,t | + ˜ /|θ i,t |<label>(18)</label></formula><p>for all i ∈ [k]. Combining ( <ref type="formula" target="#formula_109">17</ref>) and ( <ref type="formula" target="#formula_110">18</ref>) gives</p><formula xml:id="formula_111">r i,t+1 = λ1 θ 1,t+1 λi |θ i,t+1 | = λ1 θ1,t+1 λi | θi,t+1 | ≥ r 2 i,t • 1 -δ t 1 + ˜ λi θ 2 i,t = r 2 i,t • 1 -δ t 1 + ( λi / λ1 )δ t r 2 i,t ≥ r 2 i,t • 1 -δ t 1 + κδ t r 2 i,t .</formula><p>Moreover, by the triangle inequality and Hölder's inequality,</p><formula xml:id="formula_112">n i=2 [ θi,t+1 ] 2 1/2 = n i=2 λi θ 2 i,t + E(v i , θ t , θ t ) 2 1/2 ≤ n i=2 λ2 i θ 4 i,t 1/2 + n i=2 E(v i , θ t , θ t ) 2 1/2 ≤ max i =1 λi |θ i,t | n i=2 θ 2 i,t 1/2 + ˜ = (1 -θ 2 1,t ) 1/2 • max i =1 λi |θ i,t | + ˜ /(1 -θ 2 1,t ) 1/2 .<label>(19)</label></formula><p>Combining ( <ref type="formula" target="#formula_109">17</ref>) and ( <ref type="formula" target="#formula_112">19</ref>) gives</p><formula xml:id="formula_113">|θ 1,t+1 | (1 -θ 2 1,t+1 ) 1/2 = | θ1,t+1 | n i=2 [ θi,t+1 ] 2 1/2 ≥ |θ 1,t | (1 -θ 2 1,t ) 1/2 • λ1 |θ 1,t | -˜ /|θ 1,t | max i =1 λi |θ i,t | + ˜ /(1 -θ 2 1,t ) 1/2 .</formula><p>In terms of R t+1 , R t , γ t , and δ t , this reads</p><formula xml:id="formula_114">R t+1 ≥ 1 -δ t (1 -γ t ) 1-θ 2 1,t θ 2 1,t 1/2 + δ t = R t • 1 -δ t 1 -γ t + δ t R t = 1 -δ t 1-γt Rt + δ t ≥ 1 -δ t κ R 2 t + δ t</formula><p>where the last inequality follows from Proposition B.1.</p><formula xml:id="formula_115">Lemma B.2 Fix any ρ &gt; 1. Assume 0 ≤ δ t &lt; min 1 2(1 + 2κρ 2 ) , 1 -1/ρ 1 + κρ and γ t &gt; 2(1 + 2κρ 2 )δ t . 1. If r 2 i,t ≤ 2ρ 2 , then r i,t+1 ≥ |r i,t | 1 + γt 2 . 2. If ρ 2 &lt; r 2 i,t , then r i,t+1 ≥ min{r 2 i,t /ρ, 1-δt-1/ρ κδt }. 3. γ t+1 ≥ min{γ t , 1 -1/ρ}. 4. If min i =1 r 2 i,t &gt; (ρ(1 -δ t ) -1)/(κδ t ), then R t+1 &gt; 1-δt-1/ρ κδt • λmin λ1 • 1 √ k . 5. If R t ≤ 1 + 2κρ 2 , then R t+1 ≥ R t 1 + γt 3 , θ 2 1,t+1 ≥ θ 2 1,t , and δ t+1 ≤ δ t .</formula><p>Proof Consider two (overlapping) cases depending on r 2 i,t .</p><p>• Case 1:</p><formula xml:id="formula_116">r 2 i,t ≤ 2ρ 2 . By (15) from Proposition B.2, r i,t+1 ≥ r 2 i,t • 1 -δ t 1 + κδ t r 2 i,t ≥ |r i,t | • 1 1 -γ t • 1 -δ t 1 + 2κρ 2 δ t ≥ |r i,t | 1 + γ t 2</formula><p>where the last inequality uses the assumption γ t &gt; 2(1 + 2κρ 2 )δ t . This proves the first claim.</p><p>• Case 2: ρ 2 &lt; r 2 i,t . We split into two sub-cases. Suppose r 2 i,t ≤ (ρ(1 -δ t ) -1)/(κδ t ). Then, by (15),</p><formula xml:id="formula_117">r i,t+1 ≥ r 2 i,t • 1 -δ t 1 + κδ t r 2 i,t ≥ r 2 i,t • 1 -δ t 1 + κδ t ρ(1-δt)-1 κδt = r 2 i,t ρ . Now suppose instead r 2 i,t &gt; (ρ(1 -δ t ) -1)/(κδ t ). Then r i,t+1 ≥ 1 -δ t κδt ρ(1-δt)-1 + κδ t = 1 -δ t -1/ρ κδ t .<label>(20)</label></formula><p>Observe that if min i =1 r 2 i,t ≤ (ρ(1 -δ t ) -1)/(κδ t ), then r i,t+1 ≥ |r i,t | for all i ∈ [k], and hence γ t+1 ≥ γ t . Otherwise we have γ t+1 &gt; 1 -κδt 1-δt-1/ρ &gt; 1 -1/ρ. This proves the third claim.</p><p>If min i =1 r 2 i,t &gt; (ρ(1 -δ t ) -1)/(κδ t ), then we may apply the inequality (20) from the second sub-case of Case 2 above to get</p><formula xml:id="formula_118">R t+1 = 1 i =1 ( λ1 / λi ) 2 /r 2 i,t+1 1/2 &gt; 1 -δ t -1/ρ κδ t • λmin λ1 • 1 √ k .</formula><p>This proves the fourth claim. Finally, for the last claim, if R t ≤ 1 + 2κρ 2 , then by ( <ref type="formula" target="#formula_106">16</ref>) from Proposition B.2 and the assumption γ</p><formula xml:id="formula_119">t &gt; 2(1 + 2κρ 2 )δ t , R t+1 ≥ R t • 1 -δ t 1 -γ t + δ t R t ≥ R t • 1 - γt 2(1+2κρ 2 ) 1 -γ t /2 ≥ R t 1 + γ t • κρ 2 1 + 2κρ 2 ≥ R t 1 + γ t 3 .</formula><p>This in turn implies that θ 2 1,t+1 ≥ θ 2 1,t via Proposition B.1, and thus δ t+1 ≤ δ t .</p><p>Lemma B.3 Assume 0 ≤ δ t &lt; 1/2 and γ t &gt; 0. Pick any β &gt; α &gt; 0 such that</p><formula xml:id="formula_120">α (1 + α)(1 + α 2 ) ≥ ˜ γ t λ1 , α 2(1 + α)(1 + β 2 ) ≥ ˜ λ1 . 1. If R t ≥ 1/α, then R t+1 ≥ 1/α. 2. If 1/α &gt; R t ≥ 1/β, then R t+1 ≥ min{R 2 t /(2κ), 1/α}.</formula><p>Proof Observe that for any c &gt; 0,</p><formula xml:id="formula_121">R t ≥ 1 c ⇔ θ 2 1,t ≥ 1 1 + c 2 ⇔ δ t ≤ (1 + c 2 )˜ λ1 . (<label>21</label></formula><formula xml:id="formula_122">)</formula><p>Now consider the following cases depending on R t .</p><p>• Case 1: R t ≥ 1/α. In this case, we have <ref type="formula" target="#formula_121">21</ref>) (with c = α) and the condition on α. Combining this with ( <ref type="formula" target="#formula_106">16</ref>) from Proposition B.2 gives</p><formula xml:id="formula_123">δ t ≤ (1 + α 2 )˜ λ1 ≤ αγ t 1 + α by (</formula><formula xml:id="formula_124">R t+1 ≥ 1 -δ t 1-γt Rt + δ t ≥ 1 -αγt 1+α (1 -γ t )α + αγt 1+α = 1 α .</formula><p>• Case 2: 1/β ≤ R t &lt; 1/α. In this case, we have</p><formula xml:id="formula_125">δ t ≤ (1 + β 2 )˜ λ1 ≤ α 2(1 + α)</formula><p>by ( <ref type="formula" target="#formula_121">21</ref>) (with c = β) and the conditions on α and β.</p><formula xml:id="formula_126">If δ t ≥ 1/(2 + R 2 t /κ), then (16) implies R t+1 ≥ 1 -δ t κ R 2 t + δ t ≥ 1 -2δ t 2δ t ≥ 1 -α 1+α α 1+α = 1 α . If instead δ t &lt; 1/(2 + R 2 t /κ), then (16) implies R t+1 ≥ 1 -δ t κ R 2 t + δ t &gt; 1 - 1 2+R 2 t /κ κ R 2 t + 1 2+R 2 t /κ = R 2 t 2κ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 Approximate Recovery of a Single Eigenvector</head><p>We now state the main result regarding the approximate recovery of a single eigenvector using the tensor power method on T . Here, we exploit the special properties of the error Ẽ-both ( <ref type="formula">12</ref>) and ( <ref type="formula" target="#formula_102">13</ref>).</p><p>Lemma B.4 There exists a universal constant C &gt; 0 such that the following holds. Let</p><formula xml:id="formula_127">i * := arg max i∈[k] λi |θ i,0 |. If ˜ &lt; γ 0 2(1 + 8κ) • λmin • θ 2 i * ,0 and N ≥ C • log(kκ) γ 0 + log log p λi * ˜ ,</formula><p>then after t ≥ N iterations of the tensor power method on tensor T as defined in (11) and satisfying (12) and (13), the final vector θ t satisfies</p><formula xml:id="formula_128">θ i * ,t ≥ 1 - 3˜ p λi * 2 , θ t -v i * ≤ 4˜ p λi * , | T (θ t , θ t , θ t ) -λi * | ≤ 27κ ˜ pλ i * 2 + 2 ˜ p .</formula><p>Proof Assume without loss of generality that i * = 1. We consider three phases: (i) iterations before the first time t such that R t &gt; 1 + 2κρ 2 = 1 + 8κ (using ρ := 2), (ii) the subsequent iterations before the first time t such that R t ≥ 1/α (where α will be defined below), and finally (iii) the remaining iterations.</p><p>We begin by analyzing the first phase, i.e., the iterates in T 1 := {t ≥ 0 : R t ≤ 1+2κρ 2 = 1 + 8κ}. Observe that the condition on ˜ implies</p><formula xml:id="formula_129">δ 0 = ˜ λ1 θ 2 1,0 &lt; γ 0 2(1 + 8κ) • λmin λ1 ≤ min γ 0 2(1 + 2κρ 2 ) , 1 -1/ρ 2(1 + 2κρ 2 ) ,</formula><p>and hence the preconditions on δ t and γ t of Lemma B.2 hold for t = 0. For all t ∈ T 1 satisfying the preconditions, Lemma B.2 implies that δ t+1 ≤ δ t and γ t+1 ≥ min{γ t , 1 -1/ρ}, so the next iteration also satisfies the preconditions. Hence by induction, the preconditions hold for all iterations in T 1 . Moreover, for all i ∈ [k], we have</p><formula xml:id="formula_130">|r i,0 | ≥ 1 1 -γ 0 ;</formula><p>and while t ∈ T 1 : (i) |r i,t | increases at a linear rate while r 2 i,t ≤ 2ρ 2 , and (ii) |r i,t | increases at a quadratic rate while ρ 2 ≤ r 2 i,t ≤ 1-δt-1/ρ κδt . (The specific rates are given, respectively, in Lemma B.2, claims 1 and 2.) Since</p><formula xml:id="formula_131">1-δt-1/ρ κδt ≤ λ1 2κ˜ , it follows that min i =1 r 2 i,t ≤ 1-δt-1/ρ κδt for at most 2 γ 0 ln 2ρ 2 1 1-γ 0 + ln ln λ1 2κ˜ ln √ 2 = O 1 γ 0 + log log λ1 ˜ (22)</formula><p>iterations in T 1 . As soon as min i =1 r 2 i,t &gt; 1-δt-1/ρ κδt , we have that in the next iteration,</p><formula xml:id="formula_132">R t+1 &gt; 1 -δ t -1/ρ κδ t • λmin λ1 • 1 √ k ≥ 7 √ k ;</formula><p>and all the while R t is growing at a linear rate (given in Lemma B.2, claim 5). Therefore, there are at most an additional</p><formula xml:id="formula_133">1 + 3 γ 0 ln 1 + 8κ 7/ √ k = O log(kκ) γ 0 (<label>23</label></formula><formula xml:id="formula_134">)</formula><p>iterations in T 1 over that counted in <ref type="bibr">(22)</ref>. Therefore, by combining the counts in ( <ref type="formula">22</ref>) and ( <ref type="formula" target="#formula_133">23</ref>), we have that the number of iterations in the first phase satisfies</p><formula xml:id="formula_135">|T 1 | = O log log λ1 ˜ + log(kκ) γ 0 .</formula><p>We now analyze the second phase, i.e., the iterates in</p><formula xml:id="formula_136">T 2 := {t ≥ 0 : t / ∈ T 1 , R t &lt; 1/α}. Define α := 3˜ λ1 , β := 1 1 + 2κρ 2 = 1 1 + 8κ</formula><p>.</p><p>Note that for the initial iteration t := min T 2 , we have that R t ≥ 1 + 2κρ After R t ≥ 1/α (for t := max(T 1 ∪ T 2 ) + 1), we have</p><formula xml:id="formula_137">θ 2 1,t ≥ 1/α 2 1 + 1/α 2 ≥ 1 -α 2 ≥ 1 - 3˜ λ1 2 .</formula><p>Therefore, the vector θ t satisfies the condition for property (13) of Ẽ to hold. Now we apply Lemma B.3 using ˜ /p in place of ˜ , including in the definition of δ t (which we call δ t ):</p><formula xml:id="formula_138">δ t := ˜ p λ1 θ 2 1,t ;</formula><p>we also replace α and β with α and β, which we set to</p><formula xml:id="formula_139">α := 3˜ p λ1 , β := 3˜ λ1 .</formula><p>It can be checked that δ t ∈ (0, 1/2),</p><formula xml:id="formula_140">γ t ≥ 1 -3˜ κ/λ 1 &gt; 0, α (1 + α)(1 + α 2 ) ≥ ˜ p(1 -3˜ κ/λ 1 ) λ1 ≥ ˜ pγ t λ1 , α 2(1 + α)(1 + β 2 ) ≥ ˜ p λ1 .</formula><p>Therefore, the preconditions of Lemma B.3 are satisfied for the initial iteration t in this final phase, and by the same arguments as before, the preconditions hold for all subsequent iterations t ≥ t . Initially, we have Once R t ≥ 1/α, we have</p><formula xml:id="formula_141">R t ≥ 1/α ≥ 1/β,</formula><formula xml:id="formula_142">θ 2 1,t ≥ 1 - 3˜ p λ1 2 . Since sign(θ 1,t ) = r 1,t ≥ r 2 1,t-1 • (1 -δ t-1 )/(1 + κδ t-1 r 2 1,t-1 ) = (1 -δ t-1 )/(1 + κδ t-1</formula><p>) &gt; 0 by Proposition B.2, we have θ 1,t &gt; 0. Therefore we can conclude that</p><formula xml:id="formula_143">θ t -v 1 = 2(1 -θ 1,t ) ≤ 2 1 -1 -(3˜ /(p λ1 )) 2 ≤ 4˜ /(p λ1 ). Finally, | T (θ t , θ t , θ t ) -λ1 | = λ1 (θ 3 1,t -1) + k i=2 λi θ 3 i,t + Ẽ(θ t , θ t , θ t ) ≤ λ1 |θ 3 1,t -1| + k i=2 λi |θ i,t |θ 2 i,t + Ẽ(I, θ t , θ t ) ≤ λ1 1 -θ 1,t + |θ 1,t (1 -θ 2 1,t )| + max i =1 λi |θ i,t | k i=2 θ 2 i,t + Ẽ(I, θ t , θ t ) ≤ λ1 1 -θ 1,t + |θ 1,t (1 -θ 2 1,t )| + max i =1 λi 1 -θ 2 1,t k i=2 θ 2 i,t + Ẽ(I, θ t , θ t ) = λ1 1 -θ 1,t + |θ 1,t (1 -θ 2 1,t )| + max i =1 λi (1 -θ 2 1,t ) 3/2 + Ẽ(I, θ t , θ t ) ≤ λ1 • 3 3˜ p λ1 2 + κ λ1 • 3˜ p λ1 3 + ˜ p ≤ (27κ • (˜ /p λ1 ) 2 + 2)˜ p .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Deflation</head><p>Lemma B.5 Fix some ˜ ≥ 0. Let {v 1 , v 2 , . . . , v k } be an orthonormal basis for R k , and λ 1 , λ 2 , . . . , λ k ≥ 0 with λ min := min i∈[k] λ i . Also, let {v 1 , v2 , . . . , vk } be a set of unit vectors in R k (not necessarily orthogonal), λ1 , λ2 , . . . , λk ≥ 0 be non-negative scalars, and define</p><formula xml:id="formula_144">E i := λ i v ⊗3 i -λi v⊗3 i , i ∈ [k]. Pick any t ∈ [k]. If | λi -λ i | ≤ ˜ , vi -v i ≤ min{ √ 2, 2˜ /λ i }</formula><p>for all i ∈ [t], then for any unit vector u ∈ S k-1 ,</p><formula xml:id="formula_145">t i=1 E i (I, u, u) 2 2 ≤ 4(5 + 11˜ /λ min ) 2 + 128(1 + ˜ /λ min ) 2 (˜ /λ min ) 2 ˜ 2 t i=1 (u v i ) 2 + 64(1 + ˜ /λ min ) 2 ˜ 2 t i=1 (˜ /λ i ) 2 + 2048(1 + ˜ /λ min ) 2 ˜ 2 t i=1 (˜ /λ i ) 3 2 .</formula><p>In particular, for any ∆ ∈ (0, 1), there exists a constant ∆ &gt; 0 (depending only on</p><formula xml:id="formula_146">∆) such that ˜ ≤ ∆ λ min / √ k implies t i=1 E i (I, u, u) 2 2 ≤ ∆ + 100 t i=1 (u v i ) 2 ˜ 2 .</formula><p>Proof For any unit vector u and i ∈ [t], the error term</p><formula xml:id="formula_147">E i (I, u, u) = λ i (u v i ) 2 v i -λi (u vi ) 2 vi</formula><p>lives in span{v i , vi }; this space is the same as span{v i , v⊥ i }, where</p><formula xml:id="formula_148">v⊥ i := vi -(v i vi )v i</formula><p>is the projection of vi onto the subspace orthogonal to v i . Since vi</p><formula xml:id="formula_149">-v i 2 = 2(1 -v i vi ), it follows that c i := v i vi = 1 -vi -v i 2 /2 ≥ 0</formula><p>(the inequality follows from the assumption vi -v i ≤ √ 2, which in turn implies 0 ≤ c i ≤ 1). By the Pythagorean theorem and the above inequality for c i ,</p><formula xml:id="formula_150">v⊥ i 2 = 1 -c 2 i ≤ vi -v i 2 .</formula><p>Later, we will also need the following bound, which is easily derived from the above inequalities and the triangle inequality:</p><formula xml:id="formula_151">|1 -c 3 i | = |1 -c i + c i (1 -c 2 i )| ≤ 1 -c i + |c i (1 -c 2 i )| ≤ 1.5 vi -v i 2 .</formula><p>We now express E i (I, u, u) in terms of the coordinate system defined by v i and v⊥ i , depicted below. Define</p><formula xml:id="formula_152">a i := u v i and b i := u v⊥ i / v⊥ i .</formula><p>(Note that the part of u living in span{v i , v⊥ i } ⊥ is irrelevant for analyzing E i (I, u, u).)</p><p>We have</p><formula xml:id="formula_153">E i (I, u, u) = λ i (u v i ) 2 v i -λi (u vi ) 2 vi = λ i a 2 i v i -λi a i c i + v⊥ i b i 2 c i v i + v⊥ i = λ i a 2 i v i -λi a 2 i c 2 i + 2 v⊥ i a i b i c i + v⊥ i 2 b 2 i c i v i -λi a i c i + v⊥ i b i 2 v⊥ i = (λ i -λi c 3 i )a 2 i -2 λi v⊥ i a i b i c 2 i -λi v⊥ i 2 b 2 i c i =:A i v i -λi v⊥ i a i c i + v⊥ i b i 2 =:B i v⊥ i / v⊥ i = A i v i -B i v⊥ i / v⊥ i .</formula><p>The overall error can also be expressed in terms of the A i and B i :</p><formula xml:id="formula_154">t i=1 E i (I, u, u) 2 2 = t i=1 A i v i - t i=1 B i (v ⊥ i / v⊥ i ) 2 2 ≤ 2 t i=1 A i v i 2 + 2 t i=1 B i (v ⊥ i / v⊥ i ) 2 2 ≤ 2 t i=1 A 2 i + 2 t i=1 |B i | 2<label>(25)</label></formula><p>where the first inequality uses the fact (x + y) 2 ≤ 2(x 2 + y 2 ) and the triangle inequality, and the second inequality uses the orthonormality of the v i and the triangle inequality. It remains to bound A 2 i and |B i | in terms of |a i |, λ i , and ˜ . The first term, A 2 i , can be bounded using the triangle inequality and the various bounds on |λ i -λi |, vi -v i , v⊥ i , and c i :</p><formula xml:id="formula_155">|A i | ≤ (|λ i -λi |c 3 i + λ i |c 3 i -1|)a 2 i + 2(λ i + |λ i -λi |) v⊥ i |a i b i |c 2 i + (λ i + |λ i -λi |) v⊥ i 2 b 2 i c i ≤ (|λ i -λi | + 1.5λ i vi -v i 2 + 2(λ i + |λ i -λi |) vi -v i )|a i | + (λ i + |λ i -λi |) vi -v i 2 ≤ (˜ + 7˜ 2 /λ i + 4˜ + 4˜ 2 /λ i )|a i | + 4˜ 2 /λ i + ˜ 3 /λ 2 i = (5 + 11˜ /λ i )˜ |a i | + 4(1 + ˜ /λ i )˜ 2 /λ i ,</formula><p>and therefore (via (x + y) 2 ≤ 2(x 2 + y 2 ))</p><formula xml:id="formula_156">A 2 i ≤ 2(5 + 11˜ /λ i ) 2 ˜ 2 a 2 i + 32(1 + ˜ /λ i ) 2 ˜ 4 /λ 2 i .</formula><p>The second term, |B i |, is bounded similarly:</p><formula xml:id="formula_157">|B i | ≤ 2(λ i + |λ i -λi |) v⊥ i 2 (a 2 i + v⊥ i 2 ) ≤ 2(λ i + |λ i -λi |) vi -v i 2 (a 2 i + vi -v i 2 ) ≤ 8(1 + ˜ /λ i )(˜ 2 /λ i )a 2 i + 32(1 + ˜ /λ i )˜ 4 /λ 3 i .</formula><p>Therefore, using the inequality from (25) and again (x + y) 2 ≤ 2(x 2 + y 2 ),</p><formula xml:id="formula_158">t i=1 E i (I, u, u) 2 2 ≤ 2 t i=1 A 2 i + 2 t i=1 |B i | 2 ≤ 4(5 + 11˜ /λ min ) 2 ˜ 2 t i=1 a 2 i + 64(1 + ˜ /λ min ) 2 ˜ 2 t i=1 (˜ /λ i ) 2 + 2 8(1 + ˜ /λ min )(˜ 2 /λ min ) t i=1 a 2 i + 32(1 + ˜ /λ min )˜ t i=1 (˜ /λ i ) 3 2 ≤ 4(5 + 11˜ /λ min ) 2 ˜ 2 t i=1 a 2 i + 64(1 + ˜ /λ min ) 2 ˜ 2 t i=1 (˜ /λ i ) 2 + 128(1 + ˜ /λ min ) 2 (˜ /λ min ) 2 ˜ 2 t i=1 a 2 i + 2048(1 + ˜ /λ min ) 2 ˜ 2 t i=1 (˜ /λ i ) 3 2 = 4(5 + 11˜ /λ min ) 2 + 128(1 + ˜ /λ min ) 2 (˜ /λ min ) 2 ˜ 2 t i=1 a 2 i + 64(1 + ˜ /λ min ) 2 ˜ 2 t i=1 (˜ /λ i ) 2 + 2048(1 + ˜ /λ min ) 2 ˜ 2 t i=1 (˜ /λ i ) 3 2 . B.4 Proof of the Main Theorem Theorem B.1 Let T = T + E ∈ R k×k×k , where T is a symmetric tensor with orthogonal decomposition T = k i=1 λ i v ⊗3 i</formula><p>where each λ i &gt; 0, {v 1 , v 2 , . . . , v k } is an orthonormal basis, and E has operator norm := E . Define λ min := min{λ i : i ∈ [k]}, and λ max := max{λ i : i ∈ [k]}. There exists universal constants C 1 , C 2 , C 3 &gt; 0 such that the following holds. Pick any η ∈ (0, 1), and suppose</p><formula xml:id="formula_159">≤ C 1 • λ min k , N ≥ C 2 • log(k) + log log λ max ,<label>and</label></formula><formula xml:id="formula_160">ln(L/ log 2 (k/η)) ln(k) • 1 - ln(ln(L/ log 2 (k/η))) + C 3 4 ln(L/ log 2 (k/η)) - ln(8) ln(L/ log 2 (k/η)) ≥ 1.02 1 + ln(4) ln(k) .</formula><p>(Note that the condition on L holds with L = poly(k) log(1/η).) Suppose that Algorithm 1 is iteratively called k times, where the input tensor is T in the first call, and in each subsequent call, the input tensor is the deflated tensor returned by the previous call. Let (v 1 , λ1 ), (v 2 , λ2 ), . . . , (v k , λk ) be the sequence of estimated eigenvector/eigenvalue pairs returned in these k calls. With probability at least 1 -η, there exists a permutation π on</p><formula xml:id="formula_161">[k] such that v π(j) -vj ≤ 8 /λ π(j) , |λ π(j) -λj | ≤ 5 , ∀j ∈ [k],</formula><p>and</p><formula xml:id="formula_162">T - k j=1 λj v⊗3 j ≤ 55 .</formula><p>Proof We prove by induction that for each i ∈ [k] (corresponding to the i-th call to Algorithm 1), with probability at least 1 -iη/k, there exists a permutation π on [k] such that the following assertions hold.</p><p>1. For all j ≤ i, v π(j) -vj ≤ 8 /λ π(j) and |λ π(j) -λj | ≤ 12 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The error tensor</head><p>Ẽi+1</p><formula xml:id="formula_163">:= T - j≤i λj v⊗3 j - j≥i+1 λ π(j) v ⊗3 π(j) = E + j≤i λ π(j) v ⊗3 π(j) -λj v⊗3 j satisfies Ẽi+1 (I, u, u) ≤ 56 , ∀u ∈ S k-1 ; (26) Ẽi+1 (I, u, u) ≤ 2 , ∀u ∈ S k-1 s.t. ∃j ≥ i + 1 (u v π(j) ) 2 ≥ 1 -(168 /λ π(j) ) 2 . (<label>27</label></formula><formula xml:id="formula_164">)</formula><p>We actually take i = 0 as the base case, so we can ignore the first assertion, and just observe that for i = 0,</p><formula xml:id="formula_165">Ẽ1 = T - k j=1 λ i v ⊗3 i = E.</formula><p>We have Ẽ1 = E = , and therefore the second assertion holds. Now fix some i ∈ [k], and assume as the inductive hypothesis that, with probability at least 1 -(i -1)η/k, there exists a permutation π such that two assertions above hold for i -1 (call this Event i-1 ). The i-th call to Algorithm 1 takes as input Ti</p><formula xml:id="formula_166">:= T - j≤i-1 λj v⊗3 j ,</formula><p>which is intended to be an approximation to</p><formula xml:id="formula_167">T i := j≥i λ π(j) v ⊗3 π(j) .</formula><p>Observe that Ti T i = Ẽi , which satisfies the second assertion in the inductive hypothesis. We may write</p><formula xml:id="formula_168">T i = k l=1 λl v ⊗3 l</formula><p>where λl = λ l whenever π -1 (l) ≥ i, and λl = 0 whenever π -1 (l) ≤ i -1. This form is used when referring to T or the λi in preceding lemmas (in particular, Lemma B.1 and Lemma B.4).</p><p>By Lemma B.1, with conditional probability at least 1 -η/k given Event i-1 , at least one of θ (τ ) 0 for τ ∈ [L] is γ-separated relative to π(j max ), where j max := arg max j≥i λ π(j) , (for γ = 0.01; call this Event i ; note that the application of Lemma B.1 determines C 3 ). Therefore Pr</p><formula xml:id="formula_169">[Event i-1 ∩ Event i ] = Pr[Event i |Event i-1 ] Pr[Event i-1 ] ≥ (1 -η/k)(1 -(i -1)η/k) ≥ 1 -iη/k. It remains to show that Event i-1 ∩ Event i ⊆ Event i ; so henceforth we condition on Event i-1 ∩ Event i .</formula><p>Set <ref type="bibr">and (ii)</ref> that by Lemma B.4 (using ˜ /p := 2 , κ := 1, and i * := π(j max ), and providing C 2 ),</p><formula xml:id="formula_170">C 1 := min (56 • 9 • 102) -1 , (100 • 168) -1 , ∆ from Lemma B.5 with ∆ = 1/50 . (<label>28</label></formula><formula xml:id="formula_171">) For all τ ∈ [L] such that θ (τ ) 0 is γ-separated relative to π(j max ), we have (i) |θ (τ ) jmax,0 | ≥ 1/ √ k,</formula><formula xml:id="formula_172">| Ti (θ (τ ) N , θ (τ ) N , θ (τ ) N ) -λ π(jmax) | ≤ 5</formula><p>(notice by definition that γ ≥ 1/100 implies γ 0 ≥ 1 -/(1 + γ) ≥ 1/101, thus it follows from the bounds on the other quantities that ˜</p><formula xml:id="formula_173">= 2p ≤ 56C 1 • λ min k &lt; γ 0 2(1+8κ) • λmin • θ 2 i * ,0 as necessary). Therefore θ N := θ (τ * ) N must satisfy Ti (θ N , θ N , θ N ) = max τ ∈[L] Ti (θ (τ ) N , θ (τ ) N , θ (τ ) N ) ≥ max j≥i λ π(j) -5 = λ π(jmax) -5 .</formula><p>On the other hand, by the triangle inequality,</p><formula xml:id="formula_174">Ti (θ N , θ N , θ N ) ≤ j≥i λ π(j) θ 3 π(j),N + | Ẽi (θ N , θ N , θ N )| ≤ j≥i λ π(j) |θ π(j),N |θ 2 π(j),N + 56 ≤ λ π(j * ) |θ π(j * ),N | + 56</formula><p>where j * := arg max j≥i λ π(j) |θ π(j),N |. Therefore</p><formula xml:id="formula_175">λ π(j * ) |θ π(j * ),N | ≥ λ π(jmax) -5 -56 ≥ 4 5 λ π(jmax) .</formula><p>Squaring both sides and using the fact that θ 2 π(j * ),N + θ 2 π(j),N ≤ 1 for any j = j * ,</p><formula xml:id="formula_176">λ π(j * ) θ π(j * ),N 2 ≥ 16 25 λ π(jmax) θ π(j * ),N 2 + 16 25 λ π(jmax) θ π(j),N 2 ≥ 16 25 λ π(j * ) θ π(j * ),N 2 + 16 25 λ π(j) θ π(j),N which in turn implies λ π(j) |θ π(j),N | ≤ 3 4 λ π(j * ) |θ π(j * ),N |, j = j * .</formula><p>This means that θ N is (1/4)-separated relative to π(j * ). Also, observe that</p><formula xml:id="formula_177">|θ π(j * ),N | ≥ 4 5 • λ π(jmax) λ π(j * ) ≥ 4 5 , λ π(jmax) λ π(j * ) ≤ 5 4 .</formula><p>Therefore by Lemma B.4 (using ˜ /p := 2 , γ := 1/4, and κ := 5/4), executing another N power iterations starting from θ N gives a vector θ that satisfies</p><formula xml:id="formula_178">θ -v π(j * ) ≤ 8 λ π(j * ) , | λ -λ π(j * ) | ≤ 5 .</formula><p>Since vi = θ and λi = λ, the first assertion of the inductive hypothesis is satisfied, as we can modify the permutation π by swapping π(i) and π(j * ) without affecting the values of {π(j) : j ≤ i -1} (recall j * ≥ i).</p><p>We now argue that Ẽi+1 has the required properties to complete the inductive step. By Lemma B.5 (using ˜ := 5 and ∆ := 1/50, the latter providing one upper bound on C 1 as per ( <ref type="formula" target="#formula_170">28</ref>)), we have for any unit vector u</p><formula xml:id="formula_179">∈ S k-1 , j≤i λ π(j) v ⊗3 π(j) -λj v⊗3 j (I, u, u) ≤ 1/50 + 100 i j=1 (u v π(j) ) 2 1/2 5 ≤ 55 .<label>(29)</label></formula><p>Therefore by the triangle inequality,</p><formula xml:id="formula_180">Ẽi+1 (I, u, u) ≤ E(I, u, u) + j≤i λ π(j) v ⊗3 π(j) -λj v⊗3 j (I, u, u) ≤ 56 .</formula><p>Thus the bound (26) holds.</p><p>To prove that (27) holds, pick any unit vector u ∈ S k-1 such that there exists j ≥ i + 1 with (u v π(j ) ) 2 ≥ 1 -(168 /λ π(j ) ) 2 . We have, via the second bound on C 1 in (28) and the corresponding assumed bound</p><formula xml:id="formula_181">≤ C 1 • λ min k , 100 i j=1 (u v π(j) ) 2 ≤ 100 1 -(u v π(j ) ) 2 ≤ 100 168 λ π(j ) 2 ≤ 1 50 ,</formula><p>and therefore</p><formula xml:id="formula_182">1/50 + 100 i j=1 (u v π(j) ) 2 1/2 5 ≤ (1/50 + 1/50) 1/2 5 ≤ .</formula><p>By the triangle inequality, we have Ẽi+1 (I, u, u) ≤ 2 . Therefore (27) holds, so the second assertion of the inductive hypothesis holds. Thus Event i-1 ∩ Event i ⊆ Event i , and Pr[Event i ] ≥ Pr[Event i-1 ∩ Event i ] ≥ 1 -iη/k. We conclude that by the induction principle, there exists a permutation π such that two assertions hold for i = k, with probability at least 1 -η.</p><p>From the last induction step (i = k), it is also clear from (29) that T -k j=1 λj v⊗3 j ≤ 55 (in Event k-1 ∩ Event k ). This completes the proof of the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Variant of Robust Power Method that uses a Stopping Condition</head><p>In this section we analyze a variant of Algorithm 1 that uses a stopping condition. The variant is described in Algorithm 2. The key difference is that the inner for-loop is repeated until a stopping condition is satisfied (rather than explicitly L times). The stopping condition ensures that the power iteration is converging to an eigenvector, and it will be satisfied within poly(k) random restarts with high probability. The condition depends on one new quantity, r, which should be set to r := k -# deflation steps so far (i.e., the first call to Algorithm 2 uses r = k, the second call uses r = k -1, and so on).</p><p>Algorithm 2 Robust tensor power method with stopping condition input symmetric tensor T ∈ R k×k×k , number of iterations N , expected rank r. output the estimated eigenvector/eigenvalue pair; the deflated tensor. 1: repeat 2:</p><p>Draw θ 0 uniformly at random from the unit sphere in R k .</p><p>3:</p><p>for t = 1 to N do 4:</p><p>Compute power iteration update θ t := T (I, θ t-1 , θ t-1 ) T (I, θ t-1 , θ t-1 ) (30)</p><p>5:</p><p>end for 6: until the following stopping condition is satisfied:</p><formula xml:id="formula_183">| T (θ N , θ N , θ N )| ≥ max 1 2 √ r T F , 1 1.05</formula><p>T (I, I, θ N ) F .</p><p>7: Do N power iteration updates (30) starting from θ N to obtain θ, and set λ := T ( θ, θ, θ). 8: return the estimated eigenvector/eigenvalue pair ( θ, λ); the deflated tensor Tλ θ⊗3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Stopping Condition Analysis</head><p>For a matrix A, we use A F := ( i,j A 2 i,j ) 1/2 to denote its Frobenius norm. For a thirdorder tensor A, we use A F := ( i A(I, I, e i ) 2 F ) 1/2 = ( i A(I, I, v i ) 2 F ) 1/2 . Define T as before in (11):</p><formula xml:id="formula_184">T := k i=1 λi v ⊗3 i + Ẽ. Moreover, T F ≥ k i=1 λi v ⊗3 i F -Ẽ F = k j=1 k i=1 λi v i v i (v i v j ) 2 F 1/2 -Ẽ F = k j=1 λj v j v j 2 F 1/2 -Ẽ F = k j=1 λ2 j 1/2 -Ẽ F ≥ √ λavg -˜ F .</formula><p>By assumption, | T (θ, θ, θ)| ≥ (β/ √ ) T F , so Applying these bounds on ˜ to (33), we obtain</p><formula xml:id="formula_185">λ1 |θ 1 | ≥ β λavg - β √ ˜ F -˜ ≥ β λavg -β 1 2 - α β √ k λavg - α √ k</formula><formula xml:id="formula_186">λ1 |θ 1 | ≥ 1 -α (1 + α) 2 k i=1 λ2 i θ 2 i 1/2 ≥ 1 -α (1 + α) 2 λ2 1 θ 2 1 + max i =1 λ2 i θ 2 i 1/2</formula><p>which in turn implies (for α ∈ (0, 1/20))</p><formula xml:id="formula_187">max i =1 λ2 i θ 2 i ≤ (1 + α) 4 (1 -α) 2 -1 • λ2 1 θ 2 1 ≤ 7α • λ2 1 θ 2 1 .</formula><p>Therefore max i =1 λi |θ i | ≤ √ 7α • λ1 |θ 1 |, proving the second claim. Now we prove the final claim. This is done by <ref type="bibr">(i)</ref> showing that θ has a large projection onto u 1 , (ii) using an SVD perturbation argument to show that ±u 1 is close to v 1 , and (iii) concluding that θ has a large projection onto v 1 .</p><p>We begin by showing that (u 1 θ) 2 is large. Observe that from (32), we have (1 + α) 2 φ 2 1 ≥ M 2 F ≥ φ 2 1 + max i =1 φ 2 i , and therefore</p><formula xml:id="formula_188">max i =1 |φ i | ≤ 2α + α 2 • |φ 1 |.</formula><p>Moreover, by the triangle inequality,</p><formula xml:id="formula_189">|θ M θ| ≤ k i=1 |φ i |(u i θ) 2 ≤ |φ 1 |(u 1 θ) 2 + max i =1 |φ i | 1 -(u 1 θ) 2 = (u 1 θ) 2 |φ 1 | -max i =1 |φ i | + max i =1 |φ i |.</formula><p>Using (32) once more, we have |θ M θ| ≥ M F /(1 + α) ≥ |φ 1 |/(1 + α), so</p><formula xml:id="formula_190">(u 1 θ) 2 ≥ 1 1+α -max i =1 |φ i | |φ 1 | 1 -max i =1 |φ i | |φ 1 | = 1 - α (1 + α) 1 -max i =1 |φ i | |φ 1 | ≤ 1 - α (1 + α)(1 - √ 2α + α 2</formula><p>) .</p><p>Now we show that (u 1 v 1 ) 2 is also large. By the second claim, the assumption on ˜ , and (34),</p><formula xml:id="formula_191">λ1 |θ 1 | -max i =1 λi |θ i | &gt; (1 - √ 7α) • λ1 |θ 1 | ≥ (1 - √ 7α) • λmin / √ k.</formula><p>Combining this with Weyl's theorem gives</p><formula xml:id="formula_192">|φ 1 | -max i =1 λi |θ i | ≥ λ1 |θ 1 | -˜ -max i =1 λi |θ i | ≥ (1 -(α + √ 7α)) • λmin / √ k,</formula><p>so we may apply Wedin's theorem to obtain (u 1 v 1 ) 2 ≥ 1 -Ẽ(I, I, θ)</p><formula xml:id="formula_193">|φ 1 | -max i =1 λi |θ i | 2 ≥ 1 - α 1 -(α + √ 7α) 2 .</formula><p>It remains to show that θ 1 = v 1 θ is large. Indeed, by the triangle inequality, Cauchy-Schwarz, and the above inequalities on (u 1 v 1 ) 2 and (u 1 θ) 2 , where the last step uses the fact that θ 2 1,t ≥ 1 -(3˜ /(p λ1 )) 2 . Moreover, T (θ t , θ t , θ t ) ≥ λ1 -27 ˜ pλ 1 2 + 2 ˜ p .</p><formula xml:id="formula_194">|v 1 θ| = k i=1 (u i v 1 )(u i θ) ≥ |u 1 v 1 ||u 1 θ| - k i=2 |u i v 1 ||u i θ| ≥ |u 1 v 1 ||u 1 θ| - k i=2 (u i v 1 ) 2 1/2 k i=2 (u i θ) 2 1/2 = |u 1 v 1 ||u 1 θ| - 1 -(u i v 1 ) 2 1 -(u i θ) 2 1/2 ≥ 1 - α (1 + α)(1 - √ 2α + α 2 ) 1 - α 1 -(α + √ 7α) 2 1/2 - α (1 + α)(1 - √ 2α + α 2 ) • α 1 -(α + √ 7α)</formula><p>Combining these two inequalities with the assumption on ˜ implies that T (θ t , θ t , θ t ) ≥ 1 1 + α T (I, I, θ t ) F . 2)  . . . . . . . . . . . .</p><formula xml:id="formula_195">columns of      µ 1 η (1) µ 2 η (1) • • • µ k η (1) µ 1 η (2) µ 2 η (2) • • • µ k η (</formula><formula xml:id="formula_196">µ 1 η (m) µ 2 η (m) • • • µ k η (m)     </formula><p>are distinct (i.e., for each pair of column indices i, j, there exists a row index r such that the (r, i)-th and (r, j)-th entries are distinct). This is a much weaker requirement for uniqueness, and therefore may translate to an improved perturbation analysis. In fact, using the techniques discussed in Section 4.3, we may even reduce the problem to an orthogonal simultaneous diagonalization, which may be easier to obtain. Furthermore, a number of robust numerical methods for (approximately) simultaneously diagonalizing collections of matrices have been proposed and used successfully in the literature (e.g., <ref type="bibr" target="#b17">Bunse-Gerstner et al., 1993;</ref><ref type="bibr" target="#b21">Cardoso and Souloumiac, 1993;</ref><ref type="bibr" target="#b19">Cardoso, 1994;</ref><ref type="bibr" target="#b20">Cardoso and Comon, 1996;</ref><ref type="bibr" target="#b86">Ziehe et al., 2004)</ref>. Another alternative and a more stable approach compared to full diagonalization is a Schur-like method which finds a unitary matrix U which simultaneously triangularizes the respective matrices <ref type="bibr" target="#b30">(Corless et al., 1997)</ref>. It is an interesting open question whether these techniques can yield similar improved learnability results and also enjoy the attractive computational properties of the tensor power method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of latent variable models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Condition 4.1 (Non-degeneracy) The vectors µ 1 , µ 2 , . . . , µ k ∈ R d are linearly independent, and the scalars w 1 , w 2 , . . . , w k &gt; 0 are strictly positive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Let τ * := arg max τ ∈[L] { T (θ Do N power iteration updates (7) starting from θ (τ * ) N</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>A. 2</head><label>2</label><figDesc>Proof of Theorem 4.2 Theorem A.2 Let T have an orthogonal decomposition as given in (4), and consider the optimization problem max u∈R n T (u, u, u) s.t. u ≤ 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>inequality follows from the assumptions on ˜ and ˜ F . Since β &gt; 0, λavg &gt; 0, and|θ 1 | ≤ 1, it follows that λ1 ≥ β 2 λavg , λ1 |θ 1 | &gt; 0.This proves the first claim. Now we prove the second claim. Define M := T (I, I, θ) = k i=1 λi θ i v i v i + Ẽ(I, I, θ) (a symmetric k × k matrix), and consider its eigenvalue decompositionM = k i=1 φ i u i u i where, without loss of generality, |φ 1 | ≥ |φ 2 | ≥ • • • ≥ |φ k | and {u 1 , u 2 , . . . , u k } is an orthonormal basis. Let M := k i=1 λi θ i v i v i , so M = M + Ẽ(I, I, θ). Note that the λi |θ i | and |φ i | are the singular values of M and M , respectively. We now show that the assumption on | T (θ, θ, θ)| implies that almost all of the energy in M is contained in its top singular component.By Weyl's theorem,|φ 1 | ≤ λ1 |θ 1 | + M -M ≤ λ1 |θ 1 | + ˜ .Next, observe that the assumption T (I, I, θ) F ≤ (1 + α) T (θ, θ, θ) is equivalent to (1 + α)θ M θ ≥ M F .Therefore, using the fact that |φ 1 | = max u∈S k-1 |u M u|, the triangle inequality, and the fact A F ≤ √ k A for any matrix A ∈ R k×k , (1 + α)|φ 1 | ≥ (1 + α)θ M θ ≥ M F λ1 |θ 1 | &gt; 0 (by the first claim) and λ1 |θ 1 | = max i∈[k] λi |θ i |, it follows that λ1 |θ 1 | ≥ λmin max have ˜ ≤ α λ1 |θ 1 |.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>≥ 1</head><label>1</label><figDesc>-2α for α ∈ (0, 1/20). Moreover, by assumption we have T (θ, θ, θ) ≥ 02α)3  (since|θ 1 | ≥ 1 -2α) &lt; λ1 |θ 1 | 3 sign(θ 1 ) + 1 so sign(θ 1 ) &gt; -1, meaning θ 1 &gt; 0. Therefore θ 1 = |θ 1 | ≥ 1 -2α. This proves the final claim. Lemma C.2 Fix α, β ∈ (0, 1). Assume λi * = max i∈[k] λi andTo the conclusion of Lemma B.4, it can be added that the stopping condition (31) is satisfied by θ = θ t .Proof Without loss of generality, assume i * = 1. By the triangle inequality and Cauchy-Schwarz,T (I, I, θ t ) F ≤ λ1 |θ 1,t | + i =1 λ i |θ i,t | + Ẽ(I, I, θ t ) F ≤ λ1 |θ 1,t | + λ1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>2 = 1 + 8κ = 1/β, and by Proposition B.1, γ t ≥ 1 -κ/(1 + 8κ) &gt; 7/8. It can be checked that δ t , γ t , α, and β satisfy the preconditions of Lemma B.3 for this initial iteration t . For all t ∈ T 2 satisfying these preconditions, Lemma B.3 implies that R t+1 ≥ min{R t , 1/α}, θ 2 1,t+1 ≥ min{θ 2 1,t , 1/(1+α 2 )} (via Proposition B.1), δ t+1 ≤ max{δ t , (1+α) 2 ˜ / λ1 } (using the definition of δ t ), and γ t+1 ≥ min{γ t , 1 -ακ} (via Proposition B.1). Hence the next iteration t + 1 also satisfies the preconditions, and by induction, so do all iterations in T 2 . To bound the number of iterations in T 2 , observe that R t increases at a quadratic rate until R t ≥ 1/α, so</figDesc><table><row><cell>|T 2 | ≤ ln</cell><cell cols="2">ln(1/α) ln((1/β)/(2κ))</cell><cell>&lt; ln</cell><cell cols="2">ln λ1 3˜ ln 4</cell><cell>= O log log</cell><cell>λ1 ˜</cell><cell>.</cell><cell>(24)</cell></row><row><cell cols="7">Therefore the total number of iterations before R t ≥ 1/α is</cell><cell></cell></row><row><cell></cell><cell>O</cell><cell>log(kκ) γ 0</cell><cell cols="2">+ log log</cell><cell>λ1 ˜</cell><cell>.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and by Lemma B.3, we have that R t increases at a quadratic rate in this final phase until R t ≥ 1/α. So the number of iterations before R t ≥ 1/α can be bounded as</figDesc><table><row><cell>ln</cell><cell>ln(1/α) ln((1/β)/(2κ))</cell><cell>= ln</cell><cell>ln p λ1 3˜ ln λ 1 2κ 3˜ • 1</cell><cell>≤ ln ln</cell><cell>p λ1 3˜</cell><cell>= O log log</cell><cell>p λ1 ˜</cell><cell>.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Boaz Barak</rs>, <rs type="person">Dean Foster</rs>, <rs type="person">Jon Kelner</rs>, and <rs type="person">Greg Valiant</rs> for helpful discussions. We are also grateful to <rs type="person">Hanzhang Hu</rs>, <rs type="person">Drew Bagnell</rs>, and <rs type="person">Martial Hebert</rs> for alerting us of an issue with Theorem 4.2 and suggesting a simple fix. This work was completed while DH was a postdoctoral researcher at <rs type="institution">Microsoft Research New England</rs>, and partly while AA, RG, and MT were visiting the same lab. AA is supported in part by the <rs type="funder">NSF</rs> Award <rs type="grantNumber">CCF-1219234</rs>, <rs type="funder">AFOSR</rs> Award <rs type="grantNumber">FA9550-10-1-0310</rs> and the <rs type="funder">ARO</rs> Award <rs type="grantNumber">W911NF-12-1-0404</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_p2CQsjn">
					<idno type="grant-number">CCF-1219234</idno>
				</org>
				<org type="funding" xml:id="_b7cxTFy">
					<idno type="grant-number">FA9550-10-1-0310</idno>
				</org>
				<org type="funding" xml:id="_9HaRtJx">
					<idno type="grant-number">W911NF-12-1-0404</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Fixed-Point and Variational Characterizations of Orthogonal Tensor Decompositions</head><p>We give detailed proofs of Theorems 4.1 and 4.2 in this section for completeness.</p><p>A.1 Proof of Theorem 4.1</p><p>Theorem A.1 Let T have an orthogonal decomposition as given in (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Analysis of Robust Power Method</head><p>In this section, we prove Theorem 5.1. The proof is structured as follows. In Appendix B.1, we show that with high probability, at least one out of L random vectors will be a good initializer for the tensor power iterations. An initializer is good if its projection onto an eigenvector is noticeably larger than its projection onto other eigenvectors. We then analyze in Appendix B.2 the convergence behavior of the tensor power iterations. Relative to the proof of Lemma 5.1, this analysis is complicated by the tensor perturbation. We show that there is an initial slow convergence phase (linear rate rather than quadratic), but as soon as the projection of the iterate onto an eigenvector is large enough, it enters the quadratic convergence regime until the perturbation dominates. Finally, we show how errors accrue due to deflation in Appendix B.3, which is rather subtle and different from deflation with matrix eigendecompositions. This is because when some initial set of eigenvectors and eigenvalues are accurately recovered, the additional errors due to deflation are effectively only lower-order terms. These three pieces are assembled in Appendix B.4 to complete the proof of Theorem 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Initialization</head><p>Consider a set of non-negative numbers λ1 , λ2 , . . . , λk ≥ 0. For γ ∈ (0, 1), we say a unit vector</p><p>(the dependence on λ1 , λ2 , . . . , λk is implicit).</p><p>The following lemma shows that for any constant γ, with probability at least 1 -η, at least one out of poly(k) log(1/η) i.i.d. random vectors (uniformly distributed over the unit sphere S k-1 ) is γ-separated relative to arg max i∈[k] λi . (For small enough γ and large enough k, the polynomial is close to linear in k.) Lemma B.1 There exists an absolute constant c &gt; 0 such that if positive integer L ≥ 2 satisfies</p><p>the following holds. With probability at least 1/2 over the choice of L i.i.d. random vectors drawn uniformly distributed over the unit sphere S k-1 in R k , at least one of the vectors is γ-separated relative to arg max i∈[k] λi . Moreover, with the same c, L, and for any η ∈ (0, 1), with probability at least 1 -η over L • log 2 (1/η) i.i.d. uniform random unit vectors, at least one of the vectors is γ-separated.</p><p>Proof Without loss of generality, assume arg max i∈[k] λi = 1. Consider a random matrix Z ∈ R k×L whose entries are independent N (0, 1) random variables; we take the j-th column of Z to be comprised of the random variables used for the j-th random vector (before normalization). Specifically, for the j-th random vector,</p><p>We assume Ẽ is a symmetric tensor such that, for some constant p &gt; 1,</p><p>Assume that not all λi are zero, and define</p><p>We show in Lemma C.1 that if the stopping condition is satisfied by a vector θ, then it must be close to an eigenvector of T . Then in Lemma C.2, we show that the stopping condition is satisfied by θ N when θ 0 is a good starting point (as per the conditions of Lemma B.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma C.1 Fix any vector</head><p>If the stopping condition</p><p>Proof Without loss of generality, assume i * = 1. First, we claim that λ1 |θ 1 | &gt; 0. By the triangle inequality,</p><p>Using the definition of the tensor Frobenius norm, we have</p><p>Combining this with the above inequality implies</p><p>Therefore the stopping condition (31) is satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Sketch of Analysis of Algorithm 2</head><p>The analysis of Algorithm 2 is very similar to the proof of Theorem 5.1 for Algorithm 1, so here we just sketch the essential differences.</p><p>First, the guarantee afforded to Algorithm 2 is somewhat different than Theorem 5.1. Specifically, it is of the following form: (i) under appropriate conditions, upon termination, the algorithm returns an accurate decomposition, and (ii) the algorithm terminates after poly(k) random restarts with high probability.</p><p>The conditions on and N are the same (but for possibly different universal constants C 1 , C 2 ). In Lemma C.1 and Lemma C.2, there is reference to a condition on the Frobenius norm of E, but we may use the inequality E F ≤ k E ≤ k so that the condition is subsumed by the condition. Now we outline the differences relative to the proof of Theorem 5.1. The basic structure of the induction argument is the same. In the induction step, we argue that (i) if the stopping condition is satisfied, then by Lemma C.1 (with α = 0.05 and β = 1/2), we have a vector θ N such that, for some j * ≥ i,</p><p>2. θ N is (1/4)-separated relative to π(j * );</p><p>3. θ π(j * ),N ≥ 4/5; and (ii) the stopping condition is satisfied within poly(k) random restarts (via Lemma B.1 and Lemma C.2) with high probability. We now invoke Lemma B.4 to argue that executing another N power iterations starting from θ N gives a vector θ that satisfies</p><p>The main difference here, relative to the proof of Theorem 5.1, is that we use κ := 4 √ k (rather than κ = O(1)), but this ultimately leads to the same guarantee after taking into consideration the condition ≤ C 1 λ min /k. The remainder of the analysis is essentially the same as the proof of Theorem 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Simultaneous Diagonalization for Tensor Decomposition</head><p>As discussed in the introduction, another standard approach to certain tensor decomposition problems is to simultaneously diagonalize a collection of similar matrices obtained from the given tensor. We now examine this approach in the context of our latent variable models, where</p><p>Thus, the problem of determining the µ i can be cast as a simultaneous diagonalization problem: find a matrix X such that X M 2 X and X M 3 (I, I, η)X (for all η) are diagonal. It is easy to see that if the µ i are linearly independent, then the solution X = V † is unique up to permutation and rescaling of the columns.</p><p>With exact moments, a simple approach is as follows. Assume for simplicity that d = k, and define M (η) := M 3 (I, I, η)M -1 2 = V D(η)V -1 . Observe that if the diagonal entries of D(η) are distinct, then the eigenvectors of M (η) are the columns of V (up to permutation and scaling). This criterion is satisfied almost surely when η is chosen randomly from a continuous distribution over R k .</p><p>The above technique (or some variant thereof) was previously used to give the efficient learnability results, where the computational and sample complexity bounds were polynomial in relevant parameters of the problem, including the rank parameter k <ref type="bibr" target="#b66">(Mossel and Roch, 2006;</ref><ref type="bibr">Anandkumar et al., 2012c,a;</ref><ref type="bibr" target="#b46">Hsu and Kakade, 2013)</ref>. However, the specific polynomial dependence on k was rather large due to the need for the diagonal entries of D(η) to be well-separated. This is because with finite samples, M (η) is only known up to some perturbation, and thus the sample complexity bound depends inversely in (some polynomial of) the separation of the diagonal entries of D(η). With η drawn uniformly at random from the unit sphere in R k , the separation was only guaranteed to be roughly 1/k 2.5 <ref type="bibr">(Anandkumar et al., 2012c)</ref> (while this may be a loose estimate, the instability is observed in practice). In contrast, using the tensor power method to approximately recover V (and hence the model parameters µ i and w i ) requires only a mild, lower-order dependence on k.</p><p>It should be noted, however, that the use of a single random choice of η is quite restrictive, and it is easy to see that a simultaneous diagonalization of M (η) for several choices of η can be beneficial. While the uniqueness of the eigendecomposition of M (η) is only guaranteed when the diagonal entries of D(η) are distinct, the simultaneous diagonalization of M (η (1) ), M (η (2) ), . . . , M (η (m) ) for vectors η (1) , η (2) , . . . , η (m) is unique as long as the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On spectral learning of mixtures of distributions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighteenth Annual Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="458" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Identifiability of parameters in latent structure models with many observed variables</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Allman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rhodes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6A</biblScope>
			<biblScope unit="page" from="3099" to="3132" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A spectral algorithm for latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning mixtures of tree graphical models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A method of moments for mixture models and hidden Markov models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fifth Annual Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="33" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The more, the merrier: the blessing of dimensionality for learning large Gaussian mixtures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rademacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Voss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Seventh Annual Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning mixtures of separated nonspherical Gaussians</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Probability</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1A</biblScope>
			<biblScope unit="page" from="69" to="92" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning topic models -going beyond SVD</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifty-Third IEEE Annual Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Provable ICA with unknown Gaussian noise, and implications for Gaussian mixtures and autoencoders</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sachdeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On exchangeable random variables and the statistics of large graphs and hypergraphs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probab. Survey</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="80" to="145" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quadratic weighted automata: Spectral algorithm and likelihood maximization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bailly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spectral learning of general weighted automata via constrained matrix completion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Local loss optimization in operator models: A new insight into spectral learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Polynomial learning of distribution families</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifty-First Annual IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Smoothed analysis of tensor decompositions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhaskara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vijayaraghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual ACM Symposium on Theory of Computing</title>
		<meeting>the 46th Annual ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Closing the learning-planning loop with predictive state representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Siddiqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Robotics Science and Systems Conference</title>
		<meeting>the Robotics Science and Systems Conference</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Isotropic PCA and affine-invariant clustering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-Ninth Annual IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Numerical methods for simultaneous diagonalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bunse-Gerstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Byers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mehrmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="927" to="949" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Super-symmetric decomposition of the fourth-order cumulant tensor. blind identification of more sources than sensors</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing, 1991. ICASSP-91., 1991 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="3109" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Perturbation of joint diagonalizers</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Cardoso</surname></persName>
		</author>
		<idno>94D027</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Signal Department</publisher>
		</imprint>
		<respStmt>
			<orgName>Télécom Paris</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Independent component analysis, a survey of some algebraic methods</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="93" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Blind beamforming for non Gaussian signals</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souloumiac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE Proceedings-F</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="362" to="370" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The number of eigenvalues of a tensor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sturmfels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">438</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="942" to="952" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parallel proportional profiles and other principles for determining the choice of factors by rotation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Cattell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="267" to="283" />
			<date type="published" when="1944">1944</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Full reconstruction of Markov models on evolutionary trees: Identifiability and consistency</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Biosciences</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="51" to="73" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning mixtures of product distributions using correlations and independence</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-First Annual Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="9" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spectral learning of latent-variable PCFGs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fiftieth Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Independent component analysis, a new concept</title>
		<author>
			<persName><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="314" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Handbook of Blind Source Separation: Independent Component Analysis and Applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jutten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Symmetric tensors and symmetric tensor rank</title>
		<author>
			<persName><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mourrain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis Appl</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1254" to="1279" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A reordered Schur factorization method for zero-dimensional polynomial systems with multiple roots</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Corless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Gianni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Trager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1997 International Symposium on Symbolic and Algebraic Computation</title>
		<meeting>the 1997 International Symposium on Symbolic and Algebraic Computation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="133" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning mixtures of Gaussians</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fortieth Annual IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="634" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A probabilistic analysis of EM for mixtures of separated, spherical Gaussians</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="203" to="226" />
			<date type="published" when="2007-02">Feb. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fourth-order cumulant-based blind identification of underdetermined mixtures. Signal Processing</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">De</forename><surname>Lathauwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Castaing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2965" to="2973" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive blind separation of independent sources: a deflation approach</title>
		<author>
			<persName><forename type="first">N</forename><surname>Delfosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Loubaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="83" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Maximum-likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Statist. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spectral dependency parsing with latent variables</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rodu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Algebraic factor analysis: tetrads, pentads and beyond</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sturmfels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sullivant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probability Theory and Related Fields</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="463" to="493" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the convergence of ICA algorithms with symmetric orthogonalization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Erdogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="2209" to="2221" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning linear transformations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Frieze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jerrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Seventh Annual Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="359" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
		<title level="m">Matrix Computations</title>
		<imprint>
			<publisher>Johns Hopkins University Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Halko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-G</forename><surname>Martinsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Foundations of the PARAFAC procedure: model and conditions for an &apos;explanatory&apos; multi-mode factor analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
		</imprint>
		<respStmt>
			<orgName>UCLA Working Papers in Phonetics</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Most tensor problems are NP-hard</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hillar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-H</forename><surname>Lim</surname></persName>
		</author>
		<idno type="DOI">10.1145/2512329</idno>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<idno type="ISSN">0004-5411</idno>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2013-11">November 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The expression of a tensor or a polyadic as a sum of products</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematics and Physics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="164" to="189" />
			<date type="published" when="1927">1927</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multiple invariants and generalized rank of a p-way matrix or tensor</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematics and Physics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="39" to="79" />
			<date type="published" when="1927">1927</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning mixtures of spherical Gaussians: moment methods and spectral decompositions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth Innovations in Theoretical Computer Science</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Identifiability and unmixing of latent parse trees</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning hidden Markov models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1460" to="1480" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fast and robust fixed-point algorithms for independent component analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="626" to="634" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Independent component analysis: algorithms and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="411" to="430" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Observable operator models for discrete stochastic time series</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficiently learning mixtures of two Gaussians</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-second ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="553" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The spectral method for general mixture models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Salmasian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1141" to="1156" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On the best rank-1 approximation of higher-order supersymmetric tensors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kofidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Regalia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="863" to="884" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">455</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Shifted power method for computing tensor eigenpairs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Mayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1095" to="1124" />
			<date type="published" when="2011-10">October 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and Appl</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="138" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On the best rank-1 and rank-(R 1 , R 2 , ..., R n ) approximation and applications of higher-order tensors</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Lathauwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1324" to="1342" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Cam</surname></persName>
		</author>
		<title level="m">Asymptotic Methods in Statistical Decision Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Singular values and eigenvalues of tensors: a variational approach</title>
		<author>
			<persName><forename type="first">L.-H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing</title>
		<meeting>the IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="129" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Predictive representations of state</title>
		<author>
			<persName><forename type="first">M</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1555" to="1561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Spectral learning for non-deterministic dependency parsing</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Luque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the European Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Fifth Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Tensor Methods in Statistics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mccullagh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Chapman and Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Settling the polynomial learnability of mixtures of Gaussians</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifty-First Annual IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning nonsingular phylogenies and hidden Markov models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mossel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Applied Probability</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="583" to="614" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning a parallelepiped: Cryptanalysis of GGH and NTRU signatures</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Regev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cryptology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="160" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Numerical Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Subspace Identification of Linear Systems</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Overschee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Moor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Algebraic Statistics for Computational Biology</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sturmfels</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A spectral algorithm for latent tree graphical models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Contributions to the mathematical theory of evolution</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society</title>
		<imprint>
			<biblScope unit="page">71</biblScope>
			<date type="published" when="1894">1894</date>
			<pubPlace>London, A.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Eigenvalues of a real supersymmetric tensor</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Symbolic Computation</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1302" to="1324" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Mixture densities, maximum likelihood and the EM algorithm</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Redner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="239" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Monotonic convergence of fixed-point algorithms for ICA</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Regalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kofidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="943" to="949" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A short proof that phylogenetic tree reconstruction by maximum likelihood is hard</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Using regression for spectral estimation of HMMs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rodu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Language and Speech Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="212" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">On the definition of a family of automata</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Schützenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Control</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="245" to="270" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Reduced-rank hidden Markov models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Siddiqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Smoothed analysis: An attempt to explain the behavior of algorithms in practice</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="page" from="76" to="84" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Subtracting a best rank-1 approximation may increase tensor rank</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stegeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and Its Applications</title>
		<imprint>
			<biblScope unit="volume">433</biblScope>
			<biblScope unit="page" from="1276" to="1300" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Binary cumulant varieties</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sturmfels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zwiernik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Comb</title>
		<imprint>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="229" to="250" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning mixtures models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="841" to="860" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Perturbation bounds in connection with singular value decomposition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wedin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BIT Numerical Mathematics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Rank-one approximation to high order tensors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="534" to="550" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A fast algorithm for joint diagonalization with non-orthogonal transformations and its application to blind source separation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ziehe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="777" to="800" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Contrastive learning using spectral methods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
