<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hierarchical Latent Variable Model for Data Visualization</title>
				<funder ref="#_gaT5V8P">
					<orgName type="full">EPSRC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
							<email>cmbishop@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
							<email>m.e.tipping@aston.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Neural Computing Research Group</orgName>
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">St. George House</orgName>
								<address>
									<addrLine>1 Guildhall Street</addrLine>
									<postCode>CB2 3NH</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Aston Uni- versity</orgName>
								<address>
									<postCode>B4 7ET</postCode>
									<settlement>Birmingham</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Hierarchical Latent Variable Model for Data Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">received 3 Apr. 1997; revised 23 Jan. 1998. Recommended for acceptance by R.W. Picard.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Latent variables</term>
					<term>data visualization</term>
					<term>EM algorithm</term>
					<term>hierarchical mixture model</term>
					<term>density estimation</term>
					<term>principal component analysis</term>
					<term>factor analysis</term>
					<term>maximum likelihood</term>
					<term>clustering</term>
					<term>statistics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visualization has proven to be a powerful and widely-applicable tool for the analysis and interpretation of multivariate data. Most visualization algorithms aim to find a projection from the data space down to a two-dimensional visualization space. However, for complex data sets living in a high-dimensional space, it is unlikely that a single two-dimensional projection can reveal all of the interesting structure. We therefore introduce a hierarchical visualization algorithm which allows the complete data set to be visualized at the top level, with clusters and subclusters of data points visualized at deeper levels. The algorithm is based on a hierarchical mixture of latent variable models, whose parameters are estimated using the expectation-maximization algorithm. We demonstrate the principle of the approach on a toy data set, and we then apply the algorithm to the visualization of a synthetic data set in 12 dimensions obtained from a simulation of multiphase flows in oil pipelines, and to data in 36 dimensions derived from satellite images. A Matlab software implementation of the algorithm is publicly available from the World Wide Web.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>ANY algorithms for data visualization have been proposed by both the neural computing and statistics communities, most of which are based on a projection of the data onto a two-dimensional visualization space. While such algorithms can usefully display the structure of simple data sets, they often prove inadequate in the face of data sets which are more complex. A single two-dimensional projection, even if it is nonlinear, may be insufficient to capture all of the interesting aspects of the data set. For example, the projection which best separates two clusters may not be the best for revealing internal structure within one of the clusters. This motivates the consideration of a hierarchical model involving multiple two-dimensional visualization spaces. The goal is that the top-level projection should display the entire data set, perhaps revealing the presence of clusters, while lower-level projections display internal structure within individual clusters, such as the presence of subclusters, which might not be apparent in the higher-level projections.</p><p>Once we allow the possibility of many complementary visualization projections, we can consider each projection model to be relatively simple, for example, based on a linear projection, and compensate for the lack of flexibility of individual models by the overall flexibility of the complete hierarchy. The use of a hierarchy of relatively simple models offers greater ease of interpretation as well as the benefits of analytical and computational simplification. This philosophy for modeling complexity is similar in spirit to the "mixture of experts" approach for solving regression problems <ref type="bibr" target="#b0">[1]</ref>.</p><p>The algorithm discussed in this paper is based on a form of latent variable model which is closely related to both principal component analysis (PCA) and factor analysis. At the top level of the hierarchy we have a single visualization plot corresponding to one such model. By considering a probabilistic mixture of latent variable models we obtain a soft partitioning of the data set into "clusters," corresponding to the second level of the hierarchy. Subsequent levels, obtained using nested mixture representations, provide successively refined models of the data set. The construction of the hierarchical tree proceeds top down, and can be driven interactively by the user. At each stage of the algorithm the relevant model parameters are determined using the expectationmaximization (EM) algorithm.</p><p>In the next section, we review the latent-variable model, and, in Section 3, we discuss the extension to mixtures of such models. This is further extended to hierarchical mixtures in Section 4, and is then used to formulate an interactive visualization algorithm in Section 5. We illustrate the operation of the algorithm in Section 6 using a simple toy data set. Then we apply the algorithm to a problem involving the monitoring of multiphase flows along oil pipes in Section 7 and to the interpretation of satellite image data in Section 8. Finally, extensions to the algorithm, and the relationships to other approaches, are discussed in Section 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LATENT VARIABLES</head><p>We begin by introducing a simple form of linear latent variable model and discussing its application to data analysis. Here we give an overview of the key concepts, and leave the detailed mathematical discussion to Appendix A. The aim is to find a representation of a multidimensional data set in terms of two latent (or "hidden") variables. Suppose the data space is d-dimensional with coordinates y 1 , º, y d and that the data set consists of a set of d-dimensional vectors {t n } where n = 1, º, N. Now consider a twodimensional latent space x = (x 1 , x 2 )</p><p>T together with a linear function which maps the latent space into the data space</p><formula xml:id="formula_0">y = Wx + m (1)</formula><p>where W is a d ¥ 2 matrix and m is a d-dimensional vector.</p><p>The mapping (1) defines a two-dimensional planar surface in the data space. If we introduce a prior probability distribution p(x) over the latent space given by a zeromean Gaussian with a unit covariance matrix, then (1) defines a singular Gaussian distribution in data space with mean m and covariance matrix</p><formula xml:id="formula_1">•(y -m)(y -m) T Ò = WW T . Fi-</formula><p>nally, since we do not expect the data to be confined exactly to a two-dimensional sheet, we convolve this distribution with an isotropic Gaussian distribution p(t|x, s 2 ) in data space, having a mean of zero and covariance s 2 I,</p><p>where I is the unit matrix. Using the rules of probability, the final density model is obtained from the convolution of the noise model with the prior distribution over latent space in the form</p><formula xml:id="formula_2">p p p d ( ) = ( | ) ( ) t tx x x z . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>Since this represents the convolution of two Gaussians, the integral can be evaluated analytically, resulting in a distribution p(t) which corresponds to a d-dimensional Gaussian with mean m and covariance matrix WW T + s 2 I.</p><p>If we had considered a more general model in which the conditional distribution p(t|x) is given by a Gaussian with a general diagonal covariance matrix (having d independent parameters), then we would obtain standard linear factor analysis <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. In fact, our model is more closely related to principal component analysis, as we now discuss.</p><p>The log likelihood function for this model is given by L = Â n ln p(t n ), and maximum likelihood can be used to fit the model to the data and hence determine values for the parameters m, W, and s 2 . The solution for m is just given by the sample mean. In the case of the factor analysis model, the determination of W and s 2 corresponds to a nonlinear optimization which must be performed iteratively. For the isotropic noise covariance matrix, however, it was shown by Tipping and Bishop <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> that there is an exact closed form solution as follows. If we introduce the sample covariance matrix given by</p><formula xml:id="formula_4">S t t = - - = Â 1 1 N n n n N m m c hc h T ,<label>(3)</label></formula><p>then the only nonzero stationary points of the likelihood occur for:</p><formula xml:id="formula_5">W = U (L -s 2 I) 1/2 R,<label>(4)</label></formula><p>where the two columns of the matrix U are eigenvectors of S, with corresponding eigenvalues in the diagonal matrix L, and R is an arbitrary 2 ¥ 2 orthogonal rotation matrix. Furthermore, it was shown that the stationary point corresponding to the global maximum of the likelihood occurs when the columns of U comprise the two principal eigenvectors of S (i.e., the eigenvectors corresponding to the two largest eigenvalues) and that all other combinations of eigenvectors represent saddle-points of the likelihood surface. It was also shown that the maximumlikelihood estimator of s 2 is given by</p><formula xml:id="formula_6">s l ML 2 3 1 2 = - = Â d j j d ,<label>(5)</label></formula><p>which has a clear interpretation as the variance "lost" in the projection, averaged over the lost dimensions. Unlike conventional PCA, however, our model defines a probability density in data space, and this is important for the subsequent hierarchical development of the model. The choice of a radially symmetric rather than a more general diagonal covariance matrix for p(t|x) is motivated by the desire for greater ease of interpretability of the visualization results, since the projections of the data points onto the latent plane in data space correspond (for small values of s</p><p>2 ) to an orthogonal projection as discussed in Appendix A.</p><p>Although we have an explicit solution for the maximumlikelihood parameter values, it was shown by Tipping and Bishop <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> that significant computational savings can sometimes be achieved by using the following EM (expectation-maximization) algorithm <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Using (2), we can write the log likelihood function in the form</p><formula xml:id="formula_7">L p p d n n n N n n = z Â = ln t x x x d i c h 1 , (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>in which we can regard the quantities x n as missing variables. The posterior distribution of the x n , given the observed t n and the model parameters, is obtained using Bayes' theorem and again consists of a Gaussian distribution. The E-step then involves the use of "old" parameter values to evaluate the sufficient statistics of this distribution in the form</p><formula xml:id="formula_9">•x n Ò = M -1 W T (t n -m) (7) x x M x x n n n n T T = + - s 2 1 ,<label>(8)</label></formula><p>where M = W T W + s 2 I is a 2 ¥ 2 matrix, and • Ò denotes the expectation computed with respect to the posterior distribution of x. The M-step then maximizes the expectation of the complete-data log likelihood to give (10)  in which ~ denotes "new" quantities. Note that the new value for W obtained from ( <ref type="formula">9</ref>) is used in the evaluation of s 2 in <ref type="bibr" target="#b9">(10)</ref>. The model is trained by alternately evaluating the sufficient statistics of the latent-space posterior distribution using ( <ref type="formula">7</ref>) and (8) for given s 2 and W (the E-step), and re-evaluating s 2 and W using ( <ref type="formula">9</ref>) and (10) for given</p><formula xml:id="formula_10">W t x xx = - L N M M O Q P P L N M M O Q P P = = - Â Â n n n N n n n N m c h T T 1 1 1 (9) s 2 = 1 2 2 1 Nd n n n n n n N t WWxx x W t - + - - = Â m m Tr T T T T ~~ẽ j c h { }</formula><formula xml:id="formula_11">•x n Ò and x x n n</formula><p>T (the M-step). It can be shown that, at each stage of the EM algorithm, the likelihood is increased unless it is already at a local maximum, as demonstrated in Appendix E.</p><p>For N data points in d dimensions, evaluation of the sample covariance matrix requires O(Nd 2 ) operations, and so any approach to finding the principal eigenvectors based on an explicit evaluation of the covariance matrix must have at least this order of computational complexity. By contrast, the EM algorithm involves steps which are only O(Nd). This saving of computational cost is a consequence of having a latent space whose dimensionality (which, for the purposes of our visualization algorithm, is fixed at two) does not scale with d.</p><p>If we substitute the expressions for the expectations given by the E-step equations ( <ref type="formula">7</ref>) and ( <ref type="formula" target="#formula_9">8</ref>) into the M-step equations we obtain the following re-estimation formulas </p><formula xml:id="formula_12">W = SW(s 2 I + M -1 W T SW) -1<label>(11)</label></formula><p>which shows that all of the dependence on the data occurs through the sample covariance matrix S. Thus the EM algorithm can be expressed as alternate evaluations of ( <ref type="formula" target="#formula_12">11</ref>) and <ref type="bibr" target="#b11">(12)</ref>. (Note that (12) involves a combination of "old" and "new" quantities.) This form of the EM algorithm has been introduced for illustrative purposes only, and would involve O(Nd 2 ) computational cost due to the evaluation of the covariance matrix.</p><p>We have seen that each data point t n induces a Gaussian posterior distribution p(x n |t n ) in the latent space. For the purposes of visualization, however, it is convenient to summarize each such distribution by its mean, given by •x n Ò, as illustrated in Fig. <ref type="figure" target="#fig_1">1</ref>. Note that these quantities are obtained directly from the output of the E-step <ref type="bibr" target="#b6">(7)</ref>. Thus, a set of data points {t n } where n = 1, º, N is projected onto a corresponding set of points {•x n Ò} in the two-dimensional latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MIXTURES OF LATENT VARIABLE MODELS</head><p>We can perform an automatic soft clustering of the data set, and at the same time obtain multiple visualization plots corresponding to the clusters, by modeling the data with a mixture of latent variable models of the kind described in Section 2. The corresponding density model takes the form</p><formula xml:id="formula_14">p p i i i M t t a f c h = = Â p 1 0 (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>where M 0 is the number of components in the mixture, and the parameters p i are the mixing coefficients, or prior probabilities, corresponding to the mixture components p(t|i).</p><p>Each component is an independent latent variable model with parameters m i , W i , and s i 2 . This mixture distribution will form the second level in our hierarchical model. The EM algorithm can be extended to allow a mixture of the form <ref type="bibr" target="#b12">(13)</ref> to be fitted to the data (see Appendix B for details). To derive the EM algorithm we note that, in addition to the {x n }, the missing data now also includes labels which specify which component is responsible for each data point. It is convenient to denote this missing data by a set of variables z ni where z ni = 1 if t n was generated by model i (and zero otherwise). The prior expectations for these variables are given by the p i and the corresponding posterior probabilities, or responsibilities, are evaluated in the extended E-step using Bayes' theorem in the form</p><formula xml:id="formula_16">R P i p i p i ni n i n i i n = = Â ¢ ¢ ¢ t t t d i c h c h p p . (<label>14</label></formula><formula xml:id="formula_17">)</formula><p>Although a standard EM algorithm can be derived by treating the {x n } and the z ni jointly as missing data, a more efficient algorithm can be obtained by considering a twostage form of EM. At each complete cycle of the algorithm we commence with an "old" set of parameter values p i , m i , W i , and s i 2 . We first use these parameters to evaluate the posterior probabilities R ni using <ref type="bibr" target="#b13">(14)</ref>. These posterior probabilities are then used to obtain "new" values p i and mi using the following re-estimation formulas</p><formula xml:id="formula_18">p i n i n N R = Â 1 (15) mi n ni n n ni R R = Â Â t . (<label>16</label></formula><formula xml:id="formula_19">)</formula><p>The new values mi are then used in evaluation of the sufficient statistics for the posterior distribution for</p><formula xml:id="formula_20">x ni x M W t ni i i n i = - -1 T m c h (17) x x M x x ni ni i i ni ni T T = + - s 2 1 (<label>18</label></formula><formula xml:id="formula_21">)</formula><p>where M W W I</p><formula xml:id="formula_22">i i i i = + T s 2 .</formula><p>Finally, these statistics are used to evaluate "new" values Wi and s i 2 using </p><formula xml:id="formula_23">~W t x x x i n i n i n i n ni ni ni n R R = - L N M M O Q P P L N M M O Q P P Â Â - m c h T T 1 (19) ~s i n ni ni n n i d R R 2 2 1 = t Â R S | T | - Â m - - + U V | W | Â Â 2 R R ni n ni i n i ni ni ni i i n x W t x x W W T T T T Tr ~~~m c h (20) which are derived in Appendix B.</formula><p>As for the single latent variable model, we can substitute the expressions for</p><formula xml:id="formula_24">•x ni Ò and x x ni ni</formula><p>T , given by ( <ref type="formula">17</ref>) and ( <ref type="formula" target="#formula_20">18</ref>), respectively, into (19) and (20). We then see that the reestimation formulae for Wi and s i 2 take the form</p><formula xml:id="formula_25">W SW I M W SW i i i i i i i i = + - - s 2 1 1 T e j (<label>21</label></formula><formula xml:id="formula_26">)</formula><formula xml:id="formula_27">~s i i i i i i d 2 1 1 = - - Tr T S SWM W e j , (<label>22</label></formula><formula xml:id="formula_28">)</formula><p>where all of the data dependence been expressed in terms of the quantities</p><formula xml:id="formula_29">S t t i i n i n i n i n N R = - - Â 1 ~m m c hc h T ,<label>(23)</label></formula><p>and we have defined</p><formula xml:id="formula_30">N i = Â n R ni .</formula><p>The matrix S i can clearly be interpreted as a responsibility weighted covariance matrix. Again, for reasons of computational efficiency, the form of EM algorithm given by ( <ref type="formula">17</ref>) to (20) is to be preferred if d is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HIERARCHICAL MIXTURE MODELS</head><p>We now extend the mixture representation of Section 3 to give a hierarchical mixture model. Our formulation will be quite general and can be applied to mixtures of any parametric density model. So far we have considered a two-level system consisting of a single latent variable model at the top level and a mixture of M 0 such models at the second level. We can now extend the hierarchy to a third level by associating a group * i of latent variable models with each model i in the second level. The corresponding probability density can be written in the form</p><formula xml:id="formula_31">p p ij i j i j i M i t t a f d i = OE = Â Â p p , * 1 0 , (<label>24</label></formula><formula xml:id="formula_32">)</formula><p>where p(t|i, j) again represent independent latent variable models, and p j|i correspond to sets of mixing coefficients, one for each i, which satisfy Â j p j|i = 1. Thus, each level of the hierarchy corresponds to a generative model, with lower levels giving more refined and detailed representations. This model is illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>. Determination of the parameters of the models at the third level can again be viewed as a missing data problem in which the missing information corresponds to labels specifying which model generated each data point. When no information about the labels is provided the log likelihood for the model (24) would take the form</p><formula xml:id="formula_33">L pi j i j i j i M n N i = R S | T | U V | W | OE = = Â Â Â ln , p p t d i * 1 1 0 . (<label>25</label></formula><formula xml:id="formula_34">)</formula><p>If, however, we were given a set of indicator variables z ni specifying which model i at the second level generated each data point t n then the log likelihood would become</p><formula xml:id="formula_35">L z p i j ni i M i j i j n N i = R S | T | U V | W | = OE = Â Â Â 1 1 0 ln , p p t d i * . (<label>26</label></formula><formula xml:id="formula_36">)</formula><p>In fact, we only have partial, probabilistic, information in the form of the posterior responsibilities R ni for each model i having generated the data points t n , obtained from the second level of the hierarchy. Taking the expectation of (26), we then obtain the log likelihood for the third level of the hierarchy in the form</p><formula xml:id="formula_37">L R p i j ni i M i j i j n N i = R S | T | U V | W | = OE = Â Â Â 1 1 0 ln , p p t d i * , (<label>27</label></formula><formula xml:id="formula_38">)</formula><p>in which the R ni are constants. In the particular case in which the R ni are all 0 or 1, corresponding to complete certainty about which model in the second level is responsible for each data point, the log likelihood (27) reduces to the form (26). Maximization of (27) can again be performed using the EM algorithm, as discussed in Appendix C. This has the same form as the EM algorithm for a simple mixture, discussed in Section 3, except that in the E-step, the posterior probability that model (i, j) generated data point t n is given by </p><formula xml:id="formula_39">R ni,j = R ni R nj|i , (<label>28</label></formula><formula xml:id="formula_40">) in which R p i j p i j nj i j i n j j i n = Â ¢ ¢ ¢ p p t t , , d i d i . (<label>29</label></formula><formula xml:id="formula_41">)</formula><p>Note that R ni are constants determined from the second level of the hierarchy, and R nj|i are functions of the "old" parameter values in the EM algorithm. The expression (29) automatically satisfies the relation</p><formula xml:id="formula_42">R R ni j j ni i , OE Â = * (30)</formula><p>so that the responsibility of each model at the second level for a given data point n is shared by a partition of unity between the corresponding group of offspring models at the third level.</p><p>The corresponding EM algorithm can be derived by a straightforward extension of the discussion given in Section 3 and Appendix B, and is outlined in Appendix C. This shows that the M-step equations for the mixing coefficients and the means are given by ~,</p><formula xml:id="formula_43">p j i n ni j n ni R R = Â Â ,<label>(31) ~,</label></formula><p>, ,</p><formula xml:id="formula_44">m i j n ni j n n ni j R R = Â Â t . (<label>32</label></formula><formula xml:id="formula_45">)</formula><p>The posterior expectations for the missing variables z ni,j are then given by</p><formula xml:id="formula_46">x M W t ni j i j i j n i j , , , , = - -1 T m e j (33) x x M x x ni j ni j i j i j ni j ni j , ,<label>, , , , T T =</label></formula><formula xml:id="formula_47">+ - s 2 1</formula><p>(34)</p><p>Finally, the W i,j and s i j , 2 are updated using the M-step</p><formula xml:id="formula_48">x i j ni j n i j ni j n ni j ni j ni j n R R = - L N M M O Q P P L N M M O Q P P Â Â - m e j T T 1 (35) ~, , ,<label>equations ~, , , , , , , W t x x</label></formula><formula xml:id="formula_49">s i j n ni j ni j n i j n d R R 2 2 1 = Â - R S | T | Â t m - - Â 2 R ni j n ni j i j n i j , ,<label>,</label></formula><formula xml:id="formula_50">T m e j + U V | W | Â R ni j n ni j ni j i j i j , ,<label>, , ~x W t T</label></formula><formula xml:id="formula_51">T x x W W . (<label>, , , ~Tr T</label></formula><formula xml:id="formula_52">)<label>36</label></formula><p>Again, we can substitute the E-step equations into the M-step equations to obtain a set of update formulas of the form ~, , ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W S W I M W S W</head><p>i j i j i j i j i j i j i j i j</p><formula xml:id="formula_54">= + - - s 2 1 1 T e j (37) ~, , , , ,<label>,</label></formula><formula xml:id="formula_55">s i j i j i j i j i j i j d 2 1 1 = - - Tr T S S W M W e j (<label>38</label></formula><formula xml:id="formula_56">)</formula><p>where all of the summations over n have been expressed in terms of the quantities</p><formula xml:id="formula_57">S t t i j i j ni j n n ij n ij N R , ,<label>, , , ~= - - Â 1</label></formula><p>m m e je j T (39) in which we have defined N i,j = Â n R ni,j . The S i,j can again be interpreted as responsibility-weighted covariance matrices.</p><p>It is straightforward to extend this hierarchical modeling technique to any desired number of levels, for any parametric family of component distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE VISUALIZATION ALGORITHM</head><p>So far, we have described the theory behind hierarchical mixtures of latent variable models, and have illustrated the overall form of the visualization hierarchy in Fig. <ref type="figure" target="#fig_2">2</ref>. We now complete the description of our algorithm by considering the construction of the hierarchy, and its application to data visualization.</p><p>Although the tree structure of the hierarchy can be predefined, a more interesting possibility, with greater practical applicability, is to build the tree interactively. Our multilevel visualization algorithm begins by fitting a single latent variable model to the data set, in which the value of m is given by the sample mean. For low values of the data space dimensionality d, we can find W and s 2 directly by evaluating the covariance matrix and applying ( <ref type="formula" target="#formula_5">4</ref>) and ( <ref type="formula" target="#formula_6">5</ref>). However, for larger values of d, it may be computationally more efficient to apply the EM algorithm, and a scheme for initializing W and s 2 is given in Appendix D. Once the EM algorithm has converged, the visualization plot is generated by plotting each data point t n at the corresponding posterior mean •x n Ò in latent space.</p><p>On the basis of this plot, the user then decides on a suitable number of models to fit at the next level down, and selects points x (i) on the plot corresponding, for example, to the centers of apparent clusters. The resulting points y (i) in data space, obtained from (1), are then used to initialize the means m i of the respective submodels. To initialize the remaining parameters of the mixture model, we first assign the data points to their nearest mean vector m i , and then either compute the corresponding sample covariance matrices and apply a direct eigenvector decomposition, or use the initialization scheme of Appendix D followed by the EM algorithm.</p><p>Having determined the parameters of the mixture model at the second level we can then obtain the corresponding set of visualization plots, in which the posterior means •x ni Ò are again used to plot the data points. For these, it is useful to plot all of the data points on every plot, but to modify the density of "ink" in proportion to the responsibility which each plot has for that particular data point. Thus, if one particular component takes most of the responsibility for a particular point, then that point will effectively be visible only on the corresponding plot. The projection of a data point onto the latent spaces for a mixture of two latent variable models is illustrated schematically in Fig. <ref type="figure" target="#fig_3">3</ref>. The resulting visualization plots are then used to select further submodels, if desired, with the responsibility weighting of (28) being incorporated at this stage. If it is decided not to partition a particular model at some level, then it is easily seen from ( <ref type="formula">30</ref>) that the result of training is equivalent to copying the model down unchanged to the next level. Equation (30) further ensures that the combination of such copied models with those generated through further submodeling defines a consistent probability model, such as that represented by the lower three models in Fig. <ref type="figure" target="#fig_2">2</ref>. The initialization of the model parameters is by direct analogy with the second-level scheme, with the covariance matrices now also involving the responsibilities R ni as weighting coefficients, as in (23). Again, each data point is in principle plotted on every model at a given level, with a density of "ink" proportional to the corresponding posterior probability, given, for example, by (28) in the case of the third level of the hierarchy.</p><p>Deeper levels of the hierarchy involve greater numbers of parameters, and it is therefore important to avoid overfitting and to ensure that the parameter values are welldetermined by the data. If we consider principal component analysis, then we see that three (noncolinear) data points are sufficient to ensure that the covariance matrix has rank two and hence that the first two principal components are defined, irrespective of the dimensionality of the data set. In the case of our latent variable model, four data points are sufficient to determine both W and s 2 . From this, we see that we do not need excessive numbers of data points in each leaf of the tree, and that the dimensionality of the space is largely irrelevant. Finally, it is often also useful to be able to visualize the spatial relationship between a group of models at one level and their parent at the previous level. This can be done by considering the orthogonal projection of the latent plane in data space onto the corresponding plane of the parent model, as illustrated in Fig. <ref type="figure" target="#fig_4">4</ref>. For each model in the hierarchy (except those at the lowest level), we can plot the projections of the associated models from the level below.</p><p>In the next section, we illustrate the operation of this algorithm when applied to a simple toy data set, before presenting results from the study of more realistic data in Sections 7 and 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ILLUSTRATION USING TOY DATA</head><p>We first consider a toy data set consisting of 450 data points generated from a mixture of three Gaussians in a threedimensional space. Each Gaussian is relatively flat (has small variance) in one dimension, and all have the same covariance but differ in their means. Two of these pancakelike clusters are closely spaced, while the third is well separated from the first two. The structure of this data set has been chosen order to illustrate the interactive construction of the hierarchical model.</p><p>To visualize the data, we first generate a single top-level latent variable model, and plot the posterior mean of each data point in the latent space. This plot is shown at the top of Fig. <ref type="figure" target="#fig_5">5</ref>, and clearly suggests the presence of two distinct clusters within the data. The user then selects two initial cluster centers within the plot, which initialize the second-level. This leads to a mixture of two latent variable models, the latent spaces of which are plotted at the second level in Fig. <ref type="figure" target="#fig_5">5</ref>. Of these two plots, that on the right shows evidence of further structure, and so a submodel is generated, again based on a mixture of two latent variable models, which illustrates that there are indeed two further distinct clusters.</p><p>At this third step of the data exploration, the hierarchical nature of the approach is evident as the latter two models only attempt to account for the data points which have already been modeled by their immediate ancestor. Indeed, a group of offspring models may be combined with the siblings of the parent and still define a consistent density model. This is illustrated in Fig. <ref type="figure" target="#fig_5">5</ref>, in which one of the second level plots has been "copied down" (shown by the dotted line) and combined with the other third-level models. When offspring plots are generated from a parent, the extent of each offspring latent space (i.e., the axis limits shown on the plot) is indicated by a projected rectangle within the parent space, using the approach illustrated in Fig. <ref type="figure" target="#fig_4">4</ref>, and these rectangles are numbered sequentially such that the leftmost submodel is "1." In order to display the relative orientations of the latent planes, this number is plotted on the side of the rectangle which corresponds to the top of the corresponding offspring plot. The original three clusters have been individually colored, and it can be seen that the red, yellow, and blue data points have been almost perfectly separated in the third level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">OIL FLOW DATA</head><p>As an example of a more complex problem, we consider a data set arising from a noninvasive monitoring system used to determine the quantity of oil in a multiphase pipeline containing a mixture of oil, water, and gas <ref type="bibr" target="#b8">[9]</ref>. The diagnostic data is collected from a set of three horizontal and three vertical beam-lines along which gamma rays at two different energies are passed. By measuring the degree of attenuation of the gammas, the fractional path length through oil and water (and, hence, gas) can readily be determined, giving 12 diagnostic measurements in total. In practice, the aim is to solve the inverse problem of determining the fraction of oil in the pipe. The complexity of the problem arises from the possibility of the multiphase mixture adopting one of a number of different geometrical configurations. Our goal is to visualize the structure of the data in the original 12-dimensional space. A data set consisting of 1,000 points is obtained synthetically by simulating the physical processes in the pipe, including the presence of noise dominated by photon statistics. Locally, the data is expected to have an intrinsic dimensionality of two corresponding to the two degrees of freedom given by the fraction of oil and the fraction of water (the fraction of gas being redundant). However, the presence of different flow configurations, as well as the geometrical interaction between phase boundaries and the beam paths, leads to numerous distinct clusters. It would appear that a hierarchical approach of the kind discussed here should be capable of discovering this structure. Results from fitting the oil flow data using a three-level hierarchical model are shown in Fig. <ref type="figure" target="#fig_6">6</ref>.</p><p>In the case of the toy data discussed in Section 6, the optimal choice of clusters and subclusters is relatively unambiguous and a single application of the algorithm is sufficient to reveal all of the interesting structure within the data. For more complex data sets, it is appropriate to adopt an exploratory perspective and investigate alternative hierarchies, through the selection of differing numbers of clusters and their respective locations. The example shown in Fig. <ref type="figure" target="#fig_6">6</ref> has clearly been highly successful. Note how the apparently single cluster, number 2, in the top-level plot is revealed to be two quite distinct clusters at the second level, and how data points from the "homogeneous" configuration have been isolated and can be seen to lie on a two-dimensional triangular structure in the third level. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">SATELLITE IMAGE DATA</head><p>As a final example, we consider the visualization of a data set obtained from remote-sensing satellite images. Each data point represents a 3 ¥ 3 pixel region of a satellite land image, and, for each pixel, there are four measurements of intensity taken at different wavelengths (approximately red and green in the visible spectrum, and two in the near infrared). This gives a total of 36 variables for each data point. There is also a label indicating the type of land represented by the central pixel. This data set has previously been the subject of a classification study within the STATLOG project <ref type="bibr" target="#b9">[10]</ref>.</p><p>We applied the hierarchical visualization algorithm to 600 data points, with 100 drawn at random of each of six classes in the 4,435-point data set. The result of fitting a three-level hierarchy is shown in Fig. <ref type="figure" target="#fig_8">7</ref>. Note that the class labels are used only to color the data points and play no role in the maximum likelihood determination of the model parameters. Fig. <ref type="figure" target="#fig_8">7</ref> illustrates that the data can be approximately separated into classes, and the "gray soil" AE "damp gray soil" AE "very damp gray soil" continuum is clearly evident in component 3 at the second level. One particularly interesting additional feature is that there appear to be two distinct and separated clusters of "cotton crop" pixels, in mixtures 1 and 2 at the second level, which are not evident in the single top-level projection. Study of the original image <ref type="bibr" target="#b9">[10]</ref> indeed indicates that there are two separate areas of "cotton crop."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">DISCUSSION</head><p>We have presented a novel approach to data visualization which is both statistically principled and which, as illustrated by real examples, can be very effective at revealing structure within data. The hierarchical summaries of Figs. <ref type="figure" target="#fig_5">5,</ref><ref type="figure" target="#fig_6">6</ref>, and 7 are relatively simple to interpret, yet still convey considerable structural information.</p><p>It is important to emphasize that in data visualization there is no objective measure of quality, and so it is difficult to quantify the merit of a particular data visualization technique. This is one reason, no doubt, why there is a multitude of visualization algorithms and associated software available. While the effectiveness of many of these techniques is often highly data-dependent, we would expect the hierarchical visualization model to be a very useful tool for the visualization and exploratory analysis of data in many applications.</p><p>In relation to previous work, the concept of subsetting, or isolating, data points for further investigation can be traced back to Maltson and Dammann <ref type="bibr" target="#b10">[11]</ref>, and was further developed by Friedman and Tukey <ref type="bibr" target="#b11">[12]</ref> for exploratory data analysis in conjunction with projection pursuit. Such subsetting operations are also possible in current dynamic visualization software, such as "XGobi" <ref type="bibr" target="#b12">[13]</ref>. However, in these approaches there are two limitations. First, the partitioning of the data is performed in a hard fashion, while the mixture of latent variable models approach discussed in this paper permits a soft partitioning in which data points can effectively belong to more than one cluster at any given level. Second, the mechanism for the partitioning of the data is prone to suboptimality as the clusters must be fixed by the user based on a single two-dimensional projection. In the hierarchical approach advocated in this paper, the user selects only a "first guess" for the cluster centers in the mixture model. The EM algorithm is then utilized to determine the parameters which maximize the likelihood of the model, thus allowing both the centers and the widths of the clusters to adapt to the data in the full multidimensional data space. There is also some similarity between our method and earlier hierarchical methods in script recognition <ref type="bibr" target="#b13">[14]</ref> and motion planning <ref type="bibr" target="#b14">[15]</ref> which incorporate the Kohonen Self-Organizing Feature Map <ref type="bibr" target="#b15">[16]</ref> and so offer the potential for visualization. As well as again performing a hard clustering, a key distinction in both of these approaches is that different levels in the hierarchies operate on different subsets of input variables and their operation is thus quite different from the hierarchical algorithm described in this paper.</p><p>Our model is based on a hierarchical combination of linear latent variable models. A related latent variable technique called the generative topographic mapping (GTM) <ref type="bibr" target="#b16">[17]</ref> uses a nonlinear transformation from latent space to data space and is again optimized using an EM algorithm. It is straightforward to incorporate GTM in place of the linear latent variable models in the current hierarchical framework.</p><p>As described, our model applies to continuous data variables. We can easily extend the model to handle discrete data as well as combinations of discrete and continuous variables. In case of a set of binary data variables y k OE {0, 1}, we can express the conditional distribution of a binary variable, given x, using a binomial distribution </p><p>If we have a data set consisting of a combination of continuous, binary and categorical variables, we can formulate the appropriate model by writing the conditional distribution p(t|x) as a product of Gaussian, binomial and multinomial distributions as appropriate. The E-step of the EM algorithm now becomes more complex since the marginalization over the latent variables, needed to normalize the posterior distribution in latent space, will in general be analytically intractable. One approach is to approximate the integration using a finite sample of points drawn from the prior <ref type="bibr" target="#b16">[17]</ref>. Similarly, the M-step is more complex, although it can be tackled efficiently using the iterative reweighted least squares (IRLS) algorithm <ref type="bibr" target="#b17">[18]</ref>. One important consideration with the present model is that the parameters are determined by maximum likelihood, and this criterion need not always lead to the most interesting visualization plots. We are currently investigating alternative models which optimize other criteria such as the separation of clusters. Other possible refinements include algorithms which allow a self-consistent fitting of the whole tree, so that lower levels have the opportunity to influence the parameters at higher levels. While the userdriven nature of the current algorithm is highly appropriate for the visualization context, the development of an automated procedure for generating the hierarchy would clearly also be of interest.</p><p>A software implementation of the probabilistic hierarchical visualization algorithm in MATLAB is available from: <ref type="url" target="http://www.ncrg.aston.ac.uk/PhiVis">http://www.ncrg.aston.ac.uk/PhiVis</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A PROBABILISTIC PRINCIPAL COMPONENT ANALYSIS AND EM</head><p>The algorithm discussed in this paper is based on a latent variable model corresponding to a Gaussian distribution with mean m and covariance WW T + s 2 I, in which the parameters of the model, given by m, W, and s 2 are determined by maximizing the likelihood function given by <ref type="bibr" target="#b5">(6)</ref>.</p><p>For a single such model, the solution for the mean m is given by the sample mean of the data set. We can express the solutions for W and s 2 in closed form in terms of the eigenvectors and eigenvalues of the sample covariance matrix, as discussed in Section 2. Here we derive an alternative approach based on the EM (expectationmaximization) algorithm. We first regard the variables x n appearing in <ref type="bibr" target="#b5">(6)</ref> as "missing data." If these quantities were known, then the corresponding "complete data" log likelihood function would be given by</p><formula xml:id="formula_59">L p p p n n n N n n n N n C = = = = Â Â ln , ln t x t x x c h d i c h 1 1 . (<label>41</label></formula><formula xml:id="formula_60">)</formula><p>We do not, of course, know the values of the x n , but we can find their posterior distribution using Bayes' theorem in the form T . This is straightforward, and gives the results ( <ref type="formula">9</ref>) and <ref type="bibr" target="#b9">(10)</ref>. A simple proof of convergence for the EM algorithm is given in Appendix E. An important aspect of our algorithm is the choice of an isotropic covariance matrix for the noise model of the form s 2 I. The maximum likelihood solution for W is given by the scaled principal component eigenvectors of the data set, in the form</p><formula xml:id="formula_61">W= U q (L q -s 2 I) 1/2 R (44)</formula><p>where U q is a d ¥ q matrix whose columns are the eigenvectors of the data covariance matrix corresponding to the q largest eigenvalues (where q is the dimensionality of the latent space, so that q = 2 in our model), and L q is a q ¥ q diagonal matrix whose elements are given by the eigenvalues. The matrix R is an arbitrary orthogonal matrix corresponding to a rotation of the axes in latent space. This result is derived and discussed in <ref type="bibr" target="#b4">[5]</ref>, and shows that the image of the latent plane in data space coincides with the principal components plane. Also, for s 2 AE 0, the projection of data points onto the latent plane, defined by the posterior means •x n Ò, coincides with the principal components projection. To see this we note that when a point x n in latent space is projected onto a point Wx n + m in data space, the squared distance between the projected point and a data point t n is given by</p><formula xml:id="formula_62">ʈWx n + m -t n ʈ 2 . (<label>45</label></formula><formula xml:id="formula_63">)</formula><p>If we minimize this distance with respect to x n we obtain a solution for the orthogonal projection of t n onto the plane defined by W and m, given by Wx ~n + m where</p><formula xml:id="formula_64">x W W W t n n = - - T T e j c h 1 m .<label>(46)</label></formula><p>We see from (7) that, in the limit s 2 AE 0, the posterior mean for a data point t n reduces to (46) and hence the corresponding point W•x n Ò + m is given by the orthogonal projection of t n onto the plane defined by (1). For s 2 π 0, the posterior mean is skewed towards the origin by the prior, and hence the projection Wx ~n + µ is shifted toward m.</p><p>The crucial difference between our latent variable model and principal component analysis is that, unlike PCA, our model defines a probability density, and hence allows us to consider mixtures, and indeed hierarchical mixtures, of models in a probabilistically principled manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B EM FOR MIXTURES OF PRINCIPAL COMPONENT ANALYZERS</head><p>At the second level of the hierarchy we must fit a mixture of latent variable models, in which the overall model distribution takes the form</p><formula xml:id="formula_65">p p i i i M t t a f c h = = Â p 1 0 ,<label>(47)</label></formula><p>where p(t|i) is a single latent variable model of the form discussed in Appendix A and p i is the corresponding mixing proportion. The parameters for this mixture model can be determined by an extension of the EM algorithm. We begin by considering the standard form which the EM algorithm would take for this model and highlight a number of limitations. We then show that a two-stage form of EM leads to a much more efficient algorithm. We first note that in addition to a set of x ni for each model i, the missing data includes variables z ni labeling which model is responsible for generating each data point t n . At this point, we can derive a standard EM algorithm by considering the corresponding complete-data log likelihood which takes the form</p><formula xml:id="formula_66">L z p C n i i n n i i M n N = = = Â Â ln , p t x c h n s 1 1 0 . (<label>48</label></formula><formula xml:id="formula_67">)</formula><p>Starting with "old" values for the parameters p i , m i , W i , and s i 2 we first evaluate the posterior probabilities R ni using ( <ref type="formula" target="#formula_16">14</ref>) and similarly evaluate the expectations •x ni Ò and x x ni ni T using ( <ref type="formula">17</ref>) and ( <ref type="formula" target="#formula_20">18</ref>) which are easily obtained by inspection of ( <ref type="formula">7</ref>) and ( <ref type="formula" target="#formula_9">8</ref>). Then we take the expectation of L C with respect to this posterior distribution to obtain</p><formula xml:id="formula_68">L R d ni i M i i ni ni n N C T ln Tr = - - R S T = = Â Â 1 2 1 0 2 1 2 ln p s x x e j - - + - 1 2 1 2 2 2 s s i ni i i ni i n i t x W t m m T T c h - U V | W | + 1 2 2 s i i i ni ni Tr const. T T W W x x e j<label>(49)</label></formula><p>where ◊ denotes the expectation with respect to the posterior distributions of both x ni and z ni . The M-step then involves maximizing (49) with respect to p i , m i , s i 2 , and W i to obtain "new" values for these parameters. The maximization with respect to p i must take account of the constraint that Â i p i = 1. This can be achieved with the use of a Lagrange multiplier l [8] by maximizing</p><formula xml:id="formula_69">L i i M C + - F H G I K J = Â l p 1 1 0 . (<label>50</label></formula><formula xml:id="formula_70">)</formula><p>Together with the results of maximizing (48) with respect to the remaining parameters, this gives the following M-step equations</p><formula xml:id="formula_71">p i n i n N R = Â 1 (51) ~m i n ni ni i ni n ni R R = Â - Â t W x e j (52) ~W t x x x i n i n i n i n ni ni ni n R R = - L N M M O Q P P L N M M O Q P P Â Â - m c h T T 1 (<label>53</label></formula><formula xml:id="formula_72">) ~s i n ni ni n i n d R R 2 2 1 = Â - R S | T | Â t m - - + U V | W | ∑ ∑ 2 R R ni ni n i n i ni ni ni i i n x W t x x W W T T T T Tr ~~~μ c h e j . (<label>54</label></formula><formula xml:id="formula_73">)</formula><p>Note that the M-step equations for mi and Wi , given by ( <ref type="formula">52</ref>) and (53), are coupled, and so further manipulation is required to obtain explicit solutions. In fact, a simplification of the M-step equations, along with improved speed of convergence, is possible if we adopt a two-stage EM procedure as follows.</p><p>The likelihood function we wish to maximize is given by</p><formula xml:id="formula_74">L p i i n i M n N = R S | T | U V | W | = = Â Â ln p t c h 1 1 0 . (<label>55</label></formula><formula xml:id="formula_75">)</formula><p>Regarding the component labels z ni as missing data, we can consider the corresponding expected complete-data log likelihood given by</p><formula xml:id="formula_76">$ ln L R p i ni i M i n n N C = = = Â Â 1 1 0 p t c h n s ,<label>(56)</label></formula><p>where R ni represent the posterior probabilities (corresponding to the expected values of z ni ) and are given by <ref type="bibr" target="#b13">(14)</ref>. Maximization of (56) with respect to p i , again using a Lagrange multiplier, gives the M-step equation <ref type="bibr" target="#b14">(15)</ref>. Similarly, maximization of (56) with respect to m i gives <ref type="bibr" target="#b15">(16)</ref>.</p><p>In order to update W i and s i 2 , we seek only to increase the value of $ L C and not actually to maximize it. This corresponds to the generalized EM (or GEM) algorithm.</p><p>We do this by treating the labels z ni as missing data and performing one cycle of the EM algorithm. This involves using the new values mi to compute the sufficient statistics of the posterior distribution of x ni using <ref type="bibr" target="#b16">(17)</ref> and <ref type="bibr" target="#b17">(18)</ref>. The advantage of this strategy is that we are using the new rather than old values of m i in computing these statistics, and overall this leads to simplifications to the algorithm as well as improved convergence speed. By inspection of (49) we see that the expected complete-data log likelihood takes the form (57)</p><p>We then maximize (57) with respect to W i and s i 2 (keeping mi fixed). This gives the M-step equations ( <ref type="formula">19</ref>) and (20).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C EM FOR HIERARCHICAL MIXTURE MODELS</head><p>In the case of the third and subsequent levels of the hierarchy we have to maximize a likelihood function of the form (27) in which the R ni and the p i are treated as constants. To obtain an EM algorithm we note that the likelihood function can be written as Since the parameters for different values of i are independent this represents M 0 independent models each of which can be fitted separately, and each of which corresponds to a mixture model but with weighting coefficients R ni . We can then derive the EM algorithm by introducing, for each i, the expected complete-data likelihood in the form</p><formula xml:id="formula_77">L L L R p i j i i M i n i i j i j n N i = = R S | T | U V | W | = OE = Â Â Â</formula><formula xml:id="formula_78">L R R p i j i n i nj i j j i n N i C = OE = Â Â * ln , p t d i { } 1 (<label>59</label></formula><formula xml:id="formula_79">)</formula><p>where R nj|i is defined by (29) and we have omitted the constant term involving p i . Thus, the responsibility of the jth submodel in group * i for generating data point t n is effectively weighted by the responsibility of its parent model. Maximization of (59) gives rise to weighted M-step equations for the W i,j , m i j , , and s i j to obtain the M-step result (31). A final consideration is that while each offspring mixture within the hierarchy is fitted to the entire data set, the responsibilities of its parent model for many of the data points will approach zero. This implies that the weighted responsibilities for the component models of the mixture will likewise be at least as small. Thus, in a practical implementation, we need only fit offspring mixture models to a reduced data set, where data points for which the parental responsibility is less than some threshold are discarded. For reasons of numerical accuracy, this threshold should be no smaller than the machine precision (which is 2.22 ¥ 10 -16 for double-precision arithmetic). We adopted such a threshold for the experiments within this paper, and observed a considerable computational advantage, particularly at deeper levels in the hierarchy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the projection of a data point onto the mean of the posterior distribution in latent space.</figDesc><graphic coords="3,318.53,50.99,203.98,116.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The structure of the hierarchical model.</figDesc><graphic coords="4,294.55,50.99,239.98,287.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the projection of a data point onto the latent spaces of a mixture of two latent variable models.</figDesc><graphic coords="6,35.29,50.99,230.40,121.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of the projection of one of the latent planes onto its parent plane.</figDesc><graphic coords="6,336.96,50.99,155.11,144.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. A summary of the final results from the toy data set. Each data point is plotted on every model at a given level, but with a density of ink which is proportional to the posterior probability of that model for the given data point.</figDesc><graphic coords="7,139.27,50.99,298.54,336.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Results of fitting the oil data. Colors denote different multiphase flow configurations corresponding to homogeneous (red), annular (blue), and laminar (yellow).</figDesc><graphic coords="8,68.00,50.99,429.00,285.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>a) = (1 + exp(-a)) -1 is the logistic sigmoid function, and w k is the kth column of W. For data having a 1-of-D coding scheme we can represent the distribution of data variables using a multinomial distribution of the form ' =1 where m k are defined by a softmax, or normalized exponential, transformation of the form</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Results of fitting the satellite image data.</figDesc><graphic coords="9,76.91,50.99,423.27,285.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>t|x) is Gaussian with mean Wx + m and covariance s 2 I, and p(x) is Gaussian with zero mean and unit variance, it follows by completing the square that p(x|t) is also Gaussian with mean given by M We can then compute the expectation of L C with respect to this posterior distribution to give the E-step of the EM algorithm. The M-step corresponds to the maximization of •L C Ò with respect to W and s 2 , for fixed •x n Ò and x x n n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>, 2</head><label>2</label><figDesc>parameters with weighting factors R ni,j given by (28), as discussed in the text. For the mixing coefficients p j|i , we can introduce a Lagrange multiplier l i , and hence maximize the function</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was supported by <rs type="funder">EPSRC</rs> grant <rs type="grantNumber">GR/K51808</rs>: <rs type="institution">Neural Networks for Visualization of High-Dimensional Data</rs>. We are grateful to <rs type="person">Michael Jordan</rs> for useful discussions, and we would like to thank the <rs type="institution">Isaac Newton Institute in Cambridge</rs> for their hospitality.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gaT5V8P">
					<idno type="grant-number">GR/K51808</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D INITIALIZATION OF THE EM ALGORITHM</head><p>Here we outline a simple procedure for initializing W and s 2 before applying the EM algorithm. Consider a covariance matrix S with eigenvalues u j and eigenvalues l j . An arbitrary vector v will have an expansion in the eigenbasis of the form v = Â j v j u j , where v j = v T u j . If we multiply v by S, we obtain a vector Â j l j v j u j which will tend to be dominated by the eigenvector u 1 with the largest eigenvalue l 1 . Repeated multiplication and normalization will give an increasingly improved estimate of the normalized eigenvector and of the corresponding eigenvalue. In order to find the first two eigenvectors and eigenvalues, we start with a random d ¥ 2 matrix V and after each multiplication we orthonormalize the columns of V. We choose two data points at random and, after subtraction of m, use these as the columns of V to provide a starting point for this procedure. Degenerate eigenvalues do not present a problem since any two orthogonal vectors in the principal subspace will suffice. In practice only a few matrix multiplications are required to obtain a suitable initial estimate. We now initialize W using the result (4), and initialize s 2 using (5).</p><p>In the case of mixtures we simply apply this procedure for each weighted covariance matrix S i in turn.</p><p>As stated this procedure appears to require the evaluation of S, which would take O(Nd 2 ) computational steps and would therefore defeat the purpose of using the EM algorithm. However, we only ever need to evaluate the product of S with some vector, which can be performed in O(Nd) steps by rewriting the product as</p><p>and evaluating the inner products before performing the summation over n. Similarly the trace of S, required to initialize s 2 , can also be obtained in O(Nd) steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX E CONVERGENCE OF THE EM ALGORITHM</head><p>Here we give a very simple demonstration that the EM algorithms of the kind discussed in this paper have the desired property of guaranteeing that the likelihood will be increased at each cycle of the algorithm unless the parameters correspond to a (local) maximum of the likelihood. If we denote the set of observed data by D, then the log likelihood which we wish to maximize is given by</p><p>where q denotes the set of parameters of the model. If we denote the missing data by M, then the complete-data log likelihood function, i.e., the likelihood function which would be applicable if M were actually observed, is given by</p><p>In the E-step of the EM algorithm, we evaluate the posterior distribution of M given the observed data D and some current values q old for the parameters. We then use this distribution to take the expectation of L C , so that</p><p>•L C (q)Ò = z ln {p(D, M|q)}p(M|D, q old )dM. (64)</p><p>In the M-step, the quantity •L C (q)Ò is maximized with respect to q to give q new . From the rules of probability we have p(D, M|q) = p(M|D, q) p(D|q)</p><p>and substituting this into (64) gives</p><p>•L C (q)Ò = lnp(D|q) + z ln {p(M|D, q)} p(M|D, q old )dM. (66)</p><p>The change in the likelihood function in going from old to new parameter values is therefore given by</p><p>The final term on the right-hand side of ( <ref type="formula">67</ref>) is the Kullback-Leibler divergence between the old and new posterior distributions. Using Jensen's inequality it is easily shown that KL(q ʈq old ) ≥ 0 <ref type="bibr" target="#b7">[8]</ref>. Since we have maximized •L C Ò (or more generally just increased its value in the case of the GEM algorithm) in going from q old to q new , we see that p(D|q new ) &gt; p(D|q old ) as required. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hierarchical Mixtures of Experts and the EM Algorithm</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="214" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An Introduction to Latent Variable Models</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Everitt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Chapman and Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Krzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H C</forename><surname>Marriott</surname></persName>
		</author>
		<title level="m">Multivariate Analysis Part 2: Classification, Covariance Structures and Repeated Measurements</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Edward Arnold</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mixtures of Principal Component Analysers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEE Fifth Int&apos;l Conf. Artificial Neural Networks</title>
		<meeting>IEE Fifth Int&apos;l Conf. Artificial Neural Networks<address><addrLine>Cambridge, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-07">July 1997</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mixtures of Probabilistic Principal Component Analysers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<idno>NCRG/97/003</idno>
		<imprint>
			<date type="published" when="1997">1997</date>
			<pubPlace>Birmingham, U.K.</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Neural Computing Research Group, Aston University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximum Likelihood From Incomplete Data via the EM Algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Statistical Soc., B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">EM Algorithms for ML Factor Analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Thayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="76" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Neural Networks for Pattern Recognition</title>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analysis of Multiphase Flows Using Dual-Energy Gamma Densitometry and Neural Networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nuclear Instruments and Methods in Physics Research</title>
		<imprint>
			<biblScope unit="volume">327</biblScope>
			<biblScope unit="page" from="580" to="593" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Michie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Taylor</surname></persName>
		</author>
		<title level="m">Machine Learning, Neural and Statistical Classification</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Ellis Horwood</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Technique for Determining and Coding Subclasses in Pattern Recognition Problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Maltson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Dammann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="294" to="302" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Projection Pursuit Algorithm for Exploratory Data Analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="881" to="889" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interactive High-Dimensional Data Visualization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Swayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="99" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Script Recognition With Hierarchical Feature Maps</title>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="101" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Fine Motion by Using the Hierarchical Extended Kohonen Map</title>
		<author>
			<persName><forename type="first">C</forename><surname>Versino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Seelen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Vorbrüggen</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sendhoff</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="221" to="226" />
			<date type="published" when="1996">1996</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>Artificial Neural Networks-ICANN 96, C. von der Malsburg</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Self-Organizing Maps</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GTM: The Generative Topographic Mapping</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Svensén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="215" to="234" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Mccullagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Nelder</surname></persName>
		</author>
		<title level="m">Generalized Linear Models</title>
		<imprint>
			<publisher>Chapman and Hall</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
