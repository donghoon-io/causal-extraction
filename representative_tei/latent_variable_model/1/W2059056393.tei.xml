<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Word-Scale Probabilistic Latent Variable Model for Detecting Human Values</title>
				<funder ref="#_etURXXm">
					<orgName type="full">DARPA</orgName>
				</funder>
				<funder ref="#_xJZXAFp">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yasuhiro</forename><surname>Takayama</surname></persName>
							<email>takayama@tokuyama.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nat&apos;l Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Tokuyama College</orgName>
								<address>
									<postCode>3538 745-8585</postCode>
									<settlement>Gakuendai Shunan</settlement>
									<region>Yamaguchi</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yoichi</forename><surname>Tomiura</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Kyushu University</orgName>
								<address>
									<addrLine>744 Motooka Nishi-ku</addrLine>
									<postCode>812-0395</postCode>
									<settlement>Fukuoka</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emi</forename><surname>Ishita</surname></persName>
							<email>ishita.emi.982@m.kyushu-u.ac.jp</email>
							<affiliation key="aff2">
								<orgName type="department">iSchool/UMIACS</orgName>
								<orgName type="institution">Kyushu University</orgName>
								<address>
									<addrLine>6-10-1 Hakozaki Higashi-ku</addrLine>
									<postCode>812-8581</postCode>
									<settlement>Fukuoka</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@umd.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<postCode>20742</postCode>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kenneth</forename><forename type="middle">R</forename><surname>Fleischmann</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Texas at Austin</orgName>
								<address>
									<addrLine>1616 Guadalupe Suite #5.202</addrLine>
									<postCode>78701</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">An-Shou</forename><surname>Cheng</surname></persName>
							<email>ascheng@mail.nsysu.edu.tw</email>
							<affiliation key="aff5">
								<orgName type="institution">Nat&apos;l Sun Yat-sen University</orgName>
								<address>
									<addrLine>70 Lien-hai Rd. Kaohsiung</addrLine>
									<postCode>80424</postCode>
									<settlement>City</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Word-Scale Probabilistic Latent Variable Model for Detecting Human Values</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2661829.2661966</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.2.7 [Artificial Intelligence]: Natural Language Processing -text analysis Algorithms</term>
					<term>Experimentation Computational social science</term>
					<term>computational linguistics</term>
					<term>human values</term>
					<term>probabilistic model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a probabilistic latent variable model that is designed to detect human values such as justice or freedom that a writer has sought to reflect or appeal to when participating in a public debate. The proposed model treats the words in a sentence as having been chosen based on specific values; values reflected by each sentence are then estimated by aggregating values associated with each word. The model can determine the human values for the word in light of the influence of the previous word. This design choice was motivated by syntactic structures such as noun+noun, adjective+noun, and verb+adjective. The classifier based on the model was evaluated on a test collection containing 102 manually annotated documents focusing on one contentious political issue -Net neutrality, achieving the highest reported classification effectiveness for this task. We also compared our proposed classifier with human second annotator. As a result, the proposed classifier effectiveness is statistically comparable with human annotators.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Social scientists have long found it useful to consider human values as latent variables that have explanatory value for the choices that people make <ref type="bibr" target="#b35">[35]</ref>. For example, someone who values innovation over wealth might advocate opensource over proprietary software, while someone who values freedom over social order might resist efforts for gun registration. We can think of values as influencing not only how people form their own opinions, but also as undergirding how people seek to influence the opinions of others. In this paper, we focus on automatic detection of human values reflected in texts written by advocates of specific policy positions. We take a step in that direction by evaluating automated classification of human values.</p><p>Several inventories of human values are used in social science research (e.g., Friedman et al. <ref type="bibr" target="#b13">[13]</ref>; Kahle et al. <ref type="bibr" target="#b22">[22]</ref>; Kluckhohn <ref type="bibr" target="#b23">[23]</ref>; Rokeach <ref type="bibr" target="#b29">[29]</ref>; Schwartz <ref type="bibr" target="#b32">[32]</ref>). Integrating key components of these studies, we adopted Cheng and Fleischmann's <ref type="bibr" target="#b6">[6]</ref> human value definition, that is, "values serve as guiding principles of what people consider important in life." We also base our work on the Meta-Inventory of Human Values (MIHV), which was developed by Cheng and Fleischmann specifically for the test collection that we use by selecting values specific to the debate at issue and by iteratively refining annotation guidelines <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b10">10]</ref>. Our results, generated using a redistributable collection containing 102 documents with zero or more of six sentence-level human values annotations, indicate that high precision (near 0.8) can be reliably achieved for frequently invoked values with a useful degree of recall (0.55-0.82).</p><p>We achieved statistically significant classification effectiveness over existing baselines for this task using a new probabilistic latent variable model in which we first infer the association between human values and individual wordlevel human values as latent variables, and then we aggregate those results over all words in a sentence. The structure of our model allows us to model the potential effect of the preceding word, which proves to be useful. Moreover, analysis of 20 dual-annotated documents indicate that with about 80 training documents our automated technique is able to achieve results that are nearly as accurate as those obtained by an independent human annotator as a pseudo-classifier.</p><p>The remainder of this paper is organized as follows. In Section 2, we describe related work on human value research and on classification methods. Section 3 then introduces the test collection that we have used. Section 4 describes our approach to detect human values and Section 5 describes our proposed latent value model. Section 6 presents our results and Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Content analysis is one of the approaches to detect human values <ref type="bibr" target="#b11">[11]</ref>. The key idea in content analysis is for the social science researchers to personally examine naturally occurring content and to assign codes to that content that reflect their interpretation of that content using some pre-existing coding scheme. Subsequent statistical analysis is then done on the assigned codes rather than on the content. Hsieh and Shannon <ref type="bibr" target="#b17">[17]</ref> refer to this combination of human interpretation and an existing coding scheme as a "directed approach". One of the limiting factors the directed approach is that the annotation costs scale linearly with the size of the collection. Early in the annotation process, personal involvement of the researcher is important because the theory on which any preexisting coding scheme is built may need to be adapted for reflecting the unique characteristics of a collection on which social scientists wish to focus. Our automated techniques are intended only for the part of the process when coding guidelines have stabilized and a substantial amount of annotated data is available.</p><p>After we obtained sufficient annotated data, we could automate the annotation process using text classifiers <ref type="bibr" target="#b33">[33]</ref> trained with that data. We are not the first to explore the automated annotation of human values for social science research. For example, Bengston et al. <ref type="bibr" target="#b2">[2]</ref> used dictionarybased computer aided content analysis to identify how values about forestry have shifted from anthropocentric values to biocentric values over the period 1980 through 2002.</p><p>We first compared the effectiveness of a wide range of classifiers available within Weka <ref type="bibr" target="#b16">[16]</ref>, and we found that Support Vector Machines (SVMs) <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref> performed best. Therefore, we compare our proposed method to SVMs using bag-of-words and bigram features in Section 6. Because we introduce a latent variable model, supervised Latent Dirichlet Allocation (sLDA) offers another appropriate baseline <ref type="bibr" target="#b3">[3]</ref>. Essentially, sLDA is an extension of LDA <ref type="bibr" target="#b4">[4]</ref> in which the process of constructing the probabilistic latent variable model is influenced by the known association of words with labels in a set of training documents. sLDA based on generalized linear models is a general framework to model the documents and the responses. Our proposed probabilistic latent variable model also captures the relationships between the sentences and values. Thus, we compare our method with sLDA in Section 6.</p><p>Griffiths et al. <ref type="bibr" target="#b15">[15]</ref> found modeling sequential dependencies between word classes to be helpful. Sequential dependencies between the words themselves can also be useful, but sparsity risks must be managed. With this in mind, we model sequential word dependencies with the label(s) assigned to one word stem depending in part on the label(s) assigned to only the previous word stem.</p><p>The structure of our problem resembles that of sentiment classification, which has been extensively researched <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b27">27]</ref>. An important difference is that our classification of human values is most naturally cast as a multi-category multi-label classification, whereas sentiment analysis is typically modeled as single-category classification. Importantly, human values can help to explain sentiment, given their explanatory power in relation to attitudes and behavior <ref type="bibr" target="#b34">[34]</ref>. What distinguishes our work is our focus on human values with a redistributable test collection and our modeling of relation between sentence-level and word-level values, and sequential dependencies among words in a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">HUMAN VALUES TEST COLLECTION</head><p>The test collection we used in this paper was originally developed by Cheng et al. <ref type="bibr" target="#b7">[7]</ref>. The collection includes 102 written prepared statements ("testimonies") from public hearings held by the U.S. Senate, House, and Federal Communications Commission (FCC) in which representatives of stakeholder groups offered advice to legislative and regulatory bodies on Net neutrality. The key question in the Net neutrality debate is whether the public interest is better served by nondiscriminatory access for all Internet traffic or by some set of reasonable network traffic management practices for certain types of content or services. Their annotation task focused on the relationship between advocacy positions and detectable human values reflected by (or appealed by) written prepared statements.</p><p>Traditional paper-based annotations for values posed two challenges: (1) annotated passages could be of any length, and indeed both short (clause-scale) and long (paragraphscale) passages were annotated; and (2) annotated passages often did overlap, indicating that evidence for multiple values was present in some places. Cheng et al. <ref type="bibr" target="#b7">[7]</ref> therefore elected to constrain the scope of each annotation to be a single sentence, but to allow more than one value per sentence. Clause annotations were extended to sentences, and passages that spanned sentences were accommodated by annotating several consecutive sentences. This set up a well-structured sentence annotation task for supervised machine learning. Their initial experience with sentence annotation revealed poor inter-annotator agreement. After some iteration of annotation guidelines, they concluded that the Schwartz Values Inventory <ref type="bibr" target="#b32">[32]</ref>, which was developed through and for surveys, was not necessarily transferable to (manual or automatic) content analysis. To address this concern, Cheng and Fleischmann <ref type="bibr" target="#b6">[6]</ref> developed the Meta-Inventory of Human Values (MIHV) by looking for commonalities among the full range of values inventories proposed towards categories that could be reliably inferred during annotation for content analysis. They selected a subset of their MIHV appropriate to our collection, iteratively coding a subset of our collection and iteratively refined annotation guidelines using two annotators until inter-annotator agreement stabilized. The resulting collection is annotated for six human values, the definitions of which are in Appendix A.</p><p>Sentence splitting for the test collection had been performed manually, and all 9,890 sentences in 102 documents were manually annotated. Table <ref type="table" target="#tab_0">1</ref> shows examples of annotated sentences. We subsequently removed sentences whose boundaries disagreed with those of TreeTagger <ref type="bibr" target="#b31">[31]</ref>, sentences that after removing words in the SMART stopword list <ref type="bibr" target="#b30">[30]</ref> contained more than 40 words, and sentences that (after stopword removal) contained no words. The remaining 8,660 sentences were then stemmed by the Porter stemmer <ref type="bibr" target="#b28">[28]</ref>. A second annotator independently had annotated 20 of the prepared statements (containing 2,430 sentences, after the same filtering process was applied). Table <ref type="table" target="#tab_1">2</ref> also shows Cohen's kappa, a chance-corrected measure of inter-annotator agreement <ref type="bibr" target="#b1">[1,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b24">24]</ref> for those 20 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">APPROACH FOR DETECTING VALUES</head><p>In order to detect human values, we have to take into account how the values are reflected in text. Surface language expressions for human values are different from those for most other subject classification problems. In using subject classification to classify a theme of the document, the themes are often directly represented by language expressions, typically by words that occur in the documents <ref type="bibr" target="#b33">[33]</ref>. In the case of the human value classification, while a value may be indicated by a specific word in some cases, in many cases the value may be invoked somewhat more indirectly using situation-specific terminology. In a preliminary analysis of the corpus that we use in this paper, we found the following cases:</p><p>(1) A word represents value(s).</p><p>The word in a sentence represents the certain values. For example, the word "freedom" in sentence (a) ex-presses the value freedom, the word "protect" in sentence (b) expressed the value social order, and the word "winner" in sentence (c) expressed the both of justice and wealth. As shown in (a), value names themselves are usually good cue words for the values.</p><p>(a) "This preserves consumers's freedom to go where they want, use the lawful services they want, and read and say what they want online."</p><p>(b) "Protecting customers and delivering a good Internet experience is not limited to curtailing spam or thwarting identity theft, for example."</p><p>(c) "Consumers, not network operators, must be allowed to continue to choose winners and losers in the content and applications marketplace."</p><p>(2) A pair of words represents value(s).</p><p>The following sentence (d) has the value innovation, but the sentence (e) does not. The word pair "good idea" (adjective+noun) provides the value innovation ("idea" means a suggestion for possible course of action), but the word "idea" in the sentence (e) does not specify any value ("idea" means just a thought).</p><p>(d) "Make sure there is always a fertile place for all of our good ideas to flourish."</p><p>(e) "That was, I believe, the first time that idea had been presented to this Committee."</p><p>(3) A whole sentence represents value(s).</p><p>The annotator determined that the following sentence (f) invokes the value of honor based on its statement.</p><p>(f) "I am one of the network engineers involved for many years in designing, implementing and standardizing the software protocols that underpin the Internet."</p><p>(4) Contextual information is required to infer value(s).</p><p>The following sentences (h) and (i) are annotated based on context. Sentence (h) has honor because of the previous sentence (g) which has the values honor, innovation, and wealth. Sentence (i) has freedom and wealth with influence by the next sentence (j) (which also has the values freedom and wealth).</p><p>(g) "This is an extraordinarily positive development for the nation's economy ... for our global competitiveness ... and for the next wave of broadbanddriven investment and innovation." (h) "How do we continue this progress?" (i) "First and foremost, by recognizing that this market is contestable to all who wish to invest."</p><p>(j) "This is plainly evidenced by the growing array of companies doing just that in the marketplace today ... cable ... phone ... satellite power ... municipality ... WiFi ... WiMax ... Google and more ... all investing in what is increasingly a free-for-all for consumers' broadband business."</p><p>From the above actual examples in the corpus, we can see that the human values are expressed in variety of forms and multiple values are assigned to a sentence. Among several approaches to estimate the presence of a category from text, typical basic methods are naive Bayes, k nearest neighbors (kNN), and SVMs. Ishita et al. <ref type="bibr" target="#b18">[18]</ref> adopted these methods to detect human values, however, the results showed that these methods alone are not sufficient. One reason is that human values cannot be represented by simple functions such that summation of factors of words in a sentence, contributing to each human value. These function cannot be capture that some specific words play a determining role to detect certain values. Based on the above considerations, we design our model to first infer the word-level human values corresponding to each word in a sentence as latent variables, and then aggregate them by logical bitwise OR (see Section 5.1) to estimate the sentencelevel human values.</p><p>Another characteristic of language expressing human values is that multiple values can be expressed by a single sentence. There are the cases in which one word reflects multiple values, as example (c) above illustrates, and multiple words with values can appear in a sentence. As an example of that, the sentence "Part of the reason why the Internet is such a creative forum for new ideas is that there are very few barriers to using the Internet to deliver products, information and services." has the value innovation based on the word "creative" and the word pair "new ideas" (adjec-tive+noun); and freedom based on the word "barrier." The above examples (d) and (e) in the case (2) suggest that word sense disambiguation directed by syntax is required to detect correct human values for word-level. Among several syntax patterns, we focus in our work on two-word collocations, modeling the value of the word in a way that can be influenced by the previous word because this covers many typical and frequent syntax patterns, as the above examples show.</p><p>In this paper, we model cases ( <ref type="formula" target="#formula_4">1</ref>) and ( <ref type="formula" target="#formula_6">2</ref>) above in the next section, that provide an adequate coverage of major cases, in anticipating that the above cases (3) and ( <ref type="formula">4</ref>) are minor. We expect our design choice is effective for these major cases, and in our future research, we could perhaps further extend our model to represent whole-sentence meanings and longdistance context in more nuanced ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">LATENT VALUE MODEL</head><p>In this section, we propose a new method for detecting human values by using a statistical language model we call our Latent Value Model (LVM, for short), that estimates the posterior probability of sentence w having values v using Gibbs sampling in a Markov Chain Monte Carlo framework <ref type="bibr" target="#b14">[14]</ref>. In order to investigate relationships between words for detecting values as discussed in section 4, we take the effect of the preceding word into account in our LVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Preparation and Notation</head><p>A sentence w is a sequence of N words denoted by w = (w1, w2, ..., wN ), where wn is the n-th word in the sequence. The sentence w has sentence-level values v, where v ∈ {0, 1} 6 = {000000, 000001, ..., 111111}. Each bit in the sequence pattern represents one of the six values. The pattern 000000 means the corresponding sentence does not have any values.</p><p>We introduce latent variables x into the model to represent the value(s) associated with each word in the sentence. If the word wn has a value x, the sentence w also has the value x. On the other hand, if no word wn in a sentence w has value x, then the sentence w does not have value x. In addition, we assume that each word in a sentence has at most two values. The sequence of the values corresponding to the sentence w is denoted by x = (x1, x2, ..., xN ), where xn is the word-level value(s) of the word wn.</p><p>We restrict xn to be an element in χ, where χ = {000000, 000001, 000010, 000011, 000100, ..., 110000}. The cardinality of χ is 22. We denote 000000 as µ0, 000001 as µ1, ..., and 110000 as µ21, for convenience of notation. Restricting the number of values with which a word can be associated limits sparsity. Whether an at-most-two model is a good choice is an empirical question. In preliminary experiments, single-value models perform poorly and three values models show no further improvement.</p><p>The sentence-level values v are the result of logical bitwise OR operation ⊕ for all xn(1 ≤ n ≤ N ). The sequence of word-level value(s), therefore, is restricted to the following χ N (v), when the sentence-level values v are given.</p><formula xml:id="formula_0">χ N (v) = {(x1, x2, . . . , xN ) ∈ χ N |(x1 ⊕x2 ⊕. . .⊕xN ) = v}.</formula><p>For example, sentence-level values for the sentence "Congress enact safeguards to preserve American consumers' longstanding freedom of Internet content choice." calculated as social order (from the word "safeguards") and freedom (from the word "freedom" and "choice").</p><p>We also introduce another type of latent variables y = (y1, y2, ..., yN ) into the model. The context indicator yn expresses whether the previous word wn-1 influences the value of wn or not. When the values associated with word wn are subject to the influence of the previous word wn-1, yn takes the numerical value 1. This design choice is motivated by syntactic structures such as noun+noun and ad-jective+noun, or semantic disambiguation associated with verb+noun and verb+adjective. Otherwise, yn takes 0 (The values associated with wn are determined by only wn itself).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model and Estimation of Values</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Model</head><p>For the word sequence (wn-1, wn), the context indicator yn follows a Bernoulli distribution Bern(θ</p><formula xml:id="formula_1">(w n-1 ,wn) 1</formula><p>) and its parameter θ</p><formula xml:id="formula_2">(w n-1 ,wn) 1</formula><p>follows a Beta distribution with the parameters (α0, α1):   are word-level associations.</p><formula xml:id="formula_3">1 Beta(α0, α1) (1 -θ1) α 0 -1 θ α 1 -1 1 . Beta(α0,</formula><p>Our LVM is represented as a graphical model using conditioning with gates <ref type="bibr" target="#b26">[26]</ref> in Figure <ref type="figure" target="#fig_2">2</ref>. The outer plate represents sentences, while the inner plate represents generation of word-level values from a pair of the words or a single word by its context. The dotted box inside the inner plate shows the determination of previous word's influence depending on the context indicator y. The sentence-level value v is an aggregation of word-level values x for the corresponding sentence w. In Figure <ref type="figure" target="#fig_2">2</ref>, L is the number of distinct (wn-1, wn), W is the number of vocabulary and K is fixed at 22.</p><p>That is, our proposed model can be represented by the following equation <ref type="bibr" target="#b1">(1)</ref>.</p><formula xml:id="formula_4">P (x, y|w, θ, ϕ) = N ∏ n=1 P (yn|wn-1, wn, θ) × P (xn|yn, wn-1, wn, ϕ), (<label>1</label></formula><formula xml:id="formula_5">)</formula><p>where w0 is the special symbol ($) expressing the sentence head, and y1 is always 0. The probabilities P (yn|wn-1, wn, θ) and P (xn|yn, wn-1, wn, ϕ) are defined as follows:</p><formula xml:id="formula_6">P (y|a, b, θ) = { θ (a,b) 0 ; y = 0 θ (a,b) 1 ; y = 1 , P (x = µj|y, a, b, ϕ) = { ϕ (b) j ; y = 0 ϕ (a,b) j ; y = 1 . (<label>2</label></formula><formula xml:id="formula_7">)</formula><p>For simplifying notation, the symbol a represents the word wn-1, and the symbol b represents wn, the previous word of wn in the equation ( <ref type="formula" target="#formula_6">2</ref>), and the same style notation shall apply hereafter. The constant µj in equation ( <ref type="formula" target="#formula_6">2</ref>) is the j-th possible word-level value(s) pattern as described in section 5.1.</p><p>We assume the following properties about the relation between words and their values:</p><p>(1) Most words do not have any values, (2) For most two-word sequences, the values associated with the second word are probabilistically determined by that second word alone, without influence from the previous word.</p><p>We adopt a Bayesian approach to embed these properties in our model. The prior distribution of (θ</p><formula xml:id="formula_8">(a,b) 0 , θ (a,b) 1</formula><p>) is 2-dimensional Dirichlet distribution (beta distribution) Dir(α0, α1). To reflect the property (2) above, we set the meta-parameters α0 and α1 as follows: 0 &lt; α0, α1 &lt; 1 and α0 &gt; α1. The prior distributions of (ϕ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>., β21).</head><p>To reflect the property (1) above, we set the meta-parameters as follows: 0 &lt; β0, β1, ..., β21 &lt; 1 and β0 &gt; β1 + β2 + ... + β21 Furthermore, to keep the number of meta-parameters small, we set the following restrictions: α0 = α, α1 = γα, β0 = α, βi = γα/21</p><p>(3) (i = 1, 2, ..., 21, 0 &lt; α &lt; 1 and 0 &lt; γ &lt; 1).</p><p>Thus, the free meta-parameters are only α and γ.</p><p>When the word-level values x is determined, the sentencelevel values v is uniquely determined as v = x1 ⊕x2 ⊕...⊕xN . Therefore, the probability of (x, y, v) given w is then:</p><formula xml:id="formula_9">P (x, y, v|w, θ, ϕ) = { P (x, y | w, θ, ϕ) ; x ∈ χ N (v) 0 ; otherwise .</formula><p>The probability of (x, y) given (w, v) is therefore:</p><formula xml:id="formula_10">P (x, y|w, v, θ, ϕ) ∝ { P (x, y | w, θ, ϕ) ; x ∈ χ N (v) 0 ; otherwise . (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Estimation of Values</head><p>Let (W, V ) be a collection of sentences and their values. W = (w (1) , w (2) , ..., w (M ) ), where w (m) is the m-th sentence, and V = (v1, v2, ... vM ), where vm is the value(s) of the m-th sentence. The n-th word of w (m) is denoted w</p><formula xml:id="formula_11">(m)</formula><p>n , and the length of m-th sentence is denoted Nm. The collection (x (1) x (2) , ..., x (M ) ) is denoted X, and the collection (y (1) , y (2) ..., y (M ) ) is denoted Y in a like manner. We can get the probability of (X, Y) given (W, V ) from (4) as follows: When x (m) ∈ χ Nm (vm) for all m = 1, 2, ..., and M , we get the following formula by calculating the marginal probability:</p><formula xml:id="formula_12">P (X, Y | W, V, θ, ϕ) = M ∏ m=1 P (x (m) , y (m) | w (m) , vm, θ, ϕ) ∝                        ∏ a,b ∏ u∈{0,1} {θ (a,b) u } C Y ((a,b),u) × ∏ b ∏ t {ϕ (b) t } C X (b,t,0) × ∏ a,b ∏ t {ϕ (a,b) t } C X ((a,b),t,</formula><formula xml:id="formula_13">P (X, Y | W, V, α, γ) = ∫ P (X, Y | W, V, θ, ϕ)π(θ | α, γ)π(ϕ | α, γ)dθdϕ ∝ ∏ a,b Γ( ∑ u∈{0,1} αu) ∏ u∈{0,1} Γ(αu) ∏ u∈{0,1} Γ(CY ((a, b), u) + αu) Γ( ∑ u∈{0,1} {CY ((a, b), u)) + αu}) × ∏ b Γ( ∑ t βt) ∏ t Γ(βt) ∏ t Γ(CX (b, t, 0) + βt) Γ( ∑ t {CX (b, t, 0) + βt}) × ∏ a,b Γ( ∑ t βt) ∏ t Γ(βt) ∏ t Γ(CX ((a, b), t, 1) + βt) Γ( ∑ t {CX ((a, b), t, 1)) + βt}) , (<label>5</label></formula><formula xml:id="formula_14">)</formula><p>where π(θ | α, γ) and π(ϕ | β, γ) are the prior distribution of θ and the prior distribution of ϕ, respectively, and Γ(•) is the gamma function.</p><p>We can estimate the value(s) for a sentence w that has N words:</p><formula xml:id="formula_15">values(w) = argmax v P (v | w, α, γ) = argmax v ∑ y∈{0,1} N ∑ x∈χ N (v) P (x, y | w, α, γ).</formula><p>Also we can estimate that w has the j-th value (denoted by (v)j) of the six, when</p><formula xml:id="formula_16">∑ v:(v) j =1 ∑ y∈{0,1} N ∑ x∈χ N (v) P (x, y | w, α, γ) ≥ 1 2 . (<label>6</label></formula><formula xml:id="formula_17">)</formula><p>That is, whether a sentence has the j-th value is determined for each j separately. This means that the comparison among SVM, sLDA and LVM is fair enough because both judgements do not take into account combination of values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Posterior Probabilities by Gibbs Sampling</head><p>We need the probabilities P (x, y | w, α, γ) for every x and y to estimate the values of a sentence w. These are the predictive posterior probabilities after giving the training data (W, V ), P (x, y | w, W, V, α, γ), to be exact. We obtain the following predictive posterior probabilities. We give the derivation of equation ( <ref type="formula">7</ref>), the explanation of P (x, y | w, W, V, X, Y, α, γ) and its calculation in Appendix B.</p><formula xml:id="formula_18">P (x, y | w, W, V, α, γ) (7) = ∑ X ∑ Y P (X, Y | W, V, α, γ) • P (x, y | w, W, V, X, Y, α, γ).</formula><p>By the law of large numbers, equation ( <ref type="formula">7</ref>) can be approximated as follows:</p><formula xml:id="formula_19">1 T T ∑ t=1 P (x, y | w, W, V, X(to + t), Y(to + t), α, γ). (<label>8</label></formula><formula xml:id="formula_20">)</formula><p>In equation ( <ref type="formula" target="#formula_19">8</ref>), (X(s), Y(s)) is the s-th sample of (X, Y) that is drawn according to the posterior probability P (X, Y | W, V, α, γ), given by a Gibbs sampler. We get the conditional probability used in Gibbs sampling from equation ( <ref type="formula" target="#formula_13">5</ref>) as follows:</p><formula xml:id="formula_21">W hen (x (m) 1 ⊕ ... ⊕ x (m) n-1 ⊕ µj ⊕ x (m) n+1 ⊕ ... ⊕ x (m) Nm ) ̸ = vm : P (x (m) n = µj, y (m) n = u | X -(m,n) , Y -(m,n) , W, V, α, γ) = 0. W hen (x (m) 1 ⊕ ... ⊕ x (m) n-1 ⊕ µj ⊕ x (m) n+1 ⊕ ... ⊕ x (m) Nm ) = vm : P (x (m) n = µj, y (m) n = 0 | X -(m,n) , Y -(m,n) , W, V, α, γ) = C -(m,n) Y ((w (m) n-1 ,w (m) n ),0)+α 0 ∑ u∈{0,1} {C -(m,n) Y ((w (m) n-1 ,w (m) n ),u)+αu} × C -(m,n) X (w (m) n ,j,0)+β j ∑ t {C -(m,n) X (w (m) n ,t,0)+β t } , P (x (m) n = µj, y (m) n = 1 | X -(m,n) , Y -(m,n) , W, V, α, γ) = C -(m,n) Y ((w (m) n-1 ,w (m) n ),1)+α 1 ∑ u∈{0,1} {C -(m,n) Y ((w (m) n-1 ,w (m) n ),u)+αu} × C -(m,n) X ((w (m) n-1 ,w (m) n ),j,1)+β j ∑ t {C -(m,n) X ((w (m) n-1 ,w (m) n ),t,1)+β t } , where X -(m,n) is X from which x (m) n is removed, and C -(m,n) X (•) is a count that does not include the current assignment of x (m) n . The same holds for Y -(m,n) and C -(m,n) Y (•) with y (m) n .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS</head><p>In this section, we describe our experiment design, report classifier effectiveness, and compare our automated results to those of a human annotator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment Design</head><p>We use 102-fold document-scale cross-validation (except in Table <ref type="table">3</ref>, where in preliminary experiments we had not grouped sentences by document). 102-fold cross-validation seeks to model the case in which some set of 101 documents have been annotated as training data and we are interested in the degree to which the machine can automatically code all future documents. To select the meta-parameters for each fold, we use 100 documents for development training and one held-out document for development testing. We perform a parameter sweep by training on all sentences in the development training set and then testing on all sentences in the one development testing document to select the meta-parameters α and γ that yield the best F1, sweeping both parameters across 0.05, 0.1, 0.2, 0.5 and 0.9. The 101-document training set is trained using the best α and γ, and the resulting model is used to classify the sentences in the test set.</p><p>For Gibbs sampling we used 50,000 trials. Thirty percent of those trials were treated as the burn-in period. We used 1-for-3 samples of them as (X(t0 + 1), Y(t0 + 1)), (X(t0 + 2), Y(t0 + 2)), and so on to calculate equation <ref type="bibr">(8)</ref>. These parameters were empirically determined in preliminary experiments on development data. We apply the same process to determine the frequency threshold η for bigram features (use if frequency ≥ η, a meta-parameter for SVM 1 ) and to determine meta-parameters for sLDA. 2  In order to examine influence of the previous word, we compare our LVM with LVM(yn = 0) which is our model without any influence from the previous word (i.e., with the context indicator yn in the equation (1) always zero. The meta-parameters were same as for LVM.) We also compare our models with two types of SVM as fair baselines, SVM(w) and SVM(w, b). SVM(w) uses only word features, and SVM(w, b) uses word and bigram features. We use 2nd-degree polynomial kernel for SVM(w) and linear kernel for SVM(w, b), that kernels are determined respectively in experiments.</p><p>sLDA <ref type="bibr" target="#b3">[3]</ref> is a general supervised method but it inherited the property of LDA <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b14">14]</ref> which is a generative model for "documents" so that multiple topics are responsible for the words occurring in a single document. When we apply sLDA our test corpus, we assume that one sentence is regarded as a document. This setting might lose reliability of sLDA's behavior, because the expected number of words which have values in a sentence is a few. However, sLDA is a representative supervised probabilistic model, so we investigate how it works in the actual experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>Table <ref type="table">3</ref> shows results for SVM(w), SVM(w, b), sLDA, LVM (yn=0) and LVM. For the comparison between SVM(w, b) and LVM, the difference of the error rate in the average F1 between them, z = 5.98 &gt; Z0.975 (= 1.96) suggests that the equality can be rejected at significance level 0.05 by a z-test <ref type="bibr" target="#b9">[9]</ref>  <ref type="bibr" target="#b19">[19]</ref>. LVM is much better than sLDA. We can see that even LVM (yn=0) outperforms SVM(b) and SVM(w, b) (significantly for SVM(b), but not significantly for SVM(w, b)).</p><p>Table <ref type="table">4</ref> shows classifier effectiveness by 102-fold document cross-validation. As can be seen, LVM apparently outperforms SVM(w, b). This is also true for sLDA, even when the number of topics is set to 22, which is the closest approximation to our model. Note that we omit honor from these micro-averaged results in Table <ref type="table">3</ref> and 4 because no classifier did well for that category due to a scarcity of annotations for that value in our corpus, as illustrated in Table <ref type="table" target="#tab_1">2</ref>.</p><p>In sLDA, the response is regressed on the topic proportions, while the SVM calculates the weights for the response directly from words. We believe the reason why sLDA works so badly is as follows: (1) it is a model for "document" but not for "sentence" as we mention in the section 6.1; (2) linear regression of the latent variables for words to explain the response is not as well suited to our very sparse data as our estimation of the sentence-level values by a bitwise OR of the word-level values is. In Table <ref type="table" target="#tab_3">5</ref> shows per-category effectiveness measures for the SVM and for our LVM, respectively. For each comparison across the two classifiers, the bolded value is the higher of the two results. This is always true for F1, even in the case of the category with the fewest training examples, honor. As Table <ref type="table">4</ref> shows, SVM(w) and SVM(w, b) achieve nearly identical F1 with 102-fold document crossvalidation (the same condition reported in Table <ref type="table" target="#tab_3">5</ref>, which models the actual annotation process), with SVM(w) yielding F1 = 0.7166 and SVM(w,b) yielding 0.7154. We therefore chose SVM(w) with the numerically higher score as the illustrative baseline for Table <ref type="table" target="#tab_3">5</ref>.</p><p>The value honor is omitted from the averages in Tables <ref type="table">3</ref> and<ref type="table">4</ref> because we focus our analysis of those tables on relative comparisons between usable classifiers. As Table <ref type="table" target="#tab_3">5</ref> shows, the recall for honor is too low (0.26, meaning about 3 of every 4 cases are missed) for practical application. Table <ref type="table" target="#tab_3">5</ref> also shows that our LVM achieves markedly better precision and recall (and thus better F1) on honor than does SVM(w), so including honor in the micro-averages would not have changed the direction of the improvement that Tables <ref type="table">3</ref> and<ref type="table">4</ref> currently show.</p><p>To better understand the behavior of LVM on this collection, we have looked into the estimated word-level values as the first step of qualitative analysis. The social scientists collaborating on this research identified cue words used to invoke particular values during the annotation process. For example, "American consumers will lose basic Internet freedoms, the engine of innovation will be hobbled, and our global competitiveness will be compromised" which is annotated with freedom, innovation, and wealth as sentence-level values. The values names serve as good cue words, and LVM assigned the appropriate values for the words "freedom" and "innovation". As for wealth, LVM estimated that "competitiveness" has the word-level value wealth with influence from the previous word "global". We assumed that each word in a sentence has at-most-two values, and LVM aggregates the word-level values above then correctly estimated all three sentence-level values for the sentence. We plan to conduct more detailed qualitative analysis in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison with Human Annotation</head><p>Because human values are unobservable private states rather than observable facts <ref type="bibr" target="#b36">[36]</ref>, we see the annotator's task as rendering an opinion about which values a statement reflects, and the system's task as replicating that result. As our interannotator agreement in with human annotator on a per-category basis, we ran experiments with the 20 documents (2,430 sentences) annotated by a second annotator as described in Section 3.</p><p>For this experiment, we trained LVM on the remaining 82 documents with meta-parameters: α = 0.2, γ = 0.9 (most frequently selected meta-parameters during document crossvalidation). For comparability, we treat the first annotator's annotations of those 20 documents as correct, and we compute effectiveness as if the second annotator were a classifier. The results are shown in Tables <ref type="table" target="#tab_2">6</ref>.</p><p>Although human performance is not necessarily an upper bound on performance (because the classifier has more access to evidence about how one annotator makes decisions than another human would), we see it as a useful reference because the utility of our classifier depends on its relative costs and benefits when compared to the alternative for coding at large scales, which would be to hire many annotators. Our results show that automation can achieve results similar to human annotation, but at a lower cost (in terms of human effort).</p><p>The difference of the error rate in the average F1 between human and LVM, z = 0.465 ≤ Z0.975 suggests that the equality cannot be rejected in significance level 0.05. This means that LVM effectiveness is statistically indistinguishable from the human classifier. As can be seen, LVM does about as well as our human second annotator on average, and it does substantially better in both precision and recall (and thus in F1) than the second annotator on justice. Notably, honor is markedly less problematic for the human second annotator than for LVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>We have proposed a word-level probabilistic latent variable model for detecting the sentence-level human values reflected in prepared statements on a contentious political issue. The model treats the words in a sentence as having been chosen based on specific human values, and the values reflected by each sentence thus can be estimated by aggregating the values associated with each word. We have achieved the highest reported sentence classification effectiveness F1 = 0.737 in 102-document cross-validation, which is a 3% relative improvement over SVM(w) that does not take account of sequential dependencies between words, as our model does. LVM also improved over SVM(w, b), which uses bigram features.</p><p>Our model can determine the human value(s) xn for the word wn in light of the influence of the previous word wn-1. It is natural to next consider that word wn's value(s) xn might also be influenced by the both previous word wn-1 and following word wn+1. This more complex model may suffer from sparsity, however. We might also explore using longer-distance syntactic dependencies found by a dependency parser, but since dependency parsing is imperfect, proximity features will likely continue to offer some benefit.</p><p>Table <ref type="table" target="#tab_5">7</ref> shows the way we defined each annotated human value <ref type="bibr" target="#b5">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Value Definition freedom</head><p>The condition of being free of restraints and encouraging competition; allowing individuals to have their own beliefs and to make their own choices; freedom from interference or influence of another or others; the quality of being autonomous and independent. The state of being treated equally and fairly, especially having the same rights, status, and opportunities; the process of settling a matter properly and fairly for all parties according to their capabilities and needs, especially protecting the weak and correcting any injustice; need for equal or fair distribution of resources, information, benefits, burdens, and power among the members of a society. social order</p><p>Using the power of the government, military and/or legal system to protect the stability of society and/or to protect people from possible harms mentally or physically; acting in accordance with laws, regulations, and social norms. wealth An explicitly stated concern with or interest in pursuing economic goals such as money, material possessions, resources, and profit; focusing on the market value of a change, decision, or action; allocating resources appropriately and/or efficiently. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. PREDICTIVE POSTERIORS</head><p>The predictive posterior probabilities are calculated by integrating out θ and ϕ as follows: We found the difference between the theoretically-derived and the approximate calculation was not statistically significant in preliminary experiments. We therefore used the approximate calculation in our actual implement for efficiency reasons.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 ∼</head><label>1</label><figDesc>α1) is a Beta-function. When yn takes the value 0, the values associated with the word wn follow a multinomial distribution M ulti(ϕ wn 0 , ϕ wn 1 , ..., ϕ wn 21 ) and its parameters ϕ wn 0:21 follow a Dirichlet distribution with the pa-foreach n = 1, 2, ..., N do (i) draw context indicator yn: yn|θ (w n-1 ,wn) Bern(θ (w n-1 ,wn) 1 ) (ii) draw the word-level value(s) xn: if yn = 1 then xn|wn-1, wn, ϕ ∼ M ulti(ϕ (w n-1 ,wn) 0:21 ), else xn|wn, ϕ ∼ M ulti(ϕ (wn) 0:21 ). sentence-level values become: v = (x1 ⊕ x2 ⊕ . . . ⊕ xN ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Generative process of LVM.</figDesc><graphic coords="6,67.05,200.49,212.60,127.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graphical representation of LVM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc><ref type="bibr" target="#b22">22</ref>-dimensional Dirichlet distributions Dir(β0, β1, ..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 );</head><label>1</label><figDesc>x (m) ∈ χ Nm (vm) f or all m 0 ; otherwise , where CY ((a, b), u) is the number of times u has been assigned to a two-word sequence (a, b) as the value of context indicator y, CX (b, t, 0) is the number of times value µt has been assigned to word b without the influence of the previous word, and CX ((a, b), t, 1) is the number of times value µt has been assigned to the word b with the influence of the previous word a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>PYPPPP 1 × 2 P</head><label>12</label><figDesc>(x, y | w, W, V, α, γ) = ∫ P (x, y | w, θ, ϕ) • π(θ, ϕ|W, V, α, γ)dθdϕ = ∑ X ∑ (X,Y,V |W,α,γ) P (V |W,α,γ)∫ P (x, y | w, θ, ϕ) P (X,Y,V,θ,ϕ|W,α,γ) P (X,Y,V |W,α,γ) dθdϕ, where π(θ, ϕ | W, V, α, γ) is the posterior probability after giving the training data (W, V ).In the last formula above,∫ P (x, y | w, θ, ϕ) P (X, Y, V, θ, ϕ | W, α, γ) P (X, Y, V | W, α, γ) dθdϕis the predictive posterior probability after observation (W, V , X, Y), P (x, y | w, W, V, X, Y, α, γ).For instance, when w = (a, b), x = (µj, µ k ) (a ̸ = b), and y = (0, 1), P (x, y | w, W, V, X, Y, α, γ) can be calculated by integrating out θ and ϕ as follows:(X, Y, V, θ, ϕ | W, α, γ) P (X, Y, V | W, α, γ) dθdϕ = 1 × CY ((a, b), 1) + α1 ∑ u∈{0,1} {CY ((a, b), u) + αu} × CX (a, j, 0) + βj ∑ t {CX (a, t, 0) + βt} × CX ((a, b), k, 1) + β k ∑ t {CX ((a, b), t, 1) + βt} . Also, when w = (a, a), x = (µj, µ k ) (µj ̸ = µ k ), and y = (0, 0), P (x, y | w, W, V, X, Y, α, γ) can be calculated: (X, Y, V, θ, ϕ | W, α, γ) P (X, Y, V | W, α, γ) dθdϕ = 1 × CY ((a, a), 0) + α0 ∑ u∈{0,1} {CY ((a, a, u) + αu} × CX (a, j, 0) + βj ∑ t {CX (a, t, 0) + βt} × CX ((a, k, 0) + β k ∑ t {CX (a, t, 1) + βt} .However, in the case of w = (a, a), x = (µj, µj), and y = (0, 0), P (x,y | w, W, V, X, Y, α, γ) becomes: (X, Y, V, θ, ϕ | W, α, γ) P (X, Y, V | W, α, γ) dθdϕ = 1 × CY ((a, a), 0) + α0 ∑ u∈{0,1} {CY ((a, a), u) + αu} × CX (a, j, 0) + βj + 1 ∑ t {CX (a, t, 0) + βt} + CX ((a, j, 0) + βj ∑ t {CX (a, t, 1) + βt} ,because of the property of the Γ function: Γ(z + 2) = (z + 1)zΓ(z). When there are more than two occurrences for one unique word, we have to take into account a large number of combinations for the theoretically-derived calculation. Then we approximate above calculation as:∫ {ϕ (a) j } 2 P (X, Y, V, θ, ϕ | W, α, γ) P (X, Y, V | W, α, γ) dθdϕ ∼ = {∫ ϕ (a) j P (X, Y, V, θ, ϕ | W, α, γ) P (X, Y, V | W, α, γ) dθdϕ }By this approxmation, the predictive probability in the last case above becomes as follows: (X, Y, V, θ, ϕ | W, α, γ) P (X, Y, V | W, α, γ) dθdϕ = 1 × CY ((a, a), 0) + α0 ∑ u∈{0,1} {CY ((a, a), u) + αu} × CX (a, j, 0) + βj ∑ t {CX (a, t, 0) + βt} × CX (a, j, 0) + βj ∑ t {CX (a, t, 1) + βt} .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Examples of human values annotation.</head><label>1</label><figDesc>Table 2 shows the distribution across the six val-</figDesc><table><row><cell>Values</cell><cell>Sentence</cell><cell></cell><cell></cell></row><row><cell>freedom,</cell><cell cols="4">Consumers are entitled to access the lawful</cell></row><row><cell>s-order</cell><cell cols="3">Internet content of their choice</cell></row><row><cell>honor</cell><cell cols="4">I am one of the network engineers involved for</cell></row><row><cell></cell><cell cols="4">many years in designing, implementing and</cell></row><row><cell></cell><cell cols="4">standardizing the software protocols that under-</cell></row><row><cell></cell><cell cols="2">pin the Internet</cell><cell></cell></row><row><cell>innov.,</cell><cell cols="4">Part of the reason why the Internet is such a</cell></row><row><cell>freedom</cell><cell cols="4">creative forumfor new ideas is that there are very</cell></row><row><cell></cell><cell cols="4">few barriers to using the Internet to deliver</cell></row><row><cell></cell><cell cols="3">products, information and services.</cell></row><row><cell>justice</cell><cell cols="4">Under these circumstances, requiring those most</cell></row><row><cell></cell><cell cols="4">responsible for congestion to bear a greater</cell></row><row><cell></cell><cell cols="4">percentage of the costs would be both good</cell></row><row><cell></cell><cell cols="4">network management and fair from a consumer</cell></row><row><cell></cell><cell>standpoint.</cell><cell></cell><cell></cell></row><row><cell>s-order</cell><cell cols="4">The Commission, under Title I of the Communi-</cell></row><row><cell></cell><cell cols="4">cations Act, has the ability to adopt and enforce</cell></row><row><cell></cell><cell cols="4">the net neutrality principles it announced in the</cell></row><row><cell></cell><cell cols="3">Internet Policy Statement.</cell></row><row><cell>wealth</cell><cell cols="4">Private investors will fund the construction of a</cell></row><row><cell></cell><cell cols="4">broadband network only if there is a reasonable</cell></row><row><cell></cell><cell cols="4">expectation that the company making that</cell></row><row><cell></cell><cell cols="4">investment will recover the cost of its investment,</cell></row><row><cell></cell><cell cols="4">including acompetitive return on capital.</cell></row><row><cell></cell><cell>Value</cell><cell cols="3">κ # doc # sentences</cell></row><row><cell></cell><cell>wealth</cell><cell>0.629</cell><cell>102</cell><cell>3,563</cell></row><row><cell cols="3">social order 0.683</cell><cell>102</cell><cell>2,859</cell></row><row><cell></cell><cell>justice</cell><cell>0.420</cell><cell>99</cell><cell>2,641</cell></row><row><cell></cell><cell>freedom</cell><cell>0.620</cell><cell>101</cell><cell>2,431</cell></row><row><cell cols="2">innovation</cell><cell>0.715</cell><cell>94</cell><cell>1,147</cell></row><row><cell></cell><cell>honor</cell><cell>0.430</cell><cell>80</cell><cell>352</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : Inter-annotator agreement and prevalence.</head><label>2</label><figDesc></figDesc><table><row><cell>ues. A total of 1,545 sentences were annotated as containing</cell></row><row><cell>no value.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 6 : Human "classifier" and LVM effectiveness (same 20 test docs., micro-averaged).</head><label>6</label><figDesc>1 http://chasen.org/ taku/software/TinySVM/ 2 http://www.cs.cmu.edu/ chongw/slda/</figDesc><table><row><cell></cell><cell cols="2">Precision</cell><cell cols="2">Recall</cell><cell>F1</cell><cell></cell></row><row><cell>Value</cell><cell>SVM</cell><cell>LVM</cell><cell>SVM</cell><cell>LVM</cell><cell>SVM</cell><cell>LVM</cell></row><row><cell>wealth</cell><cell cols="6">0.735 0.816 0.871 0.681 0.797 0.743</cell></row><row><cell>s-order</cell><cell cols="2">0.775 0.748</cell><cell cols="4">0.759 0.820 0.767 0.782</cell></row><row><cell>justice</cell><cell cols="6">0.664 0.739 0.464 0.544 0.546 0.627</cell></row><row><cell>freedom</cell><cell cols="4">0.681 0.780 0.768 0.704</cell><cell cols="2">0.722 0.740</cell></row><row><cell>innov</cell><cell cols="6">0.764 0.736 0.720 0.640 0.741 0.685</cell></row><row><cell>honor</cell><cell cols="6">0.395 0.571 0.553 0.094 0.461 0.162</cell></row><row><cell>average</cell><cell cols="6">0.712 0.772 0.732 0.668 0.722 0.716</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 : Per-category effectiveness (102-document cross-validation, micro-averaged).</head><label>5</label><figDesc>Table 2 indicates, well trained and qualified people will sometimes make different judgments about the same sentence. To see how our LVM compares</figDesc><table><row><cell>Method</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row><row><cell>SVM(w)</cell><cell>0.7924</cell><cell>0.6802</cell><cell>0.7320</cell><cell></cell><cell></cell><cell>SVM(w)</cell><cell></cell><cell>0.7784</cell><cell>0.6638</cell><cell>0.7166</cell></row><row><cell>SVM(w, b)</cell><cell>0.7784</cell><cell>0.6988</cell><cell>0.7365</cell><cell></cell><cell></cell><cell>SVM(w, b)</cell><cell></cell><cell>0.7535</cell><cell>0.6809</cell><cell>0.7154</cell></row><row><cell>sLDA</cell><cell>0.7016</cell><cell>0.4821</cell><cell>0.5715</cell><cell></cell><cell></cell><cell>sLDA</cell><cell></cell><cell>0.6875</cell><cell>0.4591</cell><cell>0.5506</cell></row><row><cell>LVM(yn = 0)</cell><cell>0.7916</cell><cell>0.6931</cell><cell>0.7391</cell><cell></cell><cell cols="3">LVM(yn = 0)</cell><cell>0.7930</cell><cell>0.6869</cell><cell>0.7361</cell></row><row><cell>LVM</cell><cell cols="3">0.8000 0.7132 0.7542</cell><cell></cell><cell></cell><cell>LVM</cell><cell></cell><cell>0.7885 0.6909 0.7365</cell></row><row><cell cols="5">Table 3: Classifier effectiveness (micro-averaged,</cell><cell cols="4">Table 4: Classifier effectiveness (micro-averaged,</cell></row><row><cell cols="5">w/o honor, 3 ×10-fold sentence cross-validation).</cell><cell cols="4">w/o honor, 102-fold document cross-validation).</cell></row><row><cell cols="9">The meta-parameters for sLDA: α= 0.05, 0.1, 0.2, 0.5 or 0.9 (fixed at initial α), the number of topics K=16, 22, 32, 64, 96 or</cell></row><row><cell cols="9">128. The meta-parameters for SVM(w, b): Bigram frequency threshold η = 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, ∞ (= w/o bigrams).</cell></row><row><cell></cell><cell></cell><cell cols="2">Precision</cell><cell></cell><cell cols="2">Recall</cell><cell></cell><cell>F1</cell></row><row><cell></cell><cell>Value</cell><cell>SVM(w)</cell><cell>LVM</cell><cell cols="2">SVM(w)</cell><cell>LVM</cell><cell cols="2">SVM(w)</cell><cell>LVM</cell></row><row><cell></cell><cell>wealth</cell><cell>0.7859</cell><cell>0.7934</cell><cell cols="2">0.6977</cell><cell>0.7392</cell><cell></cell><cell>0.7392</cell><cell>0.7654</cell></row><row><cell></cell><cell>social order</cell><cell>0.8235</cell><cell>0.7803</cell><cell cols="2">0.7587</cell><cell>0.8174</cell><cell></cell><cell>0.7898</cell><cell>0.7984</cell></row><row><cell></cell><cell>justice</cell><cell>0.7275</cell><cell cols="3">0.7800 0.5558</cell><cell>0.5492</cell><cell></cell><cell>0.6302</cell><cell>0.6446</cell></row><row><cell></cell><cell>freedom</cell><cell>0.7461</cell><cell>0.7927</cell><cell cols="2">0.6654</cell><cell>0.6742</cell><cell></cell><cell>0.7035</cell><cell>0.7287</cell></row><row><cell></cell><cell>innovation</cell><cell>0.8139</cell><cell>0.8023</cell><cell cols="2">0.5629</cell><cell>0.5817</cell><cell></cell><cell>0.6655</cell><cell>0.6744</cell></row><row><cell></cell><cell>honor</cell><cell>0.4324</cell><cell>0.6051</cell><cell cols="2">0.2019</cell><cell>0.2593</cell><cell></cell><cell>0.2753</cell><cell>0.3631</cell></row><row><cell></cell><cell>average</cell><cell>0.7730</cell><cell>0.7849</cell><cell cols="2">0.6510</cell><cell>0.6737</cell><cell></cell><cell>0.7068</cell><cell>0.7251</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 : Definition and Annotation Scheme of Val- ues.</head><label>7</label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported in part by <rs type="funder">NSF</rs> <rs type="grantNumber">IIS-0725459</rs>, <rs type="grantName">Japan grant-in-aid for scientific research</rs> (B) 25280118, and <rs type="funder">DARPA</rs> contract <rs type="grantNumber">HR0011-12-C-0015</rs>. Thanks go to <rs type="person">Scott Block</rs> for serving as the second annotator for this project.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xJZXAFp">
					<idno type="grant-number">IIS-0725459</idno>
					<orgName type="grant-name">Japan grant-in-aid for scientific research</orgName>
				</org>
				<org type="funding" xml:id="_etURXXm">
					<idno type="grant-number">HR0011-12-C-0015</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inter-coder agreement for computational linguistics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="596" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shifting forest value orientations in the United States, 1980-2001: A computer content analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Bengston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmental Values</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="373" to="392" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised topic models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="537" to="544" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Values in the Net neutrality debate: Applying content analysis to testimonies from public hearings</title>
		<author>
			<persName><forename type="first">A.-S</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<pubPlace>College Park, MD</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note>Unpublished dissertation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Developing a meta-inventory of human values</title>
		<author>
			<persName><forename type="first">A.-S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Fleischmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Society for Information Science and Technology (ASIST2010)</title>
		<meeting>the American Society for Information Science and Technology (ASIST2010)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The role of innovation and wealth in the net neutrality debate: A content analysis of human values in congressional and FCC hearings</title>
		<author>
			<persName><forename type="first">A.-S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Fleischmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ishita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1360" to="1373" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A coefficient of agreement for nominal scales</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Approximate statistical tests for comparing supervised classification learning algorithms</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1895" to="1923" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Information and Human Values</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Fleischmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Morgan &amp; Claypool</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<pubPlace>Reading, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Content analysis for values elicitation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Fleischmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Templeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ishita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Koepfler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the ACM SIGCHI 2012 Conference on Human Factors in Computing Systems, Workshop on Methods for Accounting for Values in Human-Centered Computing</title>
		<meeting>eeding of the ACM SIGCHI 2012 Conference on Human Factors in Computing Systems, Workshop on Methods for Accounting for Values in Human-Centered Computing</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic classification of human values: Applying computational thinking to information ethics</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Fleischmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ishita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 72nd Annual Meeting of the</title>
		<meeting>the 72nd Annual Meeting of the</meeting>
		<imprint>
			<publisher>American Society for Information Science and Technology</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Value Sensitive Design and Information Systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human-computer interaction in management information systems: Foundations</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>M. E. Sharpe</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="348" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Suppl. 1</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Integrating topics and syntax</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<title level="m">The weka data mining software: An update. SIGKDD Explorations</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Three approaches to qualitative content analysis</title>
		<author>
			<persName><forename type="first">H.-F</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Qualitative Health Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1277" to="1288" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Content analysis for values elicitation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ishita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Fleischmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Templeton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Society for Information Science and Technology (ASIST2010)</title>
		<meeting>the American Society for Information Science and Technology (ASIST2010)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Evaluating Learning Algorithms: A Classification Perspective</title>
		<author>
			<persName><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>University of Dortmund</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">LS-8 Report 23</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning to classify text using support vector machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Springer Science+Business Media</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Changes in social values in the united states during the past decade</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Kahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sukhdial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Advertising Research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="35" to="41" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Values and Value-Orientations in the Theory of Action: An exploration in definition and classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kluckhohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Toward a general theory of action</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Parsons</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Shils</surname></persName>
		</editor>
		<imprint>
			<publisher>Harvard University Press</publisher>
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A one-way components of variance model for categorical data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="671" to="679" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Opinion Mining and Sentiment Analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data</title>
		<imprint>
			<publisher>Springer-Verlag Berlin Heidelberg</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="459" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gates: A graphical notation for mixture models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<idno>MSR-TR-2008-185</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An algorithm for suffix stripping</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in Information Retrieval</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1980">1997. 1980</date>
			<biblScope unit="page" from="313" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rokeach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>Chapter 13-15</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The smart and sire experimental retrieval systems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in Information Retrieval</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1980">1997. 1980</date>
			<biblScope unit="page" from="381" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Probabilistic part-of-speech tagging using decision trees</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on New Methods in Language Processing</title>
		<meeting>International Conference on New Methods in Language Processing</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Are there universal aspects in the structure and contents of human values</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Social Issues</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="19" to="45" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simulating audiences: Automating analysis of values, attitudes, and sentiment</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Templeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Fleischmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third IEEE International Conference on Social Computing</title>
		<meeting>the Third IEEE International Conference on Social Computing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Motivated decision making: effects of activation and self-centrality of values on choices and behavior</title>
		<author>
			<persName><forename type="first">B</forename><surname>Verplanken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="434" to="447" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tracking point of view in narrative</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="287" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
