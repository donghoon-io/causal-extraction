<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variable-state Latent Conditional Random Fields for Facial Expression Recognition and Action Unit Detection</title>
				<funder ref="#_qBtNzMS">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_5P4EAR4">
					<orgName type="full">European Community Horizon 2020</orgName>
				</funder>
				<funder ref="#_b7V7ut2">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2015-10-15">October 15, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Robert</forename><surname>Walecki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computing Department</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ognjen</forename><surname>Rudovic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computing Department</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vladimir</forename><surname>Pavlovic</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computing Department</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Variable-state Latent Conditional Random Fields for Facial Expression Recognition and Action Unit Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-10-15">October 15, 2015</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1510.03909v1[cs.CV]</idno>
					<note type="submission">Preprint submitted to Elsevier</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated recognition of facial expressions of emotions, and detection of facial action units (AUs), from videos depends critically on modeling of their dynamics. These dynamics are characterized by changes in temporal phases (onset-apex-offset) and intensity of emotion expressions and AUs, the appearance of which may vary considerably among target subjects, making the recognition/detection task very challenging. The state-of-the-art Latent Conditional Random Fields (L-CRF) framework allows one to efficiently encode these dynamics through the latent states accounting for the temporal consistency in emotion expression and ordinal relationships between its intensity levels, these latent states are typically assumed to be either unordered (nominal) or fully ordered (ordinal). Yet, such an approach is often too restrictive. For instance, in the case of AU detection, the goal is to discriminate between the segments of an image sequence in which this AU is active or inactive. While the sequence segments containing activation of the target AU may better be described using ordinal latent states (corresponding to the AU intensity levels), the inactive segments (i.e., where this AU does not occur) may better be described using unordered (nominal) latent states, as no assumption can be made about their underlying structure (since they can contain either neutral faces or activations of non-target AUs). To address this, we propose the variable-state L-CRF (VSL-CRF) model that automatically selects the optimal latent states for the target image sequence, based on the input data and underlying dynamics of the sequence. To reduce the model overfitting either the nominal or ordinal latent states, we propose a novel graph-Laplacian regularization of the latent states. We evaluate the VSL-CRF on the tasks of facial expression recognition using the CK+ dataset, and AU detection using the GEMEP-FERA and DISFA datasets, and show that the proposed model achieves better generalization performance compared to traditional L-CRFs and other related state-of-the-art models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial behavior is believed to be the most important source of information when it comes to affect, attitude, intentions, and social signals interpretation. Machine understanding of facial expressions could revolutionize user interfaces for artifacts such as robots, mobile devices, cars, and conversational agents. Other valuable applications are in the domain of medicine and psychology, where it can be used to improve medical assistance as well as develop automated tools for behavioral research <ref type="bibr" target="#b0">[1]</ref>. Therefore, automated analysis of facial expressions has attracted a significant research attention <ref type="bibr" target="#b1">[2]</ref>. Facial expressions (FE) are typically described at two levels: the facial affect (emotion) and facial muscle actions (AUs), which stem directly from the message and sign judgment approaches for facial expression measurement <ref type="bibr" target="#b2">[3]</ref>. The message judgment approach aims to directly decode the meaning conveyed by a facial display (e.g., in terms of the six basic emotions). Instead, the sign judgment approach aims to study the physical signal used to transmit the message (such as raised cheeks or depressed lips). To this end, the Facial Action Coding System (FACS) <ref type="bibr" target="#b3">[4]</ref> is used as a gold standard. It is the most comprehensive, anatomically-based system for encoding facial expressions by describing the facial activity based on the activations of 33 AUs. These AUs, individ-ually or in combinations, can describe nearly all-possible facial movements <ref type="bibr" target="#b4">[5]</ref>.</p><p>Early research on facial expression analysis focused mainly on recognition of prototypic facial expressions of six basic emotions (anger, happiness, fear, surprise, sadness, and disgust) and detection of AUs from static facial images <ref type="bibr" target="#b1">[2]</ref>. However, recognizing facial expressions from videos (i.e., image sequences) is more natural and has proved to be more effective <ref type="bibr" target="#b7">[8]</ref>. These is motivated by the fact that facial expressions can better be described as a dynamic process that evolves over time. For instance, facial expressions of emotions and AUs undergo a transition of their temporal phases (onset-apex-offset) during the expression development. Similarly, the activation of AUs spans different time intervals that reflect variation in their intensity, as described by FACS. Several works in the field (e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>) have emphasized the importance of modeling these dynamics for increasing the recognition performance in the target tasks compared to the static methods (see also <ref type="bibr" target="#b8">[9]</ref>).</p><p>Most of the state-of-the-art approaches for modeling facial expression dynamics are based on variants of Dynamic Bayesian Networks (DBN) (e.g., Hidden Markov Models (HMM) <ref type="bibr" target="#b9">[10]</ref> and Conditional Random Fields (CRF) <ref type="bibr" target="#b10">[11]</ref>). These methods are detailed in Sec. 2. <ref type="bibr" target="#b0">1</ref>. In what follows we x t h 1 h 2 h t y (a) H-CRF <ref type="bibr" target="#b5">[6]</ref>/H-CORF <ref type="bibr" target="#b6">[7]</ref> x</p><formula xml:id="formula_0">1 x 2 x t h 1 h 2 h t ν y (b) VSL-CRF[7]</formula><p>Figure <ref type="figure">1</ref>: The graph structure of the (a) traditional Latent CRF models H-CRF/H-CORF, and (b) proposed VSL-CRF model. In H-CRF/H-CORF, the latent states h, relating the observation sequence x = {x 1 , . . . , x T } to the target label y (e.g., emotion or AU activation), are allowed to be either nominal or ordinal, while in VSL-CRF the latent variable ν = {nominal, ordinal} performs automatic selection of the optimal latent states for each sequence.</p><p>focus on hierarchical extensions of CRFs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8]</ref>, as they are directly related to the model proposed in this paper. These methods can be cast as variants of the CRFs called Latent CRFs (L-CRF) <ref type="bibr" target="#b5">[6]</ref>, and they have also been successfully used in other vision-domains (e.g. gesture recognition <ref type="bibr" target="#b5">[6]</ref> and human motion estimation <ref type="bibr" target="#b12">[13]</ref>) to encode dynamics of the target tasks.</p><p>In the context of facial expressions, L-CRFs have been used to model temporal dynamics of facial expressions as a sequence of latent states, relating the image features to the class label (e.g., an emotion category). A typical representative of these models is the Hidden CRF (H-CRF) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, used for facial expression recognition of six basic emotions. Apart from temporal constraints imposed on its latent states, this model fails to account for the ordinal relationships between the latent states. However, the latter may turn important for the model's performance when prior knowledge about the task is available (as in the case of facial expression activations, the intensity of which changes over time). To this end, the recently proposed Hidden Conditional Ordinal Random Field (H-CORF) model <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref> imposes additional constraints on the latent states of emotions by exploiting their ordinal relationships. Specifically, this model implicitly enforces the latent states (of emotions) to correlate with temporal phases (or intensity) of emotions by representing them on an ordinal scale. This, in turn, results in the model with fewer parameters, which is less prone to overfitting, and, thus, able to discriminate better between facial expressions of different emotions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref>. However, in the L-CRF models such as H-CRF and H-CORF, and their variants, the latent states are assumed to be either nominal or ordinal for each and every class. This representation can be too restrictive since for some classes modeling the latent states as ordinal may help to better capture the structure of the states, i.e., their ordinal relationships, allowing the model to better fit the data. By contrast, it would be wrong to impose ordinal constraints on latent states of the classes that do not exhibit ordinal structure. In this case, the more flexible nominal model will better fit the data. For example, in recognition of emotion-specific expressions, and/or detection of target AU, we expect the latent states used to model the activation of facial expressions of target emotion class (e.g., happiness) to be correlated with its temporal phases defined on an ordinal scale (neutral&lt;onset&lt;apex). Similarly, for an AU activation, the latent states should be correlated with its intensity levels, as defined on the Likert scale using FACS (i.e., neutral&lt;A&lt;B&lt;C&lt;D&lt;E). On the other hand, image sequences of the negative class, i.e., containing a neutral face (without facial activity) or a mix of other non-target facial expressions (different emotions or AUs), are expected to be better fit using nominal states. This is due to the lack of the ordinal structure as well as high variability (activations of various non-target AUs) in such data. We can even go a step further by assuming that the nature of the latent states depends not only on the type of the emotion/AU class (active vs inactive), but that it can also vary for each image sequence of the target classes. For instance, this can occur due to differences in facial expressiveness of different subjects, resulting in the clustering effects of the features caused by the subject-specific variation dominating that related to the facial expressions, and noisy image features (due to the tracking errors in the case of facial landmarks). In these cases, the ordinal relationships could be altered, and, thus, modeling of the ordinal latent states may not be flexible enough to account for the increased levels of variation in the data. To mitigate this, the model should automatically infer what type of the latent states should be used for modeling the dynamics of the input/output data. To this end, we generalize the L-CRF models by relaxing their assumption that the latent states within the target sequence need only be nominal or ordinal. We do so by allowing the model to use both types of latent states for modeling sequences within and across the target classes. Specifically, we introduce a novel latent variable within the L-CRF framework, the state of which defines what type of latent states are best suited for target image sequences. The learning in the proposed model is performed using two newly defined approaches based on max-polling of the latent states, as well as an Expectation-Maximization (EM) algorithm. To reduce potential redundancy in the modeling of the underlying dynamics of facial expressions, we propose the graph-Laplacian regularization of the model parameters that is defined directly on posterior distributions of the latent states.</p><p>The contributions of the proposed work can be summarized as follows:</p><p>1) We introduce a novel Variable-state L-CRF (VSL-CRF) model for classification of image sequences that, in contrast to existing L-CRF models, has flexibility to use either nominal or ordinal latent states for modeling the underlying dynamics of target sequences. Also, the proposed model selects automatically the optimal latent states for each target sequence.</p><p>2) We propose two novel learning algorithms based on maxpooling and the EM-like learning of the latent states, as well as graph-Laplacian regularization of the model parameters, for efficient training of the proposed VSL-CRF model. This results in a model that is less prone to overfitting of target data compared to when traditional maximum-likelihood learning (ML) approach is used, as in L-CRF models such as H-CRF and H-CORF.</p><p>3) We show on three publicly available datasets (CK+, GEMEP-FERA and DISFA) that by allowing the VSL-CRF model to automatically select the optimal latent states, it can better learn the underlying dynamics of target facial expressions. This, in turn, results in its superior performance in the sequence-based classification of facial expressions of six basic emotions and detection of target AUs, as well as similar or better frame-based detection of AUs, compared to existing L-CRF models and related state-of-the-art models for the target tasks.</p><p>The rest of the paper is organized as follows. Sec. 2 describes the recent advances in the sequence-and frame-based classification of facial expressions of emotions and AU detection. Sec. 3 introduces the proposed methodology. Sec. 4 describes the conducted experiments and presents the evaluation results, and Sec. 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Facial Expression Recognition</head><p>Facial expression recognition methods can be categorized into the static and dynamic approaches (see <ref type="bibr" target="#b8">[9]</ref> for a detailed overview). The static approach attempts the expression recognition from a single image (typically, the apex of the expression) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. For example, <ref type="bibr" target="#b19">[20]</ref> proposed a two-stage multi-task sparse learning framework to efficiently locate the most discriminative facial patches for the expression classification. The SVM classifier is then used to classify the patches into the six basic emotion categories. The approach in <ref type="bibr" target="#b20">[21]</ref> exploits ensemble of features comprising of Hierarchical Gaussianization (HG), Scale Invariant Feature Transform (SIFT) and Optic Flow, followed by the SVM-based classification of emotion expressions.</p><p>However, a natural facial event such as facial expression of an emotion is dynamic, i.e., it evolves over time by (typically) starting from a neutral expression, followed by its onset, apex, and then the offset, followed by the neutral expression again. For this reason, facial expression recognition from videos is more common than from static images. Although some of the static methods use the features extracted from a window around the target frame, in order to encode dynamics of facial expressions, models for dynamic classification provide a more principled way of doing so. As we mentioned in Sec.1, most of the dynamic approaches to classification of facial expressions are based on variants of DBNs such as HMMs and CRFs. For example, <ref type="bibr" target="#b21">[22]</ref> trained independent HMMs for each emotion category, and then performed emotion classification by comparing the likelihoods of the emotion-specific HMMs. However, discriminative models based on CRFs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref> have been shown to be more effective for the facial expression classification. Furthermore, <ref type="bibr" target="#b23">[24]</ref> have shown that capturing more complex time-dependences in the data (beyond the first order dependences as done in linear-chain CRFs) can enhance the facial expression classification performance. Similarly, <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref> used a generalization of the linear-chain CRF model, a Hidden Conditional Random Field (H-CRF) <ref type="bibr" target="#b5">[6]</ref>, with additional layer of (hidden) variables used to model temporal dynamics of facial expressions. The training of the model was performed using image sequences, but classification of the expressions was done by selecting the most likely class (i.e., emotion category) at each time instance. The authors showed that: (i) having the additional layer of hidden variables results in the model being more discriminative than the standard linear-chain CRF, and (ii) that modeling of the temporal unfolding of the facial shapes is more important than their spatial variation for discriminating between facial expressions of different emotion categories (based on comparisons with SVMs). Another modification of H-CRF, named partially-observed H-CRF, was proposed in <ref type="bibr" target="#b15">[16]</ref>, where additional hidden variables are added to the model to encode the occurrence of subsets of AU combinations in each image frame, and which are assumed to be known during learning. This method outperformed the standard H-CRF, which does not use a prior information about the AU cooccurrences. In contrast to these models, which still perform per-frame classification of target expressions, <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref> proposed the Hidden Conditional Ordinal Random Field (H-CORF) models for the sequence-based classification of facial expressions of emotions and their temporal phases (onset-apex-offset) simultaneously. These models encode ordinal relationships between the temporal phases of emotion expression using either supervised or unsupervised learning of the latent states (corresponding to the temporal phases). The authors showed that improved facial expression recognition can be achieved due to the ordinal modeling of the latent states, with the supervised modeling of the latent states (i.e., using the labels for the temporal phases of emotion expression) outperforming the unsupervised modeling, as expected in this task. Nevertheless, the main limitation of the models listed here is that they restrict their latent states to be either nominal (H-CRF) or ordinal (H-CORF), which may be suboptimal in some cases, as discussed in Sec. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Facial AU Detection</head><p>As for facial expression recognition, two main approaches are typically adopted for AU detection: static and dynamic modeling. In the former, image features are extracted from each frame and then fed into a static classifiers such as SVM or AdaBoost <ref type="bibr" target="#b24">[25]</ref> specifically designed for detection of each AU independently. A more advanced static AU detector, named The Selective Transfer Machine (STM) <ref type="bibr" target="#b4">[5]</ref>, has shown great improvements over standard SVMs in the target task. It personalizes the generic SVM classifier by learning the classifier and re-weighting the training samples that are most relevant to the test subject during inference. However, a limitation of this approach is that the re-learning of the target AU detectors has to be performed for each test subject. The modified correlation filter (MCF) <ref type="bibr" target="#b25">[26]</ref> is also an approach similar in spirit to SVMs and correlation filters, but with the key difference of optimizing only a single hyperplane. This results in more robust AU detection compared to standard SVMs when sequence-level AU labels are used for the frame-based AU detection. The authors of <ref type="bibr" target="#b26">[27]</ref> proposed a multi-kernel-learning (MKL) approach to AU detection, where they investigate the fusion of different appearancebased image features via the sum of histogram-based kernel functions. These kernels are then used in the SVMs trained for each AU. To include the temporal information, the authors extract features within AU-specific windows around the image frames used for detection of target AUs. <ref type="bibr" target="#b27">[28]</ref> proposed a multitask feature learning (MTFL) method for joint AU detection. The MTFL approach and Bayesian networks are used to model AU dependences at both feature and label level, and, thus, perform sjoint AU detection in a probabilistic fashion. Likewise, <ref type="bibr" target="#b28">[29]</ref> introduces the lp-norm regularization to the MKL, in order to fuse multiple features (using various kernels) and account for the AU-dependencies. Bayesian graphical models were also used to encode sparsity and statistical co-occurrence of AUs <ref type="bibr" target="#b29">[30]</ref> for their joint modeling.</p><p>While the methods listed above focus on finding the most discriminative feature representations and/or on inference methods for joint AU detection, they fail to account for temporal information, i.e., AU dynamics. Methods that do so attempt using either temporal image features <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> or DBN-based models such as HMMs <ref type="bibr" target="#b32">[33]</ref> and CRFs <ref type="bibr" target="#b33">[34]</ref>. In general, these works perform either majority voting using the static detection <ref type="bibr" target="#b24">[25]</ref>, or detection of the temporal phases of AUs followed by the rulebased classification of the sequences (by detecting the onsetapex-offset sequence of an AU) <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35]</ref>. Other temporal models are based on Ordinal CRFs have been proposed for modeling of AU temporal phases <ref type="bibr" target="#b35">[36]</ref>, and their intensity <ref type="bibr" target="#b0">[1]</ref>, however, they do not perform AU detection. Another approach, termed Cascade of Tasks (CoT) <ref type="bibr" target="#b36">[37]</ref>, is trained on sequences and applies segment-based detection of AUs. This approach is a combination of three algorithms for static-frame-level-detection, segment-level-detection and transition-level detection. The Interval Temporal Bayesian Networks <ref type="bibr" target="#b23">[24]</ref> (ITBN) have also been proposed to capture complex temporal relations among facial events, and for AU detection. The network also represents the spatial dependences among the facial events with a larger variety of time-constrained relations.</p><p>Note that the above-mentioned approaches for facial expression recognition and AU detection use either static/dynamic classifiers which are designed for either nominal or ordinal data. While the former imposes no spatial constraints on target classes, the latter does so for all classes (e.g., all emotions are modeled by imposing ordinal constraints). In the context of the temporal models based on CRFs, this results in the models that are either under-constrained (e.g., H-CRF <ref type="bibr" target="#b5">[6]</ref>) or overconstrained (H-CORF <ref type="bibr" target="#b6">[7]</ref>), which limits their representational power. In relation to the state-of-the-art methods, the proposed VSL-CRF model focuses on two key aspects of the facial expression recogniton/ AU detection: (i) modeling of their temporal dynamics (via novel latent states of the L-CRF models) to improve the recognition/detection performance of existing graph-based dynamic models for the target task. (ii) The ap-plication of the model to the sequence-based classification and frame-based detection of facial expressions of emotions and AUs. In the following, we introduce the proposed methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we first give a short introduction to ordinal and nominal CRFs, and their L-CRF extensions. We then introduce the VSL-CRF method that generalizes these approaches. Lastly, we introduce different methods for the model optimization, including the posterior regularization of the latent states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation</head><p>We consider a K-class classification problem, where we let y ∈ {1, ..., K} be the class label (e.g., emotion category). Each class y is further represented with a sequence of (latent) states denoted as consecutive integers h ∈ {1, . . . , C}, where C is the number of possible states (e.g., temporal phases such as neutralonset-apex of emotion). The sequence of the corresponding image features, denoted by x = {x 1 . . . x T } ∈ T × D, serves as input covariates for predicting y and h = (h 1 , . . . , h T ). The length of sequences T can vary from instance to instance, while the input feature dimension D is constant. If not said otherwise, we assume a supervised setting where we are given a training set of N data pairs D = {(y i , x i )} N i=1 , which are i.i.d. samples from an underlying but unknown distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Conditional Random Fields (CRF)</head><p>CRFs <ref type="bibr" target="#b37">[38]</ref> are a class of log-linear models that represent the conditional distribution P(h|x) as the Gibbs form clamped on the observation x:</p><formula xml:id="formula_1">P(h|x, θ) = 1 Z(x; θ) e s(x,h;θ) .<label>(1)</label></formula><p>Here, Z(x; θ) = h∈H e s(x,h;θ) is the normalizing partition function (H is a set of all possible output configurations), and θ are the parameters<ref type="foot" target="#foot_0">foot_0</ref> of the score function (or the negative energy) s(x, h; θ). Note that in this model, the states h are observed and they represent the frame labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Chain Conditional Random Fields (CRF)</head><p>We further assume the linear-chain graph structure G = (V, E) in the model, described by the node (r ∈ V) and edge (e = (r, s) ∈ E) potentials. We denote the node features by</p><formula xml:id="formula_2">Ψ (V)</formula><p>r (x, h r ) and the edge features by Ψ (E) e (x, h r , h s ). By letting θ = {v, u} be the parameters of the node and edge potentials, respectively, s(x, h; θ) can then be written as the sum:</p><formula xml:id="formula_3">r∈V v Ψ (V) r (x, h r ) + e=(r,s)∈E u Ψ (E) e (x, h r , h s ). (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>Although the representation in ( <ref type="formula" target="#formula_3">2</ref>) is so general that it can subsume nearly arbitrary forms of features, the node/edge features are often defined depending on target task. We limit our consideration to two commonly used types of the node features (nominal/ordinal), which can be represented using a general probabilistic model for static modeling of nominal/ordinal classes. This is achieved by setting the potential at node r as v Ψ (V) r (x, h r ) -→ Γ (V) r (x, h r ), where</p><formula xml:id="formula_5">Γ (V) r (x, h r ) = C c=1 I(h r = c) • logP(h r = c| f (x)).<label>(3)</label></formula><p>The nominal node potential is then obtained by using the multinomial logistic regression (MLR) model <ref type="bibr" target="#b5">[6]</ref>:</p><formula xml:id="formula_6">P(h n r = c| f n (x, c)) = exp( f n (x, c)) C l=1 exp( f n (x, l)) ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_7">f n (x, c) = β T c •[1, x], for c = 1, ..., C</formula><p>, and β c is the separating hyperplane for the c-th nominal state of the target class. By plugging the likelihood function in (4) into the node potential in (3), we obtain the node features of the standard CRF model.</p><p>Recently, several authors proposed using the ranking likelihood to define the ordinal node potentials. This likelihood is derived from the threshold model for (static) ordinal regression <ref type="bibr" target="#b38">[39]</ref>, and has the form:  <ref type="formula" target="#formula_8">5</ref>) is constructed by contaminating the ideal model (see <ref type="bibr" target="#b39">[40]</ref> for details) with Gaussian noise with standard deviation σ. Again, by plugging the likelihood function in (5) into the node potential in (3), we obtain the node features of the Ordinal CRF (CORF) model <ref type="bibr" target="#b39">[40]</ref>.</p><formula xml:id="formula_8">P(h o r = c| f o (x, c)) = Φ( b c -f o (x) σ ) -Φ( b c-1 -f o (x) σ ),<label>(5)</label></formula><p>In both models defined above (the standard CRF and CORF), the edge potentials Ψ (E)  e (x, h r , h s ) are defined in the same way and have the form:</p><formula xml:id="formula_9">I(h r = c ∧ h s = l) C×C × x r -x s ,<label>(6)</label></formula><p>where I(•) is the indicator function that returns 1 (0) if the argument is true (false). The role of the edge potentials is to assure the temporal consistency of the nominal/ordinal states within a sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Conditional Random Fields (L-CRFs)</head><p>While the CRFs introduced in the previous section aim at modeling/decoding of the state-sequence within a single class, the framework of L-CRFs <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b5">6]</ref> aims at the sequence level multi-class classification. This is attained by introducing additional node in the graph structure of CRF/CORFs (see Fig. <ref type="figure">1</ref>) representing the class label, where the latent states h are now treated as unknown. Formally, L-CRFs combine the score functions of K CRFs, one for each class y = {1, . . . , K}, within the following score function:</p><formula xml:id="formula_10">s(y, x, h; Ω) = K k=1 I(k = y) • s(x, h; θ y ),<label>(7)</label></formula><p>Where s(x, h; θ y ) is the y-th CRF score function, defined as in <ref type="bibr" target="#b1">(2)</ref>, and Ω = {θ k } K k=1 denotes the model parameters. With such score function, the joint conditional distribution of the class and state-sequence is defined as:</p><formula xml:id="formula_11">P(y, h|x) = exp(s(y, x, h)) Z(x) . (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>The sequence of the states h = (h 1 , . . . , h T ) is unknown, and they are integrated out by directly modeling the class conditional distribution:</p><formula xml:id="formula_13">P(y|x) = h P(y, h|x) = h exp(s(y, x, h)) Z(x) .<label>(9)</label></formula><p>Evaluation of the class-conditional P(y|x) depends on the partition function</p><formula xml:id="formula_14">Z(x) = k Z k (x) = k h exp(s(k, x, h)), and the class-latent joint posteriors P(k, h r , h s |x) = P(h r , h s |x, k)• P(k|x).</formula><p>Both can be computed from independent consideration of K individual CRFs. The model with the nominal node potentials in the score function in ( <ref type="formula" target="#formula_13">9</ref>) is termed Hidden CRF (H-CRF) <ref type="bibr" target="#b5">[6]</ref>. Likewise, the model with the ordinal node potentials is termed Hidden CORF (H-CORF) <ref type="bibr" target="#b6">[7]</ref>.</p><p>The parameter optimization in the H-CRF/H-CORF models is carried out by maximizing the (regularized) negative loglikelihood of the class conditional distribution in <ref type="bibr" target="#b8">(9)</ref>. Furthermore, to avoid the constrained optimization in H-CORF (due to the order constraints in parameters b of the ordinal node potentials), the displacement variables γ c , where b j = b 1 + j-1 k=1 γ 2 k for j = 2, . . . , C -1 are introduced. So, b is replaced by the unconstrained parameters {b 1 , γ 1 , . . . , γ C-2 }. Similarly, the positivity of the ordinal scale parameter is ensured by setting σ = σ 2 0 . Although both the objectives of H-CRF/H-CORF are non-convex because of the log-partition function (log-sum-exp of nonlinear concave functions), their log-likelihood objective is bounded below by 0 and are both smooth functions. For this, the standard quasi-Newton (such as Limited-memory BFGS) gradient descent algorithms are typically used to estimate the model parameters (we use the former). The model parameters for H-CRF are given by θ (n) y = β 1 , . . . , β C , where C is the number of nominal latent states for class y = {1, . . . , K}. Likewise, for H-CORF we have θ (o)  y = {b 1 , γ 1 , . . . , γ C-2 , σ} for each class in y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Variable-state Latent Conditional Random Fields (VSL-CRF)</head><p>In this section, we generalize the H-CRF/H-CORF models by allowing their latent states to be modeled using either nominal or ordinal potentials (latent states) within each sequence. In this way, we allow the model to select in an unsupervised manner the optimal feature functions for representing the target sequences. In what follows, we provide a formal definition of the model, and then explain its learning and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">VSL-CRF: Model</head><p>Definition (Variable-state Latent CRF) Let ν = (ν 1 , . . . , ν K ) be a vector of symbolic states or labels encoding the nature of the latent states h ν of the i-th sequence, i = 1, . . . , N y from class y = (1, ..., K), either as nominal (ν y = 0) or ordinal (ν y = 1). The score function for class y in the VSL-CRF model is then defined as:</p><formula xml:id="formula_15">s(y, x, h, ν; Ω) =              K k=1 I(k = y) • s(x, h; θ n y ), if ν y = 0 K k=1 I(k = y) • s(x, h; θ o y ), if ν y = 1 (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>where the nominal (s(x, h; θ n y )) and ordinal (s(x, h; θ o y )) score functions represent the sum of the node and edge potentials, as given by ( <ref type="formula" target="#formula_5">3</ref>) and ( <ref type="formula" target="#formula_9">6</ref>), respectively. Then, the full conditional probability of the VSL-CRF model is given by:</p><formula xml:id="formula_17">P(y|x) = h,ν P(y, h, ν|x) = h,ν exp(s(y, x, h, ν)) Z(x)<label>(11)</label></formula><formula xml:id="formula_18">Z(x) = k,h,ν exp(s(k, x, h, ν))<label>(12)</label></formula><p>Note that, in contrast to L-CRF models introduced in Sec.3.1, the VSL-CRF performs also integration over the latent variable ν, the state of which (ordinal or nominal) defines the type of the latent states for each sequence of facial expressions. The definition of the VSL-CRF in Eq.11 allows it to simultaneously fit both ordinal and nominal latent states to each sequence, which may result in the model overfitting. In the following, we introduce two novel learning strategies in order to avoid overparametrization of the model, i.e., to prevent the model from using redundant nominal and/or ordinal latent states during inference of target sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">VSL-CRF: Learning and Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max-pooling of latent states</head><p>The first learning strategy that we propose constraints the latent states to take either nominal or ordinal sequence of latent states per target sequence. This is different from H-CRF/H-CORF where the latent states can be either nominal/ordinal for each and every target class and the sequence. Formally, the conditional probability in Eq.11 is now given by:</p><formula xml:id="formula_19">P(y|x) = max( ν h exp(s(y, x, h, ν))) Z(x)<label>(13)</label></formula><formula xml:id="formula_20">Z(x) = k max( ν h exp(s(k, x, h, ν)))<label>(14)</label></formula><p>The key aspect of this approach is that now the type of the latent states is explicitly constrained to either nominal or ordinal. This, in turn, leads to the following (regularized) loss function of the VSL-CRF model (further in the text, we denote this model as VSLm):</p><formula xml:id="formula_21">RLL(Ω) = - N i=1 log P(y i |x i ; Ω) + λ n(o) ||θ n(o) k=1..K || 2 ,<label>(15)</label></formula><p>where</p><formula xml:id="formula_22">Ω = {θ n k , θ o k } K k=1 .</formula><p>We introduce L-2 regularization over the parameters of the nominal/ordinal score functions, the effect of which is controlled by λ n /λ o , which are found using a validation procedure.</p><p>Unfortunately, the objective function of the VSLm model is both non-convex and non-smooth because of the max function in its conditional distribution. Therefore, the gradients of the objective in Eq.15 w.r.t. the parameters Ω cannot be directly computed. Yet, the nominal/ordinal score functions are both sub-differentiable. We use this property to construct the subgradient <ref type="bibr" target="#b41">[42]</ref> of the VSLm objective at Ω. Essentially, this boils down to computing the following sub-gradients:</p><formula xml:id="formula_23">∂Ω = ∇ max( ν h exp(s(k, x, h, ν))), k = 1, . . . , K,</formula><p>which are further given by</p><formula xml:id="formula_24">                 ∂Ω = ∇ h exp(s(x, h, θ n k )), i f h exp(s(x, h, θ n k )) &gt; h exp(s(x, h, θ o k )) ∂Ω = ∇ h exp(s(x, h, θ 0 k )), otherwise.</formula><p>Thus, at a point Ω * where one of the score functions, say nominal, gives a higher score than the ordinal for the given sequence, max(</p><formula xml:id="formula_25">ν h exp(s(k, x, h, ν))</formula><p>) is differentiable and has the gradient</p><formula xml:id="formula_26">∂θ n k = ∇ h exp(s(x, h; θ n k )), while ∂θ o k = 0.</formula><p>In other words, to find a subgradient of the maximum of the score functions, we choose the score functions that achieves the maximum for the target sequence at the current parameters, and compute the gradient of that score function only. Once this is performed, the gradient derivation is the same as in the H-CRF/H-CORF models (see <ref type="bibr" target="#b6">[7]</ref> for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fully integrated out latent states</head><p>The benefits of the VSLm approach are that it prevents the VSL-CRF model from redundant parametrization of the VSL-CRF model that can easily lead to the model overfitting. However, the sub-gradient optimization approach can easily get trapped in the local minimum when searching for the model parameters due to the gradient 'switching' caused by the max function in the objective. To this end, we also employ a learning strategy where both types of the latent states (ordinal and nominal) are fully integrated out, which can be solved using the standard gradient descent optimization as in the existing L-CRF models. Of course, the downside of this is that we may end up with over-parametrization of the target sequences. To remedy this, in addition to direct optimization of the conditional probability, we also introduce an EM approach to the parameter learning. In the proposed EM learning strategy, we exploit the hierarchy in the VSL-CRF model, which allows us to integrate out the latent states h and the indicator variable ν in an alternating fashion. Note that no empirical studies that investigate the performance of EM vs. the direct optimization in the context of L-CRFs have been reported so far. Furthermore, we introduce novel posterior regularization (see Sec.  enforcing the model to select either nominal or ordinal latent states for each target sequence during learning<ref type="foot" target="#foot_1">foot_1</ref> . Formally, the objective function is given by:</p><formula xml:id="formula_27">RLL(Ω) = - N i=1 log P(y i |x i ; Ω) + λ n(o) ||θ n(o) k=1..K || 2 + λ p ν R ν<label>(16)</label></formula><p>where P(y i |x i ; Ω) is defined by Eq.14, and λ p controls the strength of the posterior regularization defined in Sec.3.3. We detail below the two learning approaches. 1. Direct optimization. Direct optimization of the objective function is performed by minimizing the objective function in Eq.16 directly w.r.t. all parameters Ω of the model. We denote this approach as VSLd. The gradients of the log-likelihood function in the first term on the right side of Eq.16 are given by: ∂log(P(y, ν|x)) ∂Ω =E P(ν,h|x,y) [ ∂s(y, ν, x, h) ∂Ω ]</p><formula xml:id="formula_28">-E P(y,ν,h|x) [ ∂s(y, x, h) ∂Ω ]</formula><p>The sum of gradient derivations for H-CRF (for ν = 0) and H-CORF (for ν = 1) can be used to obtain these gradients. The computation of the gradients for the model parameters w.r.t. the regularizers in Eq.16 is then straightforward. In all our experiments, we used the Limited-memory BFGS method for the gradient computation.</p><p>2. Expectation-Maximization (EM) optimization. Alternatively, the model parameter can be obtained using the EM algorithm. The EM algorithm <ref type="bibr" target="#b9">[10]</ref> is an iterative optimization approach that can be employed to find the latent state parameters Ω that maximize the VSL-CRF objective (Eq.16) in two steps. In the E-step, the posterior probability of the binary latent variable ν is computed as P(ν|x, y), i.e., by integrating out the latent states h, for each target sequence. Then, the maximumlikelihood parameter estimates of the model parameters Ω are computed in the M-step. This process is repeated until the convergence of the objective in Eq.16. More specifically, in the E-step, we compute the posterior probabilities for each target sequence using the auxiliary function:</p><formula xml:id="formula_29">q(ν i ) = p(ν i |y i , x i , Ω j ) (<label>17</label></formula><formula xml:id="formula_30">)</formula><p>This is followed by the M-step, where a new parameter vector Ω j+1 is obtained by maximizing the likelihood function using the current posterior for ν:</p><formula xml:id="formula_31">Ω j+1 = arg max Ω i=1,...,N ν i q(ν i ) log P(y i , ν i |x i , Ω j ) -λ n(o) ||Ω j || 2 -λ p ν R ν .<label>(18)</label></formula><p>In our experiments, we initialized the model with a uniform distribution P(ν = o, k) = 0.5 and P(ν = n, k) = 0.5 for all k and ran the EM-algorithm until it converged. We denote this learning approach as VSLem. It is important to mention that the most important aspect of the VSLem approach, compared to the VSLd, is that the in the latter, the importance of both nominal and ordinal states is equal and does not change during learning. By contrast, through the E-step, the VSLem dynamically adapts the weight of each model (nominal vs ordinal) for each sequence. Together with the proposed posterior regularization, this is expected to drive the type of latent states for each sequence to either nominal or ordinal, and thus, avoid over-parametrization of the target data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction</head><p>Once the model parameters Ω are learned using either of the proposed approaches (VSLm, VSLd or VSLem), the in-ference of test data can be performed two ways, depending on the target task. The first task is sequence-based classification of facial expressions. The goal here is to classify the presegmented sequences of facial expressions (e.g., emotions) into one of target classes. In the case of AUs, the goal is to perform detection of the target AU from pre-segmented sequences classified into active (containing activations of the target AU), and 'all other' (containing neutral facial expressions and/or facial expressions of non-target AUs). The assignment of a test sequence to the particular class is accomplished by the MAP rule y * = arg max y P(y|x * ). In the case of frame-based classification of target facial expressions, the learned models are used to compute the likelihood of each time-window in the input test sequence. Then, the central frame in the window is assigned the target class, as given by the MAP rule mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Posterior Regularization</head><p>In this section, we show how geometric knowledge of the posterior probability distribution can be used in our optimization framework. This is motivated by recent works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref> on posterior regularization in the conditional models, used to improve the parameter learning by incorporating prior knowledge. Formally, let Θ denote model parameters and H denote hidden variables. Given a set of observed data D, posterior regularization is generally defined as solving a regularized maximum likelihood estimation (MLE) problem:</p><formula xml:id="formula_32">P(y|x) = max Θ L(Θ; D) + Φ(p(H|D, Θ))<label>(19)</label></formula><p>where L(Θ; D) is the marginal likelihood of D, and Φ(•) is a regularization function of the model posteriors over latent variables. A common definition for Φ(•) is the KL-divergence between a desired distribution with certain properties over latent variables and the model posterior distribution. In this paper, H corresponds to the sequence latent state ν. This parameter is not known and no assumptions can be made in order to construct the KL-divergence. However, we make use of the prior knowledge that sequences, which are sampled from the same class should have the same latent states. For instance, we assume that if two sequences {y 1 , x 1 } and {y 2 , x 2 } are from the same target class k, then the conditional probabilities P(ν|y 1 = k, x 1 ) and P(ν|y 2 = k, x 2 ) should be similar. Suppose further that there are K classes and let f ν,k (x) = P(ν|y = k, x) be the conditional posterior probability density function for each class defined as P(ν|x, y) = h P(h, ν|x, y). Then, the regularization is performed by minimizing the distance between each element of f ν having the same class label. This can be solved by using the graph Laplacian L <ref type="bibr" target="#b48">[49]</ref> regularization approach. To this end, we construct a graph G in which each node n i corresponds to a sequence x i with the class label y i . We connect all nodes with edges e i j that have the weight s i j , which is defined by a similarity matrix S . In this work, we assign value 1, if and only if y i = y j , i, j = 1, . . . , N, and 0 otherwise. This ensures that only the sequences that come from the same class of facial expressions or contain activation of the same AU, are connected. Finally, the graph Laplacian is constructed as L = D -S , where D is a diagonal matrix, the entries of which are column-sums of S , that is, D i j = j S i j . Then, the proposed posterior regularization R ν is defined as follows:</p><formula xml:id="formula_33">R ν = 1 2 m i, j=1</formula><p>S i j • (P(ν|y i , x i ) -P(ν|y j , x j ))</p><formula xml:id="formula_34">= m i=1 P(ν|y i , x i ) 2 D i j - m i, j=1</formula><p>P(ν|y i , x i )P(ν|y j , x j )S i j</p><formula xml:id="formula_35">= f T ν D f ν -f T ν S f ν = f T ν L f ν</formula><p>where</p><formula xml:id="formula_36">f ν = (P(ν|y 1 , x 1 ), ..., P(ν|y m , x m )) T<label>(20)</label></formula><p>Note that the larger values of the disparity in f ν result in a larger regularization loss R ν for state ν = {n, o}. The matrix L is positive semi-definite, so R ν is convex in f ν and by minimizing R ν , we get a conditional distribution f ν which is sufficiently smooth on the data manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate performance of the proposed VSL-CRF model and different learning strategies in the tasks of classification of facial expressions of emotions, and AU detection. The presented experiments are conducted on three publicly available facial expression datasets: Extended Cohn-Kanade (CK+) <ref type="bibr" target="#b42">[43]</ref>, GEMEP-FERA <ref type="bibr" target="#b43">[44]</ref> and DISFA <ref type="bibr" target="#b44">[45]</ref>. We also compare the performance of the proposed models with the state-of-the-art methods for both tasks, in the sequence-based classification and frame-based detection settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup Datasets</head><p>The facial expression datasets used in this work are summarized in Table <ref type="table">2</ref>. The CK+ dataset contains 593 facial expression sequences from 123 different subjects. Each sequence begins with a neutral face and ends at the peak intensity of facial expression of target emotion category. In total, 327 sequences that are labeled in terms of the basic emotions: Anger, Contempt, Disgust, Fear, Happiness, Sadness, or Surprise, are used. We performed 10-fold subject-independent cross-validation on this dataset. The GEMEP-FERA dataset contains 87 image sequences of 7 subjects with the per-frame labels for the AU <ref type="bibr">(1,2,4,6,7,10,12,15,17,18,25 and 26)</ref> activations (present or not). Furthermore, in the target videos, each participant shows facial expressions of the emotion categories: Anger, Fear, Joy, Relief or Sadness. We report our results using a 7 fold subject-independent cross validation, where each fold contained image sequences of a different subject. The DISFA dataset, contains 32 sequences from 27 subjects. Each sequence in this dataset is 4000 frames long, and each frame is labeled in terms of the intensity level (using FACS) for each  AU <ref type="bibr">(1,2,4,5,6,9,12,15,17,20,25 and 26)</ref>. For our detection approach, we used the frames with the AU intensity higher than 0 as positive examples, and the remaining ones as negative.</p><p>We performed a 10 fold subject independent cross-validation on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence-based training</head><p>The proposed models require sequential data for training and prediction and the CK+ database can be directly used. However, the AU databases GEMEP-FERA and DISFA require a pre-segmentation step in order to extract sequence training data from these databases. We created a training datasets that consists of active and not-active subsequences of each AU. More specifically, from the full dataset, we selected the segments in which the target AU is active (inactive) for the duration of at least 6 frames, and used these as positive (negative) sequences for training. We then balanced the data by removing inactive sequences. Note that we selected the threshold of 6 frames because less than this consistently downgrades the performance on most target AUs, as can be seen from Fig. <ref type="figure" target="#fig_6">5</ref>. Once the VSL-CRF models are trained using these pre-segmented data, we apply it in both sequence-based and frame-based manner, as explained in Sec.3.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Features</head><p>We used the locations of 49 facial points, extracted from target images sequences using the appearance-based facial tracker in <ref type="bibr" target="#b51">[52]</ref>. Fig. <ref type="figure" target="#fig_3">2</ref> depicts the used facial points from each dataset as input features. The pre-processing of the features was performed by first applying Procrustes analysis to align the facial points to the mean faces of the datasets. This is important in order to reduce the effects of head-pose and subject-specific variation. We then applied PCA to reduce the feature size, retaining 97% of energy, resulting in 18, 21, and 24 dimensional feature vectors for the CK+, DISFA and GEMEP-FERA datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter selection</head><p>The model parameters that need to be pre-defined are the fixed number of latent states C and the regularization parameter λ o ,λ n and λ p . We found the optimal number of latent states by applying a grid search over different settings (in a subjectindependent manner). In particular, we applied a two fold cross validation on different AUs from target datasets. To illustrate this, in Fig. <ref type="figure" target="#fig_5">4</ref> we show the F1-scores for sequence-based detection of AU6 from the GEMEP-FERA and DISFA datasets when a different number of latent states is used in the compared models: H-CRF, H-CORF and VSLd. The results drop for the H-CRF model when selecting more than 4 latent states per class. This is mainly because of overfitting but also because of the higher dimensionality of the problem. This effect is not significant for the H-CORF model since the ordinal constrains prevent this model from overfitting. However, in all experiments on all AUs, the F1-measure has a strong increase from 2 to 3 hidden states, which is the number of states corresponding to the temporal phases of expression development (neutral-onset/offset-apex). Adding more states does not improve the models' performance significantly but increases their complexity. Therefore, we set in all our experiments the number of hidden states C = 3 for both ordinal and nominal classes. It is important to mention that although VSL-CRF has more latent states per class (3 nominal and 3 ordinal), as noted above, increasing the number of states in H-CRF and H-CORF does not improve their performance significantly. Consequently, the difference in the performance of the compared models (shown in the experiments below) cannot be attributed to the difference in the number of their latent states. Lastly, the regularization parameters λ n/o and λ p were set using a grid-search procedure on the validation set found separately for each target fold (no test data were used to perform this validation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Measure</head><p>We report the classification/detection results using the standard F1-score. This score is widely used for AU-detection and classification of facial expressions of emotions because of its robustness to the imbalance in positive and negative samples, which is very common in the case of AUs. For each AU, the F1measure is computed based on a frame-based detection (i.e. an AU detection has to be specified for every frame, for every AU, as being either present or absent). We also provide the results for the sequence-based classification, where the F1-score for sequences is computed based on a sequence-based prediction, and then weighted by the number of frames in each sequence. We do so in order to have the fair comparison with the frame-based approaches. We refer to these metrics F1-sequence-based for the sequence based approaches, and the F1-frame-based for the frame-based detection. For emotion classification, we used the F1 score, without weighting with the number of frames in the expression sequence, as methods compared on the CK+ dataset perform the sequence-based classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Compared Methods</head><p>In all our experiments, as the baseline for the classification we also include the results obtained by first applying the multiclass SVMs (with the RBF kernel) and trained/evaluated per frame to obtain the F1-frame-based measure. The sequence labels and the F1-sequence-based measure were obtained by majority voting over the frames within the sequence. The results for H-CRF and H-CORF, were obtained using our own implementation <ref type="foot" target="#foot_2">3</ref> . The initial parameters of the models were set using the same approach as in the VSL-CRF. To compare the performance of target models with the state-of-the-art models for each of target tasks (sequence-based emotion recognition and frame-based AU detection), we report the results from the original papers, as detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence-based Methods</head><p>Note that some of the methods compared use different number of folds when performing cross-validation on the CK+ dataset. Specifically, PO-HCRF9 (partially observed H-CRF) <ref type="bibr" target="#b15">[16]</ref> used a 5-fold cross-validation. In this method, some states are observed during training and represent activations of AUs but the goal is to classify emotions. TMS <ref type="bibr" target="#b14">[15]</ref> (Temporal Modeling of Shapes) uses Latent-Dynamic CRFs <ref type="bibr" target="#b22">[23]</ref> for a framebased prediction. However, this predictions are then used to obtain the sequence label. They applied a 4-fold cross validation. ITBN <ref type="bibr" target="#b23">[24]</ref> (Interval Temporal Bayesian Network) aims to model temporally overlapping or sequential primitive facial events and the experiments are performed in a 15-fold cross validation setup. Cov3D <ref type="bibr" target="#b50">[51]</ref> is based on spatio-temporal covariance descriptors. The descriptors belong to the group of matrices, which can be formulated as a connected manifold. The authors used a 5-fold cross validation. The Constrained Local Method (CLM) <ref type="bibr" target="#b49">[50]</ref> is a generic or person-independent face alignment algorithm with goal of finding the shape which is described by a 2D trianguleted mesh that fits the target face. They use a 10-fold experimental setup. The MTSL <ref type="bibr" target="#b19">[20]</ref> is a multitask sparse learning framework in which expression recognition and face verification tasks, are coupled to learn specific facial patches for individual expression. Lastly, we compare our method to the state-of-the-art method for target task, STM-ExpLet <ref type="bibr" target="#b7">[8]</ref>. The approach combines low-level features from videos with a spatio-temporal manifold learning framework and they evaluate the method using 10-fold cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence-based Results</head><p>Table <ref type="table">3</ref> shows the results for facial expression recognition from the CK+ dataset. The average classification rate is obtained by unweighted averaging of the results of the 6 basic emotion (*) plus the contempt emotion. Note that while the results of the compared L-CRF models are directly comparable, as they are trained/tested on the same data/folds, this is not the case with the rest of the models as they use different evaluation settings. However, we report their performance for the sake of comparisons. Note also that in this task, i.e., the classification of facial expressions of emotions, the dynamic methods (H-CRF, H-CORF and VSL-CRF) outperform by the large margin the sequence-based SVM classifier that does not account for temporal dynamics. This table also shows that the proposed variable-state method outperforms the other methods that do not have the flexibility to select the best latent states. On the other hand, the proposed VSLem learning strategy improves the classification performance compared to the other two introduced learning methods (VSLm and VSLd). We attribute this to the iterative learning of the latent states, as well as the posterior regularization, which, evidently, together help to increase the discriminative power of the VSL-CRF model. Lastly, the proposed VSLem achieves the state-of-the-art performance in the target task by performing similar or better than the best performing state-of-the-art models, STM-ExpLet and TMS.</p><p>Tables <ref type="table">1</ref> and<ref type="table">3</ref> show the results for AU detection on the DISFA and GEMEP-FERA database using pre-segmented sequences. Again, the proposed VSL-CRF model outperforms the models that use only nominal (H-CRF) or ordinal (H-CORF) states, trained/tested on identical data/folds. Furthermore, the highest detection rate is again achieved using the VSLem model on both the DISFA and GEMEP-FERA datasets. Moreover, all the VSL-CRF methods achieve significantly higher results than the other L-CRF models, which is mainly because of the ability to select the optimal states per sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame-based Methods</head><p>We also compared the variable state models with recent methods for frame-based AU detection. The first related method, Early Fusion (EF) <ref type="bibr" target="#b20">[21]</ref>, applies a hierarchical Gaussianization and scale-invariant feature transform on motion features. The classification is done by SVMs. In MKL <ref type="bibr" target="#b26">[27]</ref>, a kernalized SVM is trained for each AU and the outputs are averaged in order to exploit temporal information. CoT (Cascade of Tasks) <ref type="bibr" target="#b36">[37]</ref> is trained on sequences and applies segmentbased detection. This approach is a combination of three simple algorithms for static-frame-level-detection, segment-leveldetection and transition-level detection. Selective Transfer Machine (STM) <ref type="bibr" target="#b4">[5]</ref> is based on static SVMs, which personalizes the generic SVM classifier by learning the classifier and reweighting the training samples that are most relevant to the test subject during inference. HMTMKL <ref type="bibr" target="#b27">[28]</ref> is a method for multiple AU recognition. A multi-task feature learning (MTFL) algorithm is adopted to learn the shared features among AUs and recognize AUs simultaneously. The AU relations are then modeled by a Bayesian graphical model. Finally, <ref type="bibr" target="#b28">[29]</ref> is also a multi task learning approach and applies simultaneous detection of multiple facial AUs by exploiting their inter-relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame-based Results</head><p>The experiments for per-frame AU detection were performed on the GEMEP-FERA and DISFA database, where we applied a sliding window to each frame in order to obtain the predictions per frame (by assigning the classifier's prediction to the central frame in the window). For each AU, we cross-validated over different window sizes to find the optimal size per AU. The results are shown in Fig. <ref type="figure" target="#fig_6">5</ref>. Interestingly, the average window size on the AUs from the GEMEP-FERA dataset is shorter than that of the AUs from the DISFA dataset. This is mainly because both datasets contain facial expressions recorded in different contexts (acted vs. spontaneous), so this difference in the duration of the AU activations is expected. Also, in DISFA, the expressions are less dynamic because the participants respond spontaneously to the watched youtube videos, while in GEMEP-FERA, the participants are actors and show much more dynamic emotions like 'Anger' or 'Fear' with fast facial muscle movements.</p><p>Table <ref type="table">4</ref> shows the F1-measure for the detection of each AU from the GEMEP-FERA dataset with the window size reported in brackets. The STM <ref type="bibr" target="#b4">[5]</ref>, despite the subject adaptation, still fails to reach the full performance of the VSLem model on the mutual set of evaluated AUs. This is attributed to the fact that the STM does not model the temporal dynamics. But again, different settings were used in these evaluations. These results demonstrate again that the assignment of both types of latent states, as done in the VSL-CRF models is critical for achieving superior performance on this task. Table <ref type="table">2</ref> shows the results on the DISFA dataset. The two Multi-task learning approaches (MTL) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b52">53]</ref> apply simultaneous detection of multiple facial AUs by exploiting their inter-relationships. They also model the correlation among AUs which results in the very high detection rate. The proposed VSL-CRF model reaches the results that are comparable with that of the state-of-the-art. The high F1frame-based score achieved by both methods demonstrates the importance of both the modeling of the inter-relationships of AUs, as done in the former, and dynamics, as done in the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Sequence-based Cross-database Results</head><p>Detecting AUs across datasets is challenging because of differences in contexts in which this data is recorded (acted vs. spontaneous, illumination, frame rate, etc.). In this experiment, we apply the VSL-CRF models, the H-CRF and H-CORF models, and the baseline SVM on the pre-segmented sequence form the AU databases GEMEP-FERA and DISFA. Table <ref type="table" target="#tab_2">6</ref> and<ref type="table" target="#tab_1">5</ref> show the results for the experiment in which we trained the models using the GEMEP-FERA database and evaluated them on the DISFA database, and the other way round, respectively. We observe that in this setting also the proposed VSL-CRF models outperform nominal-or ordinal-state methods, and the static SVM. This demonstrates the strong generalization capability of the proposed models. It is interesting to note that this difference is much smaller in the results reported in Table <ref type="table" target="#tab_2">6</ref>, where HCRF achieves similar results to VSLem, compared to Table <ref type="table" target="#tab_1">5</ref>, where the HCRF and H-CORF are largely outperformed by the VSL-CRF models. We attribute this to the fact that the acted data (GEMEP-FERA) contains much more variation in facial expressions compared to spontaneous expressions in DISFA dataset. Consequently, the models are learned on more diverse data, allowing them to generalize better to subtle facial expressions, as evidenced by this experiment. We also observe that all three VSL-CRF learning approaches perform similarly in this setting. A possible reason is that since the data distributions vary significantly across the datasets (in terms of number of active examples, as well as the AU co-occurrences), this limits the proposed learning approaches to reach their full performance. Finally, note that the perfomance on the both datasets drops significantly compared to the results in Tables <ref type="table">1</ref> and<ref type="table">3</ref>. For exmaple, for GEMEP-FERA, the results on the used set of AUs from from 60.2% to 39.2% for the best performing model. This indicates the importance of accounting for the dataset-differences during modeling of facial expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">The Effect of Posterior Regularization</head><p>On all datasets, the VSLd and VSLem outperforms VSLm. This is mainly attributed to the more flexible representation of the latent states as well as the additional posterior regularization. To get some insights into the behavior of the posterior regularization during the learning process, we performed additional experiment on the CK+ dataset. Specifically, we trained the VSLem model with and without the posterior regularization and monitored the parameter for each EM-iteration (the graphs showing the changes in the nominal/ordinal states on the training data). The training/test sets consisted of 162 sequences each, and are sorted according to the sequence label. The results are shown in Fig. <ref type="figure">6</ref>. The bar on the right side of each main figure shows the contribution of ordinal/nominal states for the prediction of the test sequences. We can see that the emotion happiness exhibited a strong ordinal structure as encoded with its ordinal states, while the other emotion were predicted using the nominal states. The figure on the right shows the same learning process with active posterior regularization. Again, the emotion happiness was trained and predicted using mainly the ordinal states but all other emotions mainly preferred using the nominal states during training and inference, as the result of the regularization. The learned type of the latent states is also consistent on the test data. Finally, although only emotion happiness showed strong ordinal nature, as learned from the employed features of facial expressions, the nominal states selected for the other emotion categories do not imply that there is no ordinal structure in their facial expressions but that the nominal states were a better fit for the target data used in this experiment. Note also that when the posterior regularization is used, the F1-sequence-based measure on the test sets is higher (69.5% vs. 67.9%), demonstrating the benefit of the posterior regularization. Furthermore, note that this regularization enforces the model to converge to either nominal or ordinal states during the model learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed a novel Variable-State Conditional Random Field model for dynamic facial expression recognition and AU detection. By allowing the structure of the latent states of target classes to vary for each target sequence, the proposed model can better discriminate between different facial expressions than the existing models that restrict their latent states to have the same and pre-defined structure for all classes (nominal or ordinal). For this model, we proposed two novel learning strategies and the posterior regularization of the latent states, resulting in a more robust model for the target tasks. This leads to superior performance compared to traditional latent CRF models. We also showed on three facial expression datasets that the proposed model performs similar or better than the state-of-theart for the task of sequence-based facial expression recognition, and that it reaches state-of-the-art performance for the task of per-frame AU-detection. The future work should focus on more detailed analysis of the learning of the target latent states within each emotion class and AU (e.g., the automated selection of the window size for each AU), as well as analysis of the relations between the learned latent states and the temporal aspects of facial expressions such as their temporal phases and intensity. Also, extending the proposed approach so that it can handle simultaneous detection of multiple AUs, and its adaptation to previously unseen datasets, are also interesting avenues to pursue. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where Φ(•) is the standard normal cumulative density function (c.d.f.), and f o (x) = a T x. The parameter vector a is used to project the input features onto an ordinal line divided by the model thresholds or cut-off points b 0 = -∞ ≤ • • • ≤ b C = ∞, with each bin corresponding to one of the ordinal states c = 1, ..., C in the model. The ranking likelihood in (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3.3)  in the objective function of these approaches, with the aim of implicitly Dataset per-frame -(a) CK+ [43] (b) GEMEP-FERA [44](c) DISFA<ref type="bibr" target="#b44">[45]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sample images with the used facial points from different datasets</figDesc><graphic coords="7,126.53,215.46,78.62,58.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Per-sequence classification rate on the CK+ database and comparison with the state-of-the-art.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Cross validation over the number of latent states. The tables show the F1-per-sequence measure on AU6 from (a) the GEMEP-FERA and (b) the DISFA datasets w.r.t. the different number of the latent states (nominal and ordinal). In the case of VSL-CRF, the shown number is used separately for nominal and ordinal states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: F1-meassure per AU for different window sizes for the frame-based VSLem detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 5 :</head><label>5</label><figDesc>Per-sequence classification rate on the cross dataset experiment DISFA → GEMEP-FERA</figDesc><table><row><cell>AU</cell><cell>SVM</cell><cell>HCRF</cell><cell cols="2">HCORF VSLm</cell><cell>VSLd</cell><cell>VSLem</cell><cell>AU</cell><cell>SVM</cell><cell>HCRF</cell><cell cols="2">HCORF VSLm</cell><cell>VSLd</cell><cell>VSLem</cell></row><row><cell>1</cell><cell>40.0</cell><cell>44.2</cell><cell>39.7</cell><cell>42.6</cell><cell>43.3</cell><cell>43.7</cell><cell>1</cell><cell>28.5</cell><cell>32.8</cell><cell>29.6</cell><cell>35.2</cell><cell>40.0</cell><cell>35.2</cell></row><row><cell>2</cell><cell>40.4</cell><cell>44.8</cell><cell>47.4</cell><cell>43.3</cell><cell>42.8</cell><cell>41.2</cell><cell>2</cell><cell>37.2</cell><cell>45.7</cell><cell>41.3</cell><cell>49.4</cell><cell>49.3</cell><cell>40.9</cell></row><row><cell>4</cell><cell>33.3</cell><cell>33.5</cell><cell>25.3</cell><cell>22.4</cell><cell>34.8</cell><cell>34.0</cell><cell>4</cell><cell>25.9</cell><cell>44.9</cell><cell>29.2</cell><cell>24.8</cell><cell>35.5</cell><cell>40.9</cell></row><row><cell>6</cell><cell>57.7</cell><cell>54.1</cell><cell>46.3</cell><cell>58.7</cell><cell>49.7</cell><cell>54.5</cell><cell>6</cell><cell>50.8</cell><cell>44.6</cell><cell>39.9</cell><cell>48.6</cell><cell>42.6</cell><cell>48.4</cell></row><row><cell>12</cell><cell>23.7</cell><cell>35.9</cell><cell>34.5</cell><cell>33.0</cell><cell>36.0</cell><cell>37.4</cell><cell>12</cell><cell>21.2</cell><cell>32.1</cell><cell>26.2</cell><cell>42.7</cell><cell>28.5</cell><cell>39.6</cell></row><row><cell>17</cell><cell>22.2</cell><cell>29.4</cell><cell>16.9</cell><cell>19.8</cell><cell>24.6</cell><cell>25.6</cell><cell>17</cell><cell>26.6</cell><cell>23.1</cell><cell>21.9</cell><cell>25.4</cell><cell>25.3</cell><cell>32.7</cell></row><row><cell>25</cell><cell>37.2</cell><cell>67.9</cell><cell>44.6</cell><cell>46.7</cell><cell>44.4</cell><cell>45.6</cell><cell>25</cell><cell>42.1</cell><cell>46.5</cell><cell>50.5</cell><cell>52.6</cell><cell>53.2</cell><cell>45.3</cell></row><row><cell>26</cell><cell>37.5</cell><cell>31.4</cell><cell>36.0</cell><cell>37.3</cell><cell>33.2</cell><cell>32.4</cell><cell>26</cell><cell>22.2</cell><cell>34.1</cell><cell>33.0</cell><cell>33.3</cell><cell>37.8</cell><cell>38.0</cell></row><row><cell>AVG</cell><cell>35.3</cell><cell>38.9</cell><cell>36.3</cell><cell>37.9</cell><cell>38.6</cell><cell>39.2</cell><cell>AVG</cell><cell>34.3</cell><cell>36.7</cell><cell>33.9</cell><cell>39.0</cell><cell>39.1</cell><cell>40.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 6 :</head><label>6</label><figDesc>Per-sequence classification rate on the cross dataset experiment GEMEP-FERA → DISFA</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For simplicity, we often drop the dependency on θ in notations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Note that this regularization does not apply to the VSLm approach as the 'hard' selection of the latent states is achieved using the max function.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We provide a toolbox with the Matlab code for the compared H-CRF, H-CORF and VSL-CRF models, at http://ibug.doc.ic.ac.uk/resources/DOC-Toolbox/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been funded by the <rs type="funder">European Community Horizon 2020</rs> [<rs type="grantNumber">H2020/2014-2020</rs>] under grant agreement no. <rs type="grantNumber">645094</rs> (SEWA). The work of <rs type="person">Vladimir Pavlovic</rs> has been funded by the <rs type="funder">National Science Foundation</rs> under Grant no. <rs type="grantNumber">IIS0916812</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5P4EAR4">
					<idno type="grant-number">H2020/2014-2020</idno>
				</org>
				<org type="funding" xml:id="_qBtNzMS">
					<idno type="grant-number">645094</idno>
				</org>
				<org type="funding" xml:id="_b7V7ut2">
					<idno type="grant-number">IIS0916812</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Context-sensitive dynamic ordinal regression for intensity estimation of facial action units</title>
		<author>
			<persName><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic analysis of facial expressions: The state of the art</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J M</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1424" to="1445" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Measuring facial actions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The New Handbook of Methods in Nonverbal Behavior Research</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Harrigan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Rosenthal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="9" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hager</surname></persName>
		</author>
		<title level="m">Facial Action Coding System (FACS): Manual. A Human Face</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Selective transfer machine for personalized facial action unit detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3515" to="3522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hidden conditional random fields for gesture recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Demirdjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1104" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hidden conditional ordinal random fields for sequence classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6322</biblScope>
			<biblScope unit="page" from="51" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1749" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey of affect recognition methods: Audio, visual, and spontaneous expressions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Roisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="58" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning (Information Science and Statistics)</title>
		<meeting><address><addrLine>Secaucus, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<title level="m">Conditional random fields: Probabilistic models for segmenting and labeling sequence data. ICML &apos;01</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-output laplacian dynamic ordinal regression for facial expression recognition and intensity estimation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="2634" to="2641" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conditional models for contextual human motion recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="210" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Hidden conditional random fields. transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">1848</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Facial expression recognition with temporal modeling of shapes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;W</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1642" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning partially-observed hidden conditional random fields for facial expression recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="533" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conditional mutual infomation based boosting for facial expression recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Real time face detection and facial expression recognition: Development and applications to human computer interaction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Facial action recognition for facial expression analysis from static face images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1449" to="1461" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning multiscale active facial patches for expression analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part B</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1499" to="1510" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Emotion recognition from an ensemble of features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hsiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhaowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tony X L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition and Workshops (FG 2011)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="872" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nonparametric discriminant hmm and application to facial expression recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2090" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Authentic facial expression analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1856" to="1863" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Capturing complex spatio-temporal relations among facial muscles for facial expression recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3422" to="3429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Facial action unit detection using probabilistic actively learned support vector machines on tracked facial point data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved facial expression recognition via uni-hyperplane classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2554" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Facial action recognition combining heterogeneous features via multikernel learning. Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sénéchal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Salam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Seguier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Prevost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="993" to="1005" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiple-facial action unit recognition by shared feature learning and semantic relation modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2014 22nd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1663" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A l p-norm mtmkl framework for simultaneous detection of multiple facial action units</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="1104" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Exploiting sparsity and co-occurrence structure for action unit recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vasisht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A dynamic texture-based approach to recognition of facial actions and their temporal models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Koelstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1940" to="1954" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Facial action detection using blockbased pyramid appearance descriptors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social Computing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fully automatic recognition of the temporal phases of facial actions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems, Man, and Cybernetics, Cybernetics, Transactions on</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="43" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Action unit classification using active appearance models and conditional random fields</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hendriks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="507" to="518" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A dynamic appearance descriptor approach to facial actions temporal modeling</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="174" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Kernel conditional ordinal random fields for temporal segmentation of facial action units</title>
		<author>
			<persName><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Facial action unit event detection by cascade of tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2400" to="2407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<title level="m">Conditional random fields: Probabilistic models for segmenting and labeling sequence data. ICML</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Regression models for ordinal data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mccullagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Stat. Society. Series B</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="109" to="142" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Structured output ordinal regression for dynamic facial emotion intensity prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="649" to="662" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Conditional random fields for object recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1104" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Validation of subgradient optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Crowder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical programming</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="88" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion expression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The first facial expression recognition and analysis challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG 2011</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="921" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Disfa: A spontaneous facial action intensity database. Affective Computing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grac ¸a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2001">2001-2049, 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bayesian inference with posterior regularization and applications to infinite latent svms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1799" to="1847" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cross-lingual discriminative learning of sequence models with posterior regularization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="1996">1996-2006, 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Spectral graph theory (cbms regional conference series in mathematics</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>American Mathematical Society</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Person-independent facial expression detection using constrained local models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="915" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Spatiotemporal covariance descriptors for action and gesture recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Tafazzoli</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian C L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2013 IEEE Workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Incremental face alignment in the wild</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On multi-task learning for facial action unit detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image and Vision Computing New Zealand (IVCNZ), 2013 28th International Conference of</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="202" to="207" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
