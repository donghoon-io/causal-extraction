<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-Aware Safe Reinforcement Learning for Non-Stationary Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-01-02">2 Jan 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Baiming</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zuxin</forename><surname>Liu</surname></persName>
							<email>zuxinl@andrew.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jiacheng</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mengdi</forename><surname>Xu</surname></persName>
							<email>mengdixu@andrew.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Wenhao</forename><surname>Ding</surname></persName>
							<email>wenhaod@andrew.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ding</forename><surname>Zhao</surname></persName>
							<email>dingzhao@andrew.cmu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Baiming Chen is with Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>China</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Context-Aware Safe Reinforcement Learning for Non-Stationary Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-02">2 Jan 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2101.00531v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Safety is a critical concern when deploying reinforcement learning agents for realistic tasks. Recently, safe reinforcement learning algorithms have been developed to optimize the agent's performance while avoiding violations of safety constraints. However, few studies have addressed the non-stationary disturbances in the environments, which may cause catastrophic outcomes. In this paper, we propose the context-aware safe reinforcement learning (CASRL) method, a meta-learning framework to realize safe adaptation in nonstationary environments. We use a probabilistic latent variable model to achieve fast inference of the posterior environment transition distribution given the context data. Safety constraints are then evaluated with uncertainty-aware trajectory sampling. The high cost of safety violations leads to the rareness of unsafe records in the dataset. We address this issue by enabling prioritized sampling during model training and formulating prior safety constraints with domain knowledge during constrained planning. The algorithm is evaluated in realistic safety-critical environments with non-stationary disturbances. Results show that the proposed algorithm significantly outperforms existing baselines in terms of safety and robustness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Reinforcement learning (RL) is a promising way to solve sequential decision-making tasks. For example, RL has shown superhuman performance in competitive games like Go <ref type="bibr" target="#b0">[1]</ref> and Starcraft <ref type="bibr" target="#b1">[2]</ref>. RL has also been used for the control of complex robotic systems <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> such as legged robots <ref type="bibr" target="#b4">[5]</ref>. However, most well-known RL algorithms <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> do not consider safety constraints during exploration. Moreover, they are usually not adaptive to non-stationary disturbances, which are common in many realistic safetycritical applications <ref type="bibr" target="#b8">[9]</ref>. These two weaknesses of current RL algorithms need to be addressed before their deployment in safety-critical environments.</p><p>Several recent studies have been proposed to address the lack of safety <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> and the lack of adaptability <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> issues of RL algorithms, respectively. However, the above two issues are entangled in realistic applications, because the environment disturbances may change the system dynamics and affect the region of safety. In other words, disturbances may cause unexpected safety violations if not properly handled. A typical example is shown in Fig. <ref type="figure">1</ref>, where a healthcare robot is trying to deliver the medicine Fig. <ref type="figure">1</ref>: Healthcare environment with and without adaptive safety. Red dots indicate direct contacts between the robot and the patient which should be avoided.</p><p>(or food) to the patient while avoiding any direct contact. The disturbance in this environment mainly comes from the patient's movements. To safely finish the delivery, the robot must be able to quickly identify the patient's moving preference and adaptively generate safe control decisions. To the best of our knowledge, there hasn't been a general framework or a complete algorithm to fully address this entangled problem.</p><p>In this paper, we propose the context-aware safe reinforcement learning (CASRL) framework to realize safe adaptation in non-stationary environments and resolve the above entangled problem. Our major contribution is threefold:</p><p>1) Fast adaptation. We study this problem under the model-based RL framework for sample efficiency. Unlike previous models that predict the next state only based on the current state and action, we use a contextaware latent variable model to infer the disturbance of the non-stationary environment based on the historical transition data, allowing task-agnostic adaptation. 2) Risk-averse control. We achieve risk-averse decision making with constrained model predictive control. Constraints are used for guarantees of safety in uncertain environments. To improve exploration safety in the early stage of training, we incorporate domain knowledge to make conservative decisions with prior models. We also enable prioritized sampling of rare unsafe data during the model training to alleviate the data imbalance problem in safety-critical environments. Combined with a context-aware probabilistic model, this control regime can realize safe adaptation in non-stationary environments and resolve the aforementioned entangled problem. 3) Extensive evaluation. We conduct experiments in a toy example and a realistic high-dimensional environ-ment with non-stationary disturbances. Results show that the proposed method can (i) realize fast adaptation for safe control in unseen environments, (ii) scale to high-dimensional tasks, and (iii) outperform existing approaches in terms of safety and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Safe reinforcement learning has attracted long-term interest in the RL community <ref type="bibr" target="#b16">[17]</ref>. The Constrained Markov Decision Processes (CMDPs) <ref type="bibr" target="#b17">[18]</ref> is often used to model the safe RL problem, where the agent aims to maximize its cumulative reward while satisfying certain safety constraints. Several approaches, such as the Lagrangian method <ref type="bibr" target="#b18">[19]</ref> and constrained policy optimization <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>, have been proposed to solve CMDPs. Gaussian Processes (GPs) have also been used to approximate the dynamics of the environment for safe exploration <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Particularly, Wachi and Sui <ref type="bibr" target="#b22">[23]</ref> discussed the situation where the safety boundary is unknown. However, most existing safe RL methods assume a consistent environment and cannot deal with timevarying disturbances. In contrast, our method aims to realize safe control in non-stationary environments, which is more realistic for safety-critical applications.</p><p>Robust adversarial learning addresses the environment disturbance problem by formulating a two-player zero-sum game between the agent and the disturbance <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. However, the robust policies trained in this way may overfit to the worst-case scenario, so the performance is not guaranteed in other cases <ref type="bibr" target="#b26">[27]</ref>.</p><p>Meta-learning for RL has recently been developed to realize adaptive control in non-stationary environments <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Since unsafe data are particularly rare in safety-critical environments, we focus on model-based methods for sample efficiency <ref type="bibr" target="#b7">[8]</ref>. Saemundsson et al. <ref type="bibr" target="#b28">[29]</ref> proposed to use Gaussian Processes to represent dynamics models, which may suffer from poor scalability as the dimension and the amount of data increases. Nagabandi et al. <ref type="bibr" target="#b14">[15]</ref> integrated model-agnostic meta-learning (MAML) <ref type="bibr" target="#b13">[14]</ref> with model-based RL. The dynamics model is represented by a neural network that uses a meta-learned initialization and is quickly updated with the latest data batch. However, the uncertainty is not estimated by the model, and we show that this may degrade the performance. Later studies from Xu et al. <ref type="bibr" target="#b15">[16]</ref> and Nagabandi et al. <ref type="bibr" target="#b2">[3]</ref> achieved online continual learning with streaming data by maintaining a mixture of meta-trained dynamics models. These approaches may suffer from the model explosion in complex environments where the potential number of dynamics type is large. We overcome this issue by constructing a probabilistic latent variable model that learns a continuous mapping from the disturbance space to the latent space.</p><p>Neural Processes (NPs) <ref type="bibr" target="#b30">[31]</ref> have been proposed for fewshot regression by learning to map a context set of inputoutput observations to a distribution of regression functions. Comparing to the Gaussian processes, NPs have the advantage of efficient data-fitting with linear complexity in the size of context pairs and can learn conditional distributions with a latent space. A later study <ref type="bibr" target="#b31">[32]</ref> proposed Attentive Neural Processes (ANPs) by incorporating attention into NPs to alleviate the underfitting problem and improve the regression performance. NP-based models have shown great performance in function regression <ref type="bibr" target="#b32">[33]</ref>, image reconstruction <ref type="bibr" target="#b31">[32]</ref>, and point-cloud modeling <ref type="bibr" target="#b33">[34]</ref>. As probabilistic latent variable models, ANPs naturally enable continual online learning in continuously parameterized environments. In this paper, we will show how to incorporate ANPs for dynamics prediction and safety constraint estimation.</p><p>The rest of the paper is organized as follows. In Sec. III, we formulate the safety-critical problem that we aim to solve in this paper. In Sec. IV, we show the inference process of unknown environment disturbances with a latent variable model. In Sec. V, we show how to perform safe adaptation with a sampling-based model-predictive controller. The experiment results and discussions are presented in Sec. VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM STATEMENT</head><p>We consider non-stationary Markov Decision Processes (MDPs) with safe constraints. An MDP is defined as a tuple (S, A, f, r, γ, ρ 0 ) where S denotes the state space, A denotes the action space, f (s |s, a) is the transition distribution of the environment dynamics that takes into the current state s ∈ S and action a ∈ A, and outputs the distribution of the next state s ∈ S. r(s, a) is the reward function, γ is the reward discount factor, and ρ 0 is the distribution of the initial state. To simulate the disturbances in real-world environments, we consider non-stationary MDPs where the transition dynamics f (s |s, a, θ) depends on certain hidden parameters θ ∼ T , where T denotes the distributions of environments parameters. For simplicity, we assume that the environment is episodically consistent -the change of f only happens at the beginning of each episode. This setting is commonly used in related papers and can be easily generalized to other consistent time-horizons.</p><p>Denote a safe state set by S saf e and a safe action set by A saf e . The goal of safe RL is to find the optimal action sequence a 0:T to maximize the discounted accumulated reward τ t=0 γ t r(s t , a t ), without violating the safety constraints (i.e., keeping s t ∈ S saf e and a t ∈ A saf e for every time step t). γ is a discount factor and τ is the task horizon. Throughout this paper, we assume S saf e and A saf e are known a priori.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONTEXT-AWARE MODEL INFERENCE</head><p>We address the proposed problem under the model-based RL framework, where the tasks are solved by learning a dynamics model f (s |s, a) to approximate the groundtruth environment dynamics f (s |s, a). However, when the environment dynamics f is non-stationary, f (s |s, a) may fail to make accurate predictions since some hidden features of the environment are not identified. To handle this problem, we propose to learn a context-aware model f (s |s, a, C) that performs state predictions based not only on the current state s and action a but also on the contexts C -the historical data collected in the current episode. In this way, the hidden information of the environment is first inferred from C, and then the posterior distribution of the next state s is calculated.</p><p>To incorporate domain knowledge for adaptive learning, we divide the dynamics model f (s |s, a, C) into two parts:</p><formula xml:id="formula_0">s := s h + s g ,<label>(1a)</label></formula><p>with</p><formula xml:id="formula_1">s h ∼ h(•|s, a),<label>(1b)</label></formula><formula xml:id="formula_2">s g ∼ g(•|s, a, C).<label>(1c)</label></formula><p>The model h in Eq. ( <ref type="formula" target="#formula_0">1b</ref>) is referred to as the prior model. Such model can be obtained by leveraging domain knowledge without necessarily interacting with the environment, e.g., training the dynamics model in a simulator <ref type="bibr" target="#b7">[8]</ref> or using first principles modeling <ref type="bibr" target="#b34">[35]</ref>. However, the drawback is that they are usually context-unaware.</p><p>The model g in Eq. ( <ref type="formula" target="#formula_0">1c</ref>) is called the disturbance model (or the error model). It represents the error between the prior model h and the overall dynamics model f . It is the model we aim to learn by interacting with the target non-stationary environment. The disturbance model is context-aware and should be able to capture the hidden information of the environment based on the contexts C. To achieve that, the disturbance model g should have the following properties:</p><p>• Flexibility: g should be able to condition on arbitrary number of contexts to make predictions. • Uncertainty awareness: g should estimate the uncertainty in its predictions to balance exploration and exploitation. • Scalability: g should be able to scale to highdimensional environments.</p><p>In this paper, we use an Attentive Neural Process (ANP) <ref type="bibr" target="#b31">[32]</ref> to represent the disturbance dynamics model g for its desirable properties and implementation simplicity. The ANP model is defined as a (infinite) family of conditional distributions, in which an arbitrary number of observed input-output contexts (x C , y C ) := (x i , y i ) i∈C is used to model an arbitrary number of input-output targets (x T , y T ) := (x i , y i ) i∈T , where C denotes a set of observed points and T denotes a set of unobserved points (the output y T is unknown). The ANP transforms the original conditional likelihood to a hierarchical inference structure:</p><formula xml:id="formula_3">g (y T |x T , x C , y C ) = p (y T |x T , z) q (z|l C ) dz (2)</formula><p>where z is a global latent vector describing uncertainty in the predictions of y T for given observations (x C , y C ), and is modeled by a factorized Gaussian parameterized by l C := l(x C , y C ), with l being a deterministic function that aggregates (x C , y C ) into a fixed dimensional representation. In ANP, l consists of a multilayer perceptron (MLP), self-attentions, and a mean aggregation layer to produce permutation-invariant representations.</p><p>For dynamics prediction, the input x is the state-action pair (s, a), and the output y is the state at the next time step s . At time t, the contexts (x C , y C ) = (s i , a i , s i ) i∈[1:t-1] contain the state-action information of the previous time steps, the target input x T = (s t , a t ) is the current state-action pair, and we aim to predict the target output y T = s t that represents the next state. The flow of using context-aware model for model-based RL is shown in Fig. <ref type="figure" target="#fig_0">2</ref>. A constrained MPC controller is used for safe planning and will be introduced in the next section.</p><p>The training of ANP is based on the amortized variational inference. The parameters of the encoders and the decoder are updated by maximizing the following evidence lower bound (ELBO) with the reparametrization trick <ref type="bibr" target="#b35">[36]</ref>:</p><formula xml:id="formula_4">log g (y T |x T , x C , y C ) ≥ E q(z|l T ) [log g(y T |x T , z)] -D KL (q (z|l T ) q (z|l C )) . (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where l T := l(x T , y T ), with l being a deterministic function introduced before. The training objective of ANP can be interpreted as improving the prediction accuracy on the targets while regularizing the Kullback-Leibler divergence between the latent encoding of the contexts and the targets.</p><p>The contexts and the targets are randomly sampled from a replay buffer that stores transition data from the same disturbance dynamics. However, the rareness of unsafe data may lead to low prediction accuracy in the unsafe state region. To alleviate this issue, inspired by <ref type="bibr" target="#b36">[37]</ref>, we enable prioritized experience sampling during model training -to train the context-aware model with a certain data batch, the unsafe data in this data batch are first added into the target set T , and then other safe data are uniformly sampled and appended to C and T . We found that this trick can effectively increase the prediction accuracy in the unsafe region, which is discussed in Sec. VI-C.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SAFE ADAPTATION WITH MPC</head><p>We formulate the safe adaptation as a constrained nonlinear optimization problem:</p><formula xml:id="formula_6">max a0:τ τ t=0 r(s t , a t ) (4a) s.t. a t ∈ A saf e (<label>4b</label></formula><formula xml:id="formula_7">)</formula><formula xml:id="formula_8">s t+1 ∼ f (•|s t , a t , C) (4c) Pr(s t / ∈ S saf e ) ≤ δ (4d) ŝt+1 ∼ h(•|ŝ t , a t ) (4e) Pr(ŝ t / ∈ S saf e ) ≤ δ<label>(4f)</label></formula><p>for t = 0, . . . , τ</p><p>Eq. (4a) shows that the objective is to maximize the cumulative reward, Eq. (4b) represents the safety constraint on actions, and Eq. (4c, 4d) define the safety constraint on the states s t that predicted by the learned model f . Eq. ( <ref type="formula" target="#formula_6">4a</ref>)-(4d) form the general problem of safe RL in most previous literature <ref type="bibr" target="#b20">[21]</ref>. However, with the non-stationary environment disturbances, the learning process of the prediction model f may be unstable, and it is difficult for the agent to keep safe when f is not accurate. To alleviate this problem, we formulate the prior safety constraint shown in Eq. (4e, 4f), where a sequence of auxiliary states ŝt is predicted only with the prior model h, and the high-probability safety constraint is applied to it (ŝ 0 = s 0 ). Though not accurate, the prior safety constraint provides extra protection for the agent based on the static prior model h. Applying the prior safety constraint is an effective way to incorporate domain knowledge to improve safe learning, especially when the unsafe data are expensive to obtain. Experiment results show that it can effectively reduce the safety violation rate especially in the early stage of training (Sec. VI-C.1).</p><p>Direct solving the optimization problem Eq. ( <ref type="formula" target="#formula_6">4</ref>) is intractable since f is a high-dimensional nonlinear stochastic function. Previous work has used approximated uncertainty propagation techniques like sigma-point transform <ref type="bibr" target="#b37">[38]</ref> and Taylor expansion <ref type="bibr" target="#b20">[21]</ref> to model the state distribution as a single Gaussian distribution, and then solve Eq. ( <ref type="formula" target="#formula_6">4</ref>) with nonlinear solvers such as the IPOPT <ref type="bibr" target="#b38">[39]</ref>. However, Deisenroth et al. <ref type="bibr" target="#b39">[40]</ref> showed that the Gaussian moment matching could corrupt after long-term propagation due to the multi-modal distribution of states, inducing huge prediction errors. Also, IPOPT cannot provide an alternative plan if no solution for Eq. ( <ref type="formula" target="#formula_6">4</ref>) is found in limited time.</p><p>In this paper, we propose to solve Eq. ( <ref type="formula" target="#formula_6">4</ref>) with a samplingbased model-predictive control (MPC) approach. We use MPC for its implementation simplicity, time flexibility, and risk aversion. Also, this sampling-based method makes no Algorithm 1 Trajectory sampling procedure TRAJSAMPLING(A, h, g, C, t 0 )</p><p>for SamplingTime = 1, N do for t = t 0 , t 0 + τ p do s th ∼ h(•|s t-1 , a t-1 ) s tg ∼ g(•|s t-1 , a t-1 , C) s t = s th + s tg return {s t0:t0+τ } 1:N assumptions on the pattern of state distributions. Denoting the planning horizon with τ p , we first define the augmented objective function for an action sequence A = a t0:t0+τp as:</p><formula xml:id="formula_9">R(A) := t0+τp t=t0 [r (s t , a t ) -λ[(1(Pr(s t / ∈ S saf e ) &gt; δ) + 1(Pr(ŝ t / ∈ S saf e ) &gt; δ) + 1(a t / ∈ A saf e ))]]<label>(5)</label></formula><p>where 1(Z) is the indicator function that returns 1 if Z is true, otherwise 0. s t and ŝt are the state particles defined in Eq. ( <ref type="formula" target="#formula_6">4</ref>) and are produced by the trajectory sampling procedure where the uncertainties are propagated (Alg 1). λ serves as the Lagrangian multiplier of the dual problem of Eq. ( <ref type="formula" target="#formula_6">4</ref>). In this paper, we regard λ as a fixed hyperparameter and make it sufficiently large</p><formula xml:id="formula_10">λ ≥ max(|r|) * τ<label>(6)</label></formula><p>so that the augmented performance is monotonically decreasing w.r.t. the safety violation number. Considering the uncertainty in the probabilistic model, we evaluate A with the Conditional Value at Risk (CVaR) <ref type="bibr" target="#b40">[41]</ref> of R(A) to make the solutions risk-averse:</p><formula xml:id="formula_11">CVaR α ( R(A)) = E R(A)| R(A) ≤ ν α ( R(A))<label>(7)</label></formula><p>where α ∈ (0, 1) and ν α is the α-quantile of the distribution of R(A). In other words, we prefer action sequences with higher CVaR. We then take the first action in the most preferred action sequence and execute it. Instead of uniformly sampling A every time, we utilize the Cross-Entropy Method (CEM) as suggested in <ref type="bibr" target="#b7">[8]</ref> to keep the historical information.</p><p>The complete algorithm along with the model-learning part is shown in Alg. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENT</head><p>For the evaluation of the proposed algorithm, we aim to answer the following questions through empirical experiments: can CASRL 1) adapt faster to unseen environments with a stream of non-stationary data than existing approaches? 2) reduce the safety violation rate with prior safety constraints? 3) scale to high-dimensional tasks?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Environments</head><p>To answer the above questions, we test CASRL in two continuously-parameterized non-stationary environments with safety constraints. The setup of the environments (Fig. <ref type="figure">3</ref>) is introduced below. State propagation in the learned model ŝt:t+τp = TRAJSAMPLING(A, h, 0, C, s t-1 , t)</p><p>State propagation in the prior model</p><formula xml:id="formula_12">A * = arg max A CVaR α R (A)</formula><p>The optimal action sequence is selected based on the CVaR Update CEM(•) Execute a * t , get s t+1 a * t is the first element of</p><formula xml:id="formula_13">A * C ← C ∪ (s t , a * t , s t+1 ) Record context R ← R ∪ C</formula><p>Update the episodic replay buffer Update g by maximizing the ELBO in Eq. ( <ref type="formula" target="#formula_4">3</ref>) with R Model learning Fig. <ref type="figure">3</ref>: Tasks with non-stationary disturbances and safety constraints.</p><p>• cart-pole. (S ⊆ R 4 , A ⊆ R<ref type="foot" target="#foot_0">foot_0</ref> ) This is the cart-pole swingup experiment proposed in <ref type="bibr" target="#b28">[29]</ref>. The goal is to swing the pole upright by applying force on the cart while keeping the cart close to the center of the rail. We add constraints on the pole angle θ ∈ [-10 • , 225 • ] so that the pole should be swung up from the right side without too much overshoot. We make the task non-stationary by changing the pole length l, the pole mass p m , and the cart mass c m at the beginning of each episode. The observation includes the position x and velocity ẋ of the cart, as well as the angle θ and angular velocity θ of the pole. The reward function is r = exp -(x-l sin θ) 2 +(l-l cos θ) 2 l 2 and the highest reward r = 1 is acquired when the cart is at the center of the rail (x = 0) and the pole is upright (θ = 0). The simulation frequency is 20 Hz.</p><p>• healthcare feeding robot.</p><formula xml:id="formula_14">(S ⊆ R 23 , A ⊆ R 7 )</formula><p>The environment is provided by <ref type="bibr" target="#b41">[42]</ref>. The goal is to deliver the medicines to the patient's mouth with a control arm.</p><p>To keep safe, there should be no direct contact between the patient and the robot. In each episode, the patient moves forward and rotates his head in 4 degree-offreedom with randomly sampled speeds (a f , a θ , a φ , a ψ ), which is the disturbance we designed to simulate different preferences. This is a relatively high-dimensional environment and is used to test the scalability of the algorithms. The observation includes the position of the robot joints and the spoon, as well as the position and orientation of the human head. The reward function has three parts: r = r dis +r med +r act , where r dis penalizes the distance between the spoon and the target position, r med is a large positive value if medicine particles are successfully delivered or a large negative value if they are spilled, and r act penalizes the magnitude of the control input. The simulation frequency is 10 Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baselines</head><p>We compare our method with the following baselines:</p><p>• Projection-Based Constrained Policy Optimization (PCPO): A projection-based safe RL algorithm <ref type="bibr" target="#b42">[43]</ref>.</p><p>The learned policy is projected to the safe region during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Probabilistic Ensemble and Trajectory Sampling (PETS):</head><p>To evaluate the importance of context-aware adaptation, we compare to PETS <ref type="bibr" target="#b7">[8]</ref>, a state-of-the-art model-based RL approach. • Model-Agnostic Meta-Learning (MAML): We use the gradient-based MAML <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> to learn the dynamics of the non-stationary environments. The dynamics model is represented by a neural network which is initialized from a pre-trained meta-model and updated online with the nearest context data. 1 • CASRL without prior safety constraint: To show whether the prior safety constraint can effectively reduce the safety violation rate, we add another baseline that follows the same structure of CASRL but does not apply the prior safety constraint.</p><p>Each algorithm (including the proposed method) is first pre-trained in non-safety-critical simulators without any disturbances (T pre ) to learn the prior model h, where the safety constraints are not applied so that we have enough data from both safe and unsafe regions. We then use these initialized models to safely adapt in disturbance spaces T adapt to learn the disturbance model g, with constraints applied. As introduced in Sec. III, we re-sample the parameters of the environments from T adapt at the beginning of each episode. The results will reflect the adaptability of the tested algorithms. T pre and T adapt used in the experiments are shown in Table <ref type="table" target="#tab_0">I</ref>. </p><formula xml:id="formula_15">l = 0.6 l ∼ U[0.2, 1.0] m pm = 0.6 pm ∼ U [0.2, 1.0] kg cm = 0.6 cm ∼ U [0.2, 1.0] kg healthcare a f = 0 a f ∼ U [-1.0, 1.0] • /s a θ = 0 a θ ∼ U [-2.0, 2.0] • /s a φ = 0 a φ ∼ U [-2.0, 2.0] • /s a ψ = 0 a ψ ∼ U [-2.0, 2.0] • /s</formula><p>In the implementation, we use a hidden size of [128, 128] for all MLP networks. The latent dimension is 8 for the deterministic encoder and latent encoder in the ANP model for both experiments. The planning horizon τ is set to be 20. Each experiment was run with 10 random seeds. We make the controller risk-averse by setting δ = 0 in Equ. 5. All hyperparameters are fine-tuned manually and are provided in our submitted code base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Result Analysis 1) During Adaptive Training:</head><p>The average returns and safety violation rates during adaptive training are shown in Fig. <ref type="figure" target="#fig_2">4</ref>. The violation rate represents the proportion of safety violation time steps in the whole episode. For PCPO, we only plot the highest average performance after its convergence since it requires a lot more samples to train than other model-based methods. It is shown that the performance of PCPO is limited since it cannot deal with non-stationary environment disturbances. Though PETS outperforms other methods in most environments during the early stage of training, it fails to continue improving due to the lack of adaptability in non-stationary environments. The proposed approach, CASRL, outperforms MAML in both average returns and safety violation rates, especially in the healthcare environment. There are two possible reasons. One is that the adaptation of MAML relies on online training of a highdimensional neural-network model in each step, which is very sensitive to the learning rate and could be unstable in high-dimensional spaces. On the other hand, CASRL only performs online inference. The other possible reason is that MAML cannot model the uncertainties in the environment, which is accomplished by CASRL with a probabilistic latent variable model. We can also observe that the prior safety constraint can significantly reduce the violation rate with minimal performance degradation.</p><p>2) After Adaptive Training: We evaluate the performance of models after adaptive training by experiment in the whole disturbance space T adapt (Tab. I). The results of average returns and safety violation rates in cartpole-swingup and healthcare are shown as heatmaps in Fig. <ref type="figure" target="#fig_3">5</ref>. It is interesting to observe that different constraint functions can lead to different patterns of heatmaps. In the cartpole-swingup environment, most constraint-violation cases concentrate at the corners of the disturbance space (Fig. <ref type="figure" target="#fig_3">5b</ref>) because the dynamics models in the corners are the most different from the center. In the healthcare environment, however, most constraint-violation cases take place when the human head has a high velocity of forward movement (Fig. <ref type="figure" target="#fig_3">5d</ref>), which is reasonable since forward movement decreases the distance between the human head and the robot, increasing the risk of direct contact. Among the methods tested, CASRL shows great robustness and adaptability to disturbances compared to other baselines.</p><p>3) Effect of pre-training: The pre-training phase is essential for CASRL. The pre-trained prior model h not only provides a start point for adaptive learning but also forms the prior safety constraint that improves the safety of the learning process. To show this, we compare the performance of CASRL with and without pre-training in Fig. <ref type="figure" target="#fig_4">6</ref>. MAML provides a baseline. It is clearly shown that the pre-training phase significantly benefits the learning process, especially for CASRL.</p><p>For the healthcare experiment, the violation rate experienced a big jump in the early stage of training for both methods. The reason is that the robot needs to learn to control its arm before it can approach the patient and possibly violate the safety constraint.  4) Effect of prioritized sampling: We evaluate the effectiveness of prioritized sampling by comparing the mean square error (MSE) of dynamics predictions in safe and unsafe regions. The results are shown in Fig. <ref type="figure" target="#fig_5">7</ref>. The prediction accuracy in the unsafe state region is improved by prioritized sampling, while the performance in the safe state region is not influenced. The reason could be that without prioritized sampling, the model is biased towards the safe data due to the rareness of the unsafe samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we propose the context-aware safe reinforcement learning (CASRL) method as a meta-learning framework to realize safe adaptation in non-stationary environments. The non-stationary disturbances are identified with a probabilistic latent variable model by online Bayesian inference. A risk-averse model-predictive controller is used for safe planning with uncertainties, where we incorporate prior safety constraints to enable fast adaptation with prior knowledge. We also utilize prioritized sampling of unsafe data to alleviate the data imbalance in safety-critical environments. The algorithm is evaluated in both toy and realistic high-dimensional environments. Results show that CASRL significantly outperforms existing baselines in terms of safety and robustness.</p><p>Although CASRL is potentially beneficial for RL applications in safety-critical tasks, it may have its limitations. For example, the disturbance space could be much larger if we use image inputs with noises. Although the ANP model has been shown to work for image reconstruction tasks <ref type="bibr" target="#b31">[32]</ref>, it may fail for dynamics prediction in complex environments.</p><p>In that case, one potential solution is to conduct dynamics prediction in the latent space as in Dreamer <ref type="bibr" target="#b43">[44]</ref>, which is directly applicable for CASRL. The hyperparameter-tuning for learning rates, network structures, and especially the latent dimensions could be another challenge for CASRL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The flow of the proposed context-aware safe reinforcement learning (CASRL) framework. A context-aware model is used to perform conditional dynamics predictions based on the context data.</figDesc><graphic coords="3,54.00,50.08,503.99,145.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 2</head><label>2</label><figDesc>Context-Aware Safe Reinforcement Learning (CASRL) Input: prior model h, state safe set X saf e , action safe set A saf e , task distribution T Output: disturbance model g, episodic replay buffer R g ← g 0 , R ← {} Initialize the disturbance model and the replay buffer for Episode = 1, M do p ∼ T , C ← {}, reset CEM(•), get s 0 Environment sampling and episode initialization for t = 1, τ do for A ∼ CEM(•) do Sampling action sequences s t:t+τp = TRAJSAMPLING(A, h, g, C, s t-1 , t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Return and violation rate during adaptive training. The proposed method CASRL greatly reduces safety violation rate while outperforming MAML in average return.</figDesc><graphic coords="6,313.20,50.08,244.80,202.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Return and violation rate after adaptive training in cart-pole and healthcare environments.</figDesc><graphic coords="7,54.00,258.48,244.80,198.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Comparison of CASRL and MAML with and without the pre-training phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: The MSE of single-step dynamics predictions by CASRL in healthcare environment. The prediction accuracy in the unsafe region is improved by prioritized sampling.</figDesc><graphic coords="7,319.32,258.48,232.56,108.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Disturbance Space. U(•) denotes uniform distribution.</figDesc><table><row><cell>Environment</cell><cell>Tpre</cell><cell>T adapt</cell><cell>Unit</cell></row><row><cell>cart-pole</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We used a publicly available implementation at https://github. com/iclavera/learning_to_adapt.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Alphastar: Mastering the real-time strategy game starcraft ii</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DeepMind blog</title>
		<imprint>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep online learning via metalearning: Continual adaptation for model-based rl</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nagabandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07671</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Delay-aware modelbased reinforcement learning for continuous control</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05440</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning agile and dynamic motor skills for legged robots</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hwangbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bellicoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tsounis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Robotics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">26</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning in a handful of trials using probabilistic dynamics models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4754" to="4765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10528</idno>
		<title level="m">Constrained policy optimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optlayer-practical constrained optimization for deep reinforcement learning in the real world</title>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">De</forename><surname>Magistris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tachibana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6236" to="6243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A lyapunov-based approach to safe reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duenez-Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8092" to="8101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Safe exploration in continuous action spaces</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dvijotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vecerik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Paduraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08757</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning to adapt in dynamic, real-world environments through meta-reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nagabandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Clavera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Fearing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11347</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Task-agnostic online reinforcement learning with an infinite mixture of gaussian processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11441</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A comprehensive survey on safe reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Garcıa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1437" to="1480" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Constrained Markov decision processes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Altman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>CRC Press</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Constrained markov decision processes with total cost criteria: Lagrangian approach and dual linear program</title>
		<author>
			<persName><forename type="first">E</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical methods of operations research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="387" to="417" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Lyapunov-based safe policy optimization for continuous control</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Faust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duenez-Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10031</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learningbased model predictive control for safe exploration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Berkenkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turchetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Decision and Control (CDC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6059" to="6066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cautious model predictive control using gaussian process regression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hewing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kabzan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Zeilinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Control Systems Technology</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Safe reinforcement learning in constrained markov decision processes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9797" to="9806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust markov decision problems with uncertain transition matrices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nilim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ghaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Robust adversarial reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02702</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning to collide: An adaptive safety-critical scenarios generating method</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11569</idno>
		<title level="m">Overfitting in adversarially robust deep learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Rl 2 : Fast reinforcement learning via slow reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02779</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Meta reinforcement learning with latent variable gaussian processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saemundsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07551</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Non-stationary reinforcement learning: The blessing of (more) optimism</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simchi-Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Available at SSRN</title>
		<imprint>
			<biblScope unit="volume">3397818</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Neural processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01622</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Attentive neural processes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05761</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Recurrent attentive neural process for sequential data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09323</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Bruinsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13556</idno>
		<title level="m">Convolutional conditional neural processes</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Modeling, identification and control of cart-pole system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Pati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Prioritized experience replay</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05952</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust constrained learning-based nmpc enabling reliable mobile robot path tracking</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Ostafew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Schoellig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Barfoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1547" to="1563" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the implementation of an interiorpoint filter line-search algorithm for large-scale nonlinear programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wächter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Biegler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical programming</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="25" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gaussian processes for data-efficient learning in robotics and control</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="408" to="423" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optimizing the cvar via sampling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Glassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2993" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Assistive gym: A physics simulation framework for assistive robotics</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gangaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapusta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Kemp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Projectionbased constrained policy optimization</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Ramadge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Dream to control: Learning behaviors by latent imagination</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01603</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
