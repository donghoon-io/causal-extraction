<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Latent Process Decomposition of cDNA Microarray Data Sets</title>
				<funder ref="#_mSZqDtn #_jGJcM7X">
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder ref="#_EYXsEUG">
					<orgName type="full">BBSRC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2005-06-02">2 June 2005</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Simon</forename><surname>Rogers</surname></persName>
							<email>srogers@dcs.gla.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Girolami</surname></persName>
							<email>girolami@dcs.gla.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Colin</forename><surname>Campbell</surname></persName>
							<email>c.campbell@bris.ac.uk.</email>
						</author>
						<author>
							<persName><forename type="first">Rainer</forename><surname>Breitling</surname></persName>
							<email>r.breitling@bio.gla.ac.uk</email>
						</author>
						<author>
							<persName><forename type="middle">C</forename><surname>Campbell</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Bioinformatics Research Centre</orgName>
								<orgName type="department" key="dep2">Department of Computing Science</orgName>
								<address>
									<postCode>A416</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Fourth Floor</orgName>
								<orgName type="department" key="dep2">Davidson Building</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<settlement>Glasgow</settlement>
									<country>Scotland United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Engineering Mathematics, Queen&apos;s Building</orgName>
								<orgName type="institution">Bristol University</orgName>
								<address>
									<postCode>BS9 1TR</postCode>
									<settlement>Bristol</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Molecular Plant Science Group &amp; Bioinformatics Research Centre</orgName>
								<orgName type="department" key="dep2">Institute for Biomedical and Life Sciences</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8QQ</postCode>
									<settlement>Glasgow</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Latent Process Decomposition of cDNA Microarray Data Sets</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2005-06-02">2 June 2005</date>
						</imprint>
					</monogr>
					<note type="submission">received 12 Sept. 2004; revised 11 Nov. 2004; accepted 7 Dec. 2004;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>cDNA microarray</term>
					<term>latent variable modeling</term>
					<term>cluster analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new computational technique (a software implementation, data sets, and supplementary information are available at <ref type="url" target="http://www.enm.bris.ac.uk/lpd/">http://www.enm.bris.ac.uk/lpd/</ref>) which enables the probabilistic analysis of cDNA microarray data and we demonstrate its effectiveness in identifying features of biomedical importance. A hierarchical Bayesian model, called Latent Process Decomposition (LPD), is introduced in which each sample in the data set is represented as a combinatorial mixture over a finite set of latent processes, which are expected to correspond to biological processes. Parameters in the model are estimated using efficient variational methods. This type of probabilistic model is most appropriate for the interpretation of measurement data generated by cDNA microarray technology. For determining informative substructure in such data sets, the proposed model has several important advantages over the standard use of dendrograms. First, the ability to objectively assess the optimal number of sample clusters. Second, the ability to represent samples and gene expression levels using a common set of latent variables (dendrograms cluster samples and gene expression values separately which amounts to two distinct reduced space representations). Third, in constrast to standard cluster models, observations are not assigned to a single cluster and, thus, for example, gene expression levels are modeled via combinations of the latent processes identified by the algorithm. We show this new method compares favorably with alternative cluster analysis methods. To illustrate its potential, we apply the proposed technique to several microarray data sets for cancer. For these data sets it successfully decomposes the data into known subtypes and indicates possible further taxonomic subdivision in addition to highlighting, in a wholly unsupervised manner, the importance of certain genes which are known to be medically significant. To illustrate its wider applicability, we also illustrate its performance on a microarray data set for yeast.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I N recent years, there has been a large growth in the volume of microarray data. Machine learning techniques have been very useful in extracting information from these data sets. For example, unsupervised techniques such as hierarchical cluster analysis have been used to indicate tentative new subtypes for some cancers <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref> and to identify biologically relevant structure in large data sets. Using class labels, supervised learning methods will have an increasingly important role to play in indicating the detailed subtype of a disease, its expected progression, and the best treatment strategy.</p><p>Cluster analysis is the most common data analysis method used throughout the microarray literature. Generation of a dendrogram allows the inference of possible group structure within the data and, thus, the identification of related samples or functionally similar genes. A weakness of clustering by dendrogram is that there is no natural way to objectively assess the most probable number of structures which underlie the data. Probabilistic model-based clustering can overcome this problem and clustering based on mixtures of multivariate Gaussians has been proposed in <ref type="bibr" target="#b8">[9]</ref>, for example. However, as the number of features (gene probes) typically far exceeds the number of samples, the estimated within-class covariances will be singular. Therefore, extensive gene selection is a prerequisite to any probabilistic model-based clustering and in <ref type="bibr" target="#b8">[9]</ref> a further level of feature extraction is required to obtain a sufficiently low-dimensional subspace within which the clustering is performed. For the purposes of biological interpretation, it would be better if group structure could be identified using the original set of genes probed in the microarray experiment with the model used to identify those genes responsible for the group structure. We will show that this is achieved naturally by the model proposed in this paper.</p><p>The main assumption underlying most clustering methods, whether dendrogram-based or probabilistic, is that a sample or gene is assigned exclusively to one class only. While this may be appropriate if mutually exclusive classes are inherent in the data, this assumption would not be valid if a number of biological processes interact to influence a given gene expression level, for example. Alternatively, a sample might validly share some characteristics with two or more sample clusters. It is therefore best to obtain models which are not restricted by this mutual exclusion of classes assumption.</p><p>Models overcoming this mutual exclusion assumption have already been proposed in the literature. An example is the Plaid Model of Lazzeroni and Owen <ref type="bibr" target="#b7">[8]</ref> which allows for overlaps between clusters. Another example is a model recently proposed by Segal et al. <ref type="bibr" target="#b9">[10]</ref>, in which a probabilistic relational model (PRM) for gene expression is developed. PRM is shown to successfully identify known biological processes and provides improved interpretability over comparable methods <ref type="bibr" target="#b9">[10]</ref>. However, PRM requires an estimation algorithm which, in its exact form, is NP-hard, and so various heuristics are required to make the model estimation tractable.</p><p>In this paper, we will present a new approach to the analysis of microarray data which is motivated by the above considerations and adopts the Latent Dirichlet Allocation (LDA) approach to data modeling <ref type="bibr" target="#b1">[2]</ref>. We have called the method Latent Process Decomposition with a process defined as a set of functionally related datapoints (samples or genes). We use the term process, rather than cluster, because a sample or gene can have partial membership of several processes simultaneously, in contrast to many cluster analysis approaches, as discussed above.</p><p>Latent Process Decomposition has linear computational scaling in the number of genes, arrays and processes. It is able to provide a representation of microarray data sets which simultaneously reflects group structure across samples and genes as well as the possible interplay between multiple processes responsible for measured gene expression levels. Furthermore, it has advantages over alternative approaches such as the use of dendrograms, mixture models <ref type="bibr" target="#b8">[9]</ref>, Naive Bayes, and other approaches. In Section 2, we will outline the method, with full derivation and pseudocode listings given in the Appendix, while in Section 3, we will outline performance on medical and biological data sets. We have implemented LPD in Matlab and C þþ and this code is freely available at <ref type="url" target="http://www.enm.bris.ac.uk/lpd">http://www.enm.bris.ac.uk/lpd</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LATENT PROCESS DECOMPOSITION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Specification</head><p>In probabilistic terms, the data generation process for a tissue sample can be described as follows (Fig. <ref type="figure" target="#fig_0">1b</ref>): For a complete microarray data set, a Dirichlet prior probability distribution for the distribution of possible processes is defined by the K-dimensional parameter . For a tissue sample a, a distribution over a set of mixture components indexed by the discrete variable k is drawn from a single prior Dirichlet distribution. Then, for each of the G features g (generally uniquely mapping to genes), we draw a process index k from the distribution with probability k which selects a Gaussian defined by the parameters gk and gk . The level of expression e ga for gene g from sample a is then drawn from the kth Gaussian denoted as N ðe ga jk; gk ; gk Þ. This is then repeated for each of A tissue samples. It can be easily seen that this generative process ensures that each sample has its own specific distribution of processes and that each measured level of gene expression in the sample is obtained by repeatedly sampling the process index from the sample-specific Dirichlet random variable , thus allowing the possibility of a number of processes having an effect on the measured levels of gene expression. This is a biologically more realistic representation than the mixture model which underlies all forms of clustering, see Fig. <ref type="figure" target="#fig_0">1a</ref>, where only one process or class is responsible for all the measured gene expression, thus disregarding gene-specific differences in effect ( gk ) and noise ( gk ) or the sample-specific interaction of various processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning with Uniform Priors: The Maximum Likelihood Solution</head><p>For a model with a set of parameters, H, and data D, Bayes's rule gives</p><formula xml:id="formula_0">pðHjDÞ / pðDjHÞpðHÞ;<label>ð1Þ</label></formula><p>where pðDjHÞ is the likelihood and pðHÞ is the prior on our parameters H. We are interested in finding the set of parameters H that maximizes pðHjDÞ (the maximum posterior or MAP solution). In the case of a uniform (or uninformative) prior, this is the maximum likelihood solution. We will begin by deriving the maximum likelihood solution and then extend this to a nonuniform (or informative) prior in Section 2.3. The log-likelihood of a set of A training samples is:</p><formula xml:id="formula_1">log pðDj; ; Þ;<label>ð2Þ</label></formula><p>which can be factorized over individual samples as follows: Marginalizing over the latent variable allows us to expand this expression as follows:</p><formula xml:id="formula_2">log pðDj; ; Þ ¼ X A a¼1 log pðaj; ; Þ:<label>ð3Þ</label></formula><formula xml:id="formula_3">log pðDj; ; Þ ¼ X A a¼1 log Z pðaj; ; ÞpðjÞd;<label>ð4Þ</label></formula><p>where the probability of the sample conditioned on the means, variances, and process weightings can be expressed in terms of its individual components giving the following:</p><formula xml:id="formula_4">log pðaj; ; Þ ¼log Z Y G g¼1 X K k¼1 N ðe ga jk; gk ; gk Þ k ( ) pðjÞd;<label>ð5Þ</label></formula><p>where N ðajk; ; Þ denotes a normal distribution in process k with mean and standard deviation .</p><p>Variational inference can be employed to estimate the parameters of this model and full details are given in the Appendices. A lower bound on (5) can be inferred by the introduction of variational parameters and the following iterative update equations provide estimates of the two variational parameters Q kga and ak</p><formula xml:id="formula_5">Q kga ¼ N ðe ga jk; gk ; gk Þ exp ð ak Þ ½ P K k 0 ¼1 N ðe ga jk; gk 0 ; gk 0 Þ exp ð ak 0 Þ ½ ;<label>ð6Þ</label></formula><formula xml:id="formula_6">ak ¼ k þ X G g¼1 Q kga ;<label>ð7Þ</label></formula><p>for given k and where ðzÞ is the digamma function. The model parameters are obtained from the following update equations:</p><formula xml:id="formula_7">gk ¼ P A a¼1 Q kga e ga P A a 0 ¼1 Q kga 0 ;<label>ð8Þ</label></formula><formula xml:id="formula_8">2 gk ¼ P A a¼1 Q kga ðe ga À gk Þ 2 P A a 0 ¼1 Q kga 0 :<label>ð9Þ</label></formula><p>The update rule for the Dirichlet model parameter k is found from the derivatives of the dependent terms in the likelihood <ref type="bibr" target="#b1">[2]</ref>. Thus, the k are modified after each iteration of the above updatings using a standard Newton-Raphson technique (see <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">Appendix A.4.2]</ref>, and the Appendix of this paper for further details).</p><p>The generative process described above for samples can equally be used over genes (the features). In probabilistic terms: For each gene g, a Dirichlet random variable is sampled, then for each experimental condition a, an index is drawn with probability k , and then the expression level e ga is sampled from the appropriate Gaussian with mean and variance ak , ak , respectively. Note that representing genes in this manner only requires a change of indices in the parameters which simply amounts to a transpose of the expression matrix. We will give one example of a decomposition over genes (the yeast cell cycle data set) later in Section 3.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning with Nonuniform Priors: The MAP Solution</head><p>In the previous section, we derived a set of update equations, guaranteed to converge to a local maximum of the lower bound on the likelihood. We now extend this argument to the MAP solution with informative priors. A suitable prior on the means could be a Gaussian distribution with zero mean. This would reflect a prior belief that most genes will be uninformative and will have log-ratio expression values around zero (i.e., they are unchanged compared to a reference sample). For the variance, we may wish to define a prior that penalizes overcomplex models and avoids overfitting. Overfitting may occur when Gaussian functions contract onto a single data point causing poor generalisation and numerical instability. With a suitable choice for the prior an extension of our previous derivation to the full MAP solution is straightforward. Our combined likelihood and prior expression is (assuming a uniform prior on ):</p><p>pð; ; jDÞ / pðDj; ; ÞpðÞpðÞ:</p><p>Taking the logarithm of both sides, we see that the maximization task is given by:</p><formula xml:id="formula_10">; ; ¼ arg max ;; log pðGj; ; Þ þ log pðÞ þ log pðÞ:<label>ð11Þ</label></formula><p>Thus, we can simply append these terms onto our bound on <ref type="bibr" target="#b0">(1)</ref>. Noting that they are functions of and only (and any associated hyperparameters), we conclude that these terms only change the update equations for ak and ak . Let us assume the following priors:</p><formula xml:id="formula_11">pð gk Þ / N ð0; Þ;<label>ð12Þ</label></formula><formula xml:id="formula_12">pð 2 gk Þ / exp À s 2 gk ( ) ;<label>ð13Þ</label></formula><p>i.e., a zero-mean Gaussian for the means and an improper prior for gk (i.e., not integrating to unity over its domain) which gives zero weight to gk ¼ 0 and asymptotically approaches 1 as gk approaches infinity. Plots of these priors can be seen in Fig. <ref type="figure" target="#fig_1">2</ref> where ¼ 0:1 and s ¼ 0:1. Adding these to our original bound, we now have to differentiate the following expression to obtain updates for gk and gk</p><formula xml:id="formula_13">X A a¼1 Q gka log pðe ga j gk ; gk Þ þ log pð gk Þ þ log pð gk Þ:<label>ð14Þ</label></formula><p>Performing the necessary differentiations, we are left with the following new updates</p><formula xml:id="formula_14">gk ¼ 2 P A a¼1 Q gka e ga 2 gk þ 2 P A a¼1 Q gka ;<label>ð15Þ</label></formula><formula xml:id="formula_15">2 gk ¼ P A a¼1 Q gka ðe ga À gk Þ 2 þ 2s P A a¼1 Q gka :<label>ð16Þ</label></formula><p>These expressions clearly show the effect of the priors. In (15), we see that we now have a dependancy on both 2 gk and 2</p><p>. In the limit of 2 ! 1, we effectively have a uniform prior and we recover our original update equations for the maximum likelihood solution. In the opposite limit of 2 ! 0, we find gk ! 0 as we would expect. In (16), the prior is effectively putting a lower bound on the value of 2 gk . As s ! 0, the prior approaches a uniform distribution over all positive reals and we recover our original 2 gk update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Computing the Likelihood and Parameter Initialization</head><p>Once the model parameters have been estimated, we can calculate the likelihood for a collection of samples A 0 using:</p><formula xml:id="formula_16">L ¼ Y A 0 a¼1 Z Y G g¼1 X K k¼1 N ðe ga jk; gk ; gk Þ k ( ) pðjÞd;<label>ð17Þ</label></formula><p>where we estimate the expectation over the Dirichlet distribution by averaging over N samples drawn from the estimated Dirichlet prior pðjÞ</p><formula xml:id="formula_17">L % Y A 0 a¼1 1 N X N n¼1 Y G g¼1 X K k¼1 N ðe ga jk; gk ; gk Þ kn ( ) :<label>ð18Þ</label></formula><p>Before iteratively updating Q kga , ak , gk , 2 gk , and k various parameter values need to be initialized. In the numerical experiments reported below, the process mean values gk were initialized to the mean expression value across the data set for each gene. Similarly, the variances 2 gk were set to the variance of their respective genes. The k values were initialized to 1 for all k and the variational Dirichlet parameters a were initialized to a positive random number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NUMERICAL EXPERIMENTS</head><p>To evaluate the performance of Latent Process Decomposition (LPD) we applied it to four microarray data sets. For the first three data sets (prostate, lung, and lymphoma cancer), we show that LPD compares favorably with other cluster analysis methods, including dendrograms and Naive-Bayes, in its ability to classify samples. In the fourth example for the yeast cell cycle <ref type="bibr" target="#b10">[11]</ref>, we show that LPD can generate a rich, biologically relevant description of complex experiments. In these numerical studies, the only preprocessing was to take the logarithm of the data values (base 2) if this had not already been performed by the originators of the data set. Any likelihood calculations were approximated by (18) where samples from pðjÞ were taken by using k independent samples from gamma distributions with parameters k and then normalizing the result to sum to 1. The algorithm performs EM-like updates of the variational and model parameters. As such, convergence is guaranteed, although only to a local maximum of the training likelihood and not to the global maximum. Therefore, any analysis given below corresponds to the best solution (in terms of the likelihood) achieved over several starts with different initial a values.</p><p>When estimating the best value of K for a particular data set, an N-fold cross-validation procedure was used. This consisted of randomly partitioning the data into N subsets, one of which would be removed before training and then used to calculate the likelihood. Each of the N subsets would be left out in turn and the N likelihood values would be averaged to give the estimate of the true value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Prostate Cancer Data Set</head><p>Our first cancer data set comes from a study of prostate cancer by Dhanasekaran et al. <ref type="bibr" target="#b3">[4]</ref> consisting of 54 samples with 9,984 features (we used their complete data set corresponding to the supplementary information of <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">Fig. 8]</ref>). This data set consists of 14 samples for benign prostatic hyperplasia (labeled BPH), three normal adjacent prostate (NAP), one normal adjacent tumour (NAT), 14 localized prostate cancer (PCA), one prostatitis (PRO), and 20 metastatic tumours (MET). We will use this data set to compare performance with uniform priors and nonuniform priors and to compare Latent Process Decomposition with hierarchical cluster analysis.</p><p>Fig. <ref type="figure" target="#fig_2">3a</ref> shows the result of hierarchical cluster analysis using a dendrogram with average linkage and a Euclidean distance measure. Although some local structure can be identified, we see that the dendrogram gives a poor separation between known classes (in line with results presented in Dhanasekaran et al. <ref type="bibr" target="#b3">[4]</ref>). However, the number of differentially expressed genes can be expected to be small relative to the total number of genes. As an unsupervised method we cannot use prior class knowledge but we can use a prior belief that genes varying little across samples are less likely to be interesting. Hence, to investigate the impact of feature selection we ranked genes based on variance across samples and used the subset of features with highest variance. Fig. <ref type="figure" target="#fig_2">3b</ref> shows the corresponding result for performing hierarchichal clustering on this reduced data set. Although the dendrogram manages to split metastatic from the rest successfully, the difference between PCA and BPH is still unclear and it is very difficult to infer from this dendrogram just how many clusters are present.</p><p>In Fig. <ref type="figure">4</ref> (lower and descending curve) we give the holdout log-likelihood versus K, the number of processes, with a uniform prior and using the top-ranked 500 features. The loglikelihood peaks at about K ¼ 4 processes, after which a model with uniform prior overfits the training data as we increase the number of processes. This is to be expected with a maximum likelihood approach as we are allowing the model too much freedom by not penalising complexity. To solve this problem, we can incorporate the informative priors discussed in Section 2.3. We will discuss the determination of the parameters in these priors shortly. However, to illustrate performance we give the log-likelihood versus K plot in Fig. <ref type="figure">4</ref> (top, level curve) using values of 2 ¼ 0:1 and s ¼ 0:1. We see that the log-likelihood remains approximately constant as the number of processes is altered, rather than descending as K increases.</p><p>Fig. <ref type="figure">5a</ref> provides an explanation for this behaviour: this is a decomposition of the prostate data into K ¼ 10 processes with a uniform prior (and using the top 500 genes ranked by variance). Given that the estimated optimum number of processes is 4, we might expect the model to severely overfit, as indeed it does. However, if we introduce priors with 2 ¼ 0:1 and s ¼ 0:5, we obtain the decomposition seen in Fig. <ref type="figure">5b</ref>. We observe that the decomposition is over just four processes, with the remaining six processes empty. This is very useful since, given suitable values for the prior parameters, we do not need to decide the exact number of processes. We also notice that the decomposition is much cleaner. For example, the metastatic samples (MET) have no overlap with localized prostate cancer (PCA) and the benign and normal samples indicating they are a distinct state.</p><p>Although we can avoid overfitting with correct parameters for the prior, this still leaves the question of the correct choice of these prior parameters. Our numerical investigations across different data sets indicated that the mean parameter, , had negligible effect and we typically set this parameter to 0:1. However, performance did depend on s, the variance parameter. In Fig. <ref type="figure">6</ref>, we plot from a cross validation study of the log-likelihood versus s for the 500 top-ranked genes in the prostate data set. We notice a peak at s ¼ 0:1, though any choice for s on the range 0:1 and 0:5 appears satisfactory.</p><p>It may appear that we have gained very little by moving from cross-validation over processes to one over s, but arguably this is not the case for two reasons. First, we are dealing with a small number of samples and, hence, by removing some samples for cross-validation, we could easily remove a substantial proportion of one particular class, suggesting a bias toward underestimating the number of classes present. The variance parameter on the other hand is a measure of noise that should be class independent and, hence, should be the same regardless of whether or not the full data set is being used. Also, the number of processes present will be dependent on the particular data set and could vary greatly depending on the given study. On the other Fig. <ref type="figure">4</ref>. Hold-out log-likelihood (y-axis) versus the K (x-axis) using the top ranked 500 genes in the prostate cancer data set using a nonuniform prior as given in Section 2.3 (upper, level curve) and a uniform prior (lower, descending curve) as described in Section 2.2.</p><p>hand, the contribution made by noise is a consequence of the experimental procedure and we may expect the optimum value of s to vary very little across data sets. Indeed, in the following sections, we shall see that this is the case.</p><p>One advantage of LPD over standard clustering techniques is the ease with which information can be extracted from the derived model. In this prostate cancer example, we could be interested in finding those genes most differentially expressed between localized prostate cancer (PCA) and the metastatic samples (MET), for example. The following score can be used to rank genes:</p><formula xml:id="formula_18">Z ¼ j gi À gj j ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi 2 gi þ 2 gj q ;<label>ð19Þ</label></formula><p>where i and j correspond to the two processes being compared. Genes ranking highly will be those with large separation between their means and small standard deviations-corresponding to genes whose values fall consistently into two distinct clusters. In Table <ref type="table" target="#tab_0">1</ref>, we give a list of the 10 top-ranked genes based on this score. Plots of the mixture densities defined by the latent processes and the actual data values for three of these genes can be seen in Fig. <ref type="figure" target="#fig_4">7</ref>. These three plots show that the extracted genes appear to distinguish well between localized cancer and metastasis and, importantly, have been highlighted as significant without the use of the class labels. An investigation of these genes (Dr. Jeremy Clark, personal communication) indicates that some of them are potentially significant for cancer biology. For example, CTGF is an immediate-early gene induced by growth factors or certain oncogenes. The expressed protein binds to integrins and modulates the invasive behavior of certain human cancer cells <ref type="bibr" target="#b2">[3]</ref>. Similarly, it has been reported <ref type="bibr" target="#b4">[5]</ref> that tumor angiogenesis and tumor growth are critically dependent on the activity of EGR1.</p><p>We have used the likelihood as our measure of model performance. One issue that we have not yet explored is how these likelihood values compare with those for alternative, and possibly simpler, cluster analysis methods. The most obvious comparison is with a standard Gaussian mixture model. The main difference between LPD and a Gaussian mixture model is the repeated sampling for each feature in LPD allowing each sample to be represented by a combination of processes. Due to the large number of features involved, it is computationally infeasible to use a Gaussian mixture model with full covariance matrices and thus we shall constrain the matrices to be diagonal (equivalent to Gaussians aligned with the coordinate axis). Fig. <ref type="figure">8</ref> shows the log-likelihood curves using maximum likelihood (i.e., no priors) for LPD and Naive-Bayes. We see that Naive-Bayes begins overfitting immediately, suggesting only two clusters in the data. LPD rises above Naive-Bayes to a peak at K ¼ 4 processes before dropping toward the Naive-Bayes curve as K increases. Although the difference in the log-likelihood is not huge, it is statistically significant, given the error bars, and the suggestion of only two clusters from Naive-Bayes is misleading. Note that we have used the maximum likelihood approach for training LPD to give the fairest comparison with Naive-Bayes which is also trained by maximum likelihood. It is also worth noting the relative sizes of the error bars. For Naive-Bayes, it is possible to calculate the likelihood exactly, whereas with LPD we must take samples from the Dirichlet. The error bars for the two methods are certainly of the same order, suggesting that most variation is due to the cross-validation procedure thus allowing us to be confident in our LPD likelihood approximation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Lung Cancer Data Set</head><p>The lung cancer data set <ref type="bibr" target="#b5">[6]</ref> consists of 73 gene expression profiles from normal and tumour samples with the tumors labelled as squamous, large cell, small cell, and adenocarci-noma. Ten-fold cross validation was performed for values of K between 2 and 20 with and without parameter priors and the results are shown in Fig. <ref type="figure" target="#fig_5">9</ref>. As for the prostate cancer example, we notice a nonuniform prior avoids overfitting and causes the log-likelihood to plateau. In <ref type="bibr" target="#b5">[6]</ref>, the authors identified seven clusters in the data-with the adenocarcinoma samples falling into three separate clusters with strong correlation with clinical outcomes. For their ordering (which  Fig. <ref type="figure">8</ref>. Hold-out log-likelihood (y-axis) for maximum likelihood LPD (upper curve) and Naive-Bayes (lower curve) for various value of K (x-axis). The top 500 features are used for both models.</p><p>we will follow), samples 1-19 belong to adenocarcinoma cluster 1, samples 20-26 belong to adenocarcinoma cluster 2, samples 27-32 are normal tissue samples, samples 33-43 are adenocarcinoma cluster 3, samples 44-60 are squamous cell carcinomas, samples 61-67 are small cell carcinomas, and samples 68-73 are from large cell tumors. For this example and the following data set (for lymphoma), features selection was found to have little effect so we give the analysis without feature selection.</p><p>As in the previous section, the plateauing of the loglikelihood with a nonuniform prior suggests that we are using more processes than are necessary and additional processes are left empty. The onset of a plateau in the loglikelihood indicates the minimum number of processes to use and K ¼ 10 would seem reasonable. The cross-validation study over s is given in Fig. <ref type="figure" target="#fig_0">10</ref> and we can see a peak between 0:3 and 0:5. This is approximately the same peak value found for the prostate cancer data set.</p><p>In Fig. <ref type="figure" target="#fig_6">11</ref>, we use LPD to decompose the data into 10 processes with the bars representing the normalized ak values (equivalent to E pðj a Þ ½) and expressing the confidence in the allocation of the ath sample to the kth process. The samples are in the order in which they are presented in the original paper <ref type="bibr" target="#b5">[6]</ref>.</p><p>First, we observe that the clustering by dendrogram is accurately reproduced by latent process decomposition. One notable difference is that the last two entries for the small cell carcinoma grouping are placed with the adenocarcinoma cluster 2 by latent process decomposition with high values of probability (close to one). For the original clustering by dendrogram, these two samples are two adenocarcinomas inaccurately placed by the dendrogram in a cluster with small cell carcinomas. This difference is therefore in favor of latent process decomposition. For latent process decomposition, we find that the adenocarcinoma clusters 1 and 2 decompose across processes 1 and 2 and adenocarcinoma cluster 3 is almost exclusively represented by process 4. There are rare instances of adenocarcinoma placed in processes 9 and 10. The normal samples (process 3) and large cell tumors (process 8) are very distinct. The squamous cell carcinomas are decomposabled into processes 5 and 6, both of which have little commonality with any other tumor types in the data set. This might suggest possible further taxonomic decomposition into subtypes. However, this possibility should be treated with caution: decomposition into subprocesses could also mirror the different genetic stages of a single subtype, for example.</p><p>In <ref type="bibr" target="#b5">[6]</ref>, the class labels were used to extract genes which distinguished well between the different adenocarcinoma clusters. Using the Z-score described in the previous section, we give the top 10 ranked genes distinguishing processes 1 and 4 (adenocarcinoma 1 and adenocarcinoma 3) in Table <ref type="table" target="#tab_1">2</ref>. Of these top genes, numbers 1, 2, and 10 were discussed in <ref type="bibr" target="#b5">[6]</ref> as being significant for cancer biology. Also, the biological relevance of the detected genes is highlighted by the internal consistency of the list: two antagonistic proteins of sugar metabolism and a variety of transmembrane proteins (including two solute carriers) dominate the picture. Importantly, and in constrast to Garber et al. <ref type="bibr" target="#b5">[6]</ref>, these features have been extracted with no prior knowledge of class membership. 1  Finally, as for the prostate cancer study, we can use the hold-out log-likelihood to compare LPD with Naive-Bayes. Again, we will use the maximum likelihood method to train  1. For a more complete list and comparisons between other processes, see <ref type="url" target="http://www.enm.bris.ac.uk/lpd">http://www.enm.bris.ac.uk/lpd</ref>. both models to ensure a fair comparison. The results can be seen in Fig. <ref type="figure" target="#fig_7">12</ref>. Although the difference between the two methods is small, LPD tends to overfit less quickly as K increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Lymphoma Cancer Data Set</head><p>Next, we consider the Lymphoma data set of Alizadeh et al. <ref type="bibr" target="#b0">[1]</ref>, which consists of 96 samples of normal and malignant lymphocytes. In this study, the authors used samples from diffuse large B-cell lymphoma (DLBCL), the most common form of non-Hodgkin's lymphoma in addition to including data for follicular lymphoma (FL), chronic lymphocytic leukaemia (CLL), and other cell types.</p><p>Determining the hold-out log-likelihood versus K with 10-fold cross-validation, we find overfitting with a maximum likelihood solution while the log-likelihood plateaus after K ¼ 10, suggesting this is the smallest number of processes to use for a good fit to the data. Using K ¼ 10 we can cross-validate over different values of the variance prior parameter s and the results are shown in Fig. <ref type="figure" target="#fig_8">13</ref>.</p><p>A decomposition into K ¼ 10 processes (using the value of s that gave the maximum in Fig. <ref type="figure" target="#fig_8">13</ref>) can be seen in Fig. <ref type="figure" target="#fig_0">14</ref>. The samples are presented in the same order as Alizadeh et al. <ref type="bibr" target="#b0">[1]</ref> with samples 1 to 16 largely consisting of DLBCL with samples 1 and 2 consisting of two DLBCL cell lines (OCI Ly3 and Ly10), while 10 and 11 are tonsil germinal cell samples.</p><p>Samples, 17 to 47 consist of DLBCL, samples 48 to 57, 58 to 63, and 65 to 70 consist of blood B and other samples, sample 64 is the OCI Ly1 cell line, sample 71 is a single DLBCL sample placed in with this general cluster of blood B and other tissues, samples 72-80 are follicular lymphoma (FL), samples 85 to 95 are CLL, and 96 is a lone DLBCL sample. The decomposition agrees well with the structure obtained using a dendrogram <ref type="bibr" target="#b0">[1]</ref>. Decomposition of the DLCL samples into several processes suggests substructure supporting the discussion in <ref type="bibr" target="#b0">[1]</ref>. LPD and the dendrogram do differ, e.g., sample 71 (DLCL) is now partly grouped with the other DLCL samples and not the FL samples in contrast to the dendrogram. Also, process 6 brings together the OCI samples which was not achieved by the dendrogram.</p><p>Finally, in Fig. <ref type="figure" target="#fig_9">15</ref>, we compare the hold-out log-likelihood for LPD with that for the Naive-Bayes model. As in the previous examples, we see that LPD does not overfit as quickly and suggests a greater number of processes than the Naive-Bayes model. It also gives a higher maximum value, suggesting it is better suited to this particular data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Yeast Cell Cycle Data Set</head><p>For the next example, we have used the yeast cell cycle data set of Spellman et al. <ref type="bibr" target="#b10">[11]</ref>. In this study, the authors investigate which yeast genes show periodically varying expression levels during cell cycle progression, using whole-genome microarrays covering approximately 6,200 genes. Using a cross-validation study of log-likelihood versus s with a   nonuniform prior, we found the curve achieved its peak value at s ¼ 0 suggesting that a nonuniform prior was not of benefit. Thus, the results in this section are presented using the maximum likelihood solution of Section 2.2. In contrast to the previous three examples, we will now decompose over genes rather than samples (see the end of Section 2.2). In Fig. <ref type="figure" target="#fig_10">16</ref>, we perform a comparison of the log-likelihood curves versus K for three methods and in all cases LPD gives the best solution for these data sets.</p><p>Fig. <ref type="figure" target="#fig_10">16</ref> compares the predictive likelihood for LPD, Naive-Bayes and a Bayesian network model introduced by Segal <ref type="bibr" target="#b9">[10]</ref>. Segal's model decomposed the data into a set of processes where each gene's membership of each process is defined by a separate binary variable. Each process has an activity value defined for each array. The expression value for a particular gene in a particular array is assumed to be Gaussian where the mean is the sum of the activities for each process to which this gene belongs. This method is considered to be state-of-the-art and as such we have included it in our comparisons. One problem with Segal's method is that estimating the binary membership parameters exactly is NP-hard and so for values of K greater than 10, it becomes very time consuming. It is for this reason that the likelihood curves for Segal's method stops at K ¼ 10 processes in Fig. <ref type="figure" target="#fig_10">16</ref>. Various heuristics exist to estimate these variables, but given the relative difference in log-likelihood between this and the other two methods, it seemed unnecessary to implement them.</p><p>While the previous data sets for cancer mainly demonstrate the performance of LPD in categorising data, this cell cycle data can reveal the biological usefulness of the identified latent processes and their combinations in any given sample. In this respect, LPD is able to provide a far richer picture than is available from a simple clustering approach. Fig. <ref type="figure" target="#fig_10">16</ref> shows that for this data set between 5 and 10 processes give the highest predictive likelihood. From the well-characterized biology of the experiment, we expect one process corresponding to each of the known cell cycle phases (G1, S, G2, M, MG1, as defined in the original paper <ref type="bibr" target="#b6">[7]</ref>) plus a small number of "classifying" processes that distinguish between the four different synchronization methods that were used in the experiment. Fig. <ref type="figure" target="#fig_11">17</ref> shows the 10 processes defined by LPD. It can easily be seen that processes 1 and 3 represent "classifying" processes that characterize alpha factor/elutriation synchronization and CDC15 synchronization, respectively. Processes 2, 4, 6, and 8 correspond to cycling processes and each of them can be uniquely mapped to one of the major cell cycle phases by reference to the biological information given in <ref type="bibr" target="#b6">[7]</ref> (highlighted in Fig. <ref type="figure" target="#fig_11">17</ref>). The fifth cell cycle phase, G2, which is very short in exponentially growing yeast cultures, is not recovered as a distinct process but can easily be obtained by the linear combination of the adjacent S and M phases (not shown). The remaining processes (5, 7, 9, and 10) seem to be dominated by noise and experimental artefacts (particularly in process 7). This emphasizes that identification of the most parsimonious number of latent processes is an extremely useful feature of the LPD approach. This would be even more important in experiments without previous knowledge of the underlying process structure, where the identification of the really informative processes would not be as easy as in the present test case.</p><p>From a biological point of view, it is particularly noteworthy that the cycling processes were identified without any prior reference to the time-series structure of the experiment. The original publication used a Fourier transformation to determine the expression peaks of cyclically expressed genes and Pearson correlation with known cell cycle-regulated genes to assign them to cell cycle phases. In the case of LPD, the same can be achieved by reference to the gene-specific variational parameter gk . This is shown in Fig. <ref type="figure" target="#fig_0">18</ref>, where the expression of a classical G1 phase marker gene (cyclin CLN2) is indeed represented most strongly by process 2, which according to Fig. <ref type="figure" target="#fig_11">17</ref>, corresponds to peak expression in G1.</p><p>Fig. <ref type="figure" target="#fig_5">19</ref> shows a more complex example of the power of this approach. The histone genes, which code for the main protein component of chromosomes, are known to be most strongly expressed in S phase, when new chromosomes need to be assembled. Indeed, Fig. <ref type="figure" target="#fig_5">19</ref> shows that expression of one  representative histone (H1) is dominated by the S phasespecific process 8 (compare Fig. <ref type="figure" target="#fig_11">17</ref>). However, there are also significant contributions by processes 2 and 6 (G1 and M phase) and the measured expression profile (solid line in Fig. <ref type="figure" target="#fig_5">19</ref>, lower panel) shows clearly that this specific histone has a much broader expression range that extends well beyond S phase. This is not entirely unexpected as the production of histone proteins should precede the replication of DNA that defines S phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>Our experimental results strongly indicate that latent process decomposition is an effective technique for discovering structure within microarray data. It has several advantages over the use of dendrograms or simple clustering: unambiguous determination of the number of processes via cross-validation using a hold-out (predictive)  Fig. <ref type="figure" target="#fig_0">18</ref>. The expression levels of the G1-specific cyclin CLN2 YPL256C peaks in G1 phase (lower panel, solid line). The dotted line is the sum of the mean for each process weighted by the value of the gene-specific variational parameter for each process (top bar chart). In this case, it can be seen that the second process profile (G1 phase) is indeed dominant for this gene. Fig. <ref type="figure" target="#fig_5">19</ref>. The expression levels of the histone H1 gene (YPL127C) peaks in S phase (lower panel, solid curve). The dotted line is the sum of the mean for each process weighted by the value of the gene-specific variational parameter for each process (top bar chart). In this case, it can be seen that a combination of the second, sixth, and eighth process profiles is required for this histone. This combination corresponds to the G1, M, and S phase processes identified by LPD (Fig. <ref type="figure" target="#fig_11">17</ref>).</p><p>likelihood. In addition, the model uses a shared latent space with the same explanatory variables for both samples and genes. Overfitting can be effectively controlled using a nonuniform prior, in contrast to many cluster analysis methods. Also, several processes can be simultaneously combined to determine gene expression levels so that the relationship between processes is more transparent. LPD has significant advantages over other cluster analysis methods, for example, models based on a mixture of Gaussians <ref type="bibr" target="#b8">[9]</ref>. With the latter models, genes must be preselected followed by another level of feature extraction due to the rank deficiency of the estimated covariance matrix in the original space. This is not required in the approach advocated here. Given the large number of features used by microarray data sets, there is a redundancy of information. For this reason, we found that feature selection can sometimes improve the performance of the model. For lung and lymphoma data sets, we used no feature selection, but for the prostate cancer and cell cycle data sets, we used the top 500 and 1,000 genes as selected by variance spread. Feature selection in this context is an avenue for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A</head><p>In this appendix, we will give a full derivation of the LPD model. We will outline the derivation of the maximum likelihood solution given in Section 2.2: the generalization to a nonuniform prior is straightforward and outlined in Section 2.3. The likelihood of a set of A training samples is as follows:</p><formula xml:id="formula_19">Y A a¼1 pðaj; ; Þ ¼ Y A a¼1 Z pðaj; ; ÞpðjÞd;<label>ð20Þ</label></formula><p>where the probability of the sample conditioned on the means, variances, and process weightings can be expressed in terms of its individual components giving the following:</p><formula xml:id="formula_20">Y A a¼1 pðaj; ; Þ ¼ Y A a¼1 Z Y G g¼1 X K k¼1 N ðe ga jk; gk ; gk Þ k ( ) pðjÞd:<label>ð21Þ</label></formula><p>For a concave function fðxÞ, Jensen's inequality states that f E pðzÞ ½z À Á ! E pðzÞ ½fðzÞ, where E p ðzÞ ¼ R zpðzÞdz for continuous z. Thus, introducing a set of sample-specific variational Dirichlets pðj a Þ, we can derive a lower bound on the log-likelihood</p><formula xml:id="formula_21">X A a¼1 log pðaj; ; Þ ! X A a¼1 E pðj a Þ log pðaj; ; Þ pðjÞ pðj a Þ &amp; ' ! ;<label>ð22Þ</label></formula><p>where</p><formula xml:id="formula_22">E pðj a Þ fðÞ ½ ¼ Z fðÞpðj a Þd:<label>ð23Þ</label></formula><p>A set of variational parameters Q kga are now introduced such that P k Q kga ¼ 1 for bounding pðaj; ; Þ in the above inequality. These parameters can be regarded as the posterior distribution over the different processes for each g and a. Jensen's inequality can then be used again to give the following additional bound</p><formula xml:id="formula_23">X A a¼1 log pðaj; ; Þ ! X K k¼1 X A a¼1 X G g¼1 Q kga log N ðe ga jk; gk ; gk Þ k Q kga &amp; ' :<label>ð24Þ</label></formula><p>Combining these bounds into one gives the following</p><formula xml:id="formula_24">X a log pðajb; ; Þ ! X a E pðj a Þ ½log pðjÞ À X a E pðjaÞ ½log pðj a Þ þ X a X g X k Q kga log N ðe ga jk; gk ; gk Þ À X a X g X k Q kga log Q kga þ X a X g X k Q kga E pðj a Þ ½log k :<label>ð25Þ</label></formula><p>A.1 Model Parameters</p><p>The process mean and variance parameters gk and 2 gk only appear in the third term of (25). This term can be expanded as follows:</p><formula xml:id="formula_25">X a X g X k Q kga log N ðe ga jk; gk ; gk Þ ¼ X a X g X k Q kga log 1 ffiffiffiffiffiffiffiffiffiffiffi ffi 2 2 gk q 2 6 4 3 7 5 À X a X g X k Q kga ðe ga À gk Þ 2 2 2 gk :<label>ð26Þ</label></formula><p>Taking partial derivatives of this expression with respect to gk and gk , setting the results to zero and solving yields the following updates</p><formula xml:id="formula_26">gk ¼ P a Q kga e ga P a Q kga ;<label>ð27Þ</label></formula><formula xml:id="formula_27">2 gk ¼ P a Q kga ðe ga À gk Þ 2 P a Q kga :<label>ð28Þ</label></formula><p>The parameter only appears in term 1 of (25), but partial differentiation of this term with respect to k reveals that a simple iterative procedure is not possible and a second order technique is required. Term 1 can be written</p><formula xml:id="formula_28">X a E pðj a Þ ½logðpðjÞ ¼ X a E pðj a Þ log Àð P k k Þ Q k Àð k Þ &amp; ' þ log Y k<label>ðkÀ1Þ</label></formula><formula xml:id="formula_29">k " # :<label>ð29Þ</label></formula><p>Thus, differentiating this expression we get</p><formula xml:id="formula_30">g i ¼ A ð Þ À i ð Þ ½ þ X A a¼1 ia ð ÞÀ a ð Þ ½ ;<label>ð30Þ</label></formula><p>from which we can derive the corresponding Hessian</p><formula xml:id="formula_31">H ij ¼ 0 ð Þ À ij A 0 i ð Þ; ð31Þ new ¼ old À Hð old Þ À1 gð old Þ;<label>ð32Þ</label></formula><p>where HðÞ is the Hessian matrix and gðÞ is the gradient. For i; j ¼ 1; . . . ; K, their components are given by:</p><formula xml:id="formula_32">g i ¼ A ð Þ À i ð Þ ½ þ X A a¼1 ia ð Þ À a ð Þ ½ ;<label>ð33Þ</label></formula><formula xml:id="formula_33">H ij ¼ 0 ð Þ À ij A 0 i ð Þ;<label>ð34Þ</label></formula><p>where a ¼ P K j¼1 jc , ¼ P K k¼1 k , and 0 ð Þ is the trigamma function. Since the Hessian matrix is of the form</p><formula xml:id="formula_34">H ¼ diagðhÞ þ 1z1 T ;<label>ð35Þ</label></formula><p>matrix inversion can be avoided <ref type="bibr" target="#b1">[2]</ref> and the ith component of the correction in the iterative update rule can be found from</p><formula xml:id="formula_35">ðH À1 gÞ i ¼ ðg i À rÞ=h i ;<label>ð36Þ</label></formula><formula xml:id="formula_36">where r ¼ X k j¼1 ðg j =h j Þ ! = z À1 þ X k j¼1 h À1 j ! :<label>ð37Þ</label></formula><p>A.2 Variational Parameters First, Q kga appears in terms 3, 4, and 5 of (25). Taking these terms, removing summations over k, a, and g and adding a Lagrangian to satisfy the summation constraint, we have</p><formula xml:id="formula_37">Q kga log N ðe ga jk; gk ; gk Þ À Q kga log Q kga þ Q kga E pðj a Þ ½log k À X a Q kga À 1 ! ;<label>ð38Þ</label></formula><p>taking partial derivatives with respect to Q kga and setting these to zero gives </p><p>where</p><formula xml:id="formula_39">E pðj a Þ ½log k ¼ Éð ak Þ À É X j aj ! :<label>ð41Þ</label></formula><p>Now, the ak variable appears in terms 1, 2, and 5 of the above bound. Extracting these terms and removing summations over k and a leaves</p><formula xml:id="formula_40">À E pðj a Þ ½log pðj a Þ þ E pðj a Þ ½log pðjÞ þ X g Q kga E pðj a Þ ½log k :<label>ð42Þ</label></formula><p>Now,</p><formula xml:id="formula_41">E pðj a Þ ½log pðj a Þ ¼ Z Á log Àð P k ak Þ Q k Àð ak Þ þ X k ð ak À 1Þ log½ k " # pðj a Þd ¼ log Àð P k ak Þ Q k Àð ak Þ ! þ X k ð ak À 1ÞE pðjaÞ ½log k ¼ log Àð P k ak Þ Q k Àð ak Þ ! þ X k ð ak À 1Þ ½Éð ak Þ À É X j aj ! ;<label>ð43Þ</label></formula><p>and similarly for the pðjÞ term. Therefore, the relevant terms simplify to the following (again, omitting unnecessary summations and nonak terms)</p><formula xml:id="formula_42">k À ak þ X g Q kga ! Éð ak Þ À É X j aj ! " # Àlog Àð P k ak Þ Q k Àð ak Þ ! :<label>ð44Þ</label></formula><p>Taking partial derivatives with respect to ak and setting these to zero leaves</p><formula xml:id="formula_43">k À ak þ X g Q kga ! É 0 ð ak Þ À É 0 X j aj ! " # ¼ 0;<label>ð45Þ</label></formula><p>giving the following update for ak</p><formula xml:id="formula_44">ak ¼ k þ X g Q kga :<label>ð46Þ</label></formula><p>A.3 Pseudocode Listings Algorithm 1: Latent Process Decomposition algorithm. We used a tolerance tol 1 ¼ 10 À5 to avoid Gaussians collapsing onto a single point, and tol 2 ¼ 10 À10 for the updating.</p><p>Parameter Initialization. From (6), we can see that in order to update Q kga , we need values for gk , gk , and ak . The first two are initialized to the array means and standard deviations, respectively. The ak values were initialized to positive random numbers since, with the means and standard deviations uniform over the K processes, uniform ak values would place the algorithm in a local minima of the likelihood and training could not proceed.</p><p>Numerical Stability. There are several points where the algorithm could potentially lose numerical stability. If a Gaussian contracts onto a single point, its standard deviation will shrink to zero and its probability density function will approach 1. To avoid this, we apply a lower bound to the value of 2 gk . The choice of the value of this bound is important since, if the value is too low, numerical problems will persist, while if it is too high it would overconstrain the model and prevent it from fitting the data. For the update equation for Q kga , we find a second possible source of instability. From (6), we occasionally find that we reach the limits of machine precision and both the numerator and the denominator approach zero. To circumvent this, we added a small constant (10 À100 ) to each value in Q before normalization (i.e., before performing the division in ( <ref type="formula" target="#formula_5">6</ref>)). Finally, when calculating the likelihood, we have to compute a product over the g. For a large number of genes, this product soon becomes smaller than the machine precision. However, we are ultimately interested in the logarithm of this product (the log-likelihood) and so we can perform the following numerical trick to avoid a problem. Namely, we observe that the expression we wish to evaluate is of the form log P i Q j R ij . This can be rewritten</p><formula xml:id="formula_45">log X i exp i " # ¼ log X i expf i þ Kg expðÀKÞ " # ¼ log X i expð i þ KÞ " # À K;</formula><p>if i ¼ P j log R ij , which can be readily evaluated given a suitable choice for K. Given that the i values will all be negative, a suitable choice is À max½ i so that we are effectively incrementing all i values so that the maximum value is 0. Following the implementation of these precautions, the algorithm is robust.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Graphical model representations of LPD. (a) Cluster model structure. (b) LPD model structure.</figDesc><graphic coords="2,70.75,69.17,425.15,159.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example priors for LPD gk and gk parameters. (a) Prior for gk . (b) Prior for gk .</figDesc><graphic coords="4,64.97,69.17,436.52,183.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Result of hierachichal cluster analysis on the prostate data set, using average linkage and a Euclidean distance measure. (a) Dendrogram with all features. (b) Dendrogram with top-ranked 500 features.</figDesc><graphic coords="5,63.55,69.17,439.43,185.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. K ¼ 10 process decomposition of the prostate cancer data set using the top 500 genes. The peaks represent the confidence in the assignment of the ath sample to the kth process using ak . (a) With a uniform prior. (b) Using a nonuniform prior with 2 ¼ 0:1 and s ¼ 0:5.</figDesc><graphic coords="6,64.97,69.17,436.52,183.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Inferred density plots for three genes separating localized prostate cancer (PCA, process 2) from the metastatic samples (MET, process 4) in Fig. 5b. Actual data values are shown below the mixtures, separated by class with circles representing expression values for localized prostate cancer and crosses representing values for metastatic prostate cancer samples. (a) Early growth response (EGR1). (b) Connective tissue growth factor (CTGF). (c) Dopachrome delta-isomerase, tyrosine-related protein 2 (DCT).</figDesc><graphic coords="7,83.51,233.01,399.67,129.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. Hold-out log-likelihood (y-axis) versus K (x-axis) for the lung cancer data set with (upper, level curve) and without parameter priors (lower, descending curve).</figDesc><graphic coords="8,29.48,69.17,244.58,189.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Normalized ak values for a 10 process decomposition of the lung cancer data.</figDesc><graphic coords="8,292.54,69.17,244.58,118.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.12. Hold-out log-likelihood (y-axis) for maximum likelihood LPD (top curve for K ¼ 5) and Naive-Bayes (bottom curve for K ¼ 5) for various value of K (x-axis).</figDesc><graphic coords="9,29.48,539.32,244.58,186.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Hold-out log-likelihood (y-axis) versus the variance prior parameter, s (x-axis), for the lymphoma data set.</figDesc><graphic coords="9,292.54,69.17,244.58,189.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Hold-out log-likelihood (y-axis) as a function of K (x-axis) for maximum likelihood LPD and the Naive-Bayes Gaussian mixture model.</figDesc><graphic coords="10,29.48,69.17,244.58,189.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Comparison of the log-likelihood (y-axis) versus K (x-axis) for LPD (top curve) and a Naive-Bayes mixture model (middle curve). We have additionally made a comparison with a Bayesian Network model introduced by Segal et al. [10] (bottom curve) which represents process membership over genes rather than samples.</figDesc><graphic coords="10,292.54,69.17,244.58,174.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 17</head><label>17</label><figDesc>Fig. 17. The 10 processes identified by LPD for the yeast cell cycle. The cycling processes peak in the various phases of the cell cycle as highlighted on subplots 2, 4, 6, and 8. The four different synchronization methods are indicated at the top. Our numbering convention is odd numbers starting from the top subfigure for the left-hand figures and even numbers starting from the top for the right-hand figures.</figDesc><graphic coords="11,111.97,69.17,342.76,282.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Fig. 17. The 10 processes identified by LPD for the yeast cell cycle. The cycling processes peak in the various phases of the cell cycle as highlighted on subplots 2, 4, 6, and 8. The four different synchronization methods are indicated at the top. Our numbering convention is odd numbers starting from the top subfigure for the left-hand figures and even numbers starting from the top for the right-hand figures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Top 10 Genes Separating Localized Prostate from Metastatic Prostate Cancer Genes are ranked by Z.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 Top</head><label>2</label><figDesc>Lung Genes, Ranked by Z</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>¼ N ðe ga jk; gk ; gk Þ exp½E pðjaÞ ½log k P A a 0 ¼1 N ðe ga 0 jk; gk ; gk Þ exp½E pðj a 0 Þ ½log k ;</figDesc><table /><note><p><p><p>0 ¼ log N ðe ga jk; ; gk ÞÀðlog½Q kga þ 1Þ þ E pðjaÞ ½log k À ;</p>ð39Þ</p>solving for and rearranging gives the following update for Q kga Q kga</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, VOL. 2, NO. 2, APRIL-JUNE 2005</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank <rs type="person">Dr. Jeremy Clark</rs>, <rs type="affiliation">Institute of Cancer Research, London</rs>, for comments on the genes mentioned in Section 3.1.1. The work of <rs type="person">Rainer Breitling</rs> was supported by a <rs type="funder">BBSRC</rs> grant (<rs type="grantNumber">17/GG17989</rs>) and <rs type="person">Colin Campbell</rs> and <rs type="person">Simon Rogers</rs> were supported by <rs type="funder">EPSRC</rs> grant <rs type="grantNumber">GR/R96255</rs> and an <rs type="funder">EPSRC</rs> <rs type="grantName">DTA award</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EYXsEUG">
					<idno type="grant-number">17/GG17989</idno>
				</org>
				<org type="funding" xml:id="_mSZqDtn">
					<idno type="grant-number">GR/R96255</idno>
				</org>
				<org type="funding" xml:id="_jGJcM7X">
					<orgName type="grant-name">DTA award</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distinct Types of Diffuse Large B-Cell Lymphoma Identified by Gene Expression Profiling</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">403</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="503" to="511" />
			<date type="published" when="2000-02">Feb. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Connective Tissue Growth Factor and Its Role in Lung Adenocarcinoma Invasion and Metastasis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nat&apos;l Cancer Inst</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="344" to="345" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Delineation of Prognostic Biomarkers in Prostate Cancer</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Dhanasekaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">412</biblScope>
			<biblScope unit="page" from="822" to="826" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transcription Factor EGR-1 Supports FGF-Dependent Angiogenesis during Neovascularization and Tumor Growth</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Fahmy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1026" to="1032" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diversity of Gene Expression in Adenocarcinoma of the Lung</title>
		<author>
			<persName><forename type="first">E</forename><surname>Garber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat&apos;l Academy of Sciences of the USA</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="12784" to="12789" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Genomic Expression Program in the Response of Yeast Cells to Environmental Changes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Gasch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Biology of the Cell</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="4241" to="4257" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Plaid Models for Expression Data</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Lazzeroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="61" to="86" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Mixture Model-Based Approach to the Clustering of Microarray Expression Data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Bean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="422" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Decomposing Gene Expression into Cellular Processes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eighth Pacific Symp. Biocomputing (PSB)</title>
		<meeting>Eighth Pacific Symp. Biocomputing (PSB)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="89" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Comprehensive Identification of Cell Cycle-Regulated Genes of the Yeast Saccharomyces Cerevisiae by Microarray Hybridization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Biology of the Cell</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="3273" to="3297" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">During June 2004, he was a visiting researcher in the Bioinformatics Research Centre at the University of Glasgow where he now holds a postdoctoral research position. Mark Girolami received a degree in mechanical engineering from the University of Glasgow (1985), and the PhD in computer science from the University of Paisley</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Riken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Japan</forename><surname>Wako-Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">London (1981), and the PhD degree in applied mathematics from King&apos;s College</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1984">2001. 2004. 1998. May to December 2000. 1998 and 1999. 1984. 1990</date>
		</imprint>
		<respStmt>
			<orgName>University of Bristol ; Laboratory for Advanced Brain Signal Processing in the Brain Science Institute, ; University of Stockholm and Newcastle University. He joined the Faculty of Engineering at Bristol University</orgName>
		</respStmt>
	</monogr>
	<note>His main interests are learning theory, algorithm design, and decision theory. Current topics of interest include kernel-based methods. Bayesian generative models, and the application of these techniques to real world problems, mainly in the area of medical decision support, bioinformatics, and onco-informatics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Since May 2003, he has been a research assistant at the University of Glasgow, Scotland, where he works on the development of automated interpretation techniques for biomedical microarray data sets. . For more information on this or any other computing topic</title>
	</analytic>
	<monogr>
		<title level="m">he worked as a postdoctoral fellow in bioinformatics in the Department of Biology at San Diego State University</title>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997. 2001. June 2001 to April 2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Hanover</orgName>
		</respStmt>
	</monogr>
	<note>and the PhD degree in biochemistry from Technische Universitat Munchen. please visit our Digital Library at www.computer.org/publications/dlib</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
