<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion</title>
				<funder ref="#_SjB7bPh">
					<orgName type="full">Shaanxi Fundamental Science Research Project for Mathematics and Physics</orgName>
				</funder>
				<funder ref="#_V2rUkPc #_HmVYrFy">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_HrDRGJk">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_DV9NZXa">
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
				<funder>
					<orgName type="full">Alexander von Humboldt Foundation</orgName>
				</funder>
				<funder ref="#_j3fD8fa">
					<orgName type="full">Macao Science and Technology Development Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zixiang</forename><surname>Zhao</surname></persName>
							<email>zixiangzhao@stu.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<address>
									<country>ETH Zürich</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haowen</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanzhi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<address>
									<country>ETH Zürich</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiangshe</forename><surname>Zhang</surname></persName>
							<email>jszhang@mail.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<address>
									<country>ETH Zürich</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<address>
									<country>ETH Zürich</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Macau University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<address>
									<country>ETH Zürich</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Würzburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<address>
									<country>ETH Zürich</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Discriminator</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<address>
									<country>ETH Zürich</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-modality image fusion aims to combine different modalities to produce fused images that retain the complementary features of each modality, such as functional highlights and texture details. To leverage strong generative priors and address challenges such as unstable training and lack of interpretability for GAN-based generative methods, we propose a novel fusion algorithm based on the denoising diffusion probabilistic model (DDPM). The fusion task is formulated as a conditional generation problem under the DDPM sampling framework, which is further divided into an unconditional generation subproblem and a maximum likelihood subproblem. The latter is modeled in a hierarchical Bayesian manner with latent variables and inferred by the expectation-maximization (EM) algorithm. By integrating the inference solution into the diffusion sampling iteration, our method can generate highquality fused images with natural image generative priors and cross-modality information from source images. Note that all we required is an unconditional pre-trained generative model, and no fine-tuning is needed. Our extensive experiments indicate that our approach yields promising fusion results in infrared-visible image fusion and medical image fusion. The code is available at <ref type="url" target="https://github.com/Zhaozixiang1228/MMIF-DDFM">https://github. com/Zhaozixiang1228/MMIF-DDFM</ref> .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image fusion integrates essential information from multiple source images to create high-quality fused images <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42]</ref>, encompassing various source image types like digital <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b73">74]</ref>, multi-modal <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b71">72]</ref>, and remote sensing <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b75">76]</ref>. This technology provides a clearer representation of objects and scenes, and has diverse applica-   <ref type="bibr" target="#b50">[51]</ref> and Road-Scene <ref type="bibr" target="#b58">[59]</ref> in Tab. 1. Hexagons formed by lines of different colors represent the values of different methods across six metrics. Our DDFM (marked in yellow) outperforms all other methods. tions such as saliency detection <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>, object detection <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b54">55]</ref>, and semantic segmentation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b55">56]</ref>. Among the different subcategories of image fusion, Infrared-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2303.06840v2 [cs.CV] 22 Aug 2023</head><p>Visible image Fusion (IVF) and Medical Image Fusion (MIF) are particularly challenging in Multi-Modality Image Fusion (MMIF) since they focus on modeling cross-modality features and preserving critical information from all sensors and modalities. Specifically, in IVF, fused images aim to retain both thermal radiation from infrared images and detailed texture information from visible images, thereby avoiding the limitations of visible images being sensitive to illumination conditions and infrared images being noisy and lowresolution. While MIF can assist in diagnosis and treatment by fusing multiple medical imaging modalities for precise detection of abnormality locations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>There have been numerous methods devised recently to address the challenges posed by MMIF <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b28">29]</ref>, and generative models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b37">38]</ref> have been extensively utilized to model the distribution of fused images and achieve satisfactory fusion effects. Among them, models based on Generative Adversarial Networks (GANs) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b25">26]</ref> are dominant. The workflow of GAN-based models, illustrated in Fig. <ref type="figure" target="#fig_0">1a</ref>, involves a generator that creates images containing information from source images, and a discriminator that determines whether the generated images are in a similar manifold to the source images. Although GAN-based methods have the ability to generate high-quality fused images, they suffer from unstable training, lack of interpretability and mode collapse, which seriously affect the quality of the generated samples. Moreover, as a black-box model, it is difficult to comprehend the internal mechanisms and behaviors of GANs, making it challenging to achieve controllable generation.</p><p>Recently, Denoising Diffusion Probabilistic Models (DDPM) <ref type="bibr" target="#b12">[13]</ref> has garnered attention in the machine learning community, which generates high-quality images by modeling the diffusion process of restoring a noise-corrupted image towards a clean image. Based on the Langevin diffusion process, DDPM utilizes a series of reverse diffusion steps to generate promising synthetic samples <ref type="bibr" target="#b45">[46]</ref>. Compared to GAN, DDPM does not require the discriminator network, thus mitigating common issues such as unstable training and mode collapse in GAN. Moreover, its generation process is interpretable, as it is based on denoising diffusion to generate images, enabling a better understanding of the image generation process <ref type="bibr" target="#b56">[57]</ref>.</p><p>Therefore, we propose a Denoising Diffusion image Fusion Model (DDFM), as shown in Fig. <ref type="figure" target="#fig_0">1c</ref>. We formulate the conditional generation task as a DDPM-based posterior sampling model, which can be further decomposed into an unconditional generation diffusion problem and a maximum likelihood estimation problem. The former satisfies natural image prior while the latter is inferred to restrict the similarity with source images via likelihood rectification. Compared to discriminative approaches, modeling the natural image prior with DDPM enables better generation of details that are difficult to control by manually designed loss functions, resulting in visually perceptible images. As a generative method, DDFM achieves stable and controllable generation of fused images without discriminator, by applying likelihood rectification to the DDPM output.</p><p>Our contributions are organized in three aspects:</p><p>• We introduce a DDPM-based posterior sampling model for MMIF, consisting of an unconditional generation module and a conditional likelihood rectification module. The sampling of fused images is achieved solely by a pre-trained DDPM without fine-tuning. • In likelihood rectification, since obtaining the likelihood explicitly is not feasible, we formulate the optimization loss as a probability inference problem involving latent variables, which can be solved by the EM algorithm. Then the solution is integrated into the DDPM loop to complete conditional image generation. • Extensive evaluation of IVF and MIF tasks shows that DDFM consistently delivers favorable fusion results, effectively preserving both the structure and detail information from the source images, while also satisfying visual fidelity requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Score-based diffusion models</head><p>Score SDE formulation. Diffusion models aim to generate samples by reversing a predefined forward process that converts a clean sample x 0 to almost Gaussian signal x T by gradually adding noise. This forward process can be described by an Itô Stochastic Differential Equation (SDE) <ref type="bibr" target="#b48">[49]</ref>:</p><formula xml:id="formula_0">dx = - β(t) 2 x t dt + β(t)dw,<label>(1)</label></formula><p>where dw is standard Wiener process and β(t) is predefined noise schedule that favors the variance-preserving SDE <ref type="bibr" target="#b48">[49]</ref>. This forward process can be reversed in time and still in the form of SDE <ref type="bibr" target="#b0">[1]</ref>:</p><formula xml:id="formula_1">dx = -β(t) 2 x t -β(t)∇ xt log p t (x t ) dt + β(t)dw, (2)</formula><p>where dw corresponds to the standard Wiener process running backward and the only unknown part ∇ xt log p t (x t ) can be modeled as the so-called score function s θ (x t , t) with denoising score matching methods, and this score function can be trained with the following objective <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b47">48]</ref>:</p><formula xml:id="formula_2">E t E x0 E xt|x0 ∥s θ (x t , t) -∇ xt log p 0t (x t |x 0 )∥ 2 2 , (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where t is uniformly sampled over [0, T ] and the data pair (x 0 , x t ) ∼ p 0 (x)p 0t (x t |x 0 ). Sampling with diffusion models. Specifically, an unconditional diffusion generation process starts with a random noise vector x T ∼ N (0, I) and updates according to the discretization of Eq. ( <ref type="formula">2</ref>). Alternatively, we can understand the sampling process in the DDIM fashion <ref type="bibr" target="#b45">[46]</ref>, where the score function can also be considered to be a denoiser and predict the denoised x0|t from any state x t at iteration t:</p><formula xml:id="formula_4">x0|t = 1 √ ᾱt (x t + (1 -ᾱt )s θ (x t , t)),<label>(4)</label></formula><p>and x0|t denotes the estimation of x 0 given x t . We use the same notation α t = 1 -β t and ᾱt = t s=1 α s following Ho et al. <ref type="bibr" target="#b12">[13]</ref>. With this predicted x0|t and the current state x t , x t-1 is updated from</p><formula xml:id="formula_5">x t-1 = √ α t (1 -ᾱt-1 ) 1 -ᾱt x t + √ ᾱt-1 β t 1 -ᾱt x0|t + σt z,<label>(5)</label></formula><p>where z ∼ N (0, I) and σ2 t is the variance which is usually set to 0. This sampled x t-1 is then fed into the next sampling iteration until the final image x 0 is generated. Further details about this sampling process can be found in the supplementary material or the original paper <ref type="bibr" target="#b45">[46]</ref>. Diffusion models applications. Recently, diffusion models have been improved to generate images with better quality than previous generative models like GANs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref>. Moreover, diffusion models can be treated as a powerful generative prior and be applied to numerous conditional generation tasks. One representative work with diffusion models is stable diffusion which can generate images according to given text prompts <ref type="bibr" target="#b43">[44]</ref>. Diffusion models are also applied to many low-level vision tasks. For instance, DDRM <ref type="bibr" target="#b18">[19]</ref> performs diffusion sampling in the spectral space of degradation operator A to reconstruct the missing information in the observation y. DDNM <ref type="bibr" target="#b63">[64]</ref> shares a similar idea with DDRM by refining the null-space of the operator A iteratively for image restoration tasks. DPS <ref type="bibr" target="#b2">[3]</ref> endorses Laplacian approximation to calculate the gradient of log-likelihood for posterior sampling and it is capable of many noisy non-linear inverse problems. In ΠGDM <ref type="bibr" target="#b46">[47]</ref>, the authors employ few approximations to make the log-likelihood term tractable and hence make it able to solve inverse problems with even non-differentiable measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-modal image fusion</head><p>The deep learning-based multi-modality image fusion algorithms achieve effective feature extraction and information fusion through the powerful fitting ability of neural networks. Fusion algorithms are primarily divided into two branches: generative methods and discriminative methods. For generative methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref>, particularly the GAN family, adversarial training <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref> is employed to generate fusion images following the same distribution as the source images. For discriminative methods, auto encoder-based models <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b64">65]</ref> use encoders and decoders to extract features and fuse them on a high-dimensional manifold. Algorithm unfolding models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b23">24]</ref> combine traditional optimization methods and neural networks, balancing efficiency and interpretability. Unified models <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b17">18]</ref> avoid the problem of lacking training data and ground truth for specific tasks. Recently, CDDFuse <ref type="bibr" target="#b68">[69]</ref> addresses cross-modality feature modeling and extracts modality-specific/shared features through a dual-branch Transformer-CNN architecture and correlation-driven loss, achieving promising fusion results in multiple fusion tasks. On the other hand, fusion methods have been combined with pattern recognition tasks such as semantic segmentation <ref type="bibr" target="#b49">[50]</ref> and object detection <ref type="bibr" target="#b25">[26]</ref> to explore the interactions with downstream tasks. Specifically, TarDAL <ref type="bibr" target="#b25">[26]</ref> demonstrates an obvious advantage in dealing with challenge scenarios with high efficiency. Selfsupervised learning <ref type="bibr" target="#b24">[25]</ref> is employed to train fusion networks without paired images. Moreover, the pre-processing registration module <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b60">61]</ref> can enhance the robustness for unregistered input images. Benefiting from the multimodality data, MSIS <ref type="bibr" target="#b16">[17]</ref> achieves realizable and outstanding stitching results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Comparison with existing approaches</head><p>The methods most relevant to our model are optimizationbased methods and GAN-based generative methods. Conventional optimization-based methods are often limited by manually designed loss functions, which may not be flexible enough to capture all relevant aspects and are sensitive to changes in the data distribution. While incorporating natural image priors can provide extra knowledge that cannot be modeled by the generation loss function alone. Then, in contrast to GAN-based generative methods, where unstable training and pattern collapse may occur, our DDFM achieves more stable and controllable fusion by rectifying the generation process towards source images and performing likelihood-based refinement in each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first present a novel approach for obtaining a fusion image by leveraging DDPM posterior sampling. Then, starting from the well-established loss function for image fusion, we derive a likelihood rectification approach for the unconditional DDPM sampling. Finally, we propose the DDFM algorithm, which embeds the solution of the hierarchical Bayesian inference into the diffusion sampling. In addition, the rationality of the proposed algorithm will be demonstrated. For brevity, we omit the derivations of some equations and refer interested readers to the supplementary material. It is worth noting that we use IVF as a case to illustrate our DDFM, and MIF can be carried out analogously to IVF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fusing images via diffusion posterior sampling</head><p>We first give the notation of the model formulation. Infrared, visible and fused images are denoted as i ∈ R HW , v ∈ R 3HW and f ∈ R 3HW , respectively.</p><p>We expect that the distribution of f given i and v, i.e., p (f |i,v), can be modeled, thus f can be obtained by sampling from the posterior distribution. Inspired by Eq. ( <ref type="formula">2</ref>), we can express the reverse SDE of diffusion process as:</p><formula xml:id="formula_6">df = -β(t) 2 f -β(t)∇ f t log p t (f t |i,v) dt+ β(t)dw, (6)</formula><p>and the score function, i.e., ∇ f t log p t (f t |i,v), can be calculated by:</p><formula xml:id="formula_7">∇ f t log p t (f t |i,v) = ∇ f t log p t (f t )+∇ f t log p t (i,v|f t ) ≈ ∇ f t log p t (f t )+∇ f t log p t (i,v| f 0|t ) (7)</formula><p>where f 0|t is the estimation of f 0 given f t from the unconditional DDPM. The equality comes from Bayes' theorem, and the approximate equation is proved in <ref type="bibr" target="#b2">[3]</ref>.</p><p>In Eq. ( <ref type="formula">7</ref>), the first term represents the score function of unconditional diffusion sampling, which can be readily derived by the pre-trained DDPM. In the next section, we explicate the methodology for obtaining ∇ f t log p t (i,v| f 0|t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Likelihood rectification for image fusion</head><p>Unlike the traditional image degradation inverse problem y = A(x) + n where x is the ground truth image, y is measurement and A(•) is known, we can explicitly obtain its posterior distribution. However, it is not possible to explicitly express p t (i,v|f t ) or p t (i,v| f 0|t ) in image fusion. To address this, we start from the loss function and establish the relationship between the optimization loss function ℓ(i, v, f 0|t ) and the likelihood p t (i,v| f 0|t ) of a probabilistic model. For brevity, f 0|t is abbreviated as f in Secs. 3.2.1 and 3.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Formulation of the likelihood model</head><p>We first give a commonly-used loss function <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b68">69]</ref> for the image fusion task:</p><formula xml:id="formula_8">min f ∥f -i∥ 1 + ϕ∥f -v∥ 1 .<label>(8)</label></formula><p>Then simple variable substitution x = f -v and y = i-v are implemented, and we get</p><formula xml:id="formula_9">min x ∥y -x∥ 1 + ϕ∥x∥ 1 . (<label>9</label></formula><formula xml:id="formula_10">)</formula><p>Since y is known and x is unknown, this ℓ 1 -norm optimization equation corresponds to the regression model: y = kx + ϵ with k fixed to 1. According to the relationship between regularization term and noise prior distribution, ϵ should be a Laplacian noise and x is governed by the Laplacian distribution. Thus, in Bayesian fashion, we have:</p><formula xml:id="formula_11">p(x) = LAP (x; 0, ρ) = i,j 1 2ρ exp - |xij| ρ , p(y|x) = LAP (y; x, γ) = i,j 1 2γ exp - |yij -xij| γ ,<label>(10)</label></formula><p>where LAP(•) is the Laplacian distribution. ρ and γ are scale parameters of p(x) and p(y|x), respectively. In order to prevent ℓ 1 -norm optimization in Eq. ( <ref type="formula" target="#formula_9">9</ref>) and inspired by <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b70">71]</ref>, we give the Proposition 1:</p><p>Proposition 1. For a random variable (RV) ξ which obeys a Laplace distribution, it can be regarded as the coupling of a normally distributed RV and an exponentially distributed RV, which in formula:</p><formula xml:id="formula_12">LAP(ξ; µ, b/2) = ∞ 0 N (ξ; µ, a)EX P(a; b)da. (11) Remark 1.</formula><p>In Proposition 1, we transform ℓ 1 -norm optimization into an ℓ 2 -norm optimization with latent variables, avoiding potential non-differentiable points in ℓ 1 -norm.</p><p>Therefore, p(x) and p(y|x) in Eq. ( <ref type="formula" target="#formula_11">10</ref>) can be rewritten as the following hierarchical Bayesian framework:</p><formula xml:id="formula_13">       y ij |x ij , m ij ∼ N (y ij ; x ij , m ij ) m ij ∼ EX P (m ij ; γ) x ij |n ij ∼ N (x ij ; 0, n ij ) n ij ∼ EX P (n ij ; ρ)<label>(12)</label></formula><p>where i = 1, . . . , H and j = 1, . . . , W . Through the above probabilistic analysis, the optimization problem in Eq. ( <ref type="formula" target="#formula_9">9</ref>) can be transformed into a maximum likelihood inference problem.</p><p>In addition, following <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b49">50]</ref>, the total variation penalty item r(x) = ∥∇x∥ 2  2 can be also added to make the fusion image f better preserve the texture information from v, where ∇ denotes the gradient operator. Ultimately, the loglikelihood function of the probabilistic inference issue is:</p><formula xml:id="formula_14">ℓ(x) = log p(x, y) -r(x) = - i,j (x ij -y ij ) 2 2m ij + x 2 ij 2n ij - ψ 2 ∥∇x∥ 2 2 ,<label>(13)</label></formula><p>and probabilistic graph of this hierarchical Bayesian model is in Fig. <ref type="figure" target="#fig_0">1b</ref>. Notably, in this way, we transform the optimization problem Eq. ( <ref type="formula" target="#formula_8">8</ref>) into a maximum likelihood problem of a probability model Eq. <ref type="bibr" target="#b12">(13)</ref>. Additionally, unlike traditional optimization methods that require manually specified tuning coefficients ϕ in Eq. ( <ref type="formula" target="#formula_8">8</ref>), ϕ in our model can be adaptively updated by inferring the latent variables, enabling the model to better fit different data distributions. The validity of this design has also been verified in ablation experiments in Sec. 4.3. We will then explore how to infer it in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Inference the likelihood model via EM algorithm</head><p>In order to solve the maximum log-likelihood problem in Eq. ( <ref type="formula" target="#formula_14">13</ref>), which can be regarded as an optimization problem with latent variables, we use the Expectation Maximization (EM) algorithm to obtain the optimal x. In E-step, it calculates the expectation of log-likelihood function with respect to p a, b|x (t) , y , i.e., the so-called Q-function:</p><formula xml:id="formula_15">Q x|x (t) = E a,b|x (t) ,y [ℓ(x)].<label>(14)</label></formula><p>Then in M-step, the optimal x is obtained by</p><formula xml:id="formula_16">x (t+1) = arg max x Q x|x (t) .<label>(15)</label></formula><p>Next, we will show the implementation detail in each step. E-step. Proposition 2 gives the calculation results for the conditional expectation of latent variables, and then gets the derivation of Q-function.</p><p>Proposition 2. The conditional expectation of the latent variable 1/m ij and 1/n ij in Eq. ( <ref type="formula" target="#formula_14">13</ref>) are:</p><formula xml:id="formula_17">E m ij |x (t) ij ,y ij 1 mij = 2(yij -x (t) ij ) 2 γ , E n ij |x (t) ij 1 nij = 2[x (t) ij ] 2 ρ .<label>(16)</label></formula><p>Proof. For convenience, we set mij ≡ 1/m ij and ñij ≡ 1/n ij . From Eq. ( <ref type="formula" target="#formula_13">12</ref>) we know that m ij ∼ EX P (m ij ; γ) = Γ(m ij ; 1, γ). Thus, mij ∼ IG (1, γ), where Γ(•, •) and IG(•, •) are the gamma distribution and inverse gamma distribution, respectively. Then we can get the posterior of mij by Bayes' theorem:</p><formula xml:id="formula_18">log p ( mij|yij, xij) = log p (yij|xij, mij) + log p ( mij) = - 3 2 log mij - mij (yij -xij) 2 2 - 1 γ mij + constant.</formula><p>(17) Subsequently, we have</p><formula xml:id="formula_19">p ( mij |y ij , x ij ) = IN mij ; 2 (y ij -x ij ) 2 /γ, 2/γ , (<label>18</label></formula><formula xml:id="formula_20">)</formula><p>where IN (•, •) is the inverse Gaussian distribution. For the posterior of ñij , it can be obtain similar to Eq. ( <ref type="formula">17</ref>):</p><formula xml:id="formula_21">log p (ñij|xij) = log p (xij|nij) + log p (ñij) = - 3 2 log ñij - ñijx 2 ij 2 - 1 ρñij + constant,<label>(19)</label></formula><p>and therefore</p><formula xml:id="formula_22">p (ñ ij |x ij ) = IN ñij ; 2x 2 ij /ρ, 2/ρ .<label>(20)</label></formula><p>Finally, the conditional expectation of 1/m ij and 1/n ij are the mean parameters of the corresponding inverse Gaussian distribution Eqs. ( <ref type="formula" target="#formula_19">18</ref>) and ( <ref type="formula" target="#formula_22">20</ref>), respectively. ■ Remark 2. The conditional expectation computed by Proposition 2 will be used to derive the Q-function below.</p><p>Afterwards, the Q-function Eq. ( <ref type="formula" target="#formula_15">14</ref>) is derived as:</p><formula xml:id="formula_23">Q = - i,j mij 2 (xij -yij) 2 + nij 2 x 2 ij - ψ 2 ∥∇x∥ 2 2 ∝ -∥m ⊙ (x -y)∥ 2 2 -∥n ⊙ x∥ 2 2 -ψ∥∇x∥ 2 2 ,<label>(21)</label></formula><p>where mij and nij represent <ref type="formula" target="#formula_17">16</ref>), respectively. ⊙ is the elementwise multiplication. m and n are matrices with each element being √ mij and √ nij , respectively.</p><formula xml:id="formula_24">E mij |x (t) ij ,yij [1/m ij ] and E nij |x (t) ij [1/n ij ] in Eq. (</formula><p>M-step. Here, we need to minimize the negative Q-function with respect to x. The half-quadratic splitting algorithm is employed to deal with this problem, i.e.,</p><formula xml:id="formula_25">min x,u,k ||m ⊙ (x -y)|| 2 2 + ||n ⊙ x|| 2 2 + ψ||u|| 2 2 , s.t. u = ∇k, k = x. (<label>22</label></formula><formula xml:id="formula_26">)</formula><p>It can be further cast into the following unconstraint optimization problem,</p><formula xml:id="formula_27">min x,u,k ||m ⊙ (x -y)|| 2 2 + ||n ⊙ x|| 2 2 + ψ||u|| 2 2 + η 2 ||u -∇k|| 2 2 + ||k -x|| 2 2 .<label>(23)</label></formula><p>The unknown variables k, u, x can be solved iteratively in the coordinate descent fashion.</p><p>Update k: It is a deconvolution issue,</p><formula xml:id="formula_28">min k L k = ||k -x|| 2 2 + ||u -∇k|| 2 2 . (<label>24</label></formula><formula xml:id="formula_29">)</formula><p>It can be efficiently solved by the fast Fourier transform (fft) and inverse fft (ifft) operators, and the solution of k is</p><formula xml:id="formula_30">k = ifft fft(x) + fft(∇) ⊙ fft(u) 1 + fft(∇) ⊙ fft(∇) , (<label>25</label></formula><formula xml:id="formula_31">)</formula><p>where • is the complex conjugation. Update u: It is an ℓ 2 -norm penalized regression issue,</p><formula xml:id="formula_32">min u L u = ψ||u|| 2 2 + η 2 ||u -∇k|| 2 2 . (<label>26</label></formula><formula xml:id="formula_33">)</formula><p>The solution of u is</p><formula xml:id="formula_34">u = η 2ψ + η ∇k. (<label>27</label></formula><formula xml:id="formula_35">)</formula><p>Update x: It is a least squares issue,</p><formula xml:id="formula_36">min x L x = ||m⊙(x-y)|| 2 2 +||n⊙x|| 2 2 + η 2 ||k-x|| 2 2 . (<label>28</label></formula><formula xml:id="formula_37">)</formula><p>The solution of x is</p><formula xml:id="formula_38">x = (2m 2 ⊙ y + ηk) ⊘ (2m 2 + 2n 2 + η),<label>(29)</label></formula><p>where ⊘ denotes the element-wise division, and final estima-</p><formula xml:id="formula_39">tion of f is f = x + v.<label>(30)</label></formula><p>Additionally, hyper-parameter γ and ρ in Eq. ( <ref type="formula" target="#formula_11">10</ref>) can be also updated after the sampling from x (Eq. ( <ref type="formula" target="#formula_38">29</ref>)) by</p><formula xml:id="formula_40">γ = 1 hw i,j E[m ij ], ρ = 1 hw i,j E[n ij ].<label>(31)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">DDFM</head><p>Overview. In Sec. 3.2, we present a methodology for obtaining a hierarchical Bayesian model from existing loss function and perform the model inference via the EM algorithm. In this section, we present our DDFM, where the inference solution and diffusion sampling are integrated within the same iterative framework for generating f 0 given i and v. The algorithm is illustrated in Algorithm 1 and Fig. <ref type="figure">3</ref>.</p><p>There are two modules in DDFM, the unconditional diffusion sampling (UDS) module and the likelihood rectification, or say, EM module. The UDS module is utilized to provide natural image priors, which improve the visual plausibility of the fused image. The EM module, on the other hand, is responsible for rectifying the output of UDS module via likelihood to preserve more information from the source images. Unconditional diffusion sampling module. In Sec. 2.1, we briefly introduce diffusion sampling. In Algorithm 1, UDS (in grey) is partitioned into two components, where the first part estimates f 0|t using f t , and the second part estimates f t-1 using both f t and f 0|t . From the perspective of scorebased DDPM in Eq. ( <ref type="formula">7</ref>), a pre-trained DDPM can directly output the current ∇ f t log p t (f t ), while ∇ f t log p t (i,v| f 0|t ) can be obtain by the EM module. EM module. The role of the EM module is to update f 0|t ⇒ f 0|t . In Algorithm 1 and Fig. <ref type="figure">3</ref>, the EM algorithm (in blue and yellow) is inserted in UDS (in grey). The preliminary estimate f 0|t produced by DDPM sampling (line 5) is utilized as the initial input for the EM algorithm to obtain f 0|t (line 6-13), which is an estimation of the fused image subjected to likelihood rectification. In other words, EM module rectify f 0|t to f 0|t to meet the likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Why does one-step EM work?</head><p>The main difference between our DDFM and conventional EM algorithm lies in that the traditional method requires numerous iterations to obtain the optimal x, i.e., the operation from line 6-13 in Algorithm 1 needs to be looped many times. However, our DDFM only requires one step of the EM algorithm iteration, which is embedded into the DDPM framework to accomplish sampling. In the following, we give Proposition 3 to demonstrate its rationality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 DDFM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>Infrared image i, Visible image v, T , {σ t }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T t=1</head><p>Output:</p><formula xml:id="formula_41">Fused image f 0 . 1: f T ∼ N (0, I) 2: for t = T -1 to 0 do 3: % DDPM Part 1: Obtain f 0|t 4: ŝ ← s θ (f t , t) 5: f 0|t ← 1 √ ᾱt (f t + (1 -ᾱt ) ŝ) 6:</formula><p>% E-step: Update latent variables</p><formula xml:id="formula_42">7: x0 = f 0|t -v, y = i -v 8:</formula><p>Evaluate expectations by Eq. ( <ref type="formula" target="#formula_17">16</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Update hyper-parameters γ, ρ by Eq. ( <ref type="formula" target="#formula_40">31</ref>). k ← arg min k L k (x 0 , u) (Eq. ( <ref type="formula" target="#formula_30">25</ref>))</p><p>12:</p><p>u ← arg min u L u (∇k) (Eq. ( <ref type="formula" target="#formula_34">27</ref>))</p><p>13: <ref type="formula" target="#formula_38">29</ref>)&amp;( <ref type="formula" target="#formula_39">30</ref>))</p><formula xml:id="formula_43">f 0|t ← arg min x L x (k, u, x0 ) + v (Eq. (</formula><p>% DDPM Part 2: Estimate f t-1</p><p>15:</p><p>z ∼ N (0, I)</p><p>16: </p><formula xml:id="formula_45">f t-1 ← √ αt(1-ᾱt-1) 1-ᾱt f t + √ ᾱt-1βt 1-ᾱt f 0|t + σt</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla DDPM</head><p>Figure <ref type="figure">3</ref>: Computational graph of our DDFM in one iteration. Different from the vanilla DDPM, likelihood rectification is completed via the EM algorithm, i.e., the update from f 0|t ⇒ f 0|t . Proposition 3. One-step unconditional diffusion sampling combined with one-step EM iteration is equivalent to onestep conditional diffusion sampling.</p><p>Proof. The estimation of f 0|t in conditional diffusion sampling, refer to Eq. ( <ref type="formula" target="#formula_4">4</ref>), could be expressed as:</p><formula xml:id="formula_46">f0|t (f t , i, v) = 1 √ ᾱt [f t + (1 -ᾱt) s θ (f t , i, v)] (32a) = 1 √ ᾱt {f t +(1-ᾱt)[sθ(f t )+∇ f t log pt(i,v|f t )]} (32b) ≈ f0|t (f t ) + 1 -ᾱt √ ᾱt ∇ f t log pt i, v| f 0|t (32c) = f0|t (f t ) -ζt∇x 0 Lx (i, v, x0) .<label>(32d)</label></formula><p>Eqs. (32a) to (32c) are respectively based on the definition of Score-based DDPM, Bayes' theorem, and proof in <ref type="bibr" target="#b2">[3]</ref>. For Eq. (32d), although optimization Eq. ( <ref type="formula" target="#formula_36">28</ref>) has a closed-form solution (Eq. ( <ref type="formula" target="#formula_38">29</ref>)), it can also be solved by gradient descent:</p><formula xml:id="formula_47">x0 = x0 + ∇ x0 L x (k, u, x0 ) = x0 + ∇ x0 L x (i, v, x0 )<label>(33)</label></formula><p>where the second equation holds true because as the input for updating x0 (Eq. ( <ref type="formula" target="#formula_38">29</ref>)), k and u are functions of i and v. ζ t in Eq. (32d) can be regraded as the update step size. Hence, conditional sampling f0|t (f t , i, v) can be split as an unconditional diffusion sampling f0|t (f t ) and onestep EM iteration ∇ x0 L x (i, v, x0 ), corresponding to UDS module (part 1) and EM module, respectively. ■ Remark 3. Proposition 3 demonstrates the theoretical explanation for the rationality of inserting the EM module into the UDS module and explains why the EM module only involves one iteration of the Bayesian inference algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Infrared and visible image fusion</head><p>In this section, we elaborate on numerous experiments for IVF task to demonstrate the superiority of our method. More related experiments are placed in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Datasets and pre-trained model. Following the protocol in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25]</ref>, IVF experiments are conducted on the four test datasets, i.e., TNO <ref type="bibr" target="#b51">[52]</ref>, RoadScene <ref type="bibr" target="#b58">[59]</ref>, MSRS <ref type="bibr" target="#b50">[51]</ref>, and M 3 FD <ref type="bibr" target="#b25">[26]</ref>. Note that there is no training dataset due to that we do not need any fine-tuning for specific tasks but directly use the pre-trained DDPM model. We choose the pre-trained model proposed by <ref type="bibr" target="#b4">[5]</ref>, which is trained on ImageNet <ref type="bibr" target="#b44">[45]</ref>.</p><p>Metrics. We employ six metrics including entropy (EN), standard deviation (SD), mutual information (MI), visual information fidelity (VIF), Q AB/F , and structural similarity index measure (SSIM) in the quantitative experiments to comprehensively evaluate the fused effect. The detail of metrics is in <ref type="bibr" target="#b31">[32]</ref>.</p><p>Implement details. We use a machine with one NVIDIA GeForce RTX 3090 GPU for fusion image generation. All input images are normalized to [-1, 1]. ψ and η in Eq. ( <ref type="formula" target="#formula_27">23</ref>) are set to 0.5 and 0.1, respectively. Please refer to the supplementary material for selecting ψ and η via grid search. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with SOTA methods</head><p>In this section, we compare our DDFM with the state-ofthe-art methods, including the GAN-based generative methods group: FusionGAN <ref type="bibr" target="#b33">[34]</ref>, GANMcC <ref type="bibr" target="#b34">[35]</ref>, TarDAL <ref type="bibr" target="#b25">[26]</ref>, and UMFusion <ref type="bibr" target="#b53">[54]</ref>; and the discriminative methods group: U2Fusion <ref type="bibr" target="#b57">[58]</ref>, RFNet <ref type="bibr" target="#b59">[60]</ref>, and DeFusion <ref type="bibr" target="#b24">[25]</ref>.</p><p>Qualitative comparison. We show the comparison of fusion results in Figs. <ref type="figure">4</ref> and<ref type="figure">5</ref>. Our approach effectively combines thermal radiation information from infrared images with detailed texture information from visible images. As a result, objects located in dimly-lit environments are conspicuously accentuated, enabling easy distinguishing of foreground objects from the background. Moreover, previously indistinct background features due to low illumination now possess clearly defined edges and abundant contour information, enhancing our ability to comprehend the scene.</p><p>Quantitative comparison. Subsequently, six metrics previously mentioned are utilized to quantitatively compare the fusion outcomes, as presented in Tab. 1. Our method demonstrates remarkable performance across almost all metrics, affirming its suitability for different lighting and object categories. Notably, the outstanding values for MI, VIF and Qabf across all datasets signify its ability to generate images that adhere to human visual perception while preserving the integrity of the source image information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation studies</head><p>Numerous ablation experiments are conducted to confirm the soundness of our various modules. The above six metrics are utilized to assess the fusion performance for the experimental groups, and results on the Roadscene testset are displayed in Tab. 2.</p><p>Unconditional diffusion sampling module. We first verify the effectiveness of DDPM. In Exp. I, we eliminate the denoising diffusion generative framework, thus only the EM algorithm is employed to solve the optimization Eq. ( <ref type="formula" target="#formula_8">8</ref>) and obtain the fusion image. In fairness, we keep the total iteration number consistent with DDFM.</p><p>EM module. Next, we verify the components in the EM module. In Exp. II, we removed the total variation penalty item r(x) in Eq. <ref type="bibr" target="#b12">(13)</ref>. Then, we remove the Bayesian inference model. As mentioned earlier, ϕ in Eq. ( <ref type="formula" target="#formula_8">8</ref>) can be automatically inferred in the hierarchical Bayesian model. Therefore, we manually set ϕ to 0.1 (Exp. III) and 1 (Exp. IV), and used the ADMM algorithm to infer the model.</p><p>In conclusion, the results presented in Tab. 2 demonstrate that none of the experimental groups is able to achieve fusion results comparable to our DDFM, further emphasizing the effectiveness and rationality of our approach.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Medical image fusion</head><p>In this section, MIF experiments are carried out to verify the effectiveness of our method. Setup. We choose 50 pairs of medical images from the Harvard Medical Image Dataset <ref type="bibr" target="#b7">[8]</ref> for the MIF experiments, including image pairs of MRI-CT, MRI-PET and MRI-SPECT. The generation strategy and evaluation metrics for the MIF task are identical to those used for IVF. Comparison with SOTA methods. Qualitative and quantitative results are shown in Fig. <ref type="figure" target="#fig_5">6</ref> and Tab. 3. It is evident that DDFM retains intricate textures while emphasizing structural information, leading to remarkable performance across both visual and almost all numerical metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose DDFM, a generative image fusion algorithm based on the denoising diffusion probabilistic model (DDPM). The generation problem is split into an unconditional DDPM to leverage image generative priors and a maximum likelihood sub-problem to preserve cross-modality information from source images. We model the latter using a hierarchical Bayesian approach and its solution based on EM algorithm can be integrated into unconditional DDPM to accomplish conditional image fusion. Experiments on infrared-visible and medical image fusion demonstrate that DDFM achieves promising fusion results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Existing GAN-based fusion method workflow. (b) Graph of the hierarchical Bayesian model in likelihood rectification, linking the MMIF loss and our statistical inference model. (c) Our DDFM workflow: the unconditional diffusion sampling (UDS) module generates f t , while the likelihood rectification module, based on (b), rectifies UDS output with source image information.EN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of results on MSRS [51] and Road-Scene [59] in Tab. 1. Hexagons formed by lines of different colors represent the values of different methods across six metrics. Our DDFM (marked in yellow) outperforms all other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>10 :%</head><label>10</label><figDesc>M-step: Obtain f 0|t via Likelihood Rectification 11:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Visual comparison of "01462" from M 3 FD IVF dataset.</figDesc><graphic coords="7,248.31,171.08,99.37,74.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visual comparison for MIF task.</figDesc><graphic coords="9,164.55,180.75,88.45,88.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The quantitative results of IVF task, with the best and second-best values in boldface and underline, respectively.Dataset: MSRS Fusion Dataset<ref type="bibr" target="#b50">[51]</ref> Dataset:M 3 FD Fusion Dataset [26] EN ↑ SD ↑ MI ↑ VIF ↑ Qabf ↑ SSIM ↑ EN ↑ SD ↑ MI ↑ VIF ↑ Qabf ↑ SSIM ↑ SD ↑ MI ↑ VIF ↑ Qabf ↑ SSIM ↑ EN ↑ SD ↑ MI ↑ VIF ↑ Qabf ↑ SSIM ↑</figDesc><table><row><cell cols="2">FGAN [34] 5.60 17.81 1.29</cell><cell>0.40</cell><cell>0.13</cell><cell>0.47</cell><cell cols="2">FGAN [34] 6.51 28.14 2.07</cell><cell>0.44</cell><cell>0.30</cell><cell>0.75</cell></row><row><cell cols="2">GMcC [35] 6.20 25.95 1.79</cell><cell>0.57</cell><cell>0.28</cell><cell>0.74</cell><cell cols="2">GMcC [35] 6.68 32.23 2.01</cell><cell>0.58</cell><cell>0.36</cell><cell>0.93</cell></row><row><cell>U2F [58]</cell><cell>6.06 29.80 1.55</cell><cell>0.59</cell><cell>0.46</cell><cell>0.76</cell><cell>U2F [58]</cell><cell>6.84 34.05 1.95</cell><cell>0.73</cell><cell>0.49</cell><cell>0.98</cell></row><row><cell>RFN [60]</cell><cell>6.07 26.82 1.36</cell><cell>0.54</cell><cell>0.46</cell><cell>0.81</cell><cell>RFN [60]</cell><cell>6.67 31.04 1.71</cell><cell>0.67</cell><cell>0.44</cell><cell>0.91</cell></row><row><cell>TarD [26]</cell><cell>5.39 22.74 1.32</cell><cell>0.38</cell><cell>0.16</cell><cell>0.45</cell><cell>TarD [26]</cell><cell>6.67 38.83 2.38</cell><cell>0.54</cell><cell>0.29</cell><cell>0.87</cell></row><row><cell>DeF [25]</cell><cell>6.85 40.20 2.25</cell><cell>0.74</cell><cell>0.56</cell><cell>0.92</cell><cell>DeF [25]</cell><cell>6.79 36.39 2.32</cell><cell>0.65</cell><cell>0.44</cell><cell>0.94</cell></row><row><cell>UMF [54]</cell><cell>5.98 23.56 1.38</cell><cell>0.47</cell><cell>0.29</cell><cell>0.58</cell><cell>UMF [54]</cell><cell>6.73 32.46 2.23</cell><cell>0.66</cell><cell>0.40</cell><cell>0.97</cell></row><row><cell>Ours</cell><cell>6.88 40.75 2.35</cell><cell>0.81</cell><cell>0.58</cell><cell>0.94</cell><cell>Ours</cell><cell>6.86 38.95 2.52</cell><cell>0.80</cell><cell>0.49</cell><cell>0.95</cell></row><row><cell></cell><cell cols="3">Dataset: RoadScene Fusion Dataset [59]</cell><cell></cell><cell></cell><cell cols="3">Dataset: TNO Fusion Dataset [52]</cell><cell></cell></row><row><cell cols="2">EN ↑ FGAN [34] 7.12 40.13 1.90</cell><cell>0.36</cell><cell>0.26</cell><cell>0.61</cell><cell cols="2">FGAN [34] 6.74 34.41 1.78</cell><cell>0.42</cell><cell>0.25</cell><cell>0.66</cell></row><row><cell cols="2">GMcC [35] 7.26 43.44 1.86</cell><cell>0.49</cell><cell>0.34</cell><cell>0.81</cell><cell cols="2">GMcC [35] 6.86 35.51 1.64</cell><cell>0.53</cell><cell>0.28</cell><cell>0.83</cell></row><row><cell>U2F [58]</cell><cell>7.16 38.97 1.83</cell><cell>0.54</cell><cell>0.49</cell><cell>0.96</cell><cell>U2F [58]</cell><cell>7.02 38.52 1.41</cell><cell>0.63</cell><cell>0.43</cell><cell>0.93</cell></row><row><cell>RFN [60]</cell><cell>7.30 43.37 1.64</cell><cell>0.49</cell><cell>0.43</cell><cell>0.88</cell><cell>RFN [60]</cell><cell>6.93 34.95 1.21</cell><cell>0.55</cell><cell>0.37</cell><cell>0.87</cell></row><row><cell>TarD [26]</cell><cell>7.31 47.24 2.15</cell><cell>0.53</cell><cell>0.41</cell><cell>0.86</cell><cell>TarD [26]</cell><cell>7.02 49.89 1.89</cell><cell>0.54</cell><cell>0.28</cell><cell>0.83</cell></row><row><cell>DeF [25]</cell><cell>7.31 44.91 2.09</cell><cell>0.55</cell><cell>0.46</cell><cell>0.86</cell><cell>DeF [25]</cell><cell>7.06 40.70 2.04</cell><cell>0.64</cell><cell>0.43</cell><cell>0.92</cell></row><row><cell>UMF [54]</cell><cell>7.29 42.91 1.96</cell><cell>0.61</cell><cell>0.50</cell><cell>0.98</cell><cell>UMF [54]</cell><cell>6.83 36.56 1.66</cell><cell>0.65</cell><cell>0.42</cell><cell>0.94</cell></row><row><cell>Ours</cell><cell>7.41 52.61 2.35</cell><cell>0.75</cell><cell>0.65</cell><cell>0.98</cell><cell>Ours</cell><cell>7.06 51.42 2.21</cell><cell>0.81</cell><cell>0.49</cell><cell>0.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation experiment results. Bold indicates the best value.</figDesc><table><row><cell></cell><cell cols="2">Configurations</cell><cell></cell><cell>EN</cell><cell>SD</cell><cell>MI</cell><cell>VIF Qabf SSIM</cell></row><row><cell></cell><cell cols="2">DDPM r(x)</cell><cell>ϕ</cell><cell></cell><cell></cell></row><row><cell>I II III IV</cell><cell>√ √ √</cell><cell>√ √ √</cell><cell cols="4">\ \ 0.1 7.25 43.16 2.26 0.66 0.49 7.19 41.82 2.11 0.60 0.42 7.33 44.12 2.29 0.69 0.52 1 7.26 42.37 2.24 0.66 0.47</cell><cell>0.92 0.93 0.90 0.91</cell></row><row><cell>Ours</cell><cell>√</cell><cell>√</cell><cell>\</cell><cell cols="3">7.41 52.61 2.35 0.75 0.65</cell><cell>0.98</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The quantitative results of the MIF task, with the best and second-best values in boldface and underline, respectively.</figDesc><table><row><cell></cell><cell>53</cell><cell>0.39</cell><cell>0.18</cell><cell>0.23</cell></row><row><cell cols="2">GMcC [35] 4.18 42.49 1.74</cell><cell>0.50</cell><cell>0.42</cell><cell>0.35</cell></row><row><cell>U2F [58]</cell><cell>4.14 48.89 1.80</cell><cell>0.50</cell><cell>0.55</cell><cell>1.14</cell></row><row><cell>RFN [60]</cell><cell>4.75 40.81 1.62</cell><cell>0.43</cell><cell>0.56</cell><cell>0.40</cell></row><row><cell>TarD [26]</cell><cell>4.61 60.64 1.44</cell><cell>0.33</cell><cell>0.21</cell><cell>0.25</cell></row><row><cell>DeF [25]</cell><cell>4.21 61.65 1.85</cell><cell>0.62</cell><cell>0.59</cell><cell>1.40</cell></row><row><cell>UMF [54]</cell><cell>4.61 27.28 1.62</cell><cell>0.40</cell><cell>0.27</cell><cell>0.30</cell></row><row><cell>Ours</cell><cell>4.64 63.11 1.99</cell><cell>0.76</cell><cell>0.60</cell><cell>1.41</cell></row></table><note><p><p>Dataset: Harvard Medical Fusion Dataset</p>[8] EN ↑ SD ↑ MI ↑ VIF ↑ Qabf ↑ SSIM ↑ FGAN [34] 4.05 29.20 1.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work has been supported by the <rs type="funder">National Key Research and Development Program of China</rs> under grant <rs type="grantNumber">2018AAA0102201</rs>, the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">61976174</rs> and <rs type="grantNumber">12201497</rs>, the <rs type="funder">Macao Science and Technology Development Fund</rs> under Grant <rs type="grantNumber">061/2020/A2</rs>, <rs type="funder">Shaanxi Fundamental Science Research Project for Mathematics and Physics</rs> under Grant <rs type="grantNumber">22JSQ033</rs>, the <rs type="funder">Fundamental Research Funds for the Central Universities</rs> under Grant <rs type="grantNumber">D5000220060</rs>, and partly supported by the <rs type="funder">Alexander von Humboldt Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HrDRGJk">
					<idno type="grant-number">2018AAA0102201</idno>
				</org>
				<org type="funding" xml:id="_V2rUkPc">
					<idno type="grant-number">61976174</idno>
				</org>
				<org type="funding" xml:id="_HmVYrFy">
					<idno type="grant-number">12201497</idno>
				</org>
				<org type="funding" xml:id="_j3fD8fa">
					<idno type="grant-number">061/2020/A2</idno>
				</org>
				<org type="funding" xml:id="_SjB7bPh">
					<idno type="grant-number">22JSQ033</idno>
				</org>
				<org type="funding" xml:id="_DV9NZXa">
					<idno type="grant-number">D5000220060</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reverse-time diffusion equation models</title>
		<author>
			<persName><forename type="first">Brian Do</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and their Applications</title>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="313" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno>CoRR, abs/2004.10934</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diffusion posterior sampling for general noisy inverse problems</title>
		<author>
			<persName><forename type="first">Hyungjin</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeongsol</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">T</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">Louis</forename><surname>Klasky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006">2023. 3, 4, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for multi-modal image restoration and fusion</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pier</forename><surname>Luigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragotti</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3333" to="3348" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-modal convolutional dictionary learning</title>
		<author>
			<persName><forename type="first">Fangyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pier</forename><forename type="middle">Luigi</forename><surname>Dragotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<ptr target="http://www.med.harvard.edu/AANLIB/home.html.9" />
		<title level="m">Harvard Medical website</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Chunming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoxia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangpeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longxiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.07829</idno>
		<title level="m">Hqg-net: Unpaired medical image enhancement with high-quality guidance</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Camouflaged object detection with feature decomposition and edge reconstruction</title>
		<author>
			<persName><forename type="first">Chunming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yachao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longxiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Weaklysupervised concealed object segmentation with sam-based pseudo labeling and multi-scale feature grouping</title>
		<author>
			<persName><forename type="first">Chunming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yachao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoxia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longxiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.11003</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Strategic preys make acute predators: Enhancing camouflaged object detectors by generating camouflaged objects</title>
		<author>
			<persName><forename type="first">Chunming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yachao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.03166</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reconet: Recurrent correction network for fast and efficient multi-modality image fusion</title>
		<author>
			<persName><forename type="first">Zhanbo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongxuan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Estimation of nonnormalized statistical models by score matching</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Medical image fusion: A survey of the state of the art</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Pappachen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Belur</forename><forename type="middle">V</forename><surname>Dasarathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="19" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards all weather and unobstructed multi-spectral image stitching: Algorithm and benchmark</title>
		<author>
			<persName><forename type="first">Zhiying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengxi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3783" to="3791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised deep image fusion with structure tensor representations</title>
		<author>
			<persName><forename type="first">Hyungjoo</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunsung</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Namkoo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="3845" to="3858" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Denoising diffusion restoration models</title>
		<author>
			<persName><forename type="first">Bahjat</forename><surname>Kawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11793</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast multiscale structural patch decomposition for multi-exposure im-age fusion</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kede</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5805" to="5816" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nestfuse: An infrared and visible image fusion architecture based on nest connection and spatial/channel attention models</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tariq</forename><forename type="middle">S</forename><surname>Durrani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="9645" to="9656" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rfn-nest: An end-toend residual fusion network for infrared and visible images</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Densefuse: A fusion approach to infrared and visible images</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Jun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2614" to="2623" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lrrnet: A novel representation learning guided fusion network for infrared and visible images</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fusion from decomposition: A self-supervised decomposition approach for image fusion</title>
		<author>
			<persName><forename type="first">Pengwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009">2022. 3, 7, 8, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Target-aware dual adversarial learning and a multi-scenario multi-modality benchmark to fuse infrared and visible for object detection</title>
		<author>
			<persName><forename type="first">Jinyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanbo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongxuan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="5792" to="5801" />
		</imprint>
	</monogr>
	<note>IEEE, 2022. 2, 3, 7, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A bilevel integrated model with data-driven layer ensemble for multi-modality image fusion</title>
		<author>
			<persName><forename type="first">Risheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongxuan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1261" to="1274" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Searching a hierarchically aggregated fusion architecture for fast multimodality image fusion</title>
		<author>
			<persName><forename type="first">Risheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bi-level dynamic learning for jointly multi-modality image fusion and beyond</title>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risheng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06720</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion via gradient transfer and total variation minimization</title>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="100" to="109" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion via detail preserving adversarial learning</title>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="85" to="98" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion methods and applications: A survey</title>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="153" to="178" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ddcgan: A dual-discriminator conditional generative adversarial network for multi-resolution image fusion</title>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoguang</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ping (</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4980" to="4995" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fusiongan: A generative adversarial network for infrared and visible image fusion</title>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="11" to="26" />
			<date type="published" when="2009">2019. 2, 3, 8, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ganmcc: A generative adversarial network with multiclassification constraints for infrared and visible image fusion</title>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenfeng</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2009">2021. 2, 3, 8, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond Yk</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A survey on region based image fusion methods</title>
		<author>
			<persName><forename type="first">Bikash</forename><surname>Meher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rutuparna</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajith</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="132" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Diverse sample generation: Pushing the limit of generative data-free quantization</title>
		<author>
			<persName><forename type="first">Yifu</forename><surname>Haotong Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangguo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiakai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bibench: Benchmarking and analyzing network binarization</title>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Haotong Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aoyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distribution-sensitive information retention for accurate binary neural network</title>
		<author>
			<persName><forename type="first">Xiangguo</forename><surname>Haotong Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichen</forename><surname>Vincent Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jägersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>CVF / IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7479" to="7489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pseudoinverse-guided diffusion models for inverse problems</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morteza</forename><surname>Mardani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno>ICLR, 2021. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image fusion in the loop of high-level vision tasks: A semantic-aware realtime infrared and visible image fusion network</title>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiteng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Piafusion: A progressive infrared and visible image fusion network based on illumination aware</title>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiteng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="page" from="79" to="92" />
			<date type="published" when="2008">2022. 1, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Progress in color night vision</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><forename type="middle">A</forename><surname>Hogervorst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2008">2012. 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Image fusion transformer</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Vibashan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeya</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Poojan</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishal</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno>CoRR, abs/2107.09011, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised misaligned infrared and visible image fusion via cross-modality image generation and registration</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>ijcai.org, 2022. 3, 8</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dual attention suppression attack: Generate adversarial camouflage in physical world</title>
		<author>
			<persName><forename type="first">Jiakai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixin</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunchang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8565" to="8574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Defensive patches for robust recognition in the physical world</title>
		<author>
			<persName><forename type="first">Jiakai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixin</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renshuai</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotong</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2456" to="2465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Tackling the generative learning trilemma with denoising diffusion gans</title>
		<author>
			<persName><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">U2fusion: A unified unsupervised image fusion network</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="502" to="518" />
			<date type="published" when="2009">2022. 1, 3, 8, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fusiondn: A unified densely connected network for image fusion</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuliang</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence, AAAI</title>
		<imprint>
			<date type="published" when="2008">2020. 1, 3, 7, 8</date>
			<biblScope unit="page" from="12484" to="12491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Rfnet: Unsupervised network for mutually reinforcing multimodal image registration and fusion</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiteng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuliang</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Murf: Mutually reinforcing multi-modal image registration and fusion</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiteng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep gradient projection networks for pan-sharpening</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangshe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junmin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxia</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>CVF / IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1366" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Deep convolutional sparse coding networks for image fusion</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junmin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangshe</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR, abs/2005.08448</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Zero shot image restoration using denoising diffusion null-space model</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Yinhuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Jiwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Jian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.00490</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Sdnet: A versatile squeeze-anddecomposition network for real-time image fusion</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Rethinking the image fusion: A fast unified image fusion network based on proportional maintenance of gradient and intensity</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12797" to="12804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep learning-based multi-focus image fusion: A survey and a comparative study</title>
		<author>
			<persName><forename type="first">Xingchen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">IFCNN: A general image fusion framework based on convolutional neural network</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="99" to="118" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Cddfuse: Correlation-driven dual-branch feature decomposition for multi-modality image fusion</title>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowen</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangshe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zudi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004">June 2023. 3, 4</date>
			<biblScope unit="page" from="5906" to="5916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowen</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangshe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.11443</idno>
		<title level="m">Equivariant multi-modality image fusion</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Bayesian fusion for infrared and visible images</title>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junmin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangshe</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">DIDFuse: Deep image decomposition for infrared and visible image fusion</title>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junmin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangshe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Efficient and model-based infrared and visible image fusion via algorithm unrolling</title>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangshe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junmin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1186" to="1196" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Spherical space feature decomposition for guided depth map super-resolution</title>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangshe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengli</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Discrete cosine transform network for guided depth map super-resolution</title>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangshe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zudi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003">June 2022. 3</date>
			<biblScope unit="page" from="5697" to="5707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">FGF-GAN: A lightweight generative adversarial network for pansharpening via fast guided filter</title>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangshe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junmin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxia</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
