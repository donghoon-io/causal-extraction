<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detector Discovery in the Wild: Joint Multiple Instance and Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2014-12-02">2 Dec 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
							<email>jhoffman@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
							<email>pathak@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<email>trevor@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<email>saenko@cs.uml.edu</email>
						</author>
						<author>
							<persName><forename type="first">Umass</forename><surname>Lowell</surname></persName>
						</author>
						<title level="a" type="main">Detector Discovery in the Wild: Joint Multiple Instance and Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-12-02">2 Dec 2014</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1412.1135v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop methods for detector learning which exploit joint training over both weak and strong labels and which transfer learned perceptual representations from stronglylabeled auxiliary tasks. Previous methods for weak-label learning often learn detector models independently using latent variable optimization, but fail to share deep representation knowledge across classes and usually require strong initialization. Other previous methods transfer deep representations from domains with strong labels to those with only weak labels, but do not optimize over individual latent boxes, and thus may miss specific salient structures for a particular category. We propose a model that subsumes these previous approaches, and simultaneously trains a representation and detectors for categories with either weak or strong labels present. We provide a novel formulation of a joint multiple instance learning method that includes examples from classification-style data when available, and also performs domain transfer learning to improve the underlying detector representation. Our model outperforms known methods on ImageNet-200 detection with weak labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>It is well known that contemporary visual models thrive on large amounts of training data, especially those that directly include labels for desired tasks. Many real world settings contain labels with varying specificity, e.g., "strong" bounding box detection labels, and "weak" labels indicating presence somewhere in the image. We tackle the problem of joint detector and representation learning, and develop models which cooperatively exploit heterogeneous sources of training data, where some classes have no "strong" annotations. Our model optimizes a latent variable multiple instance learning model over image regions while simultaneously transferring a shared representation from detectiondomain models to classification-domain models. The latter provides a key source of automatic and accurate initializa- tion for latent variable optimization, which has heretofore been unavailable in such methods.</p><p>Previous methods employ varying combinations of weak and strong labels of the same object category to learn a detector. Such methods seldom exploit available stronglabeled data of different, auxiliary categories, despite the fact that such data is very often available in many practical scenarios. Deselaers et al. <ref type="bibr" target="#b9">[10]</ref> uses auxiliary data to learn generic objectness information just as an initial step, but doesn't optimize jointly for weakly labeled data.</p><p>We introduce a new model for large-scale learning of detectors that can jointly exploit weak and strong labels, perform inference over latent regions in weakly labeled training examples, and can transfer representations learned from related tasks (see Figure <ref type="figure" target="#fig_0">1</ref>). In practical settings, such as learning visual detector models for all available ImageNet categories, or for learning detector versions of other defined categories such as Sentibank's adjective-noun-phrase models <ref type="bibr" target="#b6">[7]</ref>, our model makes greater use of available data and la-bels than previous approaches. Our method takes advantage of such data by using the auxiliary strong labels to improve the feature representation for detection tasks, and uses the improved representation to learn a stronger detector from weak labels in a deep architecture.</p><p>To learn detectors, we exploit weakly labeled data for a concept, including both "easy" images (e.g., from Ima-geNet classification training data), and "hard" weakly labeled imagery (e.g., from PASCAL or ImageNet detection training data with bounding box metadata removed). We define a novel multiple instance learning (MIL) framework that includes bags defined on both types of data, and also jointly optimizes an underlying perceptual representation using strong detection labels from related categories. The latter takes advantage of the empirical results in <ref type="bibr" target="#b18">[19]</ref>, which demonstrated knowledge of what makes a good perceptual representation for detection tasks could be learned from a set of paired weak and strong labeled examples, and the resulting adaptation could be transferred to new categories, even those for which no strong labels were available.</p><p>We evaluate our model empirically on the largest set of available ground-truth visual detection data, the ImageNet-200 category challenge. Our method outperforms the previous best MIL-based approaches for held-out detector learning on ImageNet-200 <ref type="bibr" target="#b26">[27]</ref> by 200%, and outperforms the previous best domain-adaptation based approach <ref type="bibr" target="#b18">[19]</ref> by 12%. Our model is directly applicable to learning improved "detectors in the wild", including categories in ImageNet but not in ImageNet-200, or categories defined ad-hoc for a particular user or task with just a few training examples to fine-tune a new classification model. Such models can be promoted to detectors with no (or few) labeled bounding boxes. Upon acceptance we will release an open-source implementation of our model and all network and detector weights for an improved set of detectors for the ImageNet-7.5K dataset of <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>CNNs for Visual Recognition Within the last few years, convolutional neural networks (CNNs) have emerged as the clear winners for many visual recognition tasks. A breakthrough was made when the positive performance demonstrated for digit recognition <ref type="bibr" target="#b24">[25]</ref> began to translate to the ImageNet <ref type="bibr" target="#b26">[27]</ref> classification challenge winner <ref type="bibr" target="#b21">[22]</ref>. Shortly thereafter, the feature space learned through these architectures was shown to be generic and effective for a large variety of visual recognition tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39]</ref>. These results were followed by state-of-the-art results for object detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>. Most recently, it was shown that CNN architectures can be used to transfer generic information between the classification and detection tasks <ref type="bibr" target="#b18">[19]</ref>, improving detection performance for tasks which lack bounding box training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training with Auxiliary Data Sources</head><p>There has been a large amount of prior work on training models using auxiliary data sources. The problem of visual domain adaptation is precisely seeking to use data from a large auxiliary source domain to improve recognition performance on a target domain which has little or no labeled data available. Techniques to solve this problem consist of learning a new feature representation that minimizes the distance between source and target distributions <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15]</ref>, regularizing the learning of a target classifier against the source model <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9]</ref>, or doing both simultaneously <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Multiple Instance Learning Since its inception, the MIL <ref type="bibr" target="#b10">[11]</ref> problem has been attempted in several frameworks including Noisy-OR <ref type="bibr" target="#b17">[18]</ref>, boosting <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b39">40]</ref> etc. But most commonly, it was framed as a max-margin classification problem <ref type="bibr" target="#b2">[3]</ref> with latent parameters optimized using alternating optimization <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b36">37]</ref>. Overall, MIL is tackled in two stages: first finding better initialization, and then using better heuristics for optimization. A number of methods have been proposed for initialization which include using large image region excluding boundary <ref type="bibr" target="#b25">[26]</ref>, using candidate set which covers the training data space <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, using unsupervised patch discovery <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b29">30]</ref>, learning generic objectness knowledge from auxiliary catgories <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>, learning latent categories from background to suppress it <ref type="bibr" target="#b34">[35]</ref> or using class-specific similarity <ref type="bibr" target="#b30">[31]</ref>. Approaches to better optimize the non-convex problem involve using multifold learning as a measure of regularizing overfitting <ref type="bibr" target="#b7">[8]</ref>, optimize Latent SVM for the area under the ROC curve (AUC) <ref type="bibr" target="#b5">[6]</ref> and training with easy examples in beginning to avoid bad local optimization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref>. Most of these approaches perform reasonably only when object covers most of the region of image, or when most of the candidate regions contain an object. The major challenge faced by MIL in general is that of fixed feature representation, and poor initialization particularly in non-object centric images. Our algorithm provides solutions to both of these issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background: MI-SVM</head><p>We begin by briefly reviewing a standard solution to the multiple instance learning problem, Multiple Instance SVMs (MI-SVMs) <ref type="bibr" target="#b2">[3]</ref> or Latent SVMs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b36">37]</ref>. In this setting, each weakly labeled image is considered a collection of regions which form a positive 'bag'. For a binary classification problem, the task is to maximize the bag margin which is defined by the instance with highest confidence. For each weakly labeled image I ∈ W, we collect a set of regions of interest and define the index set of those regions as R I . We next define a bag as B I = {x i |i ∈ R I }, with label Y I , and let the i th instance in the bag be For an image with a negative image-level label, Y I = -1, we label all regions in the image as negative. For an image with a positive image-level label, Y I = 1, we create a constraint that at least one positive instance occurs in the image bag.</p><formula xml:id="formula_0">(x i , y i ) ∈ R p × {-1, +1}.</formula><p>In a typical detection scenario, R I corresponds to the set of possible bounding boxes inside the image, and maximizing over R I is equivalent to discovering the bounding box that contains the positive object. We define a representation φ(x i ) ∈ R d for each instance, which is the feature descriptor for the corresponding bounding box, and formulate the MI-SVM objective as follows:</p><formula xml:id="formula_1">min w∈R d 1 2 w 2 2 + α I Y I , max i∈R I w T φ(x i )<label>(1)</label></formula><p>where α is a hyper-parameter and (y, ŷ) is the hinge loss. Interestingly, for negative bags i.e. Y I = -1, the knowledge that all instances are negative allows us to unfold the max operation into a sum over each instance. Thus, Equation (1) reduces to a standard QP with respect to w. For the case of positive bags, this formulation reduces to a standard SVM if maximum scoring instance is known.</p><p>Based on this idea, Equation ( <ref type="formula" target="#formula_1">1</ref>) is optimized using a classic concave-convex procedure <ref type="bibr" target="#b37">[38]</ref>, which decreases the objective value monotonically with a guarantee to converge to a local minima or saddle point. Due to this reason, these methods are extremely susceptible to the feature representation and detector initialization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33]</ref>. We address both these issues using annotated auxiliary data available to learn a better feature representation and reasonable initialization for MIL based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Large Scale Detection Learning</head><p>We propose a detection learning algorithm that uses a heterogeneous data source, containing only weak labels for some tasks, to produce strong detectors for all. Let the set of images with only weak labels be denoted as W and the set of images with strong labels (bounding box annotations) from auxiliary tasks be denoted as S. We assume that the set of object categories that appear in the weakly labeled set, C W , do not overlap with the set of object categories that appear in the strongly labeled set, C S . For each image in the weakly labeled set, I ∈ W, we have an image-level label per category, k: Y k I ∈ {1, -1}. For each image in the strongly labeled set, I ∈ S, we have a label per category, k, per region in the image, i ∈ R I : y k i ∈ {1, -1}. We seek to learn a representation, φ(•) that can be used to train detectors for all object categories, C = {C W ∪ C S }. For a category k ∈ C, we denote the category specific detection parameter as w k and compute our final detection scores per region, x, as</p><formula xml:id="formula_2">score k (x) = w T k φ(x).</formula><p>We propose a joint optimization algorithm which learns a feature representation, φ(•), and detectors, w k , using the combination of strongly labeled detection data, S, with weakly labeled data, W. For a fixed representation, one can directly train detectors for all categories represented in the strongly labeled set, k ∈ C S . Additionally, for the same fixed representation, we reviewed in the previous section techniques to train detectors for the categories in the weakly labeled data set, k ∈ C W . Our insight is that the knowledge from the strong label set can be used to help guide the optimization for the weak labeled set, and we can explicitly adapt our representation for the categories of interest and for the generic detection task.</p><p>Below, we state our overall objective:</p><formula xml:id="formula_3">min w k ,φ k∈C k Γ(w k ) (2) +α I∈W p∈C W (Y p I , max i∈R I w T p φ(x i ))</formula><p>+α</p><formula xml:id="formula_4">I∈S i∈R I q∈C S (y q i , w T q φ(x i ))</formula><p>where α is a scalar hyper-parameter, (.) is the loss function and Γ(.) is a regularization over the detector weights. This formulation is non-convex in nature due to the presence of instance level ambiguity. It is difficult to optimize directly, so we choose a specific alternating minimization approach (see Figure <ref type="figure" target="#fig_1">2</ref>). We begin by initializing a feature representation and initial CNN classification weights using auxiliary weakly labeled data (blue boxes Figure <ref type="figure" target="#fig_1">2</ref>). These weights can be used to compute scores per region proposal to produce initial detection scores. We next use available strongly annotated data from auxiliary tasks to transfer category invariant information about the detection problem. We accomplish this through further optimizing our feature representation and learning a generic background detection weights (red boxes Figure <ref type="figure" target="#fig_1">2</ref>). We then use the well tuned detection feature space to perform MIL on our weakly labeled data to find positive instances (yellow box Figure <ref type="figure" target="#fig_1">2</ref>. Finally, we use our discovered positive instances together with the strongly annotated data from auxiliary tasks to jointly optimize all parameters corresponding to feature representation and detection weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Initialize Feature Representation and Detector Weights</head><p>We now discuss our procedure for initializing the feature representation and detection weights. We want to use a representation which makes it possible to separate objects of interest from background and makes it easy to distinguish different object categories. Convolutional neural networks (CNNs) have proved effective at providing the desired semantically discriminative feature representation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29]</ref>. We use the architecture which won the ILSVRC2012 classification challenge <ref type="bibr" target="#b21">[22]</ref>, since it is one of the best performing and most studied models. The network contains roughly 60 million parameters, and so must be pretrained on a large labeled corpus. Following the standard protocol, we use auxiliary weakly labeled data that was collected for training a classification task for this initial training of the network parameters (Figure <ref type="figure" target="#fig_1">2</ref>: blue boxes). This data is usually object centric and is therefore effective for training a network that is able to discriminate between different categories. We remove the classification layer of the network and use the output of the fully connected layer, f c 7 , as our initial feature representation, φ(•).</p><p>We next learn initial values for all of the detection parameters, w k , ∀k ∈ C. To solve this, we begin by solving the simplified learning problem of image-level classification. The image, I ∈ S, is labeled as positive for a category k if any of the regions in the image are labeled as positive for k and is labeled as negative otherwise, we denote the image level label as in the weakly labeled case: Y k I . Now, we can optimize over all images to refine the representation and learn category specific parameters that can be used per region proposal to produce detection scores:</p><formula xml:id="formula_5">min w k ,φ k∈C k   Γ(w k ) + α I∈{W∪S} (Y k I , w T k φ(I))  <label>(3)</label></formula><p>We optimize Equation 3 through fine-tuning our CNN architecture with a new K-way last fully connected layer, where K = |C|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Optimize with Strong Labels From Auxiliary Tasks</head><p>Motivated by the recent representation transfer result of Hoffman et al. <ref type="bibr" target="#b18">[19]</ref> -LSDA, we learn to generically transform our classification feature representation into a detection representation by using the strongly labeled detection data to modify the representation, φ(•), as well as the detectors, w k , k ∈ C S (Figure <ref type="figure" target="#fig_1">2</ref> : red boxes). In addition, we use the strongly annotated detection data to initialize a new "background" detector, w b . This detector explicitly attempts to recognize all data labeled as negative in our bags. However, since we initialize this detector with the strongly annotated data, we know precisely which regions correspond to background. The intermediate objective is:</p><formula xml:id="formula_6">min wq,φ q∈{C S ,b} q Γ(w q ) + α I∈S i∈R I (y q i , w T q φ(x i ))<label>(4)</label></formula><p>Again, this is accomplished by fine-tuning our CNN architecture with the strongly labeled data, while keeping the detection weights for the categories with only weakly labeled data fixed. Note, we do not include the last layer adaptation part of LSDA, since it would not be easy to include in the joint optimization. Moreover, it is shown that the adaptation step does not contribute significantly to the accuracy <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Jointly Optimize using All Data</head><p>With a representation that has now been directly tuned for detection, we fix the representation, φ(•) and consider solving for the regions of interest in each weak labeled image. This corresponds to solving the second term in Equa-tion (2), i.e.:</p><formula xml:id="formula_7">min wp p∈{C W ,b} p Γ(w p )<label>(5)</label></formula><p>+α</p><formula xml:id="formula_8">I∈W (Y p I , max i∈R I w T p φ(x i ))</formula><p>Note, we can decouple this optimization problem and independently solve for each category in our weakly labeled data set, p ∈ C W . Let's consider a single category p. Our goal is to minimize the loss for category p over images I ∈ W. We will do this by considering two cases. First, if p is not in the weak label set of an image (Y p I = -1), then all regions in that image should be considered negative for category p. Second, if Y p I = 1, then we positively label a region x i if it has the highest confidence of containing object and negatively label all other regions. We perform the discovery of this top region in two steps. At first, we narrow down the set of candidate bounding boxes using the score, w T p φ(x i ), from our fixed representation and detectors from the previous optimization step. This set is then refined to estimate the most region likely to contain the positive instance in a Latent SVM formulation. The implementation details are discussed section 5.2.</p><p>Our final optimization step is to use the discovered annotations from our weak data-set to refine our detectors and feature representation from the previous optimization step. This amounts to the subsequent step for alternating minimization of the joint objective described in Equation <ref type="formula">2</ref>. We collectively utilize the strong annotations of images in S and estimated annotations for weakly labelled set, W, to optimize for detector weights and feature representation, as follows:</p><formula xml:id="formula_9">min w k ,φ k∈{C,b} k Γ(w k )<label>(6)</label></formula><p>+α</p><formula xml:id="formula_10">I∈{W∪S} i∈R I (y k i , w T k φ(x i ))</formula><p>This is achieved by re-finetuning the CNN architecture. The refined detector weights and representation can be used to mine the bounding box annotations for weakly labeled data again, and this process can be iterated over (see Figure <ref type="figure" target="#fig_1">2</ref>). We discuss re-training strategies and evaluate the contribution of this final optimization step in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We now study the effectiveness of our algorithm by applying it to a standard detection task. Training set has fewer objects per image than validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">ILSVRC13 Detection Dataset &amp; Setup</head><p>We use the ILSVRC13 detection dataset <ref type="bibr" target="#b26">[27]</ref> for our experiments. This dataset provides bounding box annotations for 200 categories. The dataset is separated into three pieces: train, val, test (see Table <ref type="table" target="#tab_0">1</ref>). The training images have fewer objects per image on an average than validation set images, so they constitute classification style data <ref type="bibr" target="#b18">[19]</ref>. Following prior work <ref type="bibr" target="#b15">[16]</ref>, we use the further separation of the validation set into val1 and val2. Overall, we use the train and val1 set for our training data source and evaluate our performance of the data in val2.</p><p>Specifically, we use ∼1000 randomly chosen images per class from the train set for initializing our CNN weights. For this data we consider only have weak labels for all categories and train with the classification objective. We use the train set for this purpose as it tends to have more objectcentric images and is therefore better suited to initializing classification weights.</p><p>We have bounding box annotations for 100/200 of the categories in val1 (∼5000 images with bounding boxes). Specifically, with the category names sorted alphabetically, categories 1-100 have strong annotations while 101-200 have only weak (image-level) annotations. Finally, we evaluate detection performance on the ∼ 10, 000 images in val2 across all 200 categories.</p><p>We use open source deep learning framework, Caffe <ref type="bibr" target="#b20">[21]</ref>, for the implementation, training and finetuning of our CNN architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis of Discovered Positive Boxes</head><p>One of the key components of our system is using strong annotations from auxiliary tasks to learn a representation where it's possible to discover patches that correspond to the objects of interest in our weakly labeled data source. We begin our analysis by studying the patch discovery that our feature space enables. We optimize the patch discovery (Equation ( <ref type="formula" target="#formula_7">5</ref>)) using a one vs all Latent SVM formulation and optimize the formulation for AUC criterion <ref type="bibr" target="#b5">[6]</ref>. The feature descriptor used is the output of the fully connected layer, f c 7 , of the CNN which is produced after finetuning the feature representation with strongly annotated data from auxiliary tasks. Following our alternating minimization approach, these discovered top boxes are then used Figure <ref type="figure">3</ref>: Example mined bounding boxes learned using our method. Left side shows the mined boxes after fine-tuning with images in classification settings only, and right side shows the mined boxes after fine-tuning with auxiliary strongly annotated dataset. We show top 5 mined boxes across the dataset for corresponding category. Examples with a green outline are categories for which our algorithm was able to correctly mine patches of the object, while the feature space with only weak label training was not able to produce correct patches. In yellow we highlight the specific example of "tennis racket". None of the discovered patches from the original feature space correctly located the tennis racket and instead included the person as well. After incorporating the strong annotations from auxiliary tasks, our method starts discovering tennis rackets, though still has some confusion with the person playing tennis.</p><p>to re-estimate the weights and feature representations of our CNN architecture.</p><p>To evaluate the quality of mined boxes, we do precision analysis with respect to their overlap with ground truth which is measured using the standard intersection over union (IOU) metric. Table <ref type="table" target="#tab_1">2</ref> reports the precision for varying overlapping thresholds. Our optimization approach produces one positive patch per image with a weak label, and a discovered patch is considered a true positive if it overlaps sufficiently with the ground truth box that corresponds to that label. Since each patch, once discovered, is considered an equivalent positive (regardless of score) for the purpose of retraining, this simple precision metric is a good indication of the usefulness of our mined patches. It is interesting that a significant fraction of mined boxes have high overlap with the ground truth regions. For reference, we also computed the standard mean average precision over the discovered patches and report these results.</p><p>It is important to understand not only that our new feature space improves the quality of the resulting patches, but also what type of errors our method reduces. In Figure <ref type="figure">3</ref>, we show the top 5 scoring discovered patches before and after modifying the feature space with strong annotations from auxiliary tasks. We find that in many cases the improvement comes from better localization. For example without auxiliary strong annotations we mostly discover the face of a lion rather than the body that we discover after our algorithm. Interestingly, there is also an issue with co-occurring classes. In the bottom row of Figure <ref type="figure">3</ref>, we show the top 5 discovered patches for "tennis racket". Once we incorporate strong annotations from auxiliary tasks we begin to be able to distinguish the person playing tennis from the racket itself. Finally, there are some example mined patches where we reduce quality after incorporating the strong annotations from auxiliary tasks. For example, one of our strongly annotated categories is "computer keyboard". Due to the strong training with keyboard images, some of our mined patches for "laptop" start to have higher scores on the keyboard rather than the whole laptop (see Figure <ref type="figure" target="#fig_2">4</ref>). Comparison with varying amount of overlap with ground truth box. About 25% of our mined boxes have an overlap of at least 0.9. Our method is able to significantly improve the quality of mined boxes after incorporating strong annotations from auxiliary tasks. Bottom row: The mined boxes obtained after fine-tuning with the auxiliary strongly annotated dataset that contains the category "computer keyboard". These patches were low scoring examples, but we show them here to demonstrate a potential failure case -specifically, when one of the strongly annotated classes is a part of one of the weakly labeled classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Detection Performance</head><p>Now that we've analyzed the intermediate result of our algorithm, we next study the full performance of our system. Figure <ref type="figure" target="#fig_3">5</ref> shows the mean average precision (mAP) percentage computed over the categories in val2 of ILSVRC13 for which we only have weakly annotated training data (categories 101-200). We compare to two state-of-the-art methods for this scenario and show that our algorithm significantly outperforms both of the previous state-of-the-art techniques. The first, LCL <ref type="bibr" target="#b34">[35]</ref>, detects in the standard weakly supervised setting -having no bounding box annotations for any of the 200 categories. This method also only reports results across all 200 categories. Our experiments indicate that the first 100 categories are easier on average then the second 100 categories, therefore the 6.0% mAP may actually be an upper bound of the performance of this approach. The second algorithm we compare against is LSDA <ref type="bibr" target="#b18">[19]</ref>, which does utilize the bounding box information from the first 100 categories.</p><p>We next consider different re-training strategies for learning new features and detection weights after discov- ering the positive patches in the weakly labeled data. Table 3 reports the mean average precision (mAP) percentage for no re-training (directly using the feature space learned after incorporating the strong labels), re-training only the category detection parameters, and retraining feature representations jointly with detection weights. In our experiments the improved performance is due to the first iteration of the overall algorithm. We find that the best approach is to jointly learn to refine the feature representation and the detection weights. More specifically, we learn a new feature representation by fine-tuning all fully connected layers in the CNN architecture.</p><p>We finally analyze examples where our full algorithm outperforms the previous state-of-the-art, LSDA <ref type="bibr" target="#b18">[19]</ref>. Figure <ref type="figure" target="#fig_4">6</ref> shows a sample of the types of errors our algorithm improves on. These include localization errors, confusion with other categories, and interestingly, confusion with cooccurring categories. In particular, our algorithm provides improvement when searching for a small object (ball or helmet) in a sports scene. Training only with weak labels causes the previous state-of-the-art to confuse the player and the object, resulting in a detection that includes both. Our algorithm is able to localize only the small object and recognize that the player is a separate object of interest.   <ref type="table">3</ref>: Comparison of different ways to re-train after discovery of positive patches. We show mAP on val2 set from ILSVRC13. We find that the most effective way to re-train with discovered windows is to modify the detectors and the feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented a method which jointly trains a feature representation and detectors for categories with only weakly labeled data. We use the insight that strongly annotated detection data from auxiliary tasks can be used to train a feature representation that is conducive to discovering object patches in weakly labeled data. We demonstrate using a standard detection dataset (ImageNet-200 detection) that our method of incorporating the strongly annotated data from auxiliary tasks is very effective at improving the quality of the discovered patches. We then use all strong annotations along with our discovered object patches to further refine our feature representation and produce our final detectors. We show that our full detection algorithm signifi-cantly outperforms both the previous state-of-the-art methods which uses only weakly annotated data, as well as the algorithm which uses strongly annotated data from auxiliary tasks, but does not incorporate any MIL for the weak tasks.</p><p>Upon acceptance of this paper, we will release all final weights and hyper parameters learned using our algorithm to improve the performance of the recently released ¿7.5K category detectors <ref type="bibr" target="#b18">[19]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: We learn detectors for categories with only weak labels (bottom row), by jointly transferring a representation from auxiliary categories with available strong annotations (top row) and solving an MIL problem on the weakly annotated data (green box).</figDesc><graphic coords="1,308.86,231.70,236.25,153.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Our method jointly optimizes a representation and detectors for categories with only weakly annotated data. We first learn a feature representation conducive to MIL by initializing all parameters with classification style data. We then collectively refine the feature space with strongly annotated data from auxiliary tasks, and perform MIL in our detection feature space. The discovered positive patches are further used to refine the representation and detection weights.</figDesc><graphic coords="3,50.11,72.00,495.01,150.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example mined boxes of the category "laptop" where using auxiliary strongly annotated data causes patch discovery to diverge. Top row: The mined boxes obtained after fine-tuning with images in classification settings only.Bottom row: The mined boxes obtained after fine-tuning with the auxiliary strongly annotated dataset that contains the category "computer keyboard". These patches were low scoring examples, but we show them here to demonstrate a potential failure case -specifically, when one of the strongly annotated classes is a part of one of the weakly labeled classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of mAP (%) for categories without any bounding box annotations (101-200 of val2)of ILSVRC13. Our method significantly outperforms both previous state-of-the-art algorithms: LCL<ref type="bibr" target="#b34">[35]</ref> and LSDA<ref type="bibr" target="#b18">[19]</ref>. *The value for LCL was computed across all 200 categories. Our experiments show this this is an easier task resulting in higher numbers overall.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Examples where our algorithm outperforms the previous state-of-the-art. We show the top scoring detection from the baseline detector, LSDA<ref type="bibr" target="#b18">[19]</ref>, with a Red box and label, and the top scoring detection from our method, LSDL, as a Green box and label. Our algorithm improves localization (ex: rabbit, lion etc), confusion with other categories (ex: miniskirt vs maillot), and confusion with co-occurring classes (ex: volleyball vs volleyball player)</figDesc><graphic coords="8,50.11,72.00,495.00,197.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the ILSVRC13 detection dataset.</figDesc><table><row><cell>Train</cell><cell>Num images 395905 Num objects 345854</cell></row><row><cell>Val</cell><cell>Num images 20121 Num objects 55502</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Precision analysis and mAP performance of discovered patches in our weakly labeled training set (val1) of ILSVRC13 detection dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Precision</cell><cell></cell><cell>mAP</cell></row><row><cell></cell><cell cols="5">ov=0.3 ov=0.5 ov=0.7 ov=0.9 ov=0.5</cell></row><row><cell>Without auxiliary strong dataset</cell><cell>29.63</cell><cell>26.10</cell><cell>24.28</cell><cell>23.43</cell><cell>13.13</cell></row><row><cell>Ours</cell><cell>32.69</cell><cell>28.81</cell><cell>26.27</cell><cell>24.78</cell><cell>22.81</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What is an object?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Confidence-rated multiple instance boosting for object detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="561" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tabula rasa: Model transfer for object category detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object and action classification with latent window parameters</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="237" to="251" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Largescale visual sentiment ontology and detectors using adjective nown paiars</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-fold mil training for weakly supervised object localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning with augmented features for heterogeneous domain adaptation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Tran. PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1304.1511</idno>
		<title level="m">A tractable inference algorithm for diagnosing multiple diseases</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LSDA: Large scale detection through adaptation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient learning of domain-invariant image representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What you saw is not what you get: Domain adaptation using asymmetric kernel transforms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scene recognition and weakly supervised object localization with deformable part-based models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Michael Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<title level="m">Imagenet large scale visual recognition challenge</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>CoRR, abs/1312.6229</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of mid-level discriminative patches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">In defence of negative mining for annotating weakly labelled data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Looking beyond the image: Unsupervised learning for object saliency and detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Weaklysupervised discovery of visual pattern configurations</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latet category learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Cross-domain video concept detection using adaptive svms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>ACM Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning structural svms with latent variables</title>
		<author>
			<persName><forename type="first">C.-N</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1169" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The concave-convex procedure</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="915" to="936" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiple instance boosting for object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
