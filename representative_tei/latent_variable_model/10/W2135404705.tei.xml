<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ParceLiNGAM: A causal ordering method robust against latent confounders</title>
				<funder ref="#_8q2P5dC #_vxcm77A">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2013-07-29">29 Jul 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Tatsuya</forename><surname>Tashiro</surname></persName>
							<email>tashiro@ar.sanken.osaka-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">The Institute of Scientific and Industrial Research (ISIR)</orgName>
								<orgName type="institution">Osaka University</orgName>
								<address>
									<addrLine>Mihogaoka 8-1</addrLine>
									<postCode>567-0047</postCode>
									<settlement>Ibaraki</settlement>
									<region>Osaka</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Osaka University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Helsinki</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Takashi</forename><surname>Washio</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Osaka University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ParceLiNGAM: A causal ordering method robust against latent confounders</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-07-29">29 Jul 2013</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1303.7410v2[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider learning a causal ordering of variables in a linear non-Gaussian acyclic model called LiNGAM. Several existing methods have been shown to consistently estimate a causal ordering assuming that all the model assumptions are correct. But, the estimation results could be distorted if some assumptions actually are violated. In this paper, we propose a new algorithm for learning causal orders that is robust against one typical violation of the model assumptions: latent confounders. The key idea is to detect latent confounders by testing independence between estimated external influences and find subsets (parcels) that include variables that are not affected by latent confounders. We demonstrate the effectiveness of our method using artificial data and simulated brain imaging data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Bayesian networks have been widely used to analyze causal relations of variables in many empirical sciences <ref type="bibr" target="#b0">(Bollen, 1989;</ref><ref type="bibr" target="#b13">Pearl, 2000;</ref><ref type="bibr" target="#b18">Spirtes, Glymour, &amp; Scheines, 1993)</ref>. A common assumption is linear-Gaussianity. But this poses serious identifiability problems so that many important models are indistinguishable with no prior knowledge on the structures. Recently, it was shown by <ref type="bibr" target="#b14">(Shimizu, Hoyer, Hyvärinen, &amp; Kerminen, 2006)</ref> that the utilization of non-Gaussianity allows the full structure of a linear acyclic model to be identified without pre-specifying any causal orders of variables. The new model, a Linear Non-Gaussian Acyclic Model called LiNGAM <ref type="bibr" target="#b14">(Shimizu et al., 2006)</ref>, is closely related to independent component analysis (ICA) <ref type="bibr" target="#b9">(Hyvärinen, Karhunen, &amp; Oja, 2001)</ref>.</p><p>Most existing estimation methods <ref type="bibr" target="#b14">(Shimizu et al., 2006</ref><ref type="bibr" target="#b15">(Shimizu et al., , 2011;;</ref><ref type="bibr" target="#b10">Hyvärinen &amp; Smith, 2013)</ref> for LiNGAM learn causal orders assuming that all the model assumptions hold. Therefore, these algorithms could return completely wrong estimation results when some of the model assumptions are violated. Thus, in this paper, we propose a new algorithm for learning causal orders that is robust against one typical model violation, i.e., latent confounders. A latent confounder means a variable which is not observed but which exerts a causal influence on some of the observed variables. Many real-world applications including brain imaging data analysis <ref type="bibr" target="#b17">(Smith et al., 2011)</ref> could benefit from our approach.</p><p>This paper 1 is organized as follows. We first review LiNGAM <ref type="bibr" target="#b14">(Shimizu et al., 2006)</ref> and its extension to latent confounder cases <ref type="bibr" target="#b8">(Hoyer, Shimizu, Kerminen, &amp; Palviainen, 2008)</ref> in Section 2. In Section 3, we propose a new algorithm to learn causal orders in LiNGAM with latent confounders. We empirically evaluate the performance of our algorithm using artificial data in Section 4 and simulated fMRI data in Section 5. We conclude this paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: LiNGAM with latent confounders</head><p>We briefly review a linear non-Gaussian acyclic model called LiNGAM <ref type="bibr" target="#b14">(Shimizu et al., 2006)</ref> and an extension of the LiNGAM to cases with latent confounding variables <ref type="bibr" target="#b8">(Hoyer et al., 2008)</ref>.</p><p>In LiNGAM <ref type="bibr" target="#b14">(Shimizu et al., 2006)</ref>, causal relations of observed variables x i (i = 1, • • • , d) are modeled as:</p><formula xml:id="formula_0">x i = k(j)&lt;k(i) b ij x j + e i ,<label>(1)</label></formula><p>where k(i) is a causal ordering of the variables x i . In this ordering, the variables x i graphically form a directed acyclic graph (DAG) so that no later variable determines, i.e., has a directed path on any earlier variable. The e i are external influences, and b ij are connection strengths. In matrix form, the model ( <ref type="formula" target="#formula_0">1</ref>) is written as</p><formula xml:id="formula_1">x = Bx + e,<label>(2)</label></formula><p>where the connection strength matrix B collects b ij and the vectors x and e collect x i and e i . Note that the matrix B can be permuted to be lower triangular with all zeros on the diagonal if simultaneous equal row and column permutations are made according to a causal ordering k(i) because of the acyclicity.</p><p>The zero/non-zero pattern of b ij corresponds to the absence/existence pattern of directed edges. External influences e i follow non-Gaussian continuous distributions with zero mean and non-zero variance and are mutually independent. The non-Gaussianity assumption on e i enables identification of a causal ordering k(i) based on data x only <ref type="bibr" target="#b14">(Shimizu et al., 2006)</ref>. This feature is a major advantage over conventional Bayesian networks based on the Gaussianity assumption on e i <ref type="bibr" target="#b18">(Spirtes et al., 1993)</ref>.</p><p>Next, LiNGAM with latent confounders <ref type="bibr" target="#b8">(Hoyer et al., 2008)</ref> can be formulated as follows:</p><formula xml:id="formula_2">x = Bx + Λf + e,<label>(3)</label></formula><p>where the difference with LiNGAM in Eq. ( <ref type="formula" target="#formula_1">2</ref>) is the existence of latent confounding variable vector f . A latent confounding variable is a latent variable that is a parent of more than one observed variable. The vector f collects non-Gaussian latent confounders f j with zero mean and non-zero variance (j = 1, • • • , q). Without loss of generality <ref type="bibr" target="#b8">(Hoyer et al., 2008)</ref>, latent confounders f j are assumed to be mutually independent. The matrix Λ collects λ ij which denotes the connection strength from f j to x i . For each j, at least two λ ij are non-zero since a latent confounder is defined to have at least two children <ref type="bibr" target="#b8">(Hoyer et al., 2008)</ref>. The matrix Λ is assumed to be of full column rank.</p><p>The central problem of causal discovery based on the latent variable LiNGAM in Eq. ( <ref type="formula" target="#formula_2">3</ref>) is to estimate as many of causal orders k(i) and connection strengths b ij as possible based on data x only. This is because in many cases only an equivalence class of the true model whose members produce the exact same observed distribution is identifiable <ref type="bibr" target="#b8">(Hoyer et al., 2008)</ref>.</p><p>In <ref type="bibr" target="#b8">(Hoyer et al., 2008)</ref>, an estimation method based on overcomplete ICA <ref type="bibr" target="#b11">(Lewicki. &amp; Sejnowski, 2000)</ref> was proposed. However, overcomplete ICA methods are often not very reliable and get stuck in local optima. Thus, in <ref type="bibr" target="#b3">(Entner &amp; Hoyer, 2011)</ref>, a method that does not use overcomplete ICA was proposed to first find variable pairs that are not affected by latent confounders and then estimate a causal ordering of one to the other. However, their method does not estimate a causal ordering of more than two variables. A simple cumulant-based method for estimating the model in the case of Gaussian latent confounders was further proposed by <ref type="bibr" target="#b1">(Chen &amp; Chan, 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A method robust against latent confounders</head><p>In this section, we propose a new approach for estimating causal orders of more than two variables without explicitly modeling latent confounders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Identification of causal orders of variables that are not affected by latent confounders</head><p>We first provide principles to identify an exogenous (root) variable and a sink variable which are such that are not affected by latent confounders in the latent variable LiNGAM in Eq. ( <ref type="formula" target="#formula_2">3</ref>) (if such variables exist) and next present an estimation algorithm. Recent estimation methods <ref type="bibr" target="#b15">(Shimizu et al., 2011)</ref> for LiNGAM in Eq. ( <ref type="formula" target="#formula_1">2</ref>) and its nonlinear extension <ref type="bibr" target="#b7">(Hoyer, Janzing, Mooij, Peters, &amp; Schölkopf, 2009;</ref><ref type="bibr">Mooij, Janzing, Peters, &amp; Schölkopf, 2009</ref>) learn a causal ordering by finding causal orders one by one either from the top downward or from the bottom upward assuming no latent confounders. We extend these ideas to latent confounder cases.</p><p>We first generalize Lemma 1 of <ref type="bibr" target="#b15">(Shimizu et al., 2011)</ref> for the case of latent confounders.</p><p>Lemma 1 Assume that all the model assumptions of the latent variable LiNGAM in Eq. ( <ref type="formula" target="#formula_2">3</ref>) are met and the sample size is infinite. Denote by r (j) i the residuals when x i are regressed on x j : r</p><formula xml:id="formula_3">(j) i = x i - cov(xi,xj )</formula><p>var(xj ) x j (i = j). Then a variable x j is an exogenous variable in the sense that it has no parent observed variable nor latent confounder if and only if x j is independent of its residuals r (j) i for all i = j.</p><p>Next, we generalize the idea of <ref type="bibr" target="#b12">(Mooij et al., 2009)</ref> for the case of latent confounders.</p><p>Lemma 2 Assume that all the model assumptions of the latent variable LiNGAM in Eq. ( <ref type="formula" target="#formula_2">3</ref>) are met and the sample size is infinite. Denote by x (-j) a vector that contains all the variables other than x j . Denote by r (-j) j the residual when x j is regressed on x (-j) , i.e., r</p><formula xml:id="formula_4">(-j) j = x j -σ T (-j)j Σ -1 (-j) x (-j)</formula><p>, where</p><formula xml:id="formula_5">Σ = σ j σ T j(-j) σ j(-j) Σ (-j)</formula><p>is the covariance matrix of [x j , x T (-j) ] T . Then a variable x j is a sink variable in the sense that it has no child observed variable nor latent confounder if and only if x (-j) is independent of its residual r</p><formula xml:id="formula_6">(-j) j .</formula><p>The proofs of these lemmas are given in the appendix. <ref type="foot" target="#foot_0">2</ref>Thus, we can take a hybrid estimation approach that uses these two principles. We first identify an exogenous variable by finding a variable that is most independent of its residuals and remove the effect of the exogenous variable from the other variables by regressing it out. We repeat this until independence between every variable and all of its residuals is statistically rejected. Dependency between every variable and any of its residuals implies that an exogenous variable as defined in Lemma 1 does not exist or some model assumption of latent variable LiNGAM in Eq. ( <ref type="formula" target="#formula_2">3</ref>) is violated. Similarly, we next identify a sink variable in the remaining variables by finding a variable such that its regressors and its residual are most independent and disregard the sink variable. We repeat this until independence is statistically rejected for every variable. <ref type="foot" target="#foot_1">3</ref>To test independence, we first evaluate pairwise independence between variables and the residuals using a kernel-based independence measure called HSIC <ref type="bibr" target="#b6">(Gretton et al., 2008)</ref> and then combine the resulting p-values p i (i = 1, • • • , c) using a well-known Fisher's method <ref type="bibr" target="#b4">(Fisher, 1950)</ref> to compute the test statistic -2 c i=1 log p i , which follows the chi-square distribution with 2c degrees of freedom when all the pairs are independent.</p><p>Since all the causal orders are not necessarily identifiable in the latent variable LiNGAM in Eq. (3) <ref type="bibr" target="#b8">(Hoyer et al., 2008)</ref>, we here aim to estimate a d × d causal ordering matrix C=[c ij ] that collects causal orderings between two variables, which is defined as</p><formula xml:id="formula_7">c ij :=        -1 if k(i) &lt; k(j) 1 if k(i) &gt; k(j)</formula><p>0 if it is unknown whether either of the two cases above (-1 or 1) is true.</p><p>(4)</p><p>Thus, the estimation consists of the following steps: 2. Let x := x and X := X and find causal orders one by one from the top downward:</p><p>(a) Do the following steps for all j ∈ U \ K head : Perform least squares regressions of xi on xj for all i ∈ U \ K head (i = j) and compute the residual vectors r(j) and the residual matrix R(j) . Then, find a variable xm that is most independent of its residuals: xm = arg max</p><formula xml:id="formula_8">j∈U\K head P F isher (x j , r(j) ),<label>(5)</label></formula><p>where P F isher (x j , r(j) ) is the p-value of the test statistic defined as (a) Do the following steps for all j ∈ U ′ \ K tail : Collect all the variables except x ′ j in a vector x ′ (-j) . Perform least squares regressions of x ′ j on x ′ (-j) and compute the residual r ′ (-j) j . Then, find such a variable x ′ m that its regressors and its residual are most independent:</p><formula xml:id="formula_9">-2 i log{P H (x j , r(j) i )}, where P H (x j , r(j) i ) is the p-value of the HSIC. (b) Go to Step 3 if P F isher (x m , r(m) ) &lt; α, i.e.,</formula><formula xml:id="formula_10">x ′ m = arg max j∈U ′ \K tail P F isher (x ′ (-j) , r ′ (-j) j ). (<label>6</label></formula><formula xml:id="formula_11">) (b) Terminate if P F isher (x ′ (-m) , r ′ (-m) m</formula><p>) &lt; α, i.e., all independencies are rejected.</p><p>(c) Append m to the top of K tail and let</p><formula xml:id="formula_12">x ′ = x ′ (-m) X ′ = X ′ (-m) . Terminate 4 if |U ′ \ K tail | &lt; 3 and otherwise go back to Step (3a).</formula><p>4. Estimate a causal ordering matrix C based on K head and K tail as follows.</p><p>Estimate c ij by -1, i.e., k(i) &lt; k(j) in either of the following cases: i) i is earlier than</p><formula xml:id="formula_13">j in K head ; ii) i is earlier than j in K tail ; iii) i is in K head and j is in K tail ; iv) i is in K head and j is neither K head nor K tail . Estimate c ij by 1, i.e., k(i) &gt; k(j) in either of the following cases: i) i is later than j in K head ; ii) i is later than j in K tail ; iii) i is in K tail and j is in K head ; iv) i is in K tail and j is neither K head nor K tail .</formula><p>Estimate c ij by 0, i.e., the ordering is unknown if i and j are neither in K tail nor K head . Note that causal orders of variables that are not in K head or K tail are no later than any in K tail and no earlier than any in K head .</p><p>OUTPUT: Ordered lists K head and K tail and a causal ordering matrix C</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A new estimation algorithm robust against latent confounders</head><p>Algorithm 1 outputs no causal orders in cases where exogenous variables and sink variables as in Lemmas 1 and 2 do not exist. For example, in the left of Fig. <ref type="figure" target="#fig_0">1</ref>, there is no such exogenous variable or sink variable that is not affected by any latent confounder since the latent confounder f 1 affects the exogenous variable x 1 and the sink variable x 4 . Therefore, Algorithm 1 would not find any causal orders. However, if we omit x 4 as in the right of Fig. <ref type="figure" target="#fig_0">1</ref> and apply Algorithm 1 on the remaining x 1 , x 2 , x 3 only, it will find all the causal orders of x 1 , x 2 , x 3 since f 1 does not affect any two of x 1 , x 2 , x 3 and is no longer a latent confounder. The same idea applies to the case that x 1 is omitted. Thus, we propose applying Algorithm 1 on every subset of variables with the size larger than one. This enables learning more causal orders than analyzing the whole set of variables if a subset of variables has exogenous variables or sink variables that are not affected by latent confounders. In practice, Algorithm 1 could give inconsistent causal orderings between a pair of variables for different subsets of variables because of estimation errors. To manage possible inconsistencies in the many causal orderings thus estimated, we rank the obtained The f 1 is a latent confounder that affects x 1 and x 4 . Right: Algorithm 1 finds the causal orders of x 1 , x 2 and x 3 if x 4 is omitted and only x 1 , x 2 and x 3 are analyzed.</p><p>causal ordering matrices by plausibility based on the statistical significances (this will be defined below). Then, considering any pair of two variables, we use the causal ordering given by the causal ordering matrix which has the highest plausibility and does contain an estimated causal ordering (i.e., the ordering was not considered unknown) between those two variables.</p><p>We evaluate the plausibility of every causal ordering matrix by the p-value of the test statistic created based on Fisher's method combining all the p-values computed to estimate the causal orders K head and K tail in Algorithm 1. A higher p-value can be considered to be more plausible. The test statistic is computed based on X, K head and K tail as follows:</p><p>-2(</p><formula xml:id="formula_14">m∈K head i:k(i)&gt;k(m) log{P H (x m , r<label>(m)</label></formula><formula xml:id="formula_15">i )} + m∈K tail i:k(i)&lt;k(m) log{P H (x ′ i , r ′ (-m) m )}),<label>(7)</label></formula><p>where</p><formula xml:id="formula_16">P H (x m , r<label>(m) i</label></formula><p>) and</p><formula xml:id="formula_17">P H (x ′ i , r ′ (-m) m</formula><p>) are the p-values computed to estimate ordered lists K head and K tail in Algorithm 1.</p><p>Thus, the estimation consists of the following steps:</p><p>Algorithm 2: Applying Algorithm 1 on every subset of variables and merging results 4. Estimate every element c ij (i = j) of a causal ordering matrix C by the causal ordering between x i and x j of the causal ordering matrix that has the highest plausibility and does contain an estimated causal ordering between x i and x j , that is, k(i) &lt; k(j) or k(j) &lt; k(i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INPUT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OUTPUT: A causal ordering matrix C</head><p>Algorithm 2 is a brute force approach since it applies Algorithm 1 on every subset (parcel) of variables. We could alleviate the computational load by first applying Algorithm 1 on the whole set of variables and then applying Algorithm 2 on the remaining variables whose causal orders have not been estimated after the effects of estimated exogenous variables are removed by regression. Thus, we finally propose the following algorithm called ParceLiNGAM: 2. Apply Algorithm 1 on X using the threshold α to estimate K head and K tail and update C. 4. Collect variables x j with j ∈ U res in a vector x res . Collect variables x j with j ∈ K head in a vector x head . Perform least squares regressions of x head on the i-th element of x res for all i ∈ U res and collect the residuals in the residual matrix R res whose i-th row is given by the residuals regressed on x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Let</head><p>5. Apply Algorithm 2 on R res using the threshold α to estimate C res . Replace every c ij (i = j) of C by the corresponding element of C res if c ij is zero and the corresponding element of C res is 1 or -1.</p><p>6. Estimate connection strengths b ij if all the non-descendants of x i are estimated, i.e., the i-th row of C has no zero. This can be done by doing multiple regression of x i on all of its non-descendants x j with k(j) &lt; k(i).</p><p>OUTPUT: A causal ordering matrix C and a set of estimated connection strength b ij .</p><p>In cases of no latent confounders, Algorithm 3 is essentially equivalent to DirectLiNGAM <ref type="bibr" target="#b15">(Shimizu et al., 2011)</ref>. Matlab codes for performing Algorithm 3 are available at <ref type="url" target="http://www.ar.sanken.osaka-u.ac.jp/~sshimizu/code/Plingamcode.html">http://www.ar.sanken.osaka-u.ac.jp/ ~sshimizu/code/Plingamcode.html</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments on artificial data</head><p>We compared our method with two estimation methods for LiNGAM in Eq. ( <ref type="formula" target="#formula_1">2</ref>) called ICA-LiNGAM <ref type="bibr" target="#b14">(Shimizu et al., 2006)</ref> and DirectLiNGAM <ref type="bibr" target="#b15">(Shimizu et al., 2011)</ref> that do not allow latent confounders and an estimation method for latent variable LiNGAM in Eq. ( <ref type="formula" target="#formula_2">3</ref>) called Pairwise LvLiNGAM <ref type="bibr" target="#b3">(Entner &amp; Hoyer, 2011)</ref>. If there are no latent confounders, all the methods should estimate correct causal orders for large enough sample sizes. The numbers of variables were 5, 10, and 15, and the sample sizes tested were 500, 1000, and 1500. The original networks used were shown in Fig. <ref type="figure">2</ref> to Fig. <ref type="figure">4</ref>. The e 1 , e 4 , e 7 , e 10 , e 13 , f 1 and f 4 followed a multimodal asymmetric mixture of two Gaussians, e 2 , e 5 , e 8 , e 11 , e 14 , f 2 and f 5 followed a double exponential distribution, and e 3 , e 6 , e 9 , e 12 , e 15 , f 3 and f 6 followed a multimodal symmetric mixture of two Gaussians. The variances of the e i were set so that var(e i )/var(x i )=1/2. We permuted the variables according to a random ordering. The number of trials was 100. The significance level α was 0.05. First, to evaluate performance of estimating causal orders k(i), we computed the percentage of correctly estimated causal orders among estimated causal orders between two variables (Precision) and the percentage of correctly estimated causal orders among actual causal orders between two variables (Recall). We also computed the F-measure defined as 2 × Precision × Recall/(Precision + Recall), which is the harmonic mean of Precision and Recall. The reason why only pairwise causal orders were evaluated was that Pairwise LvLiNGAM only estimates causal orders of two variables unlike our method and DirectLiNGAM. Tables 1, 2 and 3 show the results. Regarding recalls and F-measures, the maximal performances when no statistical errors occur are also shown in the right-most columns. For example in Fig. <ref type="figure">2</ref>, Pairwise LvLiNGAM can find all the causal orderings except k(2) &lt; k(4), k(2) &lt; k(5), k(3) &lt; k(4) and k(3) &lt; k(5). ParceLiNGAM further can find k(2) &lt; k(5) and k(3) &lt; k(5) since it estimates causal orderings between more than two variables. In some cases, the empirical recalls and F-measures were higher than their maximal performances. This is because causal orders of some variables that are affected by latent confounders happened to be correctly estimated. Regarding precisions and F-measures, our method ParceLiNGAM worked best for all the conditions. Regarding recalls, ParceLiNGAM worked best for most conditions and was the second-best but comparable to the best method DirectLiNGAM for the other conditions.</p><formula xml:id="formula_18">0.5 -0.7 -0.5 0.8 1.2 -0.8 1.2 1 1.2</formula><p>Next, to evaluate the performance in estimating connection strengths b ij , we computed the root mean square errors between true connection strengths and estimated ones. Note that Pairwise LvLiNGAM does not estimate b ij . Table <ref type="table" target="#tab_4">4</ref> show the results. Our method was most accurate for all the conditions.</p><p>Table <ref type="table" target="#tab_5">5</ref> shows average computation times. The amount out computation of our ParceLiNGAM was larger than the other methods when the sample size was increased. However, its amount of computation can be considered to be still tractable. For larger numbers of variables, we would need to select a subset of variables to decrease the number of variables to be analyzed. However, this selection does not bias results of our method since it allows latent confounders.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments on simulated fMRI data</head><p>Finally, we tested our method on simulated functional magnetic resonance imaging (fMRI) data generated in <ref type="bibr" target="#b17">(Smith et al., 2011)</ref> based on a well-known mathematical brain model called the dynamic causal modeling <ref type="bibr">(Friston, Harrison</ref>, &amp; Penny,    <ref type="bibr">2003)</ref>. We used Simulation 2 data and Simulation 6 data. Both datasets consisted of 10 variables whose causal structure is shown in Fig. <ref type="figure" target="#fig_6">5</ref>. The session durations were 10 minutes (200 time points) and 60 minutes (1200 time points), respectively. We also created a dataset of 30 minutes (600 time points) by taking the first half of Simulation 6 data. Although these data are time-series, we did not add lag-based approaches including vector autoregressive models into comparison as in <ref type="bibr" target="#b10">(Hyvärinen &amp; Smith, 2013)</ref> since it was shown by <ref type="bibr" target="#b17">(Smith et al., 2011)</ref> that lag-based methods worked poorly on these Simulation 2 data and Simulation 6 data.</p><p>For each of the three different duration settings, we gave the 50 datasets (one by one) to ParceLiNGAM, PairwiseLvLiNGAM, DirectLiNGAM and ICA-LiNGAM after omitting x 1 to create a latent confounder and randomly permuting the other variables. Table <ref type="table" target="#tab_6">6</ref> shows the precision, recalls, and F-measures of causal orders. Regarding precisions, we excluded such variable pairs x i and x j that one has no directed path to the other, e.g., x 2 and x 6 , since both k(i) &lt; k(j) and k(i) &gt; k(j) are correct. This was because estimation of causal directions is the main topic of this paper. The significance level α was 0.05. For all of the cases, ParceLiNGAM worked better than the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We proposed a new algorithm for learning causal orders, which is robust against latent confounders. In experiments on artificial data and simulated fMRI data, our methods learned more causal orders correctly than existing methods. An important problem for future research is to develop computationally more efficient algorithms. One approach might be to develop a divide-and-conquer algorithm that divides variables into subsets with moderate numbers of variables and integrates the estimation results on the subsets.</p><p>Proof of Lemma 1 i) Assume that x j has at least one parent observed variable or latent confounder. Let P j denote the set of the parent variables of x j . Then one can write x j = p h ∈Pj w jh p h +e j , where the parent variables p h are independent of e j and the coefficients w jh are non-zero. Let a vector x Pj and a column vector w Pj collect all the variables in P j and the corresponding connection strengths, respectively. Then, the covariances between x Pj and x j are E(x Pj x j ) = E{x Pj (w T Pj x Pj + e j )} = E(x Pj x T Pj )w Pj . The covariance matrix E(x Pj x T Pj ) is positive definite since the external influences and latent confounders are mutually independent and have positive variances. Thus, the covariance vector E(x Pj x j ) = E(x Pj x T Pj )w Pj above cannot equal the zero vector, and there must be at least one variable in P j with which x j covaries.</p><p>i-a) Suppose that x i is a parent of x j in P j that covaries with x j . For such x i , we have</p><formula xml:id="formula_19">r (j) i = x i - cov(x i , x j ) var(x j ) x j (8) = x i - cov(x i , x j ) var(x j ) ( p h ∈Pj w jh p h + e j )<label>(9)</label></formula><formula xml:id="formula_20">= 1 - w ji cov(x i , x j ) var(x j ) x i - cov(x i , x j ) var(x j ) p h ∈Pj ,p h =xi w jh p h - cov(x i , x j ) var(x j ) e j .<label>(10)</label></formula><p>Each of those parent variables (including x i ) in P j is a linear combination of external influences other than e j and latent confounders that are non-Gaussian and independent. Thus, the r (j) i and x j can be written as linear combinations of non-Gaussian and independent external influences including e j and latent confounders. Further, the coefficient of e j on r (j) i is non-zero since cov(x i , x j ) = 0 aforementioned and that on x j is one by definition. These imply that r (j) i and x j are dependent since r (j) i , x j and e j correspond to y 1 , y 2 , s j in D-S theorem, respectively.</p><p>i-b) Next, suppose that x j has a latent confounder f k in P j that covaries with x j . The latent confounder f k should have a non-zero coefficient on at least one other observed variable x i . Without loss of generality, it is enough to consider two observed variable cases that we only observe x i and x j :</p><formula xml:id="formula_21">x i = b ij x j + λ ik f k + e i + h =k λ ih f h (11) x j = b ji x i + λ jk f k + e j + l =k λ il f l ,<label>(12)</label></formula><p>where λ ik and λ jk are non-zero since f k is a latent confounder of x i and x j . Since the model is acyclic, b ij b ji = 0.</p><p>First, suppose that b ij is zero. Then, we have r (j) i = x i -cov(x i , x j ) var(x j )</p><p>x j (13) = {λ ik -cov(x i , x j ) var(x j ) (b ji λ ik + λ jk )}f k +{1 -cov(x i , x j ) var(x j ) b ji }e i -cov(x i , x j ) var(x j ) e j + D 1 ,</p><p>where D 1 is a linear combinations of non-Gaussian and independent latent confounders other than f k . If cov(x i , x j ) is zero, the coefficient of f k on r (j) i is λ ik and is non-zero. If cov(x i , x j ) is non-zero, the coefficient of e j on r (j) i is cov(xi,xj) var(xj) and is non-zero. Thus, in both of the cases, r (j) i and x j are dependent due to D-S theorem. Remember that the coefficient of e j on x j is one by definition.</p><p>Next, suppose that b ji is zero. Then, we have</p><formula xml:id="formula_23">r (j) i = x i - cov(x i , x j ) var(x j ) x j<label>(15)</label></formula><p>= {(b ij λ jk + λ ik ) -cov(x i , x j ) var(x j ) λ jk }f k +e i + (b ij -cov(x i , x j ) var(x j ) )e j + D 2 , (16</p><formula xml:id="formula_24">)</formula><p>where D 2 is a linear combinations of non-Gaussian and independent latent confounders other than f k . If cov(x i , x j ) is zero and b ij is zero, the coefficient of f k on r (j) i is λ ik and is non-zero. If cov(x i , x j ) is zero and b ij is non-zero, the coefficient of e j on r (j) i is b ij and is non-zero. If cov(x i , x j ) is non-zero and b ij is zero, the coefficient of e j on r (j) i is -cov(xi,xj ) var(xj ) and is non-zero. If cov(x i , x j ) is non-zero and b ij is non-zero, either of the followings holds: a) the coefficient of e j on r (j) i is non-zero, that is, b ij = cov(x i , x j )/var(x j ) or b) the coefficient of e j on r (j) i is zero and hence the coefficient of f k on r (j) i is λ ik and is non-zero. Thus, in all of the cases, r (j) i and x j are dependent due to D-S theorem.</p><p>ii) The converse of contrapositive of i) is straightforward using the model definition. From i) and ii), the lemma is proven.</p><p>Proof of Lemma 2 i) Assume that a variable x j has at least one child observed variable or latent confounder. First, without loss of generality, one can write x =</p><p>x j x (-j) = (I -B) -1 (Λf + e) = A(Λf + e)</p><p>= 1 a T j(-j) a (-j)j A (-j)</p><p>λ λ λ T j f + e j Λ (-j) f + e (-j) ,</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Hybrid estimation of causal orders of variables that are not affected by latent confounders INPUT: Data matrix X and a threshold α 1. Given a d-dimensional random vector x, a d×n data matrix of the random vector as X and a significance level α, define U as the set of variable indices of x, i.e., {1, • • • , d} and initialize an ordered list of variables K head := ∅ and K tail := ∅ and m := 1. K head and K tail denote the first |K head | variable indices and the last |K tail | variable indices respectively, where each of |K head | and |K tail | denotes the number of elements in the list.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Append m to the end of K head and let x := r(m) and X := R(m) . If |K head | = d -1, append the remaining variable index to the end of K head and terminate. Otherwise, go back to Step (2a).3. If |Khead | &lt; d -2, let x ′ =x and X ′ = X and U ′ := U \ K head and find causal orders one by one from the bottom upward 4 :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left: An example graph where Algorithm 1 finds no causal orders.The f 1 is a latent confounder that affects x 1 and x 4 . Right: Algorithm 1 finds the causal orders of x 1 , x 2 and x 3 if x 4 is omitted and only x 1 , x 2 and x 3 are analyzed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>:</head><label></label><figDesc>Data matrix X and a threshold α 1. Take all the l-combinations of variable indices {1, • • • , d} for l = 2, • • • , d. Denote the subsets of variable indices by U (s) subset (s = 1, • • • , S) and the corresponding data matrices by X (s)subset (s = 1, • • • , S), where S is the number of the subsets.2. Apply Algorithm 1 on X (s) subset using the threshold α to estimate K tail and C (s) for all s ∈ {1, • • • , S}, where K subset and C (s) is a causal ordering matrix of Subset U (s) subset .3. Compute the p-value of the test statistic in Eq. (7) to evaluate the plausibility of C (s) for all s ∈ {1, • • • , S}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 3 :</head><label>3</label><figDesc>The ParceLiNGAM algorithm INPUT: Data matrix X and a threshold α 1. Given a d-dimensional random vector x and a d × n data matrix of the random vector as X, define U as the set of variable indices of x, i.e., {1, • • • , d}. initialize a d × d causal ordering matrix C by the zero matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure</head><label></label><figDesc>Figure 2: 5 variable network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: The network used in the simulated fMRI experiments. We omitted x 1 to create a latent confounder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>U res := U \ (K head K tail). Denote by C res the corresponding causal ordering matrix. Denote by |U res | the number of elements in U res . Go to Step 6 if |U res | ≤ 2.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Precisions</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Sample size</cell></row><row><cell></cell><cell></cell><cell>500</cell><cell>1000</cell><cell>1500</cell></row><row><cell>ParceLiNGAM</cell><cell>dim.=5</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell></cell><cell>dim.=10</cell><cell>0.81</cell><cell>0.88</cell><cell>0.93</cell></row><row><cell></cell><cell>dim.=15</cell><cell>0.81</cell><cell>0.89</cell><cell>0.92</cell></row><row><cell>PairwiseLvLiNGAM</cell><cell>dim.=5</cell><cell>0.87</cell><cell>0.94</cell><cell>0.94</cell></row><row><cell></cell><cell>dim.=10</cell><cell>0.75</cell><cell>0.79</cell><cell>0.81</cell></row><row><cell></cell><cell>dim.=15</cell><cell>0.67</cell><cell>0.76</cell><cell>0.75</cell></row><row><cell>DirectLiNGAM</cell><cell>dim.=5</cell><cell>0.82</cell><cell>0.88</cell><cell>0.85</cell></row><row><cell></cell><cell>dim.=10</cell><cell>0.59</cell><cell>0.71</cell><cell>0.73</cell></row><row><cell></cell><cell>dim.=15</cell><cell>0.78</cell><cell>0.80</cell><cell>0.82</cell></row><row><cell>ICA-LiNGAM</cell><cell>dim.=5</cell><cell>0.80</cell><cell>0.75</cell><cell>0.76</cell></row><row><cell></cell><cell>dim.=10</cell><cell>0.62</cell><cell>0.62</cell><cell>0.58</cell></row><row><cell></cell><cell>dim.=15</cell><cell>0.58</cell><cell>0.59</cell><cell>0.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Recalls</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Sample size</cell><cell>Max. performance</cell></row><row><cell></cell><cell></cell><cell>500</cell><cell>1000</cell><cell>1500</cell><cell></cell></row><row><cell>ParceLiNGAM</cell><cell>dim.=5</cell><cell>0.86</cell><cell>0.82</cell><cell>0.80</cell><cell>0.80(8/10)</cell></row><row><cell></cell><cell>dim.=10</cell><cell>0.79</cell><cell>0.85</cell><cell>0.91</cell><cell>0.91(41/45)</cell></row><row><cell></cell><cell>dim.=15</cell><cell>0.80</cell><cell>0.87</cell><cell>0.89</cell><cell>0.94(99/105)</cell></row><row><cell>PairwiseLvLiNGAM</cell><cell>dim.=5</cell><cell>0.65</cell><cell>0.62</cell><cell>0.59</cell><cell>0.60(6/10)</cell></row><row><cell></cell><cell>dim.=10</cell><cell>0.50</cell><cell>0.55</cell><cell>0.54</cell><cell>0.49(22/45)</cell></row><row><cell></cell><cell>dim.=15</cell><cell>0.39</cell><cell>0.45</cell><cell>0.43</cell><cell>0.46(48/105)</cell></row><row><cell>DirectLiNGAM</cell><cell>dim.=5</cell><cell>0.82</cell><cell>0.88</cell><cell>0.85</cell><cell>-</cell></row><row><cell></cell><cell>dim.=10</cell><cell>0.59</cell><cell>0.71</cell><cell>0.73</cell><cell>-</cell></row><row><cell></cell><cell>dim.=15</cell><cell>0.78</cell><cell>0.80</cell><cell>0.82</cell><cell>-</cell></row><row><cell>ICA-LiNGAM</cell><cell>dim.=5</cell><cell>0.80</cell><cell>0.75</cell><cell>0.76</cell><cell>-</cell></row><row><cell></cell><cell>dim.=10</cell><cell>0.62</cell><cell>0.62</cell><cell>0.58</cell><cell>-</cell></row><row><cell></cell><cell>dim.=15</cell><cell>0.58</cell><cell>0.59</cell><cell>0.58</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>F-measures</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Sample size</cell><cell>Max. performance</cell></row><row><cell></cell><cell></cell><cell>500</cell><cell>1000</cell><cell>1500</cell><cell></cell></row><row><cell>ParceLiNGAM</cell><cell>dim.=5</cell><cell>0.92</cell><cell>0.90</cell><cell>0.89</cell><cell>0.89</cell></row><row><cell></cell><cell>dim.=10</cell><cell>0.80</cell><cell>0.86</cell><cell>0.92</cell><cell>0.95</cell></row><row><cell></cell><cell>dim.=15</cell><cell>0.81</cell><cell>0.88</cell><cell>0.90</cell><cell>0.97</cell></row><row><cell>PairwiseLvLiNGAM</cell><cell>dim.=5</cell><cell>0.75</cell><cell>0.75</cell><cell>0.72</cell><cell>0.75</cell></row><row><cell></cell><cell>dim.=10</cell><cell>0.60</cell><cell>0.65</cell><cell>0.65</cell><cell>0.66</cell></row><row><cell></cell><cell>dim.=15</cell><cell>0.49</cell><cell>0.56</cell><cell>0.54</cell><cell>0.63</cell></row><row><cell>DirectLiNGAM</cell><cell>dim.=5</cell><cell>0.82</cell><cell>0.88</cell><cell>0.85</cell><cell>-</cell></row><row><cell></cell><cell>dim.=10</cell><cell>0.59</cell><cell>0.71</cell><cell>0.73</cell><cell>-</cell></row><row><cell></cell><cell>dim.=15</cell><cell>0.78</cell><cell>0.80</cell><cell>0.82</cell><cell>-</cell></row><row><cell>ICA-LiNGAM</cell><cell>dim.=5</cell><cell>0.80</cell><cell>0.75</cell><cell>0.76</cell><cell>-</cell></row><row><cell></cell><cell>dim.=10</cell><cell>0.62</cell><cell>0.62</cell><cell>0.58</cell><cell>-</cell></row><row><cell></cell><cell>dim.=15</cell><cell>0.58</cell><cell>0.59</cell><cell>0.58</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Root Mean Square Errors</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Sample size</cell><cell></cell></row><row><cell></cell><cell></cell><cell>500</cell><cell>1000</cell><cell>1500</cell></row><row><cell>ParceLiNGAM</cell><cell>dim.=5</cell><cell>0.030</cell><cell>0.020</cell><cell>0.016</cell></row><row><cell></cell><cell>dim.=10</cell><cell>0.078</cell><cell>0.060</cell><cell>0.052</cell></row><row><cell></cell><cell>dim.=15</cell><cell>0.083</cell><cell>0.046</cell><cell>0.031</cell></row><row><cell>DirectLiNGAM</cell><cell>dim.=5</cell><cell>0.22</cell><cell>0.16</cell><cell>0.18</cell></row><row><cell></cell><cell>dim.=10</cell><cell>0.16</cell><cell>0.083</cell><cell>0.089</cell></row><row><cell></cell><cell>dim.=15</cell><cell>0.096</cell><cell>0.074</cell><cell>0.070</cell></row><row><cell>ICA-LiNGAM</cell><cell>dim.=5</cell><cell>0.11</cell><cell>0.11</cell><cell>0.10</cell></row><row><cell></cell><cell>dim.=10</cell><cell>0.16</cell><cell>0.15</cell><cell>0.15</cell></row><row><cell></cell><cell>dim.=15</cell><cell>0.16</cell><cell>0.14</cell><cell>0.13</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Computational Times</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Sample size</cell><cell></cell></row><row><cell></cell><cell></cell><cell>500</cell><cell>1000</cell><cell>1500</cell></row><row><cell>ParceLiNGAM</cell><cell>dim.=5</cell><cell>0.66 sec.</cell><cell>1.7 sec.</cell><cell>4.4 sec.</cell></row><row><cell></cell><cell>dim.=10</cell><cell>10 sec.</cell><cell>1.5 min.</cell><cell>8.1 min.</cell></row><row><cell></cell><cell>dim.=15</cell><cell>8.5 min.</cell><cell>5.3 hrs.</cell><cell>19 hrs.</cell></row><row><cell>PairwiseLvLiNGAM</cell><cell>dim.=5</cell><cell>0.64 sec.</cell><cell>2.6 sec.</cell><cell>7.0 sec.</cell></row><row><cell></cell><cell>dim.=10</cell><cell>2.8 sec.</cell><cell>12 sec.</cell><cell>30 sec.</cell></row><row><cell></cell><cell>dim.=15</cell><cell>6.6 sec.</cell><cell>29 sec.</cell><cell>74 sec.</cell></row><row><cell>DirectLiNGAM</cell><cell>dim.=5</cell><cell>0.23 sec.</cell><cell>0.84 sec.</cell><cell>1.2 sec.</cell></row><row><cell></cell><cell>dim.=10</cell><cell>1.7 sec.</cell><cell>7.3 sec.</cell><cell>11 sec.</cell></row><row><cell></cell><cell>dim.=15</cell><cell>6.4 sec.</cell><cell>29 sec.</cell><cell>44 sec.</cell></row><row><cell>ICA-LiNGAM</cell><cell>dim.=5</cell><cell>0.12 sec.</cell><cell>0.051 sec.</cell><cell>0.047 sec.</cell></row><row><cell></cell><cell>dim.=10</cell><cell>0.34 sec.</cell><cell>0.18 sec.</cell><cell>0.10 sec.</cell></row><row><cell></cell><cell>dim.=15</cell><cell>0.81 sec.</cell><cell>0.68 sec.</cell><cell>0.53 sec.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Results on simulated fMRI data</figDesc><table><row><cell></cell><cell></cell><cell>sim2 (10 min.)</cell><cell>sim6 (30 min.)</cell><cell>sim6 (60 min.)</cell></row><row><cell>ParceLiNGAM</cell><cell>Precision</cell><cell>0.54</cell><cell>0.56</cell><cell>0.60</cell></row><row><cell></cell><cell>Recall</cell><cell>0.53</cell><cell>0.55</cell><cell>0.58</cell></row><row><cell></cell><cell>F-measure</cell><cell>0.53</cell><cell>0.55</cell><cell>0.59</cell></row><row><cell>PairwiseLvLiNGAM</cell><cell>Precision</cell><cell>0.31</cell><cell>0.25</cell><cell>0.24</cell></row><row><cell></cell><cell>Recall</cell><cell>0.22</cell><cell>0.15</cell><cell>0.14</cell></row><row><cell></cell><cell>F-measure</cell><cell>0.26</cell><cell>0.19</cell><cell>0.18</cell></row><row><cell>DirectLiNGAM</cell><cell>Precision</cell><cell>0.50</cell><cell>0.51</cell><cell>0.45</cell></row><row><cell></cell><cell>Recall</cell><cell>0.50</cell><cell>0.51</cell><cell>0.45</cell></row><row><cell></cell><cell>F-measure</cell><cell>0.50</cell><cell>0.51</cell><cell>0.45</cell></row><row><cell>ICA-LiNGAM</cell><cell>Precision</cell><cell>0.49</cell><cell>0.47</cell><cell>0.47</cell></row><row><cell></cell><cell>Recall</cell><cell>0.49</cell><cell>0.47</cell><cell>0.47</cell></row><row><cell></cell><cell>F-measure</cell><cell>0.49</cell><cell>0.47</cell><cell>0.47</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We prove the lemmas without assuming the faithfulness<ref type="bibr" target="#b18">(Spirtes et al., 1993)</ref> unlike our previous work<ref type="bibr" target="#b19">(Tashiro et al., 2012)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>The issue of multiple comparisons arises in this context, which we would like to study in future work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>We do not examine remaining two variables in this step since it is already implied in Step 2 that some latent confounders exist. If there were no latent confounders between the remaining two, their causal orders would have already been estimated in Step 2.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments.</head><p>S.S and T.W. were supported by <rs type="institution">KAKENHI</rs> #<rs type="grantNumber">24700275</rs> and #<rs type="grantNumber">22300054</rs>. We thank <rs type="person">Patrik Hoyer</rs> and <rs type="person">Doris Entner</rs> for helpful comments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8q2P5dC">
					<idno type="grant-number">24700275</idno>
				</org>
				<org type="funding" xml:id="_vxcm77A">
					<idno type="grant-number">22300054</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We first give Darmois-Skitovitch theorem <ref type="bibr" target="#b2">(Darmois, 1953;</ref><ref type="bibr" target="#b16">Skitovitch, 1953)</ref>:</p><p>Theorem 1 (Darmois-Skitovitch theorem (D-S theorem)) Define two random variables y 1 and y 2 as linear combinations of independent random variables s i (i=1, • • • , q): y 1 = q i=1 α i s i , y 2 = q i=1 β i s i . Then, if y 1 and y 2 are independent, all variables s j for which α j β j = 0 are Gaussian.</p><p>In other words, this theorem means that if there exists a non-Gaussian s j for which α j β j =0, y 1 and y 2 are dependent.</p><p>where each of A (= (I -B) -1 ) and A (-j) is invertible and can be permuted to be a lower triangular matrix with the diagonal elements being ones if the rows and columns are simultaneously permuted according to the causal ordering k(i).</p><p>The same applies to the inverse of A:</p><p>where</p><p>In Eq.( <ref type="formula">22</ref>), if a T j(-j)σ T (-j)j Σ -1 (-j) A (-j) = 0 T , then we have</p><p>Thus, the coefficient of e j on r (-j) j is one. Now, suppose that x j has a child</p><p>x i . If the coefficient of e j on x i is non-zero, r (-j) j and x (-j) are dependent due to D-S theorem. Even if it is zero, i.e., cancelled out to be zero by special parameter values of the connection strengths, the coefficient of e j on at least one other variable in x (-j) is non-zero since there must be such an observed variable to cancel out the coefficient of e j on x i to be zero. It implies that r (-j) j and x (-j) are dependent due to D-S theorem. Next, suppose that x j has a latent confounder f i . Then, in Eq.( <ref type="formula">24</ref>), the corresponding element in λ λ λ j is not zero, i.e., the coefficient of f i on r (-j) j is not zero. Further, f i has a nonzero coefficient on at least one variable in x (-j) due to the definition of latent confounders. Therefore, r (-j) j and x (-j) are dependent due to D-S theorem. On the other hand, in Eq.( <ref type="formula">22</ref>), if a T j(-j)σ T (-j)j Σ -1 (-j) A (-j) = 0 T , at least one of the coefficients of the elements in e (-j) on r (-j) j is not zero. By definition, every element in e (-j) has a non-zero coefficient on the corresponding element in x (-j) , Thus, r (-j) j and x (-j) are dependent due to D-S theorem.</p><p>ii) The converse of contrapositive of i) is straightforward using the model definition. From i) and ii), the lemma is proven.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Bollen</surname></persName>
		</author>
		<title level="m">Structural equations with latent variables</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Causality in linear nongaussian acyclic models in the presence of latent gaussian confounders</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>(In press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analyse générale des liaisons stochastiques</title>
		<author>
			<persName><forename type="first">G</forename><surname>Darmois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of the International Statistical Institute</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="2" to="8" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discovering unconfounded causal relationships using linear non-gaussian models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Entner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New Frontiers in Artificial Intelligence</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6797</biblScope>
			<biblScope unit="page" from="181" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Statistical methods for research workers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950">1950</date>
			<publisher>Oliver and Boyd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic causal modelling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Penny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1273" to="1302" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A kernel statistical test of independence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonlinear causal discovery with additive noise models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Estimation of causal effects using linear non-gaussian causal models with hidden variables</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerminen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palviainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="362" to="378" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Independent component analysis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pairwise likelihood ratios for estimation of non-gaussian structural equation models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="111" to="152" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning overcomplete representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lewicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="365" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Regression by dependence minimization and its application to causal inference in additive noise models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the 26th Int. Conf. on Machine Learning</title>
		<meeting>the 26th Int. Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="745" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Causality: Models, reasoning, and inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000. 2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A linear non-gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerminen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2003" to="2030" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DirectLiNGAM: A direct method for learning a linear non-Gaussian structural equation model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Inazumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1225" to="1248" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On a property of the normal distribution</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Skitovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Doklady Akademii Nauk SSSR</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="217" to="219" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Salimi-Khorshidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network modelling methods for FMRI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="875" to="891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Causation, prediction, and search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993. 2000</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimation of causal orders in a linear non-gaussian acyclic model: a method robust against latent confounders</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tashiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Artificial Neural Networks (ICANN2012)</title>
		<meeting>Int. Conf. on Artificial Neural Networks (ICANN2012)<address><addrLine>Lausanne, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="491" to="498" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
