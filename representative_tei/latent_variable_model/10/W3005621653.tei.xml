<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A RECURRENT VARIATIONAL AUTOENCODER FOR SPEECH ENHANCEMENT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Simon</forename><surname>Leglaive</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CentraleSupélec, IETR</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Inria Grenoble Rhône-Alpes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xavier</forename><surname>Alameda-Pineda</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Inria Grenoble Rhône-Alpes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Girin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Inria Grenoble Rhône-Alpes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">GIPSA-lab</orgName>
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">Grenoble INP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Radu</forename><surname>Horaud</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Inria Grenoble Rhône-Alpes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A RECURRENT VARIATIONAL AUTOENCODER FOR SPEECH ENHANCEMENT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ICASSP40776.2020.9053164</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech enhancement</term>
					<term>recurrent variational autoencoders</term>
					<term>nonnegative matrix factorization</term>
					<term>variational inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a generative approach to speech enhancement based on a recurrent variational autoencoder (RVAE). The deep generative speech model is trained using clean speech signals only, and it is combined with a nonnegative matrix factorization noise model for speech enhancement. We propose a variational expectationmaximization algorithm where the encoder of the RVAE is finetuned at test time, to approximate the distribution of the latent variables given the noisy speech observations. Compared with previous approaches based on feed-forward fully-connected architectures, the proposed recurrent deep generative speech model induces a posterior temporal dynamic over the latent variables, which is shown to improve the speech enhancement results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Speech enhancement is an important problem in audio signal processing <ref type="bibr" target="#b1">[1]</ref>. The objective is to recover a clean speech signal from a noisy mixture signal. In this work, we focus on single-channel (i.e. single-microphone) speech enhancement.</p><p>Discriminative approaches based on deep neural networks have been extensively used for speech enhancement. They try to estimate a clean speech spectrogram or a time-frequency mask from a noisy speech spectrogram, see e.g. <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr">6]</ref>. Recently, deep generative speech models based on variational autoencoders (VAEs) <ref type="bibr" target="#b7">[7]</ref> have been investigated for single-channel <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11]</ref> and multichannel speech enhancement <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref>. A pre-trained deep generative speech model is combined with a nonnegative matrix factorization (NMF) <ref type="bibr" target="#b15">[15]</ref> noise model whose parameters are estimated at test time, from the observation of the noisy mixture signal only. Compared with discriminative approaches, these generative methods do not require pairs of clean and noisy speech signal for training. This setting was referred to as "semi-supervised source separation" in previous works <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18]</ref>, which should not be confused with the supervised/unsupervised terminology of machine learning.</p><p>To the best of our knowledge, the aforementioned works on VAE-based deep generative models for speech enhancement have only considered an independent modeling of the speech time frames, through the use of feed-forward and fully connected architectures. In this work, we propose a recurrent VAE (RVAE) for modeling the speech signal. The generative model is a special case of the one proposed in <ref type="bibr" target="#b19">[19]</ref>, but the inference model for training is different. At test time, we develop a variational expectation-maximization algorithm (VEM) <ref type="bibr" target="#b20">[20]</ref> to perform speech enhancement. The encoder Xavier Alameda-Pineda acknowledges the French National Research Agency (ANR) for funding the ML3RI project. This work has been partially supported by MIAI @ Grenoble Alpes, (ANR-19-P3IA-0003).</p><p>of the RVAE is fine-tuned to approximate the posterior distribution of the latent variables, given the noisy speech observations. This model induces a posterior temporal dynamic over the latent variables, which is further propagated to the speech estimate. Experimental results show that this approach outperforms its feed-forward and fully-connected counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DEEP GENERATIVE SPEECH MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Definition</head><formula xml:id="formula_0">Let s = {sn ∈ C F } N -1</formula><p>n=0 denote a sequence of short-time Fourier transform (STFT) speech time frames, and z = {zn ∈ R L } N -1 n=0 a corresponding sequence of latent random vectors. We define the following hierarchical generative speech model independently for all time frames n ∈ {0, ..., N -1}:</p><formula xml:id="formula_1">sn z ∼ Nc (0, diag {vs,n(z)}) , with zn i.i.d ∼ N (0, I) ,<label>(1)</label></formula><p>and where vs,n(z) ∈ R F + will be defined by means of a decoder neural network. N denotes the multivariate Gaussian distribution for a real-valued random vector and Nc denotes the multivariate complex proper Gaussian distribution <ref type="bibr" target="#b21">[21]</ref>. Multiple choices are possible to define the neural network corresponding to vs,n(z), which will lead to different probabilistic graphical models represented in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>FFNN generative speech model vs,n(z) = ϕ FFNN dec (zn; θdec) where ϕ FFNN dec (⋅ ; θdec) ∶ R L ↦ R F + denotes a feed-forward fullyconnected neural network (FFNN) of parameters θdec. Such an architecture was used in <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref>. As represented in Fig. <ref type="figure" target="#fig_0">1a</ref>, this model results in the following factorization of the complete-data likelihood:</p><formula xml:id="formula_2">p(s, z; θdec) = N -1 n=0 p(sn zn; θdec)p(zn).<label>(2)</label></formula><p>Note that in this case, the speech STFT time frames are not only conditionally independent, but also marginally independent, i.e. p(s; θdec) = ∏ N -1 n=0 p(sn; θdec). RNN generative speech model vs,n(z) = ϕ RNN dec,n (z0∶n; θdec) where ϕ RNN dec,n (⋅ ; θdec) ∶ R L×(n+1) ↦ R F + denotes the output at time frame n of a recurrent neural network (RNN), taking as input the sequence of latent random vectors z0∶n = {z n ′ ∈ R L } n n ′ =0 . As represented in Fig. <ref type="figure" target="#fig_0">1b</ref>, we have the following factorization of the complete-data likelihood:</p><formula xml:id="formula_3">p(s, z; θdec) = N -1 n=0 p(sn z0∶n; θdec)p(zn).<label>(3)</label></formula><p>Note that for this RNN-based model, the speech STFT time frames are still conditionally independent, but not marginally independent.</p><p>BRNN generative speech model vs,n(z) = ϕ BRNN dec,n (z; θdec) where ϕ BRNN dec,n (⋅ ; θdec) ∶ R L×N ↦ R F + denotes the output at time frame n of a bidirectional RNN (BRNN) taking as input the complete sequence of latent random vectors z. As represented in Fig. <ref type="figure" target="#fig_0">1c</ref>, we end up with the following factorization of the complete-data likelihood:</p><formula xml:id="formula_4">p(s, z; θdec) = N -1 n=0 p(sn z; θdec)p(zn).<label>(4)</label></formula><p>As for the RNN-based model, the speech STFT time frames are conditionally independent but not marginally. Note that for avoiding cluttered notations, the variance vs,n(z) in the generative speech model ( <ref type="formula" target="#formula_1">1</ref>) is not made explicitly dependent on the decoder network parameters θdec, but it clearly is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Training</head><p>We would like to estimate the decoder parameters θdec in the maximum likelihood sense, i.e. by maximizing ∑ I i=1 ln p s (i) ; θdec , where {s (i) ∈ C F ×N } I i=1 is a training dataset consisting of I i.i.d sequences of N STFT speech time frames. In the following, because it simplifies the presentation, we simply omit the sum over the I sequences and the associated subscript (i).</p><p>Due to the non-linear relationship between s and z, the marginal likelihood p(s; θdec) = ∫ p(s z; θdec)p(z)dz is analytically intractable, and it cannot be straightforwardly optimized. We therefore resort to the framework of variational autoencoders <ref type="bibr" target="#b7">[7]</ref> for parameters estimation, which builds upon stochastic fixed-form variational inference <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref>. This latter methodology first introduces a variational distribution q(z s; θenc) (or inference model) parametrized by θenc, which is an approximation of the true intractable posterior distribution p(z s; θdec). For any variational distribution, we have the following decomposition of log-marginal likelihood: ln p(s; θdec) = Ls(θenc, θdec) + DKL q(z s; θenc) ∥ p(z s; θdec) ,</p><p>(5) where Ls(θenc, θdec) is the variational free energy (VFE) (also referred to as the evidence lower bound) defined by: Ls(θenc, θdec) = E q(z s;θenc) [ln p(s, z; θdec)ln q(z s; θenc)] = E q(z s;θenc) [ln p(s z; θdec)] -DKL q(z s; θenc) ∥ p(z) , <ref type="bibr">(6)</ref> and DKL(q ∥ p) = Eq[ln qln p] is the Kullback-Leibler (KL) divergence. As the latter is always non-negative, we see from <ref type="bibr" target="#b5">(5)</ref> that the VFE is a lower bound of the intractable log-marginal likelihood. Moreover, we see that it is tight if and only if q(z s; θenc) = p(z s; θdec). Therefore, our objective is now to maximize the VFE with respect to (w.r.t) both θenc and θdec. But in order to fully define the VFE in <ref type="bibr">(6)</ref>, we have to define the form of the variational distribution q(z s; θenc).</p><p>Using the chain rule for joint distributions, the posterior distribution of the latent vectors can be exactly expressed as follows:</p><formula xml:id="formula_5">p(z s; θdec) = N -1 n=0 p(zn z0∶n-1, s; θdec),<label>(7)</label></formula><p>where we considered p(z0 z-1, s; θdec) = p(z0 s; θdec). The variational distribution q(z s; θenc) is naturally also expressed as:</p><formula xml:id="formula_6">q(z s; θenc) = N -1 n=0 q(zn z0∶n-1, s; θenc).<label>(8)</label></formula><p>In this work, q(zn z0∶n-1, s; θenc) denotes to the probability density function (pdf) of the following Gaussian inference model:</p><formula xml:id="formula_7">zn z0∶n-1, s ∼ N µ z,n (z0∶n-1, s), diag vz,n(z0∶n-1, s) ,<label>(9)</label></formula><p>where {µ z,n , vz,n}(z0∶n-1, s) ∈ R L × R L + will be defined by means of an encoder neural network.</p><p>Inference model for the BRNN generative speech model For the BRNN generative speech model, the parameters of the variational distribution in ( <ref type="formula" target="#formula_7">9</ref>) are defined by</p><formula xml:id="formula_8">{µ z,n , vz,n}(z0∶n-1, s) = ϕ BRNN enc,n (z0∶n-1, s; θenc),<label>(10)</label></formula><p>where</p><formula xml:id="formula_9">ϕ BRNN enc,n (⋅, ⋅ ; θenc) ∶ R L×n × C F ×N ↦ R L × R L +</formula><p>denotes the output at time frame n of a neural network whose parameters are denoted by θenc. It is composed of: 1. "Prediction block": a causal recurrent block processing z0∶n-1; 2. "Observation block": a bidirectional recurrent block processing the complete sequence of STFT speech time frames s;</p><p>3. "Update block": a feed-forward fully-connected block processing the outputs at time-frame n of the two previous blocks.</p><p>If we want to sample from q(z s; θenc) in ( <ref type="formula" target="#formula_6">8</ref>), we have to sample recursively each zn, starting from n = 0 up to N -1. Interestingly, the posterior is formed by running forward over the latent vectors, and both forward and backward over the input sequence of STFT speech time-frames. In other words, the latent vector at a given time frame is inferred by taking into account not only the latent vectors at the previous time steps, but also all the speech STFT frames at the current, past and future time steps. The anti-causal relationships were not taken into account in the RVAE model <ref type="bibr" target="#b19">[19]</ref>.</p><p>Inference model for the RNN generative speech model Using the fact that zn is conditionally independent of all other nodes in Fig. <ref type="figure" target="#fig_0">1b</ref> given its Markov blanket (defined as the set of parents, children and co-parents of that node) <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b7">(7)</ref> can be simplified as:</p><formula xml:id="formula_10">p(zn z0∶n-1, s; θdec) = p(zn z0∶n-1, s n∶N -1 ; θdec),<label>(11)</label></formula><p>where</p><formula xml:id="formula_11">s n∶N -1 = {s n ′ ∈ C F } N -1 n ′ =n .</formula><p>This conditional independence also applies to the variational distribution in <ref type="bibr" target="#b9">(9)</ref>, whose parameters are now given by:</p><formula xml:id="formula_12">{µ z,n , vz,n}(z0∶n-1, s) = ϕ RNN enc,n (z0∶n-1, s n∶N -1 ; θenc),<label>(12)</label></formula><p>where</p><formula xml:id="formula_13">ϕ RNN enc,n (⋅, ⋅ ; θenc) ∶ R L×n × C F ×(N -n) ↦ R L × R L</formula><p>+ denotes the same neural network as for the BRNN-based model, except that the observation block is not a bidirectional recurrent block anymore, but an anti-causal recurrent one. The full approximate posterior is now formed by running forward over the latent vectors, and backward over the input sequence of STFT speech time-frames.</p><p>Inference model for the FFNN generative speech model For the same reason as before, by studying the Markov blanket of zn in Fig. <ref type="figure" target="#fig_0">1a</ref>, the dependencies in <ref type="bibr" target="#b7">(7)</ref> can be simplified as follows: p(zn z0∶n-1, s; θdec) = p(zn sn; θdec). <ref type="bibr" target="#b13">(13)</ref> This simplification also applies to the variational distribution in <ref type="bibr" target="#b9">(9)</ref>, whose parameters are now given by:</p><formula xml:id="formula_14">{µ z,n , vz,n}(z0∶n-1, s) = ϕ FFNN enc (sn; θenc),<label>(14)</label></formula><p>where ϕ FFNN enc (⋅ ; θenc) ∶ C F ↦ R L × R L + denotes the output of an FFNN. Such an architecture was used in <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref>. This is the only case where, from the approximate posterior, we can sample all latent vectors in parallel for all time frames, without further approximation.</p><p>Here also, the mean and variance vectors in the inference model <ref type="bibr" target="#b9">(9)</ref> are not made explicitly dependent on the encoder network parameters θenc, but they clearly are.</p><p>Variational free energy Given the generative model <ref type="bibr" target="#b1">(1)</ref> and the general inference model ( <ref type="formula" target="#formula_7">9</ref>), we can develop the VFE defined in <ref type="bibr">(6)</ref> as follows (derivation details are provided in Appendix A.1):</p><formula xml:id="formula_15">Ls(θenc, θdec) c = - F -1 f =0 N -1 n=0 E q(z s;θenc) dIS s f n 2 , v s,f n (z) + 1 2 L-1 l=0 N -1 n=0 E q(z 0∶n-1 s;θenc) ln v z,ln (z0∶n-1, s) -µ 2 z,ln (z0∶n-1, s) -v z,ln (z0∶n-1, s) ,<label>(15)</label></formula><p>where c = denotes equality up to an additive constant w.r.t θenc and θdec, dIS(a, b) = a bln(a b) -1 is the Itakura-Saito (IS) divergence <ref type="bibr" target="#b15">[15]</ref>, s f n ∈ C and v s,f n (z) ∈ R+ denote respectively the f -th entries of sn and vs,n(z), and µ z,ln (z0∶n-1, s) ∈ R and v z,ln (z0∶n-1, s) ∈ R+ denote respectively the l-th entry of µ z,n (z0∶n-1, s) and vz,n(z0∶n-1, s).</p><p>The expectations in <ref type="bibr" target="#b15">(15)</ref> are analytically intractable, so we compute unbiased Monte Carlo estimates using a set {z (r) } R r=1 of i.i.d. realizations drawn from q(z s; θenc). For that purpose, we use the "reparametrization trick" introduced in <ref type="bibr" target="#b7">[7]</ref>. The obtained objective function is differentiable w.r.t to both θdec and θenc, and it can be optimized using gradient-ascent-based algorithms. Finally, we recall that in the final expression of the VFE, there should actually be an additional sum over the I i.i.d. sequences in the training dataset {s (i) } I i=1 . For stochastic or mini-batch optimization algorithms, we would only consider a subset of these training sequences for each update of the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SPEECH ENHANCEMENT: MODEL AND ALGORITHM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Speech, noise and mixture model</head><p>The deep generative clean speech model along with its parameters learning procedure were defined in the previous section. For speech enhancement, we now consider a Gaussian noise model based on an NMF parametrization of the variance <ref type="bibr" target="#b15">[15]</ref>. Independently for all time frames n ∈ {0, ..., N -1}, we have:</p><formula xml:id="formula_16">bn ∼ Nc(0, diag{v b,n }),<label>(16)</label></formula><p>where</p><formula xml:id="formula_17">v b,n = (W b H b )∶,n with W b ∈ R F ×K + and H b ∈ R K×N + .</formula><p>The noisy mixture signal is modeled as xn = √ gnsn+bn, where gn ∈ R+ is a gain parameter scaling the level of the speech signal at each time frame <ref type="bibr" target="#b9">[9]</ref>. We further consider the independence of the speech and noise signals so that the likelihood is defined by:</p><formula xml:id="formula_18">xn z ∼ Nc (0, diag{vx,n(z)}) ,<label>(17)</label></formula><p>where vx,n(z) = gnvs,n(z) + v b,n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Speech enhancement algorithm</head><p>We consider that the speech model parameters θdec which have been learned during the training stage are fixed, so we omit them in the rest of this section. We now need to estimate the remaining model parameters φ = g = [g0, ..., g N -1 ] ⊺ , W b , H b from the observation of the noisy mixture signal x = {xn ∈ C F } N -1 n=0 . However, very similarly as for the training stage (see Section 2.2), the marginal likelihood p(x; φ) is intractable, and we resort again to variational inference. The VFE at test time is defined by: Lx(θenc, φ) = E q(z x;θenc) [ln p(x z; φ)]-DKL q(z x; θenc) ∥ p(z) .</p><p>(18) Following a VEM algorithm <ref type="bibr" target="#b20">[20]</ref>, we will maximize this criterion alternatively w.r.t θenc at the E-step, and φ at the M-step. Note that here also, we have Lx(θenc, φ) ≤ ln p(x; φ) with equality if and only if q(z x; θenc) = p(z x; φ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variational E-</head><p>Step with fine-tuned encoder We consider a fixedform variational inference strategy, reusing the inference model learned during the training stage. More precisely, the variational distribution q(z x; θenc) is defined exactly as q(z s; θenc) in ( <ref type="formula" target="#formula_7">9</ref>) and (8) except that s is replaced with x. Remember that the mean and variance vectors µ z,n (⋅, ⋅) and vz,n(⋅, ⋅) in ( <ref type="formula" target="#formula_7">9</ref>) correspond to the VAE encoder network, whose parameters θenc were estimated along with the parameters θdec of the generative speech model. During the training stage, this encoder network took clean speech signals as input. It is now fine-tuned with a noisy speech signal as input. For that purpose, we maximize Lx(θenc, φ) w.r.t θenc only, with fixed φ. This criterion takes the exact same form as <ref type="bibr" target="#b15">(15)</ref> except that s f n 2 is replaced with x f n 2 where x f n ∈ C denotes the f -th entry of xn, s is replaced with x, and v s,f n (z) is replaced with v x,f n (z), the f -th entry of vx,n(z) which was defined along with <ref type="bibr" target="#b17">(17)</ref>. Exactly as in Section 2.2, intractable expectations are replaced with a Monte Carlo estimate and the VFE is maximized w.r.t. θenc by means of gradient-based optimization techniques. In summary, we use the framework of VAEs <ref type="bibr" target="#b7">[7]</ref> both at training for estimating θdec and θenc from clean speech signals, and at testing for fine-tuning θenc from the noisy speech signal, and with θdec fixed. The idea of refitting the encoder was also proposed in <ref type="bibr" target="#b28">[28]</ref> in a different context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point-estimate E-Step</head><p>In the experiments, we will compare this variational E-step with an alternative proposed in <ref type="bibr" target="#b29">[29]</ref>, which consists in relying only on a point estimate of the latent variables. In our framework, this approach can be understood as assuming that the approximate posterior q(z x; θenc) is a dirac delta function centered at the maximum a posteriori estimate z ⋆ . Maximization of p(z x; φ) ∝ p(x z; φ)p(z) w.r.t z can be achieved by means of gradient-based techniques, where backpropagation is used to compute the gradient w.r.t. the input of the generative decoder network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M-</head><p>Step For both the VEM algorithm and the point-estimate alternative, the M-Step consists in maximizing Lx(θenc, φ) w.r.t. φ under a non-negativity constraint and with θenc fixed. Replacing intractable expectations with Monte Carlo estimates, the M-step can be recast as minimizing the following criterion <ref type="bibr" target="#b9">[9]</ref>:</p><formula xml:id="formula_19">C(φ) = R r=1 F -1 f =0 N -1 n=0 dIS x f n 2 , v x,f n z (r) ,<label>(19)</label></formula><p>where v x,f n (z (r) ) implicitly depends on φ. For the VEM algorithm, {z (r) } R r=1 is a set of i.i.d. sequences drawn from q(z x; θenc) using the current value of the parameters θenc. For the point estimate approach, R = 1 and z (1) corresponds to the maximum a posteriori estimate. This optimization problem can be tackled using a majorize-minimize approach <ref type="bibr" target="#b30">[30]</ref>, which leads to the multiplicative update rules derived in <ref type="bibr" target="#b9">[9]</ref> using the methodology proposed in <ref type="bibr" target="#b31">[31]</ref> (these updates are recalled in Appendix A.2).</p><p>Speech reconstruction Given the estimated model parameters, we want to compute the posterior mean of the speech coefficients:</p><formula xml:id="formula_20">ŝfn = E p(s f n x f n ;φ) [s f n ] = E p(z x;φ) √ gnv s,f n (z) v x,f n (z) x f n . (<label>20</label></formula><formula xml:id="formula_21">)</formula><p>In practice, the speech estimate is actually given by the scaled coefficients √ gnŝ f n . Note that (20) corresponds to a Wiener-like filtering, averaged over all possible realizations of the latent variables according to their posterior distribution. As before, this expectation is intractable, but we approximate it by a Monte Carlo estimate using samples drawn from q(z x; θenc) for the VEM algorithm. For the point-estimate approach, p(z x; φ) is approximated by a dirac delta function centered at the maximum a posteriori.</p><p>In the case of the RNN-and BRNN-based generative speech models (see Section 2.1), it is important to remember that sampling from q(z x; θenc) is actually done recursively, by sampling q(zn z0∶n-1, x; θenc) from n = 0 to N -1 (see Section 2.2). Therefore, there is a posterior temporal dynamic that will be propagated from the latent vectors to the estimated speech signal, through the expectation in the Wiener-like filtering of <ref type="bibr" target="#b20">(20)</ref>. This temporal dynamic is expected to be beneficial compared with the FFNN generative speech model, where the speech estimate is built independently for all time frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS Dataset</head><p>The deep generative speech models are trained using around 25 hours of clean speech data, from the "si_tr_s" subset of the Wall Street Journal (WSJ0) dataset <ref type="bibr" target="#b32">[32]</ref>. Early stopping with a patience of 20 epochs is performed using the subset "si_dt_05" (around 2 hours of speech). We removed the trailing and leading silences for each utterance. For testing, we used around 1.5 hours of noisy speech, corresponding to 651 synthetic mixtures. The clean speech signals are taken from the "si_et_05" subset of WSJ0 (unseen speakers), and the noise signals from the "verification" subset of the QUT-NOISE dataset <ref type="bibr" target="#b33">[33]</ref>. Each mixture is created by uniformly sampling a noise type among {"café", "home", "street", "car"} and a signal-to-noise ratio (SNR) among {-5, 0, 5} dB. The intensity of each signal for creating a mixture at a given SNR is computed using the ITU-R BS.1770-4 protocol <ref type="bibr" target="#b34">[34]</ref>. Note that an SNR computed with this protocol is here 2.5 dB lower (in average) than with a simple sum of the squared signal coefficients. Finally, all signals have a 16 kHz-sampling rate, and the STFT is computed using a 64-ms sine window (i.e. F = 513) with 75%-overlap.</p><p>Network architecture and training parameters All details regarding the encoder and decoder network architectures and their training procedure are provided in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speech enhancement parameters</head><p>The dimension of the latent space for the deep generative speech model is fixed to L = 16. The rank of the NMF-based noise model is fixed to K = 8. W b and H b are randomly initialized (with a fixed seed to ensure fair comparisons), and g is initialized with an all-ones vector. For computing <ref type="bibr" target="#b19">(19)</ref>, we fix the number of samples to R = 1, which is also the case for building the Monte Carlo estimate of <ref type="bibr" target="#b20">(20)</ref>. The VEM algorithm and its "point estimate" alternative (referred to as PEEM) are run for 500 iterations. We used Adam <ref type="bibr" target="#b35">[35]</ref> with a step size of 10 the gradient-based iterative optimization technique involved at the E-step. For the FFNN deep generative speech model, it was found that an insufficient number of gradient steps had a strong negative impact on the results, so it was fixed to 10. For the (B)RNN model, this choice had a much lesser impact so it was fixed to 1, thus limiting the computational burden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We compare the performance of the VEM and PEEM algorithms for the three types of deep generative speech model. For the FFNN model only, we also compare with the Monte Carlo EM (MCEM) algorithm proposed in <ref type="bibr" target="#b9">[9]</ref> (which cannot be straightforwardly adapted to the (B)RNN model). The enhanced speech quality is evaluated in terms of scale-invariant signal-to-distortion ratio (SI-SDR) in dB <ref type="bibr" target="#b36">[36]</ref>, perceptual evaluation of speech quality (PESQ) measure (between -0.5 and 4.5) <ref type="bibr" target="#b37">[37]</ref> and extended short-time objective intelligibility (ESTOI) measure (between 0 and 1) <ref type="bibr" target="#b38">[38]</ref>. For all measures, the higher the better. The median results for all SNRs along with their confidence interval are presented in Table <ref type="table" target="#tab_0">1</ref>. Best results are in black-color-bold font, while gray-color-bold font indicates results that are not significantly different. As a reference, we also provide the results obtained with the noisy mixture signal as the speech estimate, and with oracle Wiener filtering. Note that oracle results are here particularly low, which shows the difficulty of the dataset. Oracle SI-SDR is for instance 7 dB lower than the one in <ref type="bibr" target="#b9">[9]</ref>. Therefore, the VEM and PEEM results should not be directly compared with the MCEM results provided in <ref type="bibr" target="#b9">[9]</ref>, but only with the ones provided here.</p><p>From Table <ref type="table" target="#tab_0">1</ref>, we can draw the following conclusions: First, we observe that for the FFNN model, the VEM algorithm performs poorly. In this setting, the performance measures actually strongly decrease after the first 50-to-100 iterations of the algorithm. We did not observe this behavior for the (B)RNN model. We argue that the posterior temporal dynamic over the latent variables helps the VEM algorithm finding a satisfactory estimate of the overparametrized posterior model q(z x; θenc). Second, the superiority of the RNN model over the FFNN one is confirmed for all algorithms in this comparison. However, the bidirectional model (BRNN) does not perform significantly better than the unidirectional one. Third, the VEM algorithm outperforms the PEEM one, which shows the interest of using the full (approximate) posterior distribution of the latent variables and not only the maximum-a-posteriori point estimate for estimating the noise and mixture model parameters. Audio examples and code are available online <ref type="bibr" target="#b39">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this work, we proposed a recurrent deep generative speech model and a variational EM algorithm for speech enhancement. We showed that introducing a temporal dynamic is clearly beneficial in terms of speech enhancement. Future works include developing a Markov chain EM algorithm to measure the quality of the proposed variational approximation of the intractable true posterior distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Variational free energy derivation details</head><p>In this section we give derivation details for obtaining the expression of the variational free energy in <ref type="bibr" target="#b15">(15)</ref>. We will develop the two terms involved in the definition of the variational free energy in <ref type="bibr">(6)</ref>.</p><p>Data-fidelity term From the generative model defined in (1) we have:</p><formula xml:id="formula_22">E q(z s;θenc) [ln p(s z; θdec)] = N -1 n=0 E q(z s;θenc) [ln p(sn z; θdec)] = - F -1 f =0 N -1 n=0 E q(z s;θenc) ln v s,f n (z) + s f n 2 v s,f n (z) -F N ln(π)<label>(21)</label></formula><p>Regularization term From the inference model defined in ( <ref type="formula" target="#formula_7">9</ref>) and ( <ref type="formula" target="#formula_6">8</ref>) we have:</p><formula xml:id="formula_23">DKL q(z s; θenc) ∥ p(z) = E q(z s;θenc) [ln q(z s; θenc) -ln p(z)] = N -1 n=0 E q(z s;θenc) [ln q(zn z0∶n-1, s; θenc) -ln p(zn)] = N -1 n=0 E q(z 0∶n s;θenc) [ln q(zn z0∶n-1, s; θenc) -ln p(zn)] = N -1<label>n=0</label></formula><p>E q(z 0∶n-1 s;θenc) E q(zn z 0∶n-1 ,s;θenc) ln q(zn z0∶n-1, s; θenc)</p><formula xml:id="formula_24">-ln p(zn) = N -1<label>n=0</label></formula><p>E q(z 0∶n-1 s;θenc) DKL q(zn z0∶n-1, s; θenc) ∥ p(zn)</p><formula xml:id="formula_25">= - 1 2 L-1 l=0 N -1 n=0 E q(z 0∶n-1 s;θenc) ln v z,ln (z0∶n-1, s) -µ 2 z,ln (z0∶n-1, s) -v z,ln (z0∶n-1, s) - N L 2 .<label>(22)</label></formula><p>Summing up ( <ref type="formula" target="#formula_22">21</ref>) and ( <ref type="formula" target="#formula_25">22</ref>) and recognizing the IS divergence we end up with the expression of the variational free energy in <ref type="bibr" target="#b15">(15)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Update rules for the M-step</head><p>The multiplicative update rules for minimizing (19) using a majorizeminimize technique <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31]</ref> are given by (see <ref type="bibr" target="#b9">[9]</ref> for derivation details):</p><formula xml:id="formula_26">H b ← H b ⊙ ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ W ⊺ b X ⊙2 ⊙ R ∑ r=1 V (r) x ⊙-2 W ⊺ b R ∑ r=1 V (r) x ⊙-1 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ ⊙1 2 ; (<label>23</label></formula><formula xml:id="formula_27">) W b ← W b ⊙ ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ X ⊙2 ⊙ R ∑ r=1 V (r) x ⊙-2 H ⊺ b R ∑ r=1 V (r) x ⊙-1 H ⊺ b ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ ⊙1 2 ; (<label>24</label></formula><formula xml:id="formula_28">)</formula><formula xml:id="formula_29">g ⊺ ← g ⊺ ⊙ ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ 1 ⊺ X ⊙2 ⊙ R ∑ r=1 V (r) s ⊙ V (r) x ⊙-2 1 ⊺ R ∑ r=1 V (r) s ⊙ V (r) x ⊙-1 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ ⊙1 2 ,<label>(25)</label></formula><p>where ⊙ denotes element-wise multiplication and exponentiation, matrix division is also element-wise, V</p><p>s , V</p><p>∈ R F ×N + are the matrices of entries v s,f n z (r) and v x,f n z (r) respectively, X ∈ C F ×N is the matrix of entries x f n and 1 is an all-ones column vector of dimension F . Note that non-negativity of H b , W b and g is ensured provided that they are initialized with non-negative values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Neural network architectures and training</head><p>The decoder and encoder network architectures are represented in Fig. <ref type="figure">2</ref> and Fig. <ref type="figure">3</ref> respectively. The "dense" (i.e. feed-forward fullyconnected) output layers are of dimension L = 16 and F = 513 for the encoder and decoder, respectively. The dimension of all other layers was arbitrarily fixed to 128. RNN layers correspond to long short-term memory (LSTM) ones <ref type="bibr" target="#b40">[40]</ref>. For the FFNN generative model, a batch is made of 128 time frames of clean speech power spectrogram. For the (B)RNN generative model, a batch is made 32 sequences of 50 time frames. Given an input sequence, all LSTM hidden states for the encoder and decoder networks are initialized to zero. For training, we use the Adam optimizer <ref type="bibr" target="#b35">[35]</ref> with a step size of 10 -3 , exponential decay rates of 0.9 and 0.999 for the first and second moment estimates, respectively, and an epsilon of 10 -8 for preventing division by zero. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Probabilistic graphical models for N = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Fig. 2: Decoder network architectures corresponding to the speech generative models in Fig. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>-2 for Median results and confidence intervals.</figDesc><table><row><cell>Algorithm</cell><cell>Model</cell><cell>SI-SDR (dB)</cell><cell>PESQ</cell><cell>ESTOI</cell></row><row><cell>MCEM [9]</cell><cell>FFNN</cell><cell>5.4 ± 0.4</cell><cell cols="2">2.22 ± 0.04 0.60 ± 0.01</cell></row><row><cell>PEEM</cell><cell>FFNN RNN BRNN</cell><cell>4.4 ± 0.4 5.8 ± 0.5 5.4 ± 0.5</cell><cell cols="2">2.21 ± 0.04 0.58 ± 0.01 2.33 ± 0.04 0.63 ± 0.01 2.30 ± 0.04 0.62 ± 0.01</cell></row><row><cell>VEM</cell><cell>FFNN RNN BRNN</cell><cell>4.4 ± 0.4 6.8 ± 0.4 6.9 ± 0.5</cell><cell cols="2">1.93 ± 0.05 0.53 ± 0.01 2.33 ± 0.04 0.67 ± 0.01 2.35 ± 0.04 0.67 ± 0.01</cell></row><row><cell cols="2">noisy mixture oracle Wiener filtering</cell><cell>-2.6 ± 0.5 12.1 ± 0.3</cell><cell cols="2">1.82 ± 0.03 0.49 ± 0.01 3.13 ± 0.02 0.88 ± 0.01</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
		<title level="m">Speech enhancement: theory and practice</title>
		<imprint>
			<publisher>CRC press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A regression approach to speech enhancement based on deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR</title>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Latent Variable Analysis and Signal Separation</title>
		<meeting>Int. Conf. Latent Variable Analysis and Signal Separation</meeting>
		<imprint>
			<publisher>LVA/ICA</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1702" to="1726" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Audio-noise power spectral density estimation using long short-term memory</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="918" to="922" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multichannel speech enhancement based on time-frequency masking using subband long short-term memory</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Applicat. Signal Process. Audio Acoust. (WAS-PAA)</title>
		<meeting>IEEE Workshop Applicat. Signal ess. Audio Acoust. (WAS-PAA)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
		<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistical speech enhancement based on probabilistic integration of variational autoencoder and non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Itoyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yoshii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="716" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A variance modeling framework based on variational autoencoders for speech enhancement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Workshop Machine Learning Signal Process. (MLSP)</title>
		<meeting>IEEE Int. Workshop Machine Learning Signal ess. (MLSP)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speech enhancement with variational autoencoders and alpha-stable distributions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Şimşekli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="541" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A statistically principled and computationally efficient approach to speech enhancement using variational autoencoders</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bayesian multichannel speech enhancement with a deep speech prior</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sekiguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yoshii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asia-Pacific Signal and Information Processing Association Annual Summit and Conference</title>
		<meeting>Asia-Pacific Signal and Information essing Association Annual Summit and Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1233" to="1239" />
		</imprint>
		<respStmt>
			<orgName>APSIPA ASC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised multichannel speech enhancement with variational autoencoders and non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="101" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cauchy multichannel speech enhancement with a deep speech prior</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fontaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Nugraha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yoshii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Signal Processing Conference (EUSIPCO)</title>
		<meeting>European Signal essing Conference (EUSIPCO)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization with the Itakura-Saito divergence: With application to music analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bertin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Durrieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="793" to="830" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Supervised and semisupervised separation of sounds from single-channel mixtures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shashanka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Indep. Component Analysis and Signal Separation</title>
		<meeting>Int. Conf. Indep. Component Analysis and Signal Separation</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="414" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A non-negative approach to semisupervised separation of speech from noise with the use of temporal dynamics</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supervised and unsupervised speech enhancement using nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mohammadiha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leijon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2140" to="2151" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Information Process. Syst</title>
		<meeting>Adv. Neural Information ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A view of the EM algorithm that justifies incremental, sparse, and other variants</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in Graphical Models</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Jordan</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Proper complex random processes with applications to information theory</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Neeser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Massey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1293" to="1302" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="183" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Approximate Riemannian conjugate gradient learning for fixed-form variational Bayes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Honkela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kuusela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tornio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3235" to="3268" />
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fixed-form variational posterior approximation through stochastic linear regression</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="837" to="882" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Refit your encoder when new data comes by</title>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Mattei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frellsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>in 3rd NeurIPS workshop on Bayesian Deep Learning</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Supervised determined source separation with multichannel variational autoencoder</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kameoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A tutorial on MM algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Algorithms for nonnegative matrix factorization with the β-divergence</title>
		<author>
			<persName><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Idier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2421" to="2456" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CSR-I (WSJ0) Sennheiser LDC93S6B</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pallett</surname></persName>
		</author>
		<ptr target="https://catalog.ldc.upenn.edu/LDC93S6B" />
	</analytic>
	<monogr>
		<title level="m">Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The QUT-NOISE-SRE protocol for the evaluation of noisy speaker recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanagasundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3456" to="3460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Algorithms to measure audio programme loudness and true-peak audio level</title>
	</analytic>
	<monogr>
		<title level="m">Recommendation BS.1770-4, International Telecommunication Union (ITU)</title>
		<imprint>
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
		<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SDR -Halfbaked or Well Done?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<imprint>
			<biblScope unit="page" from="749" to="752" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of time-frequency weighted noisy speech</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Companion website</title>
		<ptr target="https://sleglaive.github.io/demo-icassp2020.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
