<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How to deal with missing data in supervised deep learning?</title>
				<funder>
					<orgName type="full">Danish Innovation Foundation</orgName>
				</funder>
				<funder ref="#_QWZ4vk4">
					<orgName type="full">Independent Research Fund Denmark</orgName>
				</funder>
				<funder>
					<orgName type="full">Danish Center for Big Data Analytics driven Innovation (DABAI)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Niels</forename><forename type="middle">Bruun</forename><surname>Ipsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Applied Mathematics and Computer Science</orgName>
								<orgName type="institution">Technical University of Denmark</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Pierre-Alexandre</forename><surname>Mattei</surname></persName>
							<email>&lt;pierre-alexandre.mattei@inria.fr&gt;</email>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">Laboratoire J.A. Dieudonné</orgName>
								<orgName type="laboratory" key="lab2">UMR CNRS 7351</orgName>
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria (Maasai project-team)</orgName>
								<address>
									<settlement>Jes Frellsen</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jes</forename><surname>Frellsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Applied Mathematics and Computer Science</orgName>
								<orgName type="institution">Technical University of Denmark</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">How to deal with missing data in supervised deep learning?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The issue of missing data in supervised learning has been largely overlooked, especially in the deep learning community. We investigate strategies to adapt neural architectures to handle missing values. Here, we focus on regression and classification problems where the features are assumed to be missing at random. Of particular interest are schemes that allow to reuse as-is a neural discriminative architecture. One scheme involves imputing the missing values with learnable constants. We propose a second novel approach that leverages recent advances in deep generative modelling. More precisely, a deep latent variable model can be learned jointly with the discriminative model, using importance-weighted variational inference in an end-to-end way. This hybrid approach, which mimics multiple imputation, also allows to impute the data, by relying on both the discriminative and the generative model. We also discuss ways of using a pre-trained generative model to train the discriminative one. In domains where powerful deep generative models are available, the hybrid approach leads to large performance gains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Missing data affects data analysis across a wide range of domains and the sources of missing spans an equally wide range. Recently deep latent variable models (DLVMs, <ref type="bibr" target="#b4">Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b12">Rezende et al., 2014)</ref> have been applied to missing data problems in an unsupervised setting (e.g. <ref type="bibr" target="#b12">Rezende et al., 2014;</ref><ref type="bibr" target="#b11">Nazabal et al., 2018;</ref><ref type="bibr" target="#b7">Ma et al., 2018;</ref><ref type="bibr">2019;</ref><ref type="bibr" target="#b2">Ivanov et al., 2019;</ref><ref type="bibr" target="#b9">Mattei &amp; Frellsen, 2018;</ref><ref type="bibr">2019;</ref><ref type="bibr" target="#b17">Yoon et al., 2018;</ref><ref type="bibr" target="#b1">Ipsen et al., 2020)</ref>, while the supervised setting has not seen the same recent attention. The progress in the unsupervised setting is focused on inference and imputation in a joint model over features with missing values and can be useful as an imputation step before a discriminative model. However, this approach is not necessarily optimal in terms of minimizing a prediction error.</p><p>We propose and investigate strategies for handling missing data in the supervised learning setting, while keeping any existing discriminative neural architecture as is, by inspecting how learning curves depend on the chosen strategy. Our main contribution is a joint DLVM and discriminative model that can be trained using importance weighted variational inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Previous work</head><p>A recent attempt to handle missing data in discriminative models was done by <ref type="bibr">Śmieja et al. (2018)</ref>, where a Gaussian mixture model (GMM) is used as a preamble to a discriminative neural network. The GMM and discriminative model are trained jointly, and in place of any missing values the activation of the corresponding input neuron is set to the average activation over the GMM conditioned on observed values. <ref type="bibr" target="#b16">Yi et al. (2019)</ref> tackled the issue of sparsity, and specifically large variations in sparsity, by introducing sparsity normalization. This handles issues of model output covarying with the sparsity level in the input. However, it does not address the information loss due to the missing process. <ref type="bibr" target="#b7">Ma et al. (2018)</ref> used a permutation invariant setup to avoid imputing missing data in the input of a variational autoencoder. This approach can be readily extended to the supervised setting, using the permutation invariant setup as a modified input layer.</p><p>A review of approaches to handling missing data in (nondeep) supervised learning was given by <ref type="bibr" target="#b3">Josse et al. (2019)</ref>.</p><p>Here it is shown that under some assumptions, mean imputation is consistent in the supervised setting. Le Morvan et al. (2020) investigated the case of a linear predictor on covariates with missing data, showing that in the presence of missing, the optimal predictor may not be linear and how constant imputation of each feature can be optimized with regards to the model loss. <ref type="bibr" target="#b13">Rubin (1976)</ref> introduced the framework used for describing missing processes and their relation to the observed and missing data. Le <ref type="bibr" target="#b5">Morvan et al. (2020)</ref> and <ref type="bibr" target="#b14">Seaman et al. (2013)</ref> have pointed out some shortcomings in the way this framework and notation are often used. We will use a notation along the lines of Le <ref type="bibr" target="#b5">Morvan et al. (2020)</ref> here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and notation</head><p>Assume we have a data matrix X = (x 1 , . . . , x n ) ∈ X n that contain n i.i.d. copies of the random variable x ∈ X , where</p><formula xml:id="formula_0">X = X 1 × • • • × X p is a p-dimensional feature space.</formula><p>There is a response matrix Y = (y 1 , . . . , y n ) ∈ Y n that contains copies of the corresponding (possibly vector valued) response variable y ∈ Y. A missing process obscures parts of x resulting in the mask variable s ∈ {0, 1} p . The positions of observed entries in the data matrix X are contained in a mask matrix S (s 1 , . . . , s n ) ∈ {0, 1} n×p such that</p><formula xml:id="formula_1">s ij = 1 if x ij observed, 0 if x ij missing. (1)</formula><p>Then the observed data is</p><formula xml:id="formula_2">X = X S + na (1 -S),<label>(2)</label></formula><p>where is the Hadamard product and missing values are represented by na, defining na • x ij = na and na • 0 = 0. We let obs(s) denote the non-zero entries of s and miss(s) denote the zero-entries of s, such that x obs(s) are all the observed elements of x and x miss(s) are all the missing elements of x. For simplicity we will omit the s and write x obs , x miss respectively, whenever the context is clear.</p><p>We distinguish between the random variables (x obs , x miss ) and the strategies used to turn realisations of x obs into complete input vectors. Specifically an imputation function ι is used ι(x obs ) ∈ X , such that ι(x obs ) obs = x obs .</p><p>Finally, the goal is to minimize the prediction error by maximizing the discriminative log-likelihood</p><formula xml:id="formula_3">(φ) = n i=1 log p φ (y i |x obs i , s i ).</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Training deep supervised models with missing data</head><p>We wish to compare different strategies to handling missing data in supervised deep learning, specifically a convolutional neural network for classification on images. The strategies are</p><p>• 0-imputation,</p><p>• learnable imputation,</p><p>• concatenation of information in separate channels,</p><p>• three different strategies for using a DLVM with a discriminative model, M1, M2 and M3 respectively.</p><p>We describe these approaches in the sections below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Zero imputation</head><p>A simple version of constant imputation is 0-imputation, which has the intuitive appeal that the activation from the input node is zeroed out (absent). The input to the discriminative model is given by</p><formula xml:id="formula_4">ι 0 (x obs ) = x s + 0 (1 -s).</formula><p>(4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learnable imputation</head><p>In the unsupervised setting constant imputation biases marginal and joint distributions, but <ref type="bibr" target="#b3">Josse et al. (2019)</ref> have shown that mean imputation can be consistent in the supervised setting. Furthermore, Le Morvan et al. ( <ref type="formula">2020</ref>) noted that the constants can be optimized with respect to the model loss. This is the approach we take here, defining learnable parameters λ ∈ X to be inserted in place of the missing data, so that</p><formula xml:id="formula_5">ι λ (x obs ) = x s + λ (1 -s).</formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Concatenation in separate channels</head><p>In this work we are using a convolutional neural network for classification, so a straightforward way to merge information is to put it in separate channels in the input layer. We concatenate the following information: ι 0 (x obs ), λ and s. In multilayer perceptrons this could instead be done by concatenating information side by side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discriminative approaches using DLVMs</head><p>Here we explore how the recent progress made in applying DLVMs to missing data problems can be utilized in the supervised learning setting. We take three different approaches:</p><p>• M1: We propose a joint generative and discriminative model, where a joint objective (equation ( <ref type="formula" target="#formula_8">9</ref>)) ensures end-to-end training (figure <ref type="figure" target="#fig_0">1a</ref>).</p><p>• M2: The model is the same as M1, but the generative model is first pre-trained and fixed, and then the discriminative model is trained using the joint objective.</p><p>• M3: The dataset imputed by a generative model (figure <ref type="figure" target="#fig_0">1b</ref>) is given as input to a discriminative model (figure <ref type="figure" target="#fig_0">1c</ref>). In all three approaches the generative parts are identical and discriminative parts are identical. For the generative part, our choice of DLVM is the MIWAE <ref type="bibr" target="#b10">(Mattei &amp; Frellsen, 2019)</ref>, based on importance weighted variational inference. Therefore the single imputations used to impute a full dataset are generated using self-normalized importance sampling. With the joint objective, we can utilize self-normalized importance sampling as well, but instead of weighting samples from the generative part of the model to get imputations, we are weighting the predictions.</p><p>There are subtle distinctions between imputing a fixed dataset for the discriminative model (M3), training the discriminative model with the joint objective (M2) and training both the generative and discriminative model using the joint objective (M1). In M1 during training, the generative part of the model is tuned to improve the discriminative loss. In M1 and M2 importance weighted samples from the generative part of the model are fed to the discriminative model in place for the missing values, mimicking multiple imputation, where the class probabilities for each sample are importance weighted to give one final classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">LOSS DERIVATION</head><p>In this section, we will derive the loss for inference in the joint model from figure <ref type="figure" target="#fig_0">1a</ref>. The joint distribution p(y, x obs , x miss , z) over class labels, observed and missing covariates and latent variables can be factorized as p(z)p(x obs |z)p(x miss |z)p(y|x obs , x miss ),</p><p>where we assumed that the conditional distribution of x can be fully factorized as p(x|z) = j p(x j |z).</p><p>The likelihood of the observed data p(y, x obs ) is equal to p(z)p(x obs |z)p(x miss |z)p(y|x obs , x miss ) dx miss dz.</p><p>(7)</p><p>These integrals are usually analytically intractable. To approach them, we build on amortized importance-weighted variational inference <ref type="bibr" target="#b0">(Burda et al., 2016)</ref>. Indeed, the likelihood can be estimated using importance sampling</p><formula xml:id="formula_7">p(y, x obs ) ≈ 1 K K i=1 p(z k )p(x obs |z k )p(y|x obs , x miss k ) q(z k |x obs , s) ,<label>(8</label></formula><p>) where q(z k |x obs , s) is the variational distribution (learnable proposal) and (z k , x miss k ) k∈{1,...,K} are i.i.d. samples from p(x miss |z)q(z|x obs , s). This leads to the following lower bound of the log-likelihood:</p><formula xml:id="formula_8">LK = E   log   1 K K k=1 p(z k )p(x obs |z k )p(y|x obs , x miss k ) q(z k |x obs , s)      .<label>(9)</label></formula><p>We note that while <ref type="bibr" target="#b1">Ipsen et al. (2020)</ref> address a very different problem, modelling data with values missing not at random, they assume the same independence structure as in figure <ref type="figure" target="#fig_0">1a</ref> but with mask instead of label and obtain a bound with the same structure as equation (9).</p><p>Remark. If a data point is fully observed, the loss is then simply</p><formula xml:id="formula_9">L K = log p(y|x) + E   log   1 K K i=1 p(z k )p(x|z k ) q(z k |x)      ,<label>(10</label></formula><p>) which is just the sum of the discriminative likelihood and the generative vanilla IWAE bound of <ref type="bibr" target="#b0">Burda et al. (2016)</ref>.</p><formula xml:id="formula_10">Remark. When K = 1, we get L 1 = E log p(y|x obs , x miss 1 ) +E   log p(z 1 )p(x obs |z 1 ) q(z 1 |x obs , s)   ,<label>(11</label></formula><p>) which is the sum of a "data augmentation style" discriminative likelihood and the missing data VAE bound of <ref type="bibr" target="#b11">Nazabal et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">PREDICTION</head><p>Once we have trained the model, we can perform prediction by approximating p(y|x obs , s). Indeed, assuming that p(y|x obs , s) = p(y|x obs ), we can use self-normalised importance sampling:  </p><p>where</p><formula xml:id="formula_12">w k = r k r 1 + ... + r K , and r k = p(z k )p(x obs |z k ) q(z k |x obs , s) ,<label>(13)</label></formula><p>and (z k , x miss k ) k∈{1,...,K} are i.i.d. samples from p(x miss |z)q(z|x obs , s).</p><p>The prediction (seen as a probability vector) will therefore be a convex combination of the K predictions obtained by imputing the data via autoencoding. Of course, K should be much larger here than during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We apply the different strategies for handling missing data to the dynamically binarized MNIST dataset <ref type="bibr" target="#b6">(LeCun et al., 1998)</ref>, over a range of missing rates. The discriminative model is a convolutional neural network with four hidden layers. The generative model is an MLP with two hidden layers of 200 units in the encoder and decoder, and a latent space of dimension 20. During training K = 20 importance samples and a batch size of 100 are used. The generative part of the models is pre-trained for 500k iterations and used as the starting point for M1, M2 and M3.</p><p>In M3 the pre-trained model is used immediately to generate single imputations for train, validation and test-sets, using self normalised importance sampling with 10k samples. These are then used to train the discriminative model, do early stopping and get the test-set prediction error. In M2 the pre-trained generative model is kept fixed while training the discriminative model using the joint loss. In M1 the joint model is trained using the joint loss. In M1 and M2 predictions are done using self-normalized importance sampling on the class probabilities with 10k importance samples, cf. section 3.4.2.</p><p>Figure <ref type="figure" target="#fig_2">2a</ref> shows that M1 and M2 perform best and that the performance gap increases with the missing rate. The learning curves in figure <ref type="figure" target="#fig_2">2a</ref> obscures some of the relative performance gain, so in figure <ref type="figure" target="#fig_2">2b</ref> the performance is shown relative to 0-imputation.</p><p>The fact that M1 and M2 outperform models that use single imputation indicates that accounting for uncertainty of the missing values is quite valuable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future work</head><p>There are many possible approaches to deal with missing data in supervised deep learning. Our small investigations indicate that</p><p>• different ways of handling missingness may lead to quite different classification errors,</p><p>• accounting for uncertainty of the missing values can be very beneficial, even from a purely predictive perspective.</p><p>While we focused here on a simple convolutional architecture, it would be interesting to explore other kinds of architectures, from multi-layer perceptrons to recurrent/graph/group-equivariant neural nets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) Graphical model of M1and M2. For M1 the parameters (θ, φ, γ) are learnt jointly using the objective in equation (9). For M2, θ and γ are found by pre-training the generative part of the model, then held fixed while learning φ using the joint loss. (b) and (c) show the approach in M3; the connection between the generative and discriminative model is severed, the DLVM is trained separately and used to generate a fully observed dataset as input to the discriminative model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Relative performance compared to 0-imputation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. Accuracy for different missing rates m. M1 is the joint generative and discriminative model, trained jointly, M2 is the joint model, with the generative and discriminative models trained separately and M3 is the use of a generative model to impute the missing data to obtain a fully observed dataset, used to train a discriminative model.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The <rs type="funder">Danish Innovation Foundation</rs> supported this work through <rs type="funder">Danish Center for Big Data Analytics driven Innovation (DABAI)</rs>. JF acknowledge funding from the <rs type="funder">Independent Research Fund Denmark</rs> <rs type="grantNumber">9131-00082B</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QWZ4vk4">
					<idno type="grant-number">9131-00082B</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Importance weighted autoencoders</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">not-MIWAE: Deep generative modelling with missing not at random data. In review</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Ipsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Mattei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frellsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variational autoencoder with arbitrary conditioning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On the consistency of supervised learning with missing values</title>
		<author>
			<persName><forename type="first">J</forename><surname>Josse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Prost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Scornet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06931</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Linear predictor on linearly-generated data with missing values: non consistency and solutions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Le Morvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Prost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Josse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Scornet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00658</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Partial VAE for hybrid recommender system</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koenigstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">EDDI: Efficient dynamic discovery of high-value information with partial VAE</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tschiatschek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Palla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernandez-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4234" to="4243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Leveraging the exact likelihood of deep latent variable models</title>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Mattei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frellsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MIWAE: Deep generative modelling and imputation of incomplete data sets</title>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Mattei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frellsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4413" to="4423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Nazabal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Olmos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Valera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03653</idno>
		<title level="m">Handling incomplete heterogeneous data using VAEs</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inference and missing data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="581" to="592" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">What is meant by&quot; missing at random&quot;? Statistical Science</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Galati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="257" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Processing of missing data by neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Śmieja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Struski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zieliński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spurek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2719" to="2729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00150</idno>
		<title level="m">Why not to use zero imputation? correcting sparsity bias in training neural networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gain: Missing data imputation using generative adversarial nets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02920</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
