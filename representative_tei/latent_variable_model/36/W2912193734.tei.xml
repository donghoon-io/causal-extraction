<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MixDir: Scalable Bayesian Clustering for High-Dimensional Categorical Data</title>
				<funder>
					<orgName type="full">Department for International Development (DFID)</orgName>
				</funder>
				<funder>
					<orgName type="full">India, Peru and Vietnam</orgName>
				</funder>
				<funder>
					<orgName type="full">University of Oxford</orgName>
				</funder>
				<funder ref="#_aYu2Zdu">
					<orgName type="full">UK Medical Research Council</orgName>
				</funder>
				<funder>
					<orgName type="full">UK</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Constantin</forename><surname>Ahlmann-Eltze</surname></persName>
							<email>ahlmann-eltze@stud.uni-heidelberg.de</email>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Yau</surname></persName>
							<email>c.yau@bham.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Centre for Computational Biology</orgName>
								<orgName type="institution" key="instit1">The University of Manchester Research MixDir</orgName>
								<orgName type="institution" key="instit2">Heidelberg University Heidelberg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Birmingham Birmingham</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Alan Turing Institute London</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MixDir: Scalable Bayesian Clustering for High-Dimensional Categorical Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/DSAA.2018.00068</idno>
					<note type="submission">Accepted author manuscript Link to publication record in Manchester Research Explorer</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>High-dimensional</term>
					<term>categorical variables</term>
					<term>variational inference</term>
					<term>Bayesian</term>
					<term>clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multivariate analysis of high-dimensional datasets with multiple categorical variables (e.g. surveys, questionnaires) is a challenging task but can reveal patterns of responses that are masked from univariate analyses. In this paper we propose a novel variational inference algorithm to cluster high-dimensional categorical observations into latent classes. Variational inference is an approximate Bayesian inference algorithm, which combines fast optimization methods with the ability to propagate the uncertainty to the clustering (soft clustering). The model is robust to misspecification of the number of latent classes and can infer a reasonable number from the data. We assess the performance on synthetic and real world data and show that our algorithm has similar performance to the best other tested method if the correct number of classes is known and outperforms the other methods if it the number of classes needs to be inferred. An R-package implementing our algorithm is available at the Comprehensive R Archive Network 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>High-dimensional categorical datasets can be challenging to handle because the correlation structure grows exponentially with the number of variables. Consider a questionnaire which has J questions, where each question has R different categories of response. If we collect the responses of I individuals, which are stored in a matrix X with dimensions I × J and every cell contains one categorical value (A, B, C, . . .), the correlation structure (i.e. the contingency tensor Π R1×...×R J ) grows exponentially with every additional question and becomes too complex to inspect manually for any dataset with more than a handful of variables.</p><p>Clustering is a popular approach to identify low-dimensional structures embedded within high-dimensional datasets, but relatively few methods have been proposed to specifically handle the clustering of categorical datasets in comparison to the wealth of methods available for continuous data (for example: <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>). Our work is motivated by analyses of large-scale population studies such as the Young Lives study <ref type="bibr" target="#b4">[5]</ref>, the OSMI Mental Health in Tech Survey <ref type="bibr" target="#b5">[6]</ref>, and the Fig. <ref type="figure">1</ref>. Overview of MixDir algorithm. We have a high-dimensional dataset with categorical values (e.g. the NCPES). We run our MixDir clustering algorithm and obtain a soft clustering of the individuals into five classes. Each class has a particular distribution of values for each question. But this information can still be confusing, so to focus on the most telling response we look can either look at the questions that best explain the clustering or for responses r that maximize p(z|X j = r). In the example above this would for example be answer B for question 2 which is highly predictive for an individual to be in class 3. But also answer B for question 3 is highly predictive for class 1, although it is not the most common answer in class 1 for question 3.</p><p>UK National Cancer Patient Experience Survey (NCPES) <ref type="bibr" target="#b6">[7]</ref>, where large collections of questionnaire data are available for many thousands of individuals. The Young Lives study collects data on childhood poverty over 15 years in four different countries, the OSMI study collected data on the experience with mental health issues in the tech sector, and the NCPES has collected data on the experience and satisfaction of a large number of British cancer patients with the treatment they received by the UK National Health Service (NHS). These studies use questionnaires that are predominantly composed of categorical questions and the general ambition is to be able to identify groups of individuals with similar response profiles. In the case of NCPES, this would enable policy makers to develop strategies to improve the quality of cancer care in the UK.</p><p>At present, such analyses are typically performed using univariate analyses <ref type="bibr" target="#b6">[7]</ref> which attempt to associate responses to individual questions with some outcome of interest. This approach limits the ability to identify complex, multivariate response patterns that may manifest as a joint probability distribution over responses to a number of questions. In the following, we first summarize the pre-existing approaches for clustering of high-dimensional categorical data before proposing a scalable Bayesian latent class model (which we call MixDir) for modelling high-dimensional categorical data.</p><p>We will demonstrate the utility of the approach for the analysis of the Young Lives, the OSMI and NCPES survey data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. EXISTING WORK</head><p>Existing approaches for clustering of high-dimensional categorical data can be grouped into three approaches: (i) multivariate clustering approaches, (ii) latent class models and (iii) latent mixed membership models.</p><p>Multivariate clustering approaches adapt standard clustering techniques for continuous data by specifying similarity distance measures developed for categorical data explicitly. For example, k-mode <ref type="bibr" target="#b7">[8]</ref> based methods are a variation of the popular k-means clustering approach <ref type="bibr" target="#b8">[9]</ref>, where the Euclidean distance is replaced by an alternative distance metric (for example the Hamming distance) and the center of a cluster is not the mean of its member but a vector with the most common feature for each attribute (i.e. the mode of the members) <ref type="bibr" target="#b7">[8]</ref>. ROCK (short for RObust Clustering using linKs) performs agglomerative clustering <ref type="bibr" target="#b9">[10]</ref>. Similarity is measured by the number of common neighbors of a cluster and in each step the two most similar clusters are merged, until a threshold is reached.</p><p>Latent class models (LCM) <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref> are mixture models that assign the set of multivariate categorical observations to a latent class z. The idea is that within each latent class the observed variables are statistically independent. LCMs estimate the class probabilities λ and the probability of observing a particular response for a question conditioned on the latent class. <ref type="bibr" target="#b12">[13]</ref> proposed a nonparametric extension of the model, where they use a Dirichlet Process prior <ref type="bibr" target="#b13">[14]</ref> for the classes, which they call mixture of product multinomial distributions. Their model allows them to infer an appropriate number of latent classes depending on the dataset. They fit their model using a Gibbs Slice sampling algorithm, but this has two disadvantageous for clustering: first MCMC algorithms do not scale to large datasets and second they suffer from the label switching problem <ref type="bibr" target="#b14">[15]</ref>. To address those issues we have developed a variational inference (VI) method to estimate the parameters in the basic latent class model and its nonparametric extension. VI does not randomly sample from the posterior, but solves an optimization problem of fitting the complex posterior, by approximating it with a manageable distribution. This is much faster and has the additional advantage that it converges to a unique solution for clustering, where the labels of the clusters are interchangeable. A recent work by <ref type="bibr" target="#b15">[16]</ref> demonstrated that the variational approximation of tempered posteriors is consistent for mixtures of Gaussian and simple multinomial distributions.</p><p>It is important to distinguish our model from the mixed membership models <ref type="bibr" target="#b16">[17]</ref>, which are related but not identical to our model. In text processing mixed membership models are also called latent Dirichlet allocation (LDA) <ref type="bibr" target="#b17">[18]</ref>. Mixed membership models differ from latent class models because they assume that every response from an individual can come from different latent classes. In LCMs each response of one individual must come from the same latent class. This means that the mixed membership model is more flexible, which can be helpful if for example a text document discusses multiple topics, but on the other hand can complicate the interpretation, because it sets the focus on the questions and not on the individuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODEL</head><p>We now propose a variant of the LCM structure where we want to cluster the individuals into K classes depending on their answers. Our model can be summarized as follows:</p><formula xml:id="formula_0">λ|α ∼ Dirichlet(α) or DirichletProcess(α) (1)</formula><formula xml:id="formula_1">z i |λ ∼ Multinomial(λ) (2) U j,k |β ∼ Dirichlet(β) (3) X i,j |U j , z i = k ∼ Multinomial(U j,k ).<label>(4)</label></formula><p>α and β are hyper-parameters that are defined externally and govern the sparsity of the model. Eq. 1 defines that the size of the classes is governed by a Dirichlet (in the case of a simple LCM) or by a Dirichlet Process (in the case of a nonparametric LCM); for now we will describe the derivation for the simple LCM and will later present how to extend it to the nonparametric case. z is a vector that contains the latent class assignment for each individual. U is a 3-way tensor of size J × K × R and contains the probability for response r from an individual from class k for question j. Eq. 4 specifies that the response of an individual i that belongs to class k is a draw from a Multinomial distribution according to the probability vector U j,k .</p><p>The joint distribution of the model is defined as follows</p><formula xml:id="formula_2">p(λ, z, U, X|α, β) =p(λ|α) I i=1 p(z i |λ) J j=1 K k=1 p(U j,k |β) × I i=1 J j=1 K k=1 p(X i,j |U j,k ) 1(zi=k) . (<label>5</label></formula><formula xml:id="formula_3">)</formula><p>and Figure <ref type="figure" target="#fig_0">2</ref> shows the plate notation of the model. Finding the maximum likelihood solution would, for this model result, in an EM algorithm similar to the one described by <ref type="bibr" target="#b11">[12]</ref>, but to properly propagate uncertainty through the model and to be able to infer an appropriate number of latent classes, we develop a variational inference method that can address those challenges. Each node represents a random variable and X is shaded gray because it is the only variable which is observed. The arrows represent the dependency structure and the plates represent repeated values. K means that we have one U k for each cluster 1 to K. Analogous we have one cluster assignment and the corresponding set of observation for each individual 1 to I. λ is the cluster size proportion and drawn from a Dirichlet(α).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Variational Inference</head><p>The idea of VI is to define a simplified probability model q and tune its parameters to approximate the original model p. We choose q as the mean field approximation of p, which allows us to write down the variational distribution: q(λ, z, U ) = q(λ)q(z)q(U ), q(λ, z, U ) = q(λ; ω)</p><formula xml:id="formula_4">I i=1 q(z i ; ζ i ) K k=1 J j=1 q(U j,k ; φ j,k )<label>(6)</label></formula><p>where ω, ζ and φ are the free variational parameters, that are subsequently optimized. We also define that q(λ; ω) = Dirichlet(ω)</p><formula xml:id="formula_5">q(z i = k; ζ i ) = ζ i,k q(U j,k ; φ j,k ) = Dirichlet(φ j,k ).<label>(7)</label></formula><p>Using this definition we can derive the update equations for the variational parameters (Appendix A). We will measure the approximation with the KL-divergence, which allows us to maximize the evidence lower bound (ELBO). We find that iterating between the following equations maximizes the ELBO and thus also minimizes the KL divergence:</p><formula xml:id="formula_6">ω k = α + I i=1 ζ i,k .<label>(8)</label></formula><formula xml:id="formula_7">ζ i,k ∝ exp ψ(ω k ) -ψ K k=1 ω k + J j=1 ψ(φ j,k,Xi,j ) -ψ Rj r=1 φ j,k,r ,<label>(9)</label></formula><formula xml:id="formula_8">φ j,k,r = β + I i=1 ζ i,k 1[X i,j = r].<label>(10)</label></formula><p>The eq. 8 and 10 have an intuitive interpretation. They are just the weighted number of individuals per class and the weighted number of observation with a particular feature, respectively. Note that ψ(ω k ) in eq. 9 is the digamma function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Nonparametric extension</head><p>The strength of the latent class models is that it is straightforward to extend them to more complicated settings. For example if the true number of latent classes K is not known, one can use an approximation that assumes a potentially infinite number of classes of which only a finite number is ever observed for a finite number of observations. Mathematically this is expressed with a Dirichlet Process.</p><p>A constructive interpretation of the Dirichlet Process is the stick breaking process, which is very helpful as it allows us to construct a truncated approximation where we stop after making K max breaks <ref type="bibr" target="#b18">[19]</ref>. We apply this truncated stick breaking process as a prior for λ to give</p><formula xml:id="formula_9">λ k = v k k-1 k =1 (1 -v k ). As already mentioned each v k is drawn from a Beta distribution q(v k ; κ k,1 , κ k,2 ) = Beta(κ k,1 , κ k,2</formula><p>) where κ k,1 and κ k,2 are the variational parameters that are optimized in the Dirichlet Process instead of the ω k in the simple Dirichlet model.</p><p>The new joint distribution for this model thus is</p><formula xml:id="formula_10">p(λ, z, U, X|α, β) = Kmax-1 k=1 p(v k |α) I i=1 p(z i |λ) × J j=1 K k=1 p(U j,k |β) × I i=1 J j=1 K k=1 p(X i,j |U j,k ) 1(zi=k) ,<label>(11)</label></formula><p>which differs from eq. 5 in the sense that the first term has been replaced with the truncated stick breaking formulation. We derive the updates for the free variational parameters (Appendix B) and find that iteratively running the following equations maximizes the ELBO for the nonparametric model.</p><formula xml:id="formula_11">κ k,2 = α 2 + I i=1 Kmax k =k+1 ζ i,k , κ k,1 = α 1 + I i=1 ζ i,k , (12) ζ i,k ∝ exp ψ(κ k,1 ) -ψ(κ k,1 + κ k,2 ) + k-1 k =1 [ψ(κ k ,2 ) -ψ(κ k ,1 + κ k ,2 )] + J j=1 ψ(φ j,k,Xi,j ) -ψ Rj r=1 φ j,k,r<label>(13)</label></formula><p>The update equation for φ j,k,r (eq. 10) does not differ from the one in the parametric model, but the updates for the Dirichlet Process parameters and ζ i,k change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Handling missing data</head><p>We can reformulate the joint distribution of eq. 5 to incorporate missing data p(λ, z, U, X|α, β) =p(λ|α)</p><formula xml:id="formula_12">I i=1 p(z i |λ) J j=1 K k=1 p(U j,k |β) × K k=1 (i,j)∈S o p(X o i,j |U j,k ) 1(zi=k) × K k=1 (i,j)∈S m p(X m i,j |U j,k , z i = k)</formula><p>where S o is the set answer that were observed and S m is the set of answers that are missing for each individual.</p><p>If we assume that the data is missing completely at random (MCAR), which means that the chance of missing a value is unrelated to the latent class, the unobserved answer or any other previous answer, then p(X m i,j |U j,k , z i = k) = const. The estimation of the free variational parameters is thus independent of the missing values and they can be skipped during the variational updates. To impute the missing values, one would simply draw a latent class based on the observed data and draw replacements for the missing values from p(X m i,j |U j,k , z i = k). If on the other hand we believe that the missingness of a data point contains useful information for the inference, it is best recoded as an additional possible response r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. APPLICATIONS</head><p>In this section we first want to analyze the performance of our proposed algorithm using a simulation study and then we used the temporal consistency of the inferred clusters of the Young Lives survey as a real-world example. Lastly, we apply our model to analyze the latent structure of the OSMI Mental Health in Tech and the 2015 UK National Cancer Patient Experience survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulation study</head><p>To demonstrate that that our algorithm is able to identify latent structure in a high-dimensional dataset and to give an idea how it performs compared to other clustering algorithms for categorical data, we generated a dataset with a known latent structure. First, we generate four latent classes and for each class a prototypical list p k of length 5. Then we assign each individual randomly to one of the four classes. Each element in this list p k is a vector of length R, which contains the proportions to draw response r if an individual belongs to class k. Most of the entries have a roughly equal chance for each response or just a slight bias, but a few of them are highly specific for one class. Those are the ones that need to be picked up by a methods to produce a good clustering result. In each experimental run we vary the signal to noise ratio of the dataset, to test the performance with a range different settings. We compared the parametric and nonparametric variants of MixDir algorithm using the Dirichlet and Dirichlet Process priors respectively (the latter we refer to as MixDir-DP) with three other algorithms: the ROCK algorithm <ref type="bibr" target="#b19">[20]</ref>, one for k-mode <ref type="bibr" target="#b20">[21]</ref> and an EM inference implementation of the latent class model <ref type="bibr" target="#b11">[12]</ref> called poLCA. We chose these three algorithms, because they were all readily available as packages for the popular and widely used R statistical computing platform <ref type="bibr" target="#b21">[22]</ref>, which is also the platform we used for implementing our algorithm. This should be kept in mind when comparing for example the runtimes of the algorithms, where a bad performance could just be explained by an inefficient implementation. In terms of time complexity with respect to the number of observations n ROCK has a worst case time complexity of O(n 2 log(n)). The k-mode algorithm is linear to n, as are the three latent class models. Interestingly although ROCK has the worst theoretical time complexity of the compared methods, it consistently ran the fastest.</p><p>We measure the performance of the clustering algorithms using the adjusted Rand index (ARI) <ref type="bibr" target="#b22">[23]</ref>, which is a popular measure for comparing two clustering results. In our case we compare the proposed clustering of each algorithm to the ground truth. The ARI is 0 when the proposed clustering is as good as random and 1 if the algorithm recovered the ground truth. It is important to note that our algorithm produces a probabilistic output for each observation to belong to each class (also called soft clustering). For comparison with the other methods we assign each individual to the latent class for which it has the highest probability.</p><p>Figure <ref type="figure" target="#fig_1">3</ref> shows the performance on 100 differently initial- ized datasets for all five clustering methods with varying signal to noise ratios. We tuned the signal to noise ratios such that we cover the whole range of results for the methods ranging from cases where all methods achieve a decent clustering to cases where none of the methods is able produce a clustering better than random. In Figure <ref type="figure" target="#fig_1">3A</ref> we can see that if K is set to the correct number of latent classes the three LCMs outperform k-mode and ROCK and that there is no significant difference between our methods and poLCA. Only if we misspecify K (Figure <ref type="figure" target="#fig_1">3B</ref>), something that can easily happen on a real world dataset, we see that our models outperform the other approaches. When we actually look into the inferred clusters we can see that for high performing examples of MixDir with the Dirichlet Process prior the method is able to recover the correct number of four classes even if K = 8. Figure <ref type="figure" target="#fig_2">4</ref> shows that the increased performance comes at the cost of an increased runtime.</p><p>A particular challenge with the ROCK algorithm is that it relies on a user-defined parameter θ that defines the minimum similarity so that two elements are considered a neighbor. The resulting clustering strongly depends on this parameter, but there is little guidance to choosing it, so we simply used the suggested value of θ = 0.5 in all of the above examples. Our method also has hyper-parameters (we used α = 1 and β = 0.1), but they have less of an effect on the result, especially when a lot of data is provided. The hyper-parameters serve as pseudo-counts in eq. 8 and 10 and thus usually need to be within the same magnitude as the number of observations per latent class and category, respectively, to affect the clustering.</p><p>The above test is to a certain extend self serving, because we use the same model to generate the data that we also use to classify it. So it is important to see how the model behaves if the model is misspecified. We will test the performance of our model on data generated from a mixed membership model, which also emphasizes how our model differs. We generate . This is the generative model that is assumed by mixed membership models. We cluster this data using MixDir, poLCA and an R implementation for fitting mixed membership models (mixedMem) <ref type="bibr" target="#b23">[24]</ref>.</p><p>Unsurprisingly the mixed membership model performs best in the classification task and is able to recover for nearly every individual the correct fraction of membership in class A and B (Fig. <ref type="figure" target="#fig_3">5</ref>). In contrast, the latent class models (poLCA and MixDir) assume that every individual belongs exclusively to class A or B. They are still able to classify most individuals correctly whether they are mostly from class A or B, but for individuals with mixed response profiles, poLCA makes some classification errors due to the hard assignments it reports. However, the probabilistic output of MixDir means that individuals with a mixed response profile will receive an uncertain posterior class assignment. This is a good example where the probabilistic nature of our clustering algorithm can be an helpful indicator of model mis-specification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Young Lives</head><p>Next, we consider performance using a real-world data set from The Young Lives Survey. This survey is an international study of childhood poverty. It follows children in Ethiopia, India, Peru and Vietnam over 15 years tracking indicators about the health of the children, literacy, wealth of the household and many more indicators. So far four rounds of surveys have been conducted <ref type="bibr">(in 2002, 2006, 2009 and 2013)</ref> following the same children from birth to their teens. Initially, we focused on the Ethiopian dataset and specifically the younger cohort with children aged 1, 5, 8 and 12 years in the rounds respectively. We took several steps to clean the data: removal of unique identifiers, binning of continuous variables, removal of children that dropped out and removal of columns without any variation. In the end we worked with a dataset of about 1200 children, 52 questions and a median 4.5 different answers per question.</p><p>We then wanted to compare the temporal consistency of the clusters that different algorithms identify in the four rounds. We use the ID of the children as ground truth and calculate the adjusted Rand index (ARI) <ref type="bibr" target="#b22">[23]</ref> between the first and second, first and third, first and fourth, second and third, second and fourth, and third and fourth rounds. To have sufficient statistical power to detect performance differences and ensure consistent results, we repeat the procedure 25 times and each time randomly sample 20% of the individuals. The assumption in this analysis is that the groupings of children across the years do not change dramatically.</p><p>Since only MixDir-DP provides a means for automatically selecting the number of latent classes, we first considered an analysis where we prefix the number of latent classes to fixed values (K = 5 and 25) for all algorithms. With a smaller number of classes, we find that the latent class methods (poLCA, mixdir and mixdir DP) outperform k-mode and ROCK (Figure <ref type="figure" target="#fig_4">6A</ref>). With a larger number of classes (i.e. K = 25) we find that our algorithm outperforms all other methods including poLCA (Figure <ref type="figure" target="#fig_4">6B</ref>). To ensure that our result is reproducible we also ran the same experiment on the datasets from India, Peru and Vietnam (Figure <ref type="figure" target="#fig_5">7A,</ref><ref type="figure">B</ref>). In terms of the runtime, we find that on the Young Lives dataset our method outperforms the others (except for ROCK, but which had the worst performance) (Figure <ref type="figure" target="#fig_5">7C,</ref><ref type="figure">D</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. OSMI Mental Health in Tech Survey 2016</head><p>Open Sourcing Mental Illness (OSMI) is a non-profit corporation that is dedicated to mental wellness in the technology industry. Part of this effort is the collection of data on the state of mental health in the technology sector, including a survey from 2016 with responses of 1,400 individuals. The survey consists of a questionnaire with 63 different questions <ref type="bibr" target="#b5">[6]</ref>. We applied MixDir to explore latent structure within this survey data.</p><p>Before running the clustering algorithm we needed to clean the data: we removed free-form answers, filtered out selfemployed individuals, who were asked different questions, and summarized the responses which mental health disorder individuals had into single consistent values. In the end we worked with a dataset of 1,146 individuals with responses for 46 different questions and about 4% missing values.</p><p>The number of latent classes is unknown and our inference of this quantity will be determined by our prior beliefs. We therefore explored a range of hyperparameters for the Dirichlet Process prior with an increasingly penalization on the creation of new classes by setting α 1 to 1, 10, 100 and 1000. We always set K max = 25 which is enough to not hamper the fitting but keeps the algorithm tractable. Figure <ref type="figure" target="#fig_6">8A</ref> shows an alluvial plot tracing how each individual's class assignment changes as a function of increasing α 1 -the smaller classes are merged as this parameter grows. This visualisation provides a very useful means of understanding how the clustering structures alters as a function of our prior beliefs (encoded in the hyperparameter α 1 ) allowing us to partially objectify otherwise subjective beliefs about the number of latent classes. We decided to use α 1 = 100 for the subsequent analysis because it capture A is an alluvial plot that shows the effect of clustering the data with an increasing level of α 1 . If a mental health disorder has been diagnosed the band is colored green, if not purple. Flows with less than 10 individuals are suppressed to reduce the visual clutter. Individuals that were assigned to classes bigger than E are summarized in the "Other" block. The red rectangle highlights the parameter setting α 1 = 100 that was used in the other plots B-E for the more detailed analysis. B shows the class assignment probabilities for each individual. C is a bar plot that shows how many individuals in the first 4 classes have a diagnosis of a mental disorder. D is a bar plot that shows how many individuals in the first 4 classes answered the question "Do you think that discussing a mental health disorder with your employer would have negative consequences?". E is a plot of the contingency table for the questions "If a mental health issue prompted you to request a medical leave from work, asking for that leave would be:" and "Do you think that discussing a mental health disorder with your employer would have negative consequences?".</p><p>the main structure of the data, with the majority of survey participants grouped into five main classes. Note that this is not a statement that there are in fact five latent classes.</p><p>We focus on the four biggest classes (A, B, C and D) which together cover 88% of the individuals (Figure <ref type="figure" target="#fig_6">8B</ref>). We find that two groups (B and D) mainly consist of individuals that answered the question "Have you been diagnosed with a mental health condition by a medical professional?" with "Yes" and two groups (A and C) with individuals that mainly answered the question with "No" (Figure <ref type="figure" target="#fig_6">8C</ref>). It is of course interesting that the algorithm creates two groups (A and C vs. B and D) for the major phenotypic characteristic. To find out what is the main difference between A and C, and B and D, we look at the predictive features for each of the classes (the question-answer pairs that maximize the probability for class k: argmax Xj =r p(z = k|X j = r)). We notice that for group C and D answering the question "Do you think that discussing a mental health disorder with your employer would have negative consequences?" with "No" is a predictive feature (p = 41% and p = 29%, respectively), whereas answering with "Yes" is predictive for group A and B (p = 35% and p = 56%, respectively) (Figure <ref type="figure" target="#fig_6">8D</ref>). This suggests, that there are differences between the individuals who expect that being open about their mental health disorder will have negative consequences.</p><p>We looked for other features about the employer that differ by the expectation of discussing mental health and found, for example, that people are less likely to expect negative consequences if the employer has mental health care under the employer-provided coverage (Chi-squared test p = 0.0061). We also found that people have more negative expectations about asking for mental health-related leave, if their general expectations about discussing mental health with their employer is negative (Wilcoxon rank-sum test p &lt; 2.2 × 10 -16 , Figure <ref type="figure" target="#fig_6">8E</ref>).</p><p>Interestingly post-traumatic stress disorder (PTSD) is also a predictive feature for group B. This leads us to propose the hypothesis that individuals who are affected by PTSD, might have a more negative expectation about the consequences of discussing it with their employer. We check the hypothesis with a Fisher's exact test and reject the null hypothesis that individuals with PTSD have the same expectation as individuals with other mental disorder (p = 0.0417). On the other hand having a diagnosis for attention deficit hyperactivity disorder (ADHD) is a predictive feature for group D, but we did not find a significant relation of ADHD with having less negative expectations (Fisher's exact test p = 0.6341). This is a good example how the unsupervised clustering can help uncover interesting underlying structures, but on the other hand one must be careful not to over-interpret the data and check if trends can be confirmed with the whole dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. 2015 National Cancer Patient Experience Survey (NCPES)</head><p>Lastly, we wanted to analyze the 2015 NCPES <ref type="bibr" target="#b6">[7]</ref> to demonstrate the usefulness of our algorithm on a big dataset. The NCPES is an annual survey that has been conducted since 2010 and is commissioned by the British National Health Service (NHS) to monitor the state of care for cancer patients. The data consists of the responses from 71,186 individuals and has additional data about the gender, the tumor and the age of the patient. The data available from the questionnaire consists of 67 questions, of which all but three are categorical single choice questions. Those three that are not, were ignored in the subsequent analysis. The dataset also has a considerable number of missing values, in total more than 16% of all entries are not available. The NCPES is an interesting example of a high-dimensional categorical dataset, but so far most of the analysis has focused predominantly on univariate features of the dataset. Researchers have looked on the distribution of responses for individual questions (e.g. "87% of respondents said that, overall, they were always treated with dignity and respect while they were in hospital." <ref type="bibr" target="#b6">[7]</ref>).</p><p>Ideally we would want to infer an adequate number of latent classes on this dataset, but due to its inherent complexity and size, this would lead to too many classes for any manual downstream analysis (Figure <ref type="figure" target="#fig_8">10</ref>). For brevity in this paper, we decided for the sake of simplicity and interpretability to analyze the dataset using the simple Dirichlet prior with K = 5. We run our algorithm multiple times to check if the clusterings are consistent and find that the average agreement between ten iterations is ARI = 0.998.</p><p>The challenge with this dataset is that it contains such a large number of questions, which makes it difficult to decide which are the interesting variables that are important for the clustering. To find a manageable number of questions, we reduce the dimensionality of the dataset. We iteratively remove variables and test how much this affects the predicted clustering. We measure the loss of information using the Jensen-Shannon divergence on the predicted and the original class probability matrix at each step and remove the variable that least affects the clustering. This way we can narrow down the original set of 63 questions to five which are the most informative for distinguishing the clusters (Figure <ref type="figure" target="#fig_7">9</ref>). We find interesting distinct groups: cluster A is the second largest group and contains individuals that answered very positively throughout all 5 questions, cluster B is still mostly positive, but the answers are more nuanced, cluster C is the largest cluster and is defined by somewhat positive responses (e.g. "Yes, to some extent" or "Yes, some of the time"), cluster D is more negative and is also defined by individuals answering "Don't know", lastly, cluster E is the smallest cluster and the most negative with people answering the questions more often negative than positive. To validate that the overall satisfaction is a major driver of the clustering, we look at question 59, which asks the individuals to rate their overall experience from 1 to 10. We removed this question during the data cleaning, because it is not categorical, and can thus use it to demonstrate that we were nonetheless able to recover this information. We perform an Wilcoxon rank sum test on the ratings comparing that ratings(A) &gt; ratings(B) &gt; ratings(C) &gt; ratings(D) &gt; ratings(E), which is in all four cases highly significant (p &lt; 2.2 × 10 - 16 ).</p><p>An important feature of our method is that it produces probabilistic assignments of individuals to the latent classes. As we just described in the case of the NCPES we find that the latent classes have a linear relationship, so one can easily imagine that some individuals might be in between two classes, but are rarely considered a mix of more than two classes. Accordingly we find that less than 4% of the individuals have more than 10% probability for at least three classes.</p><p>When we focus on question 49 which asks about the involvement of the family and/or friends, we can see that they were less involved in clusters C, D and E. To see if indeed missing involvement of the family leads to less overall satisfaction, we test if overall individuals which answered "Yes" or "Yes, to some extend" were more satisfied than individuals that answered "No" or "No family or friends were involved" (Wilcoxon rank sum test p &lt; 2.2 × 10 -16 ). On the other hand this needs to be qualified because individuals that deliberately decided against involvement of their family are overall more satisfied than individuals that just stated their social network was not involved (Wilcoxon rank sum test p &lt; 2.2 × 10 -16 ). This underlines the importance of social networks during cancer treatment, but on the other hand which role the ability to make deliberate choices can play.</p><p>To summarize, we are able to cluster the large NCPES dataset and uncover interesting latent structure. We identify the overall satisfaction as a major underlying feature of the dataset and show how it can be related to the support patients get from their family and friends. This demonstrates that our algorithm can be a useful tool for handling large and highdimensional categorical datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>There is no universally best clustering technique without context, but we find that our method has several desirable features. It can deal with large datasets of more than 70,000 observations, it has a principled approach to handle missing data thanks to the Bayesian framework and it can handle datasets where the true number of latent classes is not known. We developed two related versions of the algorithm, one for a finite number of classes and the nonparametric version where we assume that the number of latent classes keeps increasing as long as gather more observations. One limitation is that for the analysis of the performance of the different methods using simulations, we only quantified it using data generated from a model that has the same independence assumptions as the model we developed here. Another limitation could be that for the nonparametric extension we use a Dirichlet Process prior, which has the known problem of overestimating the true number of latent classes <ref type="bibr" target="#b24">[25]</ref>. This issue should be kept in mind, but in our experience this has not been a problem for the datasets we looked at.</p><p>In this paper we have presented a variational inference algorithm for Bayesian latent class models and their nonparametric extension. We demonstrate on high-dimensional categorical data that our clustering algorithm is able infer good results on synthetic and real world datasets. We also show that its performance is comparable to the best competitor (poLCA) if the correct number of latent class is known a priori, and actually outperforms the other methods if the number of classes is not known, which is a common problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A VARIATIONAL INFERENCE DERIVATION</head><p>In this section we want to give a short introduction to variational inference (VI) and the explicit derivation of the updates for the variational parameters.</p><p>VI is an approximate method to do inference in Bayesian models <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b25">[26]</ref>. It is an alternative to the well known MCMC algorithms that randomly sample from the model until the stationary distribution of the samples correspond to the posterior of the model and the data. Instead VI converts the inference problem into an optimization problem, which can be solved much more efficiently.</p><p>In the Bayesian framework we are interested in learning about the distribution of the parameters given the observed data. Mathematically this can be written as</p><formula xml:id="formula_13">p(z|x) = p(z, x) p(x) ,<label>(14)</label></formula><p>where z are all the parameters of the model and x is the data. This is just a reformulation of the famous Bayes rule and means that the conditional distribution equals the joint distribution of data and parameters divided by the marginal p(x). Calculating this marginal is the big challenge in Bayesian inference because to calculate the probability of observing a particular dataset x one would need to consider all possible configurations of the parameters z. Or again in mathematical notation</p><formula xml:id="formula_14">p(x) = p(z, x) dz.<label>(15)</label></formula><p>Only for very simple models it is possible to calculate this integral analytically, for complex models it is necessary to find approximations for this integral for example with MCMC or VI.</p><p>In VI we choose a family F of distributions, which is easier to handle, and try to find a setting where our approximate distribution q(z) ∈ F is as close as possible to the posterior p(z|x). The closeness is measured with the Kullback-Leibler (KL) divergence KL(q(z) p(z|x)) = q(z) log q(z) p(z|x) dz</p><formula xml:id="formula_15">= E[log q(z)] -E[log p(z|x)]<label>(16)</label></formula><p>where all expectations E are taken with respect to q(z). The KL divergence is not symmetric and favors q(z) to be smaller and to underestimate the variance of p(z|x), but on the other hand it is has nice mathematical properties that makes it useful for approximating complex models.</p><p>As discussed earlier the term p(z|x) in eq. 16 is usually not available, but we can re-arrange the equation so that it is not needed:</p><formula xml:id="formula_16">KL(q(z) p(z|x)) = E[log q(z)] -E[log p(z, x)]</formula><p>+ log p(x) -KL(q(z p(z|x)) + log p(x) = E[log p(z, x)] -E[log q(z)]</p><formula xml:id="formula_17">ELBO[q] = E[log p(z, x)] -E[log q(z)] ELBO[q] = E[log p(z, x)] + H[q(z)],<label>(17)</label></formula><p>where ELBO is short for evidence lower bound and H is the entropy of a function (H[p] = -p(x) log p(x) dx). Looking at eq. 17 we can see that maximizing the ELBO is equivalent to minimizing the KL divergence up to an additive constant. So the goal of VI is to find q * (z) such that q * (z) = argmin <ref type="bibr" target="#b17">(18)</ref> In theory all kind of distributions F could be applicable here, but in practice the one that is most commonly chosen in VI is the so called mean field variational family. It assumes that the latent variables are all independent, so that q(z) factorizes to</p><formula xml:id="formula_18">q(z)∈F KL(q(z) p(z|x)) = argmax q(z)∈F ELBO[q].</formula><formula xml:id="formula_19">q(z) = m j=1 q j (z j )<label>(19)</label></formula><p>and each density q j (z j ) can be chosen independently to maximize the ELBO. Our factorization of q(z) is provided in eq. 6.</p><p>Now we can start to derive the update equations. First we will write down the expectation of the joint E[log p(z, x)] from eq. 17 </p><formula xml:id="formula_20">+ I i=1 J j=1 K k=1 E[log p(X i,j |U j,k , z i = k)] (20)</formula><p>The first line of equation eq. 20 is simply the expectation of a log Dirichlet distribution, which we can look up in <ref type="bibr" target="#b17">[18]</ref> as</p><formula xml:id="formula_21">E [log p(λ|α)] = log Γ k α k - k log Γ(α k ) + k (α k -1) ψ(ω k ) -ψ k ω k (21) where ψ(ω k ) is the digamma function.</formula><p>The expectation in the second line of eq. 20 we can again look up in <ref type="bibr" target="#b17">[18]</ref> as</p><formula xml:id="formula_22">E[log p(z i |λ)] = K k=1 ζ i,k E q [log λ k ] = K k=1 ζ i,k ψ(ω k ) -ψ k ω k . (<label>22</label></formula><formula xml:id="formula_23">)</formula><p>The expectation in the third line of eq. 20 is again just a log Dirichlet</p><formula xml:id="formula_24">E[log p(U j,k |β)] = log Γ r β r - r log Γ(β r ) + r (β r -1)ψ(φ j,k,r ) -ψ r φ j,k,r .<label>(23)</label></formula><p>The expectation in the fourth line of eq. 20 is not readily available, so we will have to derive it</p><formula xml:id="formula_25">E[log p(X i,j |U j,k , z i = k)] = E 1[z i = k] log p(X i,j |U j,k ) = E 1[z i = k]] E[log p(X i,j |U j,k )] = ζ i,k Rj r=1 1[X i,j = r] × ψ(φ j,k,r ) -ψ r φ j,k,r .<label>(24)</label></formula><p>Now that we have all the necessary expectations to calculate</p><p>x)], we can derive the Entropies for H[q] (the later half of eq. 17).</p><formula xml:id="formula_26">H [q(λ, z, U ; ω, ζ, φ)] = H [q(λ; ω)] + I i=1 H [q(z i ; ζ i )] + J j=1 K k=1 H[q(U j,k ; φ j,k )]<label>(25)</label></formula><p>The first line of eq. 25 is just the entropy of a Dirichlet distribution which we can look up as</p><formula xml:id="formula_27">H [q(λ; ω)] = -log Γ k ω k + k log Γ(ω k ) - k (ω k -1) ψ(ω k ) -ψ k ω k .<label>(26)</label></formula><p>The second line of eq. 25 is the entropy of a multinomial:</p><formula xml:id="formula_28">H [q(z i ; ζ i )] = - k ζ i,k log ζ i,k<label>(27)</label></formula><p>Analogous to the entropy in eq. 26, we can write down the entropy for the third line of eq. 25</p><formula xml:id="formula_29">H[q(U j,k ; φ j,k )] = -log Γ r φ r + r log Γ(φ r ) - r (φ r -1) ψ(φ r ) -ψ r φ r (28)</formula><p>Now we have all elements in place to actually optimize the free variational parameters ω, ζ and φ to maximize the ELBO. One approach would to apply a general purpose optimizer like BFGS, but the number of parameters in our model can grow very quickly, so that this approach becomes inefficient. Instead we will use a coordinate ascent strategy (CAVI <ref type="bibr" target="#b2">[3]</ref>), where we iteratively optimize each single free parameter while the other are hold constant until the ELBO has converged. To achieve efficient updates, we will derive analytical updates for each of the parameters, by taking the derivative of the ELBO and setting it to zero. First we will derive the update for latent group mixture parameter ω k . This step is equivalent to the derivation of the updates of γ i in the LDA model described in the appendix A.3.2 of <ref type="bibr" target="#b17">[18]</ref>.</p><formula xml:id="formula_30">∂ ELBO ∂ω k = ∂ ∂ω k k (α k -1) ψ(ω k ) -ψ k ω k + k i ζ i,k ψ(ω k ) -ψ k ω k -log Γ k ω k + log Γ(ω k ) - k (ω k -1) ψ(ω k ) -ψ k ω k = ∂ ∂ω k ψ(ω k )(α k + i ζ i,k -ω k ) -ψ k ω k k (α k + i ζ i,k -ω k ) -log Γ k ω k + log Γ(ω k ) = ψ (ω k )(α k + i ζ i,k -ω k ) -ψ k ω k k (α k + i ζ i,k -ω k )</formula><p>(29) If we assume that all α k are equal, because our prior is symmetric, we can see that the whole term is zero when (α + i ζ i,k -ω k ) = 0 and thus we can conclude that the ELBO is maximized when ω k is set to</p><formula xml:id="formula_31">ω k = α + i ζ i,k .<label>(30)</label></formula><p>We will now derive the update for ζ i,k in a similar fashion:</p><formula xml:id="formula_32">∂ ELBO ∂ζ i,k = ∂ ζ i,k ζ i,k ψ(ω k ) -ψ k ω k + ζ i,k J j=1 ψ(φ j,k,Xi,j ) -ψ r φ j,k,r -ζ i,k log ζ i,k = ψ(ω k ) -ψ k ω k + J j=1 ψ(φ j,k,Xi,j ) -ψ r φ j,k,r -log(ζ i,k ) -1<label>(</label></formula><p>31) Setting this to zero and solving for ζ i,k we find that</p><formula xml:id="formula_33">ζ i,k ∝ exp ψ(ω k ) -ψ k ω k + J j=1 ψ(φ j,k,Xi,j ) -ψ r φ j,k,r ,<label>(32)</label></formula><p>where the solution is only correct up to a proportional constant, because of the constraint that k ζ i,k = 1.</p><p>Finally we will derive the update for φ j,k,r :</p><formula xml:id="formula_34">∂ ELBO ∂φ j,k,r = ∂ ∂φ j,k,r r (β r -1) ψ(φ j,k,r ) -ψ r φ j,k,r + r I i=1 ζ i,k 1[X i,j = r] × ψ(φ j,k,r ) -ψ r φ j,k,r -log Γ r φ j,k,r + r log Γ(φ j,k,r ) - r (φ j,k,r -1) ψ(φ j,k,r ) -ψ r φ j,k,r = ∂ ∂φ j,k,r ψ(φ j,k,r ) × (β r + I i=1 ζ i,k 1[X i,j = r] -φ j,k,r ) -ψ r φ j,k,r × r (β r + I i=1 ζ i,k 1[X i,j = r] -φ j,k,r ) -log Γ r φ j,k,r + r log Γ(φ j,k,r ) = ψ (φ j,k,r )(β r + I i=1 ζ i,k 1[X i,j = r] -φ j,k,r ) -ψ r φ j,k,r × r (β r + I i=1 ζ i,k 1[X i,j = r] -φ j,k,r )<label>(33</label></formula><p>) When we set this to zero and solve for φ j,k,r we can again see that the whole term is zero when (β r + I i=1 ζ i,k 1[X i,j = r] -φ j,k,r ) = 0 and if we again assume that all β r are equal, that thus The second line of eq. 20 we can look up in <ref type="bibr" target="#b18">[19]</ref> E[log p(z i |v)] = Kmax k=1 q(z i &gt; k) E[log(1 -v k )] + q(z i = k) E[log v k ] (36) where </p><formula xml:id="formula_35">q(z i &gt; k) = Kmax k =k+1 ζ i,k q(z i = k) = ζ i,k E[log(1 -v k )] = ψ(κ k,2 ) -ψ(κ k,1 + κ k,2 ) E[log v k ] = ψ(κ k,1 ) -ψ(κ k,1 + κ k,2 ).</formula><p>We also need to update the entropy for the Dirichlet in eq. 25.</p><p>H[q(v; κ 1 , κ 2 )] = - + ψ(κ k,1 + κ k,2 )</p><formula xml:id="formula_37">× ( -α 1 + 1 -α 2 + 1 - i Kmax k =k+1 ζ i,k - i ζ i,k + κ k,1 + κ k,2<label>-2)</label></formula><p>(40) This term is zero if κ k,2 removes the additional terms in the second parentheses and κ k,1 is just equal to the remaining terms. The update equations for κ k,1 and κ k,2 are thus</p><formula xml:id="formula_38">κ k,2 = α 2 + i Kmax k =k+1 ζ i,k κ k,1 = α 1 + i ζ i,k ,<label>(41)</label></formula><p>which matches the results of <ref type="bibr" target="#b18">[19]</ref>. In this equation we see that unlike the classical Dirichlet Process our model has two hyper-parameters: α 1 and α 2 . This model is also the called the Beta two parameter process <ref type="bibr" target="#b26">[27]</ref>. It is equivalent to the Dirichlet Process if α 1 = 1. A large value for α 2 encourages the opening of more classes, whereas a large value for α 1 penalizes the opening of new classes.</p><p>Specifically for the Beta two parameter process the ratio of κ k,1 to κ k,2 determines how much of the remaining probability mass is assigned to class k. On average class k will cover κ k,1 κ k,1 +κ k,2 of the remaining space. The priors α 1 and α 2 serve as additional pseudo counts in that ratio. So if we believe a priori that each class should cover 90% of the remaining space there are in theory two ways to achieve this. We can either fix α 1 = 1 and make α 2 smaller (i.e. α 2 = 1/9) or fix α 2 = 1 and make α 1 larger (i.e. α 1 = 9). But in the first case if we actually have observed i Kmax k =k+1 ζ i,k &gt; 1 this would easily overpowers our prior believe, whereas in the second case the regularization is much stronger.</p><p>We will now similarly derive the update for ζ i,k </p><p>The update equation for φ j,k,r does not change, so we now have all the elements to maximize the ELBO.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. The latent class model in plate notation. Each node represents a random variable and X is shaded gray because it is the only variable which is observed. The arrows represent the dependency structure and the plates represent repeated values. K means that we have one U k for each cluster 1 to K. Analogous we have one cluster assignment and the corresponding set of observation for each individual 1 to I. λ is the cluster size proportion and drawn from a Dirichlet(α).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance comparison on a synthetic dataset of the k-mode, ROCK, the EM algorithm for the latent class model (poLCA) and our implementation with a Dirichlet Process prior (mixdir DP) and the simple Dirichlet prior (mixdir). The performance is measured with the adjusted Rand index (ARI) that calculates the overlap between the inferred clustering and the ground truth on the synthetic dataset. The significance test is a two-sided paired Wilcoxon rank sum test and NS. indicates a p-value &gt; 0.05, one star indicates p &gt; 0.01. two stars p &gt; 0.001 and three stars indicate p &lt; 0.001. The red box shows the mean and the bootstrapped confidence limits. The algorithms are tested in two settings one where K is the correct number of latent classes in the model (A) and one where K is an overestimate of the number of latent classes (B).</figDesc><graphic coords="5,311.98,50.54,257.04,165.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Run-time analysis of the k-mode, ROCK, the EM algorithm for the latent class model (poLCA) and our implementation with a Dirichlet Process prior (mixdir DP) and the simple Dirichlet prior (mixdir) on a dataset with 1000 observations for 5 questions with 5 to 15 categories each. The red box shows the mean and the bootstrapped confidence limits. The runtime was measured on a 4 year old laptop with a Intel Core i7-3635QM processor.</figDesc><graphic coords="6,48.96,50.54,257.04,165.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Clustering of mixed-membership data using mixdir, poLCA and mixedMem. The data consist of 2000 individuals with 40 features which are assigned to one of two classes. The top row shows the percentage for each individual how many of its features are assigned to class A.</figDesc><graphic coords="6,311.98,50.54,257.04,146.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Performance comparison on the Young Lives Ethiopia datasets. The performance is the adjusted Rand index (ARI) between the clusterings of the four rounds and the algorithms were run 25 times on 20% randomly sampled individuals. The significance test is a two-sided paired Wilcoxon rank sum test, NS. indicates a p-value &gt; 0.05 and the stars indicate p-values of &gt; 0.01, &gt; 0.001 and &lt; 0.001. The red box shows the mean and the bootstrapped confidence limits. We once set K to a small number of latent classes (A) and once we to an overestimate of the number of latent classes (B).</figDesc><graphic coords="7,311.98,50.54,257.04,214.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Cluster consistency and runtime for all algorithms on all four countries. The performance of each method is the adjusted Rand index (ARI) between the clustering of the first and second, first and third, first and fourth, second and third, second and fourth, and third and fourth round. The process was run 25 times on 20% randomly sampled individuals. The methods are ordered by mean ARI. The red boxes show the mean and the bootstrapped confidence limits. We compare two different settings K = 5 (A, C) and K = 25 (B, D), which are likely under-and overestimations of the true number of latent classes in the data. To ensure that ROCK divides the dataset into more than one cluster we had to set θ = 0.1.</figDesc><graphic coords="7,48.96,50.54,257.04,165.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Analysis of the 2016 OSMI Mental Health in Tech Survey.A is an alluvial plot that shows the effect of clustering the data with an increasing level of α 1 . If a mental health disorder has been diagnosed the band is colored green, if not purple. Flows with less than 10 individuals are suppressed to reduce the visual clutter. Individuals that were assigned to classes bigger than E are summarized in the "Other" block. The red rectangle highlights the parameter setting α 1 = 100 that was used in the other plots B-E for the more detailed analysis. B shows the class assignment probabilities for each individual. C is a bar plot that shows how many individuals in the first 4 classes have a diagnosis of a mental disorder. D is a bar plot that shows how many individuals in the first 4 classes answered the question "Do you think that discussing a mental health disorder with your employer would have negative consequences?". E is a plot of the contingency table for the questions "If a mental health issue prompted you to request a medical leave from work, asking for that leave would be:" and "Do you think that discussing a mental health disorder with your employer would have negative consequences?".</figDesc><graphic coords="8,48.96,50.54,514.09,239.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Defining features plot of the NCPES dataset, with the questions that are most informative for distinguishing the inferred clusters. The area of each square indicates the fraction of individuals in a group that answered the question a specific way. The sum of the area of all squares in each column is 1. In the original questionnaire the answers for the question 14, 15, 21, 49 and 54 differed slightly in their formulation, so we grouped them into common categories. The vertical bars have no additional meaning and only serve to enhance the visual clarity of the plot.</figDesc><graphic coords="9,311.98,50.54,257.04,330.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Challenges of inferring the number of classes on the NCPES data. A shows the number of realized clusters for the MixDir model with a Dirichlet or a Dirichlet Process prior, depending on the maximum number of possible clusters. B shows the adjusted Rand index between the relicates depending on the maximum number of available clusters. The blue line shows a linear and an inverse Gaussian model. D shows an alluival plot demonstrating the effect of increasing α 1 with a Dirichlet Process prior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>E</head><label></label><figDesc>[log p(z, x)] = E [log p(λ|α)] p(U j,k |β)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>φ j,k,r = β + I i=1 ζ i,k 1[X i,j = r].we want to derive the update equation of the variational parameters for the nonparametric model.The first term of the ELBO that obviously changes is the expectation of the Dirichlet, which now is the expectation of the Dirichlet ProcessE[log p(v|α)] = Kmax-1 k=1 E[log p(v k |α)] = Kmax-1 k=1 (α 1 -1)(ψ(κ k,1 ) -ψ(κ k,1 + κ k,2 )) + (α 2 -1)(ψ(κ k,2 ) -ψ(κ k,1 + κ k,2 )) . (35)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>expectations in line 3 and 4 of eq. 20 are unchanged and stillE[log p(U j,k |β)] = log Γ r β rr log Γ(β r ) + r (β r -1) ψ(φ j,k,r ) -ψ r φ j,k,r E[log p(X i,j |U j,k , z i = k) = ζ i,k Rj r=1 1[X i,j = r] × ψ(φ j,k,r ) -ψ r φ j,k,r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>κ k,1 ) + log Γ(κ k,2 ) -log Γ(κ k,1 + κ k,2 ) + (κ k,1 -1)ψ(κ k,1 ) + (κ k,2 -1)ψ(κ k,2 ) -(κ k,1 + κ k,2 -2)ψ(κ k,1 + κ k,2 )(39)We now again have all the elements to derive the new update equations. We will first derive the updates for κ k,1∂ ELBO ∂κ k,1 = ∂ ∂κ k,1 (α 1 -1)(ψ(κ k,1 ) -ψ(κ k,1 + κ k,2 )) -(α 2 -1)ψ(κ k,1 + κ k,2 ) + I i=1 (ψ(κ k,2 ) -ψ(κ k,1 + κ k,2 )) Kmax k =k+1 ζ i,k + (ψ(κ k,1 ) -ψ(κ k,1 + κ k,2 ))ζ i,k -log Γ(κ k,1 ) + log Γ(κ k,1 + κ k,2 ) + (κ k,1 -1)ψ(κ k,1 ) + (κ k,1 + κ k,2 -2)ψ(κ k,1 + κ k,2 ) = ψ(κ k,1 )(α 1 -1 + i ζ i,k -κ k,1 + 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>∂</head><label></label><figDesc>ELBO ∂ζ i,k = ∂ ∂ζ i,k ζ i,k (ψ(κ k,1 ) -ψ(κ k,1 + κ k,2 )) κ k ,2 ) -ψ(κ k ,1 + κ k ,2 )) + ζ i,k J j=1 ψ(φ j,k,Xi,j ) -ψ r φ j,k,r -ζ i,k log ζ i,k = ψ(κ k,1 ) -ψ(κ k,1 + κ k,2 κ k ,2 ) -ψ(κ k ,1 + κ k ,2 )) + J j=1 ψ(φ j,k,Xi,j ) -ψ r φ j,k,r -log(ζ i,k ) -1(42)We can easily set this to zero and solve this for ζ i,k again up to an proportional constantζ i,k ∝ exp ψ(κ k,1 ) -ψ(κ k,1 + κ k,2 ) κ k ,2 ) -ψ(κ k ,1 + κ k ,2 )) + J j=1ψ(φ j,k,Xi,j ) -ψ r φ j,k,r</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>CRAN.R-project.org/package=mixdir Addional links for reproducing the figures are available at cwcyau.github.io/publications.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>The authors acknowledge the support of the <rs type="funder">UK Medical Research Council</rs> Grant No. <rs type="grantNumber">MR/P02646X/1</rs>. Some of the data used in this study comes from <rs type="person">Young Lives</rs>, a 15-year study of the changing nature of childhood poverty in Ethiopia, <rs type="funder">India, Peru and Vietnam</rs> (www.younglives.org.uk). Young Lives is funded by <rs type="funder">UK</rs> aid from the <rs type="funder">Department for International Development (DFID)</rs>. The views expressed here are those of the author(s). They are not necessarily those of <rs type="person">Young Lives</rs>, the <rs type="funder">University of Oxford</rs>, <rs type="institution">DFID</rs> or other funders. This paper provides a secondary analysis of data obtained through the UK Data Service for the Young Lives Study (7483) and NCPES (8163) data.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_aYu2Zdu">
					<idno type="grant-number">MR/P02646X/1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">K-Means++: the Advantages of Careful Seeding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms</title>
		<meeting>the eighteenth annual ACM-SIAM symposium on Discrete algorithms</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1027" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BIRCH: An Efficient Data Clustering Databases Method for Very Large</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="103" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Boyden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Woldehanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Penny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duc</surname></persName>
		</author>
		<title level="m">Young Lives: an International Study of Childhood Poverty: Round</title>
		<imprint>
			<publisher>UK Data Service</publisher>
			<date type="published" when="2013">2013-2014. 2016</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">OSMI Mental Health in Tech Survey</title>
		<ptr target="https://osmihelp.org/research" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Open Sourcing Mental Illness Ltd</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">National Cancer Patient Experience Survey</title>
		<imprint>
			<date type="published" when="2015">2015. 2017</date>
			<publisher>NHS England Quality Health</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extensions to the k-Means Algorithm for Clustering Large Data Sets with Categorical Values</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1009769707641</idno>
		<ptr target="http://link.springer.com/article/10.1023/A:1009769707641" />
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="283" to="304" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Fifth Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ROCK: A Robust Clustering Algorithm for Categorical Attributes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="345" to="366" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Latent structure analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lazarsfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Henry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968</date>
			<pubPlace>Houghton Mifflin; Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">poLCA: An R Package for Polytomous Variable Latent Class Analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Linzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonparametric Bayes Modeling of Multivariate Categorical Data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<idno type="DOI">10.1198/jasa.2009.tm08439</idno>
		<ptr target="http://www.tandfonline.com/doi/abs/10.1198/jasa.2009.tm08439" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">487</biblScope>
			<biblScope unit="page" from="1042" to="1051" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Bayesian Analysis of Some Nonparameteric Prolems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="230" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dealing with label switching in mixture models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
		<idno type="DOI">10.1111/1467-9868.00265</idno>
		<ptr target="http://doi.wiley.com/10.1111/1467-9868.00265" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="795" to="809" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Consistency of Variational Bayes Inference for Estimation and Model Selection in Mixtures</title>
		<author>
			<persName><forename type="first">B.-E</forename><surname>Chérief-Abdellatif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Alquier</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1805.05054" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Introduction to Mixed Membership Models and Methods</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Erosheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Mixed Membership Models and Their Applications</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Macine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Variational inference for Dirichlet process mixtures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1 A</biblScope>
			<biblScope unit="page" from="121" to="144" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">cba: Clustering for Business Analytics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Buchta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hahsler</surname></persName>
		</author>
		<ptr target="https://cran.r-project.org/package=cba" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ligges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Luebke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Raabe</surname></persName>
		</author>
		<title level="m">klaR Analyzing German Business Cycles</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Baier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Decker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="335" to="343" />
		</imprint>
	</monogr>
	<note>Data Analysis and Decision Support</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">R: A Language and Environment for Statistical Computing</title>
		<author>
			<persName><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
		<ptr target="https://www.r-project.org/" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<pubPlace>Vienna, Austria</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fitting Mixed Membership Models using mixedMem</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Erosheva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A simple example of Dirichlet process mixture inconsistency for the number of components</title>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Variational Inference: A Review for Statisticians</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Markov chain Monte Carlo in approximate Dirichlet and beta two parameter process hierarchical models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ishwaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="371" to="390" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
