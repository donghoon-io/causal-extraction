<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Structured Latent Variable Recurrent Network With Stochastic Attention For Generating Weibo Comments</title>
				<funder ref="#_db2aTgd">
					<orgName type="full">Key Research Program of Frontier Sciences, CAS</orgName>
				</funder>
				<funder ref="#_Xde4dUH">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_FyjWkCN #_8pAcSxJ #_dPtWYGG #_pwvhwEV #_mgmfcZW #_DByh6xB #_nh4ZuFJ #_kVk9SD6">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shijie</forename><surname>Yang</surname></persName>
							<email>shijie.yang@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Li</surname></persName>
							<email>liang.li@ict.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Key Lab of Intell. Info. Process</orgName>
								<orgName type="department" key="dep2">Inst. of Comput. Tech</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
							<email>wangshuhui@ict.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Key Lab of Intell. Info. Process</orgName>
								<orgName type="department" key="dep2">Inst. of Comput. Tech</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weigang</forename><surname>Zhang</surname></persName>
							<email>wgzhang@hit.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
							<email>qmhuang@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Key Lab of Intell. Info. Process</orgName>
								<orgName type="department" key="dep2">Inst. of Comput. Tech</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
							<affiliation key="aff4">
								<orgName type="laboratory">Noah&apos;s Ark Lab, Huawei Technologies</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Structured Latent Variable Recurrent Network With Stochastic Attention For Generating Weibo Comments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Building intelligent agents to generate realistic Weibo comments is challenging. For such realistic Weibo comments, the key criterion is improving diversity while maintaining coherency. Considering that the variability of linguistic comments arises from multi-level sources, including both discourselevel properties and word-level selections, we improve the comment diversity by leveraging such inherent hierarchy. In this paper, we propose a structured latent variable recurrent network, which exploits the hierarchical-structured latent variables with stochastic attention to model the variations of comments. First, we endow both discourse-level and word-level latent variables with hierarchical and temporal dependencies for constructing multilevel hierarchy. Second, we introduce a stochastic attention to infer the key-words of interest in the input post. As a result, diverse comments can be generated with both discourse-level properties and local-word selections. Experiments on opendomain Weibo data show that our model generates more diverse and realistic comments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generating realistic comments for social applications is attaching lots of attention <ref type="bibr" target="#b2">[Shen et al., 2019;</ref><ref type="bibr">Holtzman et al., 2019;</ref><ref type="bibr" target="#b4">Zhang et al., 2018]</ref>. Compared with rulebased <ref type="bibr" target="#b0">[Goddeau et al., 1996]</ref> and retrieval-based methods <ref type="bibr" target="#b0">[Eric and Manning, 2017]</ref>, generative sequence-tosequence (Seq2Seq) <ref type="bibr">[Sutskever et al., 2014;</ref><ref type="bibr" target="#b2">Liu et al., 2018;</ref><ref type="bibr" target="#b3">Yang et al., 2019;</ref><ref type="bibr">Zha et al., 2019</ref>] models using encoderdecoder architectures have been widely used due to their super capacity for sequential data. As Figure <ref type="figure" target="#fig_0">1a</ref>, traditional Seq2Seq models directly maximize the likelihood between input and output. They tend to generate "safe" and meaningless comments of high-frequency, and such comments lack  the diversity <ref type="bibr" target="#b2">[Li et al., 2016]</ref>. Closely related to response generation, generating diverse and coherent comments is important for improving the user experience of intelligent agents.</p><formula xml:id="formula_0">I</formula><p>Recently, some methods are proposed to improve comment diversity, which can be divided into two groups. The first one focuses on improving the word generation process, including designing new variations of generation procedures and finding new types of decoding objectives. <ref type="bibr" target="#b2">[Mou et al., 2016]</ref> proposed a backward-forward decoding process which generated a reply start from diversified middle keyword. <ref type="bibr" target="#b2">[Vijayakumar et al., 2016]</ref> and <ref type="bibr" target="#b2">[Shao et al., 2017]</ref> proposed diversity-enforced beam-search procedures. Moreover, <ref type="bibr" target="#b3">[Yao et al., 2016]</ref> incorporated Inverse Document Frequency into the decoder objective to promote the comment diversity. The second group emphasizes that comments are diversified in terms of high-level properties or attributes, such as language style, topic, intention. These properties are explored as context information to generate more specific comments. <ref type="bibr" target="#b2">[Xing et al., 2017]</ref> fed topic information into a joint attention module to generate topic-related comments. <ref type="bibr">[Zhou et al., 2017b]</ref> added emotion context to build emotional chatting agents. Moreover, another series of works implicitly modeled the high-level attributes using stochastic latent variables <ref type="bibr" target="#b5">[Zhao et al., 2017;</ref><ref type="bibr" target="#b0">Cao and Clark, 2017;</ref><ref type="bibr" target="#b2">Serban et al., 2017;</ref><ref type="bibr" target="#b2">Shen et al., 2017;</ref><ref type="bibr" target="#b2">Liu et al., 2019]</ref>. In such way, diverse comments are able to be sampled conditioned on the latent variables. However, most of above models treat comment generation as a single-level process, neglecting the hierarchy in natural comments.</p><p>In practice, the variabilities of comments arise from such the hierarchy, including both global discourse-level properties and word-level selections. Given one post, first, a number of comments can be generated conditioned on discourselevel diversity (i.e. different emotions or language styles). Then, for one specific discourse-level selection, various local word-trajectories are able to be generated conditioned on word-level diversity. How to formulate the above procedure remains an open question.</p><p>In this paper, we propose a structured latent variable Recurrent Network with Stochastic Attention (SARN), a probabilistic model that exploits both hierarchical-structured latent variables and the stochastic attention to promote multi-level diversity of comments. First, we introduce both discourselevel variable and word-level latent variables to build the hierarchy of comments. In detail, the discourse-level variable is used to capture the high-level properties such as style, topic and intention. Word-level variables are leveraged by the stochastic attention to infer the variations of the key-words of interest in the input post. Second, we endow these latent variables with hierarchical and temporal dependencies, which are specifically designed to model the complex structure of comments. As result, diverse comments are able to be produced conditioned on different assignments of these latent variables. The proposed SARN is trained using the Stochastic Gradient Variational Bayesian framework <ref type="bibr" target="#b1">[Kingma and Welling, 2013]</ref> which maximizes the variational lower bound of the conditional log likelihood. Experiments on open-domain Weibo dataset showed that the proposed SARN yields significantly more diverse comments at both discourse-level and wordlevel compared to other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary</head><p>Given an input x = (x1, ..., xT ) and a comment sequence y = (y1, ..., y T ), where ytis the t-th caption word, Seq2Seq model is trained to maximize the probability of p(y|x). A bidirectional recurrent network (B-RNN) first summarizes the input sequence x into hidden states h t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft-attention.</head><p>A decoder RNN calculates the context c t , and estimates the probability of y. In detail, the decoder RNN takes a context c t and the previously decoded word yt-1to update its state st as st = f (st-1, yt-1, ct), and ctis a weighted average of the encoder hidden states as,</p><formula xml:id="formula_1">eti = g(st-1, hi), αti = exp(e ti ) T k=1 exp(e tk ) , ct = T i=1 αtihi,<label>(1)</label></formula><p>where g is inner-product function and α ti can be viewed as the similarity score between encoder's state ht and decoder's state st-1. Finally, the probability distribution of candidate words at time-step t is calculated as, p(yt|y&lt;t, x) = sof tmax(M LP (st, yt-1)).</p><p>The final objective is to maximize the (log) likelihood of p(y|x) for all timesteps as p(y|x) = T t=1 p(yt|y&lt;t, x).</p><p>This Seq2Seq model lacks the parametrization of stochastic variations of generation process, where only source of stochastic variation is provided by conditional distribution of p(yt|y&lt;t, x)(the softmax layer). This model generates highfrequency comments with low-diversity <ref type="bibr" target="#b2">[Li et al., 2016]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>To improve comment diversity, we reveal inherent multi-level hierarchy in comment generation, and propose a probabilistic model that exploits latent variables and a stochastic attention. Hierarchical structure. As Figure <ref type="figure">2</ref>, first, a decision-level latent variable z d is used to characterize the choice of highlevel properties, such as topic, intention. Second, a series of word-level latent variables zw = {z t w } T t=1 are introduced to characterize the variations of word-level selections, generate diverse comments by focusing on different input key-words.</p><p>To induce a two-level hierarchy, we endow the latent variables z d and z w with structured dependencies by defining,</p><formula xml:id="formula_2">p(y, zw, z d |x) = p(y, zw|z d , x)p(z d |x). p(y, zw|z d , x) = T t=1 p(yt|z ≤t w , y&lt;t, z d , x)p(z t w |z &lt;t w , y&lt;t, z d )</formula><p>i) discourse-level stochastic variations are modeled by the prior distribution of p(z d |x). ii) word-level stochastic variations are modeled by the distribution of p(z t w |z &lt;t w , y &lt;t , z d ). Specifically, each word-level variable z t w is not only conditioned on x and z d , but also conditioned on the previous latent trajectories of z &lt;t w . These structured dependencies promote the model capacity for capturing multi-level diversities. Rather than directly maximizing p(y|x) , we define the joint probability distribution p(y, z w , z d |x) and maximize p(y|x) by marginalizing out all the latent variables as,</p><formula xml:id="formula_3">p(y|x) = z d ,zw p(y, zw, z d |x) dzwdz d .</formula><p>(2)</p><p>Stochastic attention. One key hypothesis is that diverse comments can be generated by focusing on different keywords of input. Different from deterministic Soft-attention (Eq. 1), we explicitly inject stochastic variations in the context vector c t , by leveraging the word-level variables z t w as,</p><formula xml:id="formula_4">eti = g(z t w , hi), αti = exp(e ti ) T k=1 exp(e tk ) , ct = T i=1 αtihi,<label>(3)</label></formula><p>where the variations of the focused input-words are characterized by the latent distribution of z w . Stochastic attention helps better capture the 1-to-n mappings. For one input, more diverse input-words can be focused to generate informative and meaningful predictions. Generation. As Figure <ref type="figure">2</ref>, given an input x, a comment sentence y is generated as follows.</p><p>1) Calculate the encoding states {ht} T t=1 using a B-RNN encoder (see preliminary).</p><p>2) Sample a discourse-level variable z d from a multivariate Gaussian distribution p(z d |x)as,</p><formula xml:id="formula_5">z d ∼ p(z d |x) = N (µ d , diag(σ 2 d )).<label>(4)</label></formula><p>The mean µ d and variance σ 2 d are produced by a MLP network, which takes the last hidden states hT as input as,</p><formula xml:id="formula_6">[µ d , σ d ] = M LP d (hT ).</formula><p>(5)</p><p>3) Iteratively run step 4), 5), 6) for each step t = 1, 2, ..., T . 4) Update the hidden state s t of the decoder RNN as,</p><formula xml:id="formula_7">st = f (st-1, yt-1, z t-1 w ),<label>(6)</label></formula><p>where f is a LSTM unit, y 0 is the '&lt;start&gt;' token, and z 0 w and s 0 are initialized as z 0 w = z d and s0 = M LPs(z d ). 5) Stochastic-attention: Sample the t-th variable z t w from another Gaussian distribution p(z t w |z &lt;t w , y&lt;t, z d )with mean µ t and diagonal covariance diag(σ 2 t )as</p><formula xml:id="formula_8">z t w ∼ p(z t w |z &lt;t w , y&lt;t, z d ) = N (µ t , diag(σ 2 t )),<label>(7)</label></formula><p>where the mean and variance are also produced by a MLP network with input s t as [µ t , σt] = M LPw(st). Then, compute the context vector c t according to Eq. 3. 6) Sample a word y t with probability p(y t |y &lt;t , z ≤t w , z d , x) from vocabulary set V as,</p><formula xml:id="formula_9">py t = sof tmax(M LP (st, z t w , ct)) ∈ R |V | . (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>Inference. Directly optimizing objective of Eq. 2 involves a marginalization over the latent variables of z d and z w , which introduces an intractable inference of the posterior p(z d , z w |y, x). Exploiting Stochastic Gradient Variational Bayes (SGVB) <ref type="bibr" target="#b1">[Kingma and Welling, 2013]</ref>, we introduce an auxiliary distribution q(z d , z w |y, x) to approximate the true posterior as,</p><formula xml:id="formula_11">q(zw, z d |y, x) = q(z d |y, x) T t=1 q(z t w |z &lt;t w , y ≤t , z d ),</formula><p>where q(z d |y, x)and q(z t w |z &lt;t w , y ≤t , z d )are multivariate Gaussian distributions with diagonal covariance, parameterized by MLP networks as,</p><formula xml:id="formula_12">q(z d |y, x) = N ( μd , diag(σ 2 d )), [ μd , σd ] = M LP d(hT , h y T ), and<label>(9)</label></formula><formula xml:id="formula_13">q(z t w |z &lt;t w , y ≤t , z d ) = N ( μt , diag(σ 2 t )), [ μt , σt] = M LP ŵ (st, yt).<label>(10)</label></formula><p>As shown in Figure <ref type="figure">3</ref>, h y T is the last hidden state of the B-RNN encoder with input y. Via variational inference, we derive the objective function which is the lower bound of the conditional log likelihood of Eq. 2 as,</p><formula xml:id="formula_14">L = E q(z d ,zw |y,x) [ T t=1 (log p(yt|z ≤t w , y&lt;t, z d , x) -KL(q(z t w |y ≤t , z &lt;t w , z d , x)||p(z t w |y&lt;t, z &lt;t w , z d )))] -KL(q(z d |y, x)||p(z d |x)) ≤ log p(y|x),<label>(11)</label></formula><p>where KL is the Kullback-Leibler divergence, regularizing the approximated posteriors to be close to the priors.</p><p>Training. In Figure <ref type="figure">3</ref>, the model is trained by maximizing the variational lower bound L of Eq. 11. In practice, the expectation term can be approximated using M Monte Carlo samples {ẑ</p><formula xml:id="formula_15">(m) d , ẑ<label>(m)</label></formula><p>w } M m=1 sampled from q(z d , z w |y, x). The reparametrization trick <ref type="bibr" target="#b1">[Kingma and Welling, 2013</ref>] is adopted to reduce the variance of the approximation as,</p><formula xml:id="formula_16">ẑ(m) d = μd + σd ε (m) , ẑt(m) w = μt + σt ε (m) ,<label>(12)</label></formula><p>where ε (m) is a vector of standard Gaussian samples, and is dot production. In practice, we set M = 1 and maximize the approximated variational lower bound L as,</p><formula xml:id="formula_17">L = 1 M M m=1 T t=1 [ log p(yt|y&lt;t, ẑ≤t(m) w , ẑ(m) d , x) - KL(q(z t w |y ≤t , ẑ&lt;t(m) w , ẑ(m) d , x)||p(z t w |y&lt;t, ẑ&lt;t(m) w , ẑ(m) d ))] -KL(q(z d |y, x)||p(z d |x)) ≈ L. (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>Analogous to existing latent variable models <ref type="bibr" target="#b0">[Bowman et al., 2015;</ref><ref type="bibr" target="#b5">Zhao et al., 2017]</ref> </p><formula xml:id="formula_19">py b = sof tmax(M LP bow (z d )), log p(y bow |z d ) = V j ∈y bow log py b [j],<label>(14)</label></formula><p>where p y b is the probability vector over vocabulary V .</p><p>2) For z w , we propose an Auxiliary-path method with objective log p(y t |z t w ), which forces the model to make good predictions only conditioned on the latent variables z w . The auxiliary probability vector over the vocabulary for p(y t |z t w ) can be calculated by an auxiliary network M LP aux as,</p><formula xml:id="formula_20">pyt = sof tmax(M LP aux (z t w ) ∈ R |V | . (<label>15</label></formula><formula xml:id="formula_21">)</formula><p>The final objective function is rewritten as in Eq. 16, where α and β are hyper-parameters to balance two auxiliary terms.</p><formula xml:id="formula_22">L f inal = L + E q(z d ,zw |y,x) [α log p(y bow |z d ) +β T t=1 log p(yt|z t w )],<label>(16)</label></formula><p>There are two differences between SARN and existing methods <ref type="bibr">[Park et al., 2018;</ref><ref type="bibr" target="#b2">Serban et al., 2017;</ref><ref type="bibr" target="#b2">Shen et al., 2017]</ref> which also exploit hierarchical structure. (1) they model different hierarchies, i.e., sentence-word hierarchy for SARN and dialog-sentence hierarchy for existing methods.</p><p>(2) we leverage stochastic attention with hierarchical structure, which gives more capacity to model the complex variations in word selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Data collection. We collected training corpus from Sina Weibo including Chinese post-comment data. To construct diverse training set, we compute scores of Tf-Idf cosine distance between a given post and other posts in the corpus, and then we select the nearest neighbor comments from the top-15 similar posts. We empirically find that these selected comments are coherent enough with the selected post. Finally, we build 1 million post-comments pairs, and each post is coupled with 15 different comments. The dataset is randomly split into training, validation and testing sets, with 980k, 10k and 10k samples respectively. Compared methods. 1) Seq2Seq <ref type="bibr">[Sutskever et al., 2014]</ref> Basic encoder-decoder model. 2) Seq2Seq-MMI <ref type="bibr" target="#b2">[Li et al., 2016]</ref> Seq2Seq model with Maximum Mutual Information objective function. We used the MMI-bidi version with parameters λ = 0.5, γ = 1. 3) <ref type="bibr">VRNN [Chung et al., 2015]</ref> Latent variable Seq2Seq model using time-step dependency. 4) CVRNN <ref type="bibr" target="#b0">[Bowman et al., 2015]</ref> Latent variable Seq2Seq model using global dependency. 5) SpaceFusion <ref type="bibr">[Gao et al., 2019]</ref> Seq2Seq model using latent space fusion<ref type="foot" target="#foot_1">foot_1</ref> . Settings. We adopted the Jieba Chinese tokenizer <ref type="bibr" target="#b1">[Jieba, 2018]</ref>, and constructed a vocabulary with 40k. For compared models, we adopted a LSTM for encoding and decoding. The dimension of the word embedding, attention state, latent variables, and LSTM hidden state were respectively set to be 100, 200, 500 and 500. The network parameters were initialized from normal distribution N (0, 0.01). We used Adam as optimizer with a fixed learning rate of 0.0001, and the batch size was set to 20. Gradient clipping was adopted with a threshold of 10. We selected parameters with the best validation performance. For the vanishing latent variable problem, we adopted the KL-annealing for models of VRNN, CVRNN and SARN, by linearly increasing training weight of KL-term from 0 to 1 after running 60,000 batches. We also adopted bag-of-word method for CVRNN, and the proposed auxiliary-path method for VRNN. The source code is released<ref type="foot" target="#foot_2">foot_2</ref> . Evaluation methods. Given one post, we sample 5 comments for each model for evaluation. We adopt beam search in decoding process for Seq2Seq and Seq2Seq-MMI, where beam size was set to 10. For VRNN, CVRNN and SARN, we randomly sample 5 comments using greedy decoding (beam size is 1), where the randomness comes from the latent variables <ref type="bibr" target="#b0">[Bowman et al., 2015]</ref>. We apply three evaluation metrics, including human judgments, embedding-based evaluation, and n-gram based evaluation (BLEU-4). As <ref type="bibr">[Zhou et al., 2017a]</ref>, human judgments is performed via crowd-sourcing. We conduct blind evaluation, and outputs are randomly presented to 10 evaluators who have experiences of NLP. For each test input, every model generates 5 responses as a group.</p><p>For each response the evaluators are asked to score its quality with Bad (NOT grammatically correct or relevant), Normal (grammatically correct and relevant to the input) and Good (beyond grammatically correct and relevant, the response is interesting and meaningful). Further, we define a comment is 'Acceptable' if it is scored Normal or Good. Besides, we conduct pairwise comparisons, where a better comment is chosen from two compared models. Numbers of Wins, Losses and Ties are reported. Compared with n-gram based metrics, embedding-based evaluation focuses more on measuring the semantic similarity. As <ref type="bibr" target="#b2">[Serban et al., 2017]</ref>, three metrics are adopted, including Average, Extrema and Greedy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Performance Analysis</head><p>For human judgements, we evaluate the consistency of agreement using Fleiss' Kappa criterion, which are 0.34 (Fair agreement) for Table <ref type="table" target="#tab_5">1</ref> and<ref type="table">0</ref>.30 (Fair agreement) for Table 2 respectively. We observe that, first, SARN outperforms other models with respect to both the Diversity value and Acceptable ratio. Compared with models only improving single-level diversity, such as the word-level VRNN and the discourse-level CVRNN, Seq2Seq-MMI and SpaceFusion, our method achieved higher Diversity value and Acceptable ratio. The improvements shows the effectiveness of leveraging the multi-level diversity. Second, the statistics of Bad, Normal and Good show that our model tends to generate more Good comments than Normal or Bad comments. It reveals that promoting the multi-level diversity also helps to improve the comment quality, generating more meaningful and interesting comments. Third, our method does not achieve superior BLEU-4 score. The reason is that BLEU-4 calculates the n-gram overlapping with ground-truth sentences, and such measurement is not positively related to the comment diversity. Because of such limitations of n-gram based measurement, we provide embedding-based evaluations. According to the Average, Extrema and Greedy scores in we see SARN generates more coherent and informative comments in terms of high-level semantics. Finally, the pairwise comparison results in Table <ref type="table" target="#tab_4">2</ref> show that the comments of our SARN are preferred in the majority of the experiments, and our method achieves comparable performance with SpaceFusion. It further validates that our SARN produces more diverse comments with better quality.</p><p>Figure <ref type="figure">4</ref> shows comment examples. We can see that SARN generates more informative comments with high diversity while other methods tend to produce phrases with similar meanings. As Eq. 3, comments are generated based on the understanding of key-words of interest. Color histogram in Figure <ref type="figure">4</ref> denotes the accumulated attention weights on the post word, where we find (1) SARN can focus on different post words so that it generates diverse comments; (2) Seq2Seq focuses on post words uniformly so that their comments are very similar. This is because that Seq2Seq uses deterministic soft-attention while SARN uses stochastic attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parameter Analysis</head><p>To tackle the vanishing latent variable problem, we respectively add bag-of-word and auxiliary-path objectives for z d and z w in Eq. 16, introducing hyper-parameters α and β to balance the auxiliary terms. As shown in Table <ref type="table">3</ref> and<ref type="table" target="#tab_6">4</ref>, we find that, first, z d and z w have great influence on Diversity degree and Acceptable rate. z d is to characterize the global properties of sentencesand and z w mainly controls the wordchoice. Second, both of KL cost and Diversity are gradually increased with the increment of α and β. This indicates that the vanishing latent variable problem is alleviated. Third, Acceptable rate is first improved with the increase of α and β.  Then the performance degradation is showed when α ≥ 0.3 and β ≥ 0.05 respectively. The main reason is that too large values of α and β overemphasize the auxiliary objectives, which gives the model much freedom to generate ungrammatical sentences. These results show that we can control the trade-off between improving diversity and maintaining grammatical sentence by using different values of α and β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-level Diversity Analysis</head><p>Parameter analysis shows that the values of α and β are proportional to the 'Diversity' degree. We further investigated how these two parameters influence the results. First, we tested different α by fixing β = 0.01 as shown in Figure <ref type="figure" target="#fig_1">5</ref>. We can see that with the increase of α, the generated comments contain more different meanings, which indicates that the latent variable z d successfully catches discourse-level variations. Second, we tested different β by fixing α = 0.3. We found that the 'Diversity' degree of local words is more sensitive to β than α. As shown in Figure <ref type="figure">6</ref>, our model begins to generate ungrammatical sentences consisting of diverse key words using large values of β &gt; 0.1. This observation indicates that latent variables z w capture the word-level variations, and large value of β makes z w dominate the worddecisions, neglecting the long-term sequence dependency of LSTM hidden units. The above observations confirm our assumption that the two kinds of latent variables respectively catch discourse-level and word-level variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Large β And Search-based Model</head><p>Interestingly, we found the ungrammatical sentences caused by large β (β &gt; 0.1) usually contain highly diverse but coherent key words with respect to the post (see bolded words in the middle column of Figure <ref type="figure">6</ref>). Based on this observation, we built an effective search-based model based on these generated key words. In detail, we simply conducted Tf-Idf nearest search and selected the most similar comments from the training set. As shown in the right column of Figure <ref type="figure">6</ref>, the obtained comments are highly coherent with the input post, and the final Diversity score and Acceptable ratio are further improved. Therefore, this search-based model provides us an effective way to combine generative and search-based generative model for practical industry use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>As a probabilistic model, the proposed SARN exploits both hierarchical-structured latent variables and the stochastic attention to promote multi-level diversity of comments. SARN is highly related to encoder-decoder models, while the main difference is that we inject multi-level stochastic variations in the generation process with both hierarchical and temporal dependencies. Experiments show that our model generates more diverse and realistic comments than other methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>.Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustrations of comment generation process. (a) Traditional Seq2Seq models predict the t-th word yt via p(yt|y&lt;t, x) given input x, where stochastic variation is parameterized by this conditional output distribution. (b) We introduce latent variables z d and zw with stochastic attention to capture the discourse-level and word-level stochastic variations. Our model estimates the distribution of p(yt|z ≤t w , z d , y&lt;t, x). Diverse comments are produced conditioned on different assignments of latent variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comment examples of different α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 6: Comments examples of different β and Tf-Idf search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>have no hobby</figDesc><table><row><cell></cell><cell>step t ： p y y x ( | , ) t t</cell><cell></cell></row><row><cell>I have no hobby I What is your hobby?</cell><cell>You like (a) Seq2Seq model like 'playing' 'playing' … 'watching' … 'watching' step t ： ( | , ) t t p y y x</cell><cell cols="2">movies football football movies</cell></row><row><cell cols="4">'playing' 'watching' 'drinking' … 'dogs' … 'drinking' … wines wines movies football … t t t w w t d p y z z y x p z z y z You like nothing but working I cannot believe it You like I have no hobby You like nothing but working d z t w z step t ： step 1： attention ( | ) t t w d 'playing' 'watching' You like movies … football … d z t w z . 'dogs' I cannot believe it attention</cell></row><row><cell cols="3">Discourse-level diversity step t ： step 1： ( | ) p y z z y x p z z y z Word-level diversity t t t t w d t w w t</cell><cell>d</cell></row><row><cell>Discourse-level diversity</cell><cell>Word-level diversity</cell><cell></cell></row><row><cell></cell><cell>theme space</cell><cell></cell></row><row><cell></cell><cell>theme space</cell><cell></cell></row></table><note><p>d p z x ( | , , , ) ( | , , ) d p z x ( | , , , ) ( | , , )</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The network architecture for comment generation. Some of the MLP subnetworks are omitted for abbreviation.</figDesc><table><row><cell></cell><cell>y You like …</cell><cell cols="2">nothing but LSTM LSTM</cell><cell>working LSTM</cell><cell cols="2">h</cell><cell>y T</cell><cell>d MLP</cell><cell>d</cell><cell>d d </cell><cell>z</cell><cell>d</cell><cell>d</cell><cell>s MLP</cell><cell>s</cell><cell>0</cell><cell>&lt;start&gt; nothing w t s LSTM LSTM …</cell><cell>but w LSTM</cell><cell>working LSTM</cell></row><row><cell></cell><cell>t w d z z</cell><cell cols="6">Word-level latent variable Discourse-level latent variable</cell><cell cols="4">Prior estimate</cell><cell></cell><cell></cell><cell cols="3">You like</cell><cell>nothing t c MLP</cell><cell>but</cell><cell>s</cell><cell>t</cell><cell>working &lt;end&gt;</cell></row><row><cell></cell><cell></cell><cell cols="2">T t t=1 {h }</cell><cell></cell><cell></cell><cell cols="5">Stochastic attention</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>attention</cell><cell>…</cell></row><row><cell>x</cell><cell>I LSTM</cell><cell>have LSTM</cell><cell>no LSTM</cell><cell>hobby LSTM</cell><cell>h</cell><cell>T</cell><cell></cell><cell cols="3">d MLP Discourse-level d z d  d </cell><cell cols="7">s Word-level s MLP LSTM 0 &lt;start&gt; nothing t w z s t LSTM …</cell><cell>t+1 w LSTM z but</cell><cell>LSTM working</cell></row><row><cell cols="2">Figure 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, our model suffers from the vanishing latent variable problem. During training, the KL terms The network architecture for training. Some of the MLP subnetworks are omitted for abbreviation.in Eq. 13 are easily crushed to zero, which means that latent variables fail to capture meaningful information. Since simply applying the KL-annealing<ref type="bibr" target="#b0">[Bowman et al., 2015]</ref> is unsatisfactory, we introduce extra strategies for z d and z w .</figDesc><table><row><cell></cell><cell>z</cell><cell>d</cell><cell cols="6">Discourse-level latent variable</cell><cell>Prior estimate</cell><cell cols="2">You like</cell><cell>nothing</cell><cell>but</cell><cell>working &lt;end&gt;</cell></row><row><cell cols="3">x y LSTM t w z I … You like</cell><cell cols="4">nothing but T t t=1 {h } Word-level latent variable working LSTM LSTM LSTM have no hobby LSTM LSTM LSTM</cell><cell cols="2">T y h T h</cell><cell cols="3">d MLP d MLP Stochastic attention d z d d  d z d Posterior inference  d  (q(z ) || p(z )) d d KL MLP KL s (q(z ) || p(z )) c MLP t attention … 0 &lt;start&gt; nothing t w t w z t t w w z s t s LSTM LSTM …</cell><cell>s</cell><cell>t</cell><cell>… working LSTM but t y t+1 w z LSTM</cell></row><row><cell cols="3">Figure 3:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">t w d z z</cell><cell cols="6">Word-level latent variable Discourse-level latent variable</cell><cell>Prior estimate</cell><cell>You like</cell><cell>nothing t c MLP</cell><cell>but</cell><cell>s</cell><cell>t</cell><cell>working &lt;end&gt;</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">T t t=1 {h }</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Stochastic attention</cell><cell></cell><cell>attention</cell><cell>…</cell></row><row><cell>x</cell><cell>I LSTM</cell><cell></cell><cell>have LSTM</cell><cell>no LSTM</cell><cell>hobby LSTM</cell><cell cols="2">h</cell><cell>T</cell><cell>d MLP Discourse-level d z d  d </cell><cell cols="2">s Word-level s MLP LSTM 0 &lt;start&gt; nothing t w z s t LSTM …</cell><cell>t+1 w LSTM z but</cell><cell>LSTM working</cell></row></table><note><p><p><p>1) For z d , the bag-of-word method</p><ref type="bibr" target="#b5">[Zhao et al., 2017</ref></p>] is adopted. An auxiliary objective log p(y bow |z d ) is used to force z d to capture global bag-of-words information y bow as,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>The evaluation results of human judgements, BLEU-4 score and embedding-based score of Average, Extrema and Greedy.</figDesc><table><row><cell>Models</cell><cell cols="3">Wins Losses Ties</cell></row><row><cell>SARN vs Seq2Seq</cell><cell>55.6</cell><cell>11.3</cell><cell>33.1</cell></row><row><cell cols="2">SARN vs Seq2Seq-MMI 46.4</cell><cell>22.2</cell><cell>31.4</cell></row><row><cell>SARN vs VRNN</cell><cell>42.2</cell><cell>20.5</cell><cell>37.3</cell></row><row><cell>SARN vs CVRNN</cell><cell>36.1</cell><cell>23.7</cell><cell>40.2</cell></row><row><cell>SARN vs SpaceFusion</cell><cell>23.2</cell><cell>19.5</cell><cell>57.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>The pairwise comparisons from human judgements.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 ,</head><label>1</label><figDesc></figDesc><table><row><cell cols="4">α (β = 0.05) KL(z d ) Diversity Acceptable(Top-5)</cell></row><row><cell>0</cell><cell>0.1</cell><cell>1.53</cell><cell>61.4</cell></row><row><cell>0.1</cell><cell>2.2</cell><cell>2.31</cell><cell>63.2</cell></row><row><cell>0.3</cell><cell>5.6</cell><cell>2.95</cell><cell>69.3</cell></row><row><cell>0.7</cell><cell>7.9</cell><cell>3.12</cell><cell>65.2</cell></row><row><cell>1</cell><cell>10.2</cell><cell>3.10</cell><cell>62.5</cell></row><row><cell cols="4">Table 3: Parameter analysis for hyper-parameter α.</cell></row><row><cell cols="4">β (α = 0.3) ΣKL(zt) Diversity Acceptable(Top-5)</cell></row><row><cell>0</cell><cell>0.7</cell><cell>1.90</cell><cell>61.2</cell></row><row><cell>0.03</cell><cell>4.2</cell><cell>2.14</cell><cell>63.7</cell></row><row><cell>0.05</cell><cell>5.6</cell><cell>2.95</cell><cell>69.3</cell></row><row><cell>0.1</cell><cell>13.4</cell><cell>3.88</cell><cell>53.3</cell></row><row><cell>0.2</cell><cell>20.2</cell><cell>4.20</cell><cell>50.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Parameter analysis for hyper-parameter β.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>is still a lot to learn and a long way to go</head><label></label><figDesc>Comment examples. The color histograms represent the accumulated attention weights on the post word of the same color.</figDesc><table><row><cell>Model</cell><cell>Seq2Seq</cell><cell></cell><cell cols="2">Seq2Seq-MMI</cell><cell>VRNN</cell><cell>CVRNN</cell><cell>SARN</cell></row><row><cell>Post</cell><cell></cell><cell></cell><cell cols="4">每天上班来回真心累 I am really tired from going and returning from work every day</cell></row><row><cell>Comment1</cell><cell>每天都很累 Tired every day</cell><cell></cell><cell cols="2">我也是每天上班的 I go to work every day too</cell><cell>每天都会累 I am tired every day</cell><cell>我每天都在上班 I am working every day</cell><cell>上班的人伤不起 People who work can't afford to hurt</cell></row><row><cell>Comment2</cell><cell>我也是很累 I am also very tired</cell><cell></cell><cell>每天都很累 Tired every day</cell><cell></cell><cell>工作是多么的累啊 How tired is the work</cell><cell>我也累啊 I am also very tired</cell><cell>累并快乐着 Tired and happy</cell></row><row><cell>Comment3</cell><cell cols="2">我也很累啊 I am also very tired, ah~</cell><cell cols="2">我也是好累 I am also really tired</cell><cell>我也要上班了 I have to go to work too</cell><cell>累了就休息一下 Take a little break when you are tired</cell><cell>累了就休息 Take a break when you are tired</cell></row><row><cell>Post</cell><cell></cell><cell></cell><cell></cell><cell cols="3">天气这么好做点什么好呢 What to do with the good weather</cell></row><row><cell cols="5">Comment1 Comment2 Figure 4: Post: 要学的还很多要走的路还很长 我也很想干点啥呢 I want to do something 今天天气很好 The weather today is good 好天气就好了 Good weather is good 我这里天气很好啊 The weather here is very good Comment3 今天天气很好 It's a nice day 我也很喜欢下雨 I like raining very much too There α=0.01 β=0.01 α=0.3 β=0.01 α=1 β=0.01 Diversity=1.14 Diversity=2.31 Diversity=2.40 Acceptable=63.2 Acceptable=63.0 Acceptable=61.7</cell><cell>那就做点晒太阳 Then do some sun exposure 对啊希望有好天气 Yes, I hope there is good weather 晚上是会热的吧 It's going to be hot at night</cell><cell>天气不错心情不错 Good weather, good mood 天气是很好呢你要多休息 The weather is good, you have to get more rest 天气好热 The weather is hot</cell><cell>心情好请我们吃饭 Good mood, you should treat us a meal 天气好啊我想睡觉 The weather is good, I just want to sleep 来深圳吧 Come to Shenzhen City</cell></row><row><cell cols="2">路走着走着就走出来</cell><cell cols="2">想走就去找就可以了</cell><cell cols="2">世界上就没有那么多废话</cell></row><row><cell cols="2">You walk and then</cell><cell cols="2">If you want to go, you</cell><cell cols="2">There are not so many</cell></row><row><cell cols="2">you come out</cell><cell cols="2">should find your own way</cell><cell cols="2">nonsense in the world</cell></row><row><cell cols="2">路是自己走出来的 You should walk the road by yourself</cell><cell cols="2">加油路是很长的 Come on, the road is very long</cell><cell cols="2">只要是路就好 As long as there is a way</cell></row><row><cell cols="2">路是自己走的路 走着走着出来 You walk the road by yourself and then you come out</cell><cell cols="2">那你就走过去吧 Then you just walk through the way</cell><cell cols="2">走就有开始 There is always a start when you walk</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>https://github.com/golsun/SpaceFusion</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>https://github.com/ysjakking/weibo-comments Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported in part by the <rs type="funder">National Key R&amp;D Program of China</rs> under Grand: <rs type="grantNumber">2018AAA0102003</rs>, in part by <rs type="funder">National Natural Science Foundation of China</rs>: <rs type="grantNumber">61771457</rs>, <rs type="grantNumber">61732007</rs>, <rs type="grantNumber">61672497</rs>, <rs type="grantNumber">61772494</rs>, <rs type="grantNumber">61931008</rs>, <rs type="grantNumber">U1636214</rs>, <rs type="grantNumber">61622211</rs>, <rs type="grantNumber">61702491</rs>, and in part by <rs type="funder">Key Research Program of Frontier Sciences, CAS</rs>: <rs type="grantNumber">QYZDJ-SSW-SYS013</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Xde4dUH">
					<idno type="grant-number">2018AAA0102003</idno>
				</org>
				<org type="funding" xml:id="_FyjWkCN">
					<idno type="grant-number">61771457</idno>
				</org>
				<org type="funding" xml:id="_8pAcSxJ">
					<idno type="grant-number">61732007</idno>
				</org>
				<org type="funding" xml:id="_dPtWYGG">
					<idno type="grant-number">61672497</idno>
				</org>
				<org type="funding" xml:id="_pwvhwEV">
					<idno type="grant-number">61772494</idno>
				</org>
				<org type="funding" xml:id="_mgmfcZW">
					<idno type="grant-number">61931008</idno>
				</org>
				<org type="funding" xml:id="_DByh6xB">
					<idno type="grant-number">U1636214</idno>
				</org>
				<org type="funding" xml:id="_nh4ZuFJ">
					<idno type="grant-number">61622211</idno>
				</org>
				<org type="funding" xml:id="_kVk9SD6">
					<idno type="grant-number">61702491</idno>
				</org>
				<org type="funding" xml:id="_db2aTgd">
					<idno type="grant-number">QYZDJ-SSW-SYS013</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Jointly optimizing diversity and relevance in neural response generation</title>
		<author>
			<persName><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.05414</idno>
		<idno>arXiv:1904.09751</idno>
	</analytic>
	<monogr>
		<title level="m">Conference of the European Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="1996">2015. 2015. 2017. 2017. 2015. 2015. 2017. 2017. 2019. 2019. 1996. 1996. 2019</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="701" to="704" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>International Conference on Spoken Language. Holtzman et al., 2019] Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName><forename type="first">;</forename><surname>Jieba</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welling ; Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<ptr target="https://pypi.org/project/jieba/" />
	</analytic>
	<monogr>
		<title level="m">Jieba</title>
		<editor>
			<persName><surname>Liu</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM MM</publisher>
			<date type="published" when="2013">2018. 2018. 2013. 2013. 2016. 2016. 2018. 2018</date>
		</imprint>
	</monogr>
	<note>Context-aware visual policy network for sequence-level image captioning</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00970</idno>
		<idno>arXiv:1610.02424</idno>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks</title>
		<editor>
			<persName><surname>Park</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014">2019. 2019. 2016. 2016. 2018. 2017. 2017. 2017. 2017. 2017. 2017. 2019. 2019. 2014. 2016. 2016. 2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3351" to="3357" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making history matter: History-advantage sequence training for visual dialog</title>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01292</idno>
	</analytic>
	<monogr>
		<title level="m">An attentional neural conversation model with improved specificity</title>
		<imprint>
			<date type="published" when="2016">2019. 2019. 2016. 2016. 2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>IEEE ICCV</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating informative and diverse conversational responses via adversarial information maximization</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1810" to="1820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="654" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Emotional chatting machine: emotional conversation generation with internal and external memory</title>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01074</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence, AAAI</title>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Mechanism-aware neural machine for dialogue response generation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
