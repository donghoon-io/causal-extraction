<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Dirichlet Belief Networks for Interpretable Dynamic Relational Data Modelling</title>
				<funder ref="#_hHC6Mte">
					<orgName type="full">ARC</orgName>
				</funder>
				<funder ref="#_uqNvsQJ">
					<orgName type="full">UTS</orgName>
				</funder>
				<funder>
					<orgName type="full">Australian Research Council</orgName>
				</funder>
				<funder ref="#_jqyRBgf">
					<orgName type="full">Shanghai Municipal Science &amp; Technology Commission</orgName>
				</funder>
				<funder ref="#_dq9fNN4">
					<orgName type="full">Australian Centre of Excellence in Mathematical and Statistical Frontiers</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yaqiong</forename><surname>Li</surname></persName>
							<email>yaqiong.li@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuhui</forename><surname>Fan</surname></persName>
							<email>xuhui.fan@unsw.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematics &amp; Statistics</orgName>
								<orgName type="institution">University of New South Wales</orgName>
								<address>
									<country>Sydney</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ling</forename><surname>Chen</surname></persName>
							<email>ling.chen@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Yu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Alberta</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Sisson</surname></persName>
							<email>scott.sisson@unsw.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematics &amp; Statistics</orgName>
								<orgName type="institution">University of New South Wales</orgName>
								<address>
									<country>Sydney</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Dirichlet Belief Networks for Interpretable Dynamic Relational Data Modelling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Dirichlet Belief Network (DirBN) has been recently proposed as a promising approach in learning interpretable deep latent representations for objects. In this work, we leverage its interpretable modelling architecture and propose a deep dynamic probabilistic framework -the Recurrent Dirichlet Belief Network (Recurrent-DBN) -to study interpretable hidden structures from dynamic relational data. The proposed Recurrent-DBN has the following merits: (1) it infers interpretable and organised hierarchical latent structures for objects within and across time steps; (2) it enables recurrent long-term temporal dependence modelling, which outperforms the one-order Markov descriptions in most of the dynamic probabilistic frameworks; (3) the computational cost scales to the number of positive links only. In addition, we develop a new inference strategy, which first upwardand-backward propagates latent counts and then downward-and-forward samples variables, to enable efficient Gibbs sampling for the Recurrent-DBN. We apply the Recurrent-DBN to dynamic relational data problems. The extensive experiment results on real-world data validate the advantages of the Recurrent-DBN over the state-of-the-art models in interpretable latent structure discovery and improved link prediction performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dynamic data is a common feature in many real-world applications, including relational data analysis <ref type="bibr">[Mucha et al., 2010;</ref><ref type="bibr" target="#b15">Phan and Airoldi, 2015;</ref><ref type="bibr" target="#b17">Yang and Koeppl, 2018]</ref> for learning time-varying node interactions, and text modelling <ref type="bibr" target="#b12">[Guo et al., 2018;</ref><ref type="bibr">Schein et al., 2019]</ref> for exploring topic evolution. Modelling dynamic data has become a vibrant research topic, with popular techniques ranging from non-Bayesian methods, such as Collaborative Filtering with Temporal Dynamics (SVD++) <ref type="bibr" target="#b15">[Koren, 2009]</ref>, to Bayesian deep probabilistic frameworks such as Deep Poisson-Gamma Dynamical Systems (DPGDS) <ref type="bibr" target="#b12">[Guo et al., 2018]</ref>. The main advantage of Bayesian deep probabilistic frameworks is the flexible model design and the strong modelling performance. However, most of these frameworks are static so that they cannot account for the evolution of relationships over time. It would be highly beneficial if the frameworks can be extended to the dynamic setting to enjoy the modelling advantages.</p><p>The Dirichlet Belief Network (DirBN) <ref type="bibr" target="#b18">[Zhao et al., 2018</ref>] has been proposed recently as a promising deep probabilistic framework for learning interpretable deep latent structures. To date, the DirBN has mainly been used in two applications:</p><p>(1) topic structure learning <ref type="bibr" target="#b18">[Zhao et al., 2018]</ref>, where latent representations are used to model the word distribution for topics; and (2) relational models <ref type="bibr">[Fan et al., 2019a]</ref>, where latent representations model the nodes' membership distribution over communities. By constructing a deep architecture for latent distributions, the DirBN can model high-order dependence between topic-word distributions (in topic models) and nodes' membership distributions (in relational models).</p><p>In this work, we propose a Recurrent Dirichlet Belief Network (Recurrent-DBN) to explore the complex latent structures in dynamic relational data. In addition to constructing an interpretable deep architecture for the data within individual time steps, we also study the temporal dependence in the dynamic relational data through (layer-to-layer) connections crossing consecutive time steps. Consequently, our Recurrent-DBN can describe long-term temporal dependence (i.e., the dependence between the current variables and those in the previous several time steps), improving over the one-order Markov structures that usually describe the dependence between the current variables and those in the previous one time step only.</p><p>For model inference, we further develop an efficient Gibbs sampling algorithm. Besides upward propagating latent counts as done by DirBN, we also introduce a backward step to propagate the counts from the current time step to the previous time steps. Our experiments on real-world dynamic relational data show significant advantages of the Recurrent-DBN over the state-of-the-art models in tasks of interpretable latent structure discovery and link prediction. Similar to DirBN that can be considered as a self-contained module <ref type="bibr" target="#b18">[Zhao et al., 2018]</ref>, our Recurrent-DBN could be flexibly adapted to account for dynamic data other than evolving relational data, such as time-varying counts and dynamic drifting text data.</p><p>We summarise this paper's main merits as follows: Model Recurrent structures are designed to model long term temporal dependence. Also, interpretable and organised latent structures are well explored; Inference An efficient Gibbs sampling method is devised that first upward-backward propagates latent counts and then downward-forward samples variable; Results Significantly improved model performance in realworld dynamic relational models compared to the stateof-the-art, including better link prediction performance and enhanced interpretable latent structure visualisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background Information of DirBN</head><p>We first give a brief review of the DirBN model. In general, the DirBN constructs a multi-stochastic layered architecture to represent interpretable latent distributions for objects. We describe it within the relational data setting for illustrative purposes. Given a binary observed linkage matrix R R R ∈ {0, 1} N ×N for N nodes, where R ij denotes whether node i has a relation to node j, the DirBN constructs an L-layer and K-length community membership distribution π π π i ={π π π</p><formula xml:id="formula_0">(l) i } L l=1</formula><p>for each node i. The generative process for the membership distributions {π π π (l) i } L l=1 , as well as the observed matrix R R R, can be briefly described as:</p><p>1. For l = 1, . . . , L</p><formula xml:id="formula_1">(l-1) i i ∼ Gam(c, 1 d ), ∀i, i = 1, . . . , N (b) π π π (l) i ∼ Dirichlet(α α α 1×K 1 1 1(l = 1) + i β (l-1) i i π π π (l-1) i ) 2. X X X i ∼ Multinomial(M ; π π π (L) i ), ∀i = 1, . . . , N ; 3. R ij ∼ Bernoulli (f (X X X i , X X X j )) , ∀i, i = 1, . . . , N ;<label>(a) β</label></formula><p>where α α α 1×K is a concentration parameter generating the membership distribution in the 1st layer, β (l-1) i i represents the information propagation coefficient from node i to node i in the (l -1)th layer, c and d are the hyper-parameters generating these propagation coefficients, X X X i is the latent count information for node i and M is the sum of these counts, and f (X X X i , X X X j ) represents the probabilistic function mapping a pair of membership distributions to a linkage probability. A larger value of β</p><formula xml:id="formula_2">(l-1) i i indicates higher influence of π π π (l-1) i on the generation of π π π (l) i . Therefore, β (l-1) i i is set to 0 if node i is not connected to node i in the observed data R R R.</formula><p>It is difficult to directly implement efficient Gibbs sampling for the DirBN because the prior and posterior distributions of the membership distributions π π π (l) i are not conjugate. To address this issue, a strategy of first upward propagating latent counts and then downward sampling variables has been <ref type="bibr">developed in [Zhao et al., 2018]</ref>. Given the count information X X X i for node i, the DirBN upward propagates X X X i to all the nodes in the (L -1)th layer through a Chinese Restaurant Table (CRT) distribution. Each node in the (L -1)th layer collects these propagated counts and uses their sum as its latent count X X X (L-1) i in the (L -1)th layer. This procedure is repeated until the counts have been assigned to all layers. Thus, conjugate constructions can be created for each variable and thereby used to construct efficient Gibbs samplers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Recurrent-DBN for Dynamic Relational Data Modeling</head><p>To handle dynamic relational data, we attach an index t to variables to denote the corresponding time step. Thus, the observed dynamic relational data can be described as R R R ∈ {0, 1} N ×N ×T for N nodes at T time steps, where R ij,t denotes whether node i has relation to node j at the tth time step. Each matrix {R R R -,t } t ∈ {0, 1} N ×N can be either asymmetric (directional) or symmetric (non-directional) and we do not consider self-linkages {R ii,t } i,t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Recurrent-DBN for Latent Structure Generation</head><p>In the Recurrent-DBN, we assume the time-dependent membership distribution of a node i in the l-th layer at time step t, π π π</p><formula xml:id="formula_3">(l)</formula><p>i,t , follows a Dirichlet distribution. Its generative process can be described as below, with the propagation of π π π (l) i,t illustrated in Fig. <ref type="figure" target="#fig_0">1</ref> (Left). For notation convenience, any parameters with index 0 are set to zero. It is noted that we have already used the observed data into the generative process.</p><formula xml:id="formula_4">1. For t = 1, . . . , T, l = 1, . . . , L (a) For i , i = 1, . . . , N i. β (l-1) i i,t      = 0, if R i i,t = 0; ∼ Gam(c (l) c , 1 dc ), if i = i; ∼ Gam(c (l) u , 1 dc ), if i = i. ii. γ (l) i i,t-1      = 0, if R i i,t-1 = 0; ∼ Gam(c (l) c , 1 dc ), if i = i; ∼ Gam(c (l) u , 1 dc ), if i = i. (b) For i = 1, . . . , N i. Calculate concentration parameter ψ ψ ψ (l) i,t : ψ ψ ψ (l) i,t = i β (l-1) i i,t π π π (l-1) i ,t + i γ (l) i i,t-1 π π π (l) i ,t-1 . (1) ii. π π π (l) i,t ∼ Dirichlet(α α α 1×K 1 1 1(t = 1, l = 1) + ψ ψ ψ (l) i,t ).</formula><p>Here, β (l-1) i i,t ∈ R + is the information propagation coefficient from node i in the (l -1)-th layer to node i in the l-th layer at the same time t, γ</p><formula xml:id="formula_5">(l) i i,t-1 ∈ R + is the information propagation coefficient from node i at time t -1 to node i at time t in the same layer l, c (-) c , c (-)</formula><p>u , d c are the corresponding hyperparameters and α α α 1×K is the concentration parameter for the membership distribution in the first layer. The larger the value of these coefficients, the stronger the connections between the two corresponding latent representations (i.e., π π π</p><formula xml:id="formula_6">(l-1) i ,t and π π π (l) i,t , π π π (l) i ,t-1 and π π π (l) i,t ).</formula><p>We restrict the two nodes to have information propagated only if they are observed with positive relationship (step (a).i and (a).ii). This can reduce the computational cost of calculating β β β positive relationships. Also, it encourages connected nodes to have more similar membership distributions and larger dependencies between each other.</p><formula xml:id="formula_7">(l) -,t , γ γ γ (l) -,t from O(N 2 ) to the scale of the number of Section 3.1 Section 3.2 π (1) •,3 π (2) •,3 π (3) •,3 X •,3 π (1) •,2 π (2) •,2 π (3) •,2 X •,2 π (1) •,1 π (2) •,1 π (3) •,1 X •,1 β (2) •• β (1) •• γ (•) •,1 γ (•) •,2 C •,2 R •,2 Λ A (1) •,3 Z (1) •,3 m (1) •,3 A (2) •,3 Z (2) •,3 m (2) •,3 A (3) •,3 m (3) •,3 X •,3 A (1) •,2 Z (1) •,2 m (1) •,2 A (2) •,2 Z (2) •,2 m (2) •,2 A (3) •,2 m (3) •,2 X •,2 Z (1) •,1 m (1) •,1 Z (2) •,1 m (2) •,1 m (3) •,1 X •,1</formula><p>The concentration parameter ψ ψ ψ (l) i,t for generating π π π (l) i,t comprises two parts: the information propagated from all other nodes' latent representations in the (l -1)-th layer at time t, i β (l-1) i i,t π π π (l-1) i ,t , and those in the l-th layer at time (t -1),</p><formula xml:id="formula_8">i γ (l) i i,t-1 π π π (l) i ,t-1 . In other words, ψ ψ ψ (l)</formula><p>i,t is a linear sum of all the previous-layers' information at the same time step and all the previous-time steps' information in the same layer. When the coefficients β β β dominate over γ γ γ, the hierarchical structure plays a more important role. Otherwise, the temporal dependence has higher influence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Application to Dynamic Relational Data</head><p>After generating the membership distributions {π π π (l) i,t }, we use the Bernoulli-Poisson link function <ref type="bibr" target="#b2">[Dunson and Herring, 2005;</ref><ref type="bibr" target="#b19">Zhou, 2015;</ref><ref type="bibr">Fan et al., 2019a]</ref> to generate the relational data at each time step:</p><p>1.</p><formula xml:id="formula_9">Λ k1k2 ∼ Gamma(λ 1 , λ 0 ), ∀k 1 , k 2 2. M i,t ∼ Poisson(M ), ∀i, t 3. X X X i,t ∼ Multinomial(M i,t ; π π π (L) i,t ), ∀i, t; 4. For t = 1, . . . , T, i, j = 1, . . . , N , (a) C ij,k1k2,t ∼ Poisson(X i,k1,t Λ k1k2 X j,k2,t ), ∀k 1 , k 2 (b) R ij,t = 1 1 1( k1,k2 C ij,k1k2,t &gt; 0),</formula><p>where Λ k1k2 is a community compatibility parameter such that a larger value of Λ k1k2 indicates a larger possibility of generating the links between communities k 1 and k 2 , λ 1 , λ 0 , M are hyper-parameters, M i,t is a scaling parameter for generating the related counting information for node i at time t, and C ij,k1k2,t is a community-to-community latent integer for linkage R ij at time t.</p><p>Through the Multinomial distributions with π π π (L) i,t as event probabilities, X X X i,t can be regarded as an estimator of π π π (L) i,t . Since the sum M i ∼ Poisson(M ), according to the Poisson-Multinomial equivalence, each X i,k,t is equivalently distributed as X i,k,t ∼ Poisson(M π (L) i,k,t ). Therefore, both the prior distribution for generating X i,k,t and the likelihood based on X i,k,t are Poisson distributions. We may form feasible categorical distribution on its posterior inference. This trick is inspired by the recent advances in data augmentation and marginalisation techniques <ref type="bibr">[Fan et al., 2019a]</ref>, which allows us to implement posterior sampling for X i,k,t efficiently.</p><p>The counts X X X i,t lead to the generation of the K × K integer matrix C C C ij,t . Based on the Bernoulli-Poisson link function <ref type="bibr" target="#b2">[Dunson and Herring, 2005;</ref><ref type="bibr" target="#b19">Zhou, 2015]</ref>, the observed R ij,t is mapped to the latent Poisson count random variable matrix</p><formula xml:id="formula_10">C C C ij,t . It is shown in [Fan et al., 2019a] that {C ij,k1k2,t } k1,k2 = 0 if R ij,t = 0.</formula><p>That is, only the non-zero links are involved during the inference for C C C ij,k1k2,t , which largely reduces the computational complexity, especially for large and sparse dynamic relational data.</p><p>Recurrent structure. Before describing the inference of Recurrent-DBN, we discuss the characteristic of the recurrent structure of our model. Instead of using the one-order Markov property to describe the temporal dependence (assuming the state at time t depends on the states at time t -1 only), which is adopted by most probabilistic dynamic models, the deep structure of the Recurrent-DBN allows the latent variables at time t depend on those at time steps from t -1 to t -L. For example, by using the law of total expectations, we can have the expectation of the latent count X X X -,t in a 2-layered Recurrent-DBN as (We use the notationto denote a related parameter or variable hereafter):</p><formula xml:id="formula_11">E [X X X -,t |-] =β β β (L-1) -,t-1 γ γ γ (L-1) -,t-1 π π π (L) -,t-1 + β β β (L-1) -,t-1 β β β (L-2) -,t-1 γ γ γ (L-2) -,t-1 γ γ γ (L-2) -,t-2 π π π (L-1) -,t-2 . (2)</formula><p>In Eq. ( <ref type="formula">2</ref>), E [X X X -,t |-] depends on both π π π -,t-1 and π π π -,t-2 . This format can be extended straightforwardly to L-layers and involve more previous membership distributions. Such recurrent structures allow us to summarise and abstract those random variables, capturing both the hierarchical latent structures and the dynamic dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference</head><p>The joint distribution of the latent variables is expressed as:</p><formula xml:id="formula_12">P (π π π, β β β, γ γ γ, X X X, C C C, Λ Λ Λ, R R R|-) = i ,i,t P (R i ,i,t |C C C)P (Λ Λ Λ|λ1, λ0) • i,l,t P (π π π (l) i,t |π π π (l) -,t-1 , π π π (l-1) -,t , β β β (l) -i,t , γ γ γ (l) -i,t ) • P (β (l) -i,t |-)P (γ (l) -i,t |-) • i,t   P (Xi,t|π π πi,t, M ) i ,k 1 ,k 2 P (C i i,k 1 k 2 ,t |Xi,t, Xj,t, Λ k 1 k 2 )  </formula><p>While the DirBN only has upward-propagation for the latent counts and downward-sampling for the latent variables, for the Recurrent-DBN we develop an upward-backward propagation and forward-downward Gibbs sampling algorithm for count propagation and latent variable sampling. Posterior simulation for the Recurrent-DBN involves two key steps in each sampling iteration: (1) propagating the counts X X X upward and backward to the upper layers and previous time steps via a latent count variable m m m; (2) forward and downward sampling π π π, β β β, γ γ γ given the propagated latent counts m m m. Full updates for the other variables are similar to those in <ref type="bibr">[Fan et al., 2019a]</ref>.</p><p>Upward-Backward Propagating the Latent Counts Figure <ref type="figure" target="#fig_0">1</ref> (right) illustrates the upward-backward propagation of counts X X X to the latent count variable m m m at each hidden layers. Generally speaking, for i, i = 1, . . . , N, l = 1, . . . , L, t = 1, . . . , T, k = 1, . . . , K, the latent variable ψ ψ ψ is generated as Eq. ( <ref type="formula">1</ref>). m (l) i,k,t refers to the latent counts for the node i in layer l at time t for the k-th community. By integrating the m (l) i,k,t , the likelihood term of ψ ψ ψ (l) i,t can be calculated as:</p><formula xml:id="formula_13">L(ψ ψ ψ (l) i,t ) ∝ Γ( k ψ (l) i,k,t ) Γ( k ψ (l) i,k,t + k m (l) i,k,t ) k Γ(ψ (l) i,k,t + m (l) i,k,t ) Γ(ψ (l) i,k,t )</formula><p>where Γ(-) is a Gamma function.</p><p>By introducing the auxiliary variables q (l) i,t and y (l)</p><p>i,k,t , the likelihood term of ψ ψ ψ (l) i,t can be further augmented as:</p><formula xml:id="formula_14">L(ψ ψ ψ (l) i,t , q (l) i,t , y (l) i,k,t ) ∝ K k=1 q (l) i,t ψ (l) i,k,t ψ (l) i,k,t y (l) i,k,t</formula><p>where the q (l) i,t and y (l) i,k,t can be generated as:</p><formula xml:id="formula_15">y (l) i,k,t ∼ CRT(m (l) i,k,t , ψ (l) i,k,t ), q (l) i,t ∼ Beta( k ψ (l) i,k,t , k m (l) i,k,t )</formula><p>Consequently, y (l) i,k,t can be considered as the 'derived latent counts' for node i derived from the latent counts m (l) i,k,t . Each y (l) i,k,t can then be upward and backward distributed based on the probabilities of ψ (l) i,k,t as follows:</p><formula xml:id="formula_16">(Z (l-1) i1,k,t , . . . , Z (l-1) iN,k,t , A (l) i1,t-1,k , . . . , A (l) i1,t-1,k ) ∼Multinomial(y (l) i,k,t ; β β β (l-1) -i,t π π π (l-1) -,k,t ψ (l) i,k,t , γ γ γ (l) -i,t-1 π π π (l) -,t-1,k ψ (l) i,k,t )<label>(3)</label></formula><p>Here, the y</p><formula xml:id="formula_17">(l) i,k,t is divided into two parts: one is delivered to each i at time t of layer l -1 ((Z (l-1) i1,k,t , . . . , Z (l-1)</formula><p>iN,k,t )), and the other to each i at time t -1 of layer l (A</p><formula xml:id="formula_18">(l) i1,t-1,k , . . . , A (l) i1,t-1,k )). We denote them as Z Z Z (l-1) i-,k,t and A A A (l)</formula><p>i-,t-1,k respectively. The latent counts of lower layers and previous time steps can thus be calculated respectively as:</p><formula xml:id="formula_19">m (l-1) i,k,t = i Z (l-1) i i,k,t + i A (l-1) i i,k,t m (l) i,t-1,k = i Z (l) i i,t-1,k + i A (l) i i,t-1,k (4) Let m m m (L) i,T = X X X i,T , for t = T -1, . . . , 2, i, i = 1, . . . , N , the specification in terms of layer L is as follows, m (L) i,t-1,k = X i,t-1,k + i A (L) i i,t-1,k<label>(5)</label></formula><p>To summarize, upward and backward propagation derives y y y (l) i,t from the latent counts m m m (l) i,t . Then, y y y (l) i,t is distributed to all i at time t of layer l -1 and time t -1 of layer l respectively as Z Z Z</p><formula xml:id="formula_20">(l-1) i-,k,t and A A A (l) i-,t-1,k . Lastly, Z Z Z (l-1) -i,k,t and A A A (l) -i,t-1,k con- tribute to the generation of m m m (l-1) i,t</formula><p>and m m m (l) i,t-1 respectively. By repeating this process through layers and crossing time steps, we propagate the X X X to the m m m (l) upward and backward sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forward-Downward Sampling Latent Variables</head><p>The generated ψ ψ ψ, q q q, m m m (l) , (Z Z Z, A A A) can enable to form closed Gibbs sampling algorithm for the following variables:</p><formula xml:id="formula_21">Sampling {π π π (l) i,t } i,t,l After obtaining the latent counts m m m (l) i,t</formula><p>for each layer and each time step, the posterior inference of π π π (l) i,t can be proceeded as:</p><formula xml:id="formula_22">π π π (l) i,t ∼ Dirichlet(ψ ψ ψ (l) i,t + m m m (l) i,t ) Sampling {β (l) i i,t , γ (l) i i,t } i ,i,l,t The likelihood term of β (l) i i,t</formula><p>can be represented as:</p><formula xml:id="formula_23">L(β (l) i i,t ) ∝ e log q (l) i,t β (l) i i,t β (l) i i,t k Z (l) i i,k,t</formula><p>The prior of β</p><formula xml:id="formula_24">(l) i i,t , β (l) i i,t is Gam(γ (l) i , 1 c (l) ). Their posterior distribution is β (l) i i,t ∼Gam(γ (l) i + k Z (l) i i,k,t , 1 c (l) -log q (l) i ,t ) γ (l) i i,t ∼Gam(γ (l) i + k A (l) i i,k,t , 1 c (l) -log q (l) i ,t ) 4 Related Work</formula><p>Several Bayesian deep probabilistic frameworks have been proposed to capture the temporal dependence in dynamic data <ref type="bibr" target="#b10">[Gan et al., 2015;</ref><ref type="bibr" target="#b11">Gong, 2017;</ref><ref type="bibr" target="#b13">Henao et al., 2015]</ref>. The Deep Dynamic Sigmoid Belief Network <ref type="bibr" target="#b10">[Gan et al., 2015]</ref> sequentially stacks models of sigmoid belief networks and uses the binary-valued hidden variables to depict the log-range dynamic dependence. The Deep Dynamic Poisson Factor Analysis (DDPFA) <ref type="bibr" target="#b11">[Gong, 2017]</ref> incorporates the Recurrent Neural Networks (RNN) into the Poisson Factor Analysis (PFA) to depict the long-range dynamic dependence. However, in DDPFA, the parameters in RNN and the latent variables in PFA are optimized separately. Poisson Gamma Dynamic Systems (PGDS) <ref type="bibr">[Schein et al., 2016]</ref> are developed to model the counting data through a "shallow" modelling strategy. Dynamic-PGDS (DPGDS) <ref type="bibr" target="#b12">[Guo et al., 2018]</ref>   <ref type="bibr" target="#b17">Koeppl, 2018]</ref> are the representative works that use Poisson matrix factorization techniques to address dynamic counting data. There are also some models using the collaborative filtering techniques such as SVD++. Some methods are not developed for dynamic network data originally, but they have later been applied to the dynamic scenario, such as structurebased models like Common Neighbor (CN) <ref type="bibr" target="#b15">[Newman, 2001]</ref>, and network embedding models, including Scalable Multiplex Network Embedding (MNE) <ref type="bibr" target="#b18">[Zhang et al., 2018]</ref> and <ref type="bibr">DeepWalk [Perozzi et al., 2014]</ref>. It is noted that there is a recent trend in using the graphon theory <ref type="bibr" target="#b15">[Lloyd et al., 2012;</ref><ref type="bibr" target="#b15">Orbanz and Roy, 2014]</ref> to model the network data <ref type="bibr" target="#b4">[Fan et al., 2016;</ref><ref type="bibr">2018b;</ref><ref type="bibr">2018a;</ref><ref type="bibr">2019b;</ref><ref type="bibr" target="#b9">2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate the performance of our proposed Recurrent-DBN on five real-world data sets, by comparing with nine baseline methods: Mixed Membership Stochastic Block model (MMSB) <ref type="bibr" target="#b0">[Airoldi et al., 2008]</ref>, T-MBM, fcMMSB, BPTF, DRGPM, SVD++, CN, MNE and DeepWalk. Except MMSB, all of the other eight baseline models are implemented with the released code. For MMSB, we use Gibbs sampling for the inference of all variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Set and Experimental Setting</head><p>The real-world relational data sets used in this paper are: Coleman <ref type="bibr" target="#b1">[Coleman, 1964]</ref>, Mining Reality [Eagle and <ref type="bibr" target="#b2">Pentland, 2006]</ref>, Hypertext <ref type="bibr" target="#b14">[Isella et al., 2011]</ref>, Infectious <ref type="bibr" target="#b14">[Isella et al., 2011] and</ref><ref type="bibr">Student Net [Fan et al., 2014]</ref>. The summarized statistics are detailed in Table <ref type="table">2</ref>. For the hyperparameters, we specify M ∼ Gamma(N, 1) for all data sets, {c </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Link Prediction</head><p>For link prediction, we randomly extract a proportion of 10% of relational data entries (either links or non-links) at each time step as the test set. The remaining 90% is used for training. The test relational data are not used to construct the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a probabilistic deep hierarchical structure named Recurrent Dirichlet Belief Networks (Recurrent-DBN) for learning dynamic relational data. Through Recurrent-DBN, the evolution of the latent structure is characterized by both the cross-layer and the cross-time depen- dencies. We also develop an upward-backward-forwarddownward information propagation to enable efficient Gibbs sampling for all variables. The experimental results on a variety of real data sets demonstrate the excellent predictive performance of our model, and the inferred latent structure provides a rich interpretation for both hierarchical and dynamic information propagation. Our Recurrent-DBN can be applied to tasks like dynamic topic models <ref type="bibr" target="#b12">[Guo et al., 2018;</ref><ref type="bibr" target="#b18">Zhao et al., 2018]</ref>) and dynamic collaborative filtering. We keep these potential applications as the future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left: a brief graphical model of Recurrent-DBN with 3hidden-layers for a dynamic relational data with time steps (Sections 3.1&amp;3.2), where shaded nodes represent observed data. Hyperparameters are ignored for concise presentation. Right: the upwardbackward propagation of counts X X X to each hidden layers in each inference iteration (Section 3.3), where m m m (-) -,-represents the latent counts attached to nodes at each layer and time step, Z Z Z (-) -,-refers to the layer-wise propagated counts and A A A (-) -,-is the propagated counts between consecutive time steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>u } l , d, d c and Λ k1,k2 are all given Gamma(1, 1) priors and L = 3. For MMSB, we set the membership distribution according to Dirichlet(1 1 1 1×K ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :t</head><label>2</label><figDesc>Figure 2: Top: visualizations of the membership distributions ({π π π (l) i=1:30 } 3 l=1 ) for the Infectious data set. Rows represent the nodes and columns represent the communities (with K = 10); Bottom: visualizations of average propagation coefficients β β β (l) t , γ γ γ (l) t and their ratio. β β β (l) t , γ γ γ (l) t are re-scaled for visualization convenience.</figDesc><graphic coords="6,328.14,54.00,216.72,177.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>is probably the closest work to our approach. Compared with DPGDS, our Recurrent-DBN differs in three aspects: (1) our Recurrent-DBN generates normalized latent representations and thus Recurrent-DBN,K=30 0.569 ± 0.022 0.881 ± 0.003 0.509 ± 0.017 0.543 ± 0.022 0.373 ± 0.016 Recurrent-DBN,K=20 0.476 ± 0.081 0.869 ± 0.003 0.468 ± 0.013 0.469 ± 0.026 0.361 ± 0.016 Recurrent-DBN,K=10 0.457 ± 0.042 0.853 ± 0.007 0.450 ± 0.014 0.369 ± 0.010 0.356 ± 0.016 Links prediction performance comparison. Note:* represents a dynamic model.</figDesc><table><row><cell></cell><cell cols="3">AUC (mean and standard deviation)</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Coleman</cell><cell>Mining reality</cell><cell>Hypertext</cell><cell>Infectious</cell><cell>Student net</cell></row><row><cell>MMSB</cell><cell cols="5">0.875 ± 0.013 0.883 ± 0.009 0.869 ± 0.008 0.969 ± 0.004 0.916 ± 0.001</cell></row><row><cell>T-MBM  *</cell><cell cols="5">0.886 ± 0.012 0.863 ± 0.005 0.797 ± 0.009 0.833 ± 0.018 0.886 ± 0.015</cell></row><row><cell>fcMMSB  *</cell><cell cols="5">0.909 ± 0.005 0.932 ± 0.006 0.909 ± 0.005 0.980 ± 0.002 0.958 ± 0.003</cell></row><row><cell>BPTF  *</cell><cell cols="5">0.907 ± 0.003 0.923 ± 0.004 0.871 ± 0.006 0.845 ± 0.001 0.905 ± 0.011</cell></row><row><cell>DRGPM  *</cell><cell>• • •</cell><cell cols="4">0.935 ± 0.013 0.906 ± 0.002 0.988 ± 0.001 0.825 ± 0.004</cell></row><row><cell>CN</cell><cell cols="5">0.871 ± 0.008 0.863 ± 0.014 0.786 ± 0.016 0.889 ± 0.004 0.849 ± 0.019</cell></row><row><cell>SVD++  *</cell><cell>• • •</cell><cell cols="3">0.843 ± 0.016 0.725 ± 0.014 0.617 ± 0.001</cell><cell>• • •</cell></row><row><cell>MNE</cell><cell cols="5">0.893 ± 0.004 0.823 ± 0.004 0.869 ± 0.008 0.898 ± 0.007 0.942 ± 0.001</cell></row><row><cell>DeepWalk</cell><cell cols="5">0.916 ± 0.008 0.762 ± 0.014 0.826 ± 0.015 0.915 ± 0.010 0.915 ± 0.008</cell></row><row><cell cols="6">Recurrent-DBN,K=30 0.919 ± 0.012 0.969 ± 0.000 0.944 ± 0.004 0.995 ± 0.000 0.976 ± 0.002</cell></row><row><cell cols="6">Recurrent-DBN,K=20 0.909 ± 0.019 0.965 ± 0.001 0.932 ± 0.003 0.995 ± 0.000 0.971 ± 0.002</cell></row><row><cell cols="6">Recurrent-DBN,K=10 0.899 ± 0.011 0.961 ± 0.002 0.926 ± 0.002 0.989 ± 0.000 0.964 ± 0.010</cell></row><row><cell></cell><cell cols="3">Precision (mean and standard deviation)</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Coleman</cell><cell>Mining reality</cell><cell>Hypertext</cell><cell>Infectious</cell><cell>Student net</cell></row><row><cell>MMSB</cell><cell cols="5">0.289 ± 0.025 0.126 ± 0.009 0.121 ± 0.019 0.233 ± 0.065 0.238 ± 0.017</cell></row><row><cell>T-MBM  *</cell><cell cols="5">0.199 ± 0.015 0.443 ± 0.016 0.142 ± 0.010 0.393 ± 0.065 0.168 ± 0.007</cell></row><row><cell>fcMMSB  *</cell><cell cols="5">0.344 ± 0.017 0.835 ± 0.017 0.505 ± 0.012 0.326 ± 0.011 0.304 ± 0.007</cell></row><row><cell>BPTF  *</cell><cell cols="5">0.385 ± 0.057 0.701 ± 0.013 0.297 ± 0.010 0.371 ± 0.016 0.309 ± 0.080</cell></row><row><cell>DRGPM  *</cell><cell>• • •</cell><cell cols="4">0.855 ± 0.007 0.525 ± 0.022 0.226 ± 0.001 0.284 ± 0.017</cell></row><row><cell>CN</cell><cell cols="5">0.189 ± 0.035 0.426 ± 0.006 0.121 ± 0.009 0.333 ± 0.065 0.138 ± 0.017</cell></row><row><cell>SVD++  *</cell><cell>• • •</cell><cell cols="3">0.423 ± 0.026 0.135 ± 0.008 0.214 ± 0.016</cell><cell>• • •</cell></row><row><cell>MNE</cell><cell cols="5">0.315 ± 0.018 0.269 ± 0.004 0.227 ± 0.014 0.262 ± 0.009 0.347 ± 0.037</cell></row><row><cell>DeepWalk</cell><cell cols="5">0.167 ± 0.068 0.191 ± 0.009 0.117 ± 0.015 0.252 ± 0.019 0.192 ± 0.054</cell></row><row><cell cols="3">provides more interpretable structures; (2) the count infor-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">mation is propagated in a different way; (3) our Recurrent-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">DBN is devised in the setting of relational modelling, while</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DPGDS is for the topic modelling setting.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">For modelling dynamic network data, many of the existing</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">works are "shallow" probabilistic modelling. The dynamic</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Tensorial Mixed Membership Stochastic Block model (T-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">MBM) [Tarrés-Deulofeu et al., 2019] and the Fragmentation</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Coagulation Based MMSB (fcMMSB) [Yu and Fan, 2020]</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">combine the notable mixed-membership stochastic block</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">model with a dynamic setting. The Bayesian Poisson Tensor</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Factorization (BPTF) [Schein et al., 2015] and the Depen-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">dent Relational Gamma Process model (DRGPM) [Yang and</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is partly supported by <rs type="funder">ARC</rs> <rs type="projectName">Discovery</rs> Project <rs type="grantNumber">DP180100966</rs>. <rs type="person">Yaqiong Li</rs> is a recipient of <rs type="funder">UTS</rs> <rs type="grantName">Research Excellence Scholarship</rs>. <rs type="person">Xuhui Fan</rs> and <rs type="person">Scott A. Sisson</rs> are supported by the <rs type="funder">Australian Research Council</rs> through the <rs type="funder">Australian Centre of Excellence in Mathematical and Statistical Frontiers</rs> (ACEMS, <rs type="grantNumber">CE140100049</rs>). <rs type="person">Bin Li</rs> is supported by <rs type="funder">Shanghai Municipal Science &amp; Technology Commission</rs> (<rs type="grantNumber">16JC1420401</rs>) and the <rs type="programName">Program for Professor of Special Appointment (Eastern Scholar) at Shanghai Institutions of Higher Learning</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_hHC6Mte">
					<idno type="grant-number">DP180100966</idno>
					<orgName type="project" subtype="full">Discovery</orgName>
				</org>
				<org type="funding" xml:id="_uqNvsQJ">
					<orgName type="grant-name">Research Excellence Scholarship</orgName>
				</org>
				<org type="funding" xml:id="_dq9fNN4">
					<idno type="grant-number">CE140100049</idno>
				</org>
				<org type="funding" xml:id="_jqyRBgf">
					<idno type="grant-number">16JC1420401</idno>
					<orgName type="program" subtype="full">Program for Professor of Special Appointment (Eastern Scholar) at Shanghai Institutions of Higher Learning</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>information propagation matrix (i.e., we set {β (l) i i,t } l,t = 0 if R i i is the testing data). We estimate the posterior mean of e -k 1 ,k 2 X i,k 1 ,t Λ k 1 k 2 X j,k 2 ,t as the linkage probability for each test data. These linkage probabilities are then used to calculate two evaluation metrics: the area under the curve of the receiver operating characteristic (AUC) and the precisionrecall (precision). Higher values of AUC and precision indicate better model performance.</p><p>The detail results are shown in Table <ref type="table">1</ref>. We report the average evaluation results for each model over 16 runs. Each run uses 3000 MCMC iterations with the first 1500 discarded as burn-in. Overall, Recurrent-DBN outperforms the baseline models for both metrics on almost all data sets. As might be expected, the value of AUC and precision increase with higher model complexity of Recurrent-DBN (i.e., larger values of K). For the other methods, fcMMSB is competitive with DRGPM and outperforms the other baselines. However, they all perform worse than Recurrent-DBN, especially for data sets with large numbers of N or T . We can see that the Recurrent-DBN has clear advantages in learning dynamic relational data, thanks to the deep hierarchical structure and recurrent long-term temporal dependence modelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Latent Variable Visualization</head><p>To gain further insights, we visualize the latent variables in Figure <ref type="figure">2</ref>. It can be observed from the top part that: (1) for the same time step, the membership distributions change gradually with the increase of layers; (2) the membership distributions share some similarities for consecutive time steps and the similarities slowly shift along with the time. For example, the left bottom area of {π π π</p><p>(3) i=1:30 } seems to have 3 different patterns: time steps t = 1, t = 2 ∼ 4, and t = 5 ∼ 10. The bottom part of Figure <ref type="figure">2</ref> visualizes propagation coefficients. It is reasonable to see the values of β β β in the first layer and γ γ γ in the first several time steps are small, since less information is propagated in these cases. The values become larger when more information is propagated. Also, the layer-wise propagation seems to have a larger influence than the cross-time propagation, with an average value of β β β/γ γ γ = 1.2 ∼ 1.4.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixed membership stochastic block models</title>
		<author>
			<persName><surname>Airoldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1981" to="2014" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Introduction to mathematical sociology. Introduction to mathematical sociology</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">S</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><surname>Coleman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964">1964. 1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian latent variable models for mixed discrete outcomes</title>
		<author>
			<persName><surname>Dunson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Herring ; David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">H</forename><surname>Dunson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Herring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Eagle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pentland</forename><surname>Sandy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="255" to="268" />
			<date type="published" when="2005">2005. 2005. 2006. 2006</date>
			<publisher>Personal and Ubiquitous Computing</publisher>
		</imprint>
		<respStmt>
			<orgName>Eagle and Pentland</orgName>
		</respStmt>
	</monogr>
	<note>Reality mining: sensing complex social systems</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic infinite mixed-membership stochastic block model</title>
		<author>
			<persName><forename type="first">Fan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2072" to="2085" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Ostomachion Process</title>
		<author>
			<persName><forename type="first">Fan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1547" to="1553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rectangular bounding process</title>
		<author>
			<persName><forename type="first">Fan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="7631" to="7641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The binary space partitioning-tree process</title>
		<author>
			<persName><forename type="first">Fan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="1859" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable deep generative relational model with high-order node dependence</title>
		<author>
			<persName><forename type="first">Fan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="12637" to="12647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The binary space partitioning forests</title>
		<author>
			<persName><forename type="first">Fan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AISTATS</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="3022" to="3031" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online binary space partitioning forests</title>
		<author>
			<persName><forename type="first">Fan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep temporal sigmoid belief networks for sequence modeling</title>
		<author>
			<persName><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="2467" to="2475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Chengyue Gong. Deep dynamic poisson factorization model</title>
		<author>
			<persName><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep poisson gamma dynamical systems</title>
		<author>
			<persName><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="8442" to="8452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep poisson factor modeling</title>
		<author>
			<persName><surname>Henao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="2800" to="2808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Alain Barrat, Ciro Cattuto, Jean-Franc ¸ois Pinton, and Wouter Van den Broeck. What&apos;s in a crowd? analysis of face-toface behavioral networks</title>
		<author>
			<persName><surname>Isella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Theoretical Biology</title>
		<imprint>
			<biblScope unit="volume">271</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="166" to="180" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bayesian poisson tensor factorization for inferring multilateral relations from sparse dyadic event counts</title>
		<author>
			<persName><forename type="first">Koren</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>
			<persName><surname>Schein</surname></persName>
		</editor>
		<meeting><address><addrLine>Newman</addrLine></address></meeting>
		<imprint>
			<publisher>Aaron Schein, Hanna Wallach, and Mingyuan Zhou</publisher>
			<date type="published" when="2001">2009. 2009. 2012. 2012. 2010. 2001. 2001. 2014. 2014. 2014. 2015. 2015. 2015. 2016. 2016. 2019</date>
			<biblScope unit="volume">328</biblScope>
			<biblScope unit="page" from="781" to="792" />
		</imprint>
		<respStmt>
			<orgName>Orbanz and Roy</orgName>
		</respStmt>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tensorial and bipartite block models for link prediction in layered networks and temporal networks</title>
		<author>
			<persName><surname>Tarrés-Deulofeu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">32307</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Yu and Fan, 2020] Zheng Yu and Xuhui Fan. Fragmentation coagulation based mixed-membership stochastic blockmodel</title>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Koeppl</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sikun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinz</forename><surname>Koeppl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>Arxiv</publisher>
			<date type="published" when="2018">2018. 2018. 2020</date>
			<biblScope unit="page" from="5547" to="5556" />
		</imprint>
	</monogr>
	<note>Dependent relational gamma process models for longitudinal networks</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dirichlet belief networks for topic structure learning</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<editor>
			<persName><surname>Neurips</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2018. 2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="7955" to="7966" />
		</imprint>
	</monogr>
	<note>Scalable multiplex network embedding</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Infinite edge partition models for overlapping community detection and link prediction</title>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
