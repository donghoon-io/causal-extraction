<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Syntactic, Semantic and Lexical Regularities in Language Modeling via Directed Markov Random Fields</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shaojun</forename><surname>Wang</surname></persName>
							<email>swang@cs.ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta ¡ Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaomin</forename><surname>Wang</surname></persName>
							<email>smwang@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta ¡ Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Russell</forename><surname>Greiner</surname></persName>
							<email>greiner@cs.ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta ¡ Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Cheng</surname></persName>
							<email>licheng@cs.ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta ¡ Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Syntactic, Semantic and Lexical Regularities in Language Modeling via Directed Markov Random Fields</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a directed Markov random field (MRF) model that combines ¢ -gram models, probabilistic context free grammars (PCFGs) and probabilistic latent semantic analysis (PLSA) for the purpose of statistical language modeling. Even though the composite directed MRF model potentially has an exponential number of loops and becomes a context sensitive grammar, we are nevertheless able to estimate its parameters in cubic time using an efficient modified EM method, the generalized inside-outside algorithm, which extends the inside-outside algorithm to incorporate the effects of the ¢ -gram and PLSA language models. We generalize various smoothing techniques to alleviate the sparseness of ¢gram counts in cases where there are hidden variables. We also derive an analogous algorithm to calculate the probability of initial subsequence of a sentence, generated by the composite language model. Our experimental results on the Wall Street Journal corpus show that we obtain significant reductions in perplexity compared to the state-of-the-art baseline trigram model with Good-Turing and Kneser-Ney smoothings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of statistical language modeling is to accurately model the probability of naturally occurring word sequences in human natural language. The dominant motivation for language modeling has traditionally come from the field of speech recognition <ref type="bibr" target="#b10">(Jelinek 1998)</ref>, however statistical language models have recently become more widely used in many other application areas, such as information retrieval, machine translation and bioinformatics.</p><p>Appearing in Proceedings of the £ ¤£ ¦¥ ¨ § International Conference on Machine <ref type="bibr">Learning, Bonn, Germany, 2005.</ref> Copyright 2005 by the author(s)/owner(s).</p><p>There are various kinds of language models that can be used to capture different aspects of natural language regularity. The simplest and most successful language models are the Markov chain (¢ -gram) source models, first explored by Shannon in his seminal paper <ref type="bibr" target="#b17">(Shannon 1948)</ref>. These simple models are effective at capturing local lexical regularities in text. However, many recent approaches have been proposed to capture and exploit different aspects of natural language regularity, sentence-level syntactic structure <ref type="bibr" target="#b4">(Chelba and</ref><ref type="bibr">Jelinek 2000, Roark 2001</ref>) and document-level semantic content <ref type="bibr" target="#b1">(Bellegarda 2000</ref><ref type="bibr" target="#b9">, Hofmann 2001)</ref>, with the goal of outperforming the simple ¢gram model. Unfortunately each of these language models only targets some specific, distinct linguistic phenomena. The key question we are investigating is how to model natural language in a way that simultaneously accounts for the lexical information inherent in a Markov chain model, the hierarchical syntactic structure captured in a stochastic branching process, and the semantic content embodied by a bag-of-words mixture of log-linear models-all in a unified probabilistic framework.</p><p>Several techniques for combining language models have been investigated. The most commonly used method is simple linear interpolation <ref type="bibr" target="#b4">(Chelba and</ref><ref type="bibr">Jelinek 2000, Rosenfeld 1996)</ref>, where each individual model is trained separately and then combined by a weighted linear combination. The weights in this case are trained using held out data. Even though this technique is simple and easy to implement, it does not generally yield effective combinations because the linear additive form is too blunt to capture subtleties in each of the component models. Another approach is based on Jaynes' maximum entropy (ME) principle <ref type="bibr" target="#b2">(Berger et al. 1996</ref><ref type="bibr" target="#b12">, Khudanpur and Wu 2000</ref><ref type="bibr" target="#b16">, Rosenfeld 1996)</ref> which was first applied in language modeling a decade ago, and has since become a dominant technique in statistical natural language processing. It is now well known that for complete data, the ME principle is equivalent to maximum likelihood estimation (MLE) in an undirected Markov random field. In fact, these two problems are exact duals of one another <ref type="bibr" target="#b2">(Berger, et al. 1996)</ref>. The major weakness with ME methods, however, is that they can only model distributions over explicitly observed features, whereas in natural language we encounter hidden semantic <ref type="bibr" target="#b1">(Bellegarda 2000</ref><ref type="bibr" target="#b9">, Hofmann 2001</ref>) and syntactic information <ref type="bibr" target="#b4">(Chelba and Jelinek 2000)</ref>. Recently <ref type="bibr" target="#b19">Wang et al. (2003)</ref> proposed the latent maximum entropy (LME) principle, which extends standard ME estimation by incorporating hidden dependency structure. However, when they apply LME to build a composite language model, they have been unable to incorporate PCFGs in this framework, because the tree-structured random field component creates intractability in calculating the feature expectations and global normalization over an infinitely large configuration space. Previously they had envisioned that MCMC sampling methods <ref type="bibr" target="#b20">(Wang et al. 2005</ref>) would have to be employed, leading to enormous computational expense.</p><p>In this paper, instead of using an undirected MRF model, we present a unified generative directed Markov random field model framework that combines ¢ -gram models, PCFG and PLSA. Unlike undirected MRF models where there is a global normalization factor over an infinitely large configuration space, which often causes computational difficulty, the directed MRF model representation for the composite ¢ -gram/syntactic/semantic model only requires many local normalization constraints. More importantly it satisfies certain factorization property which greatly reduces the computational burden and makes the optimization tractable. We learn the composite model by exploiting the factorization properties of the composite model, so we can use a simple yet efficient EM iterative optimization method, the generalized inside-outside algorithm, which enhances the well known inside-outside algorithm <ref type="bibr" target="#b0">(Baker 1979)</ref> to incorporate the effects of the ¢ -gram and PLSA language models. Given that ¢ -gram, PCFG and PLSA models have each been well studied, it is striking that this procedure has gone undiscovered until now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A Composite Trigram/Syntactic/Semantic Language Model</head><p>Natural language encodes messages via complex, hierarchically organized sequences. The local lexical structure of the sequence conveys surface information, while the syntactic structure, encoding long range dependencies, carries deeper semantic information.</p><p>Let denote a set of random variables ¡ ¢ ¤£ ¦¥ §£ © taking values in a (discrete) probability spaces ¡ £ ¥ £ © where is a finite set of states. We define a (discrete) directed Markov random field to be a probability distribution which admits a recursive factorization if there exist nonnegative functions, £ ¡ ! ¥ " # %$ defined on &amp;£ (' ) 10 !2 43 £ !5 , such that 6 87 !9 @ £ ¡ BA C£ D A E0 F2 G3 £ !5 ¥ IH QP and has density R TS VU EW YX Q`a 4b dc Te a S VU a f U ¦g ih 4p a "q W</p><p>(1) If the recursive factorization respects to a graph r , then we have a Bayesian network <ref type="bibr">(Lauritzen 1996)</ref>. But broadly speaking, the recursive factorization can respect to a more complicated representation other than a graph which has a fixed set of nodes and edges.</p><p>Assume that we use a trigram Markov chain to model local lexical information, a PCFG to model the syntactic structure and a PLSA <ref type="bibr" target="#b14">(Pritchard et al. 2000</ref><ref type="bibr" target="#b9">, Hofmann 2001</ref>) to model its semantic content of natural language, see Figure <ref type="figure">1</ref>. Each of these models can be represented as a directed MRF model. If we combine these three models, we obtain a composite model that is represented by a rather complex chain-tree-table directed MRF model.</p><p>A context free grammar (CFG) <ref type="bibr" target="#b0">(Baker 1979)</ref> s is a 4-tuple ¡ ut v xw v §y @¥ that consists of: a set of non-terminal symbols t whose elements are grammatical phrase markers; a vocabulary of w H ! G ! F F © whose elements, words , are terminal symbols of the language; a sentence "start" symbol Q$ t ; and a set of grammatical production rules y of the form: , where d$ et and f$ g¡ t ih %w &amp;¥ j . A PCFG is a CFG with a probability assigned to each rule, such that the probabilities of all rules expanding a given nonterminal sum to one. A PCFG is a branching process and can be treated as a directed MRF model, although the straightforward representation as a complex directed graphical model is problematic.</p><p>A PLSA <ref type="bibr" target="#b9">(Hofmann 2001</ref>) is a generative model of worddocument co-occurrences using the bag-of-words assumption as follows: choose a document k with probability l D¡ mk ¥ , select a semantic class n with probability l C¡ ¢k o pn C¥ , pick a word q with probability l D¡ mn r sq v¥ . The joint probability model for pair of ¡ mk D xq v¥ is a mixture of log-linear model with the expression t u¡ mk D xq v¥ vH wl D¡ ¢k x¥ 6 zy l D¡ n e {q |¥ }l D¡ mk ~ n ¥ . The latent class variables function as bottleneck variables to constrain word occurrences in documents.</p><p>When a PCFG is combined with a trigram model and PLSA, the grammar becomes context sensitive. If we view each C q trigram as q , where q $ %w , then the composite trigram/syntactic/semantic language model can be represented as a directed MRF model, where the generation of nonterminals remains the same as in PCFG, but the generation of each terminal depends additionally on its surrounding context; i.e., not only its parent nonterminal but also the preceding two words as well as its semantic content node n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Training Algorithm for the Composite Model</head><p>We are interested in learning a composite trigram/syntactic/ semantic model from data. We assume we are given a training corpus ¡ consisting of a collection of documents ¢ , where each document contains a collection of sentences, and each sentence £ is composed of a sequence of words from a vocabulary w . For simplicity, but without loss of generality, we assume that the PCFG component of the composite model is in Chomsky normal form. That is, each rule is either of the form ¤ ¦¥ or q where ¤ §¥ $ t v xq $ zw . When combined with trigram and PLSA models, the terminal production rule q becomes C x vn 8 q . By examining Figure <ref type="figure">1</ref>, it should be clear that the likelihood of the observed data under this composite model can be written as below:</p><formula xml:id="formula_0">¨S © f W X ` § b ! #" $ &amp;% R (' GS ) f 0 f 21 f 43 W 45 76 8 96 8 (2)</formula><p>where</p><formula xml:id="formula_1">R ' S ) f 0 f 21 f @3 W X ` § b $ ` $ `A b B S ) DC FE W ¥ p § HG I " G A q P G Q b R G S UT 7V b W G A b B S X (Y abE cC Fd W ¥ p P Q eS A T 7V gf § hG I " G % G A q SUT pi gq b W S rsC Ft Du W ¥ p S UT pi gq f § hG I " G % q 5 75</formula><p>here t gv ©¡ ¢k C §£ xw § ey w § e ¥ is the probability of generating sen- tence £ w in document k with parse tree and seman- tic content sequence y w , ¢ @¡ ¢k C §£ w n C¥ is the count of se- mantic content n in sentence £ w of the document k , ¢ @¡ C x vn Q q 9 xk C §£ w n C¥ is the count of trigrams C q , the non-terminal symbol and semantic content n in sentence £ xw of document k with parse tree and ¢ @¡ m ¤ ¥ xk C §£ xw § ¥ is the count of nonterminal production rule ¤ ¥ in sentence £ xw of document k with parse tree . The parameters l D¡ mk f n ¥ xl D¡ r x vn g q v¥ xl D¡ ¢ ¤ ¦¥ v¥ are locally normalized so that 6 &amp; l D¡ C x vn q v¥ H P ¦ 6 &amp; l D¡ ¢ ¤ ¥ v¥ &amp;H P © 6 y ä l D¡ ¢k sn ¥ H P .Thus we have a constrained optimization problem, and there will be a Lagrange multiplier for C x vn , nonterminal and document k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Estimating Parameters of the Composite Model</head><p>At a first glance, it seems that estimating parameters of the composite model is intractable since the composite di-rected MRF model potentially has an exponential number of loops, which suggests that loopy belief propagation <ref type="bibr" target="#b21">(Yedidia et al. 2001</ref>) and/or variational approximation methods <ref type="bibr" target="#b18">(Wainwright and Jordan 2003)</ref> have to be used. It turns out that this is not the case and there is an efficient and exact recursive EM iterative optimization procedure to perform this task. Following <ref type="bibr" target="#b13">Lafferty's (2000)</ref> derivation of the inside-outside formulas for updating the PCFG parameters from a general EM <ref type="bibr" target="#b7">(Dempster et al., 1977)</ref> algorithm, we derive the generalized inside-outside algorithm for the composite language model. To apply the EM algorithm, we consider the auxiliary function</p><formula xml:id="formula_2">S H ¢f W X § ! #" % R (' dS 1 f @3 ) f 0 W &amp;d fe g R ' h S ) f 20 f 21 f 3 W R i' dS ) f 0 f 21 f @3 W</formula><p>Because of the local normalization constraints, the reestimated parameters of the composite model are then the normalized conditional expected counts:</p><formula xml:id="formula_3">H S rjC kt lu W X nm § b m m ! #" m % R i' dS 1 f @3 ) f 0 W 4o S rsC</formula><p>kt lu Dp @) f 0 f 43 W q ie Hr 2s ct d fu fv wt Hx 2u fe q le hy {z §r t lu</p><formula xml:id="formula_4">H S X (Y a|E cC kd W (3) X m § b m m ! " m % R i' dS 1 f @3 ) f 0 W 4o S X (Y a|E cC kd pp 2) f 20 f @3 if E W q }e r 2s ct d fu fv wt hx 2u ~e q De hy {z er #d H S ) DC FE W X nm m ! " m % R (' GS 1 f 43 w ) f 0 W 4o S ) DC kE p 2) f 20 f E W q }e r</formula><p>2s ct d fu fv wt hx 2u ~e q le hy {z §r E This looks very similar as the PCFG model. Thus we need to compute the conditional expected counts:</p><formula xml:id="formula_5">§ b ! #" % R (' dS 1 f @3 w ) f 0 W 4o S rsC Ft Du Dp ) f 0 f @3 W § b ! " % R (' dS 1 f @3 w ) f 0 W 4o S X (Y abE cC kd p ) f 20 f 3 xf E W ! " % R (' dS 1 f 3 w ) f 0 W 4o S ) DC E p ) f 20 f E xW</formula><p>In general, the sum requires summing over an exponential number of parse trees. However, just as with standard PCFGs, it is easy to check that the following equations still hold and it turns out that there is an efficient way of computing the partial derivative on the righthand side, the generalized inside-outside algorithm.</p><formula xml:id="formula_6">! " % R ' S 1 f @3 w ) f 0 W 4o S rsC t Du Dp ) f 0 f @3 W X S rsC kt lu W R i' dS ) f 0 W R (' dS ) f 0 W S rjC t Du W ! " % R (' dS 1 f @3 w ) f 0 W 4o S X (Y a|E ¦C d p @) f 0 f 43 if E W X S X (Y a|E cC kd W R (' dS ) f 0 W R (' dS ) f 0 W S X (Y a|E cC kd W ! #" % R (' dS 1 f @3 w ) f 0 W 4o S ) lC FE p ) f 20 f E xW X S ) DC FE W R (' dS ) f 20 W R ' S ) f 0 W S ) DC FE W inside inside outside S B C w wi+1 j-1 w wj+1wj+2 w k w wk+1 k+2 wN A 1 w k-1 w j w i-2 i w i-1 w d h h h h h h h h h h h h h h S outside A w wi-1 i-2 w w1 w i i+1 w i+2 h h h h h wN d h h (b) (a)</formula><p>Let ¡ denote that, beginning with a nonterminal , we can derive a string of words and nonterminals by applying a sequence of rewrite rules from the grammar with the flowing-in trigrams and PLSA nodes, where flowing-in trigrams and PLSA nodes are those that induce the words of the string .</p><p>Suppose the position of a rule z ¤ ¦¥ within a tree for sentence £ w H ¡ Bq £¢ ¤¢ ¥¢ q §¦ v¥ in document k can be speci- fied by a triple ¡ © ¦ i D¥ . The partial derivative of the probability t v ¡ r £ w ¤ k x¥ @H it v ¡ mk D £ w ¥ with respect to the parameter l C¡ ¢ p ¤ ¦¥ v¥ only involves those parse trees which use the rule ¤ ¥ . Consider the event " £ w ¤ k using ¤ ¦¥ in position ¡ © ¦ E¥ ". Be- cause of the Markov property of the directed MRF model, the probability of this event can be written as a product of four terms, i.e., the factorization property, as follows: Thus, the conditional expected number of times that the rule Q ¤ ¥ is used in generating the sentence £ w I$ ¡ in document k using the model l is given by ! " % R (' dS 3 ) f 20 W 4o S rsC kt lu Dp 2) f 0 f 43 W X S rsC kt lu W R (' dS 0 u fq ) ©W b 5 1U c@ SU G ed 5 G S rp 0 u fq ) ©W f 5 @ S rt p 0 u fq ) ©W f @ PC $E G ¦S u Dp 0 u fq D) ©W g</p><p>where f 5 @ S rp 0 u fq p) ©W X R i' GS rh3 kd 5 7 97 S7 d @ p 0 u ~q p) ©W i.e., the inside probability that the nonterminal , trigram parent nodes of q x q ¤i and document node k derive the word subsequence q ! F xq Hp in the sentence £ w of docu- i.e., the outside probability that beginning with the start symbol , trigram parent nodes of q sr i © xq §r i t and document node k , we can derive the sequence q 1 F F q "u F q (r i 1 ! F §q §¦ in the sentence £ w of document k .</p><p>Similarly consider the event "</p><p>£ w using |n q ¤ k in position ¡ " §¥ ". Because of the Markov property of the directed MRF model, the probability of this event can be written as a product of four terms, again the factorization property, as follows: nodes which connect the decomposition in position ¡ © §¥ to encode the information of both trigram and PLSA nodes and make influencial impact for parameter estimation of the grammatical production rules |n q . Again, the factorization property is the crucial constituent for the success to derive an efficient and exact recursive algorithm.</p><p>Thus we have</p><formula xml:id="formula_7">! #" % R (' GS 1 f 43 w ) f 0 W 4o S X iY abE cC kd pp ) f 0 f @3 W X S X (Y abE cC d &amp;W R i' dS 0 u ~q ) ©W E U 5 1U T y P Q A V S d 5 ©Q d 5 1Q RE E id 5 W S ) DC FE xW d 5 5 S rpp 0 u fq ) ©W</formula><p>where is the indicator function. Now consider the event " e £ w ¤ k using k o n in po- sition ¡ "}¥ ". Because of the Markov property of the directed MRF model, the probability of this event can be written as sums of products of three terms as follows:</p><p>R (' dS "! C 0 u fq p) (p $# v% u ~q }g b) DC FE u fq s' e x% u fx 2u fe q oS 10 mW }W Just as in the PCFG case, there is an efficient recursive method for computing the 's and ¡ 's using the CYK chartparsing algorithm <ref type="bibr">(Young 1967)</ref>. The only modification is to the definition of so that it incorporates additional information from the trigrams and PLSA nodes. The method for doing this is almost the same as for PCFG and is implicit in the following recursive formulas: <ref type="bibr" target="#b6">Chi (1999)</ref> proved that the maximum likelihood estimate of production rule probabilities for a PCFG yields a proper distribution, i.e., there is no probability mass lost to infinitely large trees. Similarly we can show that the maximum likelihood estimate of production rule probabilities for this composite model always yields a proper distribution. Due to space limitation, we omit the proof here.</p><formula xml:id="formula_8">f F5 @ S rp 0 u fq ) ©W X i q 5 1U G U V@ S rsC Ft Du W f 5 G ©S rpp 0 u fq D) ©W f G C $E ¥@ S u Dp 0 u fq ) ©W f 5 5 S rp 0 u fq ) ©W X A S ) DC kE W S d 5 1Q e d 5 1Q RE |E cC kd 5 W d 5 @ S rp 0 u fq ) ©W X i G q G £¢ 5 S rt C Fu &amp;W f G 5 1Q RE S u Dp 0 u ~q p) ©W d G @ S rt cp 0 u fq ) ©W ¤ i #G q G £¥ @ S rt C `u W f @ PC $E G ©S u Dp 0 u fq ) ©W d 5 G ¦S rt cp 0 u ~q p) ©W d E T S rp 0 u fq ) ©W X y §¦ S rp 0 u ~q p) ©W</formula><p>Theorem 1 Let ¨be the set of finite parse trees, ©t be any intermediate iteration of the EM procedure within the generalized inside-outside algorithm. Then ©t ¡ ¨¥ H P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Smoothing Techniques of the Composite Model</head><p>Current smoothing techniques only handle explicit counts, but in our case there are hidden variables and n in parameter estimation formula for l D¡ C x vn ) q v¥ . In this section, we show how to extend smoothing methods to situations where there exist hidden variables.</p><p>Notice that the sparse data problem arises from trigram counts. The Good-Turing estimate <ref type="bibr" target="#b5">(Chen and Goodman 1999)</ref> is central to combat this problem. The Good-Turing estimate states that for any trigram that occurs ¢ times, we should pretend that it occurs ¢ j times where</p><formula xml:id="formula_9">o IX S o ¤ W ¥ C FE ¥ (4)</formula><p>where is the number of trigrams that occur exactly ¢ times in the training data. To convert this count to a probability, we just normalize: for a trigram i q with ¢ counts, we take</p><formula xml:id="formula_10">! #" S X (Y d W X o $ (5)</formula><p>where % H 6 '&amp; )( 10 ¢ uj . In practice, the Good-Turing estimate is not used alone, instead it is often enhanced with back-off technique to combine higher-order models with lower-order models necessary for good performance.</p><p>A procedure of replacing a count ¢ with a modified count ¢ uj is called "discount" and we define the ratio 2 H 43 as a discount coefficient. The 2 5 are calculated as follows: large counts are taken to be reliable, so they are not discounted. In particular, <ref type="bibr" target="#b11">Katz (1987)</ref> takes 2 H pP for all ¢ 76 g for some . The discount ratios for the lower counts ¢ 4 are derived from the Good-Turing estimate applied to the global trigram distribution and is given as</p><formula xml:id="formula_11">8 ¥ X ¥ 3 ¥ @9 p G C $E q BA DC FE HG A G 9 p G C $E q BA C FE HG A G (6)</formula><p>When we use (3) to estimate l D¡ r x vn q v¥ , we use the expected count of ¢ @¡ C x vn q v¥ where and n are hidden. However, when the trigram ¦q has count ¢ @¡ r q v¥ PI RQ , if we discount the expected count of ¢ @¡ C x vn q v¥ by the ratio 2 5 Y¡ r q v¥ , then we discount the trigrams by the same ratio 2 5 Y¡ C q v¥ since 6 'S &amp; !T y ä 2 H Y¡ r ¦q v¥ ¢ @¡ r |n i q |¥ |H P2 H Y¡ r ¦q |¥ ¢ @¡ r q |¥ . Therefore instead of using iterative parameter estimation of (3), we use smoothed iterative parameter estimation as the following,</p><formula xml:id="formula_12">H U S X (Y abE cC Fd W X 8 ¥ S X (Y d W § b ! " % R (' dS 1 f @3 ) f 20</formula><p>W 4o S X (Y a|E iC d pp @) f 0 f @3 xf E W q }e r 2s ct Hd ~u fv wt hx 2u fe q e Hy z §r #d</p><p>When the trigram ¦q has count ¢ @¡ C 8 q v¥ %H VQ , we backoff to the corresponding bigram parameters and let</p><formula xml:id="formula_13">U S X (Y a|E cC kd W X XW ES X (Y d W F7 U S Y abE cC d &amp;W and W ES X (Y d W uX 9 m V Y ¥ p P Q eV q ¥ 5` U S X (Y abE cC d &amp;W 9 m V Y ¥ p P Q §V q ¥ H` U S Y a|E cC kd W</formula><p>Similarly we can use Kneser-Ney smoothing technique. Due to space limitation, we omit the details here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Computing the Probability of Initial Subsequence Generation</head><p>In automatic speech recognition or statistical machine translation, we are presented with words one at a time, in sequence. Therefore, we would like to calculate the probability t v ¡ q q §t @ F ! q r F ! x¥ ; that is, the probability that an arbitrary word sequence q q at F ! §q r is the initial subsequence of a sentence generated by the composite trigram/syntactic/semantic language model. We derive the generalized left-to-right inside algorithm to perform this computation by following the work of <ref type="bibr">(Jelinek and Lafferty, 1991)</ref>, which assumes that a PCFG model is used.</p><p>Let t v ¡ ¢ ¡ ¢ x¥ denote the sum of the probabilities of all trees with root node and document node k resulting in word sequences whose initial subsequence is q E ! F q p . Thus</p><formula xml:id="formula_14">R ' S r¤£ ¥£ 0 f ©2 W X f F5 @ ©S r&amp;W ¤ ¦ G b R R ' S rsC d 65 v7 97 97 d A@ "U RE W ¤ ¦ G ¦ ¨ § b R § R i' dS rsC kd 5 7 P7 97 d @ U E U W ¤ 7 P7 97 ¤ ¦ G © © © ¦ ¨ b {R</formula><p>R (' GS rjC kd 65 v7 S7 97 @d A@ "U RE 7 97 S7 }U ¥ W ¤ 7 S7 97</p><p>Using this notation, the desired probability t v ¡ m q "q t F ! q (r F ! i¥ is denoted by</p><formula xml:id="formula_15">t v ¡ ¢ P © i D¥ . Let t v ¡ ¢ ¤ (¥ (H 6 § &amp; t v ¡ ¢ ¤ ¤ t !¥</formula><p>be the sum of the probabilities of all the rules ¤ ¤ st whose first lefthand side element is ¤ H ¤ . Define t v ¡ m ¤ o¥ |H 6 ¨3 ( C5 3 t v ©¡ m ¤ Y¥ as the sum of probabilities of all trees with root node ¤ that produce as the leftmost first nonterminal. This term converges, since our underlying composite language model t #v is proper.</p><p>Using this definition, we get</p><formula xml:id="formula_16">R (' dS r£ £ 0 f 0 mW X f 5 G 5 S r`W ¤ i b R ! ' S rh3 Ft vW f 5 G 5 S rt |W</formula><p>Define the sum of probabilities of all trees with root node whose last leftmost production results in leaves ¤ and</p><formula xml:id="formula_17">¤ t as R ! ' S r3 Ft E t W X R (' dS rjC Ft E t W (7) ¤ q b R ' S rh3 u W R (' dS u C kt E t W Obviously, R (' dS r¤£ ¥£ 0 f 0 ¤ o W uX i #" i § b R ! ' S r`C t wE @t a W b f 5 G 5 S rt E W R (' 4S rt £ $£ 0 ¤ f 0 ¤ o CW ¤ f F5 G 5 ¥C FE S rt wE xW R ' S rt ¨£ $£ 0 ¤ &amp;% f 0 ¤ o CW ¤ 7 97 S7 ¤ f 5 G 5 ¥C ¥ Q RE S rt E 3 kd 5 7 97 97 @d 5 ¤C ¥ Q YE W R (' GS rt £ $£ 0 ¤ o f 0 ¤ o CW ¤ R (' 4S rt wE '£ ¥£ 0 f 0 ¤ o CW g (8)</formula><p>since to generate the initial subsequence q ¢q ¤i ! F q ¤i 1 , some rule ¤ ¤ t must first be applied and then the first part of the subsequence must be generated from ¤ and its remaining part from ¤ t . Define the sums in the bracket of (8) except the last term as</p><formula xml:id="formula_18">( ¡ ¤ e¤ t !¥ . Then we have R (' dS r¤£ ¥£ 0 f 0 ¤ o W X i )" ~i § b R ' S rH3 t (E 2t a W 10 vS rt wE f t W ¤ i G b R ! ' S rsC Ft E W R TS rt E £ $£ 0 f 0 ¤ o CW X 7 S7 97 97 97 97 X i )" fi § b R ' S rH3 t (E 2t a W 10 vS rt wE f t W ¤ q G G © © © G q C b V G G T 32 b R ! ' S r|C u E W G 4 R ! ' S u Q YE C u W R TS u G 5£ $£ 0 f 0 ¤ o W g</formula><p>We have shown that the maximum likelihood estimate of the composite language yields a proper distribution in Theorem 1, thus the last term of the above equation tends to 0 as grows without limit. Then using definition (7) and successive resubstitutions, we get the final formula</p><formula xml:id="formula_19">R ' S r¤£ $£ 0 f 0 ¤ o CW X i )" fi § b R ' S rH3 t wE 2t W 10 |S rt (E f t a W X i " i § b R ! ' S rH3 t E t W $ ¥ @ 4 E f F5 G 5 ¥C v@ SQ RE "S rt (E xW R ' S rt a 6£ $£ 0 ¤ 2 Gf 0 ¤ o CW 45</formula><p>Comparing with a PCFG, the only difference is the way that ( ¡ ¤ d e¤ t ¥ is recursively calculated by , which here takes into account the impact of the trigram and PLSA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental data sets</head><p>The corpus used to train our model was taken from the WSJ portion of the NAB corpus, which was composed of about 150,000 documents spanning the years 1987 to 1989, comprising approximately 42 millions words. The vocabulary was constructed by taking the 20,000 most frequent words of the training data. The PCFG production rules we use are extracted from the sections 2-21 of the WSJ treebank corpus. The test set consists of 153,000 words taken from the year 1989.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Computation in Testing</head><p>Since the representation for a document of the test data is not contained in the original training corpus, we use "foldin" heuristic approach similar to the one used in <ref type="bibr" target="#b9">(Hofmann 2001)</ref>: the parameters corresponding to the documentsemantic arcs, l D¡ mk Q n C¥ , are re-estimated by maximizing the probability of word subsequence currently seen, q F ! F Y q r , i.e., the initial subsequence of a sentence gen- erated by the composite language model, while holding the other parameters fixed.</p><p>In this case, we use the recursive gradient ascent to update l C¡ ¢k o pn C¥ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S ) DC FE</head><formula xml:id="formula_20">W p G q X S ) DC FE W p G Q RE q 9 d fe g CR (' dS "! 7£ £ f e W S ) lC kE W 8 8 8 ' 9 C ¨@ G BA</formula><p>We can then recursively calculate the gradient of loglikelihood of the initial subsequence of a sentence with respect to the parameters of document-semantic arc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experimental design</head><p>To serve as a baseline standard of performance, we use a conventional trigram model with Good-Turing backoff and Kneser-Ney smoothing. Implementing these approaches, we obtained perplexity scores of 109 and 103 respectively on test data set.</p><p>When we train the PCFG model alone, the perplexity score on test data is 678. Combining the PCFG model with Good-Turing back-off and Kneser-Ney smoothing trigram models by linear interpolation, we obtain the test perplexity score 109 and 102 respectively. Next we train the PLSA model alone where the number of hidden semantic nodes n is set to be ¡ ¢ H P ¤£ ¦¥ , we obtain perplexity score on test data 1487. When this PLSA model is combined with Good-Turing back-off and Kneser-Ney smoothing trigram models by linear interpolation, we find that the test perplexity scores remain unchanged. If we combine these three models together using linear interpolation, we obtain the perplexity scores on test data 108 and 102 respectively.</p><p>Next we introduce the composite syntactic/trigram model which is equivalent to the composite syntactic/semantic/trigram language model by setting the semantic node n to be a constant. Using the generalized inside-outside algorithm to train this composite syntactic/trigram model with Good-Turing back-off and Kneser-Ney smoothing trigram models, we achieve a perplexity scores of 94 and 90 on test data of, a 14% and 11% relative reduction in perplexity respectively.</p><p>We then introduce the composite semantic/trigram model, which is equivalent to the composite syntactic/semantic/trigram language model by setting the syntatic node to be a constant. We fix the number of possible hidden topics to be ¡ ¢ (H P ¤£ §¥ and use the generalized inside-outside algorithm to train the composite semantic/trigram model with Good-Turing back-off and Kneser-Ney smoothing trigram models. Here we achieve perplexity scores of 96 and 91 on test data, a 12% and 10% relative reduction in perplexity respectively. Since the representation for a document of the test data is not contained in the original training corpus, during testing we use "fold-in" heuristic approach similar to the one used in <ref type="bibr" target="#b9">(Hofmann, 2001)</ref>: the document-semantic parameters are re-estimated by maximum likelihood estimation while holding semantic-word parameters fixed, where the empirical distribution is given by the current updated document history.</p><p>Finally we use the generalized inside-outside algorithm to train the composite trigram/syntactic/semantic model with Good-Turing back-off and Kneser-Ney smoothing trigram models and we set the number of hidden semantic node n is again set to be ¡ ¢ ¤H P ¤£ §¥ . Again since the rep-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Discussion</head><p>We present an original approach that combines ¢gram, PCFG and PLSA to build a sophisticated mixed chain/tree/table directed MRF model for statistical language modeling, where various aspects of natural language-such as local word interaction, syntactic structure, and semantic document information-can be modeled by mixtures of exponential families with a rich expressive power that can take their interactions into account simultaneously and automatically. The composite directed MRF model we build becomes context sensitive grammar, and problems induced seem to be NP hard. However for this particular model, we show that we can generalize the wellknown inside-outside algorithm to estimate its parameters in cubic time. To alleviate the sparseness of ¢ -gram counts, we also generalize various smoothing techniques to handle cases where there exist hidden variables. The experiments we have carried out show improvement in perplexity over current state-of-the-art technique. <ref type="bibr" target="#b8">Griffiths et al. (2004)</ref> recently proposed a generative composite HMM/LDA (latent Dirichlet allocation) model which takes into account of both local sentence level syntactic class structures and global document level semantic contents for purposes of part-of-speech tagging and document classification, they have used MCMC to estimate the parameters for a much simpler model. However we propose an exact estimation algorithm for a much more complicated model.</p><p>One way to improve the quality of the language models is to use semantic smoothing <ref type="bibr" target="#b1">(Bellegarda 2000</ref><ref type="bibr" target="#b20">, Wang et al. 2005)</ref>, which has been shown to be effective in improving the perplexity results. Basically we can introduce an additional node between each topic node and word node to capture semantic similarity and subtle variation between words or introduce additional node between the topic nodes and the document node to take into account of semantic similarity and sub-topic variation within each document and among documents. <ref type="bibr" target="#b3">Blei et al. (2003)</ref> state that PLSA is not a well-defined generative model of documents, and there is no natural way to represent a document not seen in the original training corpus, this is why the "fold-in" heuristic procedure has to be used during testing to reestimate the semantic content. <ref type="bibr">Blei et al. proposed LDA model</ref> to overcome this problem. It would be interesting to integrate LDA model into our composite language model and in this case variational method may have to be used.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Inside and outside probabilities in the composite trigram/syntactic/semantic model, where each component is influenced by the injected trigram and PLSA nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>R (' dS "! C 0 u fq ) (p $# &amp;% u fq }g sC kt lu u fq (' e )% u x 2u fe q (S 10 f "lu W R i' dS rt 43 kd 65 87 97 97 d A@ p 0 u fq D) ©W R (' dS u B3 kd @ DC FE 7 97 97 d HG p 0 u ~q p) ©W R (' dS "! I3 kd E 7 P7 97 d 5 1Q RE |d HG C FE 7 S7 97 d HT 7p 0 u fq D) ©WSee Figure2(a) for an illustration. The key insight toward a solution for the composite model is that, in comparison with the PCFG model, there are additional trigrams which connect the decomposition in position ¡ © ¦ i D¥ . These dependencies encode additional information from the trigram model, and significantly influence the parameter estimation of the non-terminal grammatical production rules (the impact of the PLSA model is implicitly considered, this will become clear when we derive the estimation formula for the terminal grammatical production rules). The factorization property is the crucial constituent for the success to derive an efficient and exact recursive algorithm. From this it is not difficult to see thatR ' S "! C 0 u ~q ) ©W S r`C Ft Du W X 5 ©U V@ WU G R ' S rt43 d 65 &amp;7 97 97 d A@ p 0 u fq D) ©W R ' S u X3 kd A@ PC $E Y7 97 P7 d G p 0 u fq ) ©W R (' dS "! `3 d aE Y7 97 P7 2d 65 ©Q YE 2|d HG C $E 7 97 97 d T p 0 u fq D) ©W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>ment k ; and d 5 G ¦S rp 0 u fq p) ©W X R (' dS "! q3 d E 7 97 P7 d 5 ©Q YE |d HG C $E 7 97 97 d HT 7p 0 u fq D) ©W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(p R# v% u ~q }g X (Y a|E cC kd u fq w' e x% u x 2u ~e q oS 10 mW }W X hy P Q eV S d 5 ©Q d 5 ©Q YE d 5 W b S ) DC FE W S X (Y abE 9C kd W g R ' S "! I3 kd aE Y7 97 97 d 65 1Q RE `d 65 ¤C $E Y7 P7 97 d T p 0 u ~q ) ©W See Figure 2 (b) for illustration. The key insight toward a solution for the composite model is that comparing with the PCFG model, there are additional trigram and PLSA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>dS "! q3 d E 7 97 P7 d 5 ©Q YE |d 5 ¥C FE 7 97 97 @d HT pp 0 u fq ) ©W b S ) DC kE W S d 5 1Q e d 5 1Q RE |E 9C kd 5 W g S d 65 ©Q ed 65 ©Q YE bE cC Fd 65 ¢W d 5 ¥5 §S rp 0 u fq p) ©W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Relative perplexity reductions over baseline trigram with Good-Turing and Kneser-Ney smoothings by various composite language moddels: 1. Syntactic-trigram, 2. Semantictrigram, 3. Syntactic-semantic-trigram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Perplexity results for the composite syntactic semantic trigram model on test corpus.</figDesc><table><row><cell>LANGUAGE MODEL</cell><cell>PERPLEXITY</cell><cell>PERPLEXITY</cell></row><row><cell></cell><cell cols="2">GOOD-TURING KNESER-NEY</cell></row><row><cell>TRIGRAM (BASELINE)</cell><cell>109</cell><cell>103</cell></row><row><cell>PCFG</cell><cell>678</cell><cell></cell></row><row><cell>PLSA</cell><cell>1487</cell><cell></cell></row><row><cell>SYNTACTIC TRIGRAM</cell><cell>94</cell><cell>90</cell></row><row><cell>SEMANTIC TRIGRAM</cell><cell>96</cell><cell>91</cell></row><row><cell>SYNTACTIC, SEMANTIC</cell><cell>82</cell><cell>79</cell></row><row><cell>TRIGRAM</cell><cell></cell><cell></cell></row><row><cell cols="3">resentation for a document of the test data is not con-</cell></row><row><cell cols="3">tained in the original training corpus, during testing we use</cell></row><row><cell cols="3">"fold-in" heuristic approach as described in the last subsec-</cell></row><row><cell cols="3">tion: the document-semantic parameters are re-estimated</cell></row><row><cell cols="3">by recursive gradient ascent of maximum likelihood esti-</cell></row><row><cell cols="3">mation of the initial subsequence of a sentence while hold-</cell></row><row><cell cols="3">ing semantic-word and production rule parameters fixed.</cell></row><row><cell cols="3">This time we achieve perplexity scores of 82 and 79 on</cell></row><row><cell cols="3">test data, a 25% and 21% relative reduction in perplexity</cell></row><row><cell>respectively.</cell><cell></cell><cell></cell></row><row><cell cols="3">The perplexity results are listed in Table 1 and the perplex-</cell></row><row><cell cols="3">ity reductions of these results over baseline trigram models</cell></row><row><cell cols="3">with Good-Turing and Kneser-Ney smoothings are shown</cell></row><row><cell cols="3">in Figure 3. It shows that linear interpolation is too blunt</cell></row><row><cell cols="3">to capture subtleties of PCFG and PLSA models, however</cell></row><row><cell cols="3">our approach of integrating syntactic and semantic sources</cell></row><row><cell cols="3">of nonlocal dependency information from PCFG and PLSA</cell></row><row><cell cols="3">models into trigram model results significant perplexity</cell></row><row><cell cols="3">improvement. Basically PCFG and PLSA models carry</cell></row><row><cell cols="3">complementary long-range dependency structure and their</cell></row><row><cell cols="3">gains over trigram model are almost additive. Another ob-</cell></row><row><cell cols="3">servation is that the gains of using Kneser-Ney smoothing</cell></row><row><cell cols="3">over Good-Turing smoothing are almost additive too.</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Trainable grammars for speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 97th Meeting of the</title>
		<meeting>the 97th Meeting of the</meeting>
		<imprint>
			<publisher>Acoustical Society of America</publisher>
			<date type="published" when="1979">1979</date>
			<biblScope unit="page" from="547" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting latent semantic information in statistical language modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bellegarda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of IEEE</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1279" to="1296" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to natural language processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Della Pietra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="71" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Structured language modeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="283" to="332" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="319" to="358" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Statistical properties of probabilistic context-free grammars</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="160" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Integrating topics and syntax</title>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised learning by probabilistic latent semantic analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="196" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<title level="m">Statistical Methods for Speech Recognition</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Estimation of probabilities from sparse data for the language model component of a speech recognizer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="401" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="355" to="372" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A derivation of the inside-outside algorithm from the EM algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<idno>21636</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note type="report_type">IBM Research Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inference of population structure using multilocus genotype data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pritchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Donnelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetics</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="945" to="959" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic top-down parsing and language modeling</title>
		<author>
			<persName><forename type="first">B</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="276" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to adaptive statistical language modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="228" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Graphical models, exponential families, and variational inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<idno>649</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, UC-Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The latent maximum entropy principle</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>Manuscript</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Combining statistical language models via the latent maximum entropy principle</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalized belief propagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yedidia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="689" to="695" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognition and parsing of context free languages in time $ ¡</title>
		<author>
			<persName><forename type="first">D</forename><surname>Younger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="198" to="208" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
