<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">X -Metric: An N-Dimensional Information-Theoretic Framework for Groupwise Registration and Deep Combined Computing</title>
				<funder ref="#_cZWsqpR #_Xkhpebn #_hDDtd29">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_nQUMSQ4">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-11-03">3 Nov 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinzhe</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiahai</forename><surname>Zhuang</surname></persName>
						</author>
						<title level="a" type="main">X -Metric: An N-Dimensional Information-Theoretic Framework for Groupwise Registration and Deep Combined Computing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-11-03">3 Nov 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2211.01631v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information theory</term>
					<term>maximum likelihood</term>
					<term>groupwise registration</term>
					<term>segmentation</term>
					<term>combined computing -Metric (Sec. 3.1) -CoReg (Sec. 3.2) Groupwise Registration (Sec. 3.4) Application Deep Combined Computing (Sec. 3.5) Label Observation Application Methodology Theory Information Theory MLE + EM (Sec. 3.3) Deep Learning Image Co-registration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a generic probabilistic framework for estimating the statistical dependency and finding the anatomical correspondences among an arbitrary number of medical images. The method builds on a novel formulation of the N -dimensional joint intensity distribution by representing the common anatomy as latent variables and estimating the appearance model with nonparametric estimators. Through connection to maximum likelihood and the expectation-maximization algorithm, an information-theoretic metric called X -metric and a co-registration algorithm named X -CoReg are induced, allowing groupwise registration of the N observed images with computational complexity of O(N ). Moreover, the method naturally extends for a weakly-supervised scenario where anatomical labels of certain images are provided. This leads to a combined-computing framework implemented with deep learning, which performs registration and segmentation simultaneously and collaboratively in an end-to-end fashion. Extensive experiments were conducted to demonstrate the versatility and applicability of our model, including multimodal groupwise registration, motion correction for dynamic contrast enhanced magnetic resonance images, and deep combined computing for multimodal medical images. Results show the superiority of our method in various applications in terms of both accuracy and efficiency, highlighting the advantage of the proposed representation of the imaging process.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>M EDICAL images are usually complementary yet inher- ently correlated through their underlying common anatomy. Mutual information (MI), an information-theoretic metric measuring the statistical dependency between two images, has been particularly successful in medical image registration <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. However, generalizing MI to N images for N 2 can be challenging due to the curse of dimensionality. To devise computational tractable solutions, Learned-Miller <ref type="bibr" target="#b2">[3]</ref> proposed the pixel/voxel-wise stacked entropies, which works on i.i.d. entries of each intensity vector at individual spatial locations; Wachinger and Navab <ref type="bibr" target="#b3">[4]</ref> worked on the pairwise estimates of MI, which accumulates MI's over all image pairs; Bhatia et al. <ref type="bibr" target="#b4">[5]</ref> and Polfliet et al. <ref type="bibr" target="#b5">[6]</ref> resorted to template-based methods, which rely on an informative grey-valued template generated during the registration process.</p><p>In this paper, we choose to pay close attention to the connection between information-theoretic metrics and their interpretation from maximum likelihood. Similar approaches were taken in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> for the special cases of pairwise registration or i.i.d. intensity vectors. Unlike their methods, our approach is to consider modelling the joint intensity distribution directly by introducing latent variables and using nonparametric estimators. In this manner, we derive an Ndimensional information-theoretic framework, composed of • X. Luo and X. Zhuang are with the School of Data Science, Fudan University, 220 Handan Road, Shanghai, 200433, China. • *X. Zhuang is the corresponding author (www.sdspeople.fudan.edu.cn/ zhuangxiahai/). • Code will be available from <ref type="url" target="https://zmiclab.github.io/projects.html">https://zmiclab.github.io/projects.html</ref>. Manuscript received ..; revised .. a groupwise similarity metric and a generic co-registration algorithm with computational complexity of O(N ). As the terminology has yet to be precisely defined, they are referred to as the X -metric and the X -CoReg algorithm, respectively.</p><p>The core of the X -metric and the X -CoReg is the explicit modelling of the common anatomy as categorical latent variables, along with a generative appearance model using nonparametric estimators. These major components yield an optimization scheme inspired by the expectationmaximization algorithm that alternates between: 1) estimating the common-space parameters, including the spatial distribution and prior proportions of the common anatomy (Section 3.2.1); 2) identifying the spatial correspondences among the observed images (Section 3.2.2). Besides, the approach goes beyond registration; it actually integrates registration with segmentation, leading to a combined-computing framework that performs the two tasks simultaneously and collaboratively <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>To demonstrate its versatility, we validate X -CoReg on groupwise registration, including joint alignment for multimodal images of the brain and the most challenging task on motion correction for first-pass cardiac perfusion sequences. Furthermore, to leverage the capability of deep learning (DL) techniques, we extend the proposed algorithm to a DL-integrated framework, so that end-to-end combined computing is realized on multimodal medical images.</p><p>Our contributions can be summarized as follows:</p><p>• We propose an information-theoretic and computationally tractable co-registration framework, which highlights:</p><p>-The X -metric that is intended to measure the statistical dependency among an arbitrary number of images, based on a novel formulation of the N -dimensional joint intensity distribution and the imaging process. -The X -CoReg algorithm that features a generic yet efficient co-registration procedure. -The proposed framework and optimization scheme are theoretically grounded from the maximum-likelihood perspective.</p><p>• We extensively evaluate the applicability of the proposed algorithm on multimodal groupwise registration and motion estimation for dynamic contrast-enhanced images, and find its superiority over previous methods.</p><p>• We demonstrate the potential of the algorithm in a deep learning context, where simultaneous registration and segmentation is achieved on multimodal medical images in an end-to-end fashion.</p><p>The remainder of the paper is organized as follows. Section 2 provides an overview of the past studies on groupwise registration and combined computing, with special emphasis on the connection between information-theoretic approaches and the maximum likelihood principle. Section 3 describes the proposed N -dimensional informationtheoretic framework-the X -metric (Section 3.1) and the X -CoReg (Section 3.2), whose rationale are established from the generic image generative model and maximum likelihood (Section 3.3). We also discuss the application of the proposed algorithm to groupwise registration (Section 3.4) and an extended framework that incorporates observed labels, yielding its application to deep combined computing (Section 3.5). Section 4 presents experimental setups and evaluation results of our method on applications to multimodal groupwise registration (Sections 4.1 to 4.2), spatiotemporal motion estimation (Section 4.3), and deep combined computing (Section 4.4). Section 5 discusses implications of the proposed framework and concludes the study. Fig. <ref type="figure" target="#fig_0">1</ref> presents the roadmap of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE MAXIMUM-LIKELIHOOD PERSPECTIVE</head><p>In this section, we review the literature on groupwise registration and combined computing from the perspective of maximum likelihood. The two tasks reduce to pairwise registration and atlas-based segmentation respectively, when only two images are modelled. For these two reduced problems, Roche et al. <ref type="bibr" target="#b6">[7]</ref> and Ashburner and Friston <ref type="bibr" target="#b13">[14]</ref> have formulated them as maximum likelihood estimation (MLE), the solution to which gives the required spatial correspondence and/or segmentation with the target image.</p><p>However, direct generalization of the likelihood function to an arbitrary number of images can be complex, as the (c) Scatter plot of intensity triples. Each point indicates one intensity triple from the T1, T2 and PD images at the same spatial location.</p><p>Fig. <ref type="figure">2</ref>: The joint intensity scatter plot of the multi-sequence MR images from the BrainWeb dataset with simulated misalignments. The average number of samples in a single bin of the joint intensity space (JIS), calculated as |Ω|/L N where L is the number of discrete intensity levels of an image and |Ω| is the constant cardinality of the common-space spatial samples, will reduce exponentially as the dimension N of the joint intensity space increases.</p><p>sparsity of samples in a high-dimensional intensity space will cause the estimation of joint intensity distribution (JID) to be implausible <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Fig. <ref type="figure">2</ref> shows an example of 2D (N = 2) and 3D (N = 3) joint intensity scatter plots from a misaligned image group. The average number of samples in a single bin of the joint intensity space (JIS), calculated as |Ω|/L N where L is the number of discrete intensity levels of an image and |Ω| is the constant cardinality of the commonspace spatial samples, will reduce exponentially as the dimension N of the joint intensity space increases. This leads to the curse of dimensionality, which in our context means the number of samples required to consistently estimate probability distributions grows exponentially with N <ref type="bibr" target="#b14">[15]</ref>. Hence, additional independence and model assumptions over the JID are often prescribed, to make solutions computationally feasible for high-dimensional cases.</p><p>In the following, we describe the methodological details of the relevant literature. For convenience, we list in Table <ref type="table" target="#tab_0">1</ref> the essential mathematical symbols used in the rest of this paper, and show in Fig. <ref type="figure" target="#fig_2">3</ref> the graphical representation of the methods reviewed and investigated in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Groupwise Registration</head><p>For groupwise registration, the observed image group are denoted by U = {U j } N j=1 , where U j is the realization of some imaging process from the image space Ω j to real intensity values. We can thus represent U j = (u jω ) ω∈Ωj , where u jω U j (ω) are signal intensities at i.i.d. spatial  samples ω ∈ Ω j <ref type="bibr" target="#b16">[17]</ref>. Denote u φ x as the resampled intensity vector comprising intensity values of U j at φ j (x), i.e. u φ x = [u φ1</p><p>x,1 , . . . , u φ N x,N ] , where u φj x,j U j • φ j (x). The maximum-likelihood approach aims to find the optimal spatial correspondences through the MLE of a multivariate JID indexed by the spatial transformations φ, i.e.,</p><formula xml:id="formula_0">P φ (U ) = x∈Ω P x (u φ x ),<label>(1)</label></formula><p>which is factorized over i.i.d. spatial samples x ∈ Ω. The superscript x of P x indicates that the distribution for every intensity vector can be spatially variant, and will be omitted otherwise. Notably, by representing the JID with a spatially invariant categorical model, we can derive the joint entropy H(U ) as a groupwise similarity metric <ref type="bibr" target="#b14">[15]</ref>.</p><p>As the joint entropy can be computationally prohibitive in general when N 2, the following studies presume extra structures with the JID, yielding more computationally tractable information-theoretic metrics:</p><p>• Congealing (CG). The CG framework <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref> works on the i.i.d. assumption of the entries in every intensity vector <ref type="bibr" target="#b7">[8]</ref>, i.e.,</p><formula xml:id="formula_1">P x (u φ x ) = N j=1 f x U (u φj x,j ),<label>(2)</label></formula><p>where f x U (•) is a spatially variant univariate density. Therefore, the maximum likelihood principle reduces to minimum sum of pixel/voxel-wise stacked entropies, favouring transformations that force the local intensity space concentrated <ref type="bibr" target="#b18">[19]</ref>. However, congealing with pixel stacks may suffer from an underestimated density with insufficient samples when N is small. To mitigate this issue, <ref type="bibr" target="#b2">[3]</ref> also discussed the idea of pixel cylinder that considers neighbouring pixels for density estimation.</p><p>• Accumulated pairwise estimates (APE). The APE framework <ref type="bibr" target="#b3">[4]</ref> assumes that every two images are conditionally independent given a third one. That is, the set of the observed images forms a fully-connected pairwise Markov random field. Hence, the JID is given by a Gibbs distribution</p><formula xml:id="formula_2">P (u φ x ) ∝ N j=1 i&gt;j ψ ij (u φi x,i , u φj x,j ),<label>(3)</label></formula><p>where ψ ij (•, •) are the pairwise potentials. The APE measures the statistical dependency between two nodes via</p><formula xml:id="formula_3">ψ ij (µ i , µ j ) = f ij (µ i , µ j )/[f i (µ i )f j (µ j )</formula><p>] using a categorical model, where f ij (•, •) denotes the pairwise pmf (probability mass function) and f i (•) its marginal. Then, the loglikelihood function devolves to the sum of all pairwise mutual information. Hence, the computational burden for APE scales quadratically with N , limiting its applicability for large image groups. • Conditional template entropy (CTE). Template-based groupwise registration methods are also widely adopted in the literature <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, in which the similarity metric is computed between each image and an artificial template generated during the registration process. Among them the conditional template entropy <ref type="bibr" target="#b5">[6]</ref> computes the template image as the first principal component of the observed images via principal component analysis (PCA).</p><p>Then the sum of conditional entropies with each observed image given the template is used as the similarity metric. However, the principal component image may disguise clear structural information as it is computed from the linear combination of the warped images. Besides, the approach is theoretically inconsistent as the (probabilistic) PCA assumes a linear Gaussian model with a latent Gaussian variable corresponding to the principal-component subspace, while the conditional entropy is derived under a categorical model <ref type="bibr" target="#b6">[7]</ref>. Nevertheless, one can find that each method has certain limitations that restrict their generalizability. To establish a general co-registration framework, we seek to construct a probabilistic generative model based solely on the primary assumption that the images are correlated through the common anatomy. Thus, we derive the proposed N -dimensional information-theoretic framework, i.e. the X -metric and X -CoReg, which is generic by nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Combined Computing</head><p>Beyond groupwise registration, another stream of research attempts to achieve combined computing, i.e. combining registration with segmentation in a unified framework <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. They typically identify the structural correspondences among a group of images by modelling the average tissue classes as latent variables.</p><p>In an early work, Lorenzen et al. <ref type="bibr" target="#b21">[22]</ref> chose to estimate the class posteriors of every individual image using the Gaussian mixture model (GMM). Then, groupwise registration was realized simultaneously with finding an average atlas probability map, by minimizing the Kullback-Leibler divergence between the atlas and each class posterior. Bhatia et al. <ref type="bibr" target="#b11">[12]</ref> extended this notion by interleaving groupwise segmentation with groupwise registration, where the GMM parameters were optimized alternately with the registration parameters. Nevertheless, the two methods are less principled as no explicit modelling of the JID is presented.</p><p>Thenceforth, Orchard and Mann <ref type="bibr" target="#b15">[16]</ref> proposed representing the JID directly with a multivariate GMM. Their approach alternates between density estimation and motion adjustment, corresponding to the two steps of the expectation-maximization (EM) algorithm. However, it has computational complexity of O(N 3 ), preventing its usage for a large image group. Blaiotta et al. <ref type="bibr" target="#b22">[23]</ref> suggested a Bayesian framework with a factorized GMM for the JID, where the intensities were assumed conditionally independent given the tissue class labelling, i.e.,</p><formula xml:id="formula_4">P (U | Z) = N j=1 P (U j | Z).<label>(4)</label></formula><p>Zhuang <ref type="bibr" target="#b12">[13]</ref> sought to replace the class conditional distribution P (U j | Z) with another mixture of Gaussians, generating an enhanced predictive JID. These two approaches have the advantage of computational efficiency due to the assumption of Eq. ( <ref type="formula" target="#formula_4">4</ref>). Following the same conditional independence assumption, we propose modelling the class conditional distribution with a categorical model. Then, by leveraging the EM algorithm, we have found that the expected completedata log-likelihood has a strong connection to the proposed information-theoretic X -metric. The performance of the induced co-registration algorithm also exceeds that of the previous GMM-based methods.</p><p>Furthermore, the proposed algorithm extends naturally to a weakly-supervised scenario where the anatomical labels of a subset of images are provided. This feature enables a deep-learning framework that estimates segmentation and registration simultaneously and collaboratively in an endto-end fashion, i.e. the deep combined computing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">INFORMATION-THEORETIC CO-REGISTRATION</head><p>Given N observed images U = {U j } N j=1 , the purpose of co-registration is to find the spatial transformations φ = {φ j } N j=1 that aligns them into a common coordinate system Ω. This can be accomplished by an informationtheoretic metric defined over U . In information theory, the statistical dependency of a set of random variables can be measured by their total correlation, i.e. the Kullback-Leibler (KL) divergence between the joint distribution and the product of its marginals</p><formula xml:id="formula_5">C(U ) D KL   P (U ) N j=1 P (U j )   =   N j=1 H(U j )   -H(U ),<label>(5)</label></formula><p>where H(•) is the Shannon's entropy that evaluates the uncertainty of a random variable/vector. By definition, the total correlation is nonnegative and is maximized if the random variables are related by bijections so that one of them determines all the others. These properties can be exploited to achieve image registration, where the objective is to recover the spatial correspondences of two or multiple images by maximizing a given similarity metric. For instance, mutual information (MI), as a reduced version of the total correlation when N = 2, is widely adopted as a similarity metric for pairwise image registration <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>However, computation of the joint entropy H(U ) entails the construction of a JID P (U ), which suffers from the curse of dimensionality for N 2 with the widely adopted kernel density estimators <ref type="bibr" target="#b14">[15]</ref>. Yet for medical images, it is usually the case that U comprises a cohort of images from imaging processes that reflect a common anatomy. Let U j be the j-th observed image within the cohort as a realization of some stochastic imaging process over a discrete Cartesian grid Ω j ⊂ R d , with d the dimensionality of the sample space. These images can be considered as transmitted through distinct image formation channels (including shape and appearance transition) from the common   anatomy, also referred to as the common space Ω ⊂ R d <ref type="bibr" target="#b12">[13]</ref>.</p><p>The common space Ω is represented by a categorical random field Z = (z x ) x∈Ω , where each z x = [z x,1 , . . . , z x,K ] is a one-hot vector such that z x,k = 1 if and only if the spatial location x belongs to the k-th tissue class. Given a priori knowledge of the common anatomy through its probability distribution P (Z), the Shannon's entropy H(Z) measures the amount of uncertainty associated with P (Z). The reduction of this uncertainty due to the observed images is measured by the intensity-class relative entropy or mutual information</p><formula xml:id="formula_6">I(U , Z) = H(U ) -H(U | Z),<label>(6)</label></formula><p>where H(U | Z) is the conditional entropy of the observed images given the common anatomy. Moreover, by recognizing the conditional independence assumption in Eq. ( <ref type="formula" target="#formula_4">4</ref>) <ref type="bibr" target="#b24">[25]</ref>, Eq. ( <ref type="formula" target="#formula_6">6</ref>) can be written as</p><formula xml:id="formula_7">I(U , Z) = H(U ) - N j=1 H(U j | Z).<label>(7)</label></formula><p>The intensity-class mutual information I(U , Z) is maximized when U is deterministically dependent on Z. Thus, it can also be used as a similarity metric for co-registration of the observed images, provided that the distribution of the common anatomy is available.</p><p>Fig. <ref type="figure" target="#fig_10">4</ref>(a) shows the graphical representation of our generic framework, where the common anatomy Z is modelled as latent variables and the spatial transformations φ = {φ j } N j=1 are incorporated as deterministic parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">X -Metric: An N-Dimensional Information-Theoretic Framework</head><p>Unfortunately, both the total correlation and the intensity-class mutual information requires the computation of the joint entropy H(U ), which is computationally prohibitive in general for N 2. However, the combination of these two metrics happens to cancel out this term. Thus, we propose the following N -dimensional information-theoretic similarity metric by combining Eq. ( <ref type="formula" target="#formula_5">5</ref>) and Eq. ( <ref type="formula" target="#formula_7">7</ref>):</p><formula xml:id="formula_8">X (U , Z) C(U ) + I(U , Z) = N j=1 I(U j , Z).<label>(8)</label></formula><p>Apparently, the proposed metric is much more computationally favourable as it only aggregates the individual pairwise MI's between each observed image and the common anatomy. Since this metric is a combination of total correlation and intensity-class mutual information, and is a new yet unknown metric, thus referred to as the X -metric. The rationale for using the proposed X -metric in Eq. ( <ref type="formula" target="#formula_8">8</ref>) as a similarity metric to achieve co-registration is as follows: As spatial correspondences among the observed images are recovered, the reduction in uncertainty of the common anatomy due to the observations, measured by the mutual information I(U , Z), could be substantially improved. This could be intuitively explained as one's improved knowledge about the underlying anatomical structures with those registered images. More formally, one can write</p><formula xml:id="formula_9">I(U , Z) = H(Z) -H(Z | U ).</formula><p>Therefore, the uncertainty reduction may be accompanied by the sharpening of the inferred common anatomy. That is, as I(U , Z) increases, the conditional entropy H(Z | U ) would reduce and the posterior distribution P (Z | U ) would become more concentrated. Meanwhile, it is also plausible to expect that there will be a more consistent (or functional) relationship among their intensity values <ref type="bibr" target="#b0">[1]</ref>, thus increasing the total correlation. In summary, the maximization of the X -metric implies the maximization of I(U , Z) and/or C(U ), both of which are nonnegative and proper criteria for co-registration.</p><p>Note that computation of the X -metric requires the spatial distribution of the common anatomy, which is usually unknown a priori. In the following section, we develop an algorithm that applies the X -metric to image co-registration, which estimates the spatial distribution of the common anatomy along with the desired spatial correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">X -CoReg: Co-Registration Based on the X -metric</head><p>The proposed information-theoretic co-registration algorithm based on the X -metric, referred to as X -CoReg, solves the following optimization problem:</p><formula xml:id="formula_10">φ = arg max φ max α X (U [φ], Z),<label>(9)</label></formula><p>where U [φ] is the warped image group by spatial transformations φ, and α denotes the common-space parameters, comprising the spatial distribution Γ and the prior proportions π of the common anatomy, with</p><formula xml:id="formula_11">Γ = (γ x ) x∈Ω for γ x = [γ x,1 , . . . , γ x,K ] ∈ [0, 1] K and π = {π k } K k=1 satisfying K k=1 γ x,k = K k=1 π k = 1.</formula><p>As no closed-form solution of the inner optimization can be given, we resort to coordinate ascent by alternating the following two steps:</p><p>• Given current estimate of the spatial transformations φ [t] , update the common-space parameters α, i.e.,</p><formula xml:id="formula_12">α [t+1] = arg max α X U [φ [t] ], Z . (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>Algorithm 1: X -CoReg</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data:</head><p>The observed images U = {U j } N j=1 ; Input: Number of iterations T , regularization coefficient λ, registration step size η; Output: The estimated spatial transformations φ = { φ j } N j=1 ; 1 Initialization: φ</p><p>[0] j id for j = 1, . . . , N ; initialize π [0] and Γ [0] by Eq. ( <ref type="formula" target="#formula_28">22</ref>); 2 for t = 0, . . . , T -1 do / * Update the common-space parameters</p><formula xml:id="formula_14">* / 3 γ [t+1] x,k π [t] k N j=1 f [t] jk µj ;φ [t] j K k=1 π [t] k N j=1 f [t] jk µj ;φ [t] j ; 4 π [t+1] k x∈Ω φ [t] γ [t+1] x,k K k=1 x∈Ω φ [t] γ [t+1] x,k ; / * Update the spatial transformations * / 5 φ [t+1] = φ [t] -η • ∇L φ | U ; Γ [t+1] φ=φ [t] ; 6 if L converges then 7</formula><p>break loop;</p><formula xml:id="formula_15">8 return φ = φ [T ] .</formula><p>• Given current estimate of the common-space parameters α [t+1] , update the spatial transformations φ to increase the value of X -metric, i.e.,</p><formula xml:id="formula_16">φ [t+1] = arg max φ X U [φ], Z [t+1] ,<label>(11)</label></formula><p>where we write Z [t+1] as the common anatomy with spatial distribution Γ [t+1] for notational conciseness. Algorithm 1 summarizes the above co-registration procedure. The derivation is deferred to Section 3.3, where its rationale is established from the perspective of maximum likelihood. The following subsections detail its computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Update of the common-space parameters</head><p>Given current estimate of the spatial transformations φ [t] , the spatial distribution Γ = (γ x ) x∈Ω , with γ x = [γ x,1 , . . . , γ x,K ] ∈ [0, 1] K , are updated via computing the posterior of the common anatomy, i.e., γ</p><formula xml:id="formula_17">[t+1] x,k P z x,k = 1 | u φ x = µ; φ [t] = π [t] k N j=1 f [t] jk µ j ; φ [t] j K l=1 π [t] l N j=1 f [t] jl µ j ; φ [t] j ,<label>(12)</label></formula><p>where</p><formula xml:id="formula_18">f [t] jk µ j ; φ j P u φj x,j = µ j | z x,k = 1; φ j , Γ [t]<label>(13)</label></formula><p>describes the likelihood that the k-th anatomical label is observed with intensity level µ j in the j-th warped image and µ = (µ j ) N j=1 . Its form is defined in Section 3.2.3. Then, the prior proportions π = {π k } K k=1 are updated by normalizing Γ [t+1] across the spatial domain, i.e.,</p><formula xml:id="formula_19">π [t+1] k x∈Ω φ [t] γ [t+1] x,k K k=1 x∈Ω φ [t] γ [t+1] x,k ,<label>(14)</label></formula><p>where the overlap region</p><formula xml:id="formula_20">Ω φ {x ∈ Ω | φ j (x) ∈ Ω j , ∀ j}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Update of the spatial transformations</head><p>Given current estimate of the common-space parameters α [t+1] , the spatial transformations φ are updated by gradient descent on the loss function</p><formula xml:id="formula_21">L φ | U ; Γ [t+1] -X U [φ], Z [t+1] + λ • R(φ) = - N j=1 I U j • φ j , Z [t+1] + λ • R(φ),<label>(15)</label></formula><p>where R(φ) is a regularization term with λ its weight. Thus, we have the following update rule for φ:</p><formula xml:id="formula_22">φ [t+1] = φ [t] -η • ∇L φ | U ; Γ [t+1] φ=φ [t] ,<label>(16)</label></formula><p>with η defining the step size. The mutual information in Eq. ( <ref type="formula" target="#formula_21">15</ref>) is calculated using the formula</p><formula xml:id="formula_23">I U j • φ j , Z [t+1] = µj K k=1 p [t+1] j (µ j ,k; φ j ) ln p [t+1] j (µ j ,k; φ j ) p [t+1] j (µ j ; φ j ) p [t+1] j (k; φ j ) ,<label>(17) where p</label></formula><formula xml:id="formula_24">[t+1] j (µ j ,k; φ j ) P u φj x,j = µ j , z x,k = 1; Γ [t+1] ,<label>(18)</label></formula><p>with its marginals defined by</p><formula xml:id="formula_25">p [t+1] j (µ j ; φ j ) K k=1 p [t+1] j (µ j ,k; φ j ), p [t+1] j (k; φ j ) µj p [t+1] j (µ j ,k; φ j ).<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Density estimation</head><p>To complete the calculation of X -metric, it suffices to give a proper form of the conditional probability distribution f</p><p>[t] jk (µ j ; φ j ) in Eq. ( <ref type="formula" target="#formula_18">13</ref>) and the joint probability distribution p</p><p>[t+1] j (µ j , k; φ j ) in Eq. <ref type="bibr" target="#b17">(18)</ref>. The former essentially describes the appearance model of each anatomical structure and can be achieved by the kernel density estimator <ref type="bibr" target="#b25">[26]</ref>, namely</p><formula xml:id="formula_26">f [t] jk (µ j ; φ j ) 1 Z jk x∈Ω φ S β 3 u φj x,j -µ j h • γ [t] x,k<label>(20)</label></formula><p>with Z jk the normalizing factor, β 3 (•) the cubic B-spline kernel function fulfilling the partition of unity constraint <ref type="bibr" target="#b26">[27]</ref>, h the bandwidth of the kernel, and Ω φ S an coordinate sample drawn from the overlap region.</p><p>The latter term p</p><p>[t+1] j (µ j , k; φ j ) could be estimated via the same nonparametric approach, i.e.,</p><formula xml:id="formula_27">p [t+1] j (µ j , k; φ j ) 1 Z j x∈Ω φ S β 3 u φj x,j -µ j h • γ [t+1] x,k ,<label>(21)</label></formula><p>where Z j is the corresponding normalizing factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Initialization</head><p>The transformations φ are initialized to be the identity, and the prior proportions and the spatial distribution are initialized by</p><formula xml:id="formula_28">π [0] k 1 K , γ [0] x,k exp(g x,k ) K l=1 exp(g x,l ) ,<label>(22)</label></formula><p>where g x,k ∼ N (0, 1) for k = 1, . . . , K and ∀ x ∈ Ω.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Theoretical Insights from Maximum Likelihood</head><p>In this section, we study the rationale of the proposed algorithm from the perspective of maximum likelihood estimation (MLE) and the expectation-maximization algorithm.</p><p>Given the generative model depicted in Fig. <ref type="figure" target="#fig_10">4</ref>(a), the loglikelihood of the observed images has the form</p><formula xml:id="formula_29">(θ | U ) = x∈Ω φ ln K k=1 π k N j=1 f jk µ φj x,j ; Γ ,<label>(23)</label></formula><p>where µ φj x,j denotes the corresponding intensity level of the resampled intensity u φj x,j and θ = α ∪ φ comprises all model parameters.</p><p>One standard way to find maximum likelihood solutions is by the EM algorithm <ref type="bibr" target="#b27">[28]</ref> that alternates between: (a) E-step. Evaluate the posterior by</p><formula xml:id="formula_30">q [t] x,k P z x,k = 1 | u φ x = µ; θ [t] , ∀ x ∈ Ω φ [t]</formula><p>.</p><p>By definition, one can notice that for any</p><formula xml:id="formula_31">x ∈ Ω φ µ {x ∈ Ω φ | u φ x = µ}, the value of q x,k is a constant q µ,k , i.e., q x,k = q µ,k , ∀ x ∈ Ω φ µ .<label>(24)</label></formula><p>(b) M-step. Evaluate θ [t+1] given by</p><formula xml:id="formula_32">θ [t+1] = arg max θ Q θ | θ [t] ,<label>(25)</label></formula><p>where Q θ | θ [t] defines the expected complete-data loglikelihood at the t-th step, which expands as</p><formula xml:id="formula_33">Q θ | θ [t] E ln P (U , Z; θ) | U ; θ [t] = x∈Ω φ K k=1   q [t]</formula><p>x,k ln π k + q</p><formula xml:id="formula_34">[t] x,k N j=1 ln f jk µ φj x,j ; Γ   = K k=1 µ x∈Ω φ µ q [t] x,k   ln π k + N j=1 ln f jk (µ j ; φ j , Γ)   .<label>(26)</label></formula><p>One can find that the update of π can be solved analytically from Eq. ( <ref type="formula" target="#formula_34">26</ref>), yielding</p><formula xml:id="formula_35">π [t+1] k x∈Ω φ [t] q [t] x,k K k=1 x∈Ω φ [t] q [t] x,k .<label>(27)</label></formula><p>Then, substituting Eq. ( <ref type="formula" target="#formula_31">24</ref>) into Eq. <ref type="bibr" target="#b25">(26)</ref> gives</p><formula xml:id="formula_36">Q(θ | θ [t] ) = K k=1 µ |Ω φ µ | q [t] µ,k   ln π k + N j=1 ln f jk (µ j ; φ j , Γ)   ,<label>(28)</label></formula><p>in which the terms involving φ and Γ are rearranged into</p><formula xml:id="formula_37">S φ, Γ | θ [t] K k=1 µ |Ω φ µ | q [t] µ,k N j=1 ln f jk (µ j ; φ j , Γ). (<label>29</label></formula><formula xml:id="formula_38">)</formula><p>Note that according to the strong law of large numbers, the sample average converges almost surely to the true proportion <ref type="bibr" target="#b28">[29]</ref>, namely</p><formula xml:id="formula_39">|Ω φ µ | |Ω φ | a.s. -→ P * u φ x = µ , as |Ω φ | → ∞.<label>(30)</label></formula><p>Besides, we assume that the proposed representation can capture the true joint intensity distribution, i.e.,</p><formula xml:id="formula_40">P * u φ x = µ ≈ P u φ x = µ , ∀ x ∈ Ω φ .<label>(31)</label></formula><p>Thus, combining Eqs. <ref type="bibr" target="#b29">(30)</ref> to <ref type="bibr" target="#b30">(31)</ref> yields</p><formula xml:id="formula_41">|Ω φ µ | ≈ |Ω φ | P u φ x = µ .<label>(32)</label></formula><p>In addition, we allow q µ,k as a function of Γ and φ, i.e.,</p><formula xml:id="formula_42">q [t] µ,k = q µ,k (φ, Γ) φ=φ [t] ,Γ=Γ [t] = P z x,k = 1 | u φ x = µ; π [t] k , Γ .<label>(33)</label></formula><p>Therefore, by further denoting</p><formula xml:id="formula_43">g k (µ; φ, Γ) P u φ x = µ, z x,k = 1; π [t] k , Γ ,<label>(34)</label></formula><p>we obtain</p><formula xml:id="formula_44">S(φ, Γ | π [t] ) ≈ |Ω φ | • K k=1 µ g k (µ; φ, Γ) N j=1 ln f jk (µ j ; φ j , Γ) = -|Ω φ | • N j=1 H(U j • φ j | Z; Γ).<label>(35)</label></formula><p>Deriving an exact update rule for Γ is difficult with the kernel density estimator. However, the following strategy was empirically found to increase the value of the righthand side of Eq. ( <ref type="formula" target="#formula_44">35</ref>) w.r.t. Γ in general:</p><formula xml:id="formula_45">γ [t+1] x,k q [t] x,k , ∀ x ∈ Ω φ ,<label>(36)</label></formula><p>leading to Eq. ( <ref type="formula" target="#formula_17">12</ref>). Note that this strategy represents a generalized EM scheme <ref type="bibr" target="#b27">[28]</ref>. Besides, Eq. ( <ref type="formula" target="#formula_35">27</ref>) is thus equivalent to Eq. ( <ref type="formula" target="#formula_19">14</ref>). On the other hand, the registration parameters can only be updated numerically via gradient ascent. Specifically, given the new spatial distribution Γ [t+1] , the terms involving φ in the right-hand side of Eq. ( <ref type="formula" target="#formula_44">35</ref>) are a multiple of the sum of individual conditional entropies with each observed image given the common anatomy. Provided that the size of the overlap region is approximately identical under spatial transformations, the objective function for updating φ simply reduces to the sum of these conditional entropies. Nevertheless, to avoid a constant solution <ref type="bibr" target="#b0">[1]</ref>, the mutual information I(U j • φ j , Z) is preferred so that the proposed X -metric is optimized.</p><p>Hence, Algorithm 1 is closely related to the EM algorithm for solving the MLE of the image generative model in Fig. <ref type="figure" target="#fig_10">4</ref>(a), lending strong mathematical soundness to the proposed X -CoReg algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Application to Groupwise Registration</head><p>Groupwise registration aims to spatially align multiple images simultaneously. The process requires a similarity metric that exploits and combines information from the entire group of images.</p><p>Delightfully, the proposed X -metric is such a candidate. It first summarizes images into an anatomical template (the spatial distribution of the common anatomy) representing the average shape of the group. Then, the similarity between the average shape and each warped image is maximized to update the spatial correspondences. The procedure is interleaved and repeated until convergence, when the anatomical correspondences among the image group are recovered.</p><p>Groupwise registration can be further categorized according to the common space that one expects. If the common space is implicitly assumed during co-registration, it is referred to as the unbiased groupwise registration, as no bias is induced by assigning a certain reference image. On the other hand, if one requires a fixed target space to which the others are simultaneously registered, then the common space is set to the target space as a reference frame and the procedure is called the group-to-reference registration <ref type="bibr" target="#b29">[30]</ref>.</p><p>In unbiased groupwise registration, to avoid the degeneracy that aligns images to an arbitrary coordinate space, it is suggested that one constrain the sum of all deformations to be zero <ref type="bibr" target="#b30">[31]</ref>, i.e.,</p><formula xml:id="formula_46">1 N N j=1 φ j (x) = x, ∀ x ∈ Ω,<label>(37)</label></formula><p>effectively registering the images to an average space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Application to Deep Combined Computing</head><p>Beyond groupwise registration, combined computing aims to integrate registration with segmentation in the common space. Since the proposed X -metric measures the statistical dependency between the observed images and the common anatomy, it can naturally extend to a deep combined computing framework where registration and segmentation are performed simultaneously by neural network estimation. Besides, the framework allows a weaklysupervised setup where the anatomical label of some images within the group is already provided. In this section, we proceed with the description of this extended framework, the loss function to be optimized and the network architecture that integrates the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Extended framework</head><p>Formally, let {Y j } j∈J be the available segmentation masks of the corresponding observed images {U j } j∈J , where J is an index set. Each segmentation Y j is assumed to be a categorical random field, i.e. Y j = (y jω ) ω∈Ωj , with y jω = [y jω,1 , . . . , y jω,K ] ∈ {0, 1} K its one-hot representation. For the j-th image, the probability maps ρ j of the segmentation Y j are inferred from a neural network.</p><p>Fig. <ref type="figure" target="#fig_10">4</ref>(b) presents the graphical model for the extended framework. Specifically, given the common space anatomy Z, the observed segmentation Y j is assumed to be sampled from the conditional distribution</p><formula xml:id="formula_47">P y jφj (x),k = 1 | z x,l = 1 = δ kl • ρ jk (φ j (x)), ∀ x ∈ Ω,<label>(38)</label></formula><p>where ρ jk (φ j (x)) ∝ exp [τ • D jk (φ j (x))], δ kl is the Kronecker delta, D jk is the signed distance map of Y j for label k, and τ controls the slope of the distance.</p><p>Then, the initial appearance model, i.e. the conditional distribution of the observed image U j given Y j or Y j at t = 0, is calculated by</p><formula xml:id="formula_48">f [0] jk (µ j ) ∝ ω∈Ωj β 3 U j (ω) -µ j h • ρ jk (ω), j ∈ J , f [0] jk (µ j ) ∝ ω∈Ωj β 3 U j (ω) -µ j h • ρ jk (ω), j / ∈ J ,<label>(39)</label></formula><p>where ρ jk (ω) P ( y jω,k = 1 | U j ) for k = 1, . . . , K and ∀ ω ∈ Ω j are predicted by a neural network. However, for t ≥ 1, both f Thus, the posterior distribution of the common anatomy takes the form</p><formula xml:id="formula_49">q [t] x,k ∝ π [t] k j∈J ρ jk (φ j (x))f [t] jk (µ φj x,j ) j / ∈J f [t] jk (µ φj x,j )<label>(40)</label></formula><p>for k = 1, . . . , K and ∀ x ∈ Ω.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Loss function</head><p>To optimize the parameters of the extended framework Θ = {φ, ρ}, where ρ = { ρ j } N j=1 with ρ j K k=1 {ρ jk (ω) : ω ∈ Ω j }, we resort to the EM algorithm and its connection to the proposed X -metric. Thus, the total loss function for optimizing deep combined computing has three parts: 1) a registration loss L 1 (φ) using the proposed X -metric, 2) a hybrid loss L 2 (φ, ρ) that optimizes both registration and segmentation, and 3) a segmentation loss L 3 ( ρ) that optimizes the probability maps for both observed and unobserved segmentation masks.</p><p>Specifically, the expected complete-data log-likelihood of the extended framework can be arranged into two terms involving the network parameters. One term is the cross entropy between the posterior and the appearance model. For images with clear intensity-class correspondence, we approximate it using the proposed X -metric, which combines with regularization to fulfil registration by the loss function</p><formula xml:id="formula_50">L 1 (φ) -X (U [φ], Z [2] ) + λ • R(φ),<label>(41)</label></formula><p>where Z [2] is the common anatomy with the spatial distribution updated by Eq. ( <ref type="formula" target="#formula_17">12</ref>) and Eq. ( <ref type="formula" target="#formula_19">14</ref>) for t = 0, 1. The term R(φ) is given by the aggregated bending energy with all spatial transformations <ref type="bibr" target="#b31">[32]</ref>. Detailed explanations of the approximation can be found in Sec. 1.1 of the supplementary material. Note that we use Z [2] instead of Z [1] because at t = 0 the appearance model is calculated using the probability maps of the image anatomy rather than the spatial distribution of the common anatomy.</p><p>The other term of the complete-data log-likelihood is a hybrid loss given by the cross entropy between the posterior and the warped probability maps, i.e.,</p><formula xml:id="formula_51">L 2 (φ, ρ) j∈J H Z [2] Y j • φ j + j / ∈J H Z [2] Y j • φ j , (<label>42</label></formula><formula xml:id="formula_52">)</formula><p>where</p><formula xml:id="formula_53">H Z [2] Y j • φ j - 1 |Ω| x∈Ω K k=1 γ [2]</formula><p>x,k • log ρ jk (φ j (x)), <ref type="bibr" target="#b42">(43)</ref> and</p><formula xml:id="formula_54">H Z [2] Y j • φ j - 1 |Ω| x∈Ω K k=1 γ [2]</formula><p>x,k • log ρ jk (φ j (x)). <ref type="bibr" target="#b43">(44)</ref> Note that gradient of L 2 passes through both φ j and ρ j for j / ∈ J while only through φ j for j ∈ J . Therefore, it can optimize both registration and segmentation.</p><p>Finally, we include an additional segmentation loss, i.e.,</p><formula xml:id="formula_55">L 3 ( ρ) - N j=1 I(U j , Y j ) + j / ∈J H( Y j ) + j∈J L seg (Y j , Y j ),<label>(45)</label></formula><p>where the mutual information in the first term optimizes probability maps based on image intensities, the second term encourages the probability vector [ ρ j1 (ω), . . . , ρ jK (ω)] to be concentrated, and the last term measures the discrepancy between the network prediction and the ground-truth segmentation using cross entropy and Dice loss, namely</p><formula xml:id="formula_56">L seg (Y j , Y j ) H Yj Y j + 1 -DSC(Y j , Y j ) .<label>(46)</label></formula><p>Hence, the total loss function takes the form</p><formula xml:id="formula_57">L(φ, ρ) L 1 (φ) + L 2 (φ, ρ) + L 3 ( ρ).<label>(47)</label></formula><p>3.5.3 Network architecture Fig. <ref type="figure" target="#fig_6">5</ref> presents the network architecture for deep combined computing. The network is composed of an encoder E, a bottleneck, a segmentation decoder D s and a registration decoder D r . They comprise multiple levels of residual convolutional blocks (RCBs) and residual connections between the encoder and decoder <ref type="bibr" target="#b32">[33]</ref>. The convolutional layers of the block in the l-th level have C • 2 l-1 feature maps, for l = 1, . . . , L R , with C a constant.</p><p>The task of segmentation and registration are often regarded related. Segmentation aims to assign pixel-wise semantic labels to the input image, while registration seeks to find structural and spatial correspondences between input images. Thus, when designing a neural network to make predictions, we believe to some extent there could be shared representations utilized by both of the tasks. Therefore, the proposed network has a shared encoder and two separated decoders for the two tasks.</p><p>To extract modality-invariant features, the convolutional layers of the encoder E are shared across modalities, while domain-specific batch normalization layers seek to disentangle structural codes from the appearances of multimodal input images <ref type="bibr" target="#b33">[34]</ref>. The segmentation decoder D s is then fed with the modality-invariant features to predict the segmentation probability maps ρ for each image.</p><p>The extracted features of each image are also fused by an abstraction layer that computes their first and second moments <ref type="bibr" target="#b34">[35]</ref>. The fused feature maps then pass through the registration decoder D r to predict the desired spatial correspondences φ in the form of dense displacement fields.</p><p>Moreover, the network parameters for segmentation and registration are optimized alternately so that the improvement of one task can benefit the other. We chose to alternate training between the two branches because in Eq. ( <ref type="formula" target="#formula_51">42</ref>) the term H Z <ref type="bibr" target="#b1">[2]</ref> ( Y j • φ j ) is computed using both branches. That is, Y j is predicted by the segmentation branch while φ j is predicted by the registration branch. Therefore, to avoid interference in two branches, like the situation where the registration branch may seek to compensate for errors in the segmentation prediction, it could be better to alternate the training for the two branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND RESULTS</head><p>We evaluate our method extensively on a total of five publicly available datasets, with one synthetic dataset and four clinical datasets, i.e. BrainWeb <ref type="bibr" target="#b35">[36]</ref>, RIRE <ref type="bibr" target="#b36">[37]</ref>, MoCo <ref type="bibr" target="#b37">[38]</ref>, MS-CMR <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39]</ref> and Learn2Reg-2021 <ref type="bibr" target="#b39">[40]</ref>. Our major focus is to showcase the wide applicability of our method in groupwise registration and deep combined computing. Therefore, detailed parameter studies shall be addressed in future work. We conducted the experiments in the following three aspects: 1) Multimodal groupwise registration. Joint analysis of images from multiple acquisitions requires groupwise registration to a common coordinate space, where complementary information of the corresponded anatomies can be collected <ref type="bibr" target="#b12">[13]</ref>. To perform a proof of concept for multimodal groupwise registration, we validated the proposed method on the BrainWeb and RIRE datasets. The two experiments were aimed at testing the effectiveness of the X -metric on images from various modalities and organs, as well as the situations where registration was performed with different transformation models, i.e. nonrigid or rigid. The results were compared to previously discussed groupwise similarity metrics, including the conditional template entropy (CTE) <ref type="bibr" target="#b5">[6]</ref>, the accumulated pairwise estimates (APE) <ref type="bibr" target="#b3">[4]</ref>, and the Gaussian mixture model (GMM) <ref type="bibr" target="#b15">[16]</ref>. 2) Spatiotemporal motion estimation. Motion estimation of dynamic medical images is essential in quantifying tissue properties from images acquired after the injection of some contrast agent. For instance, the cardiac perfusion sequences are used to calculate the myocardial perfusion reserve (MPR), i.e. the ratio of myocardial blood flow at stress versus rest, which provides prognostic value in assessing suspected cardiovascular disease. However, motion artefacts may hamper the accuracy and robustness of image-based quantification <ref type="bibr" target="#b37">[38]</ref>. We therefore investigated the performance of the proposed algorithm in correcting motion artefacts for the cardiac perfusion MR images. Unlike difference in imaging modalities for multimodal groupwise registration, the appearance variation of perfusion MR comes from the contrast agent, and the number of images to be registered as well as the degree of freedom with the transformation model are much larger, requiring more robust yet efficient groupwise similarity metrics. The test images were from the MoCo dataset. Both quantitative and qualitative results were compared with alternative registration methods, including the variance of intensities (VI) <ref type="bibr" target="#b20">[21]</ref>, the congealing algorithm (CG) <ref type="bibr" target="#b2">[3]</ref> and the conditional template entropy (CTE) <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Deep combined computing for multimodal images.</head><p>We investigate the potential of the extended framework and its integration with neural networks in realizing combined computing. The task is particularly meaningful when the goal is to achieve simultaneous registration and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual connection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstraction</head><p>Residual connection , Eq. ( <ref type="formula" target="#formula_51">42</ref>)</p><p>, Eq. ( <ref type="formula" target="#formula_50">41</ref>)</p><p>, Eq. ( <ref type="formula" target="#formula_55">45</ref>) segmentation in an end-to-end fashion. The effectiveness of the proposed framework for deep combined computing was validated on the MS-CMR dataset, with various configurations of the training strategy. The results were compared with those from the multivariate mixture model (MvMM) <ref type="bibr" target="#b12">[13]</ref>, a competing method for iterative combined computing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Architecture for Deep Combined Computing</head><p>The algorithms were implemented in PyTorch <ref type="bibr" target="#b40">[41]</ref> and run on an NVIDIA RTX TM 3090 GPU. The following subsections detail the experimental design and results for each of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multimodal Groupwise Registration on BrainWeb</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Experimental design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1)</head><p>Objective. This experiment aims to demonstrate the performance of our method on multimodal nonrigid groupwise registration for multi-sequence brain MRI from the synthetic BrainWeb dataset as a proof of concept.</p><p>2) Data description. The BrainWeb online database 1 provides simulated T1-, T2-and PD-weighted MRI volumes from an anatomical phantom. The image volumes have the physical spacing of 181 × 217 × 181 mm 3 . We used the image volumes corrupted by 3% noise (relative to a reference tissue) and 20% intensity non-uniformity. The data were preprocessed by skull-stripping and the anatomical labels were remapped into foreground regions composed of cerebrospinal fluid (CSF), grey matter (GM), and white matter (WM). For a proof of concept, we only selected the middle slice from the axial view of each image for demonstration. The images were normalized by z-scoring for a fair comparison between different methods.</p><p>3) Recovery of initial misalignment. Since the simulated images were aligned by design, one could apply initial 1. <ref type="url" target="https://brainweb.bic.mni.mcgill.ca/">https://brainweb.bic.mni.mcgill.ca/</ref> spatial transformations to the images and correct the misalignment through unbiased groupwise registration. Readers are referred to Sec. 2.1 of the supplementary materials for details on how to generate the initial misalignments.</p><p>We used multi-level isotropic FFDs as the transformation model to recover spatial correspondences from the initial misalignment. The FFD spacings and the number of registration steps are described in the supplementary material. The deformation regularization was imposed by bending energy over the FFD meshes, with λ = 0.001. The Adam optimizer <ref type="bibr" target="#b41">[42]</ref> was adopted to optimize the registration, with an initial step size of η = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Registration methods. The groupwise registration methods for comparison include:</head><p>• X -Reg-UN. This approach uses the proposed X -metric in Eq. ( <ref type="formula" target="#formula_8">8</ref>) and the procedure in Algorithm 1. As no prior knowledge of the appearance model is required for the proposed algorithm, it is tagged as "UNsupervised". • X -Reg-GT. This is a variant of the proposed algorithm:</p><p>The default appearance model in Eq. ( <ref type="formula" target="#formula_26">20</ref>) is estimated in conjunction with the common space. Nevertheless, if the segmentation model is available for each image, one can construct the "Ground-Truth" appearance model directly from the intensity-class correspondences, i.e.,</p><formula xml:id="formula_58">f [t] jk (µ j ) = 1 Z jk ω∈Ωj β 3 U j (ω) -µ j h •y jω,k , ∀ t,<label>(48)</label></formula><p>where Y j = (y jω ) ω∈Ωj is the categorical random field given by the segmentation of the observed image U j , with y jω = [y jω,1 , . . . , y jω,K ] ∈ {0, 1} K . This ground-truth appearance model can be viewed as a priori knowledge of the underlying imaging process of the common anatomy when the images were already in alignment. This is in the same spirit as the work of <ref type="bibr" target="#b42">[43]</ref>, where prior information on the joint distribution of correctly aligned training  <ref type="bibr" target="#b5">[6]</ref> 0.545 ± 0.509* 9.0 × 10 -3 APE <ref type="bibr" target="#b3">[4]</ref> 0.411 ± 0.298* 1.3 × 10 -3 GMM <ref type="bibr" target="#b15">[16]</ref> 0.500 ± 0.639 0.62 CG <ref type="bibr" target="#b2">[3]</ref> N/A images were used to perform pairwise registration.</p><p>• CTE. This method uses the conditional template entropy proposed in <ref type="bibr" target="#b5">[6]</ref> as the groupwise similarity metric. Unlike our method that computes an anatomical template as Γ, it assumes a grey-valued template estimated by principal component analysis (PCA) from the warped images. • APE. This method uses the accumulated pairwise estimates proposed in <ref type="bibr" target="#b3">[4]</ref>, computed as the sum of all pairwise mutual information from the warped images. Thus, it has a heavy computational burden. • GMM. This method uses the MLE from a Gaussian mixture model to achieve co-registration <ref type="bibr" target="#b15">[16]</ref>. Instead of a factorized categorical distribution assumed in our modelling, the GMM method presumes a multivariate Gaussian intensity distribution given the common anatomy.</p><p>Note that GMM has the complexity of O(N 3 ) in general. We also emphasize that the congealing (CG) algorithm is not applicable to this experiment, as a small image group (N = 3) prevents accurate density estimation. Besides, for similarity metrics with kernel density estimators, we set the number of intensity levels as L = 64 and the sample rate as 0.1. Moreover, four common anatomical structures were assumed for X -CoReg and GMM, i.e. K = 4. 5) Evaluation metric. The root mean squared residual displacement error, a.k.a. the warping index <ref type="bibr" target="#b26">[27]</ref>, was used as the evaluation metric. The groupwise warping index was calculated within the foreground regions, i.e.,</p><formula xml:id="formula_59">gWI(φ † , φ) 1 N N j=1 1 Ω f j x∈ Ω f j rj (x) 2 2 , (<label>49</label></formula><formula xml:id="formula_60">)</formula><p>where</p><formula xml:id="formula_61">Ω f j {x ∈ Ω | φ † j • φ j (x) ∈ F } with F the foreground region of the initial phantom and rj (x) r j (x) - 1 N N j =1 r j (x), r j (x) φ † j • φ j (x) -x.</formula><p>(50) The gWI will reduce to zero when the initially misaligned images are perfectly co-registered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Results</head><p>As our algorithm falls into the category of templatebased groupwise registration, we compared it with another such method called the conditional template entropy (CTE) <ref type="bibr" target="#b5">[6]</ref>. We also compared our method with two more com-putationally demanding methods, namely the accumulated pairwise estimates (APE) <ref type="bibr" target="#b3">[4]</ref> and the Gaussian mixture model (GMM) <ref type="bibr" target="#b15">[16]</ref>, though their computational burden is non-negligible for a larger image group.</p><p>Table <ref type="table" target="#tab_2">2</ref> presents the mean and standard deviation of the groupwise warping indices before and after co-registration using different methods. All methods have achieved submillimeter accuracy on average. In particular, the two variants of the proposed algorithm perform comparably, indicating no requirement for the ground-truth appearance model. The two variants also perform consistently better than CTE. This suggests that it is more reasonable to use the proposed anatomical representation of the common space than the grey-valued template computed by PCA.</p><p>The table also shows that APE works better than all template-based methods marginally, with less than 0.1 mm improvement on average. This could be attributed to the strong modelling capacity of a pairwise MRF, if not offset by its high computational cost. The results of GMM show noticeably larger standard deviations. We present the box plots for post-registration warping indices in the supplementary material. The gWIs of GMM have more outliers than the others, suggesting its inferior capture range for large deformations.</p><p>We also demonstrate the posterior distribution of the common anatomy given by the X -CoReg algorithm for different choices of K in the supplementary material. One can see that as K increases, the proposed X -CoReg is able to reveal the anatomical structures from the observed images, including white matter (WM), grey matter (GM), and cerebrospinal fluid (CSF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multimodal Groupwise Registration on RIRE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Experimental design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1)</head><p>Objective. This experiment aims to validate the performance of our method on multimodal rigid groupwise registration for brain images from the RIRE dataset.</p><p>2) Data description. The RIRE dataset contains CT, PET and (T1w, T2w, PDw) MR scans of 18 subjects. Among them ground-truth correspondences of one training subject were provided by fiducial marker based rigid registration. As the online evaluation system is no longer available, the evaluation was conducted by warping the registered training images with known rigid transformations, and then recovering them using different registration methods.</p><p>3) Generation of initial misalignment. The training subject contains 8 images of various modalities. These images were first registered to the CT scan using the given groundtruth rigid registration. Then, the registered images were resampled to physical spacing of 1 × 1 × 4mm 3 and cropped to size of 256 × 256 × 29. A total of 50 initial random rigid transformations were generated. The transformations had three rotation parameters sampled from <ref type="bibr">[-15, 15]</ref> degrees and three translation parameters from <ref type="bibr">[-20, 20]</ref> mm. The rotation was centred at the image center and performed after translation. 4) Recovery of initial misalignment. To recover the initial misalignment, groupwise rigid registration was performed on the misaligned images. Specifically, the registration was performed in two stages. The three translation parameters were first optimized, followed by a full rigid registration with nine parameters: the center coordinates of rotation, the rotation angles and the translation offsets. To accelerate convergence, the registration was performed in four and two resolution levels for the first and stages, respectively. The optimization procedure was achieved by the Adam optimizer, with the initial step sizes for translation, rotation center and rotation as 1, 1 and 0.01, respectively. The number of iterations for the two stages was 200 and 400, which was distributed equally among different resolution levels. Moreover, the constraint in Eq. ( <ref type="formula" target="#formula_46">37</ref>) was imposed such that the images were registered unbiasedly. 5) Registration methods. The following registration methods were compared on the RIRE dataset:</p><p>• X -CoReg. This method uses the proposed algorithm in Algorithm 1.</p><p>• CTE. This method uses the conditional template entropy as the groupwise similarity metric <ref type="bibr" target="#b5">[6]</ref>.</p><p>• GMM. This method finds the MLE of a Gaussian mixture model to achieve groupwise registration <ref type="bibr" target="#b15">[16]</ref>.</p><p>• APE. This method uses the accumulated pairwise estimates from all pairwise mutual information as the similarity metric <ref type="bibr" target="#b3">[4]</ref>. In addition, for registration methods using the kernel density estimator, 32 and 64 intensity levels were adopted for the first and second registration stage, respectively. The sample rate was set as 0.1. Besides, the default number of common anatomical labels was assumed as 8 for X -CoReg and GMM, i.e. K = 8. 6) Evaluation metric. We use the groupwise registration error (gRE) in eight vertices of the image volume as the evaluation metric. Specifically, let V = {v i } 8 i=1 be the physical coordinates of the eight vertices, with v i ∈ Ω ⊂ R 3 . The groupwise registration error is defined as the average root mean squared error of the eight vertices, i.e.,</p><formula xml:id="formula_62">gRE(φ † , φ) 1 N N j=1 1 8 8 i=1 ēj (v i ) 2 2 ,<label>(51)</label></formula><p>where</p><formula xml:id="formula_63">ēj (v i ) φ † j • φ j (v i ) - 1 N N j=1 φ † j • φ j (v i ),<label>(52)</label></formula><p>with φ † = {φ † j } N j=1 the initial misalignments and φ = { φ j } N j=1 the estimated transformations. The gRE will reduce to zero when misaligned images are perfectly co-registered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results</head><p>Table <ref type="table">3</ref> presents the gREs on the RIRE dataset before and after groupwise registration using different methods. Apparently, CTE and GMM failed to achieve a good accuracy, while both APE and X -CoReg reduced the gREs remarkably. Note that the relatively low computational cost of GMM may be attributed to additional acceleration in Py-Torch. However, APE required over four times the computational complexity compared to X -CoReg as it accumulated 8 2 = 28 pairwise MI's, making it less efficient than our algorithm.</p><p>Fig. <ref type="figure" target="#fig_7">6</ref> visualizes the misaligned training pair with median gRE before registration and its registration results TABLE 3: Results on the RIRE dataset. The table presents the mean and standard deviation of the gREs (in millimeters) before and after groupwise registration using different methods, along with their time and GPU memory consumption ratio (r) compared to X -CoReg. Statistically significant differences in gRE between X -CoReg and the others, suggested by a paired t-test (p &lt; 0.05), are indicated with asterisks.  using different methods. One can observe substantial initial misalignment among different images from the first row of the figure, with initial gRE of 40.179 mm. However, neither GMM nor CTE could lead to well-registered images, especially for the CT, T2 rectified and PET scans, yielding poor gREs of 15.491 and 14.885 mm on this training pair, respectively. On the other hand, both APE and the proposed X -CoReg could register this training pair successfully, producing superior gREs of 6.691 and 6.102 mm, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">The number of common anatomical labels</head><p>We also studied the influence of K on the registration accuracy for the X -CoReg algorithm. Fig. <ref type="figure">7</ref> plots the gRE of X -CoReg against the number of common anatomical labels. One can see that increasing K seems to benefit the registration accuracy up to a certain level. This could be attributed to a better clustering of the JID using K that is close to the true number of common anatomical labels, leading to enhanced registration performance. In general, the performance of our method is robust to the choice of the hyperparameter K. The influence of the number of common anatomical labels K on the gRE with the RIRE dataset after registration using the proposed X -CoReg algorithm. One can see improved accuracy as K increases until some threshold is attained, e.g. K = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Spatiotemporal Motion Estimation on MoCo</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Experimental design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1)</head><p>Objective. This experiment intends to demonstrate the performance of our method on spatiotemporal motion estimation for dynamic contrast enhanced cardiac perfusion images from the MoCo dataset.</p><p>2) Data description. The MoCo dataset contains midventricular short-axis first-pass cardiac perfusion MR images for 10 patients at both rest and adenosine induced stress phases <ref type="bibr" target="#b37">[38]</ref>. Each perfusion sequence consists of 50 frames scanned over approximately 70 heartbeats, during which the T1-weighted MR imaging is used to track the uptake and washout of a contrast agent. The contrast agent can provide information about the myocardial blood flow, measured by the myocardial perfusion reserve (MPR). However, motion artefacts caused by involuntary respiration will affect the precision with image-based quantification of the MPR, as misalignment of the temporal frames can impair spatial correspondences among the myocardial regions. Fig. <ref type="figure" target="#fig_9">8</ref> shows an example of the perfusion sequence.</p><p>3) Motion correction procedure. To correct the motion artefacts, the groupwise registration algorithms proceeded with the following steps:</p><p>• Preprocessing. The images were preprocessed by a noise reduction filtering, z-score normalization, and the extraction of a region of interest (ROI). The noise reduction filtering was done by an isotropic Gaussian filter with the standard deviation as 1 pixel to avoid local optima and accelerate convergence of the registration. Then, to confine the information used for registration, the ROI was obtained from dilating a segmentation mask of the left ventricle (including the myocardium) on a reference frame with a circular kernel of 10-pixel radius. • Rigid registration. The bulk motion constitutes a major part of the respiration-induced misalignment in the perfusion sequence <ref type="bibr" target="#b18">[19]</ref>. To correct for this effect, a two-stage rigid registration was conducted. The translation was first optimized, followed by a full rigid transformation with five parameters: the center coordinates of rotation, the rotation angles and the translation offsets. To accelerate convergence, the registration was performed in two resolution levels for both stages. • Nonrigid registration. To account for the residual elastic motion artefacts caused by potentially inaccurate electrocardiogram (ECG) triggering or breath-related deformations, a nonrigid registration step was included, with FFD as the transformation model. As the perfusion images were scanned at approximately the same moment in the cardiac cycle <ref type="bibr" target="#b37">[38]</ref>, a large mesh spacing of 40 × 40 mm 2 were applied for the FFD control points. The deformation regularization was enforced by bending energy over the FFD meshes, with λ = 0.01. The optimization of the registration was fulfilled by the Adam optimizer, with the initial step sizes for translation, rotation center/rotation and control point displacement as 0.1, 0.1/0.001 and 0.1, respectively. The number of iterations for registration by translation/rigid transformation and FFD was 100/50 and 50, respectively. For those optimized using the multi-resolution strategy, the number of iterations were equally distributed among different resolutions. In addition, the zero-sum constraint on the transformations (Eq. ( <ref type="formula" target="#formula_46">37</ref>)) was imposed such that the groupwise registration was performed in an unbiased fashion. 4) Registration methods. The following registration methods were compared on the cardiac perfusion MR images:</p><p>• X -CoReg. This method uses the proposed algorithm with the number of intensity levels as L = 64. The number of common anatomical labels was assumed as 6 to account for the varying contrast along the sequence. • VI. This method uses the variance of intensities as the similarity metric <ref type="bibr" target="#b20">[21]</ref>. The metric is supposed to be only applicable to monomodal groupwise registration. We include it here to verify that registering perfusion MR sequences in a monomodal manner might produce suboptimal accuracy. • CG. This method minimizes the entropy of the pixel stacks to drive the registration process, a.k.a. the congealing algorithm <ref type="bibr" target="#b2">[3]</ref>. The implementation was based on the work of <ref type="bibr" target="#b43">[44]</ref>, where a Gaussian kernel G σ was used for the kernel density estimator, with the standard deviation σ = 0.05 for this experiment as it produced the best accuracy. • CTE. This method uses the conditional template entropy <ref type="bibr" target="#b5">[6]</ref>, with the number of intensity levels as L = 64. Note that the computational cost of each method is acceptable, while a more complex algorithm is prohibitive to implement for a perfusion sequence comprising 50 frames. 5) Evaluation metric. To evaluate the performance of motion correction, we compute the Dice similarity coefficient (DSC) on the propagated segmentation masks for myocardium at different time points after groupwise registration. The segmentation masks were manually delineated at the moments where large motion was observed compared to the previous frames. Roughly 18 frames were segmented for each perfusion sequence on average. The DSC was averaged over all pairs of the warped segmentation masks to produce the evaluation metric on one sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results</head><p>Table <ref type="table" target="#tab_4">4</ref> presents the DSCs on the MoCo dataset before and after motion correction using different registration methods and transformation models. One can observe that the performance of the proposed X -CoReg exceeds all previous methods in terms of the average DSC, with statistically significant improvement over VI and CTE for  all transformation models. Noticeably, while VI and CTE work moderately well with rigid transformation, their performance drops in nonrigid registration. In other words, VI and CTE may only be applicable to the dynamic contrast-enhanced perfusion sequence when the problem is to achieve global motion correction. On the other hand, our proposed algorithm works consistently better regardless of the transformation model. Fig. <ref type="figure" target="#fig_12">9</ref> visualizes the case with median improvement on average after motion correction using different registration methods. Since the proposed algorithm is based on the assumption of the common anatomy, it better preserves the structural correspondences of the images along the temporal frames. On the other hand, methods based only on certain assumptions with the joint intensity profile, e.g. VI, CG and CTE, may result in disconnected anatomical structures on the registered sequence.   of the cardiac structures. Namely, the LGE images can display the area of myocardium infarction, the bSSFP images present clear structural boundaries, and the T2 images can reflect the acute injury and ischemic regions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39]</ref>. For the purpose of demonstration, the images were preprocessed by affine co-registration, ROI cropping, background suppression and slice selection to reduce the complexity of the dataset. As a result, a total of 39 image slices were used for training, 15 for validation and 44 for testing. Besides, as the initial images are almost pre-aligned, to better demonstrate the efficacy of the proposed framework for deep combined computing, we imposed synthetic FFDs on the original images to cause additional misalignments. Five synthetic FFDs were generated with four different mesh spacings for each sequence. Moreover, the FFDs were combined among sequences during training to produce more training samples, resulting in a total of 39×5 3 ×4 = 19500 image groups.</p><p>On the other hand, the validation and test set comprised 15 × 5 × 4 = 300 and 44 × 5 × 4 = 880 image groups, respectively.</p><p>3) Training strategies. Four different training strategies were compared on the extended framework for deep combined computing:</p><p>• DGR. This strategy uses the generic framework where no segmentation masks are observed, i.e. J = ∅. Thus, the problem reduces to deep groupwise registration with neural network estimation. Note that in this case the accuracy of the predicted segmentation results will not be evaluated, as no anatomical semantics are prescribed. • DCC+AT. This strategy performs deep combined computing using an atlas Y 4 computed from majority voting over the training bSSFP labels. Therefore, it can be regarded as a special case of the extended framework for J = {4}. • DCC+BS. This strategy used the extended framework with observed segmentation mask Y 2 for bSSFP images, i.e. J = {2}. • DCC+All. This strategy assumes all segmentation masks of the image groups are provided, i.e. J = {1, 2, 3}. Besides, for all these strategies, we used four levels of RCBs, i.e. L R = 4, and the number of channels for the initial RCB was set as C = 16. For computation of the X -metric, the number of intensity levels were set as L = 32 and the number of common anatomical labels was assumed as K = 4 for the myocardium, left ventricle, right ventricle and the background. In addition, to avoid overconfidence in posterior computation caused by too concentrated probability maps, we clip the value of ρ jk (ω) and ρ</p><formula xml:id="formula_64">jk (ω) into [ζ, 1 -(K -1) • ζ],</formula><p>with ζ = 0.05 for training and ζ = 0.01 for testing. Furthermore, the constraint of Eq. ( <ref type="formula" target="#formula_46">37</ref>) was imposed and the bending energy with λ = 100 was used for deformation regularization. 4) Optimization schemes. To optimize the network parameters, we used the Adam optimizer with an initial learning rate of 1 × 10 -3 for (E, D s ), and 1 × 10 -4 for D r . The optimization for (E, D s ) and D r were alternated every 5 training steps. The training process lasted for 100 epochs with a batch size of 20. The best model on the validation set in terms of registration accuracy was used for testing on the test data. 5) Competing method. We also compare our method with another iterative approach to combined computing, known as the MvMM <ref type="bibr" target="#b12">[13]</ref>. This method optimizes the likelihood function of a multivariate mixture model using the generalized EM algorithm and iterative conditional mode. The MvMM was initialized and regularized by the probability maps from an atlas computed in the same way as DCC+AT.</p><p>To re-implement the method on the MS-CMR dataset, we used FFDs with control point spacing of 40 pixels as the transformation model, with bending energy coefficient λ = 0.001. The number of tissue subtypes were set the same as the original paper. The zero-sum constraint of the deformations and the value clipping of the probability maps were also imposed. The optimization last for 400 steps, with the initial step size as η = 0.1 for the Adam optimizer. 6) Evaluation metrics. The performance of combined computing was evaluated in terms of both registration and segmentation. The registration accuracy was calculated as the  DSC on the propagated segmentation masks {Y j • φ j } N j=1 with the registered images. The DSC was averaged over all pairs of the warped segmentation masks to produce the value. On the other hand, the segmentation accuracy was evaluated by the DSCs between the estimated posterior segmentation Z [2] and each warped segmentation mask Y j • φ j , where z <ref type="bibr" target="#b1">[2]</ref> x,k * = 1 if and only if</p><formula xml:id="formula_65">k * = arg max k=1,...,K    π [2] k j∈J ρ jk ( φ j (x)) N j=1 f [t] jk (µ φj x,j )    .<label>(53)</label></formula><p>Therefore, it was intended to measure errors in both registration and segmentation predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Results</head><p>Table <ref type="table" target="#tab_5">5</ref> presents the evaluation metrics on the MS-CMR dataset using MvMM and deep combined computing with different training strategies. One can observe that both registration and segmentation accuracies of our method improve with increased supervision. Particularly, compared to MvMM, the DCC+AT strategy performs better in registration. Fig. <ref type="figure" target="#fig_13">10</ref> visualizes results from an exemplar case with median Reg DSC before co-registration. One can see that without any supervision, the DGR strategy may produce inaccurate registration in regions with ambiguous intensity class correspondence. On the other hand, for strategies like MvMM, DCC+AT and DCC+BS with partial or weak supervision, the posterior segmentation is satisfactory but may overplay the appearance model in posterior computation, resulting in irregular boundaries of the predicted segmentation. Using DCC+All with full supervision can mitigate this issue and yield more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Symmetric Registration on Learn2Reg-Task1</head><p>We also tested the proposed X -CoReg on Task1 of the Learn2Reg-2021 challenge <ref type="bibr" target="#b39">[40]</ref>. The registration was performed symmetrically between 3 MR-CT validation pairs using rigid, affine and FFD as the transformation models. Table <ref type="table" target="#tab_6">6</ref> presents the registration accuracy evaluated by DSC. One could find that our method has successfully registered these image pairs. Please refer to Section 3.5 of the supplementary material for detailed setups and results of this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION AND CONCLUSION</head><p>In this paper, we have proposed an information-theoretic framework that facilitates groupwise registration and deep combined computing. The framework builds on a novel formulation of the joint intensity distribution by representing the common anatomy as latent variable and the appearance models as nonparametric estimators. Interestingly, the proposed X -metric can be interpreted from both the information-theoretic and the maximum-likelihood perspective: on one hand, the X -metric measures the shared information among the observed images through their underlying common anatomy; on the other hand, it serves as an approximation of the objective function for finding the MLE of the image generative model with the EM algorithm. Inspired by this connection, a co-registration algorithm named X -CoReg is empirically found to jointly align the observed images with linear computational complexity.</p><p>To examine its applicability, we investigated a variety of tasks with the proposed framework, including multimodal groupwise registration, spatiotemporal motion estimation, and deep combined computing. Compared to previous approaches, our method has shown great competence in its efficacy and efficiency. Particularly, the X -CoReg algorithm is able to register the image group even if the common anatomy is visible only from certain modalities. Fig. <ref type="figure" target="#fig_14">11</ref> shows an example of the registered images produced from the algorithm on the RIRE dataset, with posterior segmentation overlaid. One can observe that the anatomical structures manifest themselves distinctively through different imaging modalities. Nevertheless, our method is still capable of identifying the common anatomy that interrelates the images and revealing their anatomical correspondences. Additional details on the relationship between K and the common anatomy identified from the BrainWeb images can be found in the supplementary material. One limitation of the proposed algorithm is the potential struggle with image artefacts, which causes the mutual information to stumble at local maxima. An example of such effect is shown in Fig. <ref type="figure" target="#fig_16">12</ref> on the MoCo dataset. Apparently, the original sequence suffers from a low signal-to-noise ratio (SNR). This, from the viewpoint of partition alignment, will cause the image partition resulting from histogramming to become scattered islands and not align with the anatomical boundaries, leading to suboptimal registration <ref type="bibr" target="#b44">[45]</ref>. Therefore, we will consider exploring the idea of integrating image restoration techniques into our framework to improve its robustness against these artefacts. Analogously, several recent studies have attempted to integrate registration with image super-resolution and reconstruction to reduce error propagation and boost overall accuracy <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>Apart from the information-theoretic approach, there are also registration methods based on heuristic and handcrafted features, as well as learning-based multimodal metrics. For instance, Huizinga et al. <ref type="bibr" target="#b49">[50]</ref> designed a groupwise similarity metric particularly for quantitative MRI based on the eigenvalues of a correlation matrix. Haber and Modersitzki <ref type="bibr" target="#b50">[51]</ref> and Heinrich et al. <ref type="bibr" target="#b51">[52]</ref> developed normalized gradient fields and modality-independent features for pairwise multimodal registration. Simonovsky et al. <ref type="bibr" target="#b52">[53]</ref> proposed learning similarity metrics from registered patch pairs using convolutional neural networks, followed by Sedghi et al. <ref type="bibr" target="#b7">[8]</ref> who extended this notion to images only approximately registered via maximum profile likelihood. These formulations are nevertheless different from ours, and many of them <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> were developed for pairwise registration. Thus, extension of those works for groupwise registration with a detailed comparison could be considered in future work.</p><p>In summary, the proposed framework is generic and versatile in the following three senses: 1) It builds on the mild assumption that the observed  images are interrelated through the common anatomy, given which the intensities of each image are conditionally independent. The assumption is satisfied for most medical image datasets except for certain conditions where anatomical structures are only partially visible among the observed images. Nevertheless, experiments on the RIRE dataset have shown that our model can perform effectively, provided the major structures of the images can be corresponded. In other words, unlike most intensity-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref>, the proposed framework is intended to find the intrinsic anatomical correspondences that underlie image appearances.</p><p>2) The extended framework combines two fundamental tasks in medical image computing, registration and segmentation, the integration of which is expected to enhance their overall performance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. Unlike previous iterative approaches, the proposed learning-based framework is able to achieve combined computing in an end-to-end fashion, exhibiting greater computational efficiency. Though experiments were conducted on the MS-CMR dataset for intra-subject images, the extended framework can also be applied to a more general intersubject setup. In this regard, the formulation is related to multi-atlas segmentation <ref type="bibr" target="#b53">[54]</ref> but allows more flexibility, as expert-annotated segmentation masks are only partially needed for our method. 3) Finally, although this paper only demonstrates the efficacy of the proposed framework in groupwise registration, motion correction and deep combined computing for complex images, we would like to emphasize that the method is not confined to these applications. The framework also shows potential for problems like atlas construction <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>, population analysis <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref>, and tissue property estimation from quantitative MRI <ref type="bibr" target="#b49">[50]</ref>, to name a few.</p><p>We hope to explore these implications in our future studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Roadmap of the proposed framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Misaligned multi-sequence MR images from BrainWeb. (b) Scatter plot of intensity pairs. Each point indicates one intensity pair from the T1 and T2 images at the same spatial location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3:The graphical model and the computational complexity of the methods reviewed and investigated in this paper, where we assume the sample space is the entirety of Ω. Note that random variables are in circles, deterministic parameters are in rounded boxes, observed variables are shaded and boxes indicate replication. Solid arrows indicate generation while dashed ones refer to inference from principal component analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig</head><label></label><figDesc>Fig.4: The graphical representation of the proposed framework, where we omit replication over independent spatial locations of Ω for conciseness. Note that random variables are in circles, deterministic parameters are in rounded boxes, observed variables are shaded and ellipses indicate replication. Solid arrows indicate generation while dashed ones refer to inference procedure from a neural network. Dotted arrows indicate that the corresponding conditional probability distribution is not incorporated in posterior computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>) are computed in the same way as Eq. (20).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: An example of the network architecture for deep combined computing when N = 3. The encoder E, the decoders Ds and Dr, and the bottleneck are composed of residual convolutional blocks. Domain-specific batch normalization layers are indicated with different colours. Cardiac structures, i.e. myocardium, left ventricle (LV) and right ventricle (RV), are rendered as contours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: The registration results of an artificial misaligned RIRE training pair with median gRE before registration. The middle axial slices before and after co-registration using various methods are demonstrated. Each row presents registered images of different modalities from a certain groupwise registration method. One can observe substantial initial misalignment with different images from the first row. Note that only APE and the proposed X -CoReg have produced well-registered CT, T2 rectified and PET images. Readers are referred to the supplementary material or the online version of this paper for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig.7: The influence of the number of common anatomical labels K on the gRE with the RIRE dataset after registration using the proposed X -CoReg algorithm. One can see improved accuracy as K increases until some threshold is attained, e.g. K = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: An example of a first-pass cardiac perfusion sequence at the adenosine induced stress condition. The sequence has 50 temporal frames (T = 1, . . . , 50) and only a subregion around the left ventricle myocardium is visualized. At T = 7, the contrast has arrived at the right ventricle (RV) cavity. At T = 15, it has reached the left ventricle (LV) cavity. At about T = 19, it reaches the myocardium. One can observe the large bulk motion at T = 23, 25, 29.</figDesc><graphic coords="14,53.40,103.84,54.88,54.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>4. 4</head><label>4</label><figDesc>Deep Combined Computing on MS-CMR 4.4.1 Experimental design 1) Objective. This experiment aims to demonstrate the performance of the extended framework on deep combined computing for multi-sequence cardiac MRI from the MS-CMR dataset. 2) Data description. The dataset provides multisequence cardiac MR images for 45 patients. Each patient was scanned by three sequences, namely the LGE (Late Gadolinium Enhanced, j = 1), bSSFP (balanced-Steady State Free Precession, j = 2) and T2-weighted MR (j = 3). The three sequences provide complementary information</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Before motion correction. (b) After motion correction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: The cross-sectional view along the temporal frames on the case (from the MoCo dataset) with median improvement on average using different registration methods: (a) The view before motion correction. (b)The result after motion correction using different registration methods. One can notice that our proposed algorithm better preserves the structural correspondences, as indicated by the red circles.</figDesc><graphic coords="14,324.70,349.50,226.50,96.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Results of an exemplar case from the MS-CMR dataset with median Reg DSC before co-registration. Ground-truth segmentation masks are rendered as contours for None and DGR, while posterior segmentation is displayed for MvMM, DCC+AT, DCC+BS and DCC+All. Each column visualizes the registered images from a certain method. Regions with ambiguous intensity class correspondence are indicated by red circles. Readers are referred to the supplementary material or the online version of this paper for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 11 :</head><label>11</label><figDesc>Fig.11: An example of the registered images and the posterior segmentation produced from the proposed X -CoReg algorithm on the RIRE dataset. The training pair is the same as the one used in Fig.6. The first row displays the registered images from the algorithm. One can see that different modalities exhibit complementary information of the brain anatomical structures, including the lesion visible from the T2-weighed MRI. The second row shows the posterior segmentation overlaid on the registered images. One can find that major anatomical structures are revealed from the posterior segmentation.</figDesc><graphic coords="16,339.35,127.60,65.08,65.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 12: The cross-sectional view along the temporal frames on the worst case (from the MoCo dataset) after alignment using the proposed X -CoReg algorithm: (a) The view before motion correction. One can see significant motion artefacts from the incongruent boundary of the LV myocardium. (b) The result after motion correction using different registration methods.</figDesc><graphic coords="17,64.81,149.52,53.02,84.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Definition of the mathematical symbols used in this paper.</figDesc><table><row><cell cols="2">Symbol Description</cell><cell></cell><cell></cell><cell cols="3">Symbol Description</cell></row><row><cell>N</cell><cell>the number of observed images</cell><cell></cell><cell></cell><cell>j</cell><cell></cell><cell>index of the observed image, j ∈ {1, . . . , N }</cell></row><row><cell>K</cell><cell cols="3">the number of common anatomical labels</cell><cell>k</cell><cell></cell><cell>index of the anatomical label, k ∈ {1, . . . , K}</cell></row><row><cell>Ω</cell><cell cols="3">the common space with discrete Cartesian grid</cell><cell cols="2">|Ω|</cell><cell>the cardinality of the common space</cell></row><row><cell>Z</cell><cell cols="3">categorical model of the common anatomy</cell><cell cols="2">zx</cell><cell>one-hot vector of the common anatomy at x</cell></row><row><cell>Γ</cell><cell cols="3">spatial distribution of the common anatomy</cell><cell cols="2">γx</cell><cell>probability vector of the common anatomy at x</cell></row><row><cell>π</cell><cell cols="3">prior proportions of the common anatomy</cell><cell cols="2">Ωj</cell><cell>the j-th image space</cell></row><row><cell>U</cell><cell cols="3">the observed image group U = {Uj} N j=1</cell><cell cols="2">Uj</cell><cell>the j-th observed image, Uj = (ux,j)x∈Ω j</cell></row><row><cell>J</cell><cell cols="3">indices of the images with given segmentation</cell><cell cols="2">Yj</cell><cell>the predicted segmentation mask of Uj</cell></row><row><cell>Yj</cell><cell cols="2">the available segmentation mask of Uj</cell><cell></cell><cell cols="2">ρj</cell><cell>the predicted probability maps of Uj</cell></row><row><cell>φ</cell><cell cols="2">the set of transformations φ = {φj} N j=1</cell><cell></cell><cell cols="2">φj</cell><cell>the spatial transformation from Ω to Ωj</cell></row><row><cell>U [φ]</cell><cell cols="3">the warped image group U [φ] = {Uj • φj} N j=1</cell><cell cols="2">Ω φ</cell><cell>the overlap region given by φ = {φj} N j=1</cell></row><row><cell>ujω</cell><cell cols="2">abbreviation for Uj(ω), where ω ∈ Ωj</cell><cell></cell><cell cols="2">vx</cell><cell>the template intensity at x computed by PCA</cell></row><row><cell>u φ x</cell><cell cols="2">the resampled intensity vector u φ x = (u</cell><cell>φ j x,j ) N j=1</cell><cell>u</cell><cell>φ j x,j</cell><cell>abbreviation for Uj • φj(x), where x ∈ Ω</cell></row><row><cell>L</cell><cell cols="3">the assumed number of intensity levels</cell><cell cols="2">µj</cell><cell>a certain intensity level of Uj for j = 1, . . . , N</cell></row><row><cell>µ</cell><cell cols="2">the intensity level vector µ = (µj) N j=1</cell><cell></cell><cell cols="2">µ φ j x,j</cell><cell>the corresponding intensity level for u</cell><cell>φ j x,j</cell></row><row><cell>Method</cell><cell>CG</cell><cell>APE</cell><cell></cell><cell></cell><cell>CTE</cell><cell>GMM</cell><cell>-CoReg/MvMM</cell></row><row><cell>Graphical</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Computational</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>complexity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Results on the synthetic BrainWeb dataset. The table presents the mean and standard deviation of the gWIs (in millimeters) before and after groupwise registration using methods. Statistically significant differences in gWI between X -Reg-UN and the others, suggested by a paired t-test (p &lt; 0.05), are indicated with asterisks.</figDesc><table><row><cell>Method</cell><cell>gWI (mm)</cell><cell>p-value</cell></row><row><cell>None</cell><cell>3.866 ± 1.882*</cell><cell>&lt; 10 -10</cell></row><row><cell>X -Reg-UN</cell><cell>0.476 ± 0.373</cell><cell>-</cell></row><row><cell>X -Reg-GT</cell><cell>0.485 ± 0.439</cell><cell>0.74</cell></row><row><cell>CTE</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 :</head><label>4</label><figDesc>Results on the MoCo dataset. The table presents the mean and standard deviation of the myocardium DSC before and after motion correction with different methods and transformation models. Statistically significant differences in DSC (%) between the proposed X -CoReg and the others, suggested by a paired t-test (p &lt; 0.05), are indicated with asterisks. The p-values are reported for the results after registration with FFD.</figDesc><table><row><cell>Method</cell><cell>Translation</cell><cell>Rigid</cell><cell>FFD</cell><cell>p-value</cell></row><row><cell>None</cell><cell>68.6±11.9*</cell><cell>-</cell><cell>-</cell><cell>8.2 × 10 -6</cell></row><row><cell>VI [21]</cell><cell>75.7 ± 7.6*</cell><cell>75.1 ± 7.9*</cell><cell>70.4 ± 7.8*</cell><cell>6.6 × 10 -5</cell></row><row><cell>CG [3, 44]</cell><cell>76.8 ± 8.1</cell><cell>78.8 ± 5.5</cell><cell>79.7 ± 4.7</cell><cell>0.60</cell></row><row><cell>CTE [6]</cell><cell cols="3">75.0±13.2* 74.9±13.8* 74.4±13.6*</cell><cell>5.5 × 10 -3</cell></row><row><cell>X -CoReg</cell><cell>78.4 ± 8.7</cell><cell>79.2 ± 7.2</cell><cell>80.1 ± 6.0</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 :</head><label>5</label><figDesc>Results on the MS-CMR dataset. The table presents the mean and standard deviation of the registration and segmentation DSCs (%) for deep combined computing using different training strategies and another competing method MvMM.</figDesc><table><row><cell>Strategy</cell><cell>Reg DSC</cell><cell></cell><cell></cell><cell cols="2">Seg DSC</cell></row><row><cell></cell><cell></cell><cell>LGE</cell><cell></cell><cell cols="2">bSSFP</cell><cell>T2</cell></row><row><cell>None</cell><cell>72.2 ± 10.1</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>DGR</cell><cell>86.2 ± 4.1</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>MvMM [13]</cell><cell>81.8 ± 8.7</cell><cell cols="2">84.5 ± 9.0</cell><cell cols="2">84.4 ± 8.5</cell><cell>78.3 ± 14.8</cell></row><row><cell>DCC+AT</cell><cell>88.5 ± 3.4</cell><cell cols="2">82.0 ± 3.8</cell><cell cols="2">81.2 ± 4.5</cell><cell>83.4 ± 4.1</cell></row><row><cell>DCC+BS</cell><cell>87.6 ± 4.0</cell><cell cols="2">85.8 ± 3.9</cell><cell cols="2">89.9 ± 2.8</cell><cell>86.5 ± 4.2</cell></row><row><cell>DCC+All</cell><cell>89.5 ± 3.5</cell><cell cols="2">92.6 ± 2.0</cell><cell cols="2">92.4 ± 3.1</cell><cell>92.7 ± 3.4</cell></row><row><cell>None</cell><cell>DGR</cell><cell>MvMM</cell><cell cols="2">DCC+AT</cell><cell>DCC+BS</cell><cell>DCC+All</cell></row><row><cell>LGE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bSSFP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6 :</head><label>6</label><figDesc>Registration results on the Learn2Reg-Task1 validation set using the proposed X -CoReg algorithm. The table presents the average DSCs (%) before and after registration with different transformation models.</figDesc><table><row><cell>Case ID</cell><cell>None</cell><cell>Rigid</cell><cell>Affine</cell><cell>FFD</cell></row><row><cell>0012</cell><cell>24.58</cell><cell>53.71</cell><cell>68.24</cell><cell>79.99</cell></row><row><cell>0014</cell><cell>19.46</cell><cell>66.64</cell><cell>79.42</cell><cell>84.00</cell></row><row><cell>0016</cell><cell>48.49</cell><cell>76.18</cell><cell>85.32</cell><cell>87.16</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was funded by the <rs type="funder">National Natural Science Foundation of China</rs> (grant no. <rs type="grantNumber">61971142</rs>, <rs type="grantNumber">62111530195</rs> and <rs type="grantNumber">62011540404</rs>) and the development fund for Shanghai talents (no. <rs type="grantNumber">2020015</rs>). The authors would like to thank <rs type="person">Fuping Wu</rs>, <rs type="person">Hangqi Zhou</rs>, <rs type="person">Xin Wang</rs>, <rs type="person">Shangqi Gao</rs> and <rs type="person">Yuncheng Zhou</rs> for their valuable comments and proofreading of the manuscript.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_nQUMSQ4">
					<idno type="grant-number">61971142</idno>
				</org>
				<org type="funding" xml:id="_cZWsqpR">
					<idno type="grant-number">62111530195</idno>
				</org>
				<org type="funding" xml:id="_Xkhpebn">
					<idno type="grant-number">62011540404</idno>
				</org>
				<org type="funding" xml:id="_hDDtd29">
					<idno type="grant-number">2020015</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Xinzhe Luo is a Ph.D. student in statistics at the School of Data Science, Fudan University, advised by Prof. Xiahai Zhuang. He obtained his B.S. degree in information and computational science from the School of Data Science, Fudan University in 2019. His research interests include medical image computing, image registration, and multivariate image analysis.</p><p>Xiahai Zhuang is a professor with the School of Data Science, Fudan University. He graduated from the department of computer science, Tianjin University, received the MS degree in computer science from Shanghai Jiao Tong University, and obtained the doctorate degree from University College London. His research interests include medical image analysis, image processing, and computer vision. His works have been nominated twice for the MICCAl Young Scientist Awards <ref type="bibr">(2008,</ref><ref type="bibr">2012)</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Alignment by maximization of mutual information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multimodality image registration by maximization of mutual information</title>
		<author>
			<persName><forename type="first">F</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Collignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Marchal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suetens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="198" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data driven image models through continuous joint alignment</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="250" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simultaneous registration of multiple images: Similarity metrics and efficient optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1221" to="1233" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Similarity metrics for groupwise non-rigid registration</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hajnal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hammers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="544" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Intrasubject multimodal groupwise registration with the conditional template entropy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Polfliet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huizinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Paulides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Niessen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandemeulebroucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="15" to="25" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unifying maximum likelihood approaches in medical image registration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Malandain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Imaging Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="80" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image registration: Maximum likelihood, minimum entropy and deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>O'donnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kapur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101939</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simultaneous segmentation and registration for medical image</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiaohua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="663" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simultaneous segmentation and registration of contrast-enhanced breast mri</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiaohua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biennial International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="126" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A bayesian model for joint segmentation and registration</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Pohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="228" to="239" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Groupwise combined segmentation and registration for atlas construction</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Aljabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Boardman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murgasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Counsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hajnal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="532" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multivariate mixture model for myocardial segmentation combining multi-source images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2933" to="2946" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unified segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ashburner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="839" to="851" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Groupwise registration of multimodal images by an efficient joint entropy minimization scheme</title>
		<author>
			<persName><forename type="first">Ž</forename><surname>Spiclin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Likar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pernus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2546" to="2558" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Registering a multisensor ensemble of images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1236" to="1247" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A unified statistical and information theoretic framework for multi-modal image registration</title>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biennial International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="366" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning from one example through shared densities on transforms</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Matsakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="464" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Groupwise elastic registration by a new sparsity-promoting metric: Application to the alignment of cardiac magnetic resonance perfusion images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cordero-Grande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Merino-Caviedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aja-Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alberola-Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2638" to="2650" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Registration of dynamic contrast-enhanced mri using a progressive principal component registration (ppcr)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Melbourne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hawkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics in Medicine &amp; Biology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page">5147</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonrigid registration of dynamic medical imaging data using nd+ t b-splines and a groupwise optimization approach</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schaap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Van Walsum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Niessen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="238" to="249" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal image set registration and atlas formation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lorenzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prastawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bullitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="440" to="451" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative diffeomorphic modelling of large mri data sets for probabilistic template construction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blaiotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ashburner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="117" to="134" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mutual-information-based registration of medical images: a survey</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Maintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="986" to="1004" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of Information Theory</title>
		<imprint>
			<publisher>Ltd</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An information theoretic approach for non-rigid image registration using voxel class probabilities</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agostino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="431" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimization of mutual information for multiresolution image registration</title>
		<author>
			<persName><forename type="first">P</forename><surname>Thévenaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2083" to="2099" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Probability: theory and examples</title>
		<author>
			<persName><forename type="first">R</forename><surname>Durrett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Implicit referencebased group-wise image registration and its application to structural and functional mri</title>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1341" to="1351" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Consistent groupwise non-rigid registration for atlas construction</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hajnal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2004 2nd IEEE International Symposium on Biomedical Imaging: Nano to Macro</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="908" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nonrigid registration using free-form deformations: application to breast mr images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Sonoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hawkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="712" to="721" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weakly-supervised convolutional neural networks for multimodal image registration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Modat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ghavami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bonmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Emberton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Domain-specific batch normalization for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">W.-G</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7354" to="7362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hemis: Hetero-modal image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Guizard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Design and construction of a realistic digital brain phantom</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Zijdenbos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kollokian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Sled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Kabani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="463" to="468" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Comparison and evaluation of retrospective intermodality brain image registration techniques</title>
		<author>
			<persName><forename type="first">J</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Fitzpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Dawant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Maurer</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Maciunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barillot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lemoine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Collignon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer assisted tomography</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="554" to="568" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An open benchmark challenge for motion correction of myocardial perfusion mri</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pontré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dibella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kulaseharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Likhite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Noorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tustison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wollny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1315" to="1326" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cardiac segmentation on late gadolinium enhancement mri: a benchmark study from multi-sequence cardiac mr segmentation challenge</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lekadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vesal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ravikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">102528</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learn2reg: comprehensive multitask medical image registration challenge, dataset and evaluation in the era of deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Siebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kuckertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heldmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04489</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-modal volume registration using joint intensity distributions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Leventon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="1057" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Free-form b-spline deformation model for groupwise registration</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Balci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Shenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Why does mutual-information work for image registration? a deterministic explanation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Tagare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1286" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deformation corrected compressed sensing (dc-cs): a novel framework for accelerated dynamic mri</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Lingala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dibella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Nonrigid groupwise registration for motion estimation and compensation in compressed sensing reconstruction of breath-hold cardiac cine mri</title>
		<author>
			<persName><forename type="first">J</forename><surname>Royuela-Del Val</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cordero-Grande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Simmross-Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martín-Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alberola-L Ópez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic resonance in medicine</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1525" to="1536" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Joint motion correction and super resolution for cardiac segmentation via latent optimisation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Savioli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>O'regan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Variational multi-task mri reconstruction: Joint reconstruction, registration and super-resolution</title>
		<author>
			<persName><forename type="first">V</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aviles-Rivero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Debroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Le</forename><surname>Guyader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-B</forename><surname>Sch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">101941</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pca-based groupwise image registration for quantitative mri</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huizinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Poot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Guyader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klaassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Coolen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Kranenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Geuns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Uitterdijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Polfliet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandemeulebroucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="65" to="78" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Intensity gradient based registration and fusion of multi-modal images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Modersitzki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="726" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mind: Modality independent neighbourhood descriptor for multi-modal deformable registration</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bhushan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Gleeson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1423" to="1435" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A deep metric for multimodal registration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gutiérrez-Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mateus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-atlas segmentation of biomedical images: a survey</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="205" to="219" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unbiased diffeomorphic atlas construction for computational anatomy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jomier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
		<idno>pp. S151- S160</idno>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Synthesizing average 3d anatomical shapes</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Vannier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="146" to="158" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Image-driven population analysis through mixture modeling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Balci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Shenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1473" to="1487" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unsupervised segmentation, clustering, and groupwise registration of heterogeneous populations of brain mr images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ribbens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suetens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="224" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
