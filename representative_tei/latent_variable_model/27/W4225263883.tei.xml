<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A BAYESIAN PERMUTATION TRAINING DEEP REPRESENTATION LEARNING METHOD FOR SPEECH ENHANCEMENT WITH VARIATIONAL AUTOENCODER</title>
				<funder ref="#_SXq23Zc">
					<orgName type="full">Innovation Fund Denmark</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-01-24">24 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Audio Analysis Lab, CREATE</orgName>
								<orgName type="institution">Aalborg University</orgName>
								<address>
									<settlement>Aalbory</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jesper</forename><surname>Lisby Højvang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Audio Analysis Lab, CREATE</orgName>
								<orgName type="institution">Aalborg University</orgName>
								<address>
									<settlement>Aalbory</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Morten</forename><forename type="middle">Højfeldt</forename><surname>Rasmussen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Audio Analysis Lab, CREATE</orgName>
								<orgName type="institution">Aalborg University</orgName>
								<address>
									<settlement>Aalbory</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Mads</roleName><forename type="first">Graesbøll</forename><surname>Christensen</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Capturi A/S</orgName>
								<address>
									<settlement>Aarhus</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A BAYESIAN PERMUTATION TRAINING DEEP REPRESENTATION LEARNING METHOD FOR SPEECH ENHANCEMENT WITH VARIATIONAL AUTOENCODER</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-24">24 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2201.09875v1[eess.AS]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep representation learning</term>
					<term>speech enhancement</term>
					<term>Bayesian permutation training</term>
					<term>variational autoencoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, variational autoencoder (VAE), a deep representation learning (DRL) model, has been used to perform speech enhancement (SE). However, to the best of our knowledge, current VAEbased SE methods only apply VAE to model speech signal, while noise is modeled using the traditional non-negative matrix factorization (NMF) model. One of the most important reasons for using NMF is that these VAE-based methods cannot disentangle the speech and noise latent variables from the observed signal. Based on Bayesian theory, this paper derives a novel variational lower bound for VAE, which ensures that VAE can be trained in supervision, and can disentangle speech and noise latent variables from the observed signal. This means that the proposed method can apply the VAE to model both speech and noise signals, which is totally different from the previous VAE-based SE works. More specifically, the proposed DRL method can learn to impose speech and noise signal priors to different sets of latent variables for SE. The experimental results show that the proposed method can not only disentangle speech and noise latent variables from the observed signal, but also obtain a higher scale-invariant signal-to-distortion ratio and speech quality score than the similar deep neural network-based (DNN) SE method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In real-world environments, speech signals are often distorted due to the presence of background noise. To reduce the effects of noise, speech enhancement (SE) techniques have been developed <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref> to improve the quality and intelligibility of an observed signal.</p><p>Currently, many single-channel SE algorithms have been proposed, which include some unsupervised algorithms <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4]</ref> and supervised algorithms <ref type="bibr" target="#b5">[5,</ref><ref type="bibr">6]</ref>. However, these methods usually apply linear processes to model complex high-dimensional signal, which is not always reasonable in practical applications <ref type="bibr" target="#b7">[7]</ref>. Thus, non-linear deep neural network (DNN) models have been developed. As shown in <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b7">[7]</ref><ref type="bibr" target="#b8">[8]</ref><ref type="bibr" target="#b9">[9]</ref><ref type="bibr" target="#b10">[10]</ref>, DNN-based methods can achieve better SE performance than traditional linear models. However, their generalization ability is not often satisfactory for the unseen noise conditions <ref type="bibr" target="#b2">[2]</ref>.</p><p>Recently, deep probabilistic generative models have been investigated to improve the DNN's generalization ability for SE, such as generative adversarial networks (GAN) <ref type="bibr" target="#b11">[11]</ref> and the variational autoencoder (VAE) <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13]</ref>. VAE can learn the probability distribution of complex data and perform efficient approximate posterior inference, so VAE-based SE algorithms have been proposed <ref type="bibr" target="#b13">[13]</ref><ref type="bibr" target="#b14">[14]</ref><ref type="bibr" target="#b15">[15]</ref>.</p><p>However, the VAE of these methods is trained in an unsupervised manner on speech only, and the noise is modeled by an NMF model because these methods cannot disentangle the speech and noise latent variables from the observed signal. This means that these algorithms are not robust <ref type="bibr" target="#b16">[16]</ref>, and their SE performance is limited compared to DNN-based supervised methods <ref type="bibr" target="#b13">[13]</ref>. To mitigate this problem, supervised VAE-based SE methods have been proposed. In <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17]</ref>, a supervised classifier <ref type="bibr" target="#b16">[16]</ref> and a supervised noise-aware training strategy <ref type="bibr" target="#b17">[17]</ref> are introduced to the training of speech VAE. The purpose is to obtain a more robust speech latent variable from the observed signal. However, the noise is still modeled by a linear NMF model because it is a difficult task to disentangle the speech and noise latent variables from the observed signal <ref type="bibr" target="#b17">[17]</ref>.</p><p>Learning interpretable latent representation is a challenging but very useful task because it can explain how different factors influence the speech signal, which is important in speech-related applications <ref type="bibr" target="#b18">[18]</ref>. In <ref type="bibr" target="#b18">[18]</ref>, a latent space arithmetic operation was derived to modify the speech attributes (phonetic content and speaker identity). <ref type="bibr" target="#b19">[19]</ref> proposed an unsupervised method to distinguish different latent variables and generate new latent variables for the ASR application. <ref type="bibr" target="#b20">[20]</ref> applied VAE to learn the sequence-dependent and sequence-independent representations. However, interpretable latent representation is rarely considered in current SE algorithms <ref type="bibr" target="#b7">[7]</ref><ref type="bibr" target="#b8">[8]</ref><ref type="bibr" target="#b9">[9]</ref><ref type="bibr" target="#b10">[10]</ref>.</p><p>Inspired by previous work, in this paper, we propose a Bayesian permutation training method for SE. The proposed method can disentangle the latent speech and noise variables from the observed signal in a supervised manner and conduct the mapping between latent variables and targets, even though this is a difficult task <ref type="bibr" target="#b17">[17]</ref>. We hypothesize that disentangling latent variables can improve the performance of the supervised DNN-based SE method. To achieve this, a clean speech VAE (C-VAE) and a noise VAE (N-VAE) are separately pre-trained without supervision. After that, based on Bayesian theory and our derived variational lower bound, we use the two pretrained VAEs to train a noisy VAE (NS-VAE) in a supervised manner. The trained NS-VAE can learn the latent representations of the speech and noise signal. When we conduct SE, the trained NS-VAE is first used to predict the latent variables of the speech and noise signal. Then, the two latent variables are independently used as the decoder input of the C-VAE and N-VAE to estimate the corresponding speech and noise. Finally, the enhanced signal can be acquired by direct speech waveform reconstruction or with post-filtering methods. Compared to previous VAE-based SE methods <ref type="bibr" target="#b13">[13]</ref><ref type="bibr" target="#b14">[14]</ref><ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref> and interpretable latent representation learning methods <ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref>, the proposed method derives a novel variational lower bound to ensure that supervision training can be used for VAE, and VAE can disentangle different latent variables to model noise for SE. The derived supervised lower bound is very different from previous VAE-based methods <ref type="bibr" target="#b12">[12]</ref><ref type="bibr" target="#b13">[13]</ref><ref type="bibr" target="#b14">[14]</ref><ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref> that are trained on an unsupervised variational lower bound, which increases the robustness of different learned latent  variables <ref type="bibr" target="#b17">[17]</ref>. Moreover, our learned latent variables are attributed to different types of signal, so each single latent variable that is generated by NS-VAE can be used to generate the corresponding speech or noise signal, and their combination can generate noisy speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM DESCRIPTION</head><p>In this work, aim to perform SE in an additive noisy environment. Thus, the signal model can be written as</p><formula xml:id="formula_0">y(t) = x(t) + d(t),<label>(1)</label></formula><p>where  Here, we assume that the latent variables zx and z d are independent. Additionally, x and d are drawn from the speech prior distribution q(x|zx) and the noise prior distribution q(d|z d ), respectively. The whole generative process is illustrated in Fig. <ref type="figure" target="#fig_1">1(a)</ref>.</p><formula xml:id="formula_1">= [y1, • • • , yn, • • • , yN ] and yn = [Y (1, n), • • • , Y (f, n), • • • , Y (F, n)]</formula><p>In VAE, since exact posterior inference is intractable, we propose that zx and z d can be estimated from speech and noise posterior distributions p(zx|x) and p(z d |d), respectively, or they can also be estimated from the noisy speech posterior distributions p(zx|y) and p(z d |y). Here, we assume p(zx, z d |y) = p(zx|y)p(z d |y), which ensures that noise can be modeled by non-linear VAE rather than NMF. Based on these assumptions and our derivation in section 3, speech and noise latent variables can be obtained from the observed signal. The whole recognition process is shown in Fig. <ref type="figure" target="#fig_1">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b).</head><p>To sum up, we intend to first estimate the latent variable distributions of the speech p(zx|y) and the noise p(z d |y) from the observed signal to acquire latent variables zx and z d , respectively. After that, we use the estimated latent variables as the input of the decoder of C-VAE and N-VAE to obtain the probability distribution of q(x|zx) and q(d|z d ) for SE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SE WITH BAYESIAN PERMUTATION TRAINING</head><p>3.1. Variational autoencoder with multiple latent variables VAE <ref type="bibr" target="#b12">[12]</ref> defines a probabilistic generative process between observed signal and its latent variables and provides a principled method to jointly learn latent variables, generative and recognition models, which is achieved by maximizing variational lower bound using stochastic gradient descent (SGD) algorithm. This optimizing process <ref type="bibr" target="#b12">[12]</ref> is equal to minimize Kullback-Leibler (KL) divergence (DKL) between real joint probability distribution p(y, zx, z d ) and its estimation q(y, zx, z d ). This process can be written as follows:</p><formula xml:id="formula_2">D KL (p(y, zx, z d )||q(y, zx, z d )) = E y∼p(y) [log p(y)] + E y∼p(y) [D KL (p(zx, z d |y))||q(y, zx, z d ))].<label>(2)</label></formula><p>In ( <ref type="formula" target="#formula_2">2</ref>), the term E y∼p(y) [log p(y)] is a constant, so minimizing their KL divergence is equal to minimizing L(θy, ϕy, θx, ϕx, θ d , ϕ d ; y)</p><formula xml:id="formula_3">= E y∼p(y) [D KL (p(zx, z d |y))||q(y, zx, z d ))] = E y∼p(y) [D KL (p(zx, z d |y))||q(zx, z d ))] -E y∼p(y) E z d ,zx ∼p(z d ,zx |y) [log q(y|zx, z d )] ,<label>(3)</label></formula><p>where θy, ϕy, θx, ϕx, θ d , ϕ d are the parameters that are used to conduct the related probability estimation. The details will be presented later. Here, -L can be seen as the VAE variational lower bound with multiple latent variables (E y∼p(y) [log q(y)] ≥ -L) <ref type="bibr">[</ref> </p><formula xml:id="formula_4">+ E y∼p(y) [D KL (p(z d |y)||q(z d ))] -E y∼p(y) E z d ,zx ∼p(z d ,zx |y) [log q(y|zx, z d )] ,<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">DRL with Bayesian permutation training</head><p>To estimate the speech and noise latent variables from the observed signal using ( <ref type="formula" target="#formula_4">4</ref>), we propose a Bayesian permutation training process between NS-VAE, C-VAE, and N-VAE. First, the C-VAE and N-VAE are separately pre-trained using the general VAE training method <ref type="bibr" target="#b12">[12]</ref> without supervision. The purpose is to acquire the posterior estimates p(zx|x) and p(z d |d). Then, the NS-VAE is trained with the supervision of C-VAE and N-VAE. In (4), the calculation of the first and second term is similar, so we will only use the first term to show the Bayesian permutation process. To achieve supervision learning, we add an attention mechanism (p(zx|x)/p(zx|x)) for the calculation of first term in (4). Thus, its calculation can be written as (5)</p><formula xml:id="formula_5">E y∼p(y) [D KL (p(zx|y)||q(zx))] = E y∼p(y)</formula><p>p(zx|y) log p(zx|y)p(zx|x) q(zx)p(zx|x) dzx</p><formula xml:id="formula_6">= E y∼p(y),x∼p(x) [D KL (p(zx|y)||p(zx|x))] + E y∼p(y) E zx ∼p(zx |y) [log p(zx|x) q(zx) ] ,<label>(5)</label></formula><p>L(θy, ϕy, θx, ϕx, θ d , ϕ d ; y)</p><formula xml:id="formula_7">= E y∼p(y),x∼p(x) {D KL (p(zx|y)||p(zx|x)) + E zx∼p(zx |y) [log p(zx|x) q(zx) ]} + E y∼p(y),d∼p(d) {D KL (p(z d |y)||p(z d |d)) + E z d ∼p(z d |y) [log p(z d |d) q(z d ) ]} -E y∼p(y) E z d ,zx ∼p(z d ,zx|y) [log q(y|zx, z d )] .<label>(6)</label></formula><p>In ( <ref type="formula" target="#formula_6">5</ref>), we introduce posterior p(zx|x) estimated from C-VAE to conduct supervised latent variable learning. The purpose is to obtain speech latent variables from observed signal. Finally, substituting (5) into (4), the final loss function can be written as <ref type="bibr">(6)</ref>. In (6), we can find KL divergence constraints for speech and noise latent variables, which ensures that we can estimate the desired posterior distributions from noisy signal in a supervision way. This is also why our method can disentangle latent variables, and the noise can be estimated by nonlinear VAE rather than linear NMF, which is </p><formula xml:id="formula_8">+ Lc(θx, ϕx; x) + Ln(θ d , ϕ d ; d),<label>(7)</label></formula><p>where Lc(θx, ϕx; x) and Ln(θ d , ϕ d ; d) are the general VAE loss function for speech and noise, which can be written as</p><formula xml:id="formula_9">Lc(θx, ϕx; x) = E x∼p(x) {D KL (p(zx|x)||q(zx)) -E zx ∼p(zx |x) [log q(x|zx)]},<label>(8)</label></formula><formula xml:id="formula_10">Ln(θ d , ϕ d ; d) = E d∼p(d) {D KL (p(z d |d)||q(z d )) -E z d ∼p(z d |d) [log q(d|z d )]}.<label>(9)</label></formula><p>In ( <ref type="formula" target="#formula_8">7</ref>), it can be found that the NS-VAE's training also includes the training of C-VAE and N-VAE, which improves NS-VAE's ability to disentangle latent variables. Minimizing L total is our final target. Fig. <ref type="figure" target="#fig_2">2</ref> shows the proposed framework. To summarize, the proposed method includes a training and an enhancement stage. The whole training process can be described as follows: first, C-VAE and N-VAE are separately pre-trained without supervision using ( <ref type="formula" target="#formula_9">8</ref>) and <ref type="bibr" target="#b9">(9)</ref>. Then, the LPS features of speech, noise and observed signal are separately used as the encoder input of C-VAE, N-VAE, and NS-VAE to estimate posterior distributions p(zx|y), p(z d |y), p(zx|x), p(z d |d) and prior distribution q(y|zx, z d ), q(x|zx), q(d|z d ). Finally, ( <ref type="formula" target="#formula_8">7</ref>) is used as a loss function to perform related parameters update with the Adam algorithm <ref type="bibr" target="#b21">[21]</ref>. The training is completed when the neural networks converge. In the online SE stage, we assume that the zx sampled from p(zx|x) is approximately equal to the sample zx sampled from p(zx|y). Therefore, we can separately use the NS-VAE encoder's two outputs as input of C-VAE and N-VAE to obtain the prior distributions q(x|zx) and q(d|z d ). After that, using the reparameterization trick and Monte Carlo estimate (MCE) <ref type="bibr" target="#b12">[12]</ref>, the speech and noise signal can be obtained. The enhanced speech is acquired by direct waveform reconstruction or post-filtering methods. This enhanced process is shown in Fig. <ref type="figure">3 (a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Calculation of loss function</head><p>In <ref type="bibr" target="#b7">(7)</ref>, the related posterior and prior distributions need to be determined, and q(zx) and q(z d ) need to be predefined for the calculation. Here, for the simplicity of calculation, we assume that all the posterior and prior distributions are multivariate normal distributions with diagonal covariance <ref type="bibr" target="#b12">[12]</ref>, which is similar to the previous VAEbased SE methods <ref type="bibr" target="#b13">[13]</ref><ref type="bibr" target="#b14">[14]</ref><ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref>. For the NS-VAE, we have </p><formula xml:id="formula_11">p(zx|x) = N zx; µ θx (x), σ 2 θx (x)I q(x|zx) = N x; µϕ x (zx), σ 2 ϕx (zx)I ,<label>(11)</label></formula><p>where µ θx (x), σ 2 θx (x) are obtained by C-VAE's encoder G θx (x) with parameter θx, and µϕ x (zx), σ 2 ϕx (zx) can be estimated by C-VAE's decoder Gϕ x (zx) with parameter ϕx. q(z d ) and q(zx) are pre-defined as a centered isotropic multivariate Gaussian q(zx) = N (zx; 0, I) and q(z d ) = N (z d ; 0, I). Finally, when all the distributions are determined, we can apply loss function <ref type="bibr" target="#b7">(7)</ref> and the Adam algorithm to perform related parameters update for SE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENT AND RESULT ANALYSIS</head><p>In this section, the proposed algorithm is evaluated. At first, we will use an example to verify that the proposed method can disentangle different latent variables from the observed signal. After that, an experiment will show the SE performance of our method.</p><p>Dataset: In this work, we use the TIMIT database <ref type="bibr" target="#b22">[22]</ref>, 100 environmental noises <ref type="bibr" target="#b23">[23]</ref>, DEMAND database <ref type="bibr" target="#b24">[24]</ref> and NOISEX-92 database <ref type="bibr" target="#b25">[25]</ref> to evaluate the performance of the proposed algorithm. In the training stage, the Babble, F16 noise from the NOISEX-92 database <ref type="bibr" target="#b25">[25]</ref> and 90 environmental noise (N1-N90) <ref type="bibr" target="#b23">[23]</ref> are used to conduct experiments. All 4620 utterances from the TIMIT database are corrupted by 92 types of noise at four different signal-to-noise ratio (SNR) levels, i.e., -5, 0, 5, and 10 dB. The utterances are randomly selected from these corrupted utterances, and they are connected to a 12-hour noisy speech database. Meanwhile, the corresponding speech and noise databases are also obtained. In the test stage, 200 utterances from the TIMIT test set, including 1680 utterances, are randomly chosen to build the test database. 13 types of noise (office <ref type="bibr" target="#b24">[24]</ref>, factory <ref type="bibr" target="#b25">[25]</ref>, and 10 unseen environmental noise (N91-N100) <ref type="bibr" target="#b23">[23]</ref>) are randomly added to the 200 utterances at four SNR levels (i.e., -5, 0, 5, and 10 dB) to conduct experiment. In our experiments, all the signals are down-sampled to 16 kHz. The frame length is 512 samples with a frame shift of 256 samples. Baseline: To evaluate the performance of the proposed method, we use a supervised SE model as a reference method (referred to Y-CNN) <ref type="bibr" target="#b26">[26]</ref>. This is similar to the proposed method and can perform SE by direct waveform reconstruction <ref type="bibr" target="#b7">[7]</ref> or estimated mask <ref type="bibr" target="#b8">[8]</ref>. For a fair comparison, we use a convolutional neural network (CNN) to replace the original DNN <ref type="bibr" target="#b26">[26]</ref> to improve its performance. Fig. <ref type="figure">3</ref> shows the framework comparison of enhancement. Y-CNN has the same encoder and decoder as the proposed model. The only difference between Y-CNN and our method is the training loss function. The loss function of the proposed method applies deep representation learning (DRL) and reasonable assumptions to disentangle latent variables, which is not achieved by Y-CNN <ref type="bibr" target="#b26">[26]</ref>.</p><p>Experimental setups: There are three VAEs in our proposed method. The C-VAE and N-VAE have the same structure. 1D CNN which is widely used in SE <ref type="bibr" target="#b9">[9]</ref> is adopted in the experiment. C-VAE' encoder includes four hidden 1D convolutional layers. The number of channels in each layer is 32, 64, 128, and 256. The size of each convolving kernel is 3. The two output layers of the encoders are fully connected layers with 128 nodes. By using the reparameterization trick, the decoders' input size can also be set as 128. The decoder consists of four hidden 1D convolutional layers (the channel number of each layer is 256, 128, 64, and 32 with 3 kernel) and two fully connected output layers with 257 nodes. Moreover, the activation functions for the hidden and output layer are ReLU and linear activation function, respectively. For NS-VAE, its encoder also includes four 1D convolutional layers with ReLU as the activation function. The other parameter setting is the same as C-VAE. Additionally, its encoder has four output layers with 128 nodes and a linear activation function. The input size of the NS-VAE decoder is 256, which includes the latent speech and the noise variables. The decoder structure of NS-VAE's decoder is the same as that of C-VAE (Y-CNN's encoder and decoders only have one output layer with 128 and 257 nodes, respectively, because it does not disentangle latent variables. The other settings are the same as that of NS-VAE). In the training stage, all networks are trained by the Adam algorithm with a 128 mini-batch size. The learning rate is 0.001.</p><p>Experimental results: Firstly, we will verify the ability of the proposed method to effectively disentangle the speech and noise latent variables from observed signals. Based on our assumption, the observed signal y is determined by zx and z d . Thus, if we use different zx or z d as the input of NS-VAE's decoder, we can obtain the different observed signal. zx and z d can be acquired by different NS-VAE encoders. Fig. <ref type="figure">4</ref> shows the experimental result. In this example, we first disentangle the latent variables of the observed signal (babble noise with 5dB). Then, we keep the speech latent variable zx and replace the noise latent variable with another noise latent variable (f16 noise with 10dB). Finally, the new combination of latent variables is used as the input of NS-VAE's decoder to acquire the modified signal. Fig. <ref type="figure">4</ref>(b) shows the modified signal. Comparing Fig. <ref type="figure">4</ref>(a), (b), (c), and (d), it can be found that the modified signal successfully removes the babble noise character, and the original noise character is replaced by f16 noise character. (The modified signal has the constant noise around 3000 and 4000 Hz as shown in the black circle area, which is the same as the target signal.) Further- more, the modified signal also preserves the original speech character. Therefore, this experiment indicates that the proposed method can effectively disentangle different latent variables.</p><p>In the second experiment, all algorithms are evaluated by the scale-invariant signal-to-distortion ratio (SI-SDR) in decibel (dB) <ref type="bibr" target="#b27">[27]</ref>, short-time objective intelligibility (STOI) <ref type="bibr" target="#b28">[28]</ref>, and perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b29">[29]</ref>. The enhanced speech is obtained by direct waveform reconstruction <ref type="bibr" target="#b7">[7]</ref> or soft time-frequency mask estimation <ref type="bibr" target="#b26">[26]</ref>. We use Y-M and Y-L to represent that the enhanced speech is acquired by mask estimation and direct waveform reconstruction using Y-CNN, respectively. Similarly, PVAE-L and PVAE-M denote that the enhanced speech is obtained by the proposed method using direct waveform reconstruction and mask estimation, respectively. Table . 1 and 2 show the PESQ, SI-SDR, and STOI comparisons with a 95% confidence interval. The results verify that our method can learn latent speech and noise variables from observed signals because PVAE-L achieves better PESQ and SI-SDR performance than Y-L. This means that C-VAE's decoder can recognize speech latent variables that are disentangled by NS-VAE. Moreover, PVAE-M significantly achieves better PESQ and SI-SDR performance than Y-M, which shows that our method can estimate a more accurate mask for SE. This result also illustrates that our approach has better noise estimation performance than the reference method. Additionally, the results also show that Y-CNN's performance can be improved by the proposed loss function. Table . 2 shows that the STOI score is competitive between the proposed and the reference algorithms. We think that the STOI score of the proposed method can be further improved by improving PVAE's disentangling performance <ref type="bibr" target="#b30">[30]</ref>. Overall, PVAE-M achieves the best SE performance across the three evaluation criteria. Here, we only use a basic neural network to verify our algorithm. Its performance can be further improved by using more advanced neural networks and other speech features <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, a supervised Bayesian permutation training DRL method is proposed to disentangle latent speech and noise variables from the observed signal for SE. The proposed method is based on VAE and Bayesian theory. The experimental results show that our method cannot only successfully disentangle different latent variables but also obtain higher SI-SDR and PESQ scores than the state-of-the-art reference method. Moreover, the results also illustrate that the SE performance of the reference method can be improved by introducing the proposed DRL algorithm. In future work, some other strategies can be considered to further improve the disentangling performance of latent variables. In addition, the proposed method can also be applied in other speech generative tasks, e.g., voice conversion and ASR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Fig.1: Graphical illustration of the proposed model. variables<ref type="bibr" target="#b17">[17]</ref>. Moreover, our learned latent variables are attributed to different types of signal, so each single latent variable that is generated by NS-VAE can be used to generate the corresponding speech or noise signal, and their combination can generate noisy speech.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Model of Bayesian permutation training for SE. different from the previous VAE-based SE methods [13-17]. Moreover, in (6), -L can be used as a novel variational lower bound to perform supervised VAE training in other VAE-related applications. To better minimize (6), we introduce C-VAE and N-VAE to conduct joint training, which forms a Bayesian permutation training process between the three VAEs. Finally, the NS-VAE's training loss is L total = L(θy, ϕy, θx, ϕx, θ d , ϕ d ; y)</figDesc><graphic coords="3,334.16,179.88,102.05,51.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 : 1 Fig. 4 :</head><label>314</label><figDesc>Fig. 3: Enhancement framework comparison.</figDesc><graphic coords="3,449.50,179.88,102.05,51.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>F ] and n ∈ [1, N ] denote the frequency bins and time frame indices, respectively. Collecting F frequency bins and N time frames, we can obtain the LPS dataset YN , XN and DN with N samples, where YN</figDesc><table><row><cell>y(t), x(t), and d(t) represent the observed, speech, and</cell></row><row><cell>noise signal, respectively, and t is the time index. Log-power</cell></row><row><cell>spectrum (LPS) is suitable for direct signal estimation [7], so we</cell></row><row><cell>use it as a feature for SE. The LPS of y(t), x(t), and d(t) is</cell></row><row><cell>written as Y (f, n), X(f, n), and D(f, n), respectively. Here,</cell></row><row><cell>f ∈ [1,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>T , xn and dn are defined similarly to yn. XN and DN are defined similarly to YN . For simplicity, we use y, x, and d to represent a sample in dataset YN , XN and DN , respectively. In the proposed VAE model, we assume that y is generated from a random process involving the speech latent variables zx and the noise latent variables z d , where the observed speech conditional prior distribution can be written as q(y|z d , zx). The dimensions of vectors zx and z d are Lx and L d , respectively. The dataset of zx and z</figDesc><table /><note><p>d is written as ZxN and Z dN with N samples.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>12]. Minimizing L is equal to maximize this variational lower bound. Based on our assumptions in section 2 (zx and z d are independent and p(zx, z d |y) = p(zx|y)p(z d |y)), (3) can be further written as</figDesc><table /><note><p>L(θy, ϕy, θx, ϕx, θ d , ϕ d ; y) = E y∼p(y) [D KL (p(zx|y)||q(zx))]</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Average PESQ and SI-SDR comparison of different methods</figDesc><table><row><cell>SNR</cell><cell></cell><cell></cell><cell>SI-SDR (dB)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PESQ</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Noisy</cell><cell>Y-L</cell><cell>PVAE-L</cell><cell>Y-M</cell><cell>PVAE-M</cell><cell>Noisy</cell><cell>Y-L</cell><cell>PVAE-L</cell><cell>Y-M</cell><cell>PVAE-M</cell></row><row><cell>-5</cell><cell>-5.67(± 0.22)</cell><cell>1.25(± 0.67)</cell><cell>2.84(± 0.72)</cell><cell>2.04(± 0.68)</cell><cell>4.01(± 0.88)</cell><cell>1.43(± 0.02)</cell><cell>1.59(± 0.03)</cell><cell>1.87(± 0.03)</cell><cell>1.68(± 0.03)</cell><cell>1.86(± 0.03)</cell></row><row><cell>0</cell><cell>-0.69(± 0.22)</cell><cell>4.52(± 0.47)</cell><cell>6.32(± 0.48)</cell><cell>7.40 (± 0.68)</cell><cell>8.59(± 0.75)</cell><cell>1.78(± 0.02)</cell><cell>2.02(± 0.02)</cell><cell>2.24(± 0.03)</cell><cell>2.11(± 0.03)</cell><cell>2.27(± 0.03)</cell></row><row><cell>5</cell><cell>4.30(± 0.23)</cell><cell>6.76(± 0.29)</cell><cell>8.67(± 0.31)</cell><cell>11.74(± 0.62)</cell><cell>12.33(± 0.61)</cell><cell>2.13(± 0.02)</cell><cell>2.43(± 0.02)</cell><cell>2.57(± 0.02)</cell><cell>2.53(± 0.03)</cell><cell>2.63(± 0.02)</cell></row><row><cell>10</cell><cell>7.30(± 0.23)</cell><cell>8.05(± 0.18)</cell><cell>10.03(± 0.23)</cell><cell>15.17(± 0.54)</cell><cell>15.41(± 0.50)</cell><cell>2.46(± 0.01)</cell><cell>2.76(± 0.02)</cell><cell>2.80(± 0.02)</cell><cell>2.86(± 0.02)</cell><cell>2.91(± 0.02)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Average STOI comparison of different methods</figDesc><table><row><cell>SNR</cell><cell>Noisy</cell><cell>Y-L</cell><cell>PVAE-L</cell><cell>Y-M</cell><cell>PVAE-M</cell></row><row><cell>-5</cell><cell>57.62(± 1.31)</cell><cell>57.63(± 1.67)</cell><cell>60.00(± 1.33)</cell><cell>59.72(± 1.70)</cell><cell>60.32(± 1.40)</cell></row><row><cell>0</cell><cell>70.02(± 1.24)</cell><cell>69.80(± 1.48)</cell><cell>70.68(± 1.12)</cell><cell>72.02(± 1.43)</cell><cell>71.75(± 1.19)</cell></row><row><cell>5</cell><cell>80.20(± 0.90)</cell><cell>79.20(± 1.18)</cell><cell>79.87(± 0.87)</cell><cell>81.96(± 1.02)</cell><cell>80.78(± 0.91)</cell></row><row><cell>10</cell><cell>86.32(± 0.50)</cell><cell>85.60 (± 0.72)</cell><cell>84.32(± 0.54)</cell><cell>88.80(± 0.63)</cell><cell>87.24(± 0.58)</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>This work was partly supported by <rs type="funder">Innovation Fund Denmark</rs> (Grant No.<rs type="grantNumber">9065-00046</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_SXq23Zc">
					<idno type="grant-number">9065-00046</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
		<title level="m">Speech enhancement: theory and practice</title>
		<imprint>
			<publisher>CRC press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, and Lang. Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1702" to="1726" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unbiased MMSE-based noise power estimation with low complexity and low tracking delay</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, and Lang. Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1383" to="1393" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Noise reduction with optimal variable span linear filters</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, and Lang. Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="631" to="644" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An NMF-HMM speech enhancement method based on kullback-leibler divergence</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Højvang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2667" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online parametric NMF for speech enhancement</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kavalekalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boldt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Signal Processing Conf</title>
		<meeting>European Signal essing Conf</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2320" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A regression approach to speech enhancement based on deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, and Lang. Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On training targets for supervised speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, and Lang. Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, and Lang. Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Icassp 2021 deep noise suppression challenge: Decoupling magnitude and phase optimization with a two-stage deep network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04198</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A parallel-data-free speech enhancement method using multi-objective learning cycle-consistent generative adversarial network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, and Lang. Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1826" to="1838" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical speech enhancement based on probabilistic integration of variational autoencoder and non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Itoyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yoshii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<imprint>
			<biblScope unit="page" from="716" to="720" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A variance modeling framework based on variational autoencoders for speech enhancement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Machine Learning. Signal Process</title>
		<meeting>IEEE Workshop Machine Learning. Signal ess</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised multichannel speech enhancement with variational autoencoders and non-negative matrix factorization</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="101" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Guided variational autoencoder for speech enhancement with a supervised classifier</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carbajal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="681" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Variational autoencoder for speech enhancement with a noise-aware encoder</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carbajal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="676" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning latent representations for speech generation and transformation</title>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1273" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for robust speech recognition via variational autoencoder-based data augmentation</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Automatic. Speech Recognition. and Understanding</title>
		<meeting>IEEE Workshop Automatic. Speech Recognition. and Understanding</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled and interpretable representations from sequential data</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inform</title>
		<meeting>Advances in Neural Inform</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1876" to="1887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">ADAM: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">NASA STI/Recon technical report n</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A tandem algorithm for pitch estimation and voiced speech segregation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, and Lang. Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2067" to="2079" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The diverse environments multi-channel acoustic noise database (demand): A database of multichannel environmental noise recordings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thiemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Meetings on Acoustics ICA2013</title>
		<meeting>Meetings on Acoustics ICA2013</meeting>
		<imprint>
			<publisher>Acoustical Society of America</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">35081</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Assessment for automatic speech recognition: II. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Steeneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="251" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning for monaural speech separation</title>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1562" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sdrhalf-baked or well done?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of time-frequency weighted noisy speech</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, and Lang. Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>Hekstra</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Understanding disentangling in β-vae</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03599</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
