<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interpreting the Latent Space of GANs via Correlation Analysis for Controllable Concept Manipulation</title>
				<funder ref="#_FNkeCqE #_jzyWR9Q">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziqiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Geo-spatial Information Processing and Application Systems</orgName>
								<orgName type="institution" key="instit1">CAS Key Laboratory of Technology</orgName>
								<orgName type="institution" key="instit2">University of Science and Technology of China</orgName>
								<address>
									<settlement>Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rentuo</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Geo-spatial Information Processing and Application Systems</orgName>
								<orgName type="institution" key="instit1">CAS Key Laboratory of Technology</orgName>
								<orgName type="institution" key="instit2">University of Science and Technology of China</orgName>
								<address>
									<settlement>Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongjing</forename><surname>Niu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Geo-spatial Information Processing and Application Systems</orgName>
								<orgName type="institution" key="instit1">CAS Key Laboratory of Technology</orgName>
								<orgName type="institution" key="instit2">University of Science and Technology of China</orgName>
								<address>
									<settlement>Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Bin</forename><surname>Li</surname></persName>
							<email>binli@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Geo-spatial Information Processing and Application Systems</orgName>
								<orgName type="institution" key="instit1">CAS Key Laboratory of Technology</orgName>
								<orgName type="institution" key="instit2">University of Science and Technology of China</orgName>
								<address>
									<settlement>Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Interpreting the Latent Space of GANs via Correlation Analysis for Controllable Concept Manipulation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">* Equal Contribution Accepted by ICPR2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative adversarial nets (GANs) have been successfully applied in many fields like image generation, inpainting, super-resolution, and drug discovery, etc. By now, the inner process of GANs is far from being understood. To get a deeper insight into the intrinsic mechanism of GANs, in this paper, a method for interpreting the latent space of GANs by analyzing the correlation between latent variables and the corresponding semantic contents in generated images is proposed. Unlike previous methods that focus on dissecting models via feature visualization, the emphasis of this work is put on the variables in latent space, i.e. how the latent variables affect the quantitative analysis of generated results. Given a pre-trained GAN model with weights fixed, the latent variables are intervened to analyze their effect on the semantic content in generated images. A set of controlling latent variables can be derived for specific content generation, and the controllable semantic content manipulation is achieved. The proposed method is testified on the datasets Fashion-MNIST and UT Zappos50K, experiment results show its effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>GANs <ref type="bibr" target="#b0">[1]</ref> have achieved great success in many tasks like image generation <ref type="bibr" target="#b1">[2]</ref>[3], super resolution <ref type="bibr" target="#b3">[4]</ref> and image translation <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b5">[6]</ref>. However, due to the great number of parameters and a complex cascade of nonlinear activations, we can hardly understand the intrinsic logic of GANs and the quantitative relationship between input values of variables in latent space and semantic content in output generated images. Moreover, GANs still suffer some severe issues, such as unstable training, mode drop and mode collapse, etc. Many works toward mitigating these issues have been proposed: designing new architectures <ref type="bibr" target="#b6">[7]</ref> <ref type="bibr" target="#b1">[2]</ref>, new loss functions <ref type="bibr" target="#b7">[8]</ref> <ref type="bibr" target="#b8">[9]</ref> or new training methologies <ref type="bibr" target="#b6">[7]</ref> <ref type="bibr" target="#b2">[3]</ref>. Compared to the large number of works that aim at improving train stability and generation quality, few works have been done to explore the intrinsic working mechanism of GANs, i.e. the interpretability of GANs. This motivated the work of this paper.</p><p>Recently, along with the success of deep learning, interpreting deep neural models have become a hot topic in research. The methods can be generally divided into three categories according to the corresponding interpreting results: visualizing pre-trained models <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b10">[11]</ref>, diagnose pre-trained models <ref type="bibr" target="#b11">[12]</ref>[13] and construct interpretable models <ref type="bibr" target="#b13">[14]</ref> <ref type="bibr" target="#b14">[15]</ref>. The first kind of methods use activation maximization or de-convolution techniques to visualize neurons or derive an input that maximizes semantic outputs; the second kind of methods diagnose pre-trained deep models by adding understandable disturbance to the inputs or quantify the importance of features; the last kind of methods impose constraints on neurons or reconstruct deep models to make deep models more interpretable. As for GANs, researchers proposed various methods to make them more interpretable and controllable, such as concatenating latent code with conditional information <ref type="bibr" target="#b15">[16]</ref> <ref type="bibr" target="#b16">[17]</ref>, interpolating in latent manifolds <ref type="bibr" target="#b17">[18]</ref> <ref type="bibr" target="#b18">[19]</ref> arXiv:2006.10132v2 [cs.CV] 23 Jul 2020 and latent disentanglement <ref type="bibr">[5][20]</ref>. Bau et.al <ref type="bibr" target="#b20">[21]</ref> proposed to quantify the effect or importance of different layer nodes on content changes by utilizing a pre-trained segmentation model to calculate IoU scores on generated image variations. However, previous methods did not give any quantitative evaluation of contribution each variable in latent space provides for generating specific semantic contents.</p><p>For the interpretability of GANs, due to the black-box property of deep neural models, hardly can we understand how the latent variables affect the generation process. To investigate if the variables in latent space contain the necessary information to distinguish different semantic contents in generated images, we use t-SNE <ref type="bibr" target="#b21">[22]</ref> to analyze the latent representations of Fashion-MNIST samples and find that the latent representations of samples from different classes can be well-separated, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. This indicates that for samples from the same one class, there may be exist closelyrelated latent variables that made their latent representations distinguishable. This motivated us to quantify the importance of different latent dimensions for specific concept generation. We propose to analyze the correlation between the latent inputs and the corresponding generated outputs by utilizing a pretrained classifier to provide quantitative evaluations on the semantic content changes in generated images, then to interpret the meaning of variables in the latent space of GANs. We also propose two methods for locating high-correlated latent dimensions for specific concept generation or manipulation: one is by sequential intervention and the other is by optimization. Moreover, given a specific class concept, we can also intervene in the derived controlling set of latent variables for controllable content manipulation. Specifically, the main contributions of this paper are:</p><p>• We first propose to interpret the latent space of GANs by quantifying the correlation between the latent inputs and the generated outputs. • We demonstrate that for generating contents of specific concept, the importance of different latent variables may varies greatly. Moreover, we propose an optimizationbased method to find controlling latent variables for specific concept. • The proposed method can fulfill controllable concept manipulation in generated images via controlling variables discovering and intervention. The remaining part of this paper is organized as follows: in section 2, we give a brief introduction to the backgrounds of this work. In section 3, we introduc the proposed method for interpreting the latent space of GANs in detail. The experiment results and implementation details are given in section 4 and finally is the conclusion of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generative Adversarial Networks</head><p>Generative adversarial nets (GANs) aimed at modeling the train data distribution through an adversarial process. It mainly contains two models that play against each other, a generator G and a discriminator D, G maps noise inputs from latent space to image space and tries to let D classify the generated samples as true, whereas D wants to clearly distinguish real data from generated ones. The adversarial process can be formulated as below:</p><formula xml:id="formula_0">min G max D V (D, G) =E x∼p data (x) [log D (x)] + E z∼pz(z) [log (1 -D (G (z)))]<label>(1)</label></formula><p>where V (D, G) is the value function of the min-max game, p data (x) and p z (z) represent the distribution of train data and latent noise respectively. In practice, people tend to train G to maximize log D(G(z)) rather than to minimize log(1 -D(G(z))) for the reason that equation 1 may not provide sufficient gradient for G in the early training stage. In this paper, we choose a pre-trained WGAN <ref type="bibr" target="#b7">[8]</ref> model for latent space analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. GAN Dissection</head><p>In <ref type="bibr" target="#b20">[21]</ref>, the author proposed an analytic framework to visualize and understand GANs at unit-, object-and scene level respectively by identifying interpretable units that are closely related to some object concepts through a segmentation network. Moreover, the authors also quantify the causal effect of interpretable units and offer an empirical method to mitigate the artifacts problem in generation results. The method for characterizing units by dissection can be formulated as below:</p><formula xml:id="formula_1">IoU u,c ≡ E z r ↑ u,P &gt; t u,c ∧ s c (x) E z r ↑ u,P &gt; t u,c ∨ s c (x)<label>(2)</label></formula><p>where u denotes the layer node in GANs and c means concepts like classes, r ↑ u,P denote the feature map of unit u, ∧ and ∨ represent intersection and union operation respectively, t u,c is a chosen threshold for producing binary mask and s c (x) is the segmentation result of the generated image x. This method uses the IoU (Intersection of Union) score to quantify the spatial agreement between a unit's feature map and the segmentation map of a generated image, which can reflect the importance of a unit for generating a specific concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CORRELATION ANALYSIS BETWEEN LATENT AND OUTPUT SPACES</head><p>In this paper, we propose to interpreting the latent space of GANs by analyzing the correlation between latent dimensions and the corresponding content changes in generated images. A thing that needs to notice is that the proposed method aimed at analyzing a pre-trained GAN model other than training a new one during the correlation analysis. The target was to semantically interpret the latent space and quantify the importance of different latent dimensions, below are the details of the proposed method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Statement</head><p>Given a pre-trained generator G (z), the target was to analyze the latent space and quantify the importance of different latent dimensions for a generation. As for specific semantic concepts like classes or objects, we want to analyze which dimensions influence the contents of the generated results most. The problem can be formulated as below:</p><formula xml:id="formula_2">X =G (Z) = G ([z 1 , z 2 , . . . , z N ]) ∆z i =⇒∆X =⇒ ∆C l , for i = 1, . . . , N<label>(3)</label></formula><p>where [z 1 , z 2 , ..., z N ] = z ∈ R N is denoted as the latent variable, ∆X and ∆C l (C ∈ R L , L classes) represent the content change and concept(here we choose class labels as different concepts) change in the corresponding generated image. For each latent dimension, we adjust the value of this dimension and observe how much can the intervention influence the generation results. Practically, as will be introduced in the latter part, for most concepts, there may be more than one latent dimension that closely related to that concept, thus we also try to find these controlling dimensions for different concepts. The problem can be simply formulated as:</p><formula xml:id="formula_3">α i ∈ {0, 1}, i = 1, . . . , N [α 1 • z 1 , . . . , α N • z N ] ⇐⇒ ∆X ⇐⇒ ∆C l<label>(4)</label></formula><p>where α i ∈ {0, 1} represents whether the corresponding dimension is high-correlated with a given concept C l or not.</p><p>In the next two subsections, we will introduce the proposed method for finding important latent dimension by quantifying the semantic content change and finding controlling dimensions for the given concept through optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. High-correlated latent dimensions for specific concept generation</head><p>To analyze the correlation between latent space and the output image space, we adopted a pre-trained classifier Q (x) to assign a score on the generated image and its variations. The pre-trained classifier Q can quantify the content change of the base generated image as a softmax score, which can be used for latent dimension importance analysis. For i-th latent dimension and j-th class</p><formula xml:id="formula_4">Z k =Z + k • δ • [0, . . . , 1, . . . , 0] k ∈ [-m, m]<label>(5)</label></formula><formula xml:id="formula_5">X k =G Z k = G ([z 1 , . . . , z i + δ • k, . . . , z N ])<label>(6)</label></formula><formula xml:id="formula_6">S k i,j = Q j X k = Q j G Z k<label>(7)</label></formula><p>where Z k , X k and S k i,j represent the latent variable(intervened the i-th dimension), the generated image and the corresponding classification softmax score of class j while k adjust the value of i-th latent dimension bi-directional k steps(the stepsize was δ). It's easy to find that when k = 0, Z 0 and X 0 is just the same as Z and X, i.e., the base reference generated image and latent variable. To measure the i-th latent dimension's effect on the generation of j-th class contents, we use the averaged probability change ratio (APCR) as the quantitative evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AP CR</head><formula xml:id="formula_7">i,j = m k=1 S k i,j -S k-1 i,j 2 • m 1 + -m k=-1 S k i,j -S k-1 i,j 2 • m 1<label>(8)</label></formula><p>As can be seen from the above equation, the metric calculates the averaged value changes of the probability that the generated image variations belong to a specified class over the latent dimension changes. Here we use the L-1 norm to calculate the probability changes separately along with two variation directions. Then, for each class, we will get n (dimension of the latent space) APCR scores and find the most correlated dimension for this class by ranking the scores. Similarly, we can also derive L APCR scores for each latent dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Controlling set of latent dimensions for controllable concept manipulation</head><p>As mentioned above, for a given specific class, there may have multi latent dimensions that closely related to it, hence we propose to find these important dimensions, which we called controlling latent dimension here. We propose to intervene in all the latent dimensions by assigning different weights to these dimensions and observe the corresponding classification score changes. The optimization objective was to maximize the classification score changes by updating the coefficients on the latent dimensions. The model weights were keep frozen during the optimization process. To derive the controlling dimensions for j-th class, we firstly add differentiated distortions to the latent dimensions by utilizing a weight vector w = [w 1 , . . . , w N ]:</p><formula xml:id="formula_8">Z = Z + w * ξ = [z 1 + w 1 • ξ, . . . , z N + w N • ξ] (9)</formula><p>where ξ represent an experimental constant for latent intervention, and w i ∈ [-1, 1] represent the latent intervention coefficients. Positive coefficients denote positive intervention direction and vise versa. For j-th class to be analyzed, we first calculate the probability changes with respect to latent interventions. Then we can derive the optimization objective of the weight vector w:</p><formula xml:id="formula_9">∆S j = S j -S j = Q j G Z -Q j (G (Z))<label>(10)</label></formula><formula xml:id="formula_10">w * = arg max L w = arg max (|∆S j |)<label>(11)</label></formula><p>For easy optimization and make the coefficients on latent dimensions sparse, we add the L-2 norm regularization and rewrite the optimization objective, the optimal w can be derived through the optimization of equation <ref type="bibr" target="#b11">(12)</ref>. Finally, we add a threshold t j on the optimized coefficients w to filter out uncorrelated dimensions for each class j, hence the controlling dimensions can be derived.</p><formula xml:id="formula_11">w * = arg min (1 -|∆S j | + λ • w 2 ) (12) w &gt;tj ⇐= w * j,1 &gt; t j , . . . , w * j,N &gt; t j<label>(13)</label></formula><p>Moreover, for controllable concept manipulation, we can just modify the optimization objective as below and derive the controlling set of latent dimensions for class-to-class translation (class j and class k).</p><formula xml:id="formula_12">∆S k→j = Q j (G (Z k -w * ξ)) -Q j (G (Z k )) ∆S j→k = Q k (G (Z j + w * ξ)) -Q k (G (Z j ))<label>(14)</label></formula><formula xml:id="formula_13">w * = arg max L w = arg max (|∆S k→j | + |∆S j→k |) = arg min (2 -|∆S k→j | -|∆S j→k | + λ • w 2 )<label>(15)</label></formula><p>Thus for Z j and Z k that belong to class j and k, we can achieve class j ↔ class k translation by Z j + w * ξ and Z k -w * ξ respectively. The proposed method was depicted in Figure <ref type="figure" target="#fig_2">2</ref>, where the top part denotes the process of finding high-correlated latent dimensions for the specific concept by sequential intervention and adding weights on latent variables. The bottom part denotes the process of controllable manipulation through latent intervention on top-ranked and bottomranked dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we will present the experiment results to demonstrate the effectiveness of the proposed method along with some interesting findings. The experiments can be divided into four parts, the first one is designed to find high-correlated latent dimensions for the specific concept by intervening latent dimensions sequentially, the second one solve the same problem as the first but with an optimization-based method, the third experiment is about controllable concept manipulation using controlling latent dimensions and the last one is about class2class image translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Implementation Details</head><p>In this paper, we conducted designed experiments on two datasets: Fashion-MNIST <ref type="bibr" target="#b22">[23]</ref> and UT Zappos50K <ref type="bibr" target="#b23">[24]</ref>. Fashion-MNIST is a dataset of Zalando's article images-consisting of 60k training samples and 10k testing samples, just the same with the original MNIST dataset. Each example is a 28x28 grayscale image, associated with a label from 10 classes: [T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot]. UT Zappos50K is a large shoe dataset consisting of 50,025 catalog images collected from Zappos.com. The images are divided into 4 major categories -shoes, sandals, slippers, and bootsfollowed by functional types and individual brands. As for the implementation, we use a pre-trained WGAN GP model for correlation analysis. Moreover, we adopt the LeNet-5 model as the choice for classification network and EmbedNet. The length of latent variables is set to 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Find High-Correlated Latent Dimensions by Sequential Latent Intervention</head><p>As having been introduced in Section 3.2, given a specific class concept, we intervene in the latent dimensions sequentially and get corresponding APCR scores for each latent dimension. Then we rank the latent dimensions for each concept according to the APCR values, thus high-correlated latent dimensions can be derived. Here we use the softmax score to indicate the concept changes in generated images. The softmax score changes w.r.t latent offsets for each latent dimension was shown in Figure <ref type="figure" target="#fig_3">3</ref>, from which we can clearly see that the slope of different curves varies greatly. It indicates that semantic concepts are sensitive to several latent dimensions, i.e., intervening on some high-correlated latent dimensions will lead to greater semantic concept changes compared to intervene on less important dimensions. Moreover, we also give the APCR distribution information in Figure <ref type="figure" target="#fig_4">4</ref>, where the x-axis denote the range-index number (larger index number represents larger APCR values) of APCR values and the yaxis denote the number of dimensions belongs to each range. It furthermore demonstrated that the amount of high-correlated latent dimensions for each concept is small.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Find Controlling Set of Latent Dimensions via Optimization</head><p>We use the optimization-based method described in Section 3.2 to derive the coefficients vector (w ∈ [-1, 1]) on latent dimensions for each concept, then we impose thresholds on the coefficients to filter out a positive and negative controlling set of latent dimensions. Larger positive values and smaller negative values denote higher correlation when intervening along the positive and negative direction respectively. We randomly select two classes to demonstrate the controlling latent dimensions, which can be seen in Figure <ref type="figure" target="#fig_5">5</ref> (top and bottom parts represent intervention results of class 8 and class9 respectively). As illustrated in Figure <ref type="figure" target="#fig_5">5</ref>, the first three rows represent the intervention results of R 0 ,R 50 ,R 99 dimensions respectively, where the numbers denoted as the ranked order, thus R 0 and R 99 represent most correlated latent dimension along bi-directional intervention. The bottom two rows give the intervention results of top-5 positive/ negative correlated dimensions. From the top three rows, we can see that positive intervention results of R 0 are similar to negative intervention results of R 99 while intervene on R 50 car hardly leads to any generation changes. Moreover, we intervene in top-5 positive and negative dimensions and get better results, which can be seen from the bottom two rows. This indicates that for a given specific concept, we can derive the controlling set of latent dimensions of it, hence concept manipulation can be achieved by intervening on these dimensions (furthermore results can be seen in the next subsection). We also propose to use intersection ration (IR ctrl ) as an evaluation metric of the concordance of the controlling dimensions derived by sequential intervention and optimization. The results can be seen in Table <ref type="table" target="#tab_1">I</ref>, it indicates that controlling dimensions derived by two methods were quite similar with an average IR score of 0.75. It furthermore demonstrated that latent dimensions contribute differently to specific concept generation.  As introduced in the above section, we can achieve controllable concept manipulation by intervening the controlling set of latent dimensions. The controllable manipulation results of Fashion-MNIST and UT Zappos50k can be seen in Figure <ref type="figure" target="#fig_6">6</ref> and Figure <ref type="figure" target="#fig_7">7</ref> respectively. In Figure <ref type="figure" target="#fig_6">6</ref>, the leftmost column denotes the reference images come from different classes of Fashion-MNIST, and the right 10 column denote the corresponding directional manipulation results, i.e. translate the reference image to different classes. Similarly, in Figure <ref type="figure" target="#fig_7">7</ref>, the middle column in the yellow bounding-box denotes the reference image in UT Zappos50k and other columns denote bi-directional manipulation results. From the results, we can see that controllable concept manipulation can be achieved by intervention on controlling set of latent dimensions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Interesting Findigns of Extreme Intervention on Latent Dimensions</head><p>We also have some interesting findings of the latent interventions, which are demonstrated in Figure <ref type="figure">8</ref>. Usually, the common choice of latent variables is drawn from a Gaussian normal distribution, by now, researchers have not explored extreme cases of latent values. We propose to add positive and negative impulse stimuli on latent dimensions and observe the corresponding semantic content change in the generated image. The results are demonstrated in Figure <ref type="figure">8</ref>, where the middle row represents the original images come from different classes, the top and bottom row denote positive/ negative extreme intervention results respectively. From the results, we can see that for images from different classes, if we extremely intervene in some latent dimension (adding impulse stimuli), the final manipulation results will converge to some specific classes. Inspired by this finding, we can find these unique dimensions for various class pairs and achieve controllable class-to-class translation (Figure <ref type="figure">9</ref>). V. CONCLUSION</p><p>In this paper, we proposed a method to interpret the latent space of GANs with correlation analysis for controllable concept manipulation. We utilized a pre-trained classifier to evaluate the concept changes in the generation variations. We first illustrated that the importance of different latent dimensions that contribute to specific concept generation varies greatly. Moreover, we also proposed two methods to find the controlling set of latent dimensions for different concepts, with which we can fulfill controllable concept manipulation. These interesting findings provide a new route for controllable generation and image editing, moreover, it also gives us some thoughtful insight into the interpretability of GANs' latent space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: t-SNE analysis on latent representations of Fashion-MNIST dataset. Points in different color are 2D features of latent representations belong to different classes.</figDesc><graphic coords="1,340.02,273.87,178.50,178.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2:The proposed method for analyzing the correlation between latent space and output image space of GANs. Top part illustrate the process of finding high-correlated latent dimensions by sequential intervention or adding weights on latent variables. Bottom part denote the process of latent intervention on top or bottom ranked latent dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Classification score change with respect to intervention on different latent dimensions. Each color represent a latent dimension.</figDesc><graphic coords="5,50.47,270.49,231.33,173.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Number distribution of latent dimensions with respect to different APCR value range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Intervene on controlling set of latent dimensions.</figDesc><graphic coords="5,50.47,502.49,231.33,173.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Controllable concept manipulation on Fashion-MNIST through intervening on controlling set of latent dimensions (final manipulation results).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Controllable concept manipulation on UT Zappos50k through intervening on controlling set of latent dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :Fig. 9 :</head><label>89</label><figDesc>Fig. 8: Extreme intervention results by adding impulse stimuli on specific latent dimensions bidirectionally.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Intersection ration of high-correlated latent dimensions derived by sequential intervention and optimization</figDesc><table /><note><p>D. Controllable Concept Manipulation with Controlling Latent Dimensions</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was partially supported by the <rs type="funder">National Natural Science Foundation of China</rs> under grand No.<rs type="grantNumber">U19B2044</rs> and No.<rs type="grantNumber">61836011</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_FNkeCqE">
					<idno type="grant-number">U19B2044</idno>
				</org>
				<org type="funding" xml:id="_jzyWR9Q">
					<idno type="grant-number">61836011</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fader networks: Manipulating images by sliding attributes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-toimage translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8789" to="8797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Wasserstein gan</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5188" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Why should i trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identifying unknown unknowns in the open world: Representations and policies for guided exploration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interpreting cnns via decision trees</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6261" to="6270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interpretable convolutional neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8827" to="8836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ganalyze: Toward visual definitions of cognitive image properties</title>
		<author>
			<persName><forename type="first">L</forename><surname>Goetschalckx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.10112</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Interpreting the latent space of gans for semantic face editing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10786</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the&quot;steerability&quot; of generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07171</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semantically decomposing the latent spaces of generative adversarial networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07904</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Gan dissection: Visualizing and understanding generative adversarial networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10597</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic jitter: Dense supervision for visual comparisons via synthetic images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
