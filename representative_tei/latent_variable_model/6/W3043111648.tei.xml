<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Missing rating imputation based on product reviews via deep latent variable models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dingge</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Université Côte d&apos;Azur</orgName>
								<address>
									<settlement>Inria</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Marco</forename><surname>Corneli</surname></persName>
							<email>&lt;marco.corneli@univ-cotedazur.fr&gt;.</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Université Côte d&apos;Azur</orgName>
								<address>
									<settlement>Inria</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CNRS</orgName>
								<address>
									<addrLine>Lab- oratoire J.A.Dieudonneé Maasai team</addrLine>
									<settlement>Nice</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Latouche</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center of Modelling, Simulation and Interactions (MSI)</orgName>
								<orgName type="institution">Université Côte d&apos;Azur</orgName>
								<address>
									<settlement>Nice</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><surname>Bouveyron</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Université Côte d&apos;Azur</orgName>
								<address>
									<settlement>Inria</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Université de Paris</orgName>
								<address>
									<addrLine>Laboratoire MAP5</addrLine>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Missing rating imputation based on product reviews via deep latent variable models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a deep latent recommender system (deepLTRS) for imputing missing ratings based on the observed ratings and product reviews. Our approach extends a standard variational autoencoder architecture associated with deep latent variable models in order to account for both the ordinal entries and the text entered by users to score and review products. DeepLTRS assumes a latent representation of both users and products, allowing a natural visualisation of the positioning of users in relation to products. Numerical experiments on simulated and real-world data sets demonstrate that DeepLTRS outperforms the state-of-the-art, in particular in contexts of extreme data sparsity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction and related works</head><p>Matrix completion is a central machine learning problem, which consists in predicting the non observed entries of a matrix on the basis of the observed entries. We focus here on the collaborative filtering problem which aims at completing a matrix of user ratings about products. These matrices are extremely sparse in practice which makes the inference of the non-observed entries challenging. A long series of approaches have been proposed to tackle this issue.</p><p>The works of <ref type="bibr" target="#b6">Gopalan et al. (2015)</ref>; <ref type="bibr" target="#b0">Basbug &amp; Engelhardt (2016)</ref> rely on the assumption that ratings are sampled from Poisson distributions, simple or compound. Recently, coupled compound Poisson factorization (CCPF, <ref type="bibr" target="#b1">Basbug &amp; Engelhardt, 2017)</ref> was introduced as a more general framework based on different generative approaches (e.g. mixture models or matrix factorization models). Another class of collaborative filtering models rely on user-based (respectively item-based) autoencoders to produce user (item) embeddings in lower dimension, both based on recurrent <ref type="bibr" target="#b10">(Monti et al., 2017)</ref> or convolutional <ref type="bibr" target="#b11">(Sedhain et al., 2015;</ref><ref type="bibr" target="#b13">Zheng et al., 2016)</ref> architectures. The last two approaches are special cases of a more general architecture (graph autoencoder models, <ref type="bibr" target="#b7">Kipf &amp; Welling, 2016)</ref>, recently employed for matrix completion <ref type="bibr" target="#b2">(Berg et al., 2017)</ref>.</p><p>Unfortunately, although the aforementioned models can account for side information additionally to the user ratings, they do not introduce a modeling framework specific to the product reviews. However in practice, the product ratings are often paired with reviews that might contain crucial information about the user preferences. Thus, in <ref type="bibr" target="#b8">McAuley &amp; Leskovec (2013)</ref>, the hidden factors and hidden topics (HFT) model combines latent rating factors with latent review topics. Still, when a large amount of user ratings is missing, the performance of the predictions turns out to be limited.</p><p>We introduce here the deep latent recommender system (deepLTRS, Sections 2-3) for the completion of rating matrices, accounting for the textual information collected in the product reviews. DeepLTRS extends probabilistic matrix factorization <ref type="bibr" target="#b9">(Mnih &amp; Salakhutdinov, 2008)</ref> by relying partially on latent Dirichlet allocation (LDA, <ref type="bibr" target="#b3">Blei et al. (2003)</ref>) and its recent autoencoding extensions <ref type="bibr" target="#b12">(Srivastava &amp; Sutton, 2017;</ref><ref type="bibr" target="#b4">Dieng et al., 2019)</ref>. Thus, our approach adopts a variational autoencoder architecture as a generative deep latent variable model for both the ordinal matrix encoding the user/product scores, and the document-term matrix encoding the reviews. Our approach is tested on simulated and real datasets (Section 4) and compared with other state-ofthe-art approaches in contexts of extreme data sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A text based recommender system</head><p>We consider data sets involving M users scoring/reviewing P products. Such data sets can be encoded by two matrices: an ordinal data matrix Y accounting for the scores that users assign to products and a document-term matrix (DTM) W accounting for the reviews that users make about products.</p><p>Ordinal data. The ordinal data matrix Y in N M ×P is such that Y ij is the score that the i-th user assigns to the j-th product. This matrix can be very sparse (most of its entries are missing) corresponding to users not scoring/reviewing some products. Conversely, when a score is assigned it takes values in {1, . . . , H} with H &gt; 1. Assumption 1. Henceforth, we assume that an ordinal scale is consistently defined. For instance, when customers evaluate products, 1 always means "very poor" and H is always associated with "excellent" reviews.</p><p>Assumption 2. The number of ordered levels H is assumed to be the same for all (not missing) Y ij . If it is not the case, a scale conversion pre-processing algorithm (see for instance <ref type="bibr" target="#b5">Gilula et al., 2018)</ref> can be employed to normalize the number of levels.</p><p>Text data. By considering all the available reviews, it is possible to store all the different vocables employed by the users into a dictionary of size V . Thenceforth, we denote by W (i,j) a row vector of size V encoding the review by the ith user to the j-th product. The v-th entry of W (i,j) , denoted by</p><formula xml:id="formula_0">W (i,j) v</formula><p>, is the number of times (possibly zero) that the word v of the dictionary appears into the corresponding review. The document-term matrix W is obtained by row concatenation of all the row vectors W (i,j) . Assumption 3. For the sake of clarity, we assume that the review W (i,j) exists if and only if Y ij is observed.</p><p>Note that, since each row in W corresponds to one (and only one) not missing entry in Y , the number of rows in the DTM is the same as the number of observed values in Y .</p><p>It is now assumed that both users and products have latent representations in a low-dimensional space R D , with D min{M, P }. In the following, R i denotes the latent representation of the i-th user. Similarly C j is the latent representation of the j-th product.</p><p>The following generative model is now considered:</p><formula xml:id="formula_1">Y ij = R i , C j + ij , ∀i = 1, ..., M, ∀j = 1, ..., P, (1)</formula><p>where •, • is the standard scalar product and the residuals ij are assumed to be i.i.d. and normally distributed random variables, with zero mean and unknown variance η 2 : ij ∼ N (0, η 2 ).</p><p>In the following, R i and C j are seen as random vectors, such that</p><formula xml:id="formula_2">R i i.i.d ∼ N (0, I D ), ∀i C j i.i.d ∼ N (0, I D ), ∀j<label>(2)</label></formula><p>with R i ⊥ ⊥ C j . This model is knows as probabilistic matrix factorization (PMF, <ref type="bibr" target="#b9">Mnih &amp; Salakhutdinov, 2008)</ref>. Note that, due to rotational invariance of PMF, the choice of isotropic prior distributions for R i and C j is in no way restrictive</p><p>We now extend PMF by also relying on R i and C j to characterize the document-term matrix W . Following the generative model of LDA <ref type="bibr" target="#b3">(Blei et al., 2003)</ref>, each document W (i,j) is drawn from a mixture distribution over a set of K latent topics. The topic proportions in the document W (i,j)  are denoted by θ ij , a vector lying in the K -1 simplex. LDA, in its multinomial PCA formulation assumes that</p><formula xml:id="formula_3">p(W (i,j) |θ ij ) ∼ Multinomial(L ij , βθ ij ),<label>(3)</label></formula><p>where L ij is the number of words in the review W (i,j) and β ∈ R V ×K is the matrix whose entry β vk is the probability that vocable v occurs in topic k. By construction, V v=1 β vk = 1, ∀k. Moreover, conditionally to all vectors θ ij , all the reviews {W (i,j) } are independent random vectors. The following assumption calls R i and C j into play.</p><p>Assumption 4. The topic proportions are now assumed to be sampled as follows</p><formula xml:id="formula_4">θ ij = σ(f γ (R i , C j )),<label>(4)</label></formula><p>where f γ : R 2D → R K is a continuous function approximated by a neural network parametrized by γ and where σ(•) denotes the softmax function.</p><p>We emphasize that the θ ij are no longer independent contrary to LDA. We finally state that:</p><p>Assumption 5. Given the pair (R i , C j ), it is assumed that Y ij and W (i,j) are independent.</p><p>We stress that we are not assuming that Y ij and W (i,j) are independent. Instead, we describe a framework in which the dependence between them is completely captured by the latent embedding vectors R i and C j . A graphical representation of the generative model described so far can be seen in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Variational auto-encoding inference</head><p>A natural inference procedure associated with the generative model proposed would consist in looking for estimates (η 2 , γ, β) maximizing the (integrated) log-likelihood of the observed data (Y, W ). Unfortunately, this quantity is not directly tractable and we rely on a variational lower bound (a.k.a. ELBO) to approximate it. A tractable family of joint distributions q(•) over the pair (R, C) of all (R i ) i and (C j ) j is considered via the following mean-field assumption</p><formula xml:id="formula_5">q(R, C) = q(R)q(C) = M i=1 P j=1 q(R i )q(C j ).<label>(5)</label></formula><p>Moreover, since R i and C j follow Gaussian prior distributions (Eq. ( <ref type="formula" target="#formula_2">2</ref>)), q(•) is assumed to be as follows:</p><formula xml:id="formula_6">q(R i ) = g(R i ; µ R i , S R i ), (6) q(C j ) = g(C j ; µ C j , S C j ),<label>(7)</label></formula><p>where g(•; µ, S) is the pdf of a Gaussian multivariate distribution with mean µ and variance S and</p><formula xml:id="formula_7">µ R i : = h 1,φ (Y i , W (i,•) ) S R i := h 2,φ (Y i , W (i,•) ) µ C j : = l 1,ι (Y j , W (•,j) ), S C j := l 2,ι (Y j , W (•,j) )</formula><p>.</p><p>Here, Y i (respectively Y j ) denotes the i-th row (column) of Y and W (i,•) := j W (i,j) corresponds to a document concatenating all the reviews written by user i and, similarly W (•,j) := i W (i,j) corresponds to all the reviews about the j-th product. The two functions h φ : R P +V → R 2×D and l ι : R M +V → R 2×D are the network encoders and they are parametrized by φ and ι, respectively.</p><p>Thanks to Eqs. ( <ref type="formula">1</ref>)-( <ref type="formula" target="#formula_3">3</ref>)-( <ref type="formula" target="#formula_5">5</ref>)-( <ref type="formula">6</ref>)-( <ref type="formula" target="#formula_6">7</ref>) and by computing the KL divergences between q(•) and the prior distributions of R and C, the evidence lower bound (ELBO) can be explicitly computed (cf. Section A, supplementary material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Numerical experiments</head><p>4.1. Simulated data and effect of the data sparsity Simulation setup. An ordinal data matrix Y with M = 750 rows and P = 600 columns is simulated according to a latent continuous cluster model. The rows and columns of Y are randomly assigned to two latent groups, in equal proportions. Then, for each pair (i, j) corresponding to an entry of Y , a Gaussian random variable Z ij is sampled in such a way that</p><formula xml:id="formula_8">Z ij ∼ N (2, 1) if X (R) i = X (C)</formula><p>j , and Z ij ∼ N (3, 1) otherwise, where X (R) i and X (C) j label the clusters of the i-th row and the j-th column, respectively. Then the following thresholds t 0 = -∞, t 1 = 1.5, t 2 = 2.5, t 3 = 3.5, t 4 = +∞ are used to sample the note Y ij ∈ {1, ..., 4} as</p><formula xml:id="formula_9">Y ij = 4 k=1 k1(Z ij ) ]t k-1 ,t k [<label>(8)</label></formula><p>Then, four different texts from the BBC news (denoted by A, B, C, D) are used to build a message associated to the note Y ij according to the scheme summarized in Table <ref type="table">1</ref>.</p><formula xml:id="formula_10">cluster 1 cluster 2 cluster 1 A B cluster 2 C D</formula><p>Table <ref type="table">1</ref>. Topic assignments for simulated data.</p><p>Thus, when the user i in cluster X (R) i</p><p>= 2 rates the product j in cluster X (C) j = 1, a random variable Z ij ∼ N (3, 1) is sampled, Y ij is obtained via Eq. ( <ref type="formula" target="#formula_9">8</ref>) and the review W (i,j)  is built by random extraction of words from message C. All the sampled messages have an average length of 100 words. Finally and in order to introduce some noise, only 80% of words are extracted from the main topics, while the remaining 20% is extracted from the other topics at random. First, Figure <ref type="figure" target="#fig_1">2</ref> shows a t-SNE representation of R i and C j , with data sparsity of 0.99 (i.e. 99% of the observations in Y and W are replaced by NA at random). We first note that the two (row ans column) clusters are well separated despite the large degree of sparsity. Since deepLTRS assumes that the closer the distance, the greater the probability that the product is reviewed by the user, this latent representation is well representative of the simulated setup . A total of 10 data sets was simulated according to the above setup, with sparsity rates varying in the interval [0.5, 0.99]. The whole observed data is used as training data set, the remaining missing data was split into 50% for validation and 50% for test. This experimental setup was used to benchmark deepLTRS by comparing it to some state-of-the-art methods as HFT <ref type="bibr" target="#b8">(McAuley &amp; Leskovec, 2013)</ref>, HPF <ref type="bibr" target="#b6">(Gopalan et al., 2015)</ref> and CCPF <ref type="bibr" target="#b1">(Basbug &amp; Engelhardt, 2017)</ref>. Since CCPF has many choices of combination between sparsity and response models, we chose one example with better performance as described in <ref type="bibr" target="#b1">(Basbug &amp; Engelhardt, 2017)</ref>. Figure <ref type="figure" target="#fig_2">3</ref> shows the evolution of the test RMSE of deepLTRS (with D = 50 and text) and its competitors. Additional results are reported in the supplementary material, Section C. Let us recall that the simulation setup does not follow the deepLTRS generative model and therefore does not favour any method here. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Amazon Fine Food data</head><p>Data and pre-processing. This data set 1 spans over a period of more than 10 years, including all 568, 464 reviews up to October 2012. All records include product and user information, ratings, time of the review and a plain-text review. In the data pre-processing step, we only considered users with more than 20 reviews and products reviewed by more than 50 users to obtain more meaningful information. The retained data was processed by removing all punctuations and numbers. Since around half of the negative reviews have more positive than negative words in the data set, we kept the stop words (such as "not","very") to make our bag-of-words structure more clearly retain the original semantics. The final data set has M = 1, 643 users, P = 1, 733 products, a vocabulary with V = 5, 733 unique words and 32, 811 text reviews in total. The data sparsity is here of 0.989%.</p><p>Rating prediction. Five independent runs of the algorithm were performed. For each run, we randomly selected 80% of the non-missing data as the training set, 10% for validation and the remaining 10% for testing. Table <ref type="table" target="#tab_0">2</ref> reports the test RMSE for deepLTRS and its competitors (HFT, HPF and CCPF-PMF) on the predicted ratings for the Amazon Fine Food data. Once again, deepLTRS has better performance than other models, with an average test RMSE equal to 1.2518. In order to deeper understand the latent representation meaning, we provide in Figure <ref type="figure" target="#fig_3">4 a t-SNE</ref> 1 The data set can be downloaded freely at <ref type="url" target="https://snap.stanford.edu/data/web-FineFoods.html">https://snap. stanford.edu/data/web-FineFoods.html</ref>  visualisation (D = 50) of the user latent positions on two specific latent variables (var. 3 and 11) that can be easily interpreted according to average ratings (top) and number of reviews (bottom) the users give to the products. Indeed, it clearly appears that var. 11 captures the rating scale of Amazon users whereas var. 3 seems to encode the user activity (number of reviews). Additional analyses are reported in the supplementary material, Section D. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q -0.8 -0.4 0.0 0.4 0.0 0.5 1.0 1.5 V3 V11 Avg. rating q q q q q 1 2 3 4 5 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q -0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduced here the deepLTRS model for ratings imputation using both the ordinal and the text data available. Our approach adopts a variational autoencoder architecture as a generative deep latent variable model for both the ordinal matrix encoding the user/product scores, and the documentterm matrix encoding the reviews. The further ability of deepLTRS to predict the most likely words used by a reviewer to review a product will be inspected in future works.   . Projection with t-SNE of user and product latent representations for the Amazon Fine Food data set.</p><p>q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q -0.6 -0.3 0.0 0.3 -0.5 0.0 V11 V46 Avg. rating q q q q 2 3 4 5 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q -0.6 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Graphical representation of the generative model (variational parameters are not included).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Visualization of the user and product embeddings (sparsity of 0.99).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Test RMSE of models: HFT, HPF, CCPF and deepLTRS with different sparsity level on the simulated data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Latent representation of users on var. 3 and 11, according to average ratings (left) and numbers of reviews (right) they give to products.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. RMSE by deepLTRS with and without text information on simulated data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 6. Projection with t-SNE of user and product latent representations for the Amazon Fine Food data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Latent representation of products on var. 11 and 46, according to average rating (left) and number of reviews (right) they receive from users.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Test RMSE on Amazon Fine Food data.</figDesc><table><row><cell>Model</cell><cell>Run 1 Run 2 Run 3 Runt 4 Run 5 Average</cell></row><row><cell>HFT</cell><cell>1.424 1.533 1.474 1.423 1.385 1.448 (±0.051)</cell></row><row><cell>HPF</cell><cell>2.949 2.968 2.931 2.943 2.973 2.953 (±0.016)</cell></row><row><cell cols="2">CCPF-PMF 1.269 1.296 1.303 1.292 1.295 1.291 (±0.011)</cell></row><row><cell cols="2">deepLTRS 1.136 1.259 1.244 1.171 1.247 1.252 (±0.049)</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head><p>A. Evidence lower bound</p><p>E q(R i ,C j ) W (i,j) T log (βσ(fγ(Ri, Cj)))</p><p>where Θ := {η 2 , γ, β, φ, ι} denotes the set of the model and variational parameters and ξ is a constant term that includes all the elements not depending on Θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Architecture of deepLTRS</head><p>In the architecture of deepLTRS, we have two encoders for users and products separately. As a method for stochastic optimization, we adopt an Adam optimizer, with learning rate lr = 2e -3 .</p><p>In the user encoder, the first hidden layer has init dim R = (P + V ) neurons, where P is equal to the number of products, and V is the number of words in the text vocabulary; the second hidden layer has mid dim = 50 neurons. In the product encoder, the first hidden layer has init dim C = (M + V ) neurons, where M is equal to the number of users; the second hidden layer has the same neurons as in the user encoder. Sof tplus activation function and batch normalization are applied in each layer.</p><p>In the decoder, the first two layers have 2 × int dim and 80 neurons separately, where int dim = init dim R when decoding for users and int dim = init dim C for products. The number of neurons in the third layer depends on the number of topics, here we used nb of topics = 50. In addition, Relu activation function and batch normalization are applied. In order to obtain the probability of each word, the Sof tmax function is used in the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More on simulated data experiments</head><p>DeepLTRS with and without text data. We first run a simulation to highlight the interest in using the reviews to make more accurate predictions of the ratings. To do so, 10 data sets are simulated according to the above simulation setup, with sparsity rates varying in the interval [0.5, 0.99]. Figure <ref type="figure">5</ref> shows the evolution of the test RMSE of deepLTRS (with D = 50), with and without using text data, versus the data sparsity level. One can observe that, even though both models suffer the high data sparsity (increasing RMSE), the use of the text greatly help deepLTRS to maintain a high prediction accuracy for data sets with many missing values. Furthermore, the use of text reviews greatly reduce the variance of the deepLTRS predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More on Amazon Fine Food data experiments</head><p>Figure <ref type="figure">6</ref> shows a visualization with t-SNE of the high-dimensional latent representations (here D = 50) of the users and products for the Amazon data. The density of the overlapping regions and the distance between user and product embeddings reflect the probability of users reviewing the corresponding products.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hierarchical compound poisson factorization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Basbug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1795" to="1803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Basbug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Engelhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02058</idno>
		<title level="m">Coupled compound poisson factorization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04907</idno>
		<title level="m">Topic modeling in embedding spaces</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A study into mechanisms of attitudinal scale conversion: A stochastic ordering approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gilula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ritov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Urminsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable recommendation with hierarchical poisson factorization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hofman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Thirty-First Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="326" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hidden factors and hidden topics: understanding rating dimensions with review text</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM conference on Recommender systems</title>
		<meeting>the 7th ACM conference on Recommender systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3697" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Autorec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on World Wide Web</title>
		<meeting>the 24th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Autoencoding variational inference for topic models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01488</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A neural autoregressive approach to collaborative filtering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
