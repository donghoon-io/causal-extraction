<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Topic Hierarchies by Tree-Directed Latent Variable Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-08-27">August 27, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sunrit</forename><surname>Chakraborty</surname></persName>
							<email>sunritc@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Michigan Ann Arbor</orgName>
								<address>
									<postCode>MI -48105</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rayleigh</forename><surname>Lei</surname></persName>
							<email>rayleigh@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Michigan Ann Arbor</orgName>
								<address>
									<postCode>MI -48105</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuanlong</forename><surname>Nguyen</surname></persName>
							<email>xuanlong@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Michigan Ann Arbor</orgName>
								<address>
									<postCode>MI -48105</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Topic Hierarchies by Tree-Directed Latent Variable Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-08-27">August 27, 2024</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>topic model</term>
					<term>Latent Dirichlet Allocation</term>
					<term>topic hierarchy</term>
					<term>directed tree</term>
					<term>identifiability</term>
					<term>inverse bound</term>
					<term>consistency</term>
					<term>contraction rate many</term>
					<term>day</term>
					<term>because</term>
					<term>now</term>
					<term>where</term>
					<term>people</term>
					<term>city</term>
					<term>can</term>
					<term>work</term>
					<term>fire player</term>
					<term>team</term>
					<term>league</term>
					<term>season</term>
					<term>club</term>
					<term>coach</term>
					<term>game</term>
					<term>real</term>
					<term>won</term>
					<term>madrid government</term>
					<term>united</term>
					<term>official</term>
					<term>iran</term>
					<term>country</term>
					<term>american</term>
					<term>saudi</term>
					<term>egyptian</term>
					<term>report</term>
					<term>plane goal</term>
					<term>fan</term>
					<term>game</term>
					<term>minute</term>
					<term>match</term>
					<term>score</term>
					<term>group</term>
					<term>france</term>
					<term>england</term>
					<term>kick game</term>
					<term>penguin</term>
					<term>goal</term>
					<term>second</term>
					<term>pittsburgh</term>
					<term>score</term>
					<term>series</term>
					<term>period</term>
					<term>shot</term>
					<term>win her</term>
					<term>she</term>
					<term>party</term>
					<term>brazil</term>
					<term>president</term>
					<term>woman</term>
					<term>country</term>
					<term>rousseff</term>
					<term>political</term>
					<term>leader syria</term>
					<term>syrian</term>
					<term>group</term>
					<term>assad</term>
					<term>united</term>
					<term>government</term>
					<term>war</term>
					<term>russian</term>
					<term>russia</term>
					<term>aid team</term>
					<term>world</term>
					<term>player</term>
					<term>game</term>
					<term>united</term>
					<term>copa</term>
					<term>tournament</term>
					<term>cup</term>
					<term>argentina</term>
					<term>messi game</term>
					<term>shark</term>
					<term>season</term>
					<term>final</term>
					<term>san</term>
					<term>jose</term>
					<term>team</term>
					<term>cup</term>
					<term>hockey</term>
					<term>playoff israeli</term>
					<term>palestinian</term>
					<term>israel</term>
					<term>gaza</term>
					<term>military</term>
					<term>netanyahu</term>
					<term>police</term>
					<term>minister</term>
					<term>soldier</term>
					<term>kill islamic</term>
					<term>iraq</term>
					<term>force</term>
					<term>iraqi</term>
					<term>city</term>
					<term>kill</term>
					<term>fight</term>
					<term>attack</term>
					<term>fighter</term>
					<term>government</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study a parametric family of latent variable models, namely topic models, equipped with a hierarchical structure among the topic variables. Such models may be viewed as a finite mixture of the latent Dirichlet allocation (LDA) induced distributions, but the LDA components are constrained by a latent hierarchy, specifically a rooted and directed tree structure, which enables the learning of interpretable and latent topic hierarchies of interest. A mathematical framework is developed in order to establish identifiability of the latent topic hierarchy under suitable regularity conditions, and to derive bounds for posterior contraction rates of the model and its parameters. We demonstrate the usefulness of such models and validate its theoretical properties through a careful simulation study and a real data example using the New York Times articles.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Topic models have widely been used to analyze text corpora with the goal of discovering abstract topics that occur in the collection of documents <ref type="bibr" target="#b7">(Blei et al., 2003;</ref><ref type="bibr" target="#b6">Blei and Lafferty, 2006b)</ref>. A topic is a probability distribution over a vocabulary, representing a particular hidden semantic structure in the corpus. A document may be associated with multiple topics present in different proportions. Understanding such latent semantics enables automated categorization and tagging, which is required by organizations handling a large number of unstructured text documents. Apart from text corpora, the same modelling framework was developed in population genetics <ref type="bibr" target="#b26">(Pritchard et al., 2000)</ref> for identifying ancestral population structure from gene samples, and also deployed in a vast range of domains as diverse as quantitative biomedicine (e.g., extracting information of cancers' genomic samples <ref type="bibr" target="#b30">(Valle et al., 2020)</ref>) and audio analysis (e.g., understanding hidden structures in music and how music styles evolved over time <ref type="bibr" target="#b27">(Shalit et al., 2013)</ref>). Topic models may be broadly classified into two types -while non-probabilistic models such as Nonnegative Matrix Factorization <ref type="bibr" target="#b9">(Choo et al., 2013)</ref> and Latent Semantic Analysis <ref type="bibr" target="#b20">(Kherwa and Bansal, 2017)</ref> focus on low-rank decomposition of the document-proportion matrix, probabilistic models such as Latent Dirichlet Allocation <ref type="bibr" target="#b7">(Blei et al., 2003)</ref> and Pachinko Allocation <ref type="bibr" target="#b21">(Li and McCallum, 2006</ref>) employ a probabilistic generative model for the data using latent variables. In general, mixture models represent the simplest instance of models with latent variables, and provide a basic modelling framework for text corpora -e.g., mixture of product multinomial distributions is a choice for a text data generating model, where the components represent the topics. However, topic models are considerably richer than the basic mixture models in that different documents contain topics in possibly different proportions, whereas in mixture models, each document has the same mixture probabilities for different components. As a result, in topic models, the mixture probabilities are different across documents -this structure is often referred to as admixture.</p><p>Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b7">(Blei et al., 2003)</ref> is the most popular representative of topic modeling and is in fact a building block of this paper. In a nutshell, the LDA assumes there are K topics in the corpus. Document specific mixture probabilities (or allocation) are assumed to follow a Dirichlet distribution. Given the allocation for a document, words are conditionally i.i.d. from a mixture of multinomial distributions, with parameters given by the topics themselves. Thus, each document uses all the topics. The LDA model has been extended in several directions, for instance, Dynamic Topic Models for temporal topic models <ref type="bibr" target="#b6">(Blei and Lafferty, 2006b)</ref>, Correlated Topic Models for allowing correlation among the topics <ref type="bibr" target="#b4">(Blei and Lafferty, 2006a)</ref>, non-parametric LDA using a Hierarchical Dirichlet Process prior over the topics <ref type="bibr" target="#b29">(Teh et al., 2006)</ref> and Gaussian LDA for continuous observations, replacing the multinomial kernel by a Gaussian kernel <ref type="bibr" target="#b10">(Das et al., 2015)</ref>.</p><p>Despite its versatility and adoption across a vast range of domains, the LDA model is not quite suitable for many data examples. A geometric illustration of this assertion can be seen in Figure <ref type="figure" target="#fig_0">1</ref>. As will be elaborated in Section 2, the LDA model assumes that the document means lie on a single polytope, the convex hull of the topics. This polytope is commonly called topic simplex <ref type="bibr" target="#b7">(Blei et al., 2003)</ref>, and more generally population polytope <ref type="bibr" target="#b24">(Nguyen, 2015)</ref>. However, a single such topic polytope might not fully explain the latent structure in the corpus. In Figure <ref type="figure" target="#fig_0">1</ref>, the left two figures are the 2-dimensional PCA projections of the documents (color indicating category) of a subset of the 2016 New York Times corpus, where each point represents a particular document (the document-wise mean proportion of words). The figure shows that there is clearly a more complex structure in the corpus which may be not captured by using a single polytope. This motivates a "mixture of topic polytopes" approach pursued in this work. Moreover, the mixture components carry additional structures. For instance, although the sports and world categories roughly lie on disjoint polytopes, these two polytopes should share at least one vertex in the middle, which captures words common to both of these topics. Furthermore, within sports, the soccer (shown in orange) and hockey (shown in red) documents also share some topics, which are unique to sports. In addition, some have topics unique to themselves. This motivates us to use topic polytopes sharing vertices (or lower dimensional faces, using the language of convex geometry) to model the topics. Note that when fitting the topic models, we did not take the categories into consideration. The visualization in Figure <ref type="figure" target="#fig_0">1</ref> is only to motivate the need of more complex topic models. The two right images in the figure represent the unlabelled documents along with learnt topic polytopes (shown with dotted blue edges) using our model, which exhibits a finer understanding of the organization of topics in this corpus, and demonstrates that such learning from unlabelled data is possible in practice.</p><p>A different and interesting perspective of topic modeling extensions came from the original proposal of <ref type="bibr" target="#b5">Blei et al. (2010)</ref>, which focused on the learning of potential latent hierarchies among the topics using a nonparametric Bayesian prior distribution on (infinitely branched) random tree structures. These authors were motivated by a notion of hierarchy among topics in which some topics may be more significant than others, and proposed to use a tree structure to represent such a hierarchy among the topics. As an example with respect to the data used for Figure <ref type="figure" target="#fig_0">1</ref>, soccer and hockey might be naturally considered as sub-topics of the topic sports. Using the tree-theoretic language, soccer and hockey may be represented as children of the vertex sport. Thus, tree structures are natural and economical means for representing the relationship among topics. A probabilistic model which incorporates such a latent topic hierarchy can lead to not only a better understanding of the semantics of topics, thereby improving their interpretability, but also facilitates text categorization and reduces the strong reliance on pre-processing, as required by most existing topic models.</p><p>The primary goal of this paper is to study whether such a latent and abstract topic hierarchy can be learned from unlabelled data and how efficiently. To this end, we investigate the connection between topics arranged in a rooted tree structure in the sense of <ref type="bibr" target="#b5">Blei et al. (2010)</ref> and the geometric perspective of multiple topic polytopes sharing vertices described earlier. We ask, in particular, is the latent topic hierarchy even identifiable to begin with? This question remains unexplored, to the best of our knowledge. It is quite challenging, even when it is relatively simple to specify a latent structure of interest, because the relationship between the latent quantities and observed data is not very transparent in the associated statistical models. Understanding identifiability in such complex latent variable models is crucial for meaningful estimation of parameters (topics arranged in a tree for example), and for obtaining the learning rates. The approach taken in our work is to understand the geometry of the latent topic hierarchies arising from the sharing of topics across documents, and to establish conditions under which such a latent abstract topic structure is identifiable and can be learnt consistently from the corpus.</p><p>To address the theoretical issues associated with the learning of topic hierarchies, we focus on a class of probabilistic topic models, namely tree-directed topic models, which highlight the setting that the topic hierarchy be represented as a finite and rooted tree structure. Although our topic modeling formulation is inspired by the nonparametric modeling of <ref type="bibr" target="#b5">Blei et al. (2010)</ref>, its parametric character is relatively simpler to enable us to address rigorously the question whether such a tree-based hierarchical structure in the latent topics can be identified and learnable from the data. The tree-directed topic model is still quite complex to be a good representative of the broad class of latent structured topic models of interest. The finite tree assumption allows the question on identifiability and topic hierarchy interpretability to be addressed satisfactorily. It is not too restrictive in practice, as most computational techniques employ parametric approximation during some steps, and interpretability of the topic hierarchy makes practical sense only for a finite number of components. The rooted tree structure allows for economical and intuitive interpretations and a clean theoretical treatment, although such a constraint can be relaxed in a future work.</p><p>An instructive view of the tree-directed topic model studied in this paper is that of a mixture of the LDAinduced components, but with a distinction: the unobserved tree structured topic hierarchy imposes certain restrictions on how the topics associated to each component of this mixture are shared. Identifying and learning such a latent tree structured topic hierarchy consists of two aspects: (i) identifying and learning of the rooted tree structure and (ii) obtaining a topic map, a fundamental notion that we will define formally in the sequel. Such a topic map embeds the abstract tree structure into a space of "meanings", i.e., a collection of topics represented as probability distributions over the vocabulary, along with constraints on the sharing structure. It is the marriage of the rooted tree structures with the topic semantic formalism that constitutes the heart of this work, and requires a major portion of our technical effort.</p><p>Summarizing, in this paper we shall present the following contributions.</p><p>(i) Formulation of a class of rich and computationally tractable topic models, namely tree-directed topic models, which incorporates identifiable and interpretable notions of topic hierarchy.</p><p>(ii) An identifiability theory for the latent topic hierarchy arising in the tree-directed topic models, under the large document size regime and other suitable regularity conditions.</p><p>(iii) Establishing bounds on the posterior contraction rates for the latent variables representing the topic structure arising from the empirical data.</p><p>We additionally complement our model and theoretical study with a collapsed Gibbs sampler-based inference algorithm and extensive simulation and a real data analysis based on New York Times corpus. It will be shown that our model can capture interesting semantic structures in the corpus. Our work establishes that such complex latent structures can indeed be learnt from data under suitable regularity conditions, and under a well-specified model setting, tools developed here can be used to provide bounds on the learning rates of the parameters of interest. For this theory, we adopt the general framework for the Bayesian asymptotics treatment <ref type="bibr" target="#b11">(Ghosal and van der Vaart, 2017)</ref>, while the considerable novelty lies in extending the approach pursued in <ref type="bibr" target="#b24">Nguyen (2015)</ref> for a single convex polytope to the setting of a mixture of convex polytopes. The remainder of the paper is organized as follows. Section 2 provides the necessary background on topic modeling and graph-theoretic formalism that will be crucial in the rest of the paper. The tree-directed topic models will be introduced in Section 3, along with examples and geometric structures of the distributions such models induce. Section 4 investigates questions of identifiability. Section 5 presents results on the model's posterior contraction behavior. We provide simulation studies and numerical experiments with real data in Section 7. Concluding remarks are given in Section 8. Complete proofs are relegated to the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notations:</head><p>R d represents the d-dimensional Euclidean space and ∆ d-1 = {x ∈ R d | x i ≥ 0 ∀i, i x i = 1} represents the (d -1)-dimensional probability simplex. We denote the set {1, 2, . . . , n} by <ref type="bibr">[n]</ref>. For a set of points A ⊂ R d , denote the convex hull of A by convA and affine hull by affA. We use the term simplex to denote the convex hull of K affinely independent points and each of these points are called extreme points of the simplex. We also use polytope to refer to a generic convex polytope S, i.e. convex hull of finitely many points and extreme points of a polytope refer to its vertices, denoted as extrS. We recall the dimension of a polytope to be the dimension of its affine space, defined as the dimension of the unique linear subspace that is a translation of the affine space. For a tuple A = (a 1 , . . . , a n ), we use the notation set(A) to indicate the multiset (unordered collection of objects with repetitions) containing the elements in the tuple.</p><p>For x, y ∈ R d , ∥x -y∥ denotes the L 2 -norm (aka Euclidean norm) and ∥x -y∥ 1 denotes the L 1 -norm. We denote the Euclidean ball centered at x of radius r as B(x, r) := {y ∈ R d : ∥y -x∥ 2 ≤ r} (the ambient space is clear from context). The notation B p (x, r) indicates an Euclidean ball of dimension p, i.e. B(x, r) ∩ A for some p-dimensional affine space A (in this case we mention the supporting affine space on which this ball lies). For probability measures P, Q on R d , we shall denote the total variation distance, Kullback-Liebler divergence, Hellinger distance and q-Wasserstein distance as d TV (P, Q), KL(P |Q), h(P, Q) and W q (P, Q) respectively. We shall mostly denote a probability measure on R d by an upper-case letter , e.g. P and its density by the corresponding lower-case letter, e.g. p. If nothing else is mentioned, the density is with respect to the Lebesgue measure on R d . For a measureable function f : R d → R, denote P f := f (x)P (dx). Finally, for non-empty A, B ⊂ R d , the Hausdorff metric between them is denoted by d H (A, B). For probability distributions, let Mult(n, p) denote the multinomial distribution, Cat(p) the categorical distribution and Dir K (α) the symmetric Dirichlet distribution on ∆ K-1 with parameter α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we provide the necessary background on topic modeling and rooted tree formalism. Our purpose is two-fold. First, we introduce the existing ideas and relevant results in the topic modeling literature. Second, we set up further notations and the setting based on which we shall define and study our model from the following section. In particular, we recollect some notions from graph theory regarding tree and isomorphisms, that will be used extensively in the sequel.</p><p>The data corpus consists of m documents, X 1 , . . . , X m , where each document contains n words, X i = (x i1 , . . . , x in ). The words in the corpus belong to a fixed vocabulary, say <ref type="bibr">[V ]</ref>. We shall use the same notations when introducing our model in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Latent Dirichlet Allocation</head><p>The LDA model provides a probabilistic generative model for the corpus. It posits that there are K topics, specifically, θ 1 , . . . , θ K ∈ ∆ V -1 , and each document carries all these topics in varying proportions. For each document indexed by i = 1, . . . , m, first a document-specific allocation (proportion) β i ∈ ∆ K-1 is generated (the LDA assumes β i ∼ Dir K (α) for some α ∈ R K + i.i.d. across documents) and then conditionally given the β i , words in the document are i.i.d. from the categorical distribution with parameter Θ ⊤ β, where Θ ∈ R K×V , obtained by stacking the topics as rows, i.e., for i ∈</p><formula xml:id="formula_0">[m], j ∈ [n], v ∈ [V ], p(x ij = v|θ, β i ) = k β ik θ kv .</formula><p>It is more common to express the last line equivalently in terms of the latent discrete topic assignment variable, which facilitates designing Gibbs sampler based inference methods:</p><formula xml:id="formula_1">z ij |β i ∼ Cat(β i ) x ij |Θ, z ij = k ∼ Cat(θ k ).</formula><p>From the above model specification, the joint distribution of a single document can be expressed as</p><formula xml:id="formula_2">p LDA (x|Θ, α) = ∆ K-1 n j=1 v∈[V ] k β k θ kv 1(xj =v) Dir K,α (dβ) = ∆ K-1 v∈[V ] ⟨θ •v , β⟩ mv Dir K,α<label>(dβ)</label></formula><p>,</p><formula xml:id="formula_3">where θ •v = (θ 1,v , . . . , θ K,v ) ⊤ and m v = j 1(x j = v)</formula><p>is the number of times word v appear in the document. We shall write X ∼ LDA(Θ, α) to denote that a document X follows the above distribution. Let us write S = conv(Θ) to be the topic simplex. The Dirichlet allocation induces a pushforward measure G = L # Dir K,α , where</p><formula xml:id="formula_4">L : ∆ K-1 → S by L(β) = k β k θ k .</formula><p>In terms of this measure, the joint distribution of a document can be expressed as</p><formula xml:id="formula_5">p LDA (x|Θ, α) = S v∈[V ] η mv v G(dη).</formula><p>This can be visualized as follows: the Dirichlet distribution induces a probability measure on the topic simplex S, a document first chooses η ∼ G and then draws words as conditionally i.i.d. observations from a categorical distribution with parameter η. As n → ∞ (document size increases), the distribution of η, defined as ηv = m v /n, converges to G in probability. Furthermore, based on the above display, G may be viewed as the latent mixing measure for the LDA, borrowing the notion from mixture models:</p><formula xml:id="formula_6">p(x) = k π k f (x|θ k )</formula><p>, where f is some probability kernel and the mixing measure is <ref type="bibr" target="#b23">Nguyen (2013)</ref>; <ref type="bibr" target="#b15">Ho and Nguyen (2016)</ref> for details. Note that G is discrete and finitely supported for mixture models and the choice of kernel being multinomial.</p><formula xml:id="formula_7">G = k π k δ θ k so that p(x) = f (x|η)G(dη) -see</formula><p>The LDA has been studied extensively in recent years -identifiability has been studied using various tools and estimation rates have been proved under various assumptions. In the non-degenerate case assuming linear independence of {θ k }, <ref type="bibr" target="#b0">Anandkumar et al. (2012)</ref> applied the method of moments with tensor decomposition techniques to obtain parametric rates for recovering underlying topics -they also established that under linear independent, n = 3 words per document is enough for identifiability, if ᾱ = k α k is known. Under stronger 'anchor word' type assumptions, <ref type="bibr" target="#b1">Arora et al. (2012)</ref> developed algorithms beyond spectral decomposition of empirical tensors. However, if the frequency of anchor words was not that common, <ref type="bibr" target="#b19">Ke and Wang (2024)</ref> were able to improve on this rate with a singular value decomposition based technique. Meanwhile, under similar assumptions, <ref type="bibr">Bing et al. (2020a)</ref> developed estimators and established minimax lower rates for these estimators if the topics were dense and the number of topics were unknown or if the topic models were sparse <ref type="bibr" target="#b3">(Bing et al., 2020b)</ref>. As an alternative approach, <ref type="bibr" target="#b8">Chen et al. (2023)</ref> relaxed the separability condition.</p><p>Instead, they developed an estimator based on implicitly minimizing the volume of the polytope. Assuming the data is sufficiently scattered, the model is identified regardless of the number of topics or vocabulary size.</p><p>Under a Bayesian perspective, <ref type="bibr" target="#b24">Nguyen (2015)</ref> </p><formula xml:id="formula_8">establish a log n n + log m m + log m n -1/2</formula><p>rate for estimating the topic polytope under the Hausdorff metric (thus, only topics which are extreme points of the convex hull are identified) -this approach, while requiring n to increase, requires milder assumptions on the topics and applies even if the underlying Dirichlet distribution is replaced by any smooth probability distribution on ∆ K-1 , placing sufficient mass near boundary (α-regularity). Meanwhile, <ref type="bibr" target="#b22">Lijoi et al. (2023)</ref> provided theoretical insight on this type of model from the Bayesian nonparametric perspective. In contrast to the 'increasing n' regime in <ref type="bibr" target="#b24">Nguyen (2015)</ref>; <ref type="bibr" target="#b28">Tang et al. (2014)</ref>, the work in Wang (2019) considered a 'fixed n' setting, and studied a slightly weaker notion of finite identifiability in the LDA and established parametric rate for MLE of the topics, under the knowledge of the underlying Dirichlet distribution (their result applies if Dirichlet distribution is replaced by any symmetric probability measure ν 0 on ∆ K-1 satisfying a mild moment condition). The optimal achievable rate for estimation of all topics in the LDA with unknown α in terms of m and n (as conjectured in <ref type="bibr" target="#b34">Wang (2019)</ref>, the actual behavior of the MLE should be a combination of both m and n -higher n might deliver faster rates) is still an open question.</p><p>Algorithms for inference in topic models also have a rich literature. Variational inference was proposed for the LDA in <ref type="bibr" target="#b7">Blei et al. (2003)</ref>, and in an online fashion <ref type="bibr" target="#b16">(Hoffman et al., 2010)</ref>, however collapsed Gibbs sampler <ref type="bibr" target="#b12">(Griffiths and Steyvers, 2004)</ref> and spectral decomposition methods <ref type="bibr" target="#b0">(Anandkumar et al., 2012)</ref> have also been used extensively. Furthermore, the geometric structure of the LDA has been exploited to provide much faster inference algorithms such as Voronoi Latent Admixture (VLAD) <ref type="bibr" target="#b37">(Yurochkin et al., 2019)</ref> and Geometric Dirichlet Means (GDM) <ref type="bibr" target="#b38">(Yurochkin and Nguyen, 2016)</ref>, which are shown to be consistent under certain conditions. For the nonparametric topic model in <ref type="bibr" target="#b5">Blei et al. (2010)</ref>, a collapsed Gibbs sampling based inference method was proposed. On the other hand, while much faster, variational inference methods for nonparametric LDA <ref type="bibr" target="#b32">(Wang and Blei, 2009;</ref><ref type="bibr" target="#b33">Wang et al., 2011)</ref> do not come with many theoretical guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Trees and Isomorphisms</head><p>We assume familiarity with basic notions from graph theory, such as nodes/vertices, edges, directed/undirected graphs, degree, trail, cycle, tree (see, e.g., <ref type="bibr" target="#b14">Harary (1969)</ref>). Now, we define a directed rooted tree (DRT) as follows, which will serve in this paper as the fundamental structure controlling the topic hierarchy.</p><p>Definition 2.1. A directed rooted tree T is a directed graph (V, E) such that the underlying undirected graph is a tree and the directed graph has a unique node v 0 (to be called root), with inward-degree 0.</p><p>A DRT will be denoted by T = (V, E, v 0 ). We remark that this tree is an abstract graph and has no connection at this point to the data corpus. Without loss of generality, we can take V = [K], where K = |V| is the number of nodes and we shall use this in the remainder of the paper. Define a maximal path of T as φ = (v 0 , v 1 , . . . , v L-1 ), where (v i , v i+1 ) ∈ E and v L is a leaf of T , i.e., any path of the tree starting at the root and ending at a leaf (nodes with 0 outward degree) -we shall simply call these paths by slight abuse of terminology. For such a path φ, we say L is its length. Let I be the number of leaves of T , then clearly we have I paths -let J 1 , . . . , J I be the lengths of these paths. Define the size of T as the tuple (I, J, K), where J = set(J 1 , . . . , J I ) and referred to as depth set of T . Let Φ(T ) denote the set of all paths of T . Finally, for any function f : V → Ω (for any arbitrary set Ω, e.g., R) and a path φ = (v 0 , . . . , v L-1 ) ∈ Φ(T ), we write</p><formula xml:id="formula_9">f (φ) := (f (v 0 ), f (v 1 ), . . . , f (v L-1 )) ∈ Ω L .</formula><p>Next, we state the definition of graph-isomorphism, specifically for DRTs.</p><formula xml:id="formula_10">Definition 2.2. T = (V, E, v 0 ) and T ′ = (V ′ , E ′ , v ′ 0 ) are isomorphic if and only if there exists a bijection σ : V → V ′ such that (u, v) ∈ E ⇐⇒ (σ(u), σ(v)) ∈ E ′ .</formula><p>If T and T ′ are isomorphic, we write T ∼ = T ′ . Notice that the definition implies that σ(v 0 ) = v ′ 0 and that any such bijection (if exists) must map a leaf of T to a leaf of T ′ . We say that DRTs T and T ′ have the same structure if they are isomorphic. Since isomorphism is an equivalence relation on the space of DRTs, we wish to think of the equivalence classes as representing a particular 'tree structure'. We also note that for T ∼ = T ′ , the bijection is typically not unique.</p><p>Our probabilistic model, to be defined in the following section, would associate each document to a particular path of the DRT and then, that document would be following a latent Dirichlet allocation model using topics only from that path. While this will be made more precise in the next section, we make a quick remark that the paths of the DRT play important role in our model. Two paths might share a few nodes, meaning that topics associated with these nodes are shared between documents that are assigned to these paths. This topic-sharing structure as induced by the DRT is connected solely to the structure of the DRT. In particular, we wish to identify the tree structure solely from the collection of sets of nodes from each path. The following lemma shows that two non-isomorphic DRTs cannot give rise to the same collection of paths. The proof of this result is deferred to the Appendix, where we also provide further details about DRTs.</p><p>Lemma 2.1. Given two DRTs T , T ′ with the same V, we have</p><formula xml:id="formula_11">{set(φ) | φ is a path in T } = {set(φ ′ ) | φ ′ is a path in T ′ } ⇐⇒ T ∼ = T ′ .</formula><p>Moreover, T = T ′ if additionally, either T or T ′ satisfies the following: every element v in V except the leaves has at least 2 children.</p><p>3 Tree-Directed Topic Models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model formulation</head><p>Although it is simple to describe the probabilistic model rather quickly, explicating their likelihood structure and the underlying geometry requires some effort. In particular, to incorporate the DRT structure into the probabilistic model, we need to following concept. Definition 3.1. Given a DRT, T = (V, E, v 0 ), any injection map, ρ : V → ∆ V -1 , is called a topic map on T .</p><p>Given a DRT, T = (V, E, v 0 ), of size (I, J, K) and a topic map ρ on it, we are ready to describe the model. Denote a generic document with n words by X [n] and the corpus with m such documents by X</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[m]</head><p>[n] , with X ℓj , ℓ ∈ [m], j ∈ [n] representing the jth word in the ith document. Fix an enumeration of the paths of T , say φ 1 , . . . , φ I , and let π : Φ(T ) → [0, 1] be such that π(φ i ) &gt; 0 for all i ∈ [I] and i π(φ i ) = 1. Thus, if we abuse the notation to take π i = π(φ i ) and π = (π 1 , . . . , π I ) ⊤ ∈ R I , then π ∈ ∆ I-1 represents the path probability parameter. First, for each document, a path of T is chosen according to π, i.e.,</p><formula xml:id="formula_12">c 1 , . . . , c m iid ∼ Cat(π).</formula><p>Here, c ℓ ∈ [I] is a discrete random variable indicating the path chosen for document X ℓ . In other words, c ℓ = i suggests that the path φ i is chosen for the ℓ-th document. Now, given the path associated with a document, words in that document are generated in the same way as the LDA, using only topics associated with that path under the topic map ρ. More precisely, let Θ i = ρ(φ i ) ∈ R Ji×V such that the rows are the topics associated to the nodes along this path. Recall that J i denotes the length of path φ i and α i ∈ R + , i ∈ [I] the scalar parameters for the Dirichlet distribution. Let the kth row of Θ i be denoted by Θ ik . Then, the remainder of the generate model specification for words in document ℓ is:</p><formula xml:id="formula_13">β ℓ |c ℓ = i ∼ Dir Ji (α i ) z ℓ,j |β ℓ iid ∼ Cat(β ℓ ), j ∈ [n] x ℓ,j |Θ i , z ℓ,j = k ∼ Cat(Θ i,k ).</formula><p>The above four lines defining our model can be written succinctly and equivalently as: </p><formula xml:id="formula_14">c ℓ ∼ Cat(π), and X ℓ |c ℓ = i ∼ LDA(Θ i , α i ).</formula><formula xml:id="formula_15">p LDA ( ⋅ | {θ 1 , θ 2 , θ 5 }, α)</formula><p>Figure <ref type="figure">2</ref>: The basic ingredients of the tree-directed topic model. The left figure shows the underlying DRT, an abstract DRT with a single root v 1 , the middle shows topic hierarchy resulting from the DRT and a topic map, ρ(v i ) =: θ i , and the right shows an example of the generation of a document given a path chosen categorically with probability, π.</p><p>We remark that J i could potentially be different and as a result, the Dirichlet distributions for different paths might not even be same dimensional. If all J i are equal to a common value J, then we could use α 1 , . . . , α I ∈ R J as parameters for Dirichlet distributions without assuming symmetric. More generally, we could assume α i ∈ R Ji + as parameters. However, in this paper, we are more interested in the identifiability and rates for the topics (defined via the topic map ρ). Thus, we choose a simple symmetric Dirichlet distribution with a single parameter as the basis for the model, which we denote as Dir K . It is important to note from the outset that the Θ i 's share constraints imposed by the underlying DRT T . For instance, Θ i1 = Θ i ′ 1 for any i, i ′ ∈ [I], since it is always ρ(v 0 ) (topic associated to the root). Thus, the "root topic" is shared across all the paths. On the other hand, Θ iJi ̸ = Θ i ′ J i ′ for any two i ̸ = i ′ , since each leaf of T appears in a unique path and ρ is injective. This structure will be revisited shortly.</p><p>Summarizing, we assume that the documents are i.i.d. coming from the above generative model. The words in each document are conditionally i.i.d. and hence exchangeable. Marginalizing out the discrete latent variable z, we have the equivalent form of the model as</p><formula xml:id="formula_16">c ℓ ∼ Cat(π), β ℓ |c ℓ = i ∼ Dir Ji (α i ), X ℓj |β ℓ , Θ ∼ Cat   k∈[Ji] β ℓk Θ i,k   .</formula><p>It follows that the density of a document X [n] with respect to the count measure on</p><formula xml:id="formula_17">[V ] n is p(X [n] |T , ρ, π, α) = i∈[I] π i p LDA (X [n] |Θ i , α i ) = i∈[I] π i ∆ J i -1 v∈[V ] ⟨Θ i•v , β⟩ mv Dir Ji,αi (dβ),<label>(1)</label></formula><p>where a particular enumeration of the paths φ i , i ∈ [I] is (arbitrarily) fixed, Θ i = ρ(φ i ) and π i , α i are the probability associated to φ i and Dirichlet parameter for the ith path respectively, and</p><formula xml:id="formula_18">m v = j 1(X j = v) is the count of the number of occurrences of word v ∈ [V ] in the document. We note that Θ i•v = (Θ i1v , . . . , Θ iJiv ) ∈ R Ji is the v-th column of Θ i .</formula><p>We denote this density by p n,ω , where ω contains all the relevant parameters and denote the corresponding probability measure on [V ] n by P n,ω . Thus, based on the above display, our model can be viewed as a mixture of the LDA induced distributions, but the LDA components are constrained via shared topics imposed by the latent DRT T and topic map ρ to be learned. It is this sharing that makes the model interesting and (as we shall see) the latent topic hierarchy (tree) learnable and interpretable.</p><p>Remark 1. A particular topic tree leads to a particular sharing structure of the associated topics across the documents. The following illustration will be helpful. For the topic tree shown in Figure <ref type="figure">2</ref>, each document only uses 3 topics from the collection of 7 total topics. However, documents cannot use any 3 topics -each</p><formula xml:id="formula_19">v 1 v 2 v 3</formula><p>Figure <ref type="figure">3</ref>: Illustration of DRT and topic hierarchy with fixed set of topics Θ following the example in text: (top row) two DRT of size (I = 4, J = 2, K = 7); topic maps ρ 1 : v ℓ → θ ℓ and ρ 2 : v ℓ → θ 8-ℓ induce different topic hierarchies (middle row); (third row) shows how these topic hierarchies induce sharing of topics across documents, the topics are the black dots inside the vocabulary simplex (big bounding black triangle here). Each triangle (in different colors) represents a component topic polytope corresponding to a particular path in the tree (the respective leaf is highlighted with the same color). The size of the tree has the following effects: I = 4 is the number of polytopes, J = 2 is the dimension of each polytope and K = 7 is the total number of topics.</p><p>tuple of valid 3 topics arises from the paths of the hierarchy. Thus, the topic θ 1 is carried by all documentsthis root topic can be seen as a topic placing high mass on words that occur frequently across all documents in the corpus (e.g., stop words, articles etc.). Meanwhile θ 2 is carried by a certain proportion of documents.</p><p>On the other hand, if a document uses the topic θ 5 , then it has to also use the topics θ 2 , θ 1 . This captures the idea that a document about soccer (a type of sport) must also include a more concrete topic about sports (and also stop words) -this general topic about sports would place higher mass on words that occur frequently in all sports-related documents. A document about basketball (another type of sport) will share the sports topic as well as stop-words topic, while differing from the previous document in one topic -thus the topics about soccer and basketball are sub-topics of the sports topic, and capture specialized semantics within the particular category of sports.</p><p>Remark 2. We make the following additional remarks regarding the model formulation.</p><p>1. The model size is connected to the tree size as follows. If T has size (I, J, K), then there are K total topics for the model and I components (seen as a mixture as shown above). J captures the dimensionality of each of the component LDA models.</p><p>2. The LDA is obviously a special case, with a DRT T of size I = 1, J = K (a linear tree).</p><p>3. The Dirichlet distribution plays no special role in this paper other than in designing the Gibbs sampler.</p><p>Other parametric families of identifiable distributions on ∆ Ji-1 can be used in place of the Dirichlet. This particular family is considered because of its popularity in topic models and also for ease in designing inference algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model's geometry</head><p>Consider a particular component of the model, say i, corresponding to path φ i of T . Let</p><formula xml:id="formula_20">G i = Dir Ji (α i ) # L i , where L i : ∆ Ji-1 → ∆ V -1 is defined as L i (β) = Θ ⊤ i β for β ∈ ∆ Ji-1</formula><p>. G i is the push-forward measure on ∆ V -1 of a Dirichlet distribution under the linear transformation L i . Note that G i is supported on S i := convΘ i , the component topic polytope corresponding to the path φ i . The Dirichlet distribution for β ∈ ∆ Ji-1 induces the distribution G i for η = k∈[Ji] β k Θ ik . If J i ≤ V , Θ i are almost surely linearly independent, in this case L i has rank J i -1 and G i admits the following density with respect to the</p><formula xml:id="formula_21">J i -1 dimensional Hausdorff measure H Ji-1 on ∆ V -1 g i (η) = Dir Ji (L -1 i (η)|α i )J(L) -1</formula><p>, where J(L) is the Jacobian of the linear map. On the other hand, if J i ≥ V , then L i is generally (V -1)-ranked and G i has the following density with respect to the (J i -V ) dimensional Hausdorff measure on R V , given by</p><formula xml:id="formula_22">g i (η) = L -1 i (η) Dir Ji (β|α i )J(L) -1 H Ji-V (dβ).</formula><p>Using change of variable, we can write p n,ω in Eq. ( <ref type="formula" target="#formula_17">1</ref>) in the following alternate way, in terms of these latent measures G 1 , . . . , G I :</p><formula xml:id="formula_23">p(X [n] |T , ρ, π, α) = i∈[I] π i Si v∈[V ] η mv v G i (dη) = ∪iSi v η mv v   i∈[I] π i G i   (dη).<label>(2)</label></formula><p>This can be visualized as follows. For each path φ i of T i , there is a component polytope S i (formed as the convex hull of topics only from this path) and the model imposes a continuous distribution G i supported on S i as described above. For each document, first a component is chosen with probability π, given this choice, a document-specific η ∈ ∪ i S i ⊂ ∆ V -1 is chosen from the corresponding G i , conditional on which, the words in the document are i.i.d. categorical. This equivalent data generating model for a single document is expressed below</p><formula xml:id="formula_24">c|π ∼ Cat(π) η|c = i, G 1 , . . . , G I ∼ G i X j |η ∼ Cat(η).</formula><p>Remark 3. The above representation suggests a connection to mixture models with a key distinction. Standard mixture models are represented by i π i f (x|θ i ), where f is some probability kernel's density function (e.g. Gaussian mixture model corresponds to a Gaussian kernel, θ i = (µ i , Σ i ) capture the parameters of the components). The idea of a latent mixing measure is used in the theoretical analysis for the standard mixture model <ref type="bibr" target="#b23">(Nguyen, 2013)</ref>, this measure is G = i π i δ θi , a discrete probability measure capturing all information about the unknown parameters. In terms of G, the model can be written as i π i f (x|θ i ) = f (x|η)G(dη). Based on the representation in the above display in Equation (2), in our case, we can consider G = i π i G i as the latent mixing measure and the kernel being a product of multinomials (product comes for the n words in the document). However, in our case, this measure is neither discrete nor has a density with respect to Lebesgue measure on ∆ V -1 -this is because the G i are supported on different affine spaces of potentially different dimensions.</p><p>Example 1. We illustrate the model and associated notions with a concrete example, illustrated in Figure <ref type="figure">3</ref>. Consider a fixed set of K = 7 topics θ 1 , . . . , θ K (black dots inside the vocabulary simplex ∆ V -1 with V = 3, shown as the big triangles in the bottom row of Figure <ref type="figure">3</ref>). These topics can lead to very different probability distributions for the corpus under our model, based on the DRT and topic map. We demonstrate two such DRTs (first row in Figure ). Let us walk through the model under the left DRT with topic map ρ 1 (which maps v i → θ i ). There are 4 paths in the DRT, and the collections of topics along these paths are {{θ 1 , θ 2 , θ 4 }, {θ 1 , θ 2 , θ 5 }, {θ 1 , θ 3 , θ 6 }, {θ 1 , θ 3 , θ 7 }} (leaf nodes colored red, yellow, blue, green respectively). Each of these paths induce a measure G i supported on a component polytope (corresponding colored triangle inside the vocabulary simplex in the bottom row). The DRT and topic map together impose the topic hierarchy, which in turn, induce a sharing structure of the topics among the components. Clearly, the topic θ 1 is shared by all components, while θ 2 is only shared between 2 components. Note the effect that a different topic map (ρ 2 in the figure) has on this sharing structure of the topics among the components. And clearly, a different DRT has a totally different topic hierarchy and hence sharing structure, e.g. for the DRT on the right, θ 3 is shared by 3 component polytopes (no topic has this behavior for the DRT on the left). For each document, first one of these component polytopes is chosen. Then η is generated from the corresponding G i and conditionally, words in that document are generated from this η. Thus, the sharing structure of topics under the DRT and topic map, also induce how topics are shared across documents -for two documents, one associated to the first path and one to the second in the left-most example in Figure <ref type="figure">3</ref>, they both use {θ 1 , θ 2 }, however the former uses topic θ 4 , while the latter uses θ 5 (which are unique to these components).</p><p>Finally, the distribution of a corpus of i.i.d. documents</p><formula xml:id="formula_25">X [m] [n] = (X 1 [n] , . . . , X m [n]</formula><p>) can be expressed by</p><formula xml:id="formula_26">p(X [m] [n] |T , ρ, π, α) = ℓ∈[m] p(X ℓ [n] |T , ρ, π, α).</formula><p>For brevity, for the rest of the paper this density is also denoted by p X [m]</p><p>[n] |ω where ω captures all the parameters and the corresponding probability measure on [V ] m×n is denoted by P X [m]</p><p>[n] |ω . Based on the geometric viewpoint and conditional i.i.d. nature of the words, if we let ηℓ ∈ ∆ V -1 denote the document mean for X ℓ</p><p>[n] , i.e., ηℓv = j∈[n] 1(X ℓj = v)/n, then ηℓ → η ∼ G almost surely as n → ∞. We shall exploit this together with the representation of G as a convex combination of low-dimensional probability measures in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Identifiability</head><p>We are ready to investigate the identifiability of the latent structures arising in the tree-directed topic models. Our approach exploits the underlying geometry of the model and essentially shows that any potential latent measure G = i π i G i can be uniquely decomposed into the G ′ i s, from which the DRT and the topic map are identified uniquely. This approach requires that the document length is sufficiently large, and the asymptotic regime of n → ∞, while having the advantage of requiring only very mild assumptions on topics and other related quantities. A possible alternative approach to study identifiability would be through the method of moments (up to a certain order). We pursue the former approach by focusing on the geometry of a collection of polytopes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Topic hierarchies</head><p>A primary object of inferential interest is the topic hierarchy, which is composed of both the actual position of the topics in the vocabulary simplex as well as the way they are shared across components -up to this point, we have only defined isomorphism between DRTs. We begin with a brief discussion of the notion of similar topic hierarchy here, which would be important in the sequel. As discussed in Section 3.2, given a DRT T = (V, E, v 0 ) and a topic map ρ, one obtains an induced topic tree T = ( Ṽ, Ẽ, θ 0 ) where Ṽ = (ρ(u) : u ∈ V}, θ 0 = ρ(v 0 ) and (ρ(u), ρ(u ′ )) ∈ Ẽ ⇐⇒ (u, u ′ ) ∈ E. In Figure <ref type="figure">2</ref>, the tree in the middle is the induced topic tree arising due to the DRT (left) and the topic map ρ. Geometrically, the component polytopes of the model have precisely θ ∈ Ṽ as the extreme points and θ, θ ′ ∈ Ẽ are extreme points of the same polytope iff θ, θ ′ are both on some maximal path of T . Given (T , ρ), let the induced topic tree be denoted by ξ(T , ρ). The topic hierarchy is formalized as follows:</p><p>Definition 4.1 (Identical Topic Hierarchies). The two sets of DRT and associated topic maps (T , ρ) and (T ′ , ρ ′ ) are said to have identical topic hierarchy if ξ(T , ρ) ∼ = ξ(T ′ , ρ ′ ) and there is a unique isomorphism σ : ξ(T , ρ) ↔ ξ(T ′ , ρ ′ ) such that θ = σ(θ) for all nodes θ in ξ(T , ρ). Remark 4. The condition in Definition 4.1 conveys the intuition that the topic (tree) hierarchies are precisely induced by the DRT and the topic maps. Requiring only isomorphic topic trees is insufficient since they capture only the sharing structure, not the actual topics. Note that while this condition implies that T ∼ = T ′ , i.e., the underlying DRTs are isomorphic, Figure <ref type="figure">3</ref> gives examples where the same DRT might lead to different topic hierarchies, even when the set of topics are same.</p><p>The remark suggests that this notion of topic hierarchy is rigid and does not allow flexibility. Consider the examples in Figure <ref type="figure">4</ref>, and assume ∥θ 2 -θ 5 ∥ &lt; ϵ and ∥θ 4 -θ 6 ∥ &lt; ϵ in the figure. The first three examples show parameters which all lead to two components (all the polytopes are embedded in the vocabulary simplex ∆ V -1 ), each being a triangle (or almost) and sharing an edge (or almost) and moreover, the positions of the corresponding topics are similar. This motivates our next definition. Definition 4.2 (Approximate Topic Hierarchies). The two sets of DRT and associated topic maps (T , ρ) and (T ′ , ρ ′ ) are said to share ϵ-approximate topic hierarchies, for some ϵ &gt; 0, if there exist partitions P of Ṽ and P ′ of Ṽ′ with |P| = |P ′ |, where Ṽ (resp. Ṽ′ ) is the set of nodes in ξ(T , ρ) (resp. ξ(T ′ , ρ ′ )), such that ∃ bijection τ : P ↔ P ′ satisfying θ ∈ P ∈ P, θ ′ ∈ τ (P ) ∈ P ′ ⇒ ∥θ -θ ′ ∥ &lt; ϵ. Moreover, for every path (θ 1 , . . . , θ J ) in ξ(T , ρ) such that θ j ∈ P j ∈ P, there exists a path (θ</p><formula xml:id="formula_27">′ 1 . . . , θ ′ J ′ ) such that θ ′ j ′ ∈ ∪ j∈[J] τ (P j ) for every j ′ ∈ [J ′ ],</formula><p>and vice versa.</p><p>To illustrate the idea of the partition as described in the preceding definition, consider again Figure <ref type="figure">4</ref>. We argue that the topic hierarchies in examples 2 and 3 are approximately close. Consider the partition P = {{θ 1 }, {θ 4 , θ 6 }, {θ 2 , θ 5 }, {θ 3 }} for example 2 and</p><formula xml:id="formula_28">P ′ = {{θ 1 }, {θ 4 }, {θ 2 }, {θ 3 }} for example 2. The bijection {θ 1 } ↔ {θ 1 }, {θ 4 , θ 6 } ↔ {θ 4 }, {θ 2 , θ 5 } ↔ {θ 2 }</formula><p>and {θ 3 } ↔ {θ 3 } between P and P ′ satisfies the condition in the preceding definition, which shows that these two topic hierarchies are approximately close. Similarly, the topic hierarchies from example 1 and example 2 in Figure <ref type="figure">4</ref> are also approximately close. We remark that while having the same topic hierarchy implies isomorphism of the underlying DRTs, this is relaxed for the ϵ-approximate topic hierarchies: clearly, the DRTs in examples 1 and 3 are neither isomorphic, nor is one a sub-tree of the other. On the other hand, the topic hierarchy in example 4 is not approximately close to any of those in the former three examples. To see this, comparing to say, example 2, consider the partition {{θ 1 }, {θ 4 , θ 6 }, {θ 2 }, {θ 3 }} for example 4. While it has a bijection with P (for example 2) satisfying the first part of Definition 4.2, it violates the second part. The path (θ 1 , θ 4 , θ 6 ) in example 4 uses only two of the sets in the partition, while each path in the topic hierarchy in example 2 uses three distinct sets from its partition.</p><p>Remark 5. One of the simplest forms of ambiguity (non-identifiability) in our model arises due to the permutation of labels (aka topic map), subject to the DRT restrictions. This is similar in spirit to the issue of label switching for mixture models. However, in this model an arbitrary permutation of the labels might not lead to the same model -e.g., consider example 1 in Figure <ref type="figure">4</ref>: a random permutation of the labels of the topics might remove θ 1 from the root, which would lead to a very different sharing structure. For mixture models, this problem is dealt with by considering a suitable metric (e.g. Wasserstein metric on the latent mixing measures) which is agnostic to the ordering of the components. For ours, we construct a suitable metric for this case, which considers all such permutations respecting the structure imposed by the DRTs. Some non-identifiable examples, which cannot be solved by permuting the labels retaining the topic hierarchy are discussed at the end of Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics on topic hierarchies</head><p>Define T(I, J, K) to be the set of DRTs T</p><formula xml:id="formula_29">= (V, E, v 0 ) of size (I, J, K) with V = [K]. Also let T(I) = ∪ J,K T(I, J, K) and T(I, K) = ∪ J T(I, J, K) stand</formula><p>for the set of possible DRTs with I leaves and with I leaves and K nodes, respectively. Finally, T(I, J) (for J ∈ N) denotes the space of all DRTs with I leaves and each of whose paths has the same length J. Given a DRT T , let</p><formula xml:id="formula_30">R(T ) = {ρ : V → ∆ V -1 | ρ is injective, V is</formula><p>the set of nodes of T } be the space of possible topic maps associated with T .</p><p>The collection of all parameters ω of the tree-directed topic model is given by</p><formula xml:id="formula_31">Ω(I, J, K) = {(T , ρ, π) : T ∈ T(I, J, K), ρ ∈ R(T ), π : Φ(T ) → [0, 1] ∋ (π(φ) : φ ∈ Φ(T )) ∈ ∆ I-1 },</formula><p>the space of tuples (T , ρ, π) consisting of a DRT T of size (I, J, K), topic map ρ on T and path-probability π (seen as a map on the set of paths of T ). Similarly, we take Ω(I) and Ω(I, K) to be the set of tuples subject to the appropriate restrictions on T . Finally, we define</p><formula xml:id="formula_32">Ω(T ) = {(ρ, π) : ρ ∈ R(T ), π : Φ(T ) → [0, 1] ∋ (π(φ) : φ ∈ Φ(T )) ∈ ∆ I-1 , where I = |Φ(T )|}</formula><p>to be the space of topic maps and path probabilities defined on a fixed DRT T . Viewing π as a map on Φ(T ) addresses the ambiguity in the enumeration of the paths of T .</p><p>Given ω ∈ Ω(I, J, K) (or any of Ω(I), Ω(I, K), Ω(T )), let p n,ω,α be the probability density (with respect to count measure on</p><formula xml:id="formula_33">[V ] n ) of a length-n document X [n] ∈ [V ]</formula><p>n drawn from the model described in Section 3, and p n,ω,α takes the expression in Eq. ( <ref type="formula" target="#formula_17">1</ref>), where α : Φ(T ) → R + , represents the Dirichlet parameter associated with each path of the DRT. Note that Eq. ( <ref type="formula" target="#formula_17">1</ref>) is agnostic to the choice of enumeration φ 1 , . . . , φ I of the paths of the DRT T , as long as π and α are defined as maps on the set of paths. P n,ω,α is the associated probability measure whose density is p n,ω,α . When α i 's are equal to a common value α ∈ R + and is known, we drop the α from the subscript. We are now ready to define a metric on the parameter space just defined.</p><formula xml:id="formula_34">Definition 4.3. Given ω = (T , ρ, π), ω ′ = (T ′ , ρ ′ , π ′ ) ∈ Ω(I), define d H+ : Ω(I) × Ω(I) → R as d H+ (ω, ω ′ ) = min σ∈S(I) i∈[I] d H S i , S ′ σ(i) + |π i -π ′ σ(i) |<label>(3)</label></formula><p>where π i , S i are the path-probability and component topic polytope associated with path i, given a particular enumeration φ 1 , . . . , φ I of the paths of T (and similarly for π ′ i , S ′ i ) and d H is the Hausdorff metric on the space of non-empty subsets of ∆ V -1 , and S(I) is the set of permutations of <ref type="bibr">[I]</ref>.</p><p>Note that the above definition does not depend on the particular enumerations of paths chosen for T , T ′ . Thus, the defined metric seeks to find an optimal bijection between the paths of the involved DRTs which minimizes the total distance between the corresponding topic polytopes (through the Hausdorff metric) and the path probabilities. The restriction of d H+ to Ω(I, K) ⊂ Ω(I) and Ω(T ) (seen as a subset of Ω(|Φ(T )|)) is also denoted by d H+ . We call d H+ as the augmented tree-directed Hausdorff metric. It is clear that d H+ is non-negative and symmetric. The following lemma also guarantees that it satisfies the triangle inequality.</p><p>Lemma 4.1. For any ω 1 , ω 2 , ω 3 ∈ Ω(I), there holds d</p><formula xml:id="formula_35">H+ (ω 1 , ω 3 ) ≤ d H+ (ω 1 , ω 2 ) + d H+ (ω 2 , ω 2 ).</formula><p>Although the augmented tree-directed Hausdorff metric satisfies non-negativity, symmetry and the triangle inequality, d H+ (ω, ω ′ ) = 0 does not necessarily imply that ω = ω ′ . Nonetheless, this metric is useful for capturing the latent structure in terms of the topic hierarchy -in particular, we will establish that d H+ (ω, ω ′ ) = 0 iff ω and ω ′ induce the same topic hierarchy. The following mild conditions on ω will be required.</p><p>(A1) For any path φ of T , each of ρ(u), for u ∈ φ, is an extreme point of convρ(φ).</p><p>(A2) Any pair of component polytopes are distinct, i.e., for i ̸ = j ∈ [I], S i ̸ = S j .</p><p>These assumptions are jointly on the DRT and topic map. Assumption (A1) states that for each component topic polytope, it must have all its constituent topics as extreme points (vertices which cannot be expressed as convex combinations of other vertices). Thus, we consider topics such that no topic is a convex combination of other topics within a component polytope. Typically K &lt; V , and hence with topics in general positions, this condition is almost surely met. This assumption is much weaker than requiring linear independence of the topics. It is even weaker than assuming affine independence among topics in the same component. Assumption (A2) deals with distinctness of the component polytopes. The next result shows the usefulness of the d H+ metric for understanding topic hierarchies.</p><p>Proposition 1.</p><p>1. If ω 0 = (T , ρ, π) satisfying Assumptions (A1) and ( <ref type="formula" target="#formula_23">A2</ref>) is fixed, then there exists</p><formula xml:id="formula_36">ϵ 0 = ϵ 0 (ω 0 ) &gt; 0 and C 0 = C 0 (ω 0 ) &gt; 0, such that for all ω ′ = (T ′ , ρ ′ , π ′ ) ∈ Ω(I), whenever d H+ (ω 0 , ω ′ ) &lt; ϵ ≤ ϵ 0 , the topic hierarchy of (T ′ , ρ ′ ) is C 0 ϵ-</formula><p>approximately close to that of (T , ρ). If additionally, the number of nodes of T and T ′ are same, then T ∼ = T ′ .</p><p>2. For ω, ω ′ satisfying Assumption (A1), we have d H+ (ω, ω ′ ) = 0 if and only if (T , ρ) and (T ′ , ρ ′ ) have the same topic hierarchy and π(φ) = π ′ (σ(φ)) for every path φ of T , where σ(φ) is the path of T ′ mapped from φ under the unique isomorphism in Definition 4.1.</p><p>Recall that for any isomorphism σ :</p><formula xml:id="formula_37">V → V ′ between isomorphic DRTs T = (V, E, v 0 ), T ′ = (V ′ , E ′ , v ′ 0 ) of ω, ω ′ respectively, σ induces a bijection σ : Φ(T ) → Φ(T ′</formula><p>) between the set of paths of T and T ′ -the last part of the above proposition makes use of this bijection (denoted as σ(φ) by slight abuse of notation). Remark 6. We end our preparation with additional remarks.</p><p>1. As noted before, the isomorphisms between isomorphic DRTs are not necessarily unique (see Section 2). However, the particular bijection in the above proposition is special in being unique -this attests to the fact of the claim that the set of topics {ρ(u) : u ∈ V} and {ρ ′ (u) : u ∈ V ′ } are not just close but are arranged as a tree so that they have the same parent-child structures -in other words, they have the same topic hierarchy.</p><p>2. The first part of the proposition ensures that for fixed ω, whenever another parameter ω ′ comes in a sufficiently small neighborhood of ω, in terms of d H+ metric, then the underlying topic hierarchy of ω must be a subset of the topic hierarchy of ω ′ , i.e., the hierarchy for ω ′ contains the hierarchy information of ω (and possibly some very close other redundant topics). If additionally, K = K ′ is known, then J = J ′ , where T , T ′ have size (I, J, K) and (I, J ′ , K ′ ) respectively.</p><p>3. d H+ is a pseudo-metric on Ω(I). Based on part 2) of the above lemma, on {ω ∈ Ω(I) : ω satisfies (A1)}, the equivalence classes under d H+ correspond to distinct topic hierarchies. Note that Assumption (A2) is only required in the first part. The reason is that in part 2), the condition that d H+ is 0 is strong enough to identify all the components under just assumption (A1).</p><p>4. The converse direction of Proposition 1 is also true, subject to the fact that d H+ is only defined when the underlying DRTs have same number of leaves and the notion of topic hierarchy does not include the path probabilities. We can state the converse direction as follows: if (T , ρ), (T ′ , ρ ′ ) have ϵ-approximate topic hierarchy, then min</p><formula xml:id="formula_38">σ∈S(I) i∈[I] d H (S i , S σ(i) ) &lt; ϵ,</formula><p>where we note that the left side of the above display is basically d H+ as in Definition 3, ignoring the path-probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Identifiability theorems</head><p>Having developed the requisite technical tools, we are ready to describe our main identifiability results in this section. For a standard parametric model {P θ : θ ∈ Θ}, the notion of identifiability boils down to injectivity of the mapping θ → P θ . This is a necessary requirement for obtaining a consistent estimator θm from i.i.d. observations X 1 , . . . , X m ∼ P θ0 , i.e. θm → θ 0 as m → ∞. For our model setting, the data size is determined by m and n and the i.i.d. structure is across documents, not the words inside a document. Using the notations from the previous section, we wish to establish identifiability in the 'increasing n' regime, i.e. we want to ensure that ω can be estimated consistently given</p><formula xml:id="formula_39">X = (X 1 [n] , . . . , X m [n]</formula><p>) as m, n → ∞. Recall from Eq. ( <ref type="formula" target="#formula_23">2</ref>) that for ω ∈ Ω(I),</p><formula xml:id="formula_40">p n,ω,α (X [n] ) = f (X [n] |η)G(dη), f (x 1 , . . . , x n |η) = v∈[V ] η n j=1 1(xj =v) v ,</formula><p>where the kernel f is a n product of multinomials and the mixing measure G = i∈[I] π i G i , where G i is as defined in Section 3.2. More specifically, for every ω = (T , ρ, π) ∈ Ω(I) and α : Φ(T ) → R + , there is a latent mixing measure G(ω, α), based on which we have observations from</p><formula xml:id="formula_41">X [n] ∼ P n,G , with p n,G (x) = f (x|η)G(dη)</formula><p>as above. The reason we represent the model in these two steps, (ω, α) → G(ω, α), G → P n,G , is that in order to identify the latent structure as desired, we hope to identify ω (and possibly α), not just G. This leads to the question whether (ω, α) → G(ω, α) is injective (under the metric we consider on the space for ω) -this is the question we are interested in here. Note that this can be seen as 'increasing n' regime, since as n → ∞, the distribution of X (document mean), defined as Xv = j 1(X j = v)/n, converges weakly to G itself. This is equivalent to the 'noiseless' setting (similar to Javadi and Montanari ( <ref type="formula">2019</ref>)), where we observe i.i.d. η 1 , . . . , η m ∼ G(ω, α) and wish to estimate ω.</p><p>Our approach is to exploit the geometric structure of G to identify ω, noting that G is supported on the union of I convex polytopes. The particular nature of the underlying distribution which induces the measure G i on component polytope S i (the Dirichlet distribution in our case) is irrelevant for this study -any distribution, continuous on the appropriate probability simplex, parametrized by α, works. The following lemma, which deals with intersections of affine spaces and polytopes, is crucial in the results to follow. Lemma 4.2. Suppose A, A ′ are affine spaces and S is a convex polytope 1. S ∩ A is either empty or another convex polytope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A ∩ A ′ is either empty or another affine space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">If dim</head><formula xml:id="formula_42">A = dim A ′ , then A ∩ A ′ either has strictly lower dimension or A = A ′ . 4. If dim A &lt; dim A ′ , then either A ∩ A ′ has dimension strictly less than dim A or A ⊂ A ′ .</formula><p>The proof is straightforward and is included in the appendix for completeness. We state the key assumptions that we require in this section, which are both arguably mild (so as to avoid degenerate situations).</p><p>(B1) For any two maximal paths φ and φ</p><formula xml:id="formula_43">′ , if dim conv(ρ(φ)) = dim conv(ρ(φ ′ )), then aff(ρ(φ)) ̸ = aff(ρ(φ ′ )).</formula><p>(B2) π(φ) &gt; 0 for any path φ in T . Now, we are ready to state the first identifiability theorem.</p><p>Theorem 1. For any two parameter ω = (T , ρ, π) ∈ Ω(I) and ω ′ = (T ′ , ρ ′ , π ′ ) ∈ Ω(I ′ ), if both satisfy the assumptions (B1) and (B2), then the following statements are equivalent:</p><formula xml:id="formula_44">(a) d TV (P n,ω,α , P n,ω ′ ,α ′ ) = 0 for all n = 1, 2, . . .. (b) d TV (P n,ω,α , P n,ω ′ ,α ′ ) → 0 as n → ∞. (c) I = I ′ and d H+ (ω, ω ′ ) = 0.</formula><p>(a) ⇒ (b) trivially, while (c) ⇒ (a) follows immediately from the conclusion of Proposition 1 (part 2). The proof for (b) ⇒ (c) is given in Appendix C.4, but here is a high-level proof sketch. As n → ∞, the distribution of the sample mean of X [n] converges to the noiseless distribution G(ω, α) of the latent variable η, as discussed at the beginning of this section. This probability measure is a mixture of components where each component is a probability measure supported on a polytope (constituent topic polytope) and these component measures are absolutely continuous with respect to the Hausdorff measure on the respective affine spaces. The assumption on the topic map guarantees that topic polytopes of the same dimension do not share a common affine hull. Suppose we start with this mixture probability measure and try to reconstruct its components. Consider the component whose polytope has the smallest number of dimensions (if multiple, select one arbitrarily). If we take the restriction of the whole measure on the affine hull of this polytope, the only contribution comes from just this component. We argue that there cannot be any component whose polytope has smaller number of dimensions and then argue that there must be a unique component which matches it. We continue this process of eliminating one component at a time until all components have been exhausted. This process uniquely identifies all the constituent topic polytopes and associated probability measures supported on such polytopes.</p><p>Remark 7. There are additional remarks regarding the above theorem.</p><p>1. The theorem can also be expressed as follows: for ω ∈ Ω(I), ω ′ ∈ Ω(I ′ ) satisfying stated assumptions and α, α ′ (seen as maps Φ(T ) → R + on the associated set of paths),</p><formula xml:id="formula_45">G(ω, α) = G(ω ′ , α ′ ) ⇒ I = I ′ , d H+ (ω, ω ′ ) = 0.</formula><p>Note that d H+ is a metric on Ω(I) and thus is applicable only if the underlying DRTs have same number of paths. The theorem guarantees that this is indeed the case and that the topics and their hierarchy can be identified. We emphasize the fact that the theorem requires no restriction on the number of paths in the associated DRTs -hence the model size is also identifiable in this sense.</p><p>2. It is crucial that both ω, ω ′ satisfy the assumptions (B1) and (B2). This is required since we do not make assumptions about the model size (e.g. I or the underlying DRT). If only one, say ω, satisfies the assumptions, it is not hard to see that there exists ω ′ (which would have to violate at least one of the assumptions) with underlying DRT T ′ ≇ T , along with some α, α ′ such that G(ω, α) = G(ω ′ , α ′ ), where T , T ′ are the corresponding DRTs for ω, ω ′ .</p><p>3. We emphasize that we do not require specific parametric distribution for β within each path -any continuous probability distribution on ∆ Ji-1 would work (although in modeling practice the Dirichlet distribution is a natural choice as in the LDA) . For any such P (i) for path i, the component measure</p><formula xml:id="formula_46">G i = P (i) # L i , where L i (β) = Θ ⊤ i β (see Section 3.</formula><p>2) is supported on S i = convΘ i , is absolutely continuous with respect to the Hausdorff measure on A i = affS i and the density is positive in the interior of the support -this is all we require in the result. If we specialize to the case where the symmetric Dirichlet is employed in our model formulation, then the theorem entails that the α parameter for each component is also uniquely identifiable</p><formula xml:id="formula_47">, i.e. G(ω, α) = G(ω ′ , α ′ ) ⇒ I = I ′ , d H+ (ω, ω ′ ) = 0 and α = α ′ .</formula><p>The above result implies that given two distinct parameters ω, ω ′ (satisfying the assumptions (B1) and (B2)), there exists n such that d TV (P n,ω,α , P n,ω ′ ,α ′ ) &gt; 0, i.e., there is a sufficiently large document size which makes these two parameters distinguishable from the data (Note here the distinct ω, ω ′ are distinguishable under the metric d H+ ).</p><p>For parameter estimation, either by a maximizing log likelihood estimator or applying the Bayesian framework, we are interested in estimating ω based on X 1</p><p>[n] , . . . , X m</p><p>[n] ∼ P n,ω * ,α * for some true parameter ω * , α * and to be consistent, we want ωm,n → ω * as m, n → ∞, where ωm,n is some estimator based on X 1</p><p>[n] , . . . , X m</p><p>[n] . For the above result to be useful, we need to restrict the parameter space to some Ω A ⊂ ∪ I Ω(I), where Ω A is such that for any sequence ω n ∈ Ω A , any limit point of the sequence should satisfy assumptions (B1) and (B2). Thus, even if the true parameter ω * satisfies the assumptions, under either the MLE or Bayesian framework this would require making stronger restrictions on the parameter space and require such restrictions be known to the statistical modeller. Instead of this requirement, we would rather want to place minimal restrictions on</p><formula xml:id="formula_48">v 4 v 2 v 3 v 1 v 4 v 2 v 1 θ 2 θ 1 θ 3 θ 4 π 1 = 0.5 π 2 = 0.5 π′ 1 = 1 Example (A) v 4 v 2 v 3 v 1 θ 2 θ 1 θ 3 θ 4 π 1 = 0.5 π 2 = 0.5 v 3 v 1 v 4 v 2 π′ 1 = 0.5 π′ 2 = 0.5</formula><p>Example (B)</p><p>Figure <ref type="figure">5</ref>: Examples of non-identifiability when Assumption (B1) are violated -Examples (A) and (B) respectively. The outer light gray triangle is the vocabulary simplex and the dark gray region represent the noiseless probability measure under the parameter choices as shown. In both cases, the topic map is ρ : v ℓ → θ ℓ for all ℓ. Note that in (A) two different trees with I ̸ = I ′ led to the same model, which for (B), the underlying DRT is the same, as well as the set of topics -but two very different topic hierarchies lead to the same model.</p><p>the parameter space (or the prior) and hope to recover unique latent structure based on data, assuming only that the true latent structure satisfies such assumptions. The following theorem is motivated from such a practical consideration.</p><p>Theorem 2. Consider ω = (T , ρ, π) ∈ Ω(I) satisfying assumptions (B1) and (B2). Then, for any ω ′ = (T ′ , ρ, π) ∈ Ω(I) the following statements are equivalent:</p><p>(a) d TV (P n,ω,α , P n,ω ′ ,α ′ ) = 0 for all n = 1, 2, . . ..</p><formula xml:id="formula_49">(b) d TV (P n,ω,α , P n,ω ′ ,α ′ ) → 0 as n → ∞. (c) d H+ (ω, ω ′ ) = 0.</formula><p>Note the difference between Theorems 1 and 2. In Theorem 2, no assumption is required on ω ′ , while it was crucial that ω ′ also satisfied the assumptions in Theorem 1. Instead, Theorem 2 requires the knowledge of the number of paths -note that for this result, both ω, ω ′ ∈ Ω(I) --thus their underlying DRTs have the same number of paths. Since the number of paths accounts for the number of components in the overall mixture structure of the model, this assumption ensures that for each component of ω there can only be at most one component of ω ′ to match it.</p><p>Remark 8. Assumption (A1) is not required in either of the results in this section. The reason is that the metric d H+ only cares about the support of the individual components -thus, the component measures can be identified. Assumption (A1) would be additionally required to identify all the topics and the topic hierarchy, as established by Proposition 1.</p><p>Remark 9. Here are examples of non-identifiability that arises if we drop the above assumptions. Two such examples are given in Figure <ref type="figure">5</ref>. In the first example, we see two distinct DRTs with different number of paths leading to the same model (the underlying Dirichlet is assumed with parameter 1, i.e., a uniform distribution on the associated component simplexes). The problem arises since polytopes conv(θ 1 , θ 2 , θ 3 ) and conv(θ 1 , θ 3 , θ 4 ) have the same affine hull. In the second example, we see the same DRT (i.e., isomorphic) yet the topic hierarchies are different. For this set of topics (θ 1 , θ 2 , θ 3 , θ 4 for the vertices of a rectangle here) the decomposition of ∪ i S i into S 1 and S 2 become non-unique (since they have the same affine hull). We also note that if θ 4 was not in the affine hull of conv(θ 1 , θ 2 , θ 3 ), then the two topic hierarchies would lead to two distinct models (note that in the former θ 1 is shared between the two components, while in the latter θ 2 ). Thus, without regularity conditions such as Assumptions (B1) and (B2), the knowledge of the underlying DRT and the set of topics is not enough to determine the latent structure, the topic hierarchy, in this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Posterior Contraction Rates</head><p>We turn our attention to convergence behaviors of the density and parameters in our model, under a Bayesian well-specified and identifiable model setting. Given the m × n data</p><formula xml:id="formula_50">X 1 [n] , . . . , X m [n] iid ∼ P n,ω,α</formula><p>where we know the underlying DRT up to an isomorphism. Consider ω, ω ′ such that T ∼ = T ′ , then it is enough to consider a single tree. Indeed, since they are isomorphic, let σ be an isomorphism and σ be the corresponding bijection between Φ(T ) and Φ(T ′ ), then instead of ω ′ , we can consider (T , ρ ′ •σ, π ′ • σ). Thus, it is reasonable to fix a common DRT for both ω and ω ′ . Assume that ω ∈ Ω(T ) for a fixed T . Note that this does not mean that the topic hierarchy is known, as the identifiability theory has established in the previous section. Furthermore, we are interested in the estimation of ρ specifically, since ρ and T combine to determine the topics as well as the topic hierarchy, which is our target. Hence, in this section, we assume that α is known, i.e. α(φ) = α 0 &gt; 0, known and fixed (we remark that what we just require is that the distribution p β for each path only depends on the path length -i.e., for symmetric Dirichlet case, we need α(φ) = α J where J = |φ| and {α J } J∈J known), as such we drop the α in our notation. Moreover, for simplicity of exposition, in this section we assume that each path of T has the same length J.</p><p>Assume that the m×n data set is generated from true parameter ω * ∈ Ω(T ), where T ∈ T(I, J). We place an appropriate prior for ω ∼ Π on Ω(T ) and are interested in the posterior distribution of p n,ω,α0 in Section 5.1, in particular the contraction rate of the posterior to the truth p n,ω * ,α0 in terms of the Hellinger metric as m, n → ∞. We shall adopt the standard Bayesian asymptotic framework to address this problem <ref type="bibr" target="#b11">(Ghosal and van der Vaart, 2017)</ref>. In Section 5.2, we are interested in the posterior distribution of ω itself and study the contraction rate of the posterior to the truth δ ω * under an appropriate metric d. To this end, we establish an inverse bound of the form d TV (p n,ω , p n,ω ′ ) ≳ d U H (ω, ω ′ ), which allows us to use the density estimation rate to prove an upper bound for the parameter estimation rate in the metric d U H , which we shall define shortly. We utilize and extend the techniques developed in Nguyen (2015) -however, in our case, the latent measure G is no longer supported on a single convex polytope as in <ref type="bibr" target="#b24">Nguyen (2015)</ref>; here it is supported on a union of such convex polytopes, which requires novel modification to the inverse bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Density estimation</head><p>We start our study of posterior density contraction rates by deriving an upper bound on the KL divergence in terms of the W 1 distance between the component measures {G i : i ∈ [I]}. This result is crucial to understanding the behavior of the model in a KL-neighborhood of the truth. We aim to derive a connection between the KL divergence between the document-distributions and the distance between the associated parameters, in terms of the augmented tree-directed Hausdorff metric. This would allow us to control certain properties in the density space {p n,ω : ω ∈ Ω(T )} in terms of the underlying parameter space Ω(T ).</p><formula xml:id="formula_51">Lemma 5.1. Suppose ω = (ρ, π), ω ′ = (ρ ′ , π ′ ) ∈ Ω(T ). Assume that both ω, ω ′ satisfy (C1) min u∈V ρ(u) v ≥ c 0 for all v ∈ [V ] for some constant c 0 &gt; 0. (C2) π(φ) ≥ c 1 for all paths φ ∈ Φ(T ) for some constant c 1 &gt; 0.</formula><p>Then, for any permutation σ ∈ S(I), we have</p><formula xml:id="formula_52">KL(p n,ω ∥p n,ω ′ ) ≤ n c 0 i∈[I] π i W 1 G i , G ′ σ(i) + 1 c 1 i∈[I] π i |π i -π ′ σ(i) |,</formula><p>where G i , π i are the latent component measure and path probability associated to the i-th path of T .</p><p>The assumptions state that all associated topics are uniformly bounded away from the boundary of ∆ V -1 , a condition required to control the KL-divergence from exploding near the boundary. Similarly, the second condition assumes that π is bounded away from the boundary of ∆ I-1 . The above lemma, along with a result of Nguyen (2015) (Lemma 7), allows us to control the KL-divergence in terms of the d H+ metric. We use the following geometric regularity condition to ensure that for each of the associated component polytopes, each 'corner' of the polytope is sufficiently acute.</p><p>Definition 5.1. We say that a convex polytope S satisfies the non-obtuse corner property if at each vertex of S, there is a supporting hyperplane whose angle formed with any edges adjacent to that vertex is bounded from below by δ, where δ &gt; 0 is a fixed number.</p><p>Corollary 1. Given ω, ω ′ as in Lemma 5.1 such that all associated component polytopes have the non-obtuse corner property in Definition 5.1. Then,</p><formula xml:id="formula_53">KL(p n,ω ∥p n,ω ′ ) ≤ nC δ c 0 ∨ 1 c 1 d H+ (ω, ω ′ ) (4)</formula><p>where C δ &gt; 0 is a constant depending only on δ in Definition 5.1.</p><p>In the above corollary, we abuse notations slightly, treating ω as (ρ, π) (resp. (ρ ′ , π ′ )) in the left-side and as (T , ρ, π) (resp. (T , ρ ′ , π ′ )) in the right-side of Eq. 4. We note that with sufficiently large n, the result reads</p><formula xml:id="formula_54">KL(p n,ω , p n,ω ′ ) ≤ cnd H+ (ω, ω ′ ),</formula><p>i.e., the bound worsens as n becomes large. However, this corollary allows us to derive both prior concentration property and control of the complexity of the model class, based on which we establish the following posterior contraction rate for density estimation.</p><p>Theorem 3. Suppose Π is a prior distribution on Ω(T ) such that for some constants c 0 , c 1 , c 2 , c 3 &gt; 0 the following holds for any ω = (ρ, π) in the prior support:</p><p>1. Assumptions (C1), (C2) hold.</p><p>2. All component polytopes S i satisfy the non-obtuse corner property given in Defn. 5.1.</p><p>3. For any small ϵ &gt; 0, there exists constants c 2 , c 3 &gt; 0 such that</p><formula xml:id="formula_55">Π (∥ρ(u) -ρ * (u)∥ ≤ ϵ, ∀u ∈ V) ≥ c 2 ϵ K(V -1) Π (∥π(φ) -π * (φ)∥ ≤ ϵ, ∀φ ∈ Φ(T )) ≥ c 3 ϵ I-1</formula><p>and the events in the above display are independent under Π (recall I = |Φ(T )| and K = |V|, size of the set of nodes of T ), where ω * = (ρ * , π * ) is a specific parameter in the support of Π.</p><p>Then, we have the following:</p><p>(a) (fixed n case) For sufficiently large constant C, as m → ∞ with n held fixed</p><formula xml:id="formula_56">Π h(p n,ω * , p n,ω ) &gt; C log m m X 1 [n] , . . . , X m [n] → 0 in P ∞ n,ω * -probability. (<label>5</label></formula><formula xml:id="formula_57">) (b) (increasing n case) For sufficiently large constant C, as m, n → ∞ such that log n = o(m), we have Π h(p n,ω * , p n,ω ) &gt; C log(m ∨ n) m X 1 [n] , . . . , X m [n] → 0 in P ∞ n,ω * -probability. (<label>6</label></formula><formula xml:id="formula_58">)</formula><p>where h is the Hellinger metric.</p><p>The proof of the above result is provided in the Appendix D.4. We show that the conditions required to employ Theorem 8.9 from <ref type="bibr" target="#b11">Ghosal and van der Vaart (2017)</ref> are indeed satisfied for our model -prior mass property in Proposition 2 and model entropy in Proposition 3 in the Appendix.</p><p>Remark 10. We make the following remarks regarding the preceding theorem.</p><p>1. The model enjoys an almost parametric density contraction rate, up to logarithmic terms. Part (a) of the theorem shows that for density estimation, we do not require n → ∞.</p><p>2. No additional assumptions are required for the true parameter ω * as long as it is in the support of the prior Π.</p><p>3. Restrictions on the prior Π: the first two are regularity conditions required to establish Corollary 1, which in turn is required to establish prior concentration as well as controlling entropy of the model class.</p><p>The last assumption only states that the prior places sufficient mass near the true parameter. The first three regularity conditions are required for technical purposes to ensure that the associated parameters are bounded away from the boundary of the respective probability simplexes -this ensures that the multinomial density stays bounded. In practice, we use Dirichlet distribution as a prior for each of the topics and also for the path-probability vector.</p><p>4. The constant C depends only on size of the DRT T (in particular I, K) and also the vocabulary size V . It does not depend on the true parameter ω * in this case.</p><p>5. The theorem shows that large n affects the contraction rate of the density only slightly (in logarithmic terms). This arises due to the upper bound on the KL divergence in terms of the W 1 metric on the latent measures G i . As n increases, the conditional distributions of the document mean given that the document is generated from a particular path become singular if the underlying component polytopes have different affine spaces. This also results in worsening of the upper bound on the entropy of the model space as n gets large, although the upper bound on the entropy of the parameter space does not depend on n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Latent structure estimation</head><p>In this section, we turn to the convergence behavior of the parameters arising in our latent structured model. This kind of question has been studied in recent works for mixture models <ref type="bibr" target="#b23">(Nguyen, 2013;</ref><ref type="bibr" target="#b13">Guha et al., 2021)</ref> and hierarchical models <ref type="bibr" target="#b24">(Nguyen, 2015</ref><ref type="bibr" target="#b25">(Nguyen, , 2016))</ref>. A general approach considered in these works is to establish socalled inverse bounds, which are a type of lower bound for the total variation distance between distributions of the observed data in terms of a distance between the corresponding latent structures using an appropriate metric. Such an inverse bound may be of the form</p><formula xml:id="formula_59">d TV (P G , P G ′ ) ≳ d(G, G ′ ),</formula><p>where G, G ′ capture all information about the latent structure in the model and d is some appropriate metric on the space of the latent G. For finite and infinite mixture models, G is the discrete mixing measure, e.g., G = k π k δ θ k and the metric d is often chosen as W 1 or W 2 . For the LDA, G is taken as the underlying topic polytope (called population polytope) and the metric as the Hausdorff distance <ref type="bibr" target="#b24">(Nguyen, 2015)</ref>. It is worth noting that such an inverse bound is the reverse direction of what we used in the previous sections. For instance, Corollary 1 gives an upper bound for the KL divergence and by Pinsker's inequality, d 2 TV ≤ 2KL, which yields an upper bound for d TV in terms of the d H+ metric on the parameters which represent the latent structure. However, establishing lower bounds for the total variation distance turns out to be more challenging. Given such an inverse bound, say d TV (P G , P G ′ ) ≳ d(G, G ′ ), it is easy to transfer density contraction rates to parameter contraction rates under the metric d.</p><p>For the tree-directed topic models, the latent structure is represented by the parameter ω ∈ Ω(T ); the metric d H+ on this space has been utilized to identify ω and to establish the data density contraction rates. It is difficult to obtain an inverse bound directly with this metric. For a given T , the topic hierarchy is captured entirely by the topic map ρ. Thus, in this section, the following pseudo-metric for ρ, to be called union Hausdorff metric, will be employed.</p><p>Definition 5.2. For a fixed DRT T and ρ, ρ ′ ∈ R(T ), define</p><formula xml:id="formula_60">d U H (ρ, ρ ′ |T ) = d H ∪ i∈[I] S i , ∪ i∈[I] S ′ i</formula><p>where I = |Φ(T )| is the number of paths of T and S i is the component topic polytopes associated with path i under ρ, given a particular enumeration of the paths of T (and similarly for S ′ i ).</p><p>The notation d U H (ρ, ρ ′ ), which hides the dependence on the underlying DRT T , will also be used when this is clear from context. The union Hausdorff metric inherits metric properties from the Hausdorff metric directly. Since the metric is the Hausdorff distance on union of the polytopes, in general it may not be used to disentangle the individual polytope components, unless some suitable conditions hold. Lemma D.1 in the Appendix gives such sufficient conditions to identify the complete latent structure from the union of the supports, by exploiting the special geometric structure of the model. Note that ∪ i S i is not a convex set in general for our model, which makes the "disentangling" challenging. However, if the measure G places enough mass everywhere in S, then this issue can be overcome. Fortunately, because G = i π i G i , our next result demonstrates that each G i indeed places sufficient probability mass everywhere in its support, provided that the component G i 's are push-forwards of Dirichlet distributions. This guarantees that if the π ′ i s are also bounded from below, G places sufficient mass everywhere in its support.</p><p>Lemma 5.2. For G = Dir(K; α) # L, where L(β) = k∈[K] β k θ k , then for any η ∈ S, where S = conv({θ 1 , . . . , θ K }), and for all 0 &lt; ϵ &lt; 1/K, ,p,α)   where p = dim S, C(K, p, α) &gt; 0 is a constant free of ϵ and</p><formula xml:id="formula_61">G(B(η, ϵ) ∩ S) ≥ C(K, p, α)ϵ Q(K</formula><formula xml:id="formula_62">Q(K, p, α) = p + α(K -p -1) α ≤ 1 αK -1 α &gt; 1.</formula><p>The above lemma guarantees that each component measure arising in the model gives sufficient mass to each ϵ-ball inside the component, each such ball is of the same dimension as the corresponding component polytope. In the special case, α ≤ 1 and the topics are affinely independent, i.e., p = K -1, Q(K, p, α) = K -1. In all cases, the bound worsens as α &gt; 1 gets larger, because the Dirichlet distribution puts lower mass near the boundary of its support. If the intrinsic dimension is small compared to K and α &lt; 1, then we find that Q(K, p, α) = α(K -1) + (1 -α)p, a convex combination of K -1 (similar to the affinely independent case) and p (the intrinsic dimension). If α &gt; 1, then we always have the worse exponent αK -1 regardless of p. We note that max 1≤p≤K-1 Q(K, p, α) = (1 ∨ α)K -1 denotes the worst possible value of Q(K, p, α) for fixed K and α. Based on this result, we can prove the following inverse bound.</p><p>Theorem 4. There is a constant ϵ 0 &gt; 0 depending on J such that for any ω = (ρ, π) ∈ Ω(T ), ω ′ = (ρ ′ , π ′ ) ∈ Ω(T ) satisfying Assumption (C2), whenever d U H (ρ, ρ ′ ) &lt; ϵ ≤ ϵ 0 there holds</p><formula xml:id="formula_63">C 1 d U H (ρ, ρ ′ ) (1∨α0)J-1 ≤ d TV (p n,ω , p n,ω ′ ) + 6V exp - n 8V d U H (ρ, ρ ′ ) 2 , (<label>7</label></formula><formula xml:id="formula_64">)</formula><p>where J is the depth of the DRT T , C 1 = C 1 (c 1 , J, α 0 ) &gt; 0 is a constant, and c 1 is the constant in Assumption (C2).</p><p>Proof sketch: The proof uses the probabilistic model η ∼ G = i π i G i (where G i s are the component measures, each supported on a component polytope S i ) and X [n] |η are i.i.d. from a multinomial distribution with η as the parameter. By a standard concentration inequality, for large n, the distribution of the document means ηi ∈ ∆ V -1 concentrate in small balls around that of the corresponding η, under the joint distribution of (η, X [n] ). Finally, we argue that if the union Hausdorff metric is ϵ, then there exists a set A * ⊂ ∆ V -1 , which is in either S = ∪ i S i and well-separated from S ′ = ∪ i S ′ i or in S ′ and well-separated from S. In either case, this set enables one to obtain a lower bound on the total variation distance between the document distributions, since G, G ′ place different probability on a sufficiently small fattening of A * . The complete proof of this theorem is given in the Appendix.</p><p>Remark 11. We make a few remarks about the above theorem:</p><p>1. Special case: If each of the component polytopes have the same dimension p, and α ≤ 1, then the inverse bound improves to</p><formula xml:id="formula_65">C 1 d U H (ρ, ρ ′ ) αJ+(1-α)p-1 ≤ d TV (p n,ω , p n,ω ′ ) + 6V exp - n 8V d U H (ρ, ρ ′ ) 2 .</formula><p>In the case the topics in each component are affinely independent (which is the case when J &lt; V and the topics are in general positions), then the exponent of the union Hausdorff metric in the left-hand side becomes simply (J -1).</p><p>2. The only assumption required for the inverse bound is that the path probabilities for each model are bounded from below by a constant (c 1 in the lemma).</p><p>3. Although the above theorem is presented using the Dirichlet distribution p β on the probability simplex, this is not strictly required. In fact, the Dirichlet distribution may be replaced by any distribution so long as the conclusion of Lemma 5.2 holds for the pushforward probability measure (p β ) # L. Specifically, the theorem continues to hold for any probability distribution p β arising from an α-regular family (as defined in <ref type="bibr" target="#b24">Nguyen (2015)</ref>).</p><p>4. Due to the presence of the additive term (the second quantity) in the right hand side of Eq. ( <ref type="formula" target="#formula_63">7</ref>), we cannot directly obtain d U H ≲ d TV in this model and require n → ∞, since that term decays exponentially with n.</p><formula xml:id="formula_66">However, if d U H (ρ, ρ ′ ) &gt; ϵ, then taking n ≥ 8V ϵ 2 log 12V C1ϵ (1∨α)J-1 ensures that d TV (p n,ω , p n,ω ′ ) ≥ C1 2 ϵ (1∨α)J-1 .</formula><p>As a consequence, we have showed that as long as the Hausdorff distance between ∪ i S i and ∪ i S ′ i are bounded away from 0, the total variation is bounded away in terms of this for sufficiently high n, irrespective of the path probabilities subject to Assumption (C2).</p><p>The inverse bound allows us to transfer the density estimation rate we derived in the last section to derive estimation rates for the parameter ρ in terms of the d U H metric.</p><p>Theorem 5. Suppose Π is a prior on Ω(T ) satisfying the assumptions in Theorem 3 and ω 0 = (ρ 0 , π 0 ) is any point in the support of Π. Then, given data X 1</p><p>[n] , . . . , X m</p><p>[n]</p><p>iid ∼ P n,ω0 , the posterior distribution of ρ contracts to δ ρ0 as</p><formula xml:id="formula_67">Π d U H (ρ, ρ 0 ) &gt; Cϵ m,n X 1 [n] , . . . , X m [n] → 0 in P ∞ n,ω0 probability as m, n → ∞ (8)</formula><p>such that log n = o(m), for some suitably large constant C &gt; 0, for the choice</p><formula xml:id="formula_68">ϵ m,n = log(m ∨ n) 1 m + 1 n 1 2[(1∨α)J-1] (9)</formula><p>where J is the depth of the tree T .</p><p>Proof sketch: The inverse bound allows us to control lim inf n→∞ h 2 (p n,ω0 , p n,ω ) in terms of ϵ &gt; 0 for ω in the support of Π such that d U H (ρ, ρ 0 ) ≥ ϵ. In particular, as long as ϵ ≳ log n/n, for such ω, we have lim inf n→∞ h 2 (p n,ω0 , p n,ω ) ≥ Cϵ 2[(1∨α)J-1] for some constant C &gt; 0. Thus, the posterior probability on {d U H (ρ, ρ 0 ) ≥ ϵ m,n } can be upper bounded by the posterior mass on {h(p n,ω0 , p n,ω ) ≥ C ′ ϵ (1∨α)J-1 m,n } as n → ∞, which we further know goes to 0 as m → ∞ because of the density contraction result.</p><p>Remark 12. We make a few remarks regarding the preceding theorem.</p><p>1. ϵ m,n is merely an upper bound for the contraction rate in terms of the union Hausdorff metric. We require both m, n → ∞ for this result. This is in contrast to Theorem 3, which only requires m → ∞ and n could be fixed. The situation arises due to the need to dominate the second term in Eq. ( <ref type="formula" target="#formula_63">7</ref>) in Theorem 4. This can be taken as a consequence of the proof technique, where we study parameter estimation through density estimation -for fixed n, as m → ∞, the density can be estimated at the usual parametric rate (with respect to the number of iid documents); however, the component polytopes can be estimated from this density at a rate given in the inverse bound as n → ∞.</p><p>2. In the special case α ≤ 1 and each component consists of affinely independent topics, the rate boils down to ϵ m,n = [log m(1/m + 1/n)] 1/2(J-1) in the typical case when m &gt; n.</p><p>3. The only restrictions required are those on the prior, same as the ones for the density estimation result.</p><p>They arise in our attempt to upper bound K 2 , defined as K 2 (p, q) = (log(p/q) -KL(p∥q)) 2 dP , which plays a crucial role in the Bayesian asymptotic analysis for density estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Inference algorithm</head><p>In this section, we describe an inference algorithm for estimating the topics in this model. We consider a collapsed Gibbs sampler, which is derived based on the collapsed Gibbs sampler for the LDA <ref type="bibr" target="#b12">(Griffiths and Steyvers, 2004)</ref>. We utilize the equivalent model with the additional latent variables C ∈ [I] m and L ∈ [J] m×n , where C i is the label of the path of the underlying DRT associated to document</p><formula xml:id="formula_69">X i [n] , i ∈ [m] and L i,j is the depth associated to word X i,j in document X i [n] for i ∈ [m], j ∈ [n].</formula><p>Given a DRT T of size (I, J, K), topics θ 1 , . . . , θ K (where θ k = ρ(v k ) for topic map ρ), path-probability vector π ∈ ∆ I-1 and document-specific topic mixture β i ∈ ∆ J-1 for i ∈ [m], the model specification is given as</p><formula xml:id="formula_70">C i |π i.i.d. ∼ Cat(π) L i,j |β i i.i.d. ∼ Cat(β i ) X i,j |C i , L i,j ∼ Cat(θ z(ci,Li,j )) .</formula><p>Here, we use the following parametrization of the tree -for a path label c and depth label ℓ, z(c, ℓ) is the index of the node among {1, . . . , K}, such that v k is at depth ℓ from the root along path φ c in the DRT. Let z -1 (k) := {(c, ℓ) : c ∈ [I], ℓ ∈ [J], z(c, ℓ) = k} denote the set of all tuples of path and depth such that it locates node k. The following priors are used for simplicity, using symmetric Dirichlet parameterized by a scalar parameter π ∼ Dir I (π 0 ),</p><formula xml:id="formula_71">β m i.i.d.</formula><p>∼ Dir(α),</p><formula xml:id="formula_72">θ k i.i.d.</formula><p>∼ Dir(η).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The joint distribution of X [m]</head><p>[n] , C, L, Θ, β, π (where Θ includes all the topics θ 1 , . . . , θ K and β includes all the document allocations β 1 , . . . , β m ) is given by p</p><formula xml:id="formula_73">X [m] [n] , C, L, Θ, β, π = p X [m] [n] |C, L p(C, L|Θ, β, π)p(Θ|η)p(π|π 0 )p(β|α) = p(π|π 0 )p(Θ|η) i∈[m] p(C i |π)p(β i |α) j∈[n] p(L i,j |β i )p(X i,j |Θ, C i , L i,j ).</formula><p>Employing the Dirichlet-multinomial conjugacy, one can marginalize out Θ, β and π to obtain p X</p><formula xml:id="formula_74">[m] [n] , C, L = p(X [m] [n] |C, L)p(C, L) where p X [m] [n] C, L = Γ(V η) Γ(η) V K k∈[K] v∈[V ] Γ(N vk + η) Γ(N •k + V η) (10) p(C, L) = Γ(Iπ 0 ) Γ(π 0 ) I i∈[I] Γ(M i + π 0 ) Γ(M • + Iπ 0 ) ×   Γ(Jα) Γ(α) m i∈[m] k∈φ c i Γ( Ñik + α) Γ( k∈φ c i Ñik + Jα)   . (11)</formula><p>In the above expressions, k ∈ φ i indicates iterating only over those k for which node v k is in the i-th path in the DRT; and for each such i, there are J such k ′ s. The count matrices M ∈ N I 0 , N ∈ N V ×K 0 and Ñ ∈ N m×K 0 (where N 0 = N ∪ {0}) are defined as follows</p><formula xml:id="formula_75">M ℓ = i∈[M ] 1(C i = ℓ), N vk = i∈[m],j∈[n] 1 X i,j = v, (C i , L i,j ) ∈ z -1 (k) , Ñik = 0, if k not in path C i j∈[n] 1(L i,j = ℓ), if node k is at depth ℓ in path C i .</formula><p>Next, we shall describe the Gibbs sampler over just C and L. For L, we have the following update</p><formula xml:id="formula_76">p L i,j = ℓ|L -(i,j) , C, X m [n] ∝ N -(i,j) xi,j ,z(ci,ℓ) + η N -(i,j) •z(ci,ℓ) + V η × Ñ -(i,j) i,z(ci,ℓ) + α ,<label>(12)</label></formula><p>where -(i, j) just indicates the count without taking the current assignment of L i,j into account.</p><p>The update for C is a bit more involved since it affects multiple documents together. Note that if we change C i from c ′ to c, then the second (product) term in Eq. ( <ref type="formula">11</ref>) stays the same (since L i stays the same -the topics along path φ ci might change, but the counts along the depths stay the same).</p><formula xml:id="formula_77">p C i = c|C -i , L, X [m] [n] ∝ (M -i c + π 0 ) × k∈[K]   Γ(N -ci •k + V η) Γ(N (ci=c) •k + V η) × v∈[V ] Γ(N (ci=c) vk + η) Γ(N -ci vk + η)   , (<label>13</label></formula><formula xml:id="formula_78">)</formula><p>where N -ci vk is the corresponding count without considering the assignment for c i while N (ci=c) vk considers the particular assignment for c i being c. These count terms can be computed efficiently by simply saving the word-topic count matrix for each document.</p><p>Overall Sampler: The Gibbs sampler performs the following updates per iteration:</p><p>1. Update L i,j using Eq. ( <ref type="formula" target="#formula_76">12</ref>) for i ∈</p><formula xml:id="formula_79">[m], j ∈ [n]</formula><p>2. Update C i using Eq. ( <ref type="formula" target="#formula_77">13</ref>) for i ∈ [m].</p><p>Given C, L, we can estimate the parameters by using the following equations (posterior means of the corresponding variables given others)</p><formula xml:id="formula_80">πℓ = M ℓ + π 0 m + Iπ 0 , βik = Ñik + α k∈φ c i Ñik + Jα , θkv = N vk + η N •k + V η .</formula><p>To monitor the Gibbs sampler, we can approximate the likelihood of the data as the harmonic mean of p(X [n] ) <ref type="bibr" target="#b18">(Kass and Raftery, 1995)</ref>, which can be drawn from the Gibbs sampler described above. We have implemented the sampler in Jax. To overcome the sampler getting stuck at local modes, we restarted the sampler several times in our simulations, and report using the chain corresponding to the highest log-likelihood of the data, approximated as discussed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Simulation studies</head><p>We conducted a number of simulations to study the performance of the algorithms and gain further understanding into the latent structure estimation in our model. The primary objectives for the experiments are understanding the (a) estimation rate and (b) identifiability through model selection. Further details about the experiments are provided in the Appendix E. For estimation rates, we consider the metric</p><formula xml:id="formula_81">d L2 (ρ, ρ ′ ) = min σ∈S I I i=1 min τi∈S J j∈[J] θ i,j -θ ′ σ(i),τi(j)</formula><p>for the ease of computation, compared to the theoretical union Hausdorff metric. Here θ i,j is the topic corresponding to the node at level j along path i in the fixed DRT T . The intuition for this choice is clear -for each pair of component polytopes, their 'distance' is the sum of the distances between an optimal matching of their extreme points. Finally, the overall metric is optimally matching the components, following this metric. We show that d U H (ρ, ρ ′ ) ≤ d L2 (ρ, ρ ′ ) in the Appendix -hence, estimation rate in terms of d L2 provides an upper bound for the rate in terms of d U H .</p><p>Experiment 1:</p><p>In the first set of simulations, we consider a directed and rooted tree of size I = 2, J = 3, K = 5 sharing only the root. Thus, in this model there are 2 component polytopes sharing one extreme point. We take V = 10 as the vocabulary size. The true topics were drawn from a Dirichlet distribution Dir V (1.0) (uniform), the path probabilities were set to be uniform and α 0 = 0.8. Two choices were taken for the number of words per document n = 50, 100 and for each setting of n, we selected m on an equi-spaced grid from 200 to 5000 (8 values on a log scale). For each choice of (m, n), the experiment was repeated L = 15 times, each time drawing a corpus of size (m, n) from the model (keeping the same parameter) and using the Gibbs sampler (we ran for 5500 iterations, dropping the first 5000 as burn-in and using a thinning of 10, giving us 50 samples) to estimate the topics. The DRT is shown in Figure <ref type="figure" target="#fig_2">6</ref> (top row left column) and the two images on the right show the true topic polytopes and the estimated topic polytope (for an instance in the case n = 50, m = 200), each point is a document (this is the document mean ηi ) projected on the first and second principal component directions (middle column) and second and third (right column), the solid triangles are the true component polytopes (sharing one common vertex), while the dotted blue triangles are the estimated polytopes.</p><p>For each instance, the performance of our methods was measured by the d L2 metric, computed on the sample of size 50. The result is shown in Figure <ref type="figure" target="#fig_3">7</ref>  indicates the two choices of n). We can see a similar trend for both n (somehow in this simulation, n = 100 had higher standard errors compared to n = 50 with very similar means) -the estimation rate of the topics under d L2 metric is indeed very close to a parametric rate, as the slope suggests (obtained by fitting a linear regression model). Our theory in Section 5.2 showed an upper bound of [log m(1/m + 1/n)] 1/4 (in the union Hausdorff metric) in this model as α &lt; 1 and J -1 = 2. The experiment suggests that this upper bound on the parameter estimates may not be tight. Furthermore, the simulation shows that sending n → ∞ is probably not required in this case.</p><p>Experiment 2:</p><p>In the second set of simulations, we consider another DRT of size I = 4, J = 3, K = 7 shown in Figure <ref type="figure" target="#fig_2">6</ref> (bottom row left column). Thus, in this model there are 4 component polytopes, with one topic shared across all 4 components and two different pairs of polytopes share one common edge -this is shown as the solid colored triangles in the middle and right columns of the bottom row in Figure <ref type="figure" target="#fig_2">6</ref>. We take V = 30 as the vocabulary size in this experiment, n ∈ {150, 300} and m increasing from 400 to 4000 (5 values chosen equi-spaced in log scale). The true parameters are generated similarly as in experiment 1, with α 0 = 1.0. We perform a similar study as in experiment 1 (with 15 repetitions for each choice of m, n) and Figure <ref type="figure" target="#fig_3">7</ref> (middle) shows that the results, in terms of the rate, are also similar to experiment 1 -the plot shows log d L2 against log m, having a slope of around -1/2, verifying a parametric rate. However, we note that in this case, there is a marked difference between n = 150 and n = 300 case -this tells that while n → ∞ might not be necessary for consistent parameter estimation, increasing n improves the estimation rate of the parameters. We like to highlight an interesting challenge for this class of model. Although we fit the model using the true underlying DRT, it does not ensure that the Gibbs sampler converges to a correct sharing structure within our prescribed computational limit. As an example, compare the plots in Figures 6 (bottom row middle column) and 7 (right most column) -the blue dotted triangles are the estimated components polytopes. The Gibbs sampler was run for 5500 iterations for the former, while even with 10000 iterations, the Gibbs sampler in the later case did not improve beyond the shown structure. Note that in the latter plot, although the underlying DRT is the same and there is an estimated topic nearby each of the true topics, the sharing structure is not at all close to the true sharing structure. For our simulations, we used 8 parallel chains and chose the one based on highest log likelihood. This also demonstrates that d L2 metric by itself does not give a full picture of the accuracy of the estimation of the latent structure. In experiments 1 and 2, we checked for this accuracy in the following way: we first find a permutation σ ∈ S K minimizing k ∥θ k -θσ(k) ∥, i.e., an optimal matching of the estimated topics to the true topics. Then we checked whether estimated topics play the same role in the estimated topic hierarchy as the true counterparts (based on the optimal matching) -i.e., for example, if θ 1 is the root of the true topic hierarchy (the vertex shared by all the 4 polytopes), then whether θσ(1) is the vertex shared by all the estimated polytopes (also similar checks for all the other topics). In Figure <ref type="figure" target="#fig_3">7</ref> (right), this is not true since the true root of the topic hierarchy is the rightmost point, which is closest to an estimated topic (right-most blue topic) which is a topic not shared by any two polytopes (hence a leaf). In all of the instances in experiments 1 and 2 (using 8 parallel chains), the true sharing structure was estimated correctly in this sense.</p><p>These two experiments validate that the latent topic hierarchy can indeed be consistently estimated from the corpus under our model. While we provide an upper bound for the estimation rate for this latent structure, the experiment demonstrate that it is not tight. We conjecture that the estimation rate is parametric for any fixed n, i.e., ϵ m,n ≈ c n / √ m, where the constant c n depends on n -in particular, higher n would correspond to a lower c n , however, the estimation is consistent as long as n ≥ n 0 for some minimal n 0 . The nature of this dependence remains unknown, but there is some hint about such dependence based on recent results for simpler models such as the LDA <ref type="bibr" target="#b0">(Anandkumar et al., 2012)</ref> and finite mixtures of product distributions <ref type="bibr" target="#b35">(Wei and Nguyen, 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 3:</head><p>The third set of simulations is designed to demonstrate a DRT selection problem. The true underlying DRT is the same as in Experiment 1. However, we use 8 different choices of DRT (the whole collection is shown in Figure <ref type="figure" target="#fig_4">8</ref>) -out of these, the first 5 all have K = 5 vertices, while the last 3 have K &gt; 5. The tree named tree4 has just I = 1 path and is equivalent to a LDA model with K = 5 topics (notice that the true DRT has 5 topics). We generate data using the true DRT and fit the model using each of the considered trees. In this experiment, we fix n = 60, m = 400 and V = 10. For each simulation (which is repeated 10 times), we train the model with m = 280 (70% documents) randomly chosen documents and keep the remaining 30% as held-out datafinally, we compute the log-likelihood per document for the held-out dataset (which is often called perplexity for topic models). The held-out log likelihood for each tree across each experiment is shown in Figure <ref type="figure" target="#fig_5">9</ref> (left) -each colored line represents results from a particular corpus instance (out of the 10). We make the following observations: among the first 5 trees, the true DRT (tree0) has the highest held-out log likelihood, however, the last 3 have almost similar held-out log likelihood -this makes sense, since the first 5 all have K = 5 topics, while the remaining have higher K (they are all trees with the true DRT being a sub-tree of it) and higher I  for each tree (each color represents a particular corpus); (Right) minimum path probability for each tree (tree4 is left out, since it is an LDA, equivalent to a tree with a single path and hence, the path probability is 1).</p><p>(number of paths) than the true DRT. As a result, it might be possible that some of the paths are assigned very low probability -to check this, we also plot min i πi , the minimum value of the estimated path-probabilities. We note that the last 3 trees have significantly smaller values compared to tree0. Furthermore, although tree1 and tree2 have same K as tree0, they have I = 4 and I = 3 respectively (true I = 2), and thus, they too have significantly low minimum path probabilities. This experiment shows that methods involving held-out log likelihood can be used for the DRT selection task. The results in this experiment support our study of identifiability in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Real data example</head><p>In this section, we provide an analysis of a subset of the 2016 New York Times articles using our model to explore the hierarchical structure among latent topics. The original dataset contains 8887 documents across multiple categories (the categories were extracted from the news articles URLs) from articles during April to June, 2016. For this study, to keep the presentation simple and interpretable, a small subset of these documents were selected corresponding to 4 categories (world-middleeast, world-americas, sports-hockey, sport-soccer). This resulted in a corpus of size m = 641. As pre-processing, all words were converted to lower case and lemmatized, punctuation and words which occurred in less than 5 documents or more than 60% of the documents were removed. Finally, the top 500 words were selected from the remaining vocabulary corresponding to highest document frequencies. This effectively reduced the vocabulary size from over 100,000 to V = 500.</p><p>Each document was still quite large, the average size of a document turned out to be around n ≈ 200. This resulted in a corpus with 131, 415 words. A visualization of the data is given in Figure <ref type="figure" target="#fig_0">1</ref> (left two figures). Each document is represented as a point in the two dimensional space spanned by the first and second principal components (left) and second and third (right) -the points are colored by their true category. Note that for model fitting, information about such category labels are not used in any way. In Figure <ref type="figure" target="#fig_0">1</ref>, observe that the documents lie on roughly two triangular polytopes, sharing a common vertex. We also note that the two sides (one side contains the orange and red point and one contains the blue and green) clearly represent two very different topics -world and sports. Within a topic, it is harder to differentiate visually between the subtopics. Nonetheless, this provides evidence into the geometric viewpoint that we discuss throughout this work -namely, to understand the latent structure as arising from a collection of topic polytopes, restricted to certain vertices being shared.</p><p>The data set was randomly splitted into 70% for training and 30% as held-out. We considered a variety of DRTs (given in the Appendix) with J ≤ 4 and I ≤ 6. For each DRT, model fitting was achieved using the collapsed Gibbs sampler with 8 restarts --we dropped first 5000 iterates as burn-in and then retained 50 samples at intervals of 10 (hence, a total of 5500 iterations for the sampler). For DRT selection, we considered a few criteria motivated by interpretability and considerations arising from this work. Firstly, the log-likelihood per document was computed on the held-out data for each DRT. We also collect measures to check (i) redundancy of a topic within a component polytope, (ii) separation across the component polytopes and (iii) path probabilities. For (i), we computed the width of each component polytope, defined as the minimum projection distance of a vertex to the affine hull spanned by the remaining vertices, and minimum edge length. For (ii), we computed minimum minimal matching distance d M between distinct component polytopes, where</p><formula xml:id="formula_82">d M (S i , S j ) = max θ∈extrSi min θ ′ ∈extrSj ∥θ -θ ′ ∥ ∨ max θ ′ ∈extrSj min θ∈extrSi ∥θ -θ ′ ∥ ,</formula><p>which is shown to be equivalent to the Hausdorff distance between the component polytopes in <ref type="bibr" target="#b24">Nguyen (2015)</ref>. Since each of the component polytopes has the same dimension for each DRT, we also computed the Grassmanian distance between the affine hulls of the component polytopes (angle between the affine spaces). For (iii), we collected the minimum estimated path probability for each DRT. The full results are shown in Appendix. Based on the results, the DRT was selected, see in Figure <ref type="figure" target="#fig_6">10</ref>, with top 10 words for each estimated topic shown in the corresponding vertices, and also the corresponding path probabilities.</p><p>The estimated topic hierarchy shown in Figure <ref type="figure" target="#fig_6">10</ref> captures quite nicely the corpus structure. We emphasize that the news categories (shown in colors in Figure <ref type="figure" target="#fig_0">1</ref>) were not used while fitting the model -the tree-directed topic model captured the topics in a completely unsupervised manner from the corpus. The root node naturally carries words that can occur commonly in all types of documents. The topics in level 1 (light yellow and light blue) evidently capture the two major topics in the corpus, namely, sports and world. For the sports topic, the top 10 words are mostly common to both soccer or hockey, except real, madrid, which originate from the famous soccer club Real Madrid. For the world topic, we see however, that there is slightly higher influence of middle-eastern countries (e.g., we find the words iran, egyptian, saudi). The topics in the bottom two layers of hierarchy capture the unique words for each suptopic. For this DRT, since each node in the penultimate level only has 1 child each, these two levels are exchangeable from the model's perspective. Indeed, the child nodes for the sports topic divide into soccer (light orange boxes) and hockey (light red boxes). The topics for hockey capture common team names such as Pittsburgh Penguins and San Jose Sharks while the soccer topics contain words for famous European teams such as England, France and also famous player like Lionel Messi from Argentina. We note that words goal, game, score appear in both of these paths -ideally, we would want the parent topic to put more probability to these words since they appear in both. We note that the word united appears in both the soccer path, as well as the world topic -however, upon closer inspection, these two in fact have very different origin. The former united comes from Manchester United (a popular team in soccer), while the latter comes from the United States of America and United Arab Emirates. On the right side in the world sub-tree, we find the paths behaving slightly differently. The left child of world is clearly talking about the political situation in Brazil as the at-the-time woman president Dilma Rousseff was succeeded by Michael Temer in 2016. The child node of this is however again from the middle-east, representing the Israel-Palestine conflict about the Gaza strip (Benjamin Netanyahu has been the Prime Minister of Israel). Closer inspection revealed many of the articles where about struggle of Palestinian women during these troubled times, and hence possibly the model placed this node in the same path as its parent was owing to the fact that the latter puts high probability to words like her, she, woman. The right child of the world topic is mostly about Syrian civil war (Bashar Al-Assad is the President of Syria) and Russian aid. Its child is mostly about the War in Iraq, between Iraq and its allies and the Islamic State, making this fourth path a very distinct middle-east sub-topic of world. However, the third path has a mix of both American and middle-east world politics. The estimated topic polytopes are shown in Figure <ref type="figure" target="#fig_7">11</ref>  Next, we present the results corresponding to a DRT which resembles our existing knowledge of the news category's structure in the corpus (i.e., a binary tree of 7 vertices without further information). The estimated topics are shown in Figure <ref type="figure" target="#fig_8">12</ref> and the estimated component topic polytopes were shown in Figure <ref type="figure" target="#fig_0">1</ref> (right two figures). In this case, we find that the four paths majorly capture the four news categories and correctly captures the hierarchy. Compared to the previous tree, the top level topics are comparable, while there are also a few notable differences: firstly, for the soccer path, the previous tree had a distinction between European soccer (penultimate node) and South American soccer (node in the bottom level), while for this tree, they are combined together. For the path corresponding to hockey, it seems both the last two level nodes in the previous tree were about the Stanley Cup and Pittsburgh Penguins winning over San Jose Sharks in the final. The third path in the previous tree was unique since it combined documents from both American and Middle-Eastern world politics; in the present tree, they are put in different paths. Furthermore, there is no clearly identifiable topic about the War in Iraq, as we have seen in the previous tree. Even when a single best model is challenging to determine for real corpus with complex and latent contextual information such as the current example, we have demonstrated that interpretability of the learned models is feasible according to the varying model constraints. It is quite interesting that the latter tree captures correctly the news category tag assigned to the documents, and moreover discovers predominant topics from these categories of NYT articles from April to June 2016. The former tree due to its larger size also captured additionally meaningful topics and information of interest (including the war in Iraq, the distinction between European and South American soccer and connection between Syria-Palestinerelated articles and Brazil politics-related articles).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We studied a class of topic models which enables the learning of a latent hierarchy of topics enforced through a directed rooted tree (DRT) structure. The probabilistic model was formulated using the underlying tree and a topic map, which provide the embedding of tree-structured hierarchy into the space of distributions on vocabulary (the vocabulary simplex). It was established in this work that the latent topic hierarchy is identifiable under suitable regularity conditions, and can be consistently estimated from the text corpus to provide meaningful insight into the latent structure of the corpus. Examples of non-identifiability were provided when the regularity conditions are not satisfied. Our approach relies on exploiting the geometric structures underlying the model, which can be seen as representing a collection of topic polytopes sharing particular extreme points or faces constrained by a tree-structured hierarchy. As we have demonstrated, such a structure adapts quite well to complex text corpora, where the existing topic models such as the LDA were rather inadequate. While our model may be seen as a mixture of the LDA-induced distributions, it is the restriction on the sharing patterns among the LDA components that makes the model significantly richer and more interesting, and the theoretical analysis more challenging. We validated our model by numerical experiments and demonstrated its usefulness in the analysis of New York Times articles, for which the latent topics captured by the topic hierarchy were learned and meaningfully interpreted.</p><p>This paper opens up several venues for future work. On the computational side, we used collapsed Gibbs  sampler with a known DRT -utilizing the geometric insights obtained from this paper should allow developing more efficient geometric algorithms for estimating the topic polytopes. In terms of theory, deriving tighter upper bounds than what were presented in Section 5.2 and obtaining the minimal n 0 for which this model is identifiable, thereby extending the theory in Section 4.3, are interesting problems; we believe that new techniques would be required to address such questions. It is also of interest to consider estimating the tuning parameter, i.e., parameter α, for the distribution on vocabulary simplex, or at least investigating into the effect of this hyperparameter on estimation. Given the highly general identifiability results presented herein, consistent estimation for the underlying DRT is also a direction worth pursuing further.  <ref type="formula">C</ref>) also shows membership value of each node (blue number beside each node) -e.g. if we look at node v 3 , the subtree starting from it is highlighted in yellow and has 2 leaves.</p><formula xml:id="formula_83">v 1 v 2 v 3 v 7 v 6 v 5 v 4 v 8 v 1 v 8 v 3 v 7 v 6 v 5 v 4 v 2 v 1 v 2 v 3 v 6 v 7 v 5 v 4 v 8 v 1 v 4 v 3 v 6 v 5 v 8 v 7 v 2 (A) (B) (C) (D)</formula><formula xml:id="formula_84">v 1 v 8 v 3 v 7 v 6 v 2 v 1 v 1 v 3 v 2 v 8 v 1 v 3 v 6 v 8 v 1 v 3 v 2 {v 1 , v 2 , v 8 } {v 1 , v 3 , v 6 } {v 1 , v 3 , v 7 } {v 2 , v 8 }-v 1 {v 3 , v 6 }-v 1 {v 3 , v 7 }-v 1 v 3 {v 2 , v 8 }-v 1 {v 6 }-v 3 {v 7 }-v 3 v 1 {v 8 }-v 2 {v 6 }-v 3 {v 7 }-v 3 v 2 {}-v 8 {v 6 }-v 3 {v 7 }-v 3 {}-v 8 {}-v 3 {v 7 }-v 3</formula><p>Step 1</p><p>Step 2</p><p>Step 3</p><p>Step 4</p><p>Step 5</p><p>Step 6</p><p>Figure <ref type="figure" target="#fig_10">14</ref>: Illustration of the construction in the proof of Lemma 2.1. Given a collection of subsets, construct the underlying DRT: Consider Step 2, v 1 (in blue) is the desired parent, we look at histories of each current subset (in gray outside each subset) and select the element which occurs in a maximal number of times, here v 3 (in red). Hence v 3 is added as a child of v 1 in this step. Now, v 3 is removed from all subsets which contained it and the histories are updated for the next step. The only ambiguous choices in this procedure occur in Steps 3 and 5: for Step 3, any of v 2 or v 8 could have been chosen, this would have only resulted in the nodes v 2 and v 8 being swapped, keeping the tree structure same. For Step 5, whether v 6 is chosen first or v 7 does not matter, both would have ended up as children of v 3 and the trees are same (not just isomorphic).</p><p>A.1 Proof of Lemma 2.1</p><p>Proof. Given a collection of subsets, each of which arise as the set of nodes along a maximal path of a DRT, we explicitly construct the DRT and show that such a construction is unique for the tree structure. Let {C 1 , . . . , C I } be these subsets arising from some DRT T , which immediately imply that T must have I leaves. The algorithm would proceed by constructing the root and adding a child to one of the existing nodes at each step. It is worth reminding that if (u, w 1 , . . . , w k , v) is a directed trail in T with m(u) = m(v), then each of u, w 1 , . . . , w k has no other child apart from the one in this trail -this also indicates that any other tree where the nodes {u, w 1 , . . . , w k , v} are permuted is still isomorphic to T while preserving the collection of subsets along maximal paths. Firstly, ∩ i C i ̸ = ∅, since this collection arise from a DRT which has a root. Note that there might be more than one element in this intersection (e.g. consider T to be the subtree starting from v 2 in (B) in Figure <ref type="figure" target="#fig_9">13</ref>,</p><formula xml:id="formula_85">C 1 ∩ C 2 = {v 2 , v 8 }).</formula><p>Pick any one of them, say v 0 and start a tree with v 0 as the root. Each successive step in this algorithm proceeds as follows: once a new child for an existing node in the tree constructed so far is identified, it is attached at the parent node and this element is removed from all current subsets which shared this vertex and had the parent node as history. A history h is maintained for each of the subsets which keep track of the last element removed from this set. Thus when the root is fixed, we set h i = v 0 for all i.</p><p>For each node v in the tree so far, gather all C i whose history is v and select u ∈ ∩ i:hi=v C i which appears a maximal number of times in these subsets. Note that the number of times it appears must be the membership number for that particular node in any tree which preserves the subsets. If there is a unique u satisfying the above, it must be a child of v. If not, firstly u must be a descendant of v (since there were subsets which had both v and u) -suppose (v, w 1 , . . . , w k , u) be such a trail, then m(w 1 ) ≥ m(u), which contradicts the unique maximality property of u. If there are more than one such u (say the maximal count is n), pick any of them. We note that any other node which is shared by the current C i with history v and count less than n cannot be a child of v (since membership must be monotone). Attach this as a child of v and make the changes to the subsets (remove u from each subset which had v as history and contained u) and their history (change history of each of these subsets to u) accordingly. Repeat until all subsets are emptied (for an illustration, see Figure <ref type="figure" target="#fig_10">14</ref>).</p><p>Step 1: Suppose at a stage we select u as the child of a v already in the tree and decide to append u to the tree for the next step. We first note that u cannot be present in any other subset whose history was not v.</p><p>If not, suppose u is also present in some other subset with history w. From the tree so far, get the smallest subtree containing both v and w and let r be the root of this subtree with at least two children r 1 , r 2 (since there are two different trails from r to v and w and this is the smallest possible such subtree). This means that either there was a subset with r, r 1 and no r 2 or there was a set with r, r 2 and no r 1 (otherwise based on the construction, r 1 , r 2 would not be separate children of r), lets call this ⋆. Now we note that since u appears in subsets with history v (descendant of r 1 say) and w (descendant of r 2 ), there were two subsets one containing r, r 1 , v, u and one containing r, r 2 , w, u. Now we recall that the original collection comes from some DRT T and hence in that DRT, there is either a unique trail from the root to u containing r or a unique trail from the root to r containing u. In this tree if either r 1 or r 2 is in this trail, then ⋆ is violated since, say r 1 is in the trail, any subset containing r, r 2 would contain r 1 . Thus, both r 1 , r 2 must be descendants of r -one cannot be descendant of the other since again ⋆ is violated. Thus, there must exist two distinct trails from the root to r 1 and to r 2 containing both r, u. This means m(r 1 ) &lt; m(u) and m(r 2 ) &lt; m(u), which contradicts the choice that r 1 and/or r 2 were chosen as children of r in the construction when u had a higher count. Thus, we establish that after a node u is attached to a parent v by this process and the node u is removed from all subsets containing it with history v, then u is not present in any other subset -this implies that the process does not lead to any tree where the underlying undirected graph has a cycle.</p><p>Step 2: We argue that particular decisions at stages where are multiple choices do not affect the final tree structure. Suppose in this process we come to a stage for the first time when there are more than one such maximal elements. Suppose at this stage, the current subsets are C1 , . . . , CI and the target parent is u in the tree (which must appear in at least one of the Ci 's histories, otherwise this cannot be a parent). Suppose w 1 , . . . , w L all have the maximal count. There must be a partition of these w ℓ into E 1 , . . . , E Q such that w i , w j ∈ E q if and only if w i and w j both appear together in a subset. To prove this, start with w 1 and start filling a set E 1 with w 1 and all other w's which appear together with w 1 in some subset. If this covers all w's, we terminate. Else, there is some other w 2 not in this set and do the same process. In this process, suppose we come across a w ∈ E 1 that is present with w 2 in some subset. We argue that is not possible. If so, it means there is a subset with w 1 , w and not w 2 (otherwise w 2 would be in E 1 ) and a subset with w 2 , w and not w 1 (again else, w 2 would be in E 1 ). Then we can append w 1 as a child of w and then w 2 as a child of w (still one of the maximal elements) and end up with w being in two subsets with different histories (w 1 and w 2 ), which by Step 1, is not possible. Thus, we are sure to have such a partition. Now we note that for each such E q of size L q , the tree must have a trail</p><formula xml:id="formula_86">u → v 1 → • • • → v Lq ,</formula><p>where the v ℓ is any permutation of E q , such that each of v 1 , . . . , v L Q -1 has only one child. If any of these has another child, then it contradicts the fact that they all had the same maximal count (recall membership strictly decreases among children iff there are more than 1). Thus, all possible trees that incorporate these nodes can only differ up to a permutation of the nodes within a trail of the above form and it is easy to see that a bijection can be defined between any two of such trees which preserve the connectivity. Thus, all such trees are isomorphic. This process terminates in finite time since ∪ i C i is finite to start with. This construction not only ensures that each C i can be attained from the constructed tree's maximal paths but also m is monotone across levels of a trail. Furthermore, every step of this process is fixed and necessary, except possibly stages where the choice of u is not unique, where we showed that all possible different trees must be isomorphic. Thus, the tree structure is unique. Now, for the last part of the lemma, we can use the same argument as above. Starting with the collection of subsets obtained from T , we repeat the construction as in the proof. In this case, there will be no ambiguity at any stage regarding the order in which to add children, since m(u) &lt; m(v) (strict inequality) whenever u is a descendant of v in T . Thus, if we find multiple nodes having same membership at a stage as potential children, they all have to be children of the parent node and hence, the underlying DRT can be reconstructed uniquely.</p><p>Remark. We add a remark on the uniqueness of topic tree's levels. The level of a node (i.e., the depth of the node from the root) represents the position of the corresponding topic in the topic hierarchy. In practical applications, starting from the root, which represents common topic shared by all documents, the parent node has more than one children at each level (other than leaf). This incorporates the idea that each topic has several subtopics with each considered as a further specialization within that topic. Other than this interpretation, this assumption makes the level of a particular node unique. As an illustration, consider trees (B) and (C) in Figure <ref type="figure" target="#fig_9">13</ref> and suppose we use a common ρ, π for these two trees. Clearly, they lead to the same model. However the level of ρ(v 2 ) and ρ(v 8 ) are not uniquely identified. The last part of Lemma 2.1 gives a sufficient condition under which levels of each topic are unique.</p><p>We note that any collection of such subsets may not arise from a DRT. The condition that we know that there exists a DRT which gives rise to this collection is necessary. Lemma 2.1 implies that if we know that there is at least one DRT which generates this collection, then any other DRT which also generates it must be isomorphic. An example of a collection of subsets which cannot arise from a DRT would be {{v 1 , v 2 , v 4 }, {v 1 , v 2 , v 3 }, {v 1 , v 3 }}. Any directed tree which generates these must contain an undirected cycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Moments of the Model</head><p>In this section we discuss the moments of</p><formula xml:id="formula_87">X ∈ [V ] n ∼ p X [n] |ω .</formula><p>We note that X is a conditionally iid draw from a Categorical distribution with parameter K k=1 h k θ k , where the distribution of h ∈ ∆ K-1 follows a mixture of sparse Dirichlet distributions, which we delve into in this section.</p><p>We say h follows a mixture of sparse Dirichlet distributions (MSD) with respect to DRT T and parameters π ∈ ∆ I-1 and α ∈ R I + , if h has a mixture distribution: with probability π i , h φi ∼ Dir Ji (α i ) and h φ c i = 0, where h φi = (h k : v k ∈ φ i ). Thus, with probability π i , only a part of h (as dictated through the paths of T ) is non-zero and follows a Dirichlet distribution, while the remaining part is degenerate with all coordinates 0. To illustrate, consider the left DRT in Figure <ref type="figure">3</ref>. Since K = 7, we have h ∈ ∆ 6 . Let P 1 be the distribution of h such that (h 1 , h 2 , h 4 ) ∼ Dir 3 (α 1 ) and h 3 = h 5 = h 6 = h 7 = 0 -this corresponds to the first path (whose leaf is v 4 ). As another example, P 3 (corresponding to path ending at v 6 ) is the distribution of h such that (h 1 , h 3 , h 6 ) ∼ Dir 3 (α 3 ) and h 2 = h 4 = h 5 = h 7 = 0. Then h ∼ i π i P i . Thus, for our model,</p><formula xml:id="formula_88">X [n] |h ∼ ⊗ n Cat(Θ ⊤ h) with h ∼ MSD(π, α|T )</formula><p>, where Θ is the K × V matrix with topics θ k as the rows (note that it includes all the topics in the model). We look at the moments of the distribution of h first.</p><p>For the first moment,</p><formula xml:id="formula_89">E[h k ] = i π i E Pi [h k ] = i π i αi Jiαi 1(v k ∈ φ i ) = i∈[I]:v k ∈φi π i /J i . Note that this implies v k1 , . . . , v k L are children of v k ⇒ E[h k ] = ℓ∈[L] E[h k ℓ ].</formula><p>In particular, for the leaves of the DRT, if a leaf v k is in path φ i , then E[h k ] = π i /J i and starting at the roots, E[h k ] for other nodes can be computed as the sum of the values for its children, as given in the above display. We remark that this property arises because of the symmetric Dirichlet distribution being used (α depends only on the path, not the depth). Similarly, for the second moment, we have</p><formula xml:id="formula_90">E[h k1 h k2 ] = i π i E Pi [h k1 h k2 ] = i:v k 1 ∈φi π i αi(αi+1) Jiαi(Jiαi+1) if k 1 = k 2 i:v k 1 ,v k 2 ∈φi π i α 2 i Jiαi(Jiαi+1) if k 1 ̸ = k 2</formula><p>using the second moments for the Dirichlet distribution (sum over an empty set is 0). Similarly, we can use the n-order moments of the Dirichlet distribution to compute the n-order moments for h. We illustrate this with the example in case of the left DRT in Figure <ref type="figure">3</ref>, with ρ 1 as in the figure. As before, h ∈ ∆ 6 in this case and suppose π 1 , π 2 , π 3 , π 4 correspond to paths φ 1 , . . . , φ 4 , ending in leaves v 4 , v 5 , v 6 , v 7 respectively. The first moments for h and hence X 1 ∼ p X [1] ,ω are as follows (note that J i = J(= 3) in this case):</p><formula xml:id="formula_91">M (1) := E[h] = 1 J , π 1 + π 2 J , π 3 + π 4 J , π 1 J , π 2 J , π 3 J , π 4 J ⊤ , E[X 1 ] = Θ ⊤ M (1) .</formula><p>The second moment for h is the</p><formula xml:id="formula_92">K × K matrix M (2) with M (2) k1,k2 = E[h k1 , h k2 ].</formula><p>The form of M (2) and corresponding second moment of the observations for this case takes the form</p><formula xml:id="formula_93">M (2) = E[h ⊗ h] =           a 1,2,3,4 b 1,2 b 3,4 b 1 b 2 b 3 b 4 b 1,2 a 1,2 0 b 1 b 2 0 0 b 3,4 0 a 3,4 0 0 b 3 b 4 b 1 b 1 0 a 1 0 0 0 b 2 b 2 0 0 a 2 0 0 b 3 0 b 3 0 0 a 3 0 b 4 0 b 4 0 0 0 a 4           , E[X 1 ⊗ X 2 ] = Θ ⊤ M (2) Θ.</formula><p>where</p><formula xml:id="formula_94">a I = i∈I π i (α i + 1) J i (J i α i + 1) b I = i∈I π i α i J i (J i α i + 1)</formula><p>.</p><p>We note that M</p><p>(2) k1,k2 is non-zero only if there is a path in T such that v k1 and v k2 both are on it. Thus, the sparse-structure of M (2) inherits from the tree structure of T . In a similar manner, higher order moment tensors M (n) can also be expressed in terms of the parameters, allowing us to compute</p><formula xml:id="formula_95">E[X 1 ⊗ • • • ⊗ X n ] for X [n] ∼ p X [n] |ω .</formula><p>We remark that M (n) will also be sparse, with M (n) k1,...,kn only non-zero if there exists a path in T such that v k1 , . . . , v kn are all on it (here k 1 , . . . , k n ∈ [K]).</p><p>As opposed to the study of identifiability in <ref type="bibr" target="#b0">Anandkumar et al. (2012)</ref>, the moment tensors E</p><formula xml:id="formula_96">[X 1 ⊗ X 2 ⊗ X 3 ], E[X 1 ⊗ X 2</formula><p>] and E[X 1 ] cannot be 'diagonalized' (algebraically combined to form Θ ⊤ DΘ, with D as a diagonal tensor) easily, even with the knowledge of α. Applying techniques in tensor decomposition, including Kruskal decomposition, is also not straightforward, precisely because of this challenge in diagonalising the moments. We hope to resolve this issue in the future -also, we expect to recover the sparsity pattern in E[h ⊗ h] from the data moments, which would help in estimating the tree structure as well. We postpone these to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C Proofs in Section 4</head><p>C.1 Proof of Lemma 4.1 -augmented tree-directed Hausdorff metric satisfies the triangle inequality</p><p>Proof. Let φ 1 j , . . . , φ I j be enumerations of the maximal paths of the DRTs in ω j for j ∈ {1, 2, 3}. Let σ 12 be the optimal permutation for d H+ (ω 2 , ω 3 ) as in Equation ( <ref type="formula" target="#formula_34">3</ref>) and similarly σ 23 for d H+ (ω 2 , ω 3 ). Let σ = σ 23 • σ 12 , then clearly σ ∈ S(I). Then we have</p><formula xml:id="formula_97">d H+ (ω 1 , ω 3 ) ≤ i d H (G 1 i , G 3 σ(i) ) + π 1 i -π 3 σ(i) ≤ i d H (G 1 i , G 2 σ12(i) ) + d H (G 2 σ12(i) , G 3 σ(i) ) + |π 1 i -π 2 σ12(i) | + |π 2 σ12(i) -π 3 σ ( i) | = i d H (G 1 i , G 2 σ12(i) ) + |π 1 i -π 2 σ12(i) | + i d H (G 2 σ12(i) , G 3 σ(i) ) + |π 2 σ12(i) -π 3 σ ( i) | = d H+ (ω 1 , ω 2 ) + i d H (G 2 σ12(i) , G 3 σ23•σ12(i) ) + |π 2 σ12(i) -π 3 σ23•σ12(i) | = d H+ (ω 1 , ω 2 ) + i ′ d H (G 2 i ′ , G 3 σ23(i ′ ) ) + |π 2 i ′ -π 3 σ23(i ′ ) | V = V ′ = [K],</formula><p>hence this means that the collection of sets of nodes along maximal paths of T and T ′ are the same -by application of lemma 2.1, we conclude that T ∼ = T ′ . Furthermore, τ identifies a unique isomorphism from T to T ′ and ∥ρ(v) -ρ ′ (τ (v))∥ &lt; ϵ, which completes the proof. Of course, we can also conclude that corresponding π i 's are also ϵ-close.</p><p>In particular part: The 'if' part of the proof is trivial. We show the 'only if' part. Assume d H+ = 0. Enumerate the maximal paths of T and T ′ to get φ 1 , . . . , φ I and φ ′1 , . . . , φ ′I respectively. By definition, we have a permutation τ of [I], such that conv(ρ(φ i )) = conv(ρ ′ (φ ′τ (i) )) and π(φ i ) = π ′ (φ ′τ (i) ) for all i. By assumption (A1), this means that for every i, the set ρ(φ i ) is the same as the set ρ ′ (φ ′(τ (i) ). This immediately gives that K = K ′ (K is the number of vertices in T ).</p><p>We wish to apply Lemma 2.1, but that applies only to DRT, not the topic maps. To resolve it, for the tree T = (V, E, v) we consider a new DRT T = ( Ṽ, Ẽ, ṽ), where {ρ(u) : u ∈ V} is the set of vertices and (ρ(u), ρ(u ′ )) ∈ Ẽ iff (u, u ′ ) ∈ E and ṽ = ρ(v). Having constructed T from T and ρ and T ′ from T ′ and ρ ′ , we can now apply Lemma 2.1 to guarantee that T ∼ = T ′ . However, by construction, T ∼ = T and T ′ ∼ = T ′ , which together gives T ∼ = T ′ .</p><p>We are only left to find the unique isomorphism σ as in the statement of the lemma. We claim that it suffices to show that one exists. If there were two such isomorphisms σ and σ ′ , then ρ(u) = ρ ′ (σ(u)) = ρ ′ (σ ′ (u)) for every vertex u of T , this implies that σ(u) = σ ′ (u) since ρ ′ is one-to-one (definition of topic map). Thus, σ = σ ′ .</p><p>Recall the existence of a permutation τ of [I]. Consider a matched pair of maximal paths, φ 1 of T and φ ′τ (1) of T ′ . Let φ 1 = (φ 1 1 , . . . , φ 1 J1 ), then since conv(ρ(φ 1 )) = conv(ρ ′ (φ ′τ (1) )), by assumption (A1), ρ(φ 1 ) must be a permutation of ρ ′ (φ τ (1) ). Define the unique map σ which maps u ∈ φ 1 to u ′ ∈ φ τ (1) such that ρ(u) = ρ ′ (σ(u)). Now, we keep repeating and expanding the domain of σ. At step i = 2, . . . , I, take another pair of maximal paths φ i and φ ′τ (i) . For all u ∈ φ i \ ∩ 1≤i&lt;I φ i , define σ(u) as previously based on the fact that ρ(φ i ) must be a permutation of ρ ′ (φ ′τ (i) ). It is easy to check that the constructed σ satisfies all conditions in the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Proof of Lemma 4.2</head><p>In the lemma, the ambient space is irrelevant. Thus, take A, A ′ , S as subsets of R d , say. </p><formula xml:id="formula_98">A ′ = S ′ + v ′ . Then, by assumption, dim S = dim S ′ . If A ∩ A ′ ̸ = ∅, let x ∈ A ∩ A ′ . Then, A = S + x and A ′ = S ′ + x. We claim that A ∩ A ′ = (S ∩ S ′ ) + x. Any y ∈ (S ∩ S ′ ) +</formula><p>x is of the form y = v + x where v ∈ S ∩ S ′ and hence, y ∈ A and y ∈ A ′ (as A = S + x). Conversely, suppose y ∈ A ∩ A ′ , then y -x ∈ S and y -x ∈ S ′ , hence y -x ∈ S ∩ S ′ , thus y = (y -x) + x ∈ (S ∩ S ′ ) + x. Thus, dim A ∩ A ′ = dim S ∩ S ′ . Now, either dim S ∩ S ′ = dim S (iff S = S ′ ) or dim S ∩ S ′ &lt; dim S (otherwise), which proves the statement of the lemma.</p><p>Part 4: Similar to part 3. Note that for subspaces S, S ′ with dim S &lt; dim S ′ , either dim S ∩ S ′ = dim S (iff S is a subspace of S ′ ) or dim S ∩ S ′ &lt; dim S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Proof of Theorem 1</head><p>Before we look at the proof, we make the following observations regarding mixture models of the form k π k G k , where the mixture components G k are supported on simplices S k with affine hulls A k . For a mea-sure G on ∆ V -1 and a measureable H ⊂ ∆ V -1 such that G(H) &gt; 0, we define the restricted and normalized measure G| H by G| H (A) = G(A ∩ H)/G(H). Using Lemma 4.2, we have 1. For two component measures G i , G j supported on S i , S j respectively with dimensions d i &lt; d j , (πG i + (1 -π)G j ) | Ai = G i . This occurs since G j is absolutely continuous with respect to the Hausdorff measure on A j and hence any measurable subset of A i is a null set with respect to this measure, since d i &lt; d j . Note that the other case is not true, i.e. (πG i + (1 -π)G j ) | Aj is, in general, not G j . 2. For two components G i and G j with d i = d j , by Assumption (B1), A i ∩ A j is a strictly lower dimensional flat (by Lemma 4.2). Thus,</p><formula xml:id="formula_99">(πG i + (1 -π)G j ) | Ai = G i and (πG i + (1 -π)G j ) | Aj = G j .</formula><p>To prove this, say the first one, we only note that for any measurable A ⊂ A i , G j (A) = 0 since either A ∩ S j = ∅ or if they have non-empty intersection, A ∩ S j is a null set with respect to G j (since it is a subset of the strictly lower dimensional flat</p><formula xml:id="formula_100">A i ∩ A j ). 3. For G = k π k G k and affine space A such that G(A) &gt; 0, G| A (B) = k π k G k (B ∩ A)/G(A). This</formula><p>follows from the definition of restricted and normalized measure. Let us denote G k (B ∩ A)/G(A) = Gk (B) and think of π k Gk as the contribution from component k towards G| A . For an affine space A and a convex polytope S, A ∩ S is either ∅ or S or a strictly lower dimensional (&lt; dim S) convex polytope (by lemma 4.2). For k such that A ∩ S k = ∅, clearly, Gk is the 0 measure. For k such that dim(A ∩ S k ) &lt; dim S k , also Gk is the 0 measure, since for any B, B ∩ A ∩ S k is a null set with respect to G k (recall G k is absolutely continuous with respect to the Hausdorff measure on S k ). Combining these, we obtain that for a mixture of the form G = i π i G i , for any affine space A of dimension d, the only measures among the components which may contribute to G| A are those for which dim G i ≤ d. Moreover, for a component G i with d i = d which contributes to this, it must have A i = A.</p><p>Proof. Firstly, let X [n] = (x 1 , . . . , x n ) with x j ∈ [V ] and consider the sequence of random variables Xn ∈ ∆ V -1 where ( Xn ) v = j∈[n] 1(x j = v)/n being the sample mean. Let µ n be the law of Xn , then µ n is a probability measure on ∆ V -1 . Recall from the model (e.g. see Section 3.2 and Equation <ref type="formula" target="#formula_23">2</ref>), that conditional on η ∼ G, x j 's are i.i.d. from the measure Cat(•|η). Thus, conditionally given η, by Strong Law of Large Numbers, Xn |η → η a.s. as n → ∞ which yields µ n ⇒ G. Now, since V (P n,ω,α , P n,ω ′ ,α ′ ) → 0 as n → ∞, then it shows that G(ω, α) = G(ω ′ , α ′ ). We now show that this implies that the tree topology and the topic map are both uniquely identified.</p><p>Arguing rigorously, we have 1. µ n ⇒ G: Xn can be seen as Xn = j Y j /n where Y 1 , . . . , Y n |η ∼ Mult(1, η), i.e. Y j = e v with probability η v (η ∈ ∆ V -1 ), where e k = (0, . . . , 0, 1, 0, . . . , 0) ⊤ , with 1 in the kth coordinate. We prove that the characteristic function of Xn converges to that of G pointwise, which will ensure weak convergence. To start,</p><formula xml:id="formula_101">φ Y |η (t) = E Y ∼Mult(1,η) [e it ⊤ Y ] = v η v e itv . Now, φ Xn (t) = EE e it ⊤ j Yj /n | η = E φ Y |η (t/n) n = v η v e itv n G(dη) = v η v 1 + it v n + O 1 n 2 n G(dη) = 1 + i t ⊤ η n + O 1 n 2 n G(dη) → e it ⊤ η G(dη), n → ∞</formula><p>where the last limit is by dominated convergence theorem. Thus, noting that the last expression is indeed the characteristic function of η ∼ G, we have the desired result. Furthermore, E[ Xn ] = EE[ Xn |η] = η, which shows W 1 (µ n , G) → 0. 2. Let µ n|ω and µ n|ω ′ be the law of Xn where X 1 , . . . , X n |η ∼ Mult(1, η), η ∼ G(ω, α) and G(ω ′ , α ′ ) respectively. µ n|ω , µ n|ω ′ are measures supported on ∆ V -1 , a bounded subset of R V . By Theorem 6.15 from <ref type="bibr" target="#b31">Villani et al. (2009)</ref>, we know that for every fixed n, W 1 (µ n|ω , µ n|ω ′ ) ≤ CV (µ n|ω , µ n|ω ′ ), where C = diam∆ V -1 and furthermore, by data processing inequality, we have V (µ n|ω , µ n|ω ′ ) ≤ V (P n,ω , P n,ω ′ ) (since Xn is a statistic of X = (X 1 , . . . , X n )). Thus,</p><formula xml:id="formula_102">W 1 (P, Q) ≤ W 1 (P, P n ) + W 1 (P n , Q n ) + W 1 (Q n , Q) ≤ W 1 (P, P n ) + CV (P n,ω,α , P n,ω ′ ,α ′ ) + W 1 (Q n , Q) → 0</formula><p>showing that W 1 (P, Q) = 0, and hence</p><formula xml:id="formula_103">P = Q. From our model, G(ω, α) = i∈[I] π i G i where G i = Dir Ji (α i ) # L i with L i (β) = Θ ⊤</formula><p>i β is the component measure corresponding to path φ i of T (fix an enumeration) and</p><formula xml:id="formula_104">S i = convΘ i is the support of G i . Similarly we have G(ω ′ , α ′ ) = i∈[I ′ ] π ′ i G ′ i , with G ′ i supported on S ′ i (fix enumeration for paths of T ′ ).</formula><p>Recall each of the constituent measures G i (resp. G ′ i ) is supported on the polytope S i (resp. S ′ i ) and is absolutely continuous with respect to Hausdorff measure on A i = affS i (resp. A ′ i ). For brevity of notations, we denote G(ω, α), G(ω ′ , α ′ ) as G, G ′ respectively. Without loss of generality, we permute the components (relabel) of G so that d 1 ≤ d 2 ≤ • • • ≤ d I , where d i = dim S i , and similarly for components in G ′ . Now we start the main argument for the proof. We have G = G ′ and we want to identify the components in the first step. Firstly, it is easy to see that</p><formula xml:id="formula_105">d 1 = d ′ 1 . If not, suppose d 1 &lt; d ′ 1 , then taking B = S 1 , clearly G(B) &gt; 0 while G ′ (B) = 0. Thus, we have d 1 = d ′</formula><p>1 . Now we consider G| A1 . By our arguments in the digression and assumption (B1), we know that G| A1 = G 1 . Thus, G ′ | A1 must also be G 1 . However, we know the only components contributing to</p><formula xml:id="formula_106">G ′ | A1 are G ′ i with d ′ i ≤ d 1 . Since d 1 is the minimum, hence any such d ′ i = d 1 ,</formula><p>and thus any such i must satisfy A ′ i = A i . But by assumption (B1) on ρ ′ , we know that no two of such measures (components of ω ′ ) have the same affine hull. This implies that there is a unique</p><formula xml:id="formula_107">G ′ i with d ′ i = d 1 such that G ′ i = G 1</formula><p>, which would also imply d H (S 1 , S ′ i ) = 0. Furthermore, since π 1 = P (G 1 ) = P ′ (G ′ i ) = π ′ i , we conclude that the corresponding mixture probabilities are same too. Thus, we identify a unique component in G ′ , such that it exactly matches G 1 (for symmetric Dirichlet as in our case, G 1 = G ′ i also implies α 1 = α ′ isee ).</p><p>Having identified this component, we remove it, i.e. we consider</p><formula xml:id="formula_108">G = i̸ =1 πi 1-π1 G i and G′ = j̸ =i π ′ j 1-π ′ i G ′ j , since we had G = G ′ and showed G 1 = G ′ i , π 1 = π ′ i ,</formula><p>we now have G = G′ . Now, we can apply the same argument iteratively. This process terminates in finite time since at each step we are removing a component from the finite mixture. Note that Assumption (B2) ensures that each step in this process is valid and there are no extraneous components left at the end. Furthermore, in this process, we have ensured that G ′ also has exactly I components (otherwise the process terminates with one of G or G′ having at least one non-trivial component left) and each component of G matches a unique component of G ′ . This implies that d H+ (ω, ω ′ ) = 0 by definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Proof of Theorem 2</head><p>Following the same strategy as in the proof of Theorem 1, we arrive at G(ω, α) = G(ω ′ , α ′ ). Using the same definitions as before, permute the labels of the components of ω (without loss of generality) such that</p><formula xml:id="formula_109">d 1 ≤ • • • ≤ d I</formula><p>Firstly, arguing as in the preceding theorem, we have min{d ′ i } = d 1 . Considering G| A1 , we get that G ′ A1 = G 1 (note that ω satisfies assumption (B1)). However, at this stage we cannot argue that this identifies a unique G ′ i as before since ω ′ does not necessarily satisfy the assumption (in particular, there might be two components sharing the same affine hull). Here we utilize the knowledge of I to recover the components. Suppose, if possible,</p><formula xml:id="formula_110">G ′ 1 G ′ 2 be two component measures of G ′ such that G 1 = π ′ 1 G ′ 1 + π ′ 2 G ′ 2 .</formula><p>Then, by argument as in the preceding theorem, it must hold that A ′ 1 = A ′ 2 = A 1 -for any other G i (from ω), either d i &gt; d 1 or G i is supported on a different affine space (assumption (B1)). In either case, there are I -1 distinct affine spaces for the {G i : i ̸ = 1}, while there are only I -2 possible {G ′ i : i ̸ = 1, i ̸ = 2}. This cannot be possible since each G ′ i accounts for exactly one affine space. Thus, there cannot be two G ′ 1 , G ′ 2 accounting for G 1 . Thus, we conclude that there is a unique G</p><formula xml:id="formula_111">′ i with A ′ i = A 1 and π ′ i G ′ i = π 1 G 1 . This gives d H (G 1 , G ′ i ) = 0 and π ′ i = π 1 .</formula><p>The rest of the proof is identical to that of Theorem 1.</p><p>Appendix D Proofs in Section 5 D.1 Proofs in Section 5.1</p><formula xml:id="formula_112">D.1.1 Proof of Lemma 5.1 Proof. Associate each sample X := X [n] = (X 1 , . . . , X n ) with a V dimensional vector η(X) ∈ ∆ V -1 , where η(X) v = 1 n j∈[n] 1(X j = v) for v = 1, . . . , V . The density of X [n] given ω takes the form p n,ω (X [n] ) = i∈[I] π i Si p(X [n] |η)G i (dη) = i∈[I] π i Si exp   n v∈[V ] η(X) v log η v   G i (dη).</formula><p>Given a fixed permutation σ of [I], we denote</p><formula xml:id="formula_113">S ′ i to indicate S ′ σ(i) and π i to indicate π ′ σ(i) for notational simplicity. For X ∈ [V ] n , let a i (X) = π i Si p(X|η)G i (dη) and b i (X) = π ′ σ(i) S ′ i p(X|η ′ )G ′ i (dη ′ ). so that p n,ω (X) = i a i (X), p n,ω ′ (X) = i b i (X).</formula><p>By Jensen's inequality, for any X ∈</p><formula xml:id="formula_114">[V ] n , i a i (X) log a i (X) b i (X) ≥ i a i (X) log i a i (X) i b i (X)</formula><p>.</p><p>Adding over all X ∈ [V ] n , we get</p><formula xml:id="formula_115">KL(p n,ω ∥p n,ω ′ ) = X∈[V ] n p n,ω (X) log p n,ω (X) p n,ω ′ (X) = X i a i (X) log i a i (X) i b i (X) ≤ X i a i (X) log a i (X) b i (X) = i X π i Si p(X|η)G i (dη) log π i π ′ i + log Si p(X|η)G i (dη) S ′ i p(X|η ′ )G ′ i (dη ′ ) which gives KL(p n,ω ∥p n,ω ′ ) ≤ i π i log π i π ′ i X Si p(X|η)G i (dη) =1 + i π i KL Si p(X|η)G i (dη) S ′ i p(X|η ′ )G ′ i (dη ′ ) ≤ i π i log π i π ′ i + KL Si p(X|η)G i (dη) S ′ i p(X|η ′ )G ′ i (dη ′ ) = i π i | log π i -log π ′ i | + KL Si p(X|η)G i (dη) S ′ i p(X|η ′ )G ′ i (dη ′ ) ≤ 1 c 1 i π i |π i -π ′ i | + i π i n c 0 W 1 (G i , G ′ i ) ,</formula><p>where the last inequality follows from Lemma 6 in Nguyen (2015) (for the second term) and min π ≥ c 1 (for the first term).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.2 Proof of Corollary 1</head><p>Proof. Lemma 7 in Nguyen (2015) (which applies since each component has J topics and non-obtuse property 5.1 is assumed to hold for all associated component polytopes) states that W</p><formula xml:id="formula_116">1 (G i , G ′ i ) ≤ C δ d H (S i , S ′ i ) for any two component measures G i , G ′ i supported on polytopes S i , S ′ i ,</formula><p>where C δ &gt; 0 is an absolute constant depending on δ as in 5.1. Together with Lemma 5.1, we have for any permutation σ</p><formula xml:id="formula_117">KL(p n,ω ∥p n,ω ′ ) ≤ n c 0 i π i W 1 (G i , G ′ σ(i) ) + 1 c 1 i π i |π i -π ′ σ(i) | ≤ nC δ c 0 i d H (S i , S ′ i ) + 1 c 1 i |π i -π ′ σ(i) | ≤ nC δ c 0 ∨ 1 c 1 i d H (S i , S ′ σ(i) ) + |π i -π ′ σ(i) |</formula><p>Since the above holds for any permutation, it holds true for σ * , the minimizer of the above sum, which gives the desired result because of the definition of d H+ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Prior Concentration Property</head><p>We recall the definition of the KL-ball B K to be used for stating the prior concentration property. Let P denote the space of probability densities on ∆ V -1 . For a density p 0 ∈ P and radius δ &gt; 0, define</p><formula xml:id="formula_118">B K (p 0 , δ) = p ∈ P : KL(p 0 ∥p) ≤ δ 2 , K 2 (p 0 ∥p) ≤ δ 2 where K 2 (p∥q) = P log p q -KL(p∥q) 2 = log p(x) q(x) 2 P (dx) -KL(p∥q) 2</formula><p>is the KL-variation. We verify that under appropriate conditions, the prior puts enough mass on all sufficiently small KL-balls centered at the truth (this condition is typically referred to as the KL-property of the prior).</p><p>Proposition 2. Suppose ω * = (ρ * , π * ) ∈ Ω(T ), where T ∈ T(I, J). Π is a prior distribution on Ω(T ) satisfying conditions (1), ( <ref type="formula" target="#formula_23">2</ref>), (4) from Theorem 3. Then, for all sufficiently small δ &gt; 0,</p><formula xml:id="formula_119">Π (p n,ω ∈ B K (p n,ω * ,δ ) ≥ c δ 2 n[n ∨ log(1/δ)] 2 K(V -1)+I-1 (14)</formula><p>where the constant c depends on c 0 , c 1 , c 2 , c 3 .</p><p>Proof. Denoting G i to be the component measures supported on component polytope S i and π i as the corresponding path probability (and similarly G * i , S * , π * ). Consider the set</p><formula xml:id="formula_120">E ϵ = {∥ρ(u) -ρ * (u)∥ ≤ ϵ, ∀u ∈ V, |π i -π * i | ≤ ϵ, ∀i ∈ [I]} .</formula><p>Under the prior (by condition 3), we have</p><formula xml:id="formula_121">Π(E ϵ ) ≥ cϵ K(V -1)+I-1 (15)</formula><p>where c is a constant depending on c 2 , c 3 . Now, we argue that for all ω ∈ E ϵ , we have an upper bound (in terms of ϵ) for KL(p n,ω * ∥p n,ω ) and K 2 (p n,ω * ∥p n,ω ). Take such a ω ∈ E ϵ . First, for any i, construct the coupling Q i between G * i and G i as the law of (η i1 , η i2 ) where η i1 = j∈[J] β j ρ * (φ i j ) and η i2 = j∈[J] β j ρ(φ i j ), where β = (β 1 , ,β J ) ∼ Dir J (α 0 ). Since ∥ρ(u) -ρ * (u)∥ ≤ ϵ for all nodes, it is clear that for this coupling, the expected ∥η</p><formula xml:id="formula_122">i1 -η i2 ∥ ≤ ϵ, hence W 1 (G * i , G i ) ≤ ϵ.</formula><p>Then, using h 2 ≤ K/2 and Lemma 5.1, we have for any</p><formula xml:id="formula_123">ω ∈ E ϵ , h 2 (p n,ω * , p n,ω ) ≤ 1 2 KL(p n,ω * ∥p n,ω ) ≤ n 2c 0 i π * i W 1 (G * i , G i ) + 1 2c 1 i π * i |π * i -π i | ≤ ϵ n 2c 0 + 1 2c 1 . Now, let p n,Gi (X) = Si p(X|η)G i (dη)</formula><p>be the conditional density of a document, given that it is generated from the ith component, thus p n,ω (X) = i π i p n,Gi (X). For every i, the conditional density ratio p n,G * i /p n,Gi ≤ 1/c n 0 and hence,</p><formula xml:id="formula_124">p n,ω * p n,ω = i π * i p n,G * i i π i p n,Gi &lt; i π * i π i p n,G * i p n,Gi ≤ 1 c 1 c n 0 ,</formula><p>where the inequality is trivial given all terms are positive (by expanding the terms). This implies that</p><formula xml:id="formula_125">X∈[V ] n p n,ω * (X) 2 /p n,ω (X) ≤ 1/c 1 c n 0 ,</formula><p>where the sum is taken over all possible realizations of documents in [V ] n . Now, we invoke a bound from <ref type="bibr" target="#b36">Wong and Shen (1995)</ref> Theorem 5 on the KL divergence. According to this result, if p 2 /q &lt; M for two densities p, q, then for some universal constant ϵ 0 &gt; 0, as long as h(p, q) ≤ ϵ &lt; ϵ 0 , we have KL(p∥q) = O(ϵ 2 log(M/ϵ)) and K 2 (p∥q) = O(ϵ 2 [log(M/ϵ)] 2 )., where the big O constants are universal.</p><p>In our case, we have shown that ω</p><formula xml:id="formula_126">∈ E ϵ ⇒ h 2 (p n,ω * , p n,ω ) ≤ ϵ n 2c0 + 1 2c1 , and p 2 n,ω * /p n,ω ≤ 1/c 1 c n 0 . Hence, we have for ω ∈ E ϵ K 2 (p n,ω * ∥p n,ω ) = O ϵ(nc 1 + c 0 ) 2c 0 c 1 log n log 1 c 0 + 1 2 log 2c 0 ϵ(nc 1 + c 0 )c 1 2 .</formula><p>Now, if we choose ϵ = δ 2 /n 3 , the term inside the big O is of the order δ 2 [n + log n log(1/δ)] 2 /n 2 , in which the dominating term is δ 2 for all small δ if n &gt; log(1/δ). On the other hand, if we set ϵ = δ 2 /n(log(1/δ) 2 , then the term in the big O is of the order δ 2 [log log(1/δ) + log(1/δ) + n) 2 /[log(1/δ)] 2 , which is dominated by δ 2 if n ≤ log 1/δ. Thus, in either case, depending on n &lt; log(1/δ) or otherwise, we have K 2 = O(δ 2 ) (note that by the result, control on K 2 is enough for control on KL). Hence, comined with Eq 15 we obtain</p><formula xml:id="formula_127">Π(p n,ω ∈ B K (p n,ω * , δ)) ≥ c δ 2 n [n ∨ log(1/δ)] 2 K(V -1)+I-1</formula><p>for some constant c, which depends on c 0 , c 1 , c 2 , c 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Model Entropy</head><p>For a metric space (X , d), the covering number N (ϵ, X , d) is defined as the minimal number of balls of radius ϵ, i.e. sets of the form {x ∈ X : d(x, x 0 ) ≤ ϵ} which cover X . In our case we are interested in N (ϵ, {P n,ω |ω ∈ Ω(T )}, h), i.e., the covering number for our model space in terms of the Hellinger metric. It captures the model space complexity and plays a fundamental role in constructing tests to differentiate two densities from this space. However, it is easier to bound N (ϵ, Ω(T ), d H+ ), i.e. covering number for the parameter space in terms of a suitable metric (in our case the augmented tree-directed Hausdorff metric), since it allows us to use bounds on covering number for nicer Euclidean spaces like probability simplex. Result in Corollary 1 allows us to translate these bounds to covering number for the density space.</p><p>Proposition 3. For Ω = {ω ∈ Ω(T ) | associated polytopes satisfy Property 5.1}, we have</p><formula xml:id="formula_128">log N (ϵ, {P n,ω : ω ∈ Ω}, h) ≲ [K(V -1) + I -1] log C n ϵ 2</formula><p>where C n = n 2c0 ∨ 1 2c1 and ≲ hides additive constants in terms of I, K, V , free of ϵ. Proof. We argue in two steps as follows.</p><p>Entropy for parameter space: Suppose π1 , . . . , πL be an optimal δ-cover of ∆ I-1 with L = N (δ, ∆ I-1 , ∥•∥ 1 ) and θ1 , . . . , θM be an optimal δ ′ -cover of ∆ V -1 with M = N (δ ′ , ∆ V -1 , ∥•∥ 2 ). We argue that this allows us to construct a cover for Ω(T ), and hence also for Ω ⊂ Ω(T ). Define</p><formula xml:id="formula_129">C = {(θ 1 , . . . , θ K , π)|θ k ∈ { θj } j∈[M ] , π ∈ {π ℓ } ℓ∈[L] } with |C| = M K L. For each ω = (θ 1 , . . . , θ k , π) ∈ C, define ω = (ρ, π) as ρ(v k ) = θ k for all k ∈ [K] and π(φ i ) = π i</formula><p>for all i (fixing an enumeration v 1 , . . . , v K of V and φ 1 , . . . , φ I of Φ(T )). Thus, every ω ∈ C can be mapped to a unique ω ∈ Ω(T ), call this map h. Now, given an arbitrary ω ∈ Ω(T ), let θ 1 , . . . , θ K be the topics with θ k = ρ(v k ) and π i = π(φ i ).</p><p>By construction, there is an element ω = ( θ1 , . . . , θ K , π) ∈ C such that θ k -θk 2 ≤ δ ′ for all k and ∥π -π∥ 1 ≤ δ. Hence we have</p><formula xml:id="formula_130">d H+ (ω, h • ω) ≤ i d H (S i , S ′ i ) + |π i -π ′ i | &lt; i δ ′ + δ = Iδ ′ + δ</formula><p>where we used the fact that</p><formula xml:id="formula_131">d H (S, S ′ ) ≤ max θ∈extrS min θ ′ ∈extrS ′ ∥θ -θ ′ ∥ 2 ∨ max θ ′ ∈extrS ′ min θ∈extrS ∥θ -θ ′ ∥ 2</formula><p>which follows from Lemma 1(a) <ref type="bibr" target="#b24">Nguyen (2015)</ref>. Hence, for every ω ∈ Ω(T ), there exists ω ∈ C such that d H+ (ω, h • ω) &lt; I(δ ′ + δ). Thus, taking δ ′ = ϵ/2I and δ = ϵ/2, we get</p><formula xml:id="formula_132">N (ϵ, Ω(T ), d H+ ) ≤ N ϵ 2 , ∆ I-1 , ∥•∥ 1 × N ϵ 2I , ∆ V -1 , ∥•∥ 2 K .</formula><p>From existing upper bounds on covering numbers and using the fact that ∥x∥ 2 ≤ ∥x∥ 1 for all x ∈ R V , we have</p><formula xml:id="formula_133">N ϵ 2 , ∆ I-1 , ∥•∥ 1 ≤ 10 ϵ I-1 N ϵ 2I , ∆ V -1 , ∥•∥ 2 ≤ N ϵ 2I , ∆ V -1 , ∥•∥ 1 ≤ 10I ϵ V -1</formula><p>which together with the previous display gives</p><formula xml:id="formula_134">N (ϵ, Ω(T ), d H+ ) ≤ C 1 ϵ K(V -1)+I-1</formula><p>where C = 10 I-1 × (10I) K(V -1) is a constant in terms of ϵ. Thus, we have</p><formula xml:id="formula_135">log N (ϵ, Ω, d H+ ) ≤ log C + [K(V -1) + I -1] log(1/ϵ).</formula><p>Cover for density space: Let ω 1 , . . . , ω H be an ϵ-optimal cover for Ω. We argue that {p n,ωj |j ∈ [H]} is a δ-cover for {P n,ω |ω ∈ Ω}, for an appropriate δ depending on ϵ. Indeed, thanks to Corollary 1 and the fact that any ω ∈ Ω satisfies non-obtuse corner property, for every such ω, there is ω h with d H+ (ω, ω h ) ≤ ϵ and hence</p><formula xml:id="formula_136">h 2 (p n,ω , p n,ω h ) ≤ 1 2 KL(p n,ω ∥p n,ω h ) ≤ nC δ 2c 0 ∨ 1 c 1 d H+ (ω, ω h ) &lt; C n ϵ. Thus, {p n,ωj |j ∈ [H]} is a √ C n ϵ-cover, i.e. δ = √ C n ϵ, hence ϵ = δ 2 /C n . Thus, N (ϵ, {P n,ω |ω ∈ Ω}, h) ≤ N (ϵ 2 /C n , Ω, d H+ )</formula><p>which gives, using the covering number bound from the first part of the proof</p><formula xml:id="formula_137">log N (ϵ, {P n,ω |ω ∈ Ω}, h) ≤ log C + [K(V -1) + I -1] log(C n /ϵ 2 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Proof of Theorem 3</head><p>We apply Theorem 8.9 from Ghosal and van der Vaart (2017) with P m,1 = {p n,ω |ω ∈ Ω}, where Ω is defined in Proposition 3 and P m,2 = ∅ and verify the conditions in the theorem. Trivially Π(P m,2 ) = 0, so the last condition is satisfied. Also, we note that mϵ 2 m,n → ∞ with ϵ m,n = log m/m in part (1) and ϵ m,n = log(m ∨ n)/m in part (2) and moreover, ϵ m,n → 0 in both cases.</p><p>Case 1: (fixed n case) For condition 1, we use Proposition 2 to obtain (note that as</p><formula xml:id="formula_138">ϵ m = log m/m → 0, n &lt; log(1/ϵ m ) eventually) log Π(p n,ω ∈ B K (p n,ω * , ϵ m )) ≳ (K(V -1) + I -1) log ϵ 2 m n log(1/ϵ m ) = -c ′ (2 log(1/ϵ m ) + log log(1/ϵ m ) + log n) ≳ -C ′ log m = -C ′ mϵ 2 m for sufficiently large constant C ′ depending on K, V, I, since log(1/ϵ m ) = 1 2 (log m -log log m) is dominated by log m as m → ∞.</formula><p>For condition 2, we use Proposition 3 along with a similar argument to obtain</p><formula xml:id="formula_139">log N (ϵ m , P m,1 , h) ≲ c ′ log √ C n ϵ m = c ′ log(1/ϵ m ) + C ≲ C ′′ mϵ 2 m .</formula><p>for some sufficiently large C ′′ , following a similar argument as before. Finally, applying Theorem 8.9 from Ghosal and van der Vaart (2017), we have the desired result.</p><p>Case 2: (Increasing n) We again use Proposition 2 noting that n</p><formula xml:id="formula_140">&lt; log(1/ϵ m,n ) iff n &lt; 1 2 (log m - log log(m ∨ n)) if n ≲ log m, otherwise n &gt; log(1/ϵ m,n ) eventually. In the first case, i.e. n ≲ log m, log Π(p n,ω ∈ B K (p n,ω * , ϵ m,n )) ≳ (K(V -1) + I -1) log ϵ 2 m,n n log(1/ϵ m,n ) = -c ′ (2 log(1/ϵ m,n ) + log log(1/ϵ m,n ) + log n) ≳ -C ′ log(m ∨ n) = -C ′ mϵ 2 m,n have G(B(η, 2ϵ) ∩ S) ≥ G(A 1 (ϵ)) = Γ(Kα) Γ(α) K A1(ϵ) K-1 k=1 β α-1 k 1 - K k=1 β k α-1 dβ ≥ Γ(Kα) Γ(α) K A1(ϵ) β k ≥δ ∀k K-1 k=1 β α-1 k 1 - K k=1 β k α-1 dβ ≥ min{1, δ α-1 } Γ(Kα) Γ(α) K   i≤K-1 min{β * +ϵ/K,1} max{β * i -ϵ/K,δ} β α-1 i dβ i   ≥ min{1, δ α-1 } Γ(Kα) Γ(α) K i≤K-1 min{1, δ α-1 }2ϵ/K = 2Γ(Kα) K K-1 Γ(α) K min{1, δ α-1 } K ϵ K-1 = C(α, K)ϵ K-1 α ≤ 1 C(α, K)ϵ Kα-1 α &gt; 1</formula><p>where we used for δ &lt; x &lt; 1, x α-1 ≥ δ α-1 for α &gt; 1 and x α-1 ≥ 1 for α ≤ 1, thus x α-1 ≥ min{1, δ α-1 } covering both cases. The last line follows from this and the choice of δ = ϵ/3K, where C</p><formula xml:id="formula_141">(α, K) is a constant. Moreover, if α ≥ α 1 , C(α, K) ≥ C 0 (α 1 , K) only depending on α (since Γ(Kα)/Γ(α) K is increasing) and moreover, if α ≤ α 2 , then if α 2 ≤ 1, then ϵ K works while if α 2 &gt; 1, then ϵ Kα2-1 works as a uniform lower bound.</formula><p>Now, if dim S = p &lt; K -1, then for η ∈ S, we can write η as the convex combination of some p+1 topics out of the extreme points of S, without loss of generality, suppose η = p+1 i=1 β * i θ i , for some β * ∈ ∆ p . Let β * j = 0 for j = p+2, . . . , K. Consider the set A</p><formula xml:id="formula_142">1 (ϵ) = {β ∈ ∆ K-1 : |β i -β * i | &lt; ϵ/K, i = 1, . . . , K -1}. For any β ∈ A 1 (ϵ), and η ′ = k β k θ k , we have ∥η -η ′ ∥ = ∥ k (β k -β * k )θ k ∥ ≤ 2 k∈[K-1] |β k -β * k | ≤ 2ϵ. WLOG assume β * p+1 ≥ 1/(p + 1).</formula><p>Then, for all ϵ &lt; 1/(p + 1), we have</p><formula xml:id="formula_143">G(B(η, 2ϵ) ∩ S) ≥ G(A 1 (ϵ)) = Γ(Kα) Γ(α) K A1(ϵ) K-1 k=1 β α-1 k 1 - K k=1 β k α-1 dβ ≥ δ 1∨α-1 Γ(Kα) Γ(α) K   i≤p min{β * +ϵ/K,1} max{β * i -ϵ/K,δ} β α-1 i dβ i     K i=p+2 ϵ/K δ β α-1 i dβ i   ≥ Γ(Kα) Γ(α) K δ 1∨α-1 p+1 ϵ K p ϵ K α -δ α K-p-1 K i=p+2 α = Γ(Kα) Γ(α) K δ 1∨α-1 p+1 ϵ K p ϵ K α -δ α K-p-1 α K-p-1 = Γ(Kα) Γ(α) K ϵ 3K 1∨α-1 p+1 ϵ K p ϵ K α -ϵ 3K α K-p-1 α K-p-1</formula><p>≥ C(α, K, p)ϵ (p+1)(1∨α-1) ϵ p ϵ α(K-p-1) = C(α, K, p)ϵ p+α(K-p-1) α ≤ 1 C(α, K, p)ϵ αK-1 α &gt; 1</p><p>Combined with equation ( <ref type="formula">17</ref>), we have</p><formula xml:id="formula_144">|P n,ω (η ∈ A * ϵ ) -P n,ω ′ (η ∈ A * ϵ )| ≥ cϵ (1∨α)J-1 1 -6V exp - 2nϵ 2 V</formula><p>Noting that ϵ = d U H (ρ, ρ ′ )/4, this concludes the proof. A similar argument works if (ii) was true above.</p><p>Why one of (i) or (ii) must hold: Since d H (S, S ′ ) = 4ϵ 1 , there either exists x ∈ S with d(x, S ′ ) = 4ϵ 1 or x ′ ∈ S ′ with d(x ′ , S) = 4ϵ 1 . Assume the former. Let A * = B(x, ϵ 1 ) ∩ S. Clearly, A * ⊂ S, also, since d(x, S ′ ) = 4ϵ 1 , we have A * ∩ S ′ = ∅, hence A * ⊂ S \ S ′ . Note that min y∈A * d(y, S ′ ) = 3ϵ 1 . Hence, for any ϵ ≤ ϵ 1 , say ϵ = ϵ 1 , we have A * ϵ ∩ S ′ ϵ = ∅. Finally, G(A * ) = i π i G i (B(x, ϵ) ∩ S i ). Noting that x ∈ S i for some i, we get G(A * ) ≥ c 1 G(B(x, ϵ 1 ) ∩ S i ) ≥ c 1 C(J, α)ϵ (1∨α)J-1 1</p><p>, by application of Lemma 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.8 Proof of Theorem 5</head><p>In this proof, we assume that all ω are in the support of the prior Π. This entails that for any sequence ω n , there is a convergent subsequence ω n k with d U H (ρ n k , ρ * ) → 0 (here ω = (ρ, π) for topic map ρ and path probability π), where ω * is also in the support (and thereby satisfies the properties assumed on the prior). We first show that as n → ∞, the Hellinger distance between densities is lower bounded by a function of the union Hausdorff metric between the corresponding topic maps. This requires combining the inverse bound (local property, since we need d U H ≤ ϵ 0 for this result), along with a global identifiability property (which we can also deduce by modifying the inverse bound proof).</p><p>Firstly, we note that for ϵ 0 as in Theorem 4, there exists δ 0 &gt; 0 such that for all ω in the support of Π with d U H (ρ, ρ 0 ) ≥ ϵ 0 , we have lim inf n→∞ h 2 (p n,ω0 , p n,ω ) ≥ δ 0 . This can be seen by modifying the proof of the inverse bound Theorem 4 -in the very last step, where we used Lemma 5.2 under the assumption that ϵ 1 &lt; ϵ 0 , if ϵ 1 &gt; ϵ 0 , then we can lower bound G(B(x, ϵ 1 )∩S i ) ≥ G(B(x, ϵ 0 )∩S i ) ≥ Cϵ where the right side is further lower bounded by a constant multiple of ϵ (1∨α)J-1 if ϵ ≤ d U H (ρ, ρ 0 ) ≤ ϵ 0 and C 1 ϵ (1∨α)J-1 ≳ 6V exp(-nϵ 2 /8V ), which is true is ϵ &gt; c log n/n. Thus, in such a case, h 2 (p n,ω0 , p n,ω ) ≥ c ′ ϵ 2[(1∨α)J-1] . Combining the above cases, we obtain the following: for a sequence ϵ n → 0, lim inf</p><formula xml:id="formula_145">n→∞ inf ω∈Supp(Π) d U H (ρ,ρ0)≥ϵn ϵn≳ √ log n/n h 2 (p n,ω0 , p n,ω ) ϵ 2[(1∨α)J-1] n ≥ C ′ &gt; 0.</formula><p>Hence, with ϵ m,n as in the statement of the theorem, as m, n → ∞, (which ensures ϵ m,n ≳ log n/n),</p><formula xml:id="formula_146">Π d U H (ρ, ρ 0 ) ≥ ϵ m,n | X [m] [n] ≤ Π h 2 (p n,ω0 , p n,ω ) ≥ C ′ ϵ 2[(1∨α)J-1] m,n | X [m] [n] ≤ Π h(p n,ω0 , p n,ω ) ≥ C ′ 1 ϵ (1∨α)J-1 m,n | X [m] [n]</formula><p>≤ Π h(p n,ω0 , p n,ω ≥ C ′ 1 log(m ∨ n)</p><formula xml:id="formula_147">1 m + 1 n 1/2 | X [m] [n]</formula><p>→ 0 in probability, where the last convergence follows from density estimation rate Theorem 3. required was around 43 minutes (about 0.0001186 seconds per iteration per document). The run times shown in the table are for a single chain, in the simulations, 8 chains were run in parallel on a 2.9 GHz Quad-Core Intel Core i7 processor machine. Apart from the total size of the corpus, the run time also depends on the vocabulary size V (rather small here in our toy experiments) and the size of the DRT -for experiment 1, the DRT has two paths while for experiment 2, the DRT has four paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 DRTs for Experiment 3</head><p>We use a total of 8 different DRTs (including LDA with true total number of topics) -these are shown in Figure <ref type="figure" target="#fig_4">8</ref>. The tree tree0 in the figure is the true DRT responsible for the data generation in this experiment. The results obtained from this experiment are discussed in the main text, see Section 7. We note that the first 5 DRTs all have K = 5 (which is the actual total number of topics for this experiment), while the last 3 DRTs have more -however, the true DRT tree0 is a sub-tree of each of these last 3 trees. We have seen that while these tree have similar held-out log-likelihood compared to the first tree, one of the paths for these 3 trees have very small probability assigned to it. On the other hand, out of the first 5 DRTs, tree0 has a significantly higher held-out log-likelihood for each data instance.</p><p>Remark 13 (Note on initialization). For each dataset, we ran the Gibbs sampler across 8 independent chains. Four of these chains were started randomly. The other four were initialized using a combination of usual LDA and hierarchical clustering. We provide a brief description of the latter method. Given the total number of topics K based on the DRT, we fit a LDA model with K topics and a small Dirichlet parameter α (to promote sparsity) -the results from this model provide the topics and also the mixing proportions for each document β i ∈ ∆ K-1 . However, in our model, each document is associated to only J topics (out of the K), where J is the depth -hence, for each document, we chose the J coordinates of the mixing probabilities with highest values -made the other coordinates 0 and normalized the vector, to obtain βi ∈ ∆ K-1 with βi 0 = J. Then we perform an agglomerative hierarchical clustering on these modified document-wise mixing probabilities. Finally, the sub-tree from this hierarchical clustering starting from the root, matching the desired DRT was chosen -this provided the initialization for the collapsed Gibbs sampling for our model. Although a heuristic approach, in most of the cases, this initialization turned out to be better than random initialization, based on the log-likelihood at the end of the chains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Additional Details about NYT Corpus Data Analysis</head><p>We have already discussed pre-processing of the dataset. The training data has 641 documents with a vocabulary of V = 500. We use 25 different DRTs (shown in Figure <ref type="figure" target="#fig_10">17</ref>) and also usual LDA with K ∈ [2, 12]. Among the 25 DRTs used, we have all possible DRTs with J = 3, I ≤ 5 and a few others with J = 3, I = 6 and some with J = 4.  Model selection in this case was done using held-out log-likelihood along with few other geometric considerations from the model. Table <ref type="table" target="#tab_4">2</ref> shows the results from the DRTs (where there are at least two component polytopes) and Table <ref type="table" target="#tab_5">3</ref> shows the corresponding results for the LDA model. Note that since the LDA can be seen as special case with a DRT with I = 1, there is only one topic polytope -hence the other columns in Table <ref type="table" target="#tab_4">2</ref> do not make sense for LDA models. All the LDA models have lower held-out log likelihood compared to most of the DRT models. For the DRT models, first we filtered only models where the minimum path probability was more than 0.1, then we further filtered to cases where the minimum width and the minimum projection distance of topics to other component polytopes was more than the median of all the values. This was to ensure that the components are well-separated and each is given sufficient weight. This left us with trees Tree8 and Tree20 (shown in Figure <ref type="figure" target="#fig_12">15</ref>, with the latter having the better held-out log likelihood. We chose this tree as the most appropriate for this dataset. However, we also include Tree8 and Tree7 (which respects the topics structure based on the corpus news category knowledge) for comparison of the topics. This is not a very in-depth model selection process, which is not the focus of this paper -but, we should keep in mind that for practical applications, interpretation of topics are not often aligned with performance measures like perplexity. We already presented the topic hierarchies for Tree20 and Tree7 in the main text, here we include the estimated topic hierarchy from Tree8, shown in Figure <ref type="figure" target="#fig_13">16</ref>, which can be interpreted similar to Tree20 or Tree7, as discussed in the paper.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of a corpus (sample of 2016 New York Times articles) which demonstrates that one topic polytope is insufficient for explaining the heterogeneity in the corpus. Details about the data are in Section 7. The right two pictures show estimated component topic polytopes using a tree-directed topic model (the model fitting does not take the labeled news categories into account).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>|C, L) based on a sample of size S from p(C, L|X [m]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Illustrations for Experiments 1 (top row) and 2 (bottom row): The left-most column shows the underlying DRTs, the middle and right columns show the true polytopes, estimated polytopes and documents projected on the labelled PCA-subspaces (for one particular run of the experiments, m, n given at the top respectively). Each gray circle represents a document (as the document mean), the true polytopes are shown in solid colors and the estimated polytopes in blue dotted lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: (Left) Results for Experiment 1, (Middle) Results for Experiment 2 and (Right) One example of Gibbs sampler getting stuck at a local mode (compare with Figure 6 bottom row middle column).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The collection of DRTs considered for Experiment 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: Results from Experiment 3 with tree0 as the true DRT: (Left) held-out log likelihood per document for each tree (each color represents a particular corpus); (Right) minimum path probability for each tree (tree4 is left out, since it is an LDA, equivalent to a tree with a single path and hence, the path probability is 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Estimated Topic Hierarchy for NYT articles (showing 10 top words for each topic) -DRT with I = 4, J = 4, K = 11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: DRT and estimated topic polytopes for the model whose hierarchy is shown in Figure 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Estimated Topic Hierarchy for NYT articles (showing 10 top words for each topic) -DRT with I = 4, J = 3, K = 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Directed Rooted Tree: (A) is not a DRT since the underlying undirected graph has a cycle, although the graph has no directed cycle. (B), (C), (D) all are DRTs of size (I = 4, J = {2, 2, 3, 3}, K = 8). Notice how (B), (C) have the same intuitive structure (nodes v 2 and v 8 are swapped) while (D) has a totally different structure -the first two are isomorphic, while (B) and (D) are not. (C) also shows membership value of each node (blue number beside each node) -e.g. if we look at node v 3 , the subtree starting from it is highlighted in yellow and has 2 leaves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Proof. Part 1 :</head><label>1</label><figDesc>An affine space A can be represented as {x ∈ R d : Ax = b}, i.e., as the solution of a system of linear equations where A ∈ R m×d , b ∈ R m . A convex polytope S is the intersection of a set of hyperplanes and can be represented as {x ∈ R d : Cx ≤ d}, where the inequality is coordinate-wise, C ∈ R n×d , d ∈ R n . By appending A and -A as rows in C (to form C ∈ R (n+2m)×d and appending b and -b to d (to form d ∈ R n+2m , we get the intersection of A and S as the solution set of {x ∈ R d : Cx ≤ d}, which by definition is another convex polytope. Part 2: Similar to part 1. Part 3: Let A = S + v, where S is a linear subspace of R d and v ∈ A, similarly let</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>it is a uniform lower bound, not depending on the d U H ), which would showlim inf n→∞ d TV (p n,ω , p n,ω ′ ) ≥ cϵ (1∨α)J-1 0 , as long as d U H (ρ, ρ ′ ) ≥ ϵ 0 . Since, √ 2h ≥ d TV ,we have for some δ 0 &gt; 0(ρ,ρ0)≥ϵ0h 2 (p n,ω0 , p n,ω ) ≥ δ &gt; 0.Now, for ω ∈ Supp(Π), if d U H (ρ, ρ 0 ) ≤ ϵ 0 , we can use Theorem 4, to have a lower bound depending on the d U H :√ 2h(p n,ω0 , p n,ω ) ≥ C 1 d U H (ρ, ρ 0 ) (1∨α)J-1 -6V exp(-nd U H (ρ, ρ 0 ) 2 /8V )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Visual representation of DRT-model selection for NYT data (corresponding to the results in Table 2) -the red dashed lines are the cut-offs chosen and the black marked points are Tree8 and Tree20, which passes all the filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Topic hierarchy for Tree 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Figure4: Illustration of topic hierarchies. The top row is the topic tree induced by an underlying DRT T and a topic map ρ : v i → θ i . The bottom row shows the geometry of the component polytopes resulting due to the corresponding topic tree above. Examples 1, 2 and 3 are approximately close topic hierarchies, while example 4 is not. Topic trees need not be isomorphic while isomorphic topic trees do not necessarily yield the same topic hierarchy.</figDesc><table><row><cell></cell><cell></cell><cell>θ 1</cell><cell></cell></row><row><cell>Example 1</cell><cell>Example 2</cell><cell>Example 3</cell><cell>Example 4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results for the different DRTs used: the columns are (i) index of the tree corresponding to Figure17, (ii) held-out log likelihood per document, (iii) minimum distance between distinct topics, (iv) minimum path probability, (v) minimum width of the component polytopes, (vi) minimum minimum-matching distance among distinct component polytopes, (vii) minimum Grassmanian distance between affine hulls of distinct component polytopes and (viii) minimum projection distance of topics to other component polytopes, not sharing that topic.</figDesc><table><row><cell>Figure 17: Different DRTs used for NYT dataset</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results for LDA fit on NYT data with different number of topics (K): computed the (i) held-out log likelihood per document, (ii) minimum distance between distinct topics and (iii) minimum width of the topic polytope.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>The result does not explicitly require the use of the Dirichlet distribution, and is applicable to any distribution p β on the probability simplex subject to a certain restriction. Specifically, under the well-specified setting as considered here, the restriction is related to placing sufficient mass near the boundary of the simplex, as discussed in the third remark after Theorem 4.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A DRTs and Isomorphisms</head><p>We recall a few standard definitions from graph theory <ref type="bibr" target="#b14">(Harary, 1969)</ref>. A graph consists of a set of vertices and a set of edges connecting pairs of vertices (in this paper vertex and node are used interchangeably). A directed edge is an ordered pair of vertices (u, v) which links from a parent vertex u toward a child vertex v while an undirected edge is a set of two vertices {u, v} which connect u and v (there is no sense of direction). If the edges in a graph are directed, we call it a directed graph. A trail is a sequence of vertices v 1 , . . . , v ℓ in an undirected graph such that consecutive pairs are connected and no edge is repeated. A cycle is a trail with same starting and ending vertices. A tree is an undirected graph with no cycle. In a directed graph, a vertex is called a root if it has no parent and a vertex is called a leaf if it has no children. The distance to a vertex from a root is defined as the length of the shortest trail from the root to that vertex. Finally, the undirected underlying graph of a directed graph (V, E) is an undirected graph with the same set of vertices V and set of edges {set(e) : e ∈ E}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Membership function</head><p>We define a function membership, denoted by m, on the vertices of a DRT, m : V → N. Suppose u ∈ V is a node in a DRT T = (V, E, v 0 ). Then, the subtree starting at u is a DRT. Suppose it is of size (I ′ , J ′ , K ′ ), then we define membership of u in the original DRT as I ′ . This function (illustrated in blue in (C) in Figure <ref type="figure">13</ref>) plays an important role in the sequel. Some trivial properties of m include that m(v 0 ) = I and m(v) = 1 for all leaf nodes v ∈ V. Furthermore, due to the DRT structure m(v) = u:(v,u)∈E m(u), which basically says that the membership of a node is the sum of memberships of its children. This implies that if u is a descendant of v (i.e. there is a directed trail from v to u), then m(v) ≥ m(u). We refer to this as the monotonicity of the membership function across trails. Suppose (v, w 1 , . . . , w k , u) is a directed trail in the tree, then m(u) = m(v) if and only if v and each of w 1 , . . . , w k has no other child apart from the one in this trail. As we shall see, the trails, as a set, in such a DRT starting from the root to a leaf gives rise to the topic hierarchy. In this context, some directed trees, while not DRT themselves (for example (A) in Figure <ref type="figure">13</ref>), can be expressed as a DRT (e.g. (B) or (C) in Figure <ref type="figure">13</ref>) which preserves these trails. In Figure <ref type="figure">13</ref>, any of (A), (B) and (C) has exactly 4 trails (as sets)</p><p>Here the first inequality is by definition and the second by the fact that both d H and | • | are metrics by their own right. The following chain of equalities merely use the definitions of σ 12 , σ 23 and σ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proof of Proposition 1</head><p>Proof. First part: For fixed ω 0 , consider</p><p>which considers the minimum inter-topic distance between topics from the same component or different topics (not the shared ones) across different components in r 1 and the minimum inter-component Hausdorff distance in r 2 , by our assumptions, r 1 &gt; 0, r 2 &gt; 0 (first one by injectivity of ρ and the second one by assumption (A2)). Furthermore, by assumption (A1), there is r 3 (ω 0 ) &gt; 0 such that for all i ∈ [I] and θ ∈ extrS i , there is a hyperplane passing through θ that makes at least angle r 3 (ω 0 ) with each of the edges of S i incident at that extreme point θ. Fix ϵ 0 (ω 0 ) sufficiently small (to be decided later).</p><p>Condition 1 : ϵ 0 should be such that for all ω &lt; ω 0 , if d H+ (ω 0 , ω ′ ) &lt; ϵ, then r 1 (ω) &gt; r 1 (ω 0 )/2, r 2 (ω) &gt; r 2 (ω 0 )/2 and ω ′ also satisfies the non-obtuse (existence of hyperplane) property with r 3 (ω) &gt; r 3 (ω 0 )/2. Now, we have d H+ (ω, ω ′ ) &lt; ϵ ≤ ϵ 0 , this shows that there is a permutation σ ∈ S(I) such that</p><p>without loss of generality, assume σ is the identity permutation (otherwise relabel the components). Hence, in particular, d H (S i , S ′ i ) &lt; ϵ for all i, which by Lemma 1(b) <ref type="bibr" target="#b24">Nguyen (2015)</ref> yields that d M (S i , S ′ i ) &lt; C 0 ϵ where d M is the minimal matching metric defined in 18 and C 0 is a constant depending on r 3 (ω 0 ). This shows that for every θ ∈ extrS i (because of assumption (A1), each topic θ from ω is an extreme point of some component polytope), there is a θ ′ ∈ extrS ′ i such that ∥θ -θ ′ ∥ &lt; C 0 ϵ (and also vice-versa). Thus, if ϵ ≤ ϵ 0 &lt; r 1 (ω 0 )/3C 0 , then ∥θ -θ ′ ∥ &lt; r 1 (ω 0 )/3 and hence the balls {B(θ, ϵ) : θ ∈ extrS i for some i ∈ [I]} are all disjoint. Thus, we have the partitions P 1 = {{u} : u ∈ T } for ω and P 2 = {{u ′ ∈ T : ∥ρ(u) -ρ ′ (u ′ )∥ &lt; C 0 ϵ} : u ∈ T } as a valid partition satisfying that for any P ∈ P i , there is a unique P ′ ∈ P 2 such that for any u ∈ P and u ′ ∈ P ′ , ∥ρ(u) -ρ ′ (u ′ )∥ &lt; C 0 ϵ. Let τ : P 1 → P 2 denote this map P → P ′ satisfying the above. Now consider a component polytope S 1 of ω 0 , with extreme points</p><p>) must be separated from each extreme point of S 1 . Firstly, for any v ∈ P τ -1 (1) , we have ∥ρ(v) -ρ ′ (v ′ 1 )∥ &lt; C 0 ϵ by construction of the partitions. Now, for any such v, since it is not an extreme point of</p><p>&lt; ϵ for ϵ &lt; 2r 1 /3. Thus, overall ϵ 0 satisfying Condition 1 above and ϵ 0 &lt; min{r 1 (ω 0 )/3C 0 (ω 0 ), 2r 1 (ω 0 )/3} works, all depending on ω 0 . Now, if the number of nodes of T and T ′ are same, say K, each set in the partition P 2 must be a singleton containing only one node (note that |P 1 | = K). By the condition proved in the last part, this means that v 1 , . . . , v k , with v i ∈ P i ∈ P 1 , are on a maximal path (equivalently, ρ(v 1 ), . . . , ρ(v k ) are extreme points of a component polytope</p><p>and the dominant term in the parenthesis in the second line in the above display is either log m, if m &gt; n or log n, if n &gt; m -which controls the lower bound in the last line.</p><p>In the second case,</p><p>again following a similar argument. Thus, in both cases, condition 1 for applying Theorem 8.9 from Ghosal and van der Vaart (2017) holds. For condition 2, we again use Equation ( <ref type="formula">3</ref>) to obtain</p><p>where again, the dominating term is either log m (coming from log(1/ϵ m,n ) or log n (coming from the second term) and they are both dominated by log(m ∨ n). Thus the desired result follows by applying Theorem 8.9 from Ghosal and van der Vaart (2017). </p><p>We note that the condition (A1') is slightly stronger than Assumption (B1) -Assumption (B1) allows the possibility of S i ⊂ S i ′ with dim S i &lt; dim S i ′ , while such a case is not allowed under condition (2) of the above result. In the case that each path of T has length J and topics in a component are affinely independent, the first condition is true. On the other hand, even if the paths have different depths, it is possible that the component polytopes have the same dimension, thereby satisfying the first condition. In particular, assumption (B1) along with the fact that component polytopes have same dimension imply condition (A1').</p><p>Proof. Let ρ satisfy Assumption (A1') and (I, J, K) be the size of T . Since d U H (ρ, ρ ′ ) = 0, we know that</p><p>i . Let S = ∪ i S i and A i = affS i , then by assumption, we know that A 1 , . . . , A I are affine spaces such that their pairwise intersections are lower dimensional. Furthermore,</p><p>showing that the intersection of S with any one of these affine spaces is the union of a convex polytope (S i ) and other convex polytopes (A i ∩ S i ), of dimension strictly smaller than A i (some or all of these could be empty). This shows that {A 1 , . . . , A I } is a unique set of I affine spaces such that S ⊂ ∪</p><p>Thus, since S = S ′ , also consisting of exactly I component polytopes and there are precisely I affine spaces (with pairwise trivial intersection), it shows that each component of S ′ must be lying in precisely one of these affine spaces. As a result, S ′ i can be identified from S ′ .</p><p>We state another result which connects d H and d U H , where we recollect d H to be the tree-directed Hausdorff metric d H (ρ, ρ ′ ) = min</p><p>Lemma D.2. Suppose {ρ n } n≥0 is a sequence of topic maps in Ω(T ). Then we have 1. For any ρ 0 ∈ Ω(T )</p><p>2. If ρ 0 satisfies Assumption (A1'), then</p><p>The proof uses the fact that Ω(T ) is compact and hence, any sequence ρ n has a convergent subsequence. We note that the above lemma states that if only ρ 0 satisfies assumption (A1'), then d U H and d H are equivalent in terms of local convergence. We also remark that due to the fact that d H is precisely d H+ for a fixed DRT T and uniform path probabilities, we have d H (ρ, ρ ′ ) = 0 iff there exists a unique automorphism (i.e., isomorphism between T and T ) σ on T such that ρ(u) = ρ ′ (σ(u)) for all vertex u of T . It is important to note that the last two lemmas use the fact that associated topic maps are defined on the same tree, so that the tree size is fixed.</p><p>Proof. Firstly, for each n, ρ n : V → ∆ V -1 where V is the set of vertices of the underlying DRT (with finite number of vertices). Enumerating these vertices as v 1 , . . . , v K , we can think of each ρ n as θ n) has a convergent subsequence. If we look along that subsequence, say θ * is the limit point, then we can define</p><p>Part (1) Suppose d H (ρ * , ρ 0 ) = 0. Using an enumeration φ i of the maximal paths of T , this means, by definition of d H that there exists a permutation σ of [I] such that S * i = S 0 σ(i) (here S * i is the topic polytope corresponding to path φ i using topic map ρ * and similarly S 0 i for ρ 0 ). This trivially implies that</p><p>Part (2) Conversely, suppose d U H (ρ * , ρ 0 ) = 0. Thus, we obtain that ∪ i S 0 i = ∪ i S * i , where we know that S 0 i satisfies the conditions in the statement. Hence, from ∪ i S 0 i , each of the component polytopes can be identified uniquely from Lemma D.1. Moreover, by assumption (A1), each S 0 i uniquely identifies the associated topics and dim S 0 i = J i -1, where J i is the length of path φ i of T associated with this component polytope. Note that there are exactly I many components in ∪ i S * i as well and both {S 0 i } i and {S * i } i come from the same underlying tree T (hence for each φ ∈ Φ(T ) of depth J i , there must be a component polytope with exactly J i extremal points), there must be a one-to-one correspondence between them. This implies that d H (ρ * , ρ 0 ) = 0. D.6 Proofs in Section 5.2 D.6.1 Proof of Lemma 5.2 Since all topics θ ∈ ∆ V -1 , ∥θ∥ ≤ 1. First consider the case where all topics are affinely independent, hence dim</p><p>Then, for all ϵ &lt; 1/K we using δ = ϵ/3K as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.7 Proof of Theorem 4</head><p>Proof. Given a document</p><p>/n, to be the observed document mean. By definition of total variation,</p><p>where the supremum is taken over all measurable subsets of ∆ V -1 . Since the conditional distribution of</p><p>given η is a product of i.i.d. multinomials, we have by Hoeffding's inequality</p><p>, where G k are the component latent measures. Treating η ∼ G as random, the above inequality holds with probability 1, hence</p><p>The same bound also holds for P η×X [n] |ω ′ , where</p><p>We now show that there exists A for which the first term in the above display is bounded from below. Let S = ∪ i S i and S ′ = ∪ i S ′ i be the union of the component polytopes arising from the two parameters ω, ω ′ respectively. Let ϵ 1 = d H (S, S ′ )/4. We prove that for with ϵ = ϵ 1 , one of the following holds:</p><p>. where A ϵ = A + B(0, ϵ) = {y : d(y, A) &lt; ϵ}. We first complete the proof assuming this is correct and then prove that it indeed is true.</p><p>Assuming that (i) above is true:</p><p>Experiment 1 (V = 10, K = 5) Recall the Hausdorff metric is defined as  <ref type="bibr">(18)</ref> which in turn, for convex polytopes S 1 , S 2 , each with J extreme points θ 1 , . . . , θ J (and θ ′ 1 , . . . , θ ′ J ), satisfies d M (S 1 , S 2 ) ≤ min τ ∈S J j θ j -θ ′ τ (j) .</p><p>Hence, (as max i a i ≤ i a i for a i ≥ 0) we obtain d H (S, S ′ ) ≤ inf</p><p>and we recognize the right side to be precisely d L2 . Hence, we have d U H (ρ, ρ ′ ) ≤ d L2 (ρ, ρ ′ ). Note that in the last display, the extreme points of the convex polytope S i are denoted as θ i,1 , . . . , θ i,J .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runtime</head><p>The running time for the Gibbs sampler in experiments 1 and 2 are shown in Table <ref type="table">1</ref>. For a choice of (m, n), we recall that the corpus had mn words in total. For m = 5000, n = 100 (total 5 × 10 5 words in the corpus) case in experiment 1, total time required was around 16 minutes (about 0.0000356 seconds per iteration per document). For m = 4000, n = 300 (total 1.2 × 10 6 words in the corpus) case in experiment 2, total time</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A spectral algorithm for latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning topic models-going beyond svd</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE 53rd annual symposium on foundations of computer science</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A fast algorithm with minimax optimal guarantees for topic models with an unknown number of topics</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bunea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wegkamp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimal estimation of sparse topic models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bunea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wegkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">177</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Correlated topic models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">147</biblScope>
			<date type="published" when="2006">2006a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Nested Chinese Restaurant Process and Bayesian Nonparametric Inference of Topic Hierarchies</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning -ICML &apos;06</title>
		<meeting>the 23rd International Conference on Machine Learning -ICML &apos;06<address><addrLine>Pittsburgh, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006b</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2003-01">2003. January</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning topic models: Identifiability and finite-sample analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">544</biblScope>
			<biblScope unit="page" from="2860" to="2875" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Utopian: User-driven topic modeling based on interactive nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1992" to="2001" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gaussian lda for topic models with word embeddings</title>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="795" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fundamentals of Nonparametric Bayesian Inference</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Van Der Vaart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Number 44 in Cambridge Series in Statistical and Probabilistic Mathematics</title>
		<meeting><address><addrLine>Cambridge ; New York</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">suppl 1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On posterior contraction of parameters and interpretability in bayesian mixture modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2159" to="2188" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Graph Theory</title>
		<author>
			<persName><forename type="first">F</forename><surname>Harary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<publisher>Addison-Wesley Series in Maethematics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On strong identifiability and convergence rates of parameter estimation in finite mixtures</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Online learning for latent dirichlet allocation. advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization via archetypal analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Javadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bayes factors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the american statistical association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">430</biblScope>
			<biblScope unit="page" from="773" to="795" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using svd for topic modeling</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">T</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">545</biblScope>
			<biblScope unit="page" from="434" to="449" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Latent semantic analysis: An approach to understand semantic of text</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kherwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Current Trends in Computer, Electrical, Electronics and Communication (CTCEEC)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="870" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pachinko allocation: Dag-structured mixture models of topic correlations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Flexible clustering via hidden hierarchical dirichlet priors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lijoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Prünster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rebaudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="213" to="234" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Convergence of latent mixing measures in finite and infinite mixture models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Posterior contraction of the population polytope in finite admixture models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="618" to="646" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Borrowing strengh in hierarchical Bayes: Posterior concentration of the Dirichlet base measure</title>
		<author>
			<persName><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inference of Population Structure Using Multilocus Genotype Data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Pritchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Donnelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetics</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="945" to="959" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling musical influence with topic models</title>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Understanding the Limiting Factors of Topic Modeling via Posterior Contraction Analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hierarchical Dirichlet Processes</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="1566" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A topic modeling analysis of tcga breast and lung cancer transcriptomic data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Osella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Caselle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancers</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">3799</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<title level="m">Optimal transport: old and new</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Variational inference for the nested chinese restaurant process</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Online variational inference for the hierarchical dirichlet process</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="752" to="760" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convergence rates of latent topic models under relaxed identifiability conditions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="66" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Convergence of de finetti&apos;s mixing measure in latent structure models for observed exchangeable sequences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1859" to="1889" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Probability inequalities for likelihood ratios and convergence rates of sieve mles</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="339" to="362" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dirichlet Simplex Nest and Geometric Inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yurochkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7262" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Geometric Dirichlet Means algorithm for topic inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yurochkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS 2016)</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2505" to="2513" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
