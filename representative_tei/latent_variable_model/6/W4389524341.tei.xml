<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Latent Variable Modeling for Knowledge-Grounded Dialogue Generation</title>
				<funder>
					<orgName type="full">Institute of Information &amp; communications Technology Planning &amp; Evaluation</orgName>
					<orgName type="abbreviated">IITP</orgName>
				</funder>
				<funder>
					<orgName type="full">Korea University</orgName>
				</funder>
				<funder ref="#_V5YVKWK">
					<orgName type="full">Korea government (MSIT)</orgName>
				</funder>
				<funder>
					<orgName type="full">Artificial</orgName>
				</funder>
				<funder>
					<orgName type="full">Ministry of Science and ICT(MSIT, Korea) Gwangju Metropolitan City</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Gunsoo</forename><surname>Han</surname></persName>
							<email>gunsoo.han@kakaobrain.com</email>
						</author>
						<author>
							<persName><forename type="first">Daejin</forename><surname>Jo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">Wontae</forename><surname>Nam</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eunseop</forename><surname>Yoon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Taehwan</forename><surname>Kwon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Seungeun</forename><surname>Rho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kyoung-Woon</forename><surname>On</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chang</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kakao</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName><surname>Kaist</surname></persName>
						</author>
						<author>
							<persName><surname>Krafton</surname></persName>
						</author>
						<title level="a" type="main">Efficient Latent Variable Modeling for Knowledge-Grounded Dialogue Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge-grounded dialogue generation requires to first retrieve appropriate external knowledge based on a conversational context and then generate a response grounded on the retrieved knowledge. In general, these two sequential modules, a knowledge retriever and a response generator, have been separately trained by supervised data for each module. However, obtaining intermediate labels of the ground-truth knowledge is expensive and difficult especially in open-domain conversation. Latent variable modeling can circumvent it and enables a joint training without the knowledge supervision. In this paper, we propose an efficient algorithm for this latent variable modeling that is able to leverage a large amount of dialogue data. In specific, rather than directly training the complex retriever, we adapt a query generator with an off-the-shelf retriever, and the query generator and response generator are simultaneously trained over the latent variable of query. Moreover, we employ the evidence lower bound as a training objective and modify it to efficiently and robustly perform the joint training. Experimental results on diverse knowledge-grounded dialogue datasets show that the proposed algorithm achieves state-ofthe-art performances even without the use of the annotated knowledge while maintaining the efficiency and scalability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, knowledge-grounded dialogue generation has drawn increasing attention especially as a key ingredient for open-domain conversational agents <ref type="bibr" target="#b8">(Dinan et al., 2018;</ref><ref type="bibr">Zhou et al., 2018b;</ref><ref type="bibr" target="#b52">Zhan et al., 2021;</ref><ref type="bibr" target="#b28">Liu et al., 2018;</ref><ref type="bibr">Zhou et al., 2018a;</ref><ref type="bibr" target="#b25">Lian et al., 2019;</ref><ref type="bibr" target="#b55">Zhao et al., 2020;</ref><ref type="bibr" target="#b17">Kim et al., 2020;</ref><ref type="bibr">Chen et al., 2022;</ref><ref type="bibr">kom;</ref><ref type="bibr">Shuster et al., 2022a,b;</ref><ref type="bibr" target="#b4">Cai et al., 2023;</ref><ref type="bibr" target="#b46">Wu et al., 2022;</ref><ref type="bibr" target="#b22">Lewis et al., 2020;</ref><ref type="bibr" target="#b13">Huang et al., 2021;</ref><ref type="bibr" target="#b42">Thulke et al., 2021;</ref><ref type="bibr" target="#b2">Anantha et al., 2021)</ref>. It usually obtains knowledge from external resources such as Wikipedia-based databases and web search engines since internal knowledge even in large-scale parametric language models <ref type="bibr" target="#b3">(Brown et al., 2020;</ref><ref type="bibr" target="#b41">Thoppilan et al., 2022;</ref><ref type="bibr" target="#b7">Chowdhery et al., 2022)</ref> is incomplete or outdated, and moreover it can provide hallucinated information.</p><p>In order for a dialogue response to be grounded on such external knowledge, the conversational agent generally consists of a knowledge retriever, which retrieves knowledge corresponding to a given dialogue context, followed by a response generator that produces an informative response based on the dialogue context and the retrieved knowledge. In many previous methods, supervised learning has often been applied to independently optimize each module using the ground-truth or gold knowledge <ref type="bibr" target="#b8">(Dinan et al., 2018;</ref><ref type="bibr">Shuster et al., 2022a,b;</ref><ref type="bibr">Glass et al., 2022;</ref><ref type="bibr">Adolphs et al., 2021a;</ref><ref type="bibr" target="#b32">Nogueira and Cho, 2017;</ref><ref type="bibr" target="#b29">Ma et al., 2020;</ref><ref type="bibr" target="#b47">Xie et al., 2022)</ref>. However, human annotation of knowledge information is cumbersome, expensive, and often incomplete due to the existence of multiple possible knowledge candidates and the overlooking of the target response. In addition, existing automatic annotations are generally limited to a simple extractive question answering <ref type="bibr" target="#b55">(Zhao et al., 2020;</ref><ref type="bibr" target="#b27">Liu et al., 2021)</ref>. This difficulty in obtaining annotations of knowledge information could be severe under the open-domain conversation and hinders the use of large-scale dialogue data.</p><p>Therefore, there have been a number of recent approaches to learn the knowledge retriever and the response generator without the knowledge supervision. In specific, they have treated the retrieved knowledge, document, or passage as an unobserved latent variable and adapt latent variable modeling based on approximated marginalization (e.g. top-k) <ref type="bibr" target="#b22">(Lewis et al., 2020;</ref><ref type="bibr" target="#b13">Huang et al., 2021;</ref><ref type="bibr" target="#b4">Cai et al., 2023;</ref><ref type="bibr" target="#b11">Guu et al., 2020)</ref>, reinforcement learning <ref type="bibr" target="#b55">(Zhao et al., 2020;</ref><ref type="bibr" target="#b54">Zhang et al., 2022;</ref><ref type="bibr">Chen et al., 2022;</ref><ref type="bibr" target="#b46">Wu et al., 2022)</ref> or variational methods <ref type="bibr" target="#b52">(Zhan et al., 2021;</ref><ref type="bibr" target="#b33">Paranjape et al., 2022;</ref><ref type="bibr" target="#b25">Lian et al., 2019;</ref><ref type="bibr" target="#b17">Kim et al., 2020;</ref><ref type="bibr" target="#b5">Chen et al., 2020;</ref><ref type="bibr" target="#b50">Xu et al., 2023)</ref>. However, joint training of the retriever along with the generator under this latent variable modeling has some restrictions in utilizing the retriever. For example, a retriever needs to produce a differentiable prior probability for the gradient propagation through the marginalization or to be intermittently updated to rebuild the whole passage index during training <ref type="bibr" target="#b16">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b22">Lewis et al., 2020;</ref><ref type="bibr" target="#b54">Zhang et al., 2022)</ref>. This leads to limitation in facilitating complicated or large-scale retrievers and further in leveraging large-scale data. In addition, the posterior probability used in the previous variational methods has been modeled by a separated network, which grows the training complexity and also incurs the discrepancy between the trainingtime and test-time knowledge samplings.</p><p>To overcome such restrictions, in this paper, we propose an efficient latent variable modeling, named ELVM, for knowledge-grounded dialogue generation. In particular, we reduce the burden for training a whole retriever by employing a query generator followed by an off-the-shelf retriever that is fixed during training. More precisely, in tandem with the response generator, the query generator rather than the retriever is jointly optimized by latent variable modeling, in which a query is taken as a latent variable, on paired observations of dialogue contexts and responses.</p><p>To this end, we exploit the variational method using the evidence lower bound (ELBO) <ref type="bibr" target="#b19">(Kingma and Welling, 2013;</ref><ref type="bibr" target="#b15">Jordan et al., 1999)</ref> and approximate the expected conditional likelihood in the ELBO by subset sampling from the prior distribution, which acts as the training objective for the response and the query generators. This approximation gets rid of an extra modeling of a surrogate posterior distribution or online posterior inference such as Markov Chain Monte Carlo (MCMC) and also reduces the training-inference discrepancy in knowledge retrieval. Moreover, we further modify the Kullback-Leibler (KL) regularization of the ELBO by constructing the approximated posterior distribution from the conditional likelihood and prior distribution and set this posterior distribution as a teacher for a distillation objective to further learn the query generator.</p><p>Experimental results show that the proposed ELVM allows to efficiently and robustly perform training without (1) the use of the annotated knowledge, (2) an explicit training of the knowledge retrieval, and (3) a complex posterior sampling. Especially, it significantly outperforms previous state-of-the-art methods for knowledge-grounded dialogue generation. In addition, the proposed posterior distillation improves the performances over the baseline that solely maximizes the expectation of the conditional likelihood.</p><p>Our main contributions can be summarized as: 2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Knowledge-Grounded Dialogue Generation</head><p>Knowledge-grounded conversation has been extensively studied through recently released public datasets <ref type="bibr" target="#b8">(Dinan et al., 2018;</ref><ref type="bibr">kom;</ref><ref type="bibr">Xu et al., 2022b;</ref><ref type="bibr" target="#b20">Kwiatkowski et al., 2019;</ref><ref type="bibr" target="#b2">Anantha et al., 2021;</ref><ref type="bibr" target="#b30">Moghe et al., 2018)</ref>. Existing approaches for this task have mostly exploited two successive modules, the knowledge retriever and the knowledgegrounded response generator. Many previous works have used manual annotations of gold knowledge provided by some of public datasets to optimize the modules <ref type="bibr" target="#b8">(Dinan et al., 2018;</ref><ref type="bibr">Shuster et al., 2022a,b;</ref><ref type="bibr">Glass et al., 2022;</ref><ref type="bibr">Adolphs et al., 2021a;</ref><ref type="bibr" target="#b32">Nogueira and Cho, 2017;</ref><ref type="bibr" target="#b29">Ma et al., 2020;</ref><ref type="bibr" target="#b47">Xie et al., 2022)</ref>. Specifically, SeekeR <ref type="bibr">(Shuster et al., 2022a)</ref> and BlenderBot3 <ref type="bibr">(Shuster et al., 2022b)</ref> have recently proposed to build a series of modules by a single transformer with different prompts for each module, and trained its transformer on a large number of modular tasks using annotated datasets. However, this manual annotation is expensive, time-consuming, and often inaccurate, which impedes the utilization of a large-scale dialogue data that is especially necessary for open-domain conversation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unsupervised Joint Training</head><p>Recently, unsupervised training methods have been widely applied, and many of them have tries to jointly learn the modules without knowledge labels <ref type="bibr" target="#b22">(Lewis et al., 2020;</ref><ref type="bibr" target="#b13">Huang et al., 2021;</ref><ref type="bibr" target="#b55">Zhao et al., 2020;</ref><ref type="bibr" target="#b54">Zhang et al., 2022;</ref><ref type="bibr" target="#b52">Zhan et al., 2021;</ref><ref type="bibr" target="#b33">Paranjape et al., 2022;</ref><ref type="bibr" target="#b25">Lian et al., 2019;</ref><ref type="bibr" target="#b17">Kim et al., 2020)</ref>. For example, PLATO-KAG <ref type="bibr" target="#b13">(Huang et al., 2021)</ref> has approximated the marginal likelihood by top-k knowledge samples while RetGen <ref type="bibr" target="#b54">(Zhang et al., 2022)</ref> has trained to reward knowledge retrieval with the highest utilization in response generation by reinforcement learning and mixture-of-experts ensembling.</p><p>Meanwhile, latent variable modeling based on variational methods has also been developed by several recent works. CoLV <ref type="bibr" target="#b52">(Zhan et al., 2021)</ref> has introduced collaborative latent spaces to reflect the inherent correlation between the knowledge selection and the response generation. SKT <ref type="bibr" target="#b17">(Kim et al., 2020)</ref> has developed a sequential latent variable model for the multi-turn knowledge-grounded dialogue generation. In order to perform the posterior sampling of knowledge selection during joint training, some works have proposed to separately train the posterior distribution model <ref type="bibr" target="#b33">(Paranjape et al., 2022;</ref><ref type="bibr" target="#b25">Lian et al., 2019)</ref> or the posterior information prediction model <ref type="bibr" target="#b5">(Chen et al., 2020)</ref>. Very recently, SPI <ref type="bibr" target="#b50">(Xu et al., 2023)</ref> has applied short-run MCMC <ref type="bibr" target="#b9">(Erik et al., 2019)</ref> for posterior sampling on the collaborative latent spaces.</p><p>While these latent variable modeling algorithms can effectively perform unsupervised joint training, the entire training of the retriever is still difficult and imposes restrictions on the selection of the retriever in terms of both a search algorithm and a knowledge resource. Furthermore, the additional posterior sampling through separate networks or multiple iterations in the previous variational methods also leads to increased training complexity as well as the training-inference discrepancy in knowledge generation. In contrast, the proposed ELVM can employ any kind of retriever in latent variable modeling since it controls a retrieval output by only changing of a generated query. Moreover, ELVM removes an extra posterior modeling or sampling by prior subset sampling and approximated posterior distillation, which leads to efficient training without the discrepancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Query Generation</head><p>When an off-the-shelf retriever is used for knowledge retrieval, how to make an input text query is important for obtaining appropriate documents. Especially, in knowledge-grounded dialogue, a self-contained query should be generated from the multi-turn dialogue context. A number of prior works have tried to train a supervised query rewriting model by human rewrites <ref type="bibr" target="#b51">(Yu et al., 2020;</ref><ref type="bibr" target="#b26">Lin et al., 2020;</ref><ref type="bibr" target="#b44">Vakulenko et al., 2021;</ref><ref type="bibr" target="#b45">Voskarides et al., 2020)</ref>. Similar to the knowledge annotation, the limitations of manual query annotation have came up with unsupervised learning of the query generator. A number of works have proposed a novel rewards for reinforcement learning of a query rewriter <ref type="bibr" target="#b46">(Wu et al., 2022;</ref><ref type="bibr">Chen et al., 2022)</ref>.</p><p>QKConv <ref type="bibr" target="#b4">(Cai et al., 2023)</ref> has proposed an unsupervised query enhanced method for knowledgegrounded conversation. This work is similar to ours in that it consists of a query generator, an offthe-shelf knowledge retriever, and a response generator, and unsupervised joint training is applied to the query generator and the response generator. However, their training objective is based on the approximated marginal likelihood over candidate queries, which is different from our objective that includes the posterior distillation based on the ELBO loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Efficient Latent Variable Model for</head><p>Knowledge-Grounded Dialogue Generation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>In this section, we explain an overall structure of our ELVM, as described in Figure <ref type="figure" target="#fig_1">1a</ref>, and define notations. Unlike previous works of document (passage) retrieval <ref type="bibr" target="#b54">(Zhang et al., 2022;</ref><ref type="bibr" target="#b13">Huang et al., 2021;</ref><ref type="bibr" target="#b22">Lewis et al., 2020)</ref> using a query of an embedded vector of a dialogue context x, we use a text query of natural language, which enables to utilize any kind of retriever for dialogue response generation. We define the probability of generating a natural language query u for a given dialogue context x, p ϕ (u|x), as below:</p><formula xml:id="formula_0">p ϕ (u|x) = t p ϕ (u t |u 0:t-1 , x),<label>(1)</label></formula><p>where u t is the t-th token in u and p ϕ is the probability obtained from the model parameterized by ϕ.</p><p>Similarly, the probability of generating a response y for a given query u and dialogue context x is defined as  where y t is the t-th token in y, Z(u) = {z 1 , z 2 , ..., z k } is the set of retrieved documents (passages) by u, and θ is the model parameters to produce the response probability. Here, we use top-k result obtained by an off-the-shelf retriever such as BM25 <ref type="bibr" target="#b37">(Robertson et al., 2009)</ref> and DPR <ref type="bibr" target="#b16">(Karpukhin et al., 2020)</ref> and freeze the retriever. We assume that there is a deterministic mapping from u to Z. We now set a query u as a latent variable. Then, using the prior p ϕ (u|x) associated with the query generator and the conditional likelihood p θ (y|Z(u), x) corresponding to the response generator, we can define the marginal likelihood of the knowledge-grounded response y given the dialogue context x as below:</p><formula xml:id="formula_1">p θ (y|Z(u), x) = t p θ (y t |y 0:t-1 , Z(u), x),<label>(2)</label></formula><formula xml:id="formula_2">p θ,ϕ (y|x) = u∈Q p ϕ (u|x)p θ (y|Z(u), x), (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where Q is the set of all possible queries. Since it is intractable to marginalize over all queries, we sample a small subset composed of m unique queries, Q s (x) = {u 1 , u 2 , ..., u m } using p ϕ (•|x) and then marginalize over this set as below:</p><formula xml:id="formula_4">pθ,ϕ (y|x) = u∈Q s pϕ (u|x)p θ (y|Z(u), x),<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">pϕ (u|x) = p ϕ (u|x) u ′ ∈Q s p ϕ (u ′ |x) . (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>We construct Q s by obtaining m-unique queries sampled from p ϕ (•|x). It is noted that our response generator takes multiple retrieved documents efficiently using FiD <ref type="bibr" target="#b14">(Izacard and Grave, 2020)</ref>, which is different from the use of a single document for response generation in previous methods. In addition, while we can utilize relative matching scores from the retriever to assign varying likelihoods to each document given a query for p(Z(u)|u), we have noticed that this approach can occasionally hinder the effective and comprehensive learning of the query generator. During the training process, that will be described in the next subsection, our objective is to amplify the impact of the generated query. Consequently, we opt for a deterministic mapping from a query to a set of retrieved documents as a unified whole, aiming to enhance the gradient flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint Training</head><p>Our training objective is the ELBO <ref type="bibr" target="#b19">(Kingma and Welling, 2013;</ref><ref type="bibr" target="#b15">Jordan et al., 1999)</ref> of log pθ,ϕ which can be written as below:</p><formula xml:id="formula_7">log pθ,ϕ (y|x) ≥ E u∼q(u|x,y) [log p θ (y|Z(u), x)] -D KL q(u|x, y)||p ϕ (u|x) ,<label>(6)</label></formula><p>where q(u|x, y) is the variational posterior. Note that the equality holds when q(u|x, y) is identical to the true posterior distribution of it. In this work, we do not parameterize q(u|x, y) and approximate the expected conditional likelihood in the ELBO by sampling u from not q(u|x, y) but pϕ (u|x) such as</p><formula xml:id="formula_8">E u∼q(u|x,y) [log p θ (y|Z(u), x)] ≈ E u∼p ϕ (u|x) [log p θ (y|Z(u), x)]. (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>This approximation is due to that it minimizes the discrepancy between training and test time inference and it does not separately model q(u|x, y) or complicatedly sample from it during training. Then, we obtain the following two losses corresponding to the expected conditional likelihood and the KL regularization such as</p><formula xml:id="formula_10">L y,u (θ, ϕ) = -E u∼p ϕ (u|x) [log p θ (y|Z(u), x)],(8) L u (ϕ) = D KL q(u|x, y)||p ϕ (u|x) .<label>(9)</label></formula><p>Moreover, we define q(u|x, y) in L u (ϕ) by the approximated posterior derived from the prior and the likelihood such as</p><formula xml:id="formula_11">q(u|x, y) = p θ (y|Z(u), x)p ϕ (u|x) u ′ ∈Q s p θ (y|Z(u ′ ), x)p ϕ (u ′ |x)</formula><p>.</p><p>(10) We set this posterior distribution q(u|x, y) as a teacher for a distillation to update the prior associated with the query generation at test time. Namely, L u (ϕ) becomes the posterior distillation loss, and our prior is explicitly updated to be similar to the posterior. Here, the gradient is not propagated from q(u|x, y), and q(u|x, y) can be computed easily without additional model or approximation since we define it over the subset Q s .</p><p>To sum up, we obtain the conditional likelihood loss and the posterior distillation loss by multiple samples from the prior distribution that is aligned with query sampling at the inference time. We would mitigate the bias of prior sampling by multiple query samples for each input during training. In addition, our modification of ELBO is different from the expectation-maximization (EM) algorithm in that we sample a latent variable from the prior distribution rather than the posterior distribution that is used for sampling of E-step in EM. Moreover, in contrast to the M-step loss, we use the prior-weighted conditional likelihood and the gradient is also propagated to the sampled latents and the corresponding prior model. For experiments in this paper, the query generator ϕ and the response generator θ share parameters, with different input prompts to distinguish each module, for simplicity.</p><p>Overall, the loss for our joint training is</p><formula xml:id="formula_12">L(θ, ϕ) = L y,u (θ, ϕ) + βL u (ϕ), (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>where β is the relative weight for the posterior distillation loss. Training. We begin with publicly available R2C2 pre-trained transformer model <ref type="bibr">(Shuster et al., 2022a)</ref> with 400M parameter size and fine-tune it on multiple datasets to acquire the essential skill sets for knowledge-grounded dialogue such as query generation and knowledge-grounded response generation. Detailed tasks and statistics are shown in Table <ref type="table" target="#tab_1">1</ref>. In line with <ref type="bibr">Shuster et al. (2022b)</ref>, we adopt a modular system with a single model that incorporates control tokens into the dialogue context to perform different modules. For example, appending the control token __generate-query__ to the dialogue context promotes the model to generate relevant search query. We name the initial model from this pretraining procedure as R2C2-PT. Subsequently, we apply our proposed algorithm to R2C2-PT model, <ref type="bibr" target="#b25">(Lian et al., 2019)</ref> 79.1 --13.0 1.0 193.8 --13.1 1.0 ITDD <ref type="bibr" target="#b24">(Li et al., 2019)</ref> 17 Seen Unseen Model R@1 R@5 R@10 R@1 R@5 R@10 R2C2-PT 25 resulting in ELVM. Furthermore, to dispel concerns that our training gains might predominantly stem from R2C2-PT's comprehensive fine-tuning across multiple datasets rather than ELVM's inherent efficacy, we opted to utilize the base R2C2 model. Upon integrating it with ELVM, the resulting configuration is denoted as ELVM-from-R2C2.</p><formula xml:id="formula_14">Seen Unseen Model PPL ↓ B3 ↑ B4 ↑ R1 ↑ R2 ↑ PPL ↓ B3 ↑ B4 ↑ R1 ↑ R2 ↑ PostKS</formula><p>During the training of ELVM, we employ a sampling-based generation method, specifically nucleus sampling <ref type="bibr" target="#b12">(Holtzman et al., 2020)</ref>, to promote diverse query generation in p ϕ . When retrieving documents based on the generated queries, we utilize an off-the-shelf sparse retriever such as BM25 <ref type="bibr" target="#b37">(Robertson et al., 2009)</ref>, prioritizing low latency over the dense retriever (DR). We train for 3 epochs with m = 4 and k = 5 for WoW and 1 epoch with m = 8 and k = 10 for QReCC. β is set to 1 for both tasks, unless stated otherwise. To ensure a thorough evaluation of ELVM's performance and its ability to generalize to unseen dialogues, we purposely avoid dataset overlap between training of R2C2-PT and ELVM. More detailed information on the training process can be found in Appendix A. <ref type="bibr" target="#b34">(Raposo et al., 2022)</ref> 18.9 1.0 ---DPR-IHN <ref type="bibr" target="#b18">(Kim and Kim, 2022)</ref> 30.4 4.7 ---QKConv <ref type="bibr" target="#b4">(Cai et al., 2023)</ref> 33.5 5.9 Inference. During inference, unlike the training phase, only a single query is generated by the query generator p ϕ . The default number of retrieved documents k from the off-the-shelf retriever is set to 5 for WoW and 10 for QReCC. The knowledgegrounded response is generated by the response generator, p θ . An illustration of the inference is depicted in Figure <ref type="figure" target="#fig_1">1b</ref>. During the inference, both query and response generation are conducted using beam search with a beam size of 5.</p><formula xml:id="formula_15">Model F1 ↑ EM ↑ R1 ↑ R2 ↑ PPL ↓ Q.Rewriting</formula><p>Variants of ELVM. frozen. During inference, given a dialogue context the frozen query generator p ϕ generates query and use the off-the-shelf retriever to bring relevant documents. Finally the response generator of ELVM-OK generates the final response. In Section 3.2, we describe the difference between ELVM training and EM algorithm. In order to quantitatively compare these two methods, we perform EM-like training in ELVM where we apply the posterior-weighted conditional likelihood in L y,u (θ, ϕ): L y,u (θ, ϕ) = u SG(q(u|x, y)) log p θ (y|Z(u), x). Here, the query latents u are still sampled from the prior distribution, and SG means the stop-gradient that prevents the gradient propagation to the query generator. We name this variant as ELVM-EM-Like.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Automatic Evaluation</head><p>The evaluation results of WoW test seen and unseen tasks are shown in Table <ref type="table" target="#tab_2">2</ref>. ELVM surpasses all its variants and previous state-of-the-art (SOTA) model, achieving a new SOTA on both WoW test seen unseen tasks, across all metrics. In addition, ELVM-OK demonstrates notable improvements in BLEU and Rouge scores compared to previous models with a slight drawback in PPL. Similarly, ELVM-EM-Like also exhibits competitive performance across multiple metrics. Moreover, when comparing ELVM-from-R2C2 and ELVM, the experiment demonstrates that while R2C2-PT offers marginal benefits, the marked performance increase is predominantly attributable to ELVM. We further examine the performance of query generator by assessing the recall of the ground-truth documents. The results summarized in Table <ref type="table" target="#tab_3">3</ref> reveal that training with our proposed algorithm leads to an increase in recall for both ELVM-β=0 and ELVM compared to R2C2-PT while the latter yields a larger performance gain. This highlights the effectiveness of distilling the posterior distribution of the response generator into the prior distribution associated with the query generator.</p><formula xml:id="formula_16">WoW Unseen QReCC Model B4 ↑ R2 ↑ R@5 ↑ F1 ↑ EM ↑ R@</formula><p>For QReCC, as shown in Table <ref type="table" target="#tab_4">4</ref>, ELVM surpasses the previous SOTA, QKConv <ref type="bibr" target="#b4">(Cai et al., 2023)</ref>, the recent previous works <ref type="bibr" target="#b34">(Raposo et al., 2022;</ref><ref type="bibr" target="#b18">Kim and Kim, 2022)</ref>, and the variants of ELVM including R2C2-PT and ELVM-OK in terms of all metrics. The recall performance on QReCC is included in Table <ref type="table">C</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Human Evaluation</head><p>To gauge performance across multiple aspects of the quality of generated responses, human evaluation is performed on the generated responses of ELVM and KnowledGPT <ref type="bibr" target="#b55">(Zhao et al., 2020)</ref> <ref type="foot" target="#foot_0">foot_0</ref> . Similar to <ref type="bibr" target="#b35">(Rashkin et al., 2021)</ref> and <ref type="bibr" target="#b50">(Xu et al., 2023)</ref>, we assess the response quality in two aspects: Fluency and Relevance. Fluency measures whether the response is understandable, self-consistent without repetition, and proficient. Relevance assesses the extent to which the response aligns with the dialogue context, incorporates pertinent knowledge, and maintains appropriateness. In total, 50 data samples are randomly selected from WoW tasks where 25 samples are randomly selected from each seen and unseen task. The qualities of the responses are measured by A/B testing on the two aspects.</p><p>The feedback is collected from 11 human experts. Further details and annotator instructions can be found in Table <ref type="table">G</ref>. As shown in Table <ref type="table">5</ref>, ELVM significantly outperforms KnowledGPT in all aspects especially on the unseen task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>Number of Queries. The size of sampled query set Q s , m, plays a huge role in training of ELVM, as it directly influences the bias of the marginal likelihood of the knowledge-grounded response y. To investigate the impact of m, we gradually increase m from 1 to 8 and measure the performance on WoW while keeping other hyperparameters fixed.</p><p>As shown in Table <ref type="table" target="#tab_5">6</ref>, we observe a positive correlation where increasing m leads to improved performance. Notably, setting m = 4 yields the optimal performance for both seen and unseen tasks.</p><p>Posterior Distillation Weight. We examine the influence of the posterior distillation loss on ELVM training by adjusting the value of β. The experimental results presented in Table <ref type="table" target="#tab_6">7</ref> reveal that the optimal performance is attained when β = 1 for both the WoW unseen and QReCC tasks, indicating the effectiveness of incorporating the posterior distillation loss. However, a substantial decline in performance is observed when β is increased to 5 or set to 0, particularly in the case of QReCC.</p><p>Varying the Retriever. We extend our experiments by incorporating the DR in two distinct ways. For WoW, we train and evaluate using all-MiniLM-L6-v2<ref type="foot" target="#foot_1">foot_1</ref> DR. In contrast, for QReCC, due to its extensive knowledge pool, we leverage DR solely during evaluation by employing both the BM25 retriever and all-mpnet-base-v2<ref type="foot" target="#foot_2">foot_2</ref> DR in a hybrid manner to retrieve the top-k documents.</p><p>Concretely, we score each document by summing up the normalized score of each retriever and rerank to obtain top-k documents. Table <ref type="table" target="#tab_7">8</ref> presents a summary of the performance results of employing DR, demonstrating a successful integration of DR in different settings of ELVM.</p><p>Scaling Up Model. We investigate the impact of increasing the model size on performance. Starting with the publicly available 2.7B R2C2 pretrained transformer, we follow exactly the same procedure outlined in Section 3.2 to obtain R2C2-PT Large , ELVM-OK Large and ELVM Large . Detailed hyperparameters for training 2.7B model is given in Appendix A. The results presented in Table <ref type="table" target="#tab_8">9</ref> demonstrate that as the model size increases, there is an improvement in performance across all metrics, supporting the scalability of ELVM.</p><p>BART as Base. To ensure a fair comparison and further demonstrate the robustness of our algorithm on diverse model scales, we conduct an experiment employing BART <ref type="bibr" target="#b21">(Lewis et al., 2019)</ref> base as an initial model and then apply our proposed algorithm on WoW dataset. We choose BART base model since the previous SOTA model <ref type="bibr" target="#b50">(Xu et al., 2023)</ref> utilized BART base as their base model. The outcome of this training is termed ELVM-from-BART.</p><p>As shown in Table <ref type="table" target="#tab_9">10</ref>, the initial BART model shows very poor performances. However, upon training the BART-base with our ELVM algorithm, there is a great improvement in performance across both WoW Seen and Unseen tasks. Importantly, the results surpass the performance benchmarks set by the previous SOTA model (SPI).</p><p>Retriever Switching To assess the robustness of our algorithm to the retriever mismatch between training and inference phases, we undertake an additional experiment, of which the results are depicted in Table <ref type="table" target="#tab_10">11</ref>. In this experiment, we train the WoW task utilizing either BM25 or dense retriever (utilizing all-MiniLM-L6-v2 retriever, as detailed Section 4), and subsequently evaluate using either BM25 or dense retriever. The results on the WoW unseen test indicate that utilizing a different retriever during testing doesn't necessitate model re-training, particularly given the superior performance of the dense retriever compared to BM25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Analysis</head><p>For qualitative analysis, we examine the generated samples from KnowledGPT, R2C2-PT and other variants of ELVM, whose samples are presented in Table <ref type="table" target="#tab_11">12</ref>. The example reveals that the queries generated by KnowledGPT, R2C2-PT and ELVM-β=0 fail to retrieve the relevant knowledge effectively while the queries generated by ELVM with BM25 and DR (all-MiniLM-L6-v2) reflects the ability to access the relevant knowledge, leading to improved responses. The results also demonstrate that the query generator trained with our algorithm effectively adapts to the chosen retriever.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose ELVM as an efficient latent variable modeling for knowledge-intensive dialogue generation. In ELVM, the knowledge retrieval is realized by the query generation followed by document retrieval using the off-the-shelf retriever for ease of joint training of both the retriever and the response generator, without the knowledge supervision. Furthermore, the training scheme of ELVM modifies the ELBO for the marginal likelihood to effectively perform the joint training without the use of complex posterior sampling, which also eliminates the discrepancy between the training-time and test-time knowledge samplings.</p><p>ELVM empirically demonstrates that it significant outperforms over previous latent variable methods with in-depth analysis on diverse dialogue datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>There are a number of limitations and possible directions to future works of ELVM proposed in this paper. First, as more large language models beyond the scale of hundreds of billion parameters are emerging, there are rooms for emergent behaviors in ELVM as the sizes of data and model grow that has not been tested in this paper. Second, instead of the prior sampling that we use, fast posterior sampling such as short-run MCMC <ref type="bibr" target="#b9">(Erik et al., 2019;</ref><ref type="bibr" target="#b50">Xu et al., 2023)</ref> is also an alternative candidate. Third, the experiments in this paper retrieve documents from confined set (e.g., Wikipedia) using light retriever such as BM25 and dense retriever. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>This work presents no direct ethical issues. However, there is a possibility that the generated responses may inadvertently contain inadequate, biased, or discriminatory content. We do not introduce new datasets, and all experiments are conducted using publicly available datasets. Model F1 ↑ EM ↑ Q.Rewriting <ref type="bibr" target="#b34">(Raposo et al., 2022)</ref> 18.9 1.0 DPR-IHN <ref type="bibr" target="#b18">(Kim and Kim, 2022)</ref> 30.4 4.7 QKConv <ref type="bibr" target="#b4">(Cai et al., 2023)</ref> 33.5 5.9 ELVM 36.5 6.2 ELVM-w-GT-Doc 55.4 16.2 involves training our response generator under the assumption that the model has access to the groundtruth document during both training and evaluation.</p><p>The performance metrics of this specific setup are designated as "ELVM-w-GT-Doc" in Table <ref type="table" target="#tab_13">15</ref> and Table <ref type="table" target="#tab_14">16</ref>. From both tables, it's evident that there still remains a margin for improvement for both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Document Recall Performance</head><p>Query from R@1 R@5 R@10 We report the document recall performance on QReCC, including different types of queries, as show in Table <ref type="table" target="#tab_15">17</ref>. Our proposed model, ELVM, surpasses both its variants and non-model generated queries such as using dialogue context as a query. The document recall performance on QReCC, encompassing various query types, is presented in Table <ref type="table" target="#tab_15">17</ref>. Our proposed ELVM model outperforms its variants as well as non-model generated queries, including the use of dialogue context as a query. Moreover, incorporating the dense retriever for reranking documents (ELVM DR ) leads to further improvements in recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seen</head><p>Unseen Model B4 ↑ R2 ↑ R@5 ↑ R@10 ↑ B4 ↑ R2 ↑ R@5 ↑ R@10 ↑ k=5 11.9 13.9 68.4 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Knowledge Pool Scaling</head><p>In order to create a more realistic and challenging retrieval scenario, we expand the original setting of the WoW dataset by increasing the number of relevant documents per instance to 1000. This modification better reflects real-world information retrieval scenarios where a vast array of both relevant and irrelevant documents are typically encountered.</p><p>We construct this extended dataset by randomly augmenting 1000 -K documents from the WoW dataset to the document pool for each instance, where K represents the number of annotated relevant documents for each instance. On average, K ≈ 20 for each instance in WoW dataset.</p><p>Results in Table <ref type="table" target="#tab_16">18</ref> show performances of ELVM models with different number of retrieved documents, k, and knowledge pool size, where models with increased knowledge pool has tailing keyword Bulk at then end of their names. Despite the increase in complexity of the retrieval task, ELVM demonstrates robust performance, with only a marginal drop in metrics compared to the original setting. Furthermore, it is important to highlight that even under these more difficult conditions with 1000 relevant documents, ELVM still outperforms SPI by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Effect of Parameter Sharing</head><p>In our ELVM training, parameters are shared between the query and response generators. To probe this design choice, we decoupled these parameters, resulting in the ELVM-Decouple configuration, as detailed in Decouple showed a slight edge over the standard ELVM. Nevertheless, the benefits of shared parameters-particularly regarding computational efficiency and knowledge transfer during training and inference-cannot be understated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Metrics</head><p>We report the evaluation of our models on the WoW task using both F1 and knowledge F1 (KF1) metrics in Table <ref type="table" target="#tab_19">20</ref>. For reference, values pertaining to previous models are derived <ref type="bibr">from Sun et al. (2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Human Evaluation</head><p>For a comprehensive evaluation, we conduct the human evaluation between ELVM and KnowledGPT on seen and unseen tasks in WoW <ref type="bibr" target="#b8">(Dinan et al., 2018)</ref>. We randomly select 50 samples (25 samples per task), and each sample is evaluated by 11 different human experts. In specific, the two generated responses from each model in the same context are assigned to the annotators. For A/B testing, we give one score to the model if it's response is received an equally good or better than the other one. Figure <ref type="figure">2</ref> shows the annotator instructions for the two aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Generated Samples</head><p>Generated samples for the WoW test unseen are presented in Table <ref type="table" target="#tab_2">21</ref> and<ref type="table" target="#tab_2">Table 22</ref>. Similarly, Table 23 and Table <ref type="table" target="#tab_4">24</ref> showcase the generated samples for the QReCC test.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall structure and example inference case of ELVM. During training, for a given dialogue context x, the query generator generates a set of unique queries Q s and for each query u a set of top-k documents Z(u) is retrieved using the off-the-shelf retriever. Then, response generator utilizes both x and Z(u): (a) The approximated marginal likelihood pθ,ϕ (y|x) used during training is defined by Q s and the re-normalized prior pϕ (u|x); (b) At inference time, a single sampled query u is used for retrieving multiple documents and producing a final response.</figDesc><graphic coords="4,74.01,79.50,326.58,191.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>List of dataset and number of train and validation examples used for fine-tuning R2C2 model to obtain R2C2-PT. KG is an abbreviation for knowledgegrounded.</figDesc><table><row><cell cols="2">4 Experiment</cell><cell></cell><cell></cell></row><row><cell cols="2">4.1 Experimental Setup</cell><cell></cell><cell></cell></row><row><cell cols="4">Dataset. Our experiments are conducted on two widely used knowledge-grounded dialogue</cell></row><row><cell cols="4">datasets, Wizard of Wikipedia (WoW) (Dinan et al.,</cell></row><row><cell cols="4">2018) and QReCC (Anantha et al., 2021). WoW</cell></row><row><cell cols="4">is structured around dialogues that are grounded in</cell></row><row><cell cols="4">knowledge sentences sourced from Wikipedia, con-</cell></row><row><cell cols="4">sisting of roughly 18K, 2K, and 2K dialogues, for</cell></row><row><cell cols="4">training, validation, and test subsets, respectively.</cell></row><row><cell cols="4">QReCC is a comprehensive compilation of conver-</cell></row><row><cell cols="4">sational data, consisting of 14K open-domain con-</cell></row><row><cell cols="4">versations comprising 80K question-answer pairs.</cell></row><row><cell>Task</cell><cell>Dataset</cell><cell>Train</cell><cell>Valid</cell></row><row><cell>Query</cell><cell>WizInt (Komeili et al., 2022) Fits (xu2)</cell><cell cols="2">351,375 2,467 3,587 392</cell></row><row><cell></cell><cell>WizInt (Komeili et al., 2022)</cell><cell cols="2">22,488 1,687</cell></row><row><cell>KG Response</cell><cell>Fits (xu2) Ms Marco (Nguyen et al., 2017)</cell><cell cols="2">6,279 281,636 36,859 656</cell></row><row><cell></cell><cell>NQ Open (Adolphs et al., 2021b)</cell><cell cols="2">79,168 8.757</cell></row><row><cell></cell><cell>WizInt (Komeili et al., 2022)</cell><cell>8,335</cell><cell>587</cell></row><row><cell>Dialogue Response</cell><cell>MSC (Xu et al., 2022a) SaferDialogues (Ung et al., 2022)</cell><cell cols="2">105,549 17,691 6,306 7,88</cell></row><row><cell></cell><cell>PersonaChat (Zhang et al., 2018)</cell><cell cols="2">131,438 7.801</cell></row><row><cell></cell><cell cols="3">EmpatheicDialogues (Rashkin et al., 2019) 64,636 5,738</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison in WoW. PPL represents perplexity, B3 and B4 denote BLEU-3 and BLEU-4 scores, respectively, and R1 and R2 indicate Rouge-1 and Rouge-2 scores. Our proposed model, ELVM, achieves new SOTA performance for both seen and unseen tasks with a substantial margin.</figDesc><table><row><cell>SKT (Kim et al., 2020) PIPM (Chen et al., 2020) ZRKGC (Li et al., 2021) CoLV (Zhan et al., 2021) KnowledGPT (Zhao et al., 2020) SPI (Xu et al., 2023) R2C2 R2C2-PT ELVM-OK ELVM-EM-Like ELVM-from-R2C2 ELVM</cell><cell>.8 52.0 42.7 40.4 39.6 19.2 17.1 40.7 24.7 19.8 16.2 15.8 14.6</cell><cell>4.0 ----9.5 10.2 1.7 6.0 12.0 11.6 14.5 14.7</cell><cell>2.5 -3.3 -2.9 7.2 7.7 0.9 4.4 9.5 9.1 11.8 11.9</cell><cell>16.2 19.3 19.9 -20.6 22.0 22.7 12.3 17.3 25.3 25.7 28.8 29.3</cell><cell>-6.8 7.3 -7.9 7.9 8.8 1.9 5.2 10.7 10.7 13.3 13.9</cell><cell>44.8 81.4 65.7 41.5 54.3 22.3 19.1 46.6 30.4 25.8 19.4 18.9 18.0</cell><cell>2.1 ----8.3 9.6 1.6 6.2 12.1 12.0 14.3 14.5</cell><cell>1.1 -2.5 -2.1 6.0 7.3 0.9 4.6 9.7 9.5 11.5 11.8</cell><cell>11.4 16.1 17.6 -19.7 20.5 22.0 11.9 17.7 25.2 25.7 28.8 29.2</cell><cell>-4.2 5.4 -6.3 6.7 8.5 5.4 5.4 10.5 10.8 13.3 13.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of document recall in WoW between ELVM variants. Note that we omit the performance of ELVM-OK whose performance is identical to R2C2-PT since it is trained with keeping its query generator frozen.</figDesc><table><row><cell></cell><cell>.7</cell><cell>63.8</cell><cell>74.3</cell><cell>29.2</cell><cell>64.8</cell><cell>76.7</cell></row><row><cell>ELVM-EM-Like</cell><cell>26.4</cell><cell>63.9</cell><cell>74.7</cell><cell>28.9</cell><cell>64.5</cell><cell>75.8</cell></row><row><cell>ELVM-β=0</cell><cell>26.1</cell><cell>64.6</cell><cell>75.7</cell><cell>28.9</cell><cell>65.3</cell><cell>76.8</cell></row><row><cell>ELVM</cell><cell>29.1</cell><cell>68.4</cell><cell>80.3</cell><cell>30.4</cell><cell>68.3</cell><cell>80.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note><p><p><p>Performance comparison in QReCC with QK-Conv</p><ref type="bibr" target="#b4">(Cai et al., 2023)</ref> </p>which is the previous SOTA on this task. We report F1 scores, exact match (EM), Rouge score (R) and perplexity (PPL). * : p &lt; 0.01, * : p &lt; 0.025).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Effect of varying the number of queries, m, sampled during training on WoW. We see the highest performance on unseen task when m = 4. The superscripted value by * is the default setting for ELVM.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Seen</cell><cell cols="2">Unseen</cell><cell></cell></row><row><cell></cell><cell cols="5">m B4 ↑ R2 ↑ B4 ↑ R2 ↑ 1 10.8 12.4 10.5 12.2</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>11.1</cell><cell>13.0</cell><cell>11.1</cell><cell>12.8</cell><cell></cell></row><row><cell></cell><cell>4  *  8</cell><cell>11.9 11.8</cell><cell>13.9 13.7</cell><cell>11.8 11.7</cell><cell>13.7 13.6</cell><cell></cell></row><row><cell></cell><cell cols="3">Wow Unseen</cell><cell></cell><cell>QReCC</cell><cell></cell></row><row><cell>β 0</cell><cell cols="6">B4 ↑ R2 ↑ R@5 ↑ F1 ↑ EM ↑ R@5 ↑ 9.3 10.6 65.3 34.5 5.1 55.8</cell></row><row><cell>1  *  5</cell><cell>11.8 10.8</cell><cell>13.7 12.4</cell><cell>68.3 66.7</cell><cell>36.5 31.3</cell><cell>6.9 3.5</cell><cell>65.6 31.9</cell></row></table><note><p>To examine the impact of the proposed ELVM training, we explore another variant, ELVM-OK, which is trained on the response generation task with annotated oracle knowledge documents while keeping the query generator p ϕ</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Ablation study of the posterior distillation weight β on WoW. From the results, we can see that the trivial value of β=1 shows the best performance. The super-scripted value by * is the default setting for ELVM.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Performance results achieved by employing the dense retriever instead of the default BM25 retriever for WoW unseen and QReCC evaluation.</figDesc><table><row><cell>ELVM</cell><cell>11.8</cell><cell cols="2">13.7</cell><cell>68.3</cell><cell>36.5</cell><cell>6.2</cell><cell>5 ↑ 59.9</cell></row><row><cell>ELVMDR</cell><cell>12.8</cell><cell cols="2">15.5</cell><cell>81.2</cell><cell>37.9</cell><cell>6.9</cell><cell>65.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Seen</cell><cell></cell><cell></cell><cell>Unseen</cell></row><row><cell>Model R2C2-PT</cell><cell cols="7">B4 ↑ R2 ↑ R@5 ↑ B4 ↑ R2 ↑ R@5 ↑ 4.4 5.2 63.8 4.6 5.4 64.8</cell></row><row><cell>ELVM-OK</cell><cell></cell><cell>9.5</cell><cell>10.7</cell><cell>-</cell><cell>9.7</cell><cell>10.5</cell><cell>-</cell></row><row><cell>ELVM</cell><cell cols="2">11.9</cell><cell>13.9</cell><cell>68.4</cell><cell>11.8</cell><cell>13.7</cell><cell>68.3</cell></row><row><cell>R2C2-PTLarge</cell><cell></cell><cell>8.7</cell><cell>10.2</cell><cell>62.9</cell><cell>8.9</cell><cell>10.3</cell><cell>64.4</cell></row><row><cell cols="2">ELVM-OKLarge</cell><cell>9.6</cell><cell>10.6</cell><cell>-</cell><cell>10.2</cell><cell>11.2</cell><cell>-</cell></row><row><cell>ELVMLarge</cell><cell cols="2">12.6</cell><cell>14.3</cell><cell>72.6</cell><cell>12.4</cell><cell>14.1</cell><cell>74.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Effect of model scaling tested on WoW. ELVM scales out to larger parameter size with increase in performance. The subscript Large indicates the model size of 2.7B parameters.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Performance comparison with BART as our initial model.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Seen</cell><cell></cell><cell></cell><cell>Unseen</cell></row><row><cell>CoLV</cell><cell>Model</cell><cell></cell><cell cols="5">PPL ↓ B4 ↑ R2 ↑ PPL ↓ B4 ↑ R2 ↑ 39.6 2.9 7.9 54.3 2.1 6.3</cell></row><row><cell cols="2">KnowledGPT</cell><cell></cell><cell>19.2</cell><cell>7.2</cell><cell>7.9</cell><cell>22.3</cell><cell>6.0</cell><cell>6.7</cell></row><row><cell>SPI</cell><cell></cell><cell></cell><cell>17.1</cell><cell>7.7</cell><cell>8.8</cell><cell>19.1</cell><cell>7.3</cell><cell>8.5</cell></row><row><cell>BART</cell><cell></cell><cell></cell><cell>974</cell><cell>1.9</cell><cell>2.6</cell><cell>973</cell><cell>1.3</cell><cell>2.6</cell></row><row><cell cols="3">ELVM-from-BART</cell><cell>15.3</cell><cell>8.4</cell><cell>11.1</cell><cell>16.4</cell><cell>8.2</cell><cell>10.9</cell></row><row><cell cols="2">ELVM</cell><cell></cell><cell>14.6</cell><cell>11.9</cell><cell>13.9</cell><cell>18.0</cell><cell>11.8</cell><cell>13.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">WoW Unseen</cell></row><row><cell cols="2">Model CoLV</cell><cell cols="6">Train Ret Test Ret PPL ↓ B4 ↑ R2 ↑ F1 ↑ --54.3 2.1 6.3 18.5</cell></row><row><cell cols="2">KnowledGPT</cell><cell>-</cell><cell></cell><cell>-</cell><cell>22.3</cell><cell>6.0</cell><cell>6.7</cell><cell>20.5</cell></row><row><cell>SPI</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>19.1</cell><cell>7.3</cell><cell>8.5</cell><cell>-</cell></row><row><cell>ELVM</cell><cell></cell><cell cols="2">BM25</cell><cell>BM25</cell><cell>18.0</cell><cell>11.8</cell><cell>13.7 27.0</cell></row><row><cell>ELVM</cell><cell></cell><cell cols="2">BM25</cell><cell>Dense</cell><cell>14.8</cell><cell>12.6</cell><cell>15.2 29.3</cell></row><row><cell>ELVM</cell><cell></cell><cell cols="2">Dense</cell><cell>Dense</cell><cell>12.9</cell><cell>12.8</cell><cell>15.5 29.0</cell></row><row><cell>ELVM</cell><cell></cell><cell cols="2">Dense</cell><cell>BM25</cell><cell>21.6</cell><cell>11.8</cell><cell>13.1 28.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Impact of alternating the retriever between training and testing phases on WoW unseen test. Dense indicates utilizing all-MiniLM-L6-v2 retriever as described in Section 4 and Ret is an abbreviation for retriever.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Generated samples from WoW unseen task. The first row represents the dialogue context, ground truth (GT) knowledge (highlighted in olive), and GT response. Following rows contain the model generated query, response and whether the GT knowledge is included in top-5 retrieved documents. More samples can be found in Appendix H. We truncate the GT knowledge and display only relevant spans of the GT knowledge to the ground-truth response.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 15 :</head><label>15</label><figDesc>Performance comparison in WoW in the presence of ground-truth document during evaluation (ELVM-w-GT-Doc).</figDesc><table><row><cell>Model ELVM ELVM-w-GT-Doc</cell><cell>Seen PPL ↓ B3 ↑ B4 ↑ R1 ↑ R2 ↑ PPL ↓ B3 ↑ B4 ↑ R1 ↑ R2 ↑ Unseen 14.6 14.7 11.9 29.3 13.9 18.0 14.5 11.8 29.2 13.7 9.7 22.1 18.5 40.9 23.9 10.5 21.5 17.9 40.7 23.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 16 :</head><label>16</label><figDesc>Performance comparison in QReCC in the presence of ground-truth document during evaluation (ELVM-w-GT-Doc).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 17 :</head><label>17</label><figDesc>Query performance comparison in QReCC for different types of queries.</figDesc><table><row><cell>Dialogue History</cell><cell>33.4</cell><cell>58.6</cell><cell>68.7</cell></row><row><cell>Last Utterance</cell><cell>25.5</cell><cell>50.8</cell><cell>62.6</cell></row><row><cell>R2C2-PT</cell><cell>17.4</cell><cell>33.0</cell><cell>39.5</cell></row><row><cell>ELVM-β=0</cell><cell>30.6</cell><cell>55.8</cell><cell>65.7</cell></row><row><cell>ELVM</cell><cell>35.2</cell><cell>59.9</cell><cell>68.6</cell></row><row><cell>ELVMDR</cell><cell>37.5</cell><cell>65.6</cell><cell>75.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 18 :</head><label>18</label><figDesc>Effect of increasing the document pool size to 1000 on the performance of ELVM. As before, k indicates the number of retrieved documents used during train and inference.</figDesc><table><row><cell></cell><cell></cell><cell>80.3</cell><cell cols="2">11.8 13.7 68.3</cell><cell>80.3</cell></row><row><cell>k=5 Bulk</cell><cell>9.4 10.9 62.7</cell><cell>73.9</cell><cell>8.2 9.5</cell><cell>47.2</cell><cell>61.9</cell></row><row><cell>k=10</cell><cell>10.3 12.2 64.8</cell><cell>76.4</cell><cell cols="2">10.6 12.7 64.8</cell><cell>76.9</cell></row><row><cell cols="2">k=10 Bulk 10.2 12.2 62.1</cell><cell>74.1</cell><cell cols="2">9.0 10.5 47.2</cell><cell>61.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 19 .</head><label>19</label><figDesc>Upon evaluation on WoW's test seen and unseen tasks, ELVM-</figDesc><table><row><cell></cell><cell></cell><cell>Seen</cell><cell></cell><cell></cell><cell>Unseen</cell><cell></cell></row><row><cell>Model ELVM</cell><cell cols="6">PPL ↓ B4 ↑ R2 ↑ PPL ↓ B4 ↑ R2 ↑ 14.6 11.9 13.9 18.0 11.8 13.7</cell></row><row><cell>ELVM-Decouple</cell><cell>16.2</cell><cell>12.6</cell><cell>14.5</cell><cell>21.6</cell><cell>12.2</cell><cell>14.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 19 :</head><label>19</label><figDesc>Performance comparison on the WoW dataset using the ELVM algorithm. ELVM denotes training with shared parameters between query and response generators, while ELVM-Decouple denotes a non-shared parameter approach.</figDesc><table><row><cell></cell><cell></cell><cell>Seen</cell><cell cols="2">Unseen</cell></row><row><cell>Model CoLV</cell><cell cols="4">F1 ↑ KF1 ↑ F1 ↑ KF1 ↑ 20.3 18.2 18.5 17.5</cell></row><row><cell>KnowledGPT</cell><cell>22.0</cell><cell>23.8</cell><cell>20.5</cell><cell>22.1</cell></row><row><cell>SPI</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>R2C2-PT</cell><cell>15.4</cell><cell>20.5</cell><cell>15.7</cell><cell>22.3</cell></row><row><cell>ELVM-OK</cell><cell>23.0</cell><cell>33.0</cell><cell>22.9</cell><cell>33.0</cell></row><row><cell cols="2">ELVM-EM-Like 24.5</cell><cell>33.2</cell><cell>23.5</cell><cell>33.5</cell></row><row><cell>ELVM</cell><cell>27.2</cell><cell>40.8</cell><cell>27.0</cell><cell>40.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 20 :</head><label>20</label><figDesc>F1 and KF1 scores for ELVM and baseline models.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We choose KnowledGPT because it is the best performing model among publicly available models.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://huggingface.co/sentence-transformers/all-mpnetbase-v2</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by <rs type="funder">Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP)</rs> grant funded by the <rs type="funder">Korea government (MSIT)</rs>(No. <rs type="grantNumber">2019-0-00079</rs>, <rs type="programName">Artificial Intelligence Graduate School Program</rs>, <rs type="funder">Korea University</rs>). In addition, this work was also supported by <rs type="funder">Artificial</rs> intelligence industrial convergence cluster development project funded by the <rs type="funder">Ministry of Science and ICT(MSIT, Korea) Gwangju Metropolitan City</rs>. Finally, we would also like to thank <rs type="institution">Brain Cloud Team at Kakao Brain</rs> for their support.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_V5YVKWK">
					<idno type="grant-number">2019-0-00079</idno>
					<orgName type="program" subtype="full">Artificial Intelligence Graduate School Program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dialogue Context</head><p>I want to teach my family about investing by buying them a few shares of stock, but I really don't know much about it. The stock market can also be called the equity market or share market. That is where you buy and sell stocks. The stock represent ownership claims on businesses. Oh, so I could tell my kids they own part of a company. That's cool. Maybe I should get them a share in a company they know then, like Apple or XBox or Snapchat? GT knowledge <ref type="bibr">[...]</ref> Examples of the latter include shares of private companies which are sold to investors through equity crowdfunding platforms. [...]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT Response</head><p>Yes, it's like the shares of a company are sold to investors through equity crowdfunding platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KnowledGPT</head><p>Query: n/a GT knowledge selected: False Response: yes, you can get shares in a company like apple or xbox. you can also get shares in company like stock market.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R2C2-PT</head><p>Query: How to teach kids about investing GT knowledge selected: False Response: The stock market can also be called the equity market or share market. That is where you buy and sell stocks. The stock represent ownership claims on businesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ELVM-β = 0</head><p>Query: Investing tips for beginners GT knowledge selected: False Response: That's a good idea. The stock market is a place where you can buy and sell stocks.  Table <ref type="table">13</ref> we provide hyperparameters utilized during this training process. We also report hyperparameters used for training ELVM and its variants model (ELVM-OK and ELVM-EM-Like) in Table 14. In addition, during training of QReCC, we adopt a strategy to decrease the knowledge pool size from 54M to 1M. This reduction aims to mitigate latency issues during document retrieval. Concretely, the 1M pool is constructed by initially gathering the ground-truth relevant passage for each instance in the training dataset then adding randomly sampled documents. During the evaluation stage, we utilize the original full-size knowledge pool of 54M passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ELVM w/ BM25</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Upper Bound Performance</head><p>We believe in the value of understanding the utmost capability in an ideal scenario. To this end, we pursue an upper-bound performance analysis for both the WoW and QReCC tasks. This approach  ELVM-β = 0 Query: red meat protein GT knowledge selected: True Response: In nutritional science, red meat is defined as any meat that has more myoglobin than white meat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ELVM w/ BM25</head><p>Query: red meat protein GT knowledge selected: True Response: In nutritional science, red meat is defined as any meat that has more myoglobin than white meat.</p><p>ELVM w/ DR Query: red meat protein GT knowledge selected: True Response: Red meat is defined as any meat that has more myoglobin than white meat.</p><p>Table <ref type="table">22</ref>: Generated samples from WoW test unseen. We truncate the ground-truth knowledge and display only relevant spans of the ground-truth knowledge to the ground-truth response. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT Response</head><p>The funds were also spent on the training and dispatching of Ahmadiyya missionaries outside the Indian sub-continent and their sustenance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R2C2-PT</head><p>Query: Mirza Basher-ud Din Mahmood Ahmad missionaries GT knowledge selected: False Response: Mirza Basheer-ud-Din Mahmood Ahmad used missionaries to try to spread the New Scheme to foreign countries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ELVM-β = 0</head><p>Query: New Scheme Mahmud Ahmad missionaries to spread New Scheme in foreign countries GT knowledge selected: True Response: Yes, I have. I have experienced heartache, also known as loss of a loved one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ELVM w/ BM25</head><p>Query: New scheme missionaries used to spread Ahmadiyyah teachings in foreign countries GT knowledge selected: True Response: The funds were also spent on the training and dispatching of Ahmadiyya missionaries outside the Indian sub-continent and their sustenance. Yes, but he was also able to avoid prosecution despite numerous federal and state investigations in part due to the fact he took most of his kickbacks in cash.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R2C2-PT</head><p>Query: Frank Hague prosecution GT knowledge selected: False Response: No, no one tried to prosecute Frank Hague.</p><p>ELVM-β = 0 Query: Frank Hague ordinance ordinance frank Hague charges Frank Hague Hague for flouting constitutionality ordinances GT knowledge selected: False Response: Hague was found guilty of contempt of court for ignoring the subpoena. He was fined $100 and stripped of his duties as Deputy Sheriff.</p><p>ELVM w/ BM25 Query: Hague prosecution attempt failed after Hague passed unconstitutional ordinance. no Hague was prosecuted.</p><p>GT knowledge selected: True Response: Hague was able to stay in power despite a nearly constant effort to turn him out of office from 1921 onward. He was also able to avoid prosecution despite numerous federal and state investigations in part due to the fact he took most of his kickbacks in cash.</p><p>Table <ref type="table">24</ref>: Generated samples from QReCC test. We truncate the ground-truth knowledge and display only relevant spans of the ground-truth knowledge to the ground-truth response. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Adolphs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Boerschinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><forename type="middle">Chen</forename><surname>Huebscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannic</forename><surname>Kilcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00527</idno>
		<title level="m">Pier Giuseppe Sessa, et al. 2021a. Boosting search engines with interactive agents</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Adolphs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05204</idno>
		<title level="m">Reason first, then respond: Modular generation for knowledge-infused dialogue</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Open-domain question answering goes conversational via question rewriting</title>
		<author>
			<persName><forename type="first">Raviteja</forename><surname>Anantha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svitlana</forename><surname>Vakulenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhucheng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Chappidi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.44</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies, Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Query enhanced knowledgeintensive conversation via unsupervised joint modeling</title>
		<author>
			<persName><forename type="first">Mingzhu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bridging the gap between prior and posterior knowledge selection for knowledge-grounded dialogue generation</title>
		<author>
			<persName><forename type="first">Xiuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feilong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3426" to="3437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Anjie Fang, Besnik Fetahu, Oleg Rokhlenko, and Shervin Malmasi. 2022. Reinforced question rewriting for conversational question answering</title>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.15777</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><surname>Sepassi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<editor>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shivani</forename><surname>Agrawal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Omernick</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thanumalayan</forename><surname>Sankaranarayana Pillai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marie</forename><surname>Pellat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aitor</forename><surname>Lewkowycz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Erica</forename><surname>Moreira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Brennan</forename><surname>Saeta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Diaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kathy</forename><surname>Meier-Hellstern</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</editor>
		<meeting><address><addrLine>Palm</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01241</idno>
		<title level="m">Wizard of wikipedia: Knowledge-powered conversational agents</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning non-convergent non-persistent short-run mcmc toward energy-based model</title>
		<author>
			<persName><forename type="first">Nijkamp</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitch</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Md Faisal Mahbub Chowdhury, Ankita Rajaram Naik, Pengshan Cai, and Alfio Gliozzo. 2022. Re2g: Retrieve, rerank, generate</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaetano</forename><surname>Rossiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
		<title level="m">Realm: Retrievalaugmented language model pre-training</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PLATO-KAG: Unsupervised knowledge-grounded conversation via joint modeling</title>
		<author>
			<persName><forename type="first">Xinxian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.nlp4convai-1.14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</title>
		<meeting>the 3rd Workshop on Natural Language Processing for Conversational AI</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01282</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04906</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sequential latent knowledge selection for knowledge-grounded dialogue</title>
		<author>
			<persName><forename type="first">Byeongchang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Saving dense retriever from shortcut dependency in conversational search</title>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangwoo</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Internet-augmented dialogue generation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling ; Mojtaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Komeili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.579</idno>
		<idno type="arXiv">arXiv:1312.6114</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013. 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8460" to="8478" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Natural questions: A benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00276</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note>translation, and comprehension</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Xueliang Zhao, and Chongyang Tao. 2021. Zero-resource knowledge-grounded dialogue generation</title>
		<author>
			<persName><forename type="first">Linxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Zhao</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Incremental transformer with deliberation decoder for document grounded conversations</title>
		<author>
			<persName><forename type="first">Zekang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning to select knowledge for response generation in dialog systems</title>
		<author>
			<persName><forename type="first">Rongzhong</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04911</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Conversational question reformulation via sequence-to-sequence architectures and pretrained language models</title>
		<author>
			<persName><forename type="first">Sheng-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan-Ju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01909</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Three-Stage Learning Framework for Low-Resource Knowledge-Grounded Dialogue Generation</title>
		<author>
			<persName><forename type="first">Shilei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bochao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujuan</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Knowledge diffusion for neural dialogue generation</title>
		<author>
			<persName><forename type="first">Shuman</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongshen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1138</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1489" to="1498" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A compare aggregate transformer for understanding document-grounded dialogue</title>
		<author>
			<persName><forename type="first">Longxuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00190</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Towards exploiting background knowledge for building conversation systems</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Moghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suman</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08205</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">MS MARCO: A human-generated MAchine reading COmprehension dataset</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04572</idno>
		<title level="m">Taskoriented query reformulation with reinforcement learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hindsight: Posterior-guided training of retrievers for improved open-ended generation</title>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Question rewriting? assessing its importance for conversational question answering</title>
		<author>
			<persName><forename type="first">Gonçalo</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luísa</forename><surname>Coheur</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-99739-7_23</idno>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="199" to="206" />
			<date type="published" when="2022">2022</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Increasing faithfulness in knowledge-grounded dialogue with controllable features</title>
		<author>
			<persName><forename type="first">David</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Reitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Singh Tomar</surname></persName>
		</author>
		<author>
			<persName><surname>Das</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.58</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="704" to="718" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards empathetic opendomain conversation models: A new benchmark and dataset</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Boureau</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1534</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5370" to="5381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: Bm25 and beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Arthur Szlam, and Jason Weston. 2022a. Language models that seek for knowledge: Modular search &amp; generation for dialogue and prompt completion</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Komeili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Adolphs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.13224</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Komeili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megan</forename><surname>Ung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.03188</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. 2022b</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generative knowledge selection for knowledgegrounded dialogues</title>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.findings-eacl.155</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EACL 2023</title>
		<meeting><address><addrLine>Dubrovnik, Croatia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2077" to="2088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongrae</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Steven Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Ghafouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Menegali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Ching</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">S</forename><surname>Pickett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tulsee</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renelito Delos</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnny</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Soraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinodkumar</forename><surname>Zevenbergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandra</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Hoffman-John</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239,abs/2201.08239</idno>
		<title level="m">Lamda: Language models for dialog applications</title>
		<editor>
			<persName><forename type="first">Lora</forename><surname>Aroyo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ravi</forename><surname>Rajakumar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alena</forename><surname>Butryna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matthew</forename><surname>Lamm</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Viktoriya</forename><surname>Kuzmina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joe</forename><surname>Fenton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aaron</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rachel</forename><surname>Bernstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Blaise</forename><surname>Aguera-Arcas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Claire</forename><surname>Cui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marian</forename><surname>Croak</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Efficient retrieval augmented generation from unstructured knowledge for taskoriented dialog</title>
		<author>
			<persName><forename type="first">David</forename><surname>Thulke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nico</forename><surname>Daheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Dugast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04643</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SaFeR-Dialogues: Taking feedback gracefully after conversational safety failures</title>
		<author>
			<persName><forename type="first">Megan</forename><surname>Ung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.447</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6462" to="6481" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Question rewriting for conversational question answering</title>
		<author>
			<persName><forename type="first">Svitlana</forename><surname>Vakulenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhucheng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raviteja</forename><surname>Anantha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM international conference on web search and data mining</title>
		<meeting>the 14th ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="355" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Query resolution for conversational search with limited supervision</title>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Voskarides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="921" to="930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Conqrr: Conversational query rewriting for retrieval with reinforcement learning</title>
		<author>
			<persName><forename type="first">Zeqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Reitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><forename type="middle">Singh</forename><surname>Tomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Tianbao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Henry</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Scholak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.05966</idno>
		<title level="m">Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">2022a. Beyond goldfish memory: Long-term open-domain conversation</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.356</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5180" to="5197" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">2022b. Learning new skills after deployment: Improving open-domain internet-driven dialogue with human feedback</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megan</forename><surname>Ung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Komeili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.03270</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Diverse and faithful knowledge-grounded dialogue generation via sequential posterior inference</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqian</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fewshot generative conversational query rewriting</title>
		<author>
			<persName><forename type="first">Shi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingqin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1933" to="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Colv: A collaborative latent variable model for knowledge-grounded dialogue generation</title>
		<author>
			<persName><forename type="first">Haolan</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongshen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hainan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Personalizing dialogue agents: I have a dog, do you have pets too?</title>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1205</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Retgen: A joint framework for retrieval and grounded text generation modeling</title>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Knowledgegrounded dialogue generation with pre-trained language models</title>
		<author>
			<persName><forename type="first">Xueliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">2018a. Commonsense knowledge aware conversation generation with graph attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence, IJCAI&apos;18</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence, IJCAI&apos;18</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<biblScope unit="page" from="4623" to="4629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A dataset for document grounded conversations</title>
		<author>
			<persName><forename type="first">Kangyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
