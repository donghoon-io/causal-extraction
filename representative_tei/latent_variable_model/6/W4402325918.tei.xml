<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LIST OF APPENDICES Appendix</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">LIST OF APPENDICES Appendix</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Latent variable models play an increasingly crucial role in modern statistics and machine learning for analyzing large-scale and complex-structured data, with wideranging applications across various scientific fields. For instance, in educational assessments, latent variable models capture unobservable traits, such as intelligence, personality, and attitude. In biology and genomics, latent variable models uncover underlying genetic factors, gene expression patterns, or hidden biological mechanisms.</p><p>By inferring the latent variables, researchers gain a deeper understanding of the mechanisms governing the observed data. Despite wide applications of latent variable models, the large-scale and complex structures of data, and the involvement of covariates pose numerous challenges for its identifiability, estimation, and inference. This dissertation focuses on developing identifiability theory, estimation approaches, and inference methodologies for interpretable latent variable models, addressing three important problems: (I) The first part addresses the identifiability issues of latent class models with covariates. Despite widely used in various applications, the fundamental identifiability issue of the latent class models with covariates has not been fully addressed. To address this open identifiability issue, we establish conditions to ensure the global identifiability of the model parameters in both strict and generic sense. Moreover, our results extend to polytomous-response Cognitive Diagnosis Models (CDMs) with covariates, which generalizes the existing identifiability results for CDMs.</p><p>(II) The second part develops estimation and inference methods for generalized xiii linear framework with latent confounders. Statistical inferences for high-dimensional regression models have been extensively studied for their wide applications ranging from genomics, neuroscience, to economics. However, in practice, there are often potential unmeasured confounders associated with both the response and covariates, which can lead to invalidity of standard debiasing methods. In this part, we focus on a generalized linear regression framework with hidden confounding and propose a debiasing approach to address this high-dimensional problem, by adjusting for the effects induced by the unmeasured confounders. We establish consistency and asymptotic normality for the proposed debiased estimator.</p><p>(III) The third part focuses on statistical inference for covariate-adjusted generalized factor models. In addition to understanding the latent factors, the covariate effect on responses controlling for latent factors is also of great scientific interest and has wide applications, such as evaluating the fairness of educational testing, where the covariate effect reflects whether a test question is biased toward certain individual characteristics (e.g. gender and race) taking into account their latent abilities.</p><p>However, the large sample size, substantial covariate dimension, and great test length pose great challenges to developing efficient methods and drawing valid inferences.</p><p>Moreover, to accommodate the commonly encountered discrete type of responses, nonlinear factor models are often assumed, bringing in further complexity to the problem. To address these challenges, we consider a covariate-adjusted generalized factor model and develop novel and interpretable conditions to address the identifiability issue. Based on the identifiability conditions, we propose a joint maximum likelihood estimation method and establish estimation consistency and asymptotic normality results for the covariate effects under a practical yet challenging asymptotic regime. Furthermore, we derive estimation and inference results for latent factors and the factor loadings. xiv</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>this dissertation could not possibly have been completed. Gongjun is an exemplary researcher who is passionate and dedicated to both research and mentorship and is sharp-minded and exceptional in a broad area of statistics research. Besides being my academic role model, Gongjun is the best advisor I could have ever asked for.</p><p>He is always available for discussions, responding promptly with insightful comments whenever I reach out to him with questions or updates. His invaluable guidance has played a pivotal role in my growth from a junior PhD student to a researcher. I am deeply indebted to Gongjun for devoting enormous time and energy to mentoring me and for all his encouraging words and unwavering support throughout my PhD journey. Our meetings, conversations, and phone calls are treasured memories that will continue to inspire my strength, shape my research taste, and bring out the best in me in my future academic career. I am also grateful to my doctoral dissertation committee members: Prof. Kean Ming Tan, Prof. Ji Zhu, and Prof. Zhenke Wu. My collaboration with Kean Ming began in the second year of my PhD. I feel extremely fortunate to work with Kean Ming over the years. I thank Kean Ming for always inviting me to his group meetings and gatherings, and for mentoring and supporting me as much as he could. His mentorship has greatly contributed to my academic and personal development. I am grateful to Ji for his enormous support and encouragement. When I encountered challenges and reached out to Ji, our conversations always filled me with confidence and reassurance. I thank Ji for his consistent confidence in me and his invaluable guidance. I would like to thank Zhenke for all the enjoyable and interesting discussions. I gained insights into research from these conversations. I was extremely fortunate to collaborate with Prof. Yunxiao Chen and Prof. Chun Wang. I thank them for their constructive and insightful comments, as well as for their patience and generosity in dedicating time to our discussions. I am also thankful to Prof. Xuming He, my academic mentor during my first year of PhD, for his continuous care and support, and for providing me with invaluable suggestions for my job search. I extend my gratitude to Prof. Liza Levina for her strong encouragement during the Women in Statistics Lunch series, and her warm care and invaluable advice for my job application. Last but not least, I sincerely thank all the faculty members and staff members in the Department of Statistics of the University of Michigan for their guidance and support in my graduate study.</p><p>I extend my heartfelt gratitude to my amazing friends and brilliant peer collaborators: Chenchen Ma, Chengcheng Li, and Chengyu Cui. Collaborating with them makes the research process more enjoyable and I greatly benefit from our discussions. I thank Yuqi Gu, Yinqiu He, Weijing Tang, Tiffany Tang, and Ziping Xu for generously sharing their experiences and warm encouragement during my job application. I also thank my peer friends in our department, who helped me rehearse my job talk and offered detailed suggestions. Special thanks are owed to my wonderful friends in our research group, in our cohort, in our student offices, as well as friends outside the statistics department who consistently offer me encouragement and care. Their companionship makes Michigan feel like another home to me. I will remember those interesting conversations and precious moments we shared, and I am grateful for their support and for standing by me through my joys and sorrows. Without them, the PhD journey would not have been as enjoyable and memorable.</p><p>Last but not least, I owe my thanks to my parents for their unconditional love. I iii am deeply grateful to them for their support and care. Their unwavering support is always the strongest courage in my life.  Confidence interval estimation using proposed method under logistic regression for the stimulation groups of PAM (top), PIC (middle) and LPS (bottom). Purple intervals indicate confidence intervals that do not cover zero. Red intervals represent confidence intervals that are among purple intervals and significant after Bonferroni Correction. . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Latent variable models are important tools for interpreting machine learning models and discovering hidden structures in complex datasets, with wide-ranging applications across various scientific fields including educational assessments <ref type="bibr" target="#b80">(Junker and Sijtsma, 2001;</ref><ref type="bibr" target="#b143">von Davier Matthias, 2008)</ref>, biology and genomics <ref type="bibr" target="#b92">(Leek and Storey, 2008;</ref><ref type="bibr" target="#b26">Carvalho et al., 2008)</ref>, and economics and finances <ref type="bibr" target="#b6">(Bai , 2003;</ref><ref type="bibr" target="#b48">Fan et al., 2013)</ref>.</p><p>In various applications, latent variable models help to enhance interpretability and increase representational and predictive power. In particular, latent variable models are commonly used to interpret the latent variables, which find wide applications in performing personalized diagnosis and inferring fine-grained latent information <ref type="bibr" target="#b135">(Templin and Henson, 2006;</ref><ref type="bibr" target="#b72">Henson et al., 2009;</ref><ref type="bibr" target="#b44">de la Torre, 2011)</ref>. For example, in educational testing, latent variables can represent students' abilities or skills. Inferring these latent variables from observed responses provides educators with a more comprehensive understanding of students' achievements and learning progresses <ref type="bibr" target="#b143">(von Davier Matthias, 2008)</ref>. On top of interpreting the latent variables, researchers are also investigating the covariate effects controlling latent variables or adjusting latent confounders <ref type="bibr" target="#b92">(Leek and Storey, 2008;</ref><ref type="bibr">Wang et al., 2017;</ref><ref type="bibr">Chen et al., 2023a)</ref>. For instance, the effect of certain segments of DNA on gene expression might be confounded by population stratification <ref type="bibr" target="#b92">(Leek and Storey, 2008;</ref><ref type="bibr">Wang et al., 2017)</ref>. Properly adjusting for the bias from the latent confounders is necessary for quantifying the uncertainty of covariate effects. Another important application is measuring testing fairness, which raises great concern within not only academia but also the broader public <ref type="bibr" target="#b138">(Toch, 1984)</ref>. To measure if the test items are fairly designed, researchers aim to interpret the demographic group effect on the item response probability conditioned on the levels of assessed abilities <ref type="bibr" target="#b73">(Holland and Wainer , 2012)</ref>.</p><p>Despite the interpretability and wide applicability of latent variable models, several challenges emerge when using them to address urgent and practical problems.</p><p>These challenges arise due to the discrete nature of responses, the existence of observed covariates and their dependence on latent variables, and the increasing amount of massive and complex data, which will be further elaborated in the following chapters. Limited by these challenges, the existing methods cannot be directly applied to model such complex data. There is a critical need for novel theories and methodologies capable of handling the complexities inherent in both the model structure and the observed data. To address these challenges, we develop identifiability theories, estimation approaches, and inference methods for analyzing large-scale complex data, which find wide applications in addressing problems such as making better personalized inference and enhancing fairness in educational assessments. Specifically, this dissertation studies the following three problems.</p><p>Chapter II studies the open identifiability problem for regression latent class models. One popular used latent variable model is the latent class model, which is to perform personalized diagnosis and achieve fine-grained inference on individuals' latent attributes <ref type="bibr" target="#b80">(Junker and Sijtsma, 2001;</ref><ref type="bibr" target="#b44">de la Torre, 2011)</ref>. Such latent attribute is often assumed to be discrete and of particular scientific interpretations, such as the presence or absence of certain underlying mental disorders in psychiatric diagnosis <ref type="bibr" target="#b135">(Templin and Henson, 2006;</ref><ref type="bibr" target="#b151">Wu et al., 2017)</ref> and mastery or deficiency of a targeted skill in educational diagnosis <ref type="bibr" target="#b80">(Junker and Sijtsma, 2001;</ref><ref type="bibr" target="#b72">Henson et al., 2009)</ref>.</p><p>Before the parameter estimation and statistical inference, one important prerequisite is to ensure the model identifiability <ref type="bibr" target="#b2">(Allman et al., 2009;</ref><ref type="bibr" target="#b152">Xu, 2017;</ref><ref type="bibr" target="#b62">Gu and Xu, 2020)</ref>. Existing identifiability theories primarily focus on general or restricted latent class models <ref type="bibr" target="#b152">(Xu, 2017;</ref><ref type="bibr" target="#b42">Culpepper , 2019;</ref><ref type="bibr" target="#b62">Gu and Xu, 2020)</ref>, and cannot be directly applied when observed covariates are involved. Therefore, a new identifiability theory needs to be developed to account for the presence of covariates. In this chapter, We establish the conditions for the identifiability of latent class models with covariates.</p><p>The established results open the door for developing identifiability theory and estimation methods for general covariate-adjusted latent variable models (see Chapter IV).</p><p>This chapter contributes to the research paper <ref type="bibr" target="#b110">Ouyang and Xu (2022)</ref>.</p><p>Chapter III focuses on the high-dimensional inference problem for generalized linear models with hidden confounding. High-dimensional and complex data are collected across various scientific fields, bringing up ever-growing interests among statisticians in developing methodologies to handle them <ref type="bibr" target="#b118">(Peng et al., 2010;</ref><ref type="bibr" target="#b15">Belloni et al., 2012;</ref><ref type="bibr" target="#b49">Fan et al., 2014;</ref><ref type="bibr" target="#b22">Bühlmann et al., 2014)</ref>. However, there are often potential latent confounders (latent variables) associated with both response and covariates in the regression models. The existence of such latent confounders leads to the invalidity of standard debiasing methods established for high-dimensional inferences <ref type="bibr" target="#b156">(Zhang and Zhang, 2014;</ref><ref type="bibr" target="#b141">van de Geer et al., 2014;</ref><ref type="bibr" target="#b79">Javanmard and Montanari , 2014;</ref><ref type="bibr" target="#b109">Ning and Liu, 2017)</ref>. Among recent high-dimensional studies accounting for latent confounders, the existing methods either require linearity assumptions between response and covariates <ref type="bibr" target="#b65">(Guo et al., 2022;</ref><ref type="bibr" target="#b51">Fan et al., 2023)</ref>, which are hard to apply to the case when the response is discrete, or only consider estimation methods but leave inference problems unsolved <ref type="bibr">( Ćevid et al., 2020)</ref>. To bridge this gap, this chapter considers a generalized linear framework to relate the response to covariates and latent confounders. Latent variable models are assumed to model the relation between covariates and latent confounders. Under this framework, we develop a general method applicable to generalized linear models. Theoretically, the nonlinear framework poses extra challenges as the decomposition and transformation techniques commonly used for linear models are no longer applicable <ref type="bibr" target="#b65">(Guo et al., 2022;</ref><ref type="bibr" target="#b51">Fan et al., 2023)</ref>. We address the aforementioned limitation and establish the estimation consistency with the estimation error rates comparable to that of the regularized generalized linear models without latent confounders. Moreover, we also prove the asymptotic normality for the estimators, validating my inference methods under this model framework.</p><p>This chapter contributes to the research paper <ref type="bibr" target="#b111">Ouyang et al. (2023)</ref>.</p><p>Chapter IV focuses on statistical inference problems for covariate-adjusted latent factor models, with important applications to testing fairness in educational assessments. Fairness is important to ensure equity in various decision-making processes and societal structures. In education, fairness relates to providing equal educational opportunities and resources to all students, irrespective of their background <ref type="bibr" target="#b73">(Holland and Wainer , 2012)</ref>. It includes addressing issues like test inequity and ensuring that students from different demographic groups have the same probability of endorsing the test items, controlling for individual proficiency levels <ref type="bibr" target="#b73">(Holland and Wainer , 2012)</ref>. In such problems, nonlinear latent factor models are popularly used to analyze large-scale response data from educational tests and to understand covariate effects conditioned on latent factors <ref type="bibr" target="#b18">(Belzak and Bauer , 2020;</ref><ref type="bibr" target="#b13">Bauer et al., 2020;</ref><ref type="bibr">Chen et al., 2023a)</ref>. Motivated by the testing fairness problem, Chapter IV studies the inference problem for large-scale educational data under a general model framework of nonlinear latent factor model with covariates. To analyze the increasing amount of extensive educational data with large sample sizes, high dimensional covariates, and a substantial number of test items, we propose a scalable and interpretable joint likelihoodbased estimation method under identifiability constraints. We obtain the estimation consistency and asymptotic normality for the covariate coefficient, ensuring the accuracy of covariate effect estimation. Furthermore, we also prove the consistency and asymptotic normality for the latent factors and the factor loadings under a practical yet challenging asymptotic regime, advancing the current understanding of statistical nonlinear latent variable models, where most of the existing works only focus on the consistency properties with statistical inferences largely underexplored. This chapter contributes to the research paper <ref type="bibr" target="#b112">Ouyang et al. (2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CHAPTER II</head><p>Identifiability of Latent Class Models with Covariates</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Introduction</head><p>Latent class models are extensively applied in numerous scientific fields, including educational assessments, biological research, and psychological measurements, to infer the latent subgroups of a population as well as each subject's latent classification information. For instance, one application of latent class models in cognitive diagnosis is to classify individuals with different latent attributes based on their observed responses to items, for which reason they are key components in educational measurements <ref type="bibr" target="#b80">(Junker and Sijtsma, 2001;</ref><ref type="bibr" target="#b143">von Davier Matthias, 2008)</ref>, psychiatric evaluations <ref type="bibr" target="#b135">(Templin and Henson, 2006)</ref>, and disease detections <ref type="bibr" target="#b151">(Wu et al., 2017)</ref>.</p><p>In addition to understanding the basic parameters in latent class models, researchers are also interested in studying the relations between latent class parameters with the observed covariates, which are fixed and known information of the subjects including their gender, race, education level and other characteristics <ref type="bibr" target="#b55">Formann (1985)</ref>; <ref type="bibr" target="#b39">Collins and Lanza (2009)</ref>; <ref type="bibr" target="#b76">Huang and Bandeen-Roche (2004)</ref>.</p><p>The latent class models with covariates help to improve the classification accuracy of the latent classes and are useful in testing whether the covariates are related to the latent class membership probability or response probability. Such latent class models involving covariates have been studied in many works in psychometrics and statistics literature, where covariates were mostly constrained to be discrete at early stage Clogg and <ref type="bibr" target="#b38">Goodman (1984)</ref>; <ref type="bibr" target="#b55">Formann (1985)</ref>, and further extended to be in general forms <ref type="bibr" target="#b43">Dayton and Macready (1988)</ref>; van der <ref type="bibr" target="#b142">Heijden et al. (1996)</ref>; <ref type="bibr" target="#b107">Muthén and Muthén (2017)</ref>. The models have been popularly applied in educational, psychological, and behavioral sciences <ref type="bibr" target="#b39">Collins and Lanza (2009)</ref>; <ref type="bibr" target="#b106">Muthén and Masyn (2005)</ref> For latent class models with or without covariates, identifiability is one of the most fundamental issues as it is the pre-requisite for parameter estimations and statistical inferences. Identifiability could be interpreted as the feasibility of recovering the model parameters based on observed responses, i.e., the parameters in identifiable models should be distinct given the probablistic distribution of the observations. A rich body of literature have studied identifiability issues, dating back to <ref type="bibr" target="#b83">Koopmans (1950)</ref> and <ref type="bibr" target="#b84">Koopmans and Reiersol (1950)</ref>. <ref type="bibr">Specifically, McHugh (1956)</ref> proposed conditions to determine the local identifiability for latent class models with binary responses, and <ref type="bibr" target="#b60">Goodman (1974)</ref> further extended the local identifiability conditions to polytomous response models. In the sense of strict identifiability, <ref type="bibr" target="#b66">Gyllenberg et al. (1994)</ref> found that latent class models with binary responses can not be strictly identifiable. Nonetheless, <ref type="bibr" target="#b2">Allman et al. (2009)</ref> considered the concept of generic identifiability and established sufficient conditions for the generic identifiability of latent class models, where a model is said to be generically identifiable if the model parameters are identifiable except for a measure-zero set of parameters. However, their generic identifiability conditions can only be applied to unrestricted latent class models, but not directly to restricted latent class models. To address this issue, <ref type="bibr" target="#b152">Xu (2017)</ref> and <ref type="bibr" target="#b153">Xu and Shang (2018)</ref> established the results for the identifiability of the Q-restricted latent class models with binary responses. For polytomous response models, Culpepper (2019) and <ref type="bibr" target="#b52">Fang et al. (2019)</ref> established strict identifiability conditions based on the algebraic theorems proposed by <ref type="bibr" target="#b86">Kruskal (1977)</ref>. Moreover, <ref type="bibr" target="#b62">Gu and Xu (2020)</ref> studied the generic and partial identifiability of restricted latent class models with binary responses and extended their conditions to models with polytomous responses as well.</p><p>Among existing research, most focus on the identifiability of general or restricted latent class models with no covariate, whereas few investigate on the identifiability of latent class models with covariates. As the observed covariates represent characteristics of certain homogeneous groups, incorporating covariates into the latent class models would help to explain the association of these characteristics with latent classes. Such regression latent class models with covariates are general extensions of latent class models without covariates, where the regular or restricted latent class models can be viewed as special sub-models in the family of latent class models with covariates, where all covariates values are zero. Technically speaking, existing identifiability results for regular or restricted latent class models cannot be directly applied to the regression latent class models due to the additionally included covariates, and new techniques are needed to establish the identifiability of the corresponding regression coefficients for those covariates, which do not exist in the regular or restricted latent class models. In the literature, <ref type="bibr" target="#b76">Huang and Bandeen-Roche (2004)</ref> was among the first to study the identifiability of the latent class models with covariates. The authors studied the local identifiability conditions for the model parameters, that is, the conditions to ensure that the model parameters are identifiable in a neighborhood of the true parameters.</p><p>However, as to be shown in the paper, the proposed identifiability conditions in <ref type="bibr" target="#b76">Huang and Bandeen-Roche (2004)</ref>  In light of these, this chapter establishes identifiability conditions to check the global identifiability for the latent class models with covariates. Furthermore, we also establish the identifiability results for the CDMs with covariates, which is a special family of regression latent class models. Our results extend many identifiability conditions for the binary-response CDMs to the polytomous-response CDMs with covariates, and these conditions are beyond results in the existing literature related to CDMs identifiability <ref type="bibr" target="#b152">(Xu, 2017;</ref><ref type="bibr" target="#b42">Culpepper , 2019;</ref><ref type="bibr" target="#b62">Gu and Xu, 2020)</ref>.</p><p>The organization of this chapter is as follows. Section 2.2 introduces the setup of the regression latent class models with covariates as well as the regression CDMs, and reviews some existing identifiability results. Section 2.3 discusses the necessity and sufficiency of the existing identifiability conditions for the regression latent class models. Section 2.4 presents our main results for both strict and generic identifiability of the regression latent class models as well as the regression CDMs. Section 2.6 gives a discussion. The proofs for the main theorems and propositions are provided in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Setup and Existing Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Regression Latent Class Models (RegLCMs)</head><p>The setup of the regression latent class models starts with some general notation definitions. Suppose there are N subjects responding to J items. The response of subject i is denoted as R i = (R ij ; j = 1, . . . , J), where R ij denotes the response of subject i to item j, for i = 1, . . . , N . And R ij ∈ {0, . . . , M j -1}, where M j denote the number of possible values for R ij . Denote S = × J j=1 {0, . . . , M j -1} as the set of all response patterns, and its cardinality is denoted as S = |S| = J j=1 M j . The case when M j = 2 corresponds to binary response models. Consider there are C latent classes and denote L i as the latent class membership for subject i. Assume the N subjects are independent; for c = 0, . . . , C -1, L i = c implies that the subject i is in the cth latent class category and η c = P (L i = c) defines the latent class membership probability, i.e. the probability for subject i being in the cth latent class. The latent class membership probabilities are summarized as η = (η c ; c = 0, . . . , C -1). For any j = 1, . . . , J, r = 0, . . . , M j -1, and c = 0, . . . , C -1, we use θ jrc = P (R ij = r | L i = c) to denote the conditional response probability, i.e. the probability of the response to item j being r given the subject i is in the cth latent class. Let the vector θ jc = (θ j0c , • • • , θ j(M j -1)c ) to denote the probability vector for item j given the latent class membership to be c. The conditional response probabilities are summarized as Θ = (θ jc ) J×C .</p><p>Following the model setting proposed in <ref type="bibr" target="#b76">Huang and Bandeen-Roche (2004)</ref>, we let the latent class membership probability η c 's and the conditional response probability θ jrc 's to be functionally dependent on covariates. Denote (x i , z i ) to be the covariates of subject i, where</p><formula xml:id="formula_0">x i = (1, x i1 , • • • , x ip ) T</formula><p>(p+1)×1 are the primary covariates related to the latent class membership η c for c = 0, . . . , C -1, and</p><formula xml:id="formula_1">z i = (z i1 , • • • , z iJ ) T J×q with z ij = (z ij1 , • • • , z ijq ) T q×1</formula><p>being the secondary covariates associated with the conditional response probability θ jrc for any j = 1, . . . , J, r = 0, . . . , M j -1, and c = 0, . . . , C -1.</p><p>The x it and z ijs can be categorical covariates representing gender, race or marital status. They can also be continuous, such as the subject's age or years of education.</p><p>As in some applications, we may have certain prior knowledge on the set of the covariates related to η c and that of the covariates related to θ jrc , where the two sets may or may not contain the same covariates. Hence we follow the general framework in <ref type="bibr" target="#b76">Huang and Bandeen-Roche (2004)</ref> by applying different notations, x i and z i , to distinguish the covariates related to η c and θ jrc , while allowing the x i and z i to have some overlapped covariates.</p><p>Before presenting the generalized linear model framework, we need to clarify some notations. In models without covariates, e.g., latent class models or CDMs to be discussed in Section 2.2.2, we use η c and θ jrc to denote the corresponding latent class membership probability and conditional response probability, respectively. When covariates are involved in models, the parameters are dependent on the covariates.</p><p>In this situation, we denote η i c = P (L i = c | x i , z i ) to be the latent class membership probability for subject i, and θ i jrc = P (R ij = r | L i = c, x i , z i ) to be the conditional response probability for subject i, for i = 1, . . . , N .</p><p>Under the generalized linear model framework, we use logit link function to relate η i c 's and θ i jrc 's to covariates (x i , z i ). We let the log-odds to be linearly dependent on the covariates and characterize the RegLCMs by the following equations</p><formula xml:id="formula_2">log η i c η i 0 = β 0c + β 1c x i1 + • • • + β pc x ip ,<label>(2.1)</label></formula><p>for i = 1, . . . , N, c = 1, . . . , C -1, and</p><formula xml:id="formula_3">log θ i jrc θ i j0c = γ jrc + λ 1jr z ij1 + • • • + λ qjr z ijq ,<label>(2.2)</label></formula><p>for i = 1, . . . , N , j = 1, . . . , J, r = 1, • • • , M j -1 and c = 0, . . . , C -1, where β, γ, λ are regression coefficient parameters. We would like to point out that the identifiability conditions that we will propose in Section 4 still hold for RegLCMs when the logarithmic function in (2.1) and (2.2) is replaced with other monotonic functions. The key components in establishing the identifiability conditions for the coefficient parameters is the function monotonicity, which build the bijective mapping between identifiable (η, Θ) and identifiable <ref type="bibr">(β, γ, λ)</ref>. In this chapter, without loss of generality, we shall focus on the popularly used logit link function.</p><p>From (2.1) and (2.2), we can equivalently express η i c and θ i jrc as</p><formula xml:id="formula_4">η i c = exp(β 0c + β 1c x i1 + • • • + β pc x ip ) 1 + C-1 l=1 exp(β 0l + β 1l x i1 + • • • + β pl x ip ) ,<label>(2.3)</label></formula><p>for i = 1, . . . , N , c = 0, . . . , C -1, and</p><formula xml:id="formula_5">θ i jrc = exp(γ jrc + λ 1jr z ij1 + • • • + λ qjr z ijq ) 1 + M j -1 s=1 exp(γ jsc + λ 1js z ij1 + • • • + λ qjs z ijq ) ,<label>(2.4)</label></formula><p>for i = 1, . . . , N , j = 1, . . . , J, r = 0, • • • , M j -1 and c = 0, . . . , C -1. From the above expressions, we can see that η i c and θ i jrc are functionally dependent on the linear functions x T i β and γ jc + z T ij λ j , where β = (β c ; c = 0, . . . , C -1) (p+1)×C with β c = (β lc ; l = 0, . . . , p) T (p+1)×1 , γ jc = (γ jrc ; r = 0, . . . , M j -1) 1×M j , and λ j = (λ jr ; r = 0, . . . , M j -1) q×M j with λ jr = (λ ljr ; l = 1, . . . , q) T q×1 .</p><p>Here following <ref type="bibr" target="#b76">Huang and Bandeen-Roche (2004)</ref> 1. The latent class membership probability η i c is dependent on x i only and the conditional response probability θ i jrc is dependent on z i only:</p><formula xml:id="formula_6">P (L i = c | x i , z i ) = P (L i = c | x i ); P (R i1 = r 1 , • • • , R iJ = r J | L i , x i , z i ) = P (R i1 = r 1 , • • • , R iJ = r J | L i , z i ).</formula><p>2. The measurements for different items are independent given latent class and z i (that is, the local independence assumption):</p><formula xml:id="formula_7">P (R i1 = r 1 , • • • , R iJ = r J | L i , z i ) = J j=1 P (R ij = r j | L i , z i ).</formula><p>When the coefficients β 1c </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Cognitive Diagnosis Models as Special RegLCMs</head><p>In CDMs, each latent class corresponds to a distinct vector α </p><formula xml:id="formula_8">= (α 1 , • • • , α K ) ∈ A = {0, 1} K where α 1 , • • • , α K denote K</formula><formula xml:id="formula_9">v = (2 K-1 , 2 K-2 , • • • , 1) T and denote the latent membership as L = α T v = c ∈ {0, • • • , 2 K -1}.</formula><p>The key characteristics of CDMs is its introduction of the latent attributes and let the combinations of mastery or deficiency of each attribute to represent the latent class memberships in restricted latent class models.</p><p>The relationship between any subject with response R = (R 1 , • • • , R J ) and attribute profile α could be summarized through a binary matrix Q J×K . Denote the jth row in Q-matrix to be Q j = (q j1 , • • • , q jK ), where q jk ∈ {0, 1} and q jk = 1 means that the kth attribute is required for subjects to solve item j. Similar to RegLCMs, we consider the general polytomous responses</p><formula xml:id="formula_10">R j ∈ {0, • • • , M j -1}.</formula><p>Given a subject's latent profile α with α T v = c, each R j follows a Categorical distribution with the probability vector to be</p><formula xml:id="formula_11">θ jc = (θ j0c , • • • , θ j(M j -1)c ), where θ jrc = P (R j = r | α T v = c)</formula><p>is the probability for getting response value r in item j. The probability mass function for R j is</p><formula xml:id="formula_12">P (R j | α T v = c, θ jc ) = M j -1 r=0 θ I{R j =r} jrc .</formula><p>The distribution for R is</p><formula xml:id="formula_13">P (R | α T v = c, η, Θ) = α T v=c α∈{0,1} K η c J j=1 P (R j | α T v = c, θ jc ) = α T v=c α∈{0,1} K η c J j=1 M j -1 r=0 θ I{R j =r} jrc .</formula><p>Following the generalized DINA (G-DINA) model framework, we decompose the log-odds of θ jrc into a sum of effects depending on the product of the presences and requirements of attributes combinations. This framework was introduced in de la <ref type="bibr" target="#b44">Torre (2011)</ref> for G-DINA model with binary responses, and extended to G-DINA with polytomous responses in Chen and de la Torre (2018). Specifically, given a latent profile α = (α 1 , . . . , α K ), we have log</p><formula xml:id="formula_14">θ jrc θ j0c = b jr0 + K k=1 b jr1 q jk α k + K k ′ =k+1 K-1 k=1 b jrkk ′ (q jk α k )(q jk ′ α k ′ ) + • • • + b jr12•••K K k=1 q jk α k , (2.5) where b jr0 , b jr1 , • • • , b jrK , b jr12 , • • • , b jr(K-1)K , • • • , b jr12•••K are the coefficients in the</formula><p>generalized linear regression of the log-odds of conditional response probability on all latent attribute mastery situations, that is, all the subsets of</p><formula xml:id="formula_15">{q j1 α 1 , • • • , q jK α K }.</formula><p>Specifically, b jr0 is the intercept of the log-odds; b jr1 , • • • , b jrK are the main effects of attributes, representing the change of log-odds due to the mastery of the single</p><formula xml:id="formula_16">attribute of α 1 , . . . , α K respectively; b jr12 , • • • , b jr(K-1)K , • • • , b jr12•••K are the interac-</formula><p>tion effects of attributes, representing the change of log-odds due to the mastery of the combination of two or more attributes from α 1 , . . . , α K .</p><p>For subjects with covariates values being zeros, the log-odds in G-DINA model (2.5) is equivalent to general log-odds setting (2.2), which is the log-odds for RegLCMs and written as log</p><formula xml:id="formula_17">θ jrc θ j0c = γ jrc + λ 1jr z j1 + • • • + λ qjr z jq = γ jrc + λ 1jr 0 + • • • + λ qjr 0 = γ jrc ,<label>(2.6)</label></formula><p>which could be further expressed as</p><formula xml:id="formula_18">θ jrc = exp(γ jrc ) 1 + M j -1 s=1 exp(γ jsc )</formula><p>.</p><p>for j = 1, . . . , J, r = 1, . . . , M j -1 and c = 0, . . . , C -1. We can show (2.5) and</p><p>(2.6) are equivalent. Because in (2.5), the log-odds of conditional response probability are linear combinations of all the subsets of {q j1 α 1 , • • • , q jK α K }, and are dependent on latent profile α = (α 1 , • • • , α K ) only, equivalently dependent on c at c = α T v. When covariates are zeros, the latent class category information is entirely captured by the intercept γ jrc in (2.6), implying that for given j and r, each γ jrc is bijectively corresponding to α ∈ A, which further implies that there exist a bijective</p><formula xml:id="formula_19">linear correspondence between {b jr0 , b jr1 , • • • , b jrK , b jr12 , • • • , b jr(K-1)K , • • • , b jr12•••K } and {γ jrc : c = 0, • • • , C -1}.</formula><p>When covariates are involved in CDMs, we introduce the regression CDMs (Reg-CDMs) by the following equations (2.7) and (2.8) adapted from (2.1) and (2.2), with the additional characteristics of CDMs that each latent membership c is represented by a latent profile α. To make notations clear under this case, we denote the latent attributes of subject i as α i = (α i1 , . . . , α iK ) for i = 1, . . . , N . And similarly as in RegLCMs, we use</p><formula xml:id="formula_20">η i c = P (α T i v = c | x i , z i ) to denote the latent class membership</formula><p>probability for subject i, and use</p><formula xml:id="formula_21">θ i jrc = P (R ij = r | α T i v = c, x i , z i )</formula><p>to denote the conditional response probability for subject i when these parameters are dependent on covariates.</p><p>Assuming that the latent membership c = 0 denotes the latent profile that the subject i does not master any of K attributes, i.e. α i = 0 K×1 , we model</p><formula xml:id="formula_22">log η i c η i 0 = β 0c + β 1c x i1 + • • • + β pc x ip , (2.7) for i = 1, . . . , N and α T i v = c with α i ∈ {0, 1} K \ 0 K×1 , and log θ i jrc θ i j0c = γ jrc + λ 1jr z ij1 + • • • + λ qjr z ijq ,<label>(2.8)</label></formula><p>for i = 1, . . . , N , j = 1, . . . , J, r = 1, . . . , M j -1, and</p><formula xml:id="formula_23">α T i v = c with α i ∈ {0, 1} K .</formula><p>RegCDMs combine the regression setting on covariates from RegLCMs and the latent attribute representation from CDMs, which is to use binary latent profiles to represent latent classes. In addition, the two assumptions 1 and 2 in Section 2.2.1 are also needed for RegCDMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Identifiability Conditions in Existing Literature</head><p>Before discussing our main results for the identifiability of the models introduced in Section 2. To study the global identifiability, <ref type="bibr" target="#b86">Kruskal (1977)</ref> established algebraic results</p><p>to ensure the uniqueness of factors in the decomposition of a three-way array. this chapter defined Kruskal rank which is analogous to the normal rank of a matrix.</p><p>And it proved that if the Kruskal ranks of a triple product of matrices satisfy certain arithmetic condition, the matrix decomposition will be unique. Based on Kruskal's theorems, <ref type="bibr" target="#b2">Allman et al. (2009)</ref> extended the conditions to the decomposition into more than three variates, and used it in the identifiability conditions for the latent class models with finite items. Besides, <ref type="bibr" target="#b2">Allman et al. (2009)</ref> argued that even the parameters are not identifiable, the parameter inference could still be valid empirically when the model is generically identifiable, that is, the parameters are identifiable except for a zero-measure set of parameters. The generic identifiability results allow us to circumvent the complex calculation on the column rank of the Jacobian matrix.</p><p>In the recent literature, the identifiability of restricted latent class models, such as CDMs, has also been studied. Related identifiability results on restricted models with binary responses are developed in <ref type="bibr" target="#b31">Chen et al. (2015)</ref>, <ref type="bibr" target="#b154">Xu and Zhang (2016)</ref>, where ϕ c is an (S -1)-dimensional vector in which each element corresponds to a response pattern r = (r 1 , • • • , r J ) ∈ S ′ and is defined as</p><formula xml:id="formula_24">Xu</formula><formula xml:id="formula_25">ϕ rc = P (R = r | L = c, z = 0) = J j=1 e γ jr j c 1 + M j -1 s=1 e γ jsc ,<label>(2.9)</label></formula><p>where γ jr j c are defined as in (2.2) with r = r j and we set γ j0c = 0 for all j = 1, . . . , j and c = 0, . . . , C -1. <ref type="bibr" target="#b76">Huang and Bandeen-Roche (2004)</ref> proposed that the latent class models is locally identifiable at free parameters of (β, γ, λ) = {β dc , γ jrc , λ tjr : j = 1, . . . , J, r = 0, . . . , M j -1, c = 0, . . . , C -1, d = 0, . . . , p, t = 1, . . . , q} if the following conditions are satisfied,</p><formula xml:id="formula_26">(A1) J j=1 M j -1 ≥ C( J j=1 M j -J) + C -1;</formula><p>(A2) Free parameters γ jrc , λ qjr , β pc and covariate values x ip , z ijq are all finite;</p><p>(A3) The design matrix of the covariates</p><formula xml:id="formula_27">X =       x T 1 . . . x T N       =       1 x 11 • • • x 1p . . . . . . . . . . . . 1 x N 1 • • • x N p      </formula><p>and</p><formula xml:id="formula_28">Z j =       1 z T 1j . . . . . . 1 z T N j       =       1 z 1j1 • • • z 1jq . . . . . . . . . . . . 1 z N j1 • • • x N jq       , j = 1, • • • , J have full column rank; (A4) ϕ 0 , • • • , ϕ C-1 are linearly independent.</formula><p>Remark II.1. As in <ref type="bibr" target="#b76">Huang and Bandeen-Roche (2004)</ref>, if we consider F to be the number of pre-fixed conditional probabilities θ jrc = 0 or 1, then (A1) should be ex-</p><formula xml:id="formula_29">tended to J j=1 M j -1 ≥ C( J j=1 M j -J) + C -1 -F .</formula><p>For simplicity, we assume F = 0 throughout the paper.</p><p>Remark II.2. The condition (A1) implies the number of independent response probabilities</p><formula xml:id="formula_30">J j=1 M j -1 = card({P (R 1 = r 1 , . . . , R J = r J ) : r j = 0, . . . , M j -1, j = 1, . . . , J}),</formula><p>exceeds the number of independent parameters in (η, Θ),</p><formula xml:id="formula_31">C( J j=1 M j -J) + C -1 = card({η c , θ jrc : j = 1, . . . , J, r = 1, . . . , M j -1, c = 0, . . . , C -1}).</formula><p>The condition (A1) is necessary, without which the observed response information may produce infinite parameter solutions and lead the model to be not identifiable.</p><p>For technical rigorousness, condition (A2) as proposed in <ref type="bibr" target="#b76">Huang and Bandeen-Roche (2004)</ref> specifies the model parameters and covariates x ip , z ijq are finite. In practice, the observed covariates are documented as finite values, and thus the finite condition on x ip and z ijq is automatically satisfied.</p><p>For RegLCMs without covariates, which are equivalently to RegLCMs with x i = (1, 0 1×p ) T and z i = 0 J×q , Huang and Bandeen-Roche (2004) gave a reduced form of identifiability conditions. They claimed an equivalence between the full column rank condition on the Jacobian matrix and linear independence condition on the columns of marginal probability matrix Ψ defined as follows:</p><formula xml:id="formula_32">Ψ = (ψ c ; c = 0, . . . , C -1) (S-1)×C ,</formula><p>where ψ c is a (S -1)-dimensional vector with each element corresponds to a distinct</p><formula xml:id="formula_33">response pattern r = (r 1 , • • • , r J ) ∈ S ′ and ψ rc = P (R = r | L = c) = J j=1 M j -1 r=0 θ I{r j =r} jrc = J j=1 θ jr j c .</formula><p>(2.10)</p><p>Here for notational convenience, we let θ jr j c to denote θ jrc defined in Section 2.2.1 with r = r j . Under the particular covariate latent class models where</p><formula xml:id="formula_34">x i = (1, 0 1×p ) T</formula><p>and z i = 0 J×q , <ref type="bibr" target="#b76">Huang and Bandeen-Roche (2004)</ref> proposed that (η, Θ) = {η c , θ jrc : j = 1, . . . , J, r = 0, . . . , M j -1, c = 0, . . . , C -1} are locally identifiable if (A1) and the following conditions are satisfied:</p><p>(A2 * ) For all free parameters, θ jrc &gt; 0 and η c = P (L = c) &gt; 0;</p><formula xml:id="formula_35">(A3 * ) ψ 0 , • • • , ψ C-1 are linearly independent.</formula><p>We can see that (A1)-( <ref type="formula" target="#formula_60">A3</ref>) and (A2 * ), are necessary for the respective models.</p><p>The necessity of (A1) and (A2) are discussed in Remark II.2. The (A2 * ) guarantees that the latent class membership probabilities and conditional response probabilities are non-zero, and thus it is also a necessary condition. ( <ref type="formula" target="#formula_60">A3</ref>) is needed to ensure β and λ are uniquely estimable. As for (A3 * ), it is related to the condition that the Jacobian matrix has full column rank. In the next section, we show under the assumption that (A1)-(A2 * ) hold, (A3 * ) is necessary for the local identifiability of the special latent class models when covariates are zeros, but is actually not sufficient. Similarly, for RegLCMs containing covariates, under (A1)-( <ref type="formula" target="#formula_60">A3</ref>), (A4) is a necessary identifiability condition but not sufficient condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Necessity but insufficiency of the conditions in Huang</head><p>and <ref type="bibr" target="#b76">Bandeen-Roche (2004)</ref> In this section, we show that the identifiability conditions in Huang and Bandeen-Roche (2004) are not sufficient. Following the discussion in Section 2.2.3, we first present the necessity of (A4) for RegLCMs and (A3 * ) for RegLCMs without covariates, respectively.</p><p>Proposition II.3. For RegLCMs, the condition (A4) is the necessary identifiability condition under the assumption that (A1)-(A3) hold. For RegLCMs without covariates, the condition (A3 * ) is the necessary identifiability condition under the assumption that (A1) and (A2 * ) hold.</p><p>Despite the necessary results, we next show that satisfying (A1), (A2 (P 1) Some latent attribute is required by only one item, (P 2) After rows permutation, the Q-matrix contains an identity matrix I K .</p><p>Then we conclude that (i) For the considered CDMs, the matrix Ψ in (A3 * ) has full column rank but the CDM parameters (η, Θ) are not identifiable;</p><p>(ii) For the considered RegCDMs, the matrix Φ in (A4) has full column rank but the RegCDM parameters (β, γ, λ) are not identifiable.</p><p>According to Proposition II.4, the Q-matrix in the following form after rows permutation satisfies (P 1) and (P 2),</p><formula xml:id="formula_36">Q =          I K 0 . . . 0 Q *          .</formula><p>In above Q-matrix, the top K ×K block is an identity matrix I K . From the (K +1)th row to the Jth row, the entries in first column are 0 (J-K)×1 , and other entries are denoted as a submatrix Q * . Under this Q-matrix, the first result (i) in Proposition II.4 is derived by extending a similar conclusion for CDMs with binary responses in <ref type="bibr" target="#b62">Gu and Xu (2020)</ref> to CDMs with polytomous responses. With an identity matrix contained in the Q-matrix, Ψ in (A3 * ) can be shown to have full column rank, or equivalently, ψ 0 , • • • , ψ C-1 are linearly independent. And further, we can show that for RegCDMs, the Φ in (A4) has full column rank, that is,</p><formula xml:id="formula_37">ϕ 0 , • • • , ϕ C-1 are linearly independent.</formula><p>With Proposition II.4, we see that RegLCMs without covariate may not be identifiable when (A1), (A2 * ) and (A3 * ) are satisfied. Specifically, consider CDMs without covariate, given (P 1)-(P 2) of Proposition II.4 are satisfied, (A3 * ) will be true since <ref type="bibr">However,</ref><ref type="bibr">Proposition II.4(i)</ref> shows that such CDMs are not identifiable. Therefore (A1), (A2 * ) and (A3 * ) are not sufficient for the identifiability of CDMs.</p><formula xml:id="formula_38">ψ 0 , • • • , ψ C-1 are linearly independent.</formula><p>Similarly, RegLCMs may not be identifiable provided that (A1)-(A4) hold. For RegCDMs, given (A1)-(A3) and (P 1)-(P 2) of Proposition II.4 are met, (A4) will be true since ϕ 0 , • • • , ϕ C-1 are linearly independent, but the RegCDMs are not identifiable according to Proposition II.4(ii). Therefore (A1)-(A4) are not sufficient for the identifiability of RegCDMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Sufficient and Practical Identifiability Conditions</head><p>As shown in Section 2.3, (A1)-(A4) are necessary but not sufficient conditions for the identifiability of RegLCMs. To address the issue, this section provides sufficient conditions to determine the identifiability of RegLCMs. In addition, we also establish the sufficient identifiability conditions for RegCDMs, which are of great interest in cognitive diagnosis.</p><p>For completeness, we first review the fundamental method to check the local identifiability before discussing the global strict and generic identifiability. In Section 2.2.3, we have introduced the results of the local identifiability conditions proposed by <ref type="bibr" target="#b60">Goodman (1974)</ref>. The conditions can be generalized to finite many items and be applicable to RegLCMs.</p><p>We first consider RegLCMs without covariates. The definitions of conditional response probabilities θ jrc follow from Section 2.2.1. For RegLCMs without covariates, we define the response probability as</p><formula xml:id="formula_39">P (R = r) = P (R 1 = r 1 , • • • , R J = r J ), r j = 0, . . . , M j -1; j = 1, . . . , J.</formula><p>which is the probability for response being r = (r 1 , • • • , r J ), and we have</p><formula xml:id="formula_40">P (R = r) = C-1 c=0 P (R = r | L = c). For c = 0, . . . , C -1, P (R = r | L = c) = η c J j=1 θ jr j c</formula><p>. The local identifiability condition proposed by Goodman is associated with the Jacobian matrix J below,</p><formula xml:id="formula_41">J = J η 1 , • • • , J η C-1 , J θ 110 , • • • , J θ 1(M 1 -1)0 , • • • , J θ J1(C-1) , • • • , J θ J(M J -1)(C-1) .</formula><p>The row dimension of J is S -1 and the column dimension is C( J j=1 M j -J)+C -1, where each row index corresponds to one response probability P (R = r) for r ∈ S ′ and each column index corresponds to one free parameter from</p><formula xml:id="formula_42">{η 1 , • • • , η C-1 , θ 110 , • • • , θ 1(M 1 -1)0 , • • • , θ J1(C-1) , • • • , θ J(M J -1)(C-1) }. For c = 1, . . . , C -1, J ηc is a vector of dimension ( J j=1 M j -1) × 1.</formula><p>Each entry is a partial derivative of the response probability P (R = r) with respect to η c at true value of η c , which is computed to be</p><formula xml:id="formula_43">∂P (R = r) ∂η c = J j=1 θ jr j c - J j=1</formula><p>θ jr j 0 .</p><p>And for j = 1, . . . , J, r = 1, . . . , M j -1 and c = 0, . . . , C -1, J θ jrc is a (S -1)dimensional vector. Each entry is a partial derivative of the response probability P (R = r) with respect to θ jrc at true value of θ jrc , which is computed to be</p><formula xml:id="formula_44">∂P (R = r) ∂θ jrc =                η c d̸ =j θ dr d c , if r j = r, -η c d̸ =j θ dr d c , if r j = 0, 0, otherwise.</formula><p>Theorem II. (A3 * * ) The Jacobian matrix J formed above has full column rank.</p><p>To better present the following local identifiability theorem for RegLCMs and RegCDMs, we consider a "hypothetical" subject with all covariates being zeros, that is, x = (1, 0 1×p ) T and z = 0 J×q . Denote the parameters of this particular subject to be η 0 and Θ 0 . The Jacobian matrix J 0 formed by the derivatives of conditional response probabilities with respect to parameters including η 0 and Θ 0 is equivalent to the Jacobian matrix J of general restricted latent class models shown in Theorem II.5. Next, we present a theorem to associate the J 0 with the local identifiability of parameters (β, γ, λ).</p><p>Theorem II.6 (Local identifiability for RegLCMs and RegCDMs). Consider Reg-LCMs or general RegCDMs. Assume (A1), (A2) and (A3) are true, (β, γ, λ) are locally identifiable if and only if the following condition holds.</p><p>(A4 ′ ) The Jacobian matrix J 0 formed from the hypothetical subject whose covariates values are zeros has full column rank Theorem II.5 and II.6 are intuitively straightforward but nontrivial to apply in practice. When the number of latent classes C and the number of item responses M j increase, the dimension of the Jacobian matrix would increase at a faster rate, making it challenging to compute the rank of the Jacobian matrix.</p><p>Moreover, the conditions introduced by Theorem II.5 and II.6 only guarantee the local identifiability, while the global identifiability is not discussed. To ensure the sufficiency for global strict identifiability, we combine Goodman's idea with the algebraic results from Kruskal Theorem to form our conditions. Recall that Φ = (ϕ c ; c = 0, . . . , C -1) defined in (2.9) is a matrix of dimension (S -1) × C. And ϕ c is a vector where each element corresponds to one response pattern and is denoted as</p><formula xml:id="formula_45">ϕ rc = P (R = r | L = c, z = 0).</formula><p>To apply Kruskal Theorem to establish the strict identifiability conditions, we consider a three-way decomposition of Φ and propose the linear independence condition regarding the decomposed matrices instead of Φ.</p><p>We divide the total of J items of Φ into three mutually exclusive item sets J 1 , J 2 and J 3 containing J 1 , J 2 and J 3 items respectively, with J 1 + J 2 + J 3 = J. For t = 1, 2 and 3, each set J t can be viewed as one polytomous variable T t taking on values in (C4 * ) After rows permutation, Q-matrix takes the form Q = (I K , I K , Q * ) T containing two identity matrices I K and one submatrix Q * (J-2K)×K . And for any different latent classes c and c ′ , there exist at least one item j &gt; 2K such</p><formula xml:id="formula_46">that (θ j0c , • • • , θ j(M j -1)c ) T ̸ = (θ j0c ′ , • • • , θ j(M j -1)c ′ ) T .</formula><p>It has been established that (C4 * ) itself is the sufficient condition for the identifiability of general restricted latent class models with binary responses Xu (2017).</p><p>In addition, <ref type="bibr" target="#b153">Xu and Shang (2018)</ref> showed that the Q-matrix is also identifiable under (C4 * ). This condition is further extended to restricted latent class models with polytomous responses in <ref type="bibr" target="#b42">Culpepper (2019)</ref>. Compared to the previous literature, the major contribution of Proposition II.8 is to extend this constraint to the polytomous RegCDMs that the Q-matrix contains two identity matrices and the conditional re-</p><formula xml:id="formula_47">sponse probability (θ j0c , • • • , θ j(M j -1)c ) T is distinct among different latent classes.</formula><p>However, in practice, Φ is rarely accessible. The theoretical results in Theorem II.7 and Proposition II.8 may need further adjustments to accommodate the empirical needs. As previously discussed, generic identifiability is commonly used in practice as it guarantees the identifiability of most parameters other than a measure-zero set of parameters <ref type="bibr" target="#b2">Allman et al. (2009)</ref>. The following theorem and proposition will provide us with easily checkable methods to determine the generic identifiability of RegLCMs and RegCDMs.</p><p>Theorem II.9 (Generic Identifiability for RegLCMs). For RegLCMs with polytomous responses, if we replace the condition (C4) in Theorem II.7 with the following condition, then (β, γ, λ) are generically identifiable.</p><p>(C4 ′ ) The matrix Φ can be decomposed into Φ 1 , Φ 2 and Φ 3 with row dimensions κ 1 , κ 2 and κ 3 respectively. And min{C,</p><formula xml:id="formula_48">κ 1 } + min{C, κ 2 } + min{C, κ 3 } ≥ 2C + 2.</formula><p>Theorem II.9 largely facilitates the process of checking the identifiability of Reg-LCMs as the variables in (C4 ′ ) are row dimensions instead of Kruskal ranks of the decomposed matrices. But Theorem II.9 does not apply to all latent class models.</p><p>For instance, the parameter space of restricted latent class models can lie in the nonidentifiable measure-zero set from the parameter space of general latent class models. Therefore, Theorem II.9 is not applicable to restricted latent class models with covariates such as RegCDMs. To address this issue, Proposition II.10 is established to determine the generic identifiability condition for RegCDMs with polytomous responses.</p><p>Proposition II.10 (Generic Identifiability for RegCDMs). For RegCDMs with polytomous responses, if we replace the condition (C4) in Theorem II.7 with the following condition, then (β, γ, λ) are generically identifiable.</p><formula xml:id="formula_49">(C4 ′′ ) After rows permutation, Q-matrix takes the form Q = (Q 1 , Q 2 , Q * ) T contain- ing one submatrix Q * (J-2K</formula><p>)×K in which each attribute is required by at least one item, and two submatrices Q 1 and Q 2 in the following form,</p><formula xml:id="formula_50">Q i =          1 * • • • * * 1 • • • * . . . . . . . . . . . . * * • • • 1          , i = 1, 2 (2.11)</formula><p>where " * " indicates the entry is either 1 or 0.</p><p>(C4 ′′ ) was first proposed by <ref type="bibr" target="#b62">Gu and Xu (2020)</ref> to determine the generic identifiability of CDMs. For RegCDMs, Proposition II.10 gives more flexible conditions than Proposition II.8 as (C4 ′′ ) puts a weaker condition on the Q-matrix compared to (C4 * ). In the Q-matrix form required by (C4 ′′ ), the two identity submatrices are replaced by two flexible matrices in the form of (2.11), which relaxes the constraint on the off-diagonal entries. Under this new condition, the parameters are no longer strictly identifiable but still identifiable in the generic sense.</p><p>Proposition II.10 is sufficient to guarantee the generic identifiability of RegCDMs.</p><p>Under certain special cases, we can also show that Proposition II.10 is necessary. Next, we introduce a particular example where Proposition II.10 is not only sufficient, but also necessary for the generic identifiability of the parameters (β, γ, λ).</p><p>Example II.11. Consider RegCDMs with binary responses and two latent attributes only, i.e. M j = 2 and K = 2. We show that Proposition II.10 is necessary and sufficient for the generic identifiability of the parameters (β, γ, λ). For instance, after rows permutation, the Q-matrix takes the following form</p><formula xml:id="formula_51">Q =             1 * * 1 1 * * 1 Q ′             , (2.12)</formula><p>where " * " is either zero or one and Q ′ is a matrix with at least one entry to be 1 in each column. Proposition 3 in <ref type="bibr" target="#b63">Gu and Xu (2021)</ref> shows that (C4 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Data Application</head><p>In this section, we use a real data set to illustrate the application of the proposed identifiability conditions in educational assessments. Trends in Mathematics and Science Study (TIMSS) is an international and large-scale assessment to evaluate the mathematics skills and science knowledge of students at different grades. We binary responses (M j = 2) to J = 25 items together with their gender information.</p><p>The genders are denoted as binary variables with g i = 1 for female student and g i = 0</p><p>for male student.</p><p>We model the TIMSS 2007 dataset using RegCDMs and analyze its identifiability.</p><p>We consider gender g i as covariates with x i = (1, g i ) T and z ij = (g i ) for i = 1, . . . , N and j = 1, . . . , J, under the assumption that both η and Θ can be associated with the student's gender. Following <ref type="bibr" target="#b115">Park and Lee (2014)</ref> We first show that the RegCDMs with K = 7 are generically identifiable by Proposition II.10. As there are C = 2 7 = 128 latent classes, condition (A1) holds as</p><formula xml:id="formula_52">J j=1 M j -1 -C( J j=1 M j -J) -C + 1 = 2 25 -2 7 × 25 -2 7 &gt; 0.</formula><p>Condition (A2) holds as the binary covariate are finite and coefficient parameters are free since no constraint are imposed on coefficients. Condition (A3) holds as the design matrices are</p><formula xml:id="formula_53">X = Z j =          1 g 1 1 g 2 . . . . . . 1 g N          =          1 0 1 0 . . . . . . 1 1         </formula><p>, for j = 1, . . . , J, which have full column rank given the sample has both female and male students.</p><p>Lastly for (C4 ′′ ), the Q-matrix after rows permutations from <ref type="bibr" target="#b115">Park and Lee (2014)</ref> is presented in Table <ref type="table">2</ref>.1. The Q-matrix implies that (C4 ′′ ) holds as the Q 1 and Q 2 have diagonal entries to be 1 and each column of sub-matrix Q * contains 1 for at least once. According to Proposition II.10, the RegCDMs are generically identifiable.</p><p>However, the Q-matrix is not complete and the RegCDMs are not strictly identifiable,</p><formula xml:id="formula_54">Item No. α 1 α 2 α 3 α 4 α 5 α 6 α 7 Q 1 1 1 0 0 0 0 0 3 1 1 0 0 0 0 5 1 0 1 0 0 0 10 0 0 0 1 1 0 9 0 0 0 0 1 0 6 0 0 0 0 1 1 12 1 0 0 0 0 0 Q 2 15 1 0 0 0 0 0 4 1 1 0 0 0 0 17 1 0 1 0 0 0 11 1 0 0 1 0 0 24 0 0 0 0 1 0 22 0 0 0 0 1 1 13 1 0 0 0 0 0 Q * 2 0 1 0 0 0 0 8 1 0 0 0 1 0 7 0 0 0 1 1 1 14 1 1 0 0 0 0 16, 23 1 0 0 0 0 0 18, 20 1 0 1 0 0 0 19, 25 1 0 0 0 0 0 21 1 0 1 0 0 0</formula><p>Table <ref type="table">2</ref>.1: The Q-matrix for TIMSS 2007 Data at K = 7.</p><p>for which (C4 * ) does not hold and Proposition II.8 cannot be applied.</p><p>We next show that the RegCDMs with K ′ = 3 are generically identifiable as well by Proposition II.10. As there are C = 2 3 = 8 latent classes, condition (A1)</p><formula xml:id="formula_55">holds as J j=1 M j -1 -C( J j=1 M j -J) -C + 1 = 2 25 -2 3 × 25 -2 3 &gt; 0.</formula><p>As the items, the students' responses and covariates are unchanged, we have (A2)-(A3) hold by the same arguments as in RegCDMs with K = 7. In assessing the three general attributes, the Q-matrix used in Park and Lee (2014) is given in Table <ref type="table">2</ref>.2 after rows permutations. This Q-matrix contains Q 1 and Q 2 with diagonal entries to be 1 and the sub-matrix Q * with each attribute column containing 1 for at least one entry. Therefore, condition (C4 ′′ ) holds and Proposition II.10 shows the RegCDMs at K ′ = 3 are generically identifiable. However, the Q-matrix does not contain identity matrix as the α ′ 3 is not solely required by any item, and the RegCDMs are not strictly identifiable. </p><formula xml:id="formula_56">Item No. α ′ 1 α ′ 2 α ′ 3 Q 1 1 1 0 0 6 0 1 0 12 1 0 1 Q 2 2 1 0 0 7 0 1 0 13 1 0 1 Q * 3-5,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Discussion</head><p>this chapter studies the latent class models with covariates, in particular, Reg-LCMs. Under RegLCMs setup and its special case RegCDMs, we focus on the identifiability conditions for the coefficient parameters of the covariates. We show that</p><p>Huang and Bandeen-Roche (2004) presented necessary but not sufficient conditions for the local identifiability of RegLCMs. Then we establish identifiability conditions for local and global identifiability of RegLCMs and RegCDMs.</p><p>The classical and fundamental method for local identifiability is based on Goodman's results, which is to ensure the full column rank of the Jacobian matrix formed by derivatives of general response probabilities with respect to parameters. We propose sufficient and practical conditions based on Huang and Bandeen-Roche (2004) to replace the previous linear independence condition on the whole marginal probability matrix with linear independence conditions concerning three decomposed probability matrices. Noticing the empirical convenience from generic identifiability, we present specific conditions to ensure the generic identifiability as well. The conditions for generic identifiability involve more accessible variables from decomposed submatrices. In addition to the global identifiability of general RegLCMs, the conditions for the global identifiability of RegCDMs are related to the form of Q-matrix, and these conditions are extended from the binary-response CDMs to the polytomous-response CDMs.</p><p>Regarding the consistency of estimation, <ref type="bibr" target="#b62">Gu and Xu (2020)</ref> proved that for the general restricted latent class models, the latent class membership probability and conditional response probability can be consistently estimated with maximum likelihood estimators. The estimation consistency preserves for the parameters in RegLCMs because the parameters are linearly related with the log-odds and the design matrices of covariates have full column ranks. The proposed conditions are sufficient and practical but not necessary in strict identifiability case. For generic identifiability, we only discover the sufficient and necessary condition for CDMs with the binary attribute, except which the necessary part of conditions are still under research. For future works, we plan to investigate the sufficient and necessary conditions for the identifiability of the latent class models with covariates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CHAPTER III</head><p>High-Dimensional Inference for Generalized Linear Models with Hidden Confounding</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Introduction</head><p>Statistical inferences for high-dimensional regression models have received a growing interest due to the increasing number of complex data sets with high-dimensional covariates that are collected across many different scientific disciplines ranging from genomics to social science to econometrics <ref type="bibr" target="#b118">(Peng et al., 2010;</ref><ref type="bibr" target="#b15">Belloni et al., 2012;</ref><ref type="bibr" target="#b49">Fan et al., 2014;</ref><ref type="bibr" target="#b22">Bühlmann et al., 2014)</ref>. One of the most popularly used highdimensional regression methods is the lasso linear regression, which assumes that the underlying regression coefficients are sparse <ref type="bibr" target="#b137">(Tibshirani , 1996)</ref>. However, the lasso penalty introduces non-negligible bias that renders high-dimensional statistical inference challenging <ref type="bibr" target="#b144">(Wainwright, 2019)</ref>. To address this challenge, Zhang and Zhang (2014) and van <ref type="bibr" target="#b141">de Geer et al. (2014)</ref> proposed the debiasing method to construct confidence intervals for the lasso regression coefficients: the main idea is to first obtain the lasso estimator, and then correct for the bias of the lasso estimator using a low-dimensional projection method. We refer the reader to Javanmard and Montanari (2014), <ref type="bibr" target="#b16">Belloni et al. (2014)</ref>, <ref type="bibr" target="#b109">Ning and Liu (2017)</ref>, <ref type="bibr" target="#b37">Chernozhukov et al. (2018)</ref>, among others, for detailed discussions of the debiasing approach, and also <ref type="bibr" target="#b144">Wainwright (2019)</ref> for an overview of high-dimensional statistical inference.</p><p>The aforementioned studies were established under the assumption that there are no unmeasured confounders that are associated with both the response and covariates.</p><p>However, this assumption is often violated in observational studies. For instance, in genetic studies, the effect of certain segments of DNA on the gene expression may be confounded by population structure and microarray expression artifacts <ref type="bibr" target="#b98">(Listgarten et al., 2010)</ref>. Another example is in healthcare studies where the effect of nutrients intake on the risk of cancer may be confounded by physical wellness, social class, and behavioral factors <ref type="bibr" target="#b53">(Fewell et al., 2007)</ref>. Without adjusting for the unmeasured confounders, the resulting inferences from the standard debiasing methods could be biased and consequently, lead to spurious scientific discoveries.</p><p>Various methods have been proposed to perform valid statistical inferences for regression parameters in the presence of hidden confounders. One commonly used approach is the instrumental variables regression, which typically requires domain knowledge to identify valid instrumental variables, making it challenging for highdimensional applications <ref type="bibr" target="#b81">(Kang et al., 2016;</ref><ref type="bibr" target="#b23">Burgess et al., 2017;</ref><ref type="bibr" target="#b64">Guo et al., 2018;</ref><ref type="bibr" target="#b150">Windmeijer et al., 2019)</ref>. Recently, under a relaxed linear structural equation modeling framework, <ref type="bibr" target="#b65">Guo et al. (2022)</ref> proposed a deconfounding approach for statistical inference for individual regression coefficients. <ref type="bibr" target="#b51">Fan et al. (2023)</ref> developed a factoradjusted debiasing method and conducted statistical estimation and inference for the coefficient vector. However, these two methods rely on the linearity assumption between the response and covariates, which may not hold when the response is not continuous but categorical <ref type="bibr">(binary, count, etc.)</ref>. Such categorical data commonly occurs in genetic, biomedical, and social science applications, and thus alternative methods are needed to perform valid statistical inferences in these cases.</p><p>To bridge the gap in the existing literature, we propose a novel framework to perform statistical inference in the context of high-dimensional generalized linear models with hidden confounding. The main idea of our proposed method involves estimating the unmeasured confounders using high-dimensional factor analysis techniques and then applying the debiasing method on the regression coefficient of interest, with the estimated unmeasured confounders treated as surrogate variables. This method does not rely on the linear model assumption or any specific model form and it is applicable to more general models beyond the generalized linear models, such as graphical models <ref type="bibr" target="#b125">(Ren et al., 2015;</ref><ref type="bibr" target="#b158">Zhu et al., 2020)</ref> and additive hazards models <ref type="bibr" target="#b94">(Lin and Ying, 1994;</ref><ref type="bibr" target="#b95">Lin and Lv , 2013)</ref>.</p><p>Theoretically, we show that under some mild scaling conditions, the estimation errors of proposed estimators achieve comparable rates as that of the ℓ 1 -penalized generalized linear model without unmeasured confounders. We further show that the debiased estimator for the coefficient of interest is asymptotically Gaussian after adjusting for the unmeasured confounders, which results in a valid statistical inference for high-dimensional generalized linear models with hidden confounding. It is worth highlighting that when using a factor model to relate covariates and unmeasured confounders, we make more general assumptions on the random noise compared to existing works. Specifically, we allow the random noise to be non-identically distributed. This represents a significant improvement over the majority of previous works, which assume that the random noise follows an independent and identical distribution <ref type="bibr" target="#b65">(Guo et al., 2022;</ref><ref type="bibr" target="#b51">Fan et al., 2023)</ref>. Furthermore, unlike existing methods under the linear framework <ref type="bibr" target="#b65">(Guo et al., 2022;</ref><ref type="bibr" target="#b51">Fan et al., 2023)</ref> Our paper is organized and structured as follows. In Section 3.2, we introduce the model setup and provide a comprehensive discussion of related literature. Section 3.3 presents our two-step approach for estimating the parameter of interest while adjusting for the effect of unmeasured confounders. Section 3.4 establishes the theoretical properties of the parameter including the estimation consistency as well as the asymptotic normality for the estimator. In Section 3.5, we demonstrate the performance of our proposed method and the validity of theoretical results via extensive simulation studies. We also provide an application to genetic data containing gene expression quantifications and stimulation statuses in mouse bone marrow-derived dendritic cells, where we identify significant gene expressions under stimulations, which are consistent with the experimental findings in genetic studies <ref type="bibr" target="#b108">(Nemetz et al., 1999;</ref><ref type="bibr" target="#b4">Ather and Poynter , 2018;</ref><ref type="bibr" target="#b77">Jang et al., 2018;</ref><ref type="bibr" target="#b139">Toyoshima et al., 2019)</ref>. Lastly, in Section 3.6, we</p><p>provide concluding remarks and outline potential future directions for this research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generalized Linear Models with Hidden Confounding</head><p>In this section, we first set up a generalized linear model with hidden confounding and introduce a scientific application of our model framework. Then we will discuss related high-dimensional models in the existing literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Problem Setup</head><p>Consider a high-dimensional regression problem with a unidimensional response y and a p-dimensional observed covariates X. In addition, assume that there is a Kdimensional unmeasured confounders U that are related to both y and X. Without loss of generality, we write X = (D, Q T ) T , where D ∈ R is the covariate of interest and Q ∈ R p-1 is a (p -1)-dimensional vector of nuisance covariates. Furthermore, we denote θ ∈ R as the univariate parameter of interest, v ∈ R p-1 as parameters for the nuisance covariates, and β ∈ R K as parameters that quantify the effects induced by the unmeasured confounders. The goal is to perform statistical inference on the parameter θ.</p><p>We assume that y given D, Q, and the unmeasured confounders U follows a generalized linear model with probability density (mass) function:</p><formula xml:id="formula_57">f (y) = exp [{y(θD + v T Q + β T U ) -b(θD + v T Q + β T U )}/a(ϕ) + c(y, ϕ)] , (3.1)</formula><p>where ϕ is the scale parameter, and a(•), b(•), and c(•) are some known functions.</p><p>As the distribution of y belongs to the exponential family, we have</p><formula xml:id="formula_58">E(y) = b ′ (θD + v T Q + β T U ) and var(y) = b ′′ (θD + v T Q + β T U )a(ϕ).</formula><p>For simplicity, we take a(ϕ) = 1. For notational convenience, let Z = (D, Q T , U T ) T be a vector that includes all observed covariates and the unmeasured confounders, and let η = (θ, v T , β T ) T be the corresponding parameters. We now provide three commonly used examples. Example III.3 (Linear Regression). Let y ∈ R be a real-valued response variable.</p><p>Given covariates D, Q, and unmeasured confounders U , the response y follows the</p><formula xml:id="formula_59">linear regression model y = θD + v T Q + β T U + ε with E(ε) = 0 and var(ε) = σ 2 . The model parameters are ϕ = σ 2 , a(σ 2 ) = σ 2 , b(t) = t 2 /2 and c(y, σ 2 ) = -y 2 /(2σ 2 ) - log(2πσ 2 )/2.</formula><p>In addition, we assume that the relationship between covariates X and unmeasured confounders U is captured by the following model:</p><formula xml:id="formula_60">X = W T U + E,<label>(3.2)</label></formula><p>where W ∈ R K×p is the loading matrix that describes the linear effect of unmeasured confounders U on covariates X, and E is the random noise independent of U . While similar model as in (3.2) is considered in <ref type="bibr" target="#b65">Guo et al. (2022)</ref> and Fan et al.</p><p>(2023), here we assume model (3.2) is an approximate factor model which allows for weak correlation and non-identical distribution of the random noise. This is a general setting compared to many existing works assuming E to be identically and independently distributed <ref type="bibr" target="#b65">(Guo et al., 2022;</ref><ref type="bibr" target="#b51">Fan et al., 2023)</ref>. We will elaborate on this model setup and expound the assumptions for weakly correlated random noise in Sections 3.3 and 3.4.</p><p>The aforementioned structural equation modeling framework can be applied to many scientific applications. In the following, we provide one motivating example in genetic studies. Various authors have found that the effect of gene expression in response to the environmental conditions, e.g., viral or bacterial stimulation, might be confounded by unmeasured factors <ref type="bibr" target="#b121">(Price et al., 2006;</ref><ref type="bibr" target="#b90">Lazar et al., 2013)</ref>. One interesting scientific problem is to assess the effect of gene expression responding to the viral stimulation to cells, while adjusting for the confounding effects from the unmeasured variables. The viral stimulation status, a binary variable, is considered as the response variable y. Therefore, a generalized linear model for modeling y is preferred. In this example, y is the viral stimulation status, X is a vector of highdimensional gene expression, and U is a vector of possible unmeasured confounders.</p><p>More details of the data application will be provided in Section 3.5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Related Models in Existing Literature</head><p>In high-dimensional regression, the adjustment for the hidden confounding effects Recently, many researchers studied the following linear hidden confounding model, which can be viewed as a special case of our framework,</p><formula xml:id="formula_61">y = θD + v T Q + β T U + ε, X = W T U + E. (3.3)</formula><p>Under this model, <ref type="bibr" target="#b82">Kneip and Sarda (2011)</ref> used the principal component method to estimate the unmeasured confounders and then applied a selection procedure on a projected model. <ref type="bibr">Ćevid et al. (2020)</ref> proposed a method that first performs the spectral transformation pre-processing step and then applies the lasso regression on the transformed response and covariates. However, these two works focused on estimation consistency and did not address inference issues. Several works have investigated statistical inference on the covariate coefficients. For example, <ref type="bibr" target="#b65">Guo et al. (2022)</ref> proposed a doubly debiased lasso method to perform statistical inference for θ. Different 3). However, these techniques are designed for linear models and may not be directly applicable to generalized linear model settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Estimation Method</head><p>In this section, we propose a novel framework to perform statistical inference for a parameter of interest in the context of high-dimensional generalized linear models with hidden confounding. In the proposed framework, we first estimate the unmeasured confounders using a factor analysis approach. Subsequently, the estimated unmeasured confounders are treated as surrogate variables for fitting a high-dimensional generalized linear model, and a debiased estimator is constructed to perform statistical inference.</p><p>Throughout this section, we assume that the observed data {y i , X i } i=1,...,n and the unmeasured confounders {U i } i=1,...,n are realizations of (3.1) and (3.2). Moreover, the random noise E i = (E i1 , . . . , E ip ) T has mean zero and variance</p><formula xml:id="formula_62">Ω i = E(E i E T i ). Let Σ e = diag(n -1 n i=1 Ω i )</formula><p>, where diag(A) denotes a diagonal matrix by setting offdiagonal entries in A to zero. In the p × p diagonal matrix Σ e , we denote the j-th diagonal element to be σ 2 j = n -1 n i=1 τ i,jj , where τ i,jj is the (j, j) element of Ω i . The model assumption on the random noise is general as it does not assume that the random noise E i is identical nor does it require the covariance matrix Ω i to be diagonal. The detailed theoretical assumptions regarding the random noise will be presented in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimation of the Unmeasured Confounders:</head><p>In this chapter, we consider the dimension K as pre-specified. In practice, there are various methods to estimate the dimension of unmeasured confounders such as scree plot <ref type="bibr" target="#b27">(Cattell , 1966)</ref>, crossvalidation method <ref type="bibr" target="#b113">(Owen and Wang, 2016)</ref>, information criteria method <ref type="bibr" target="#b9">(Bai and Ng, 2002)</ref>, the eigenvalue ratio method <ref type="bibr" target="#b87">(Lam and Yao, 2012;</ref><ref type="bibr" target="#b0">Ahn and Horenstein, 2013)</ref>, among others. In the implementation of our proposed method, we recommend the parallel analysis approach because of its good finite sample performance, easy implementation, and popularity in scientific applications <ref type="bibr" target="#b68">(Hayton et al., 2004;</ref><ref type="bibr" target="#b41">Costello and Osborne, 2005;</ref><ref type="bibr" target="#b21">Brown, 2015)</ref> -it has shown superior performances compared to many other methods in various empirical studies <ref type="bibr" target="#b159">(Zwick and Velicer , 1986;</ref><ref type="bibr" target="#b119">Peres-Neto et al., 2005)</ref>. Detailed discussions on the estimation of the dimension of unmeasured confounders are presented in Appendix C.</p><p>We employ the maximum likelihood estimation to estimate the unmeasured confounders under (3.2). Without loss of generality, let Ū = n -1 n i=1 U i = 0 and let S u = n -1 n i=1 U i U T i be the sample variance of U . Similarly, let S x = n -1 n i=1 (X i -X)(X i -X) T be the sample variance of X, where X = n -1 n i=1 X i . Given unmeasured confounders U 1 , . . . , U n , define an approximation of population variance of X to be Σ x = W T S u W + Σ e . Note that Σ x is not exactly the covariance matrix of X because we do not restrict Ω i to be diagonal and define Σ e to be diagonal by setting the off-diagonal of n -1 n i=1 Ω i to be zero. Based on the factor model in (3.2), the maximum likelihood estimators of W , S u and Σ e are obtained as follows:</p><formula xml:id="formula_63">( W , S u , Σ e ) = argmax W ,Su,Σe -(2p) -1 log |Σ x | -(2p) -1 tr(S x Σ -1 x ) .</formula><p>Computationally, we employ the Expectation-Maximization (EM) algorithm to obtain the maximum likelihood estimators as suggested in <ref type="bibr" target="#b7">Bai and Li (2012)</ref> and <ref type="bibr" target="#b8">Bai and Li (2016)</ref>, where the authors proved the EM solutions are the stationary points for the likelihood function. Specifically, in this iterated EM algorithm, we use the principal components estimator as the initial estimator. Because principal component estimators are shown to be consistent estimators under similar model assumptions as ours <ref type="bibr" target="#b48">(Fan et al., 2013;</ref><ref type="bibr" target="#b148">Wang and Fan, 2017)</ref>, using the principal component estimators in initialization instead of using random initialization helps to improve algorithm efficiency and find more refined estimation results. At the t-th iteration, denote the estimators at this step to be W (t) and Σ</p><p>(t)</p><p>e . The EM algorithm updates the estimators to be</p><formula xml:id="formula_64">{W (t+1) } T = n -1 n i=1 E(X i U T i | X, W (t) , Σ (t) e ) × n -1 n i=1 E(U i U T i | X, W (t) , Σ (t) e ) -1 Σ (t+1) e = diag(S x -W (t+1) {W (t) } T {Σ (t) x } -1 S x )</formula><p>where</p><formula xml:id="formula_65">Σ t x = {W (t) } T W (t) + Σ (t) e , n -1 n i=1 E(X i U T i | X, W (t) , Σ (t) e ) = S x {Σ (t) e } -1 {W (t) } T , n -1 n i=1 E(U i U T i | X, W (t) , Σ (t) e ) = W (t) {Σ (t) e } -1 S x {Σ (t) e } -1 {W (t) } T +I K -W (t) {Σ (t) e } -1 {W (t) } T .</formula><p>The iterative steps stop when ∥W (t+1) -W (t) ∥ F and ∥Σ</p><formula xml:id="formula_66">(t+1) e -Σ<label>(t)</label></formula><p>e ∥ F are less than certain tolerance value. Let W † and Σ † e to be the estimators at the last step, and V to be the matrix containing eigenvectors of p -1 W † (Σ † e ) -1 (W † ) T corresponding to the descending eigenvalues. The maximum likelihood estimators are W = V T W † and</p><formula xml:id="formula_67">Σ e = Σ † e .</formula><p>The estimator S u is obtained after estimating W and Σ e . As S u is not our focus in this method, we omit its derivation details in this chapter.</p><p>With W and Σ e , we then estimate U i using the generalized least squares estimator:</p><formula xml:id="formula_68">U i = ( W Σ -1 e W T ) -1 W Σ -1 e (X i -X). (3.4)</formula><p>Next, we treat the estimators U 1 , . . . , U n as surrogate variables for the underlying unmeasured confounders to fit a high-dimensional generalized linear model. We then construct a debiased estimator for the parameter of interest by generalizing the decorrelated score method proposed by <ref type="bibr" target="#b109">Ning and Liu (2017)</ref>. Other debiasing approaches such as that of van de Geer et al. (2014) could also be similarly developed.</p><p>Initial ℓ 1 -Penalized Estimator: Recall that η = (θ, v T , β T ) T . For vector r = (r 1 , . . . , r l ) T , define ∥r∥ 1 = l j=1 |r j |. To fit a high-dimensional generalized linear model, we solve the ℓ 1 -penalized optimization problem η = argmin</p><formula xml:id="formula_69">η∈R (p+K) - 1 n n i=1 {y i (θD i + v T Q i + β T U i ) -b(θD i + v T Q i + β T U i )} + λ(|θ| + ∥v∥ 1 ) ,</formula><p>where η = ( θ, v T , β T ) T and b(t) is a known function, given a specific generalized linear model. Throughout the manuscript, for notational convenience, let ζ = (v T , β T ) T be regression coefficients for the nuisance covariates and unmeasured confounders.</p><p>Accordingly, we have η = (θ, ζ T ) T , with the goal of performing statistical inference on θ. In the following, we construct a debiased estimator for θ, generalizing the approach in <ref type="bibr" target="#b109">Ning and Liu (2017)</ref> to situations involving unmeasured confounders.</p><p>A Debiased Estimator: Before we unfold the details of constructing a debiased estimator, we start with introducing some notations. Let</p><formula xml:id="formula_70">I = E{b ′′ (θD i + v T Q i + β T U i )Z i Z T</formula><p>i } be the Fisher information matrix, and let w T = I θζ I -1 ζζ , where I θζ and I ζζ are corresponding block matrices of I. In addition, let</p><formula xml:id="formula_71">I θ|ζ = E{b ′′ (θD i + v T Q i + β T U i )D i (D i -w T M i )} be the partial Fisher information matrix, where M i = (Q T i , U T i )</formula><p>T is a vector of nuisance covariates and unmeasured confounders. Finally, We estimate w by solving the following convex optimization problem:</p><formula xml:id="formula_72">let l(θ, ζ) = -n -1 n i=1 {y i (θD i + ζ T Ṁi ) -b(θD i + ζ T Ṁi )} with Ṁi = (Q T i , U T i ) T be the loss function. We define S(θ, ζ) = ∇ θ l(θ, ζ) -w T ∇ ζ l(θ,</formula><formula xml:id="formula_73">w = argmin w∈R (p+K-1) 1 2n n i=1 {w T ∇ ζζ l i ( θ, ζ)w -2w T ∇ ζθ l i ( θ, ζ)} + λ ′ ∥w∥ 1 , where l i (θ, ζ) = -y i (θD i + ζ T Ṁi ) + b(θD i + ζ T Ṁi )</formula><p>is the ith component of the loss function and λ ′ &gt; 0 is a sparsity tuning parameter for w. Equivalently, the estimator w is obtained by</p><formula xml:id="formula_74">w = argmin w∈R (p+K-1) 1 2n n i=1 b ′′ ( θD i + ζ T Ṁi )(D i -w T Ṁi ) 2 + λ ′ ∥w∥ 1 . (3.5)</formula><p>The estimator w is constructed with the intuition of finding a sparse vector such that the generalized decorrelated score function is approximately zero. This coincides with the intuition to solve for θ from the first-order approximation of the generalized decorrelated score function. Under the null hypothesis H 0 : θ * = θ 0 , we estimate the generalized decorrelated score function and the partial Fisher information matrix by</p><formula xml:id="formula_75">S(θ 0 , ζ) = - 1 n n i=1 {y i -b ′ (θ 0 D i + v T Q i + β T U i )}(D i -w T Ṁi ); I θ|ζ = 1 n n i=1 b ′′ ( θD i + v T Q i + β T U i )D i (D i -w T Ṁi ).</formula><p>The debiased estimator can then be constructed as θ = θ -( I θ|ζ ) -1 S(θ 0 , ζ). We will show in Section 3.4 that the debiased estimator θ is asymptotically normal.</p><p>Subsequently, the (1 -α) × 100% confidence interval for θ can be constructed as</p><formula xml:id="formula_76">{ θ -(n I θ|ζ ) -1/2 Φ -1 (1 -α/2), θ + (n I θ|ζ ) -1/2 Φ -1 (1 -α/2)}, (3.6)</formula><p>where Φ(t) is the cumulative distribution function for the standard normal random variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Theoretical Results</head><p>Recall that our proposed method yields estimators η, w, and the debiased estimator θ. In this section, we first establish upper bounds for the estimation errors of η and w under the ℓ 1 norm. Subsequently, we show that the debiased estimator θ is asymptotically normal.</p><p>For a vector r = (r 1 , . . . , r l ) T , let ∥r∥ q = ( l j=1 |r j | q ) 1/q for q ≥ 1 and let ∥r∥ ∞ = max j=1,...,l |r j |. For any matrix A, let λ max (A) and λ min (A) represent the largest and smallest eigenvalues of A, respectively. Moreover, for sequences {a n } and {b n }, we write a n ≲ b n if there exists a constant C &gt; 0 such that a n ≤ Cb n for all n, and</p><formula xml:id="formula_77">a n ≍ b n if a n ≲ b n and b n ≲ a n . For a sub-exponential random variable Y 1 , we write ∥Y 1 ∥ φ 1 = inf[s &gt; 0 : E{exp(Y 1 /s)} ≤ 2] as the sub-exponential norm. For a sub-Gaussian random variable Y 2 , we write ∥Y 2 ∥ φ 2 = inf[s &gt; 0 : E{exp (Y 2 2 /s 2 )} ≤ 2]</formula><p>as the sub-Gaussian norm. Throughout the manuscript, we will use an asterisk on the upper subscript to indicate the population parameters. In addition, we define</p><formula xml:id="formula_78">s η = card{(j : η * j ̸ = 0)} and s w = card{(j : w * j ̸ = 0)} as the cardinalities of η *</formula><p>and w * , respectively. All of our theoretical analysis are performed under the regime in which n, p, s η , and s w are allowed to increase, and the number of unmeasured confounders K is fixed.</p><p>We start with some conditions on the factor model in (3.2). Similar conditions were also considered in <ref type="bibr" target="#b8">Bai and Li (2016)</ref> in the context of high-dimensional approximate factor model.</p><p>Assumption III.4. For some large constant C &gt; 0,</p><formula xml:id="formula_79">(a) E(E ij ) = 0, E(E 8 ij ) ≤ C. (b) E(E ih E ij ) = τ i,hj with |τ i,hj | ≤ τ</formula><p>hj for some τ hj &gt; 0 and all i = 1, . . . , n, and</p><formula xml:id="formula_80">p h=1 τ hj ≤ C for all j = 1, . . . , p. (c) E(E ij E sj ) = ρ is,j with |ρ is,j | ≤ ρ is</formula><p>for some ρ is &gt; 0 and all j = 1, . . . , p, and</p><formula xml:id="formula_81">n -1 n i=1 n s=1 ρ is ≤ C. (d) For all j, q = 1, . . . , p, E    1 √ n n i=1 [E ij E iq -E(E ij E iq )] 4    ≤ C. (e) ∥W * j ∥ 2 ≤ C and σ 2 j are estimated within the set [C -2 , C 2 ] for all j. For positive definite matrices Γ * and Υ * , lim p→∞ p -1 W * (Σ * e ) -1 (W * ) T = Γ * and lim p→∞ p -1 p j=1 (σ * j ) -4 {(W * j ) T ⊗ (W * j ) T }(W * j ⊗ W * j ) = Υ *</formula><p>, where ⊗ is the Kronecker product.</p><p>Assumption III.4 is more general than assumptions in classical factor analysis <ref type="bibr" target="#b3">(Anderson and Amemiya, 1988;</ref><ref type="bibr" target="#b6">Bai , 2003;</ref><ref type="bibr" target="#b48">Fan et al., 2013;</ref><ref type="bibr" target="#b8">Bai and Li , 2016)</ref>. Instead of constraining all the E i to have a diagonal covariance matrix, we now only require the higher-order moment of E i to be bounded, the diagonal entries σ * j 's to be bounded, as well as the magnitudes of the correlations among entries of E i to be controlled in Assumption III.4. The conditions are mild as we only control the magnitude of correlations rather than assuming zero correlations as in classical factor analysis.</p><p>Assumption III.4(e) is a regularity condition for restricting the parameters. Overall, Assumption III.4 follows standard conditions for the approximate factor model in <ref type="bibr" target="#b8">Bai and Li (2016)</ref> and is required for the estimation consistency of the unmeasured confounders.</p><p>Comparing Assumption III.4 to the dense confounding assumption (A2) imposed on the linear framework in <ref type="bibr" target="#b65">Guo et al. (2022)</ref>, our assumption is mild as it holds for a broad regime of n and p. Specifically, the dense confounding assumption (A2) in <ref type="bibr" target="#b65">Guo et al. (2022)</ref> is related to our assumption 1(e) that lim p→∞ p</p><formula xml:id="formula_82">-1 W * (Σ * e ) -1 (W * ) T = Γ *</formula><p>where Γ * is positive definite. Our assumption follows from classical factor analysis literature and as a result, we have λ q (W * ) ≍ √ p, where λ q (W * ) is the q-th singular value of the factor loading matrix W * . With γ T = (θ, v T ) being the coefficient for all covariates and γ j being the individual coefficient of interest, the dense confounding assumption mainly requires that</p><formula xml:id="formula_83">λ q (W * -j ) ≫ max M Kpn -1 (log p) 3/4 , √ M Kp 1/4 (log p) 3/8 , Kn log p ,</formula><p>where W * -j denotes the factor loading matrix W with j-th column removed. Guo et al.</p><p>(2022) focuses on the setting p ≫ n and they point out that in the high-dimensional regime and under certain settings, the dense confounding assumption holds with high probability. Specifically, when the entries of W are i.i.d. Sub-Gaussian with zero mean and variance σ 2 w , it holds that λ q (W -j ) ≍ √ pσ w and the dense confounding assumption requires p ≫ Kn log p and min{n, p} ≫ K 3 (log p) 3/2 M 2 to make σ w diminish to zero. However, this condition is restricted to the high-dimensional regime and may not hold when p is of relatively lower order.</p><p>Moreover, without loss of generality, we assume a working identifiability condition:</p><formula xml:id="formula_84">S u = I K and p -1 W * (Σ * e ) -1 (W * )</formula><p>T is a diagonal matrix with distinct entries. Note that the aforementioned working identifiability condition is for presentation purpose only and not an assumption on the model structure. As presented in Appendix B, when such a working identifiability condition is not satisfied, our theoretical results in Theorems III.7 and III.8 are still valid. We also illustrate this via simulation in Section 3.5.1. Next, we impose some assumptions on the generalized linear model with unmeasured confounders in (3.1).</p><p>Assumption III.5. (a) The Fisher information</p><formula xml:id="formula_85">I * = E[b ′′ {(η * ) T Z i }Z i Z T i ] satisfies λ min (I * ) ≥ κ, where κ &gt; 0 is some constant. (b) For some constant M &gt; 0, ∥X i ∥ ∞ ≤ M , ∥U i ∥ ∞ ≤ M , ∥η * ∥ ∞ ≤ M and |(w * q ) T Q i | ≤ M , where w * q = (w * 2 , . . . , w * p ) T . (c) The term |y i -b ′ {(η * ) T Z i }| is sub-exponential with ∥y i -b ′ {(η * ) T Z i }∥ φ 1 ≤ M . (d) Assume that a 1 ≤ (η * ) T Z i ≤ a 2 and 0 ≤ |b ′ (t)| ≤ B with |b ′ (t 1 ) -b ′ (t)| ≤ B|(t 1 -t)b ′ (t)| and 0 ≤ b ′′ (t) ≤ B with |b ′′ (t 1 )-b ′′ (t)| ≤ B|t 1 -t|b ′′ (t) for constants a 1 , a 2 and B, where t ∈ [a 1 -ϵ, a 2 + ϵ] for ϵ &gt; 0 and sequence t 1 satisfies |t 1 -t| = o(1).</formula><p>In the absence of unmeasured confounders, similar conditions in Assumption III.5 can be implied from the conditions in Theorem 3.3 in van de Geer et al. ( <ref type="formula">2014</ref>) and Assumption E.1 in <ref type="bibr" target="#b109">Ning and Liu (2017)</ref>. When U i and X i are binary or categorical, Assumption III.5(b) holds with a constant M &gt; 0. When U i and X i are sub-exponential random vectors, Assumption III.5(b) holds with M = cn -1/2 (log p) 1/2 for some constant c &gt; 0, with probability at least 1 -p -1 . Assumption III.5(d) imposes mild regularity conditions on the function b(t), and is commonly used in analyzing highdimensional generalized linear models without unmeasured confounders. We require the function b(t) to be at least twice differentiable and b ′ (t) and b ′′ (t) to be bounded.</p><formula xml:id="formula_86">Specifically, |b ′′ (t 1 ) -b ′′ (t)| ≤ B|t 1 -t|b ′′ (t) can be implied by |b ′′′ (t)| ≤ Bb ′′ (t) when</formula><p>b ′′′ (t) exists, which is a weaker condition than the self-concordance property <ref type="bibr" target="#b5">(Bach, 2010)</ref>. This boundary assumption is important for the concentration of the Hessian matrix of the loss function. Assumption III.5(d) holds for commonly used generalized linear models. For example, for logistic regression where b(t) = log{1 + exp(t)}, Assumption III.5(d) holds with B = 1, and a 1 , a 2 extended to infinity and it can be similarly verified to hold at B = max{1, exp(a 2 + ϵ)} for Poisson regression and B = max{1/(a 2 + ϵ) 2 , 2/|a 2 + ϵ|} for exponential regression with a 2 &lt; 0. For linear model, Assumption III.5 can be relaxed as stated in Remark III.9.</p><p>The theoretical analysis on the unmeasured confounders estimator is important in establishing the theoretical guarantee for our debiased method. As the decomposition techniques commonly used in linear models may not be applicable in generalized linear model settings, it is necessary to establish more general and refined intermediate results as the foundation of our theoretical analysis (see Remark B.2 in Appendix D.1 for more details). We first present a uniform convergence result for the estimators of the unmeasured confounders.</p><p>Proposition III.6. Under Assumptions III.4-III.5, if n, p → ∞, we have max i∈{1,...,n}</p><formula xml:id="formula_87">∥ U i -U * i ∥ ∞ = O p 1 √ n + log n p .</formula><p>From Proposition III.6, the estimator of unmeasured confounders U i uniformly converges to U i at p ≫ log n, which holds naturally under the high-dimensional regime p ≫ n. Moreover, the convergence rate</p><formula xml:id="formula_88">O p (n -1/2 + p -1/2 (log n) 1/2</formula><p>) is of a similar order to the convergence rates of principal component estimators <ref type="bibr" target="#b48">(Fan et al., 2013;</ref><ref type="bibr" target="#b148">Wang and Fan, 2017)</ref>. The estimation results of η and w are dependent on the accuracy of the estimator of unmeasured confounders and we next establish upper bounds on the estimation errors for η and w.</p><p>Theorem III.7 (Estimation consistency). Suppose that Assumptions III.4-III.5 hold.</p><p>With</p><formula xml:id="formula_89">λ ≍ λ ′ ≍ n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 , if the scaling condition n, p → ∞ and (s w ∨ s η ) {n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 } = o p (1) hold, then we have ∥ η -η * ∥ 1 = O p s η log p n + log n p ; ∥ w -w * ∥ 1 = O p (s w ∨ s η ) log p n + log n p .</formula><p>The effect of unmeasured confounders enters the rate of the estimation error in our results through the term p -1/2 (log n) 1/2 . Under the high-dimensional setting with p ≫ n, the unmeasured confounders effect is dominated by n -1/2 (log p) 1/2 and as a result, the convergence rates in Theorem III.7 are of the same order as in the oracle case when the unmeasured confounders are assumed to be known <ref type="bibr" target="#b141">(van de Geer et al., 2014;</ref><ref type="bibr" target="#b109">Ning and Liu, 2017)</ref>. With the estimation consistency established, we show that the debiased estimator from the proposed estimation method is asymptotically normal, and thus valid confidence intervals can be constructed.</p><p>Theorem III.8 (Asymptotic normality). Suppose that Assumptions III.4-III.5 hold.</p><p>With</p><formula xml:id="formula_90">λ ≍ λ ′ ≍ n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 , if the scaling condition n, p → ∞ and (s w ∨ s η ) (n -1/2 log p + p -1 n 1/2 log n) = o p (1) hold, then n 1/2 (I * θ|ζ ) 1/2 ( θ -θ * ) → d N (0, 1). (3.7)</formula><p>The result in Theorem III.8 is similar to the existing inference results for highdimensional generalized linear models without unmeasured confounders (van de Geer <ref type="bibr">et al., 2014;</ref><ref type="bibr" target="#b109">Ning and Liu, 2017;</ref><ref type="bibr" target="#b101">Ma et al., 2021;</ref><ref type="bibr" target="#b130">Shi et al., 2021;</ref><ref type="bibr" target="#b24">Cai et al., 2023)</ref>.</p><p>Theorem III.8 is established under the condition that the estimation errors of η and w in Theorem III.7 are o p (1). As a consequence of the asymptotic normality result in Theorem III.8 and that I θ|ζ is consistent estimator for I * θ|ζ , the construction of the confidence interval in (3.6) is valid.</p><p>Remark III.9. For the linear model, estimation consistency and asymptotic normality results in Theorems III.7 and III.8 hold with less stringent conditions than that in Assumption III.5. Suppose Assumption III.4 and Assumption III.5(a) hold and assume that the random noise ε i is independent sub-Gaussian random variable and</p><formula xml:id="formula_91">Z i is sub-Gaussian vector such that ∥ε i ∥ φ 2 ≤ M and ∥Z ij ∥ φ 2 ≤ M for some constant M &gt; 0. By choosing λ ≍ λ ′ ≍ n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 , if n, p → ∞ and (s w ∨ s η )(n -1/2 log p + p -1 n 1/2 log n) = o p (1)</formula><p>, the estimation error bounds in Theorem III.7 hold and the debiased estimator θ is asymptotically normal with limiting distribution (3.7). Similar assumptions are commonly imposed in many existing in-ference methods for high-dimensional linear models without unmeasured confounders <ref type="bibr" target="#b156">(Zhang and Zhang, 2014;</ref><ref type="bibr" target="#b79">Javanmard and Montanari, 2014)</ref> as well as in the existence of confounders <ref type="bibr" target="#b28">( Ćevid et al., 2020;</ref><ref type="bibr" target="#b65">Guo et al., 2022)</ref>.</p><p>Remark III.10. The sparsity assumption on w is a standard assumption in highdimensional regression models without unmeasured confounders. It may be more suitable and even weaker in our proposed framework with unmeasured confounders. In high-dimensional regression models without unmeasured confounders, the sparsity assumption on w is implied by the sparse inverse population Hessian condition <ref type="bibr" target="#b141">(van de Geer et al., 2014;</ref><ref type="bibr" target="#b17">Belloni et al., 2016;</ref><ref type="bibr" target="#b109">Ning and Liu, 2017;</ref><ref type="bibr" target="#b78">Jankova and van de Geer, 2018)</ref>. Specifically, the sparse inverse population Hessian condition implies the sparsity of the coefficient parameter w in a weighted node-wise lasso regression</p><formula xml:id="formula_92">D i ∼ Q i ,</formula><p>which is similar to (3.5) but regresses the covariate of interest D i on the rest of the covariates Q i .</p><p>In our proposed setting with unmeasured confounders, as shown from (3.5), we assume that in the weighted node-wise lasso regression D i ∼ Q i + U i , the coefficient of Q i , denoted as w q , to be sparse. That is, w q is sparse, conditional on the unmeasured confounders U i . The sparsity assumption is mild and can be satisfied in many settings. For example, under the assumptions in classical factor analysis in which the covariance matrix of the random error E i is diagonal, D i is uncorrelated with the other covariates Q i , conditioned on the unmeasured confounders U i . In this case, we have w q = 0, conditioned on U i and thus the sparsity assumption holds naturally.</p><p>We also want to point out that the imposed sparsity assumption may be weaker than that of existing work on high-dimensional inference without unmeasured confounders, where the coefficients of regression D i ∼ Q i are assumed to be sparse. For instance, we allow D i to be densely correlated with Q i marginally. In other words, we only require w q , the coefficient for Q i in D i ∼ Q i + U i , to be sparse, conditional on the confounders U i while marginally the coefficients of D i ∼ Q i could be dense.</p><p>Remark III.11. For the simplicity of theoretical analysis, we assume the dimension of unmeasured confounders K to be fixed and known, which is also assumed in <ref type="bibr" target="#b148">Wang and Fan (2017)</ref>, <ref type="bibr" target="#b51">Fan et al. (2023)</ref>, <ref type="bibr" target="#b146">Wang (2022)</ref> and many other works. Nevertheless, our theoretical results hold as long as K is consistently estimated. As introduced in Section 3.3, we use parallel analysis to estimate the dimension of unmeasured confounders in practice. Theoretically, it has been shown that parallel analysis consistently selects the dimension of unmeasured confounders in factor analysis <ref type="bibr" target="#b46">(Dobriban, 2020)</ref>. Specifically, when the dimension p is relatively large compared to the sample size n, each factor loads on not too few variables, and the signal size of unmeasured confounders is not too large, parallel analysis selects the number of factors with probability tending to one <ref type="bibr" target="#b46">(Dobriban, 2020)</ref>. These conditions can be satisfied under our framework, implying that the dimension of unmeasured confounders is consistently determined. Moreover, empirically we find that the proposed method still provides satisfying inference results under some overestimation of K. Intuitively, as long as the corresponding linear combinations of the true underlying factors U in the considered models can be well approximated by those of the estimated U , the developed inference results for θ * would still hold. To further illustrate this, we perform simulation studies in Appendix C to show that the overestimation of K may not affect the asymptotic properties of the debiased estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Numerical Studies</head><p>To illustrate the performance of our proposed method, we conduct numerical experiments including simulation studies and an application of our method to a genetic data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Simulations</head><p>We consider two different models in (3.1), a linear regression model and a logistic regression model. We compare our method with two alternative approaches for performing high-dimensional inference: the oracle method where we perform the debiasing approach assuming that the true values of unmeasured confounders are known; and the naive method in which the unmeasured confounders are neglected in the estimation and we perform the debiasing approach with the observed covariates only. In addition, for the linear regression setting, we compare the proposed method with the doubly debiased lasso method proposed by <ref type="bibr" target="#b65">Guo et al. (2022)</ref>. The sparsity tuning parameters for all of the aforementioned methods are selected using 10-fold cross-validation. Our proposed method involves estimating the dimension of the unmeasured confounders, which we estimate using the parallel analysis <ref type="bibr" target="#b74">(Horn, 1965;</ref><ref type="bibr" target="#b45">Dinno, 2009)</ref>. To evaluate the performance across the different methods, we construct the 95% confidence intervals for the parameter of interest, and compute the average confidence length and the coverage probability of the true parameter over 300 independent replications. First, we generate each entry of the unmeasured confounders U 1 , . . . , U n ∈ R 3 and each entry of the random error E 1 , . . . , E n ∈ R p from a standard normal distribution.</p><p>We set</p><formula xml:id="formula_93">(W * ) T =       0.5 p/3 0 0 0 1 p/3 0 0 0 1.5 p/3       p×3 ,</formula><p>where 0.5 p/3 , 1 p/3 , and 1.5 p/3 are vectors of dimension p/3 with all entries equal 0.5, 1, and 1.5, respectively (in our simulation, p/3 is set as integer values). The 0 in the matrix denotes a vector of dimension p/3 with all entries equal 0. We then generate the covariates</p><formula xml:id="formula_94">X i = (W * ) T U i + E i with D i = X i,1 and Q i = X i,-1 .</formula><p>It can be verified that the above setting satisfies the working identifiability condition described in Section 3.4. Later in this section, we also perform additional simulation studies as to illustrate the validity of the theory when the working identifiability condition does not hold.</p><p>We consider both the linear regression model and the logistic regression model:</p><p>for linear regression, we generate the response y i according to</p><formula xml:id="formula_95">y i = θ * D i + (v * ) T Q i + (β * ) T U i +ε i</formula><p>, where the random noise ε i is generated from a standard normal distribution; for logistic regression, the response y i is generated from a Bernoulli distribution</p><formula xml:id="formula_96">with probability p i (θ * , v * , β * ) = 1/(1 + exp[-{θ * D i + (v * ) T Q i + (β * ) T U i }]).</formula><p>The regression coefficients for the unmeasured confounders, parameter of interest, and nuisance parameters are set to be β * = (1, 1, 1) T , θ * = 0, and v * = (1, 0 p-2 ) T , respectively.</p><p>Results for the cases when n = 500 and p varies from 100 to 1500 and when p = 600 and n varies from 100 to 1500 under both linear regression model and logistic regression model are presented in Fig. <ref type="figure">3</ref>.1 and Fig. <ref type="figure">3</ref>.2, respectively. From Fig. <ref type="figure">3</ref>.1, we see that the naive method suffers from undercoverage since the effects induced by the unmeasured confounders are not taken into account. Our proposed method has coverage at approximately 0.95 and is similar to that of the oracle method. The proposed method also has similar length of the confidence intervals as that of the oracle method. As a comparison, the doubly debiased lasso method suffers from undercoverage when p is relatively small (p &lt; 100 and n = 500): our finding is consistent with that of <ref type="bibr" target="#b65">Guo et al. (2022)</ref>, where they indicate that when p is relatively small to n, bias from the hidden confounding effects can be relatively large, which leads to undercoverage of their method. As p increases, we see that coverage for the doubly debiased lasso method approaches to 0.95, but its confidence interval lengths are larger than that of our proposed method, indicating our method is preferred in this case.</p><p>Under logistic regression model, our proposed method performs similarly to the oracle method in terms of both coverage and length of the confidence intervals. Results for doubly debiased lasso are not reported as it is not directly applicable to generalized linear model.</p><p>In the remaining subsection, we show that when the working identifiability condition fails, our proposed method can still perform well. More discussions about the working identifiability condition are in Appendix B. Specifically, we set W * jk ∼</p><p>Unif[0, 1] for j = 1, . . . , p and k = 1, 2, and 3. This loading matrix implies that each covariate vector X i is related to all the confounders. The identifiability condition fails in such case as p -1 W * (Σ * e ) -1 (W * ) T is not a diagonal matrix. We continue with the parameter setting at the start of this section and the data generation process is unchanged except for replacing the original loading matrix with the new loading matrix where elements are uniformly distributed.</p><p>At varying dimensions n and p, we construct the confidence intervals using each method over 300 simulations, and then compute the coverage probabilities of the 95% confidence intervals on the true parameter and the average confidence interval We observe the coverage rates of our proposed method can still achieve the desirable 0.95 level and are close to the oracle case. The naive method performs worse than the proposed method, with coverage rates much less than the 0.95 level for most cases.</p><p>For the linear regression results, although a little under coverage in some cases, the doubly debiased lasso method exhibits good performance in this setting with coverage rates approximating to the oracle case, while its average confidence interval length is larger than that of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Data Application</head><p>In this section, we apply the proposed method to a genetic data containing gene quantifications are computed on 768 genes for PAM stimulation group, 697 genes on PIC stimulation group, and 798 genes for LPS stimulation group. For each group of cells, the high-dimensional covariates are the gene expression levels, the stimulation status is a binary response variable recording whether a cell is stimulated.</p><p>We fit the data using our proposed method and the naive method in which we perform the debiasing approach without adjusting for unmeasured confounders. Applying each method, we construct 95% confidence intervals for the regression coefficient for every gene. In addition, we compute the p-value and effect size for each gene using each of two methods, respectively. Our proposed method found that there are 7 possible confounders for the PAM and PIC stimulation groups, and 9 possible confounders for the LPS stimulation group. The 95% confidence intervals constructed with our proposed method for each of the three groups are shown in Fig. <ref type="figure">3</ref>.5. We see from Fig. <ref type="figure">3</ref>.5 that the confidence intervals for some genes do not contain zero, suggesting that they are possibly associated with their respective stimulations.</p><p>For valid statistical inference, we perform a Bonferroni correction to adjust for multiple hypothesis testing. As a comparison, we perform similar procedures using the confidence intervals constructed with the naive method to identify the significant genes. Genes that are significantly associated with the stimulation after Bonferroni correction from the proposed method and naive method respectively are reported in Table <ref type="table">3</ref>.1. The small p-values and large effect sizes indicate that the corresponding genes are strongly associated with their respective stimulations. Some gene codes in Table <ref type="table">3</ref>.1 coincide with the findings in <ref type="bibr" target="#b24">Cai et al. (2023)</ref>, and their functional consequences to the stimulation are also supported in existing literature. Specifically, <ref type="bibr" target="#b24">Cai et al. (2023)</ref> suggested that there exist significant association for "IL6" with PAM stimulation, "RSAD2" with PIC stimulation, and "CXCL10" and "IL12B" with LPS stimulation. Applying our proposed method in which the effects of unmeasured confounders are considered, we identify that "IL1B", "IL6", and "SAA3" are significant genes for PAM stimulated cells, "RSAD2" is significant for PIC stimulated cells, and "IL12B" is significant for LPS stimulated cells. On the other hand, when we apply the naive method which does not adjust for unmeasured confounders, we identify significant genes "IL1B", "IL6" for PAM stimulated cells, "RSAD2" for PIC stimulated cells and "IL12B" for LPS stimulated cells. Comparing the proposed method with the naive method, our method that adjusts for unmeasured confounders identifies an additional gene "SAA3". In the genetics literature, experimental studies support the finding that "SAA3" plays important roles in immune reactions <ref type="bibr" target="#b4">(Ather and Poynter , 2018)</ref>. For instance, mice lacking the gene "SAA3" develop metabolic dysfunction along with defects in innate immune development <ref type="bibr" target="#b4">(Ather and Poynter , 2018)</ref>. This comparison suggests that our proposed method can identify significant genes/variables that are not captured by existing debiasing procedures without for adjusting unmeasured confounders.</p><p>Moreover, many genetic studies support that in addition to the "SAA3", genes discovered by our proposed method all play important roles in the immune response.</p><p>The gene "IL1B" encodes protein in the family of interleukin 1 cytokine, which is an important mediator of the inflammatory response and its induction can contribute</p><formula xml:id="formula_97">Method Stimulation Gene code p-value Effect size Proposed method PAM IL1B 1•743 ×10 -5 4•295 IL6 8•940×10 -11 6•484 SAA3 8•800×10 -6 4•445 PIC RSAD2 4•441×10 -16 8•144 LPS IL12B 5•180×10 -9 5•841 Naive method PAM IL1B 6•908×10 -7 4•963 IL6 2•745×10 -12 6•990 PIC RSAD2 &lt; 1 × 10 -16 8•604 LPS IL12B 4•187×10 -8 5•482</formula><p>Table <ref type="table">3</ref>.1: Significant gene expression associated with stimulations after Bonferroni correction estimated using the proposed method and naive method, respectively.</p><p>to inflammatory pain hypersensitivity <ref type="bibr" target="#b108">(Nemetz et al., 1999)</ref>. The genes "IL6" and "IL12B" are also known to encode cytokines that play key roles in hosting defense through stimulation and immune reactions <ref type="bibr" target="#b105">(Müller-Berghaus et al., 2004;</ref><ref type="bibr" target="#b139">Toyoshima et al., 2019)</ref>. The expression of the gene "RSAD2" is important in antiviral innate immune responses and "RSAD2" is also a powerful stimulator of adaptive immune response mediated via mature dendritic cells <ref type="bibr" target="#b77">(Jang et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Discussion</head><p>This chapter studies statistical inference problem for the high-dimensional generalized linear model under the presence of hidden confounding bias. We propose a debiasing approach to construct a consistent estimator of the individual coefficient of interest and its corresponding confidence intervals, which generalizes the existing debiasing approach to account for the effects induced by the unmeasured confounders.</p><p>Theoretical properties were also established for the proposed procedure.</p><p>Our goal of this chapter is to conduct statistical inference for individual coefficients θ. The purpose of adjusting for the unmeasured confounders in the proposed method is to use the latent information for the better inference of covariates coefficient θ, while the inference for the coefficient of unmeasured confounders β is not the focus of the current work. This problem setting has wide scientific applications. For instance, as introduced in our data application, biologists are interested in the significance of the genes under the simulations with the unmeasured confounders adjusted. Moreover, in many practices with finite samples, consistent estimation of the number of factors may be problematic; our method only requires that the estimated factors can well approximate the confounders and the interpretation of the factors is of less interest in this setting. Here to investigate how the accuracy of the estimation of the dimension of unmeasured confounders K affects the asymptotic distribution of debiased estimator, we conducted a simulation study in Appendix C and the results suggest that in practice, the overestimation of K may not affect the asymptotic normality results, which may be because an abundant amount of unmeasured confounders can still fully capture the covariate information and would not incur bias in the point and interval estimation.</p><p>Nevertheless, the inference of unmeasured confounders is also of great interest in many econometrics and psychometrics applications. For instance, <ref type="bibr" target="#b51">Fan et al. (2023)</ref> recently proposed an ANOVA-type testing procedure for testing the existence of unmeasured confounders or not. It would be interesting to develop similar inference testing procedures under the generalized linear models in applications when the inference of the unmeasured confounders of scientific interest.</p><p>Another interesting related problem is regarding the inference on group-wise covariate coefficients. We next briefly describe how to generalize our method to obtain the group-wise asymptotic distribution of max j∈G |γ j | via a bootstrap-assisted procedure for γ = (θ, v T ) ∈ R p and any subset G ⊆ {1, . . . , p}. The procedures follow simultaneous inference results from Zhang and Cheng (2017) and are shown as follows.</p><p>•</p><p>Step 1: For each γ j , we construct a debiased estimator γ j using our proposed method for j ∈ G. We generate a sequence of i.i.d. standard normal random variables and denote them as {ϖ i } i=1,...,n .</p><p>• Step 2: Then, under the null hypothesis that H 0,G : γ * j = γ 0 j for j ∈ G, let</p><formula xml:id="formula_98">W G = max j∈G | n i=1 I -1 γ j |η -j S(γ 0 j , η -j )ϖ i / √ n|,</formula><p>where S(γ 0 j , η -j ) can be similarly calculated as S(θ 0 , ζ) and I γ j |η -j can be similarly calculated as I θ|ζ in Section 3.3 by treating γ j as the coefficient of interest and η -j as the other coefficient for nuisance covariates and unmeasured confounders.</p><formula xml:id="formula_99">• Step 3: Next, we calculate the critical value C G (α) at (1 -α) significance level by C G (α) = inf[t ∈ R : P {W G ≤ t | (y i , X i ) i=1,...,n } ⩾ 1 -α].</formula><p>With the debiased estimators and the critical value, we expect a similar result as Theorem 4.1 in Zhang and Cheng (2017) that the asymptotic distribution of max j∈G</p><formula xml:id="formula_100">√ n| γ j -γ * j | satisfy sup α∈(0,1) |P {max j∈G √ n| γ j -γ * j | &gt; C G (α)} -α| = o(1)</formula><p>. While the inference on group-wise maximum coefficients is an interesting problem, it is not the focus of this chapter and we leave the related theoretical proof for future research.</p><p>Besides the aforementioned extensions, there are several related problems worth investigating in the future. For instance, as discussed in Section 3.2.2, there are many models in existing literature related to our problem, in particular, the factor-adjusted method can be extended to the situation involving hidden confounding <ref type="bibr" target="#b50">(Fan et al., 2020)</ref>. In addition, in our theoretical analysis, we assume that the dimension of unmeasured confounders, K, is fixed, which has also been imposed in the existing literature (e.g., <ref type="bibr" target="#b148">Wang and Fan, 2017;</ref><ref type="bibr" target="#b51">Fan et al., 2023)</ref>. One possible extension is to allow K to grow as n and p increase, and it involves generalizations of the theoretical results on the maximum likelihood estimation for the unmeasured confounders. It would also be interesting to generalize the factor model to a nonlinear structure and investigate the theoretical properties of the debiased estimator under generalized factor model <ref type="bibr">(Chen et al., 2020b;</ref><ref type="bibr" target="#b99">Liu et al., 2023)</ref>. Besides generalized linear models, the high-dimensional debiasing technique is also popularly studied in a variety of models such as Gaussian graphical models <ref type="bibr" target="#b125">(Ren et al., 2015;</ref><ref type="bibr" target="#b158">Zhu et al., 2020)</ref> and additive hazards models <ref type="bibr" target="#b94">(Lin and Ying, 1994;</ref><ref type="bibr" target="#b95">Lin and Lv , 2013)</ref>. For instance, consider ad-ditive hazards models, which are popularly used in survival analysis and assume the conditional hazard function at time t as λ</p><formula xml:id="formula_101">(t | D, Q, U ) = λ 0 (t) + θD + v T Q + β T U</formula><p>with D and Q covariates and U unmeasured confounders. The relationship between covariates {D, Q} and unmeasured confounders U are also modeled by a linear factor model. Our method can be used to perform inference on θ under the aforementioned model setup based on the quadratic loss function. Generalizing our proposed approach to these models to adjust for possible unmeasured confounders is also an interesting direction to investigate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CHAPTER IV</head><p>Statistical Inference for Covariate-Adjusted and Interpretable Generalized Factor Model with Application to Testing Fairness</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Introduction</head><p>Latent factors, often referred to as hidden factors, play an increasingly important role in modern statistics to analyze large-scale complex measurement data and find wide-ranging applications across various scientific fields, including educational assessments <ref type="bibr" target="#b125">(Reckase, 2009;</ref><ref type="bibr" target="#b67">Hambleton and Swaminathan, 2013)</ref>, macroeconomics forecasting <ref type="bibr" target="#b133">(Stock and Watson, 2002;</ref><ref type="bibr" target="#b88">Lam et al., 2011)</ref>, and biomedical diagnosis <ref type="bibr" target="#b26">(Carvalho et al., 2008;</ref><ref type="bibr" target="#b56">Frichot et al., 2013)</ref>  <ref type="bibr" target="#b26">(Carvalho et al., 2008;</ref><ref type="bibr" target="#b56">Frichot et al., 2013)</ref>. To uncover the latent factors and analyze large-scale complex data, various latent factor models have been developed and extensively investigated in the existing literature <ref type="bibr" target="#b6">(Bai , 2003;</ref><ref type="bibr" target="#b7">Bai and Li, 2012;</ref><ref type="bibr" target="#b48">Fan et al., 2013;</ref><ref type="bibr">Chen et al., 2023b;</ref><ref type="bibr" target="#b146">Wang, 2022)</ref>.</p><p>In addition to measuring the latent factors, the observed covariates and the covariate effects conditional on the latent factors hold significant scientific interpretations in many applications <ref type="bibr" target="#b124">(Reboussin et al., 2008;</ref><ref type="bibr" target="#b116">Park et al., 2018)</ref>. One important application is testing fairness, which receives increasing attention in the fields of education, psychology, and social sciences <ref type="bibr" target="#b25">(Candell and Drasgow , 1988;</ref><ref type="bibr" target="#b18">Belzak and Bauer , 2020;</ref><ref type="bibr">Chen et al., 2023a)</ref>. In educational assessments, testing fairness, or measurement invariance, implies that groups from diverse backgrounds have the same probability of endorsing the test items, controlling for individual proficiency levels <ref type="bibr" target="#b104">(Millsap, 2012)</ref>. Testing fairness is not only of scientific interest to psychometricians and statisticians but also attracts widespread public awareness <ref type="bibr" target="#b138">(Toch, 1984)</ref>. In the era of rapid technological advancements, international and large-scale educational assessments are becoming increasingly prevalent. One example is the Programme for International Student Assessment (PISA), which is a large-scale international assessment with substantial sample size and test length <ref type="bibr">(OECD, 2019)</ref>. PISA assesses the knowledge and skills of 15-year-old students in mathematics, reading, and science domains (OECD, 2019). In PISA 2018, over 600,000 students from 37 OECD<ref type="foot" target="#foot_0">foot_0</ref> countries and 42 partner countries/economies participated in the test (OECD, 2019). To assess fairness of the test designs in such large-scale assessments, it is important to develop modern and computationally efficient methodologies for interpreting the effects of observed covariates (e.g., gender and race) on the item responses, controlling for the latent factors.</p><p>However, the discrete nature of the item responses, the increasing sample size, and the large amount of test items in modern educational assessments pose great challenges for the estimation and inference for the covariate effects as well as for the latent factors. For instance, in educational and psychological measurements, such a testing fairness issue (measurement invariance) is typically assessed by differential item functioning (DIF) analysis of item response data that aims to detect the DIF items, where a DIF item has a response distribution that depends on not only the measured latent factors but also respondents' covariates (such as group membership).</p><p>Despite many statistical methods that have been developed for DIF analysis, existing methods often require domain knowledge to pre-specify DIF-free items, namely anchor items, which may be misspecified and lead to biased estimation and inference results <ref type="bibr" target="#b136">(Thissen, 1988;</ref><ref type="bibr" target="#b134">Tay et al., 2016)</ref>. To address this limitation, researchers developed item purification methods to iteratively select anchor items through stepwise selection models <ref type="bibr" target="#b25">(Candell and Drasgow , 1988;</ref><ref type="bibr" target="#b54">Fidalgo et al., 2000;</ref><ref type="bibr" target="#b85">Kopf et al., 2015)</ref>.</p><p>More recently, tree-based methods <ref type="bibr" target="#b140">(Tutz and Berger , 2016)</ref> Notation: For any integer N , let [N ] = {1, . . . , N }. For any set S, let #S be its cardinality. For any vector r = (r 1 , . . . , r l ) T , let ∥r∥ 0 = #({j :</p><formula xml:id="formula_102">r j ̸ = 0}),</formula><p>∥r∥ ∞ = max j=1,...,l |r j |, and ∥r∥ q = ( l j=1 |r j | q ) 1/q for q ≥ 1. We define 1 (y) x to be the y-dimensional vector with x-th entry to be 1 and all other entries to be 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Setup</head><p>Consider n independent subjects with q measured responses and p * observed covariates. For the ith subject, let Y i ∈ R q be a q-dimensional vector of responses corresponding to q measurement items and X c i ∈ R p * be a p * -dimensional vector of observed covariates. Moreover, let U i be a K-dimensional vector of latent factors representing the unobservable traits such as skills and personalities, where we assume K is specified as in many educational assessments. We assume that the q-dimensional responses Y i are conditionally independent, given X c i and U i . Specifically, we model the jth response for the ith subject, Y ij , by the following conditional distribution:</p><formula xml:id="formula_103">Y ij ∼ p ij (y | w ij ),</formula><p>where</p><formula xml:id="formula_104">w ij = β j0 + γ ⊺ j U i + β ⊺ jc X c i . (4.1)</formula><p>Here β j0 ∈ R is the intercept parameter, β jc = (β j1 , . . . , β jp * ) ⊺ ∈ R p * are the coefficient parameters for the observed covariates, and γ j = (γ j1 , . . . , γ jK ) ⊺ ∈ R K are the factor loadings. For ease of presentation, we write β j = (β j0 , β T jc ) T as an assembled vector of intercept and coefficients and define X i = (1, (X c i ) T ) T with dimension p = p * + 1, which gives</p><formula xml:id="formula_105">w ij = γ ⊺ j U i + β ⊺ j X i .</formula><p>Given w ij , the function p ij is some specified probability density (mass) function. Here, we consider a general and flexible modeling framework by allowing different types of p ij functions to model diverse response data in wide-ranging applications, such as binary item response data in educational and psychological assessments <ref type="bibr" target="#b103">(Mellenbergh, 1994;</ref><ref type="bibr" target="#b125">Reckase, 2009)</ref> and mixed types of data in educational and macroeconomic applications <ref type="bibr" target="#b126">(Rijmen et al., 2003;</ref><ref type="bibr" target="#b146">Wang, 2022)</ref>; see also Remark IV.1. A schematic diagram of the proposed model setup is presented in Figure <ref type="figure" target="#fig_17">4</ref>.1.</p><p>Our proposed covariate-adjusted generalized factor model in (4.1) is motivated by applications in testing fairness. In the context of educational assessment, the subject's responses to questions are dependent on latent factors U i such as students' abilities and skills, and are potentially affected by observed covariates X c i such as age, gender, and race, among others (Linda M. <ref type="bibr">Collins, 2009)</ref>. The intercept β j0 is often interpreted as the difficulty level of item j and is referred to as the difficulty parameter in psychometrics <ref type="bibr" target="#b67">(Hambleton and Swaminathan, 2013;</ref><ref type="bibr" target="#b125">Reckase, 2009)</ref>. The capability of item j to further differentiate individuals based on their latent abilities is captured by γ j = (γ j1 , . . . , γ jK ) ⊺ , which are also referred to as discrimination parameters <ref type="bibr" target="#b67">(Hambleton and Swaminathan, 2013;</ref><ref type="bibr" target="#b125">Reckase, 2009)</ref>. The effects of observed covariates X c i on subject's response to the jth question Y ij , conditioned on latent abilities U i , are captured by β jc = (β j1 , . . . , β jp * ) ⊺ , which are referred to as DIF effects in psychometrics <ref type="bibr" target="#b73">(Holland and Wainer , 2012)</ref>. This setting gives rise to the fairness problem of validating whether the response probabilities to the measurements differ across different genders, races, or countries of origin while holding their abilities and skills at the same level.</p><formula xml:id="formula_106">X i Y i1 U i Y i2 Y i,q-1 Y iq … … β 1 β 2 β q-1 β q … γ 1 γ 2 γ q-1 γ q … X i ∈ R p U i ∈ R K Y ij ∈ R, j ∈ [q]</formula><p>Given the observed data from n independent subjects, we are interested in studying the relationships between Y i and X c i after adjusting for the latent factors U i in (4.1). Specifically, our goal is to test the statistical hypothesis H 0 : β js = 0 versus</p><formula xml:id="formula_107">H a : β js ̸ = 0 for s ∈ [p * ]</formula><p>, where β js is the regression coefficient for the sth covariate and the jth response, after adjusting for the latent factor U i . In many applications, the latent factors and factor loadings also carry important scientific interpretations such as students' abilities and test items' characteristics. This also motivates us to perform statistical inference on the parameters β j0 , γ j , and U i .</p><p>Remark IV.1. The proposed model setup (4.1) is general and flexible as various functions p ij 's could be used to model diverse types of response data in wide-ranging applications. For instance, in educational assessments, logistic factor model <ref type="bibr" target="#b125">(Reckase, 2009)</ref> with</p><formula xml:id="formula_108">p ij (y | w ij ) = exp(w ij y)/{1 + exp(w ij )}, y ∈ {0, 1},</formula><p>and probit factor model <ref type="bibr" target="#b20">(Birnbaum, 1968)</ref> with</p><formula xml:id="formula_109">p ij (y | w ij ) = {Φ(w ij )} y {1 -Φ(w ij )} 1-y , y ∈ {0, 1},</formula><p>where Φ(•) is the cumulative density function of standard normal distribution, are widely used to model the binary responses, indicating correct or incorrect answers to the test items. Such types of models are often referred to as item response theory models <ref type="bibr" target="#b125">(Reckase, 2009)</ref>. In economics and finances, linear factor models with p ij (y |</p><formula xml:id="formula_110">w ij ) ∝ exp{-(y -w ij ) 2 /(2σ 2 )}</formula><p>, where y ∈ R and σ 2 is the variance parameter, are commonly used to model continuous responses, such as GDP, interest rate, and consumer index <ref type="bibr" target="#b6">(Bai, 2003;</ref><ref type="bibr" target="#b7">Bai and Li, 2012;</ref><ref type="bibr" target="#b132">Stock and Watson, 2016)</ref>. Moreover, depending on the the observed responses, different types of function p ij 's can be used to model the response from each item j ∈ [q]. Therefore, mixed types of data, which are common in educational measurements <ref type="bibr" target="#b126">(Rijmen et al., 2003)</ref> and macroeconomic applications <ref type="bibr" target="#b146">(Wang, 2022)</ref>, can also be analyzed by our proposed model.</p><p>Remark IV.2. In addition to testing fairness, the considered model finds wide-ranging applications in the real world. For instance, in genomics, the gene expression status may depend on unmeasured confounders or latent biological factors and also be associated with the variables of interest including medical treatment, disease status, and gender <ref type="bibr">(Wang et al., 2017;</ref><ref type="bibr" target="#b111">Ouyang et al., 2023;</ref><ref type="bibr" target="#b47">Du et al., 2023)</ref>. The covariateadjusted general factor model helps to investigate the effects of the variables of interest on gene expressions, controlling for the latent factors <ref type="bibr" target="#b47">(Du et al., 2023)</ref>. This setting is also applicable to other scenarios, such as brain imaging, where the activity of a brain region may depend on measurable spatial distance from neighboring regions and latent structures due to unmodeled factors <ref type="bibr" target="#b92">(Leek and Storey, 2008)</ref>.</p><p>To analyze large-scale measurement data, we aim to develop a computationally efficient estimation method and to provide inference theory for quantifying uncertainty in the estimation. Motivated by recent work in high-dimensional factor analysis, we treat the latent factors as fixed parameters and apply a joint maximum likelihood method for estimation <ref type="bibr" target="#b6">(Bai , 2003;</ref><ref type="bibr" target="#b48">Fan et al., 2013;</ref><ref type="bibr">Chen et al., 2020b)</ref>.</p><p>Specifically, we let the collection of the item responses from n independent subjects be Y = (Y 1 , . . . , Y n ) ⊺ n×q and the design matrix of observed covariates to be</p><formula xml:id="formula_111">X = (X 1 , . . . , X n ) ⊺ n×p .</formula><p>For model parameters, the discrimination parameters for all q items are denoted as Γ = (γ 1 , . . . , γ q ) ⊺ q×K , while the intercepts and the covariate effects for all q items are denoted as B = (β 1 , . . . , β q ) ⊺ q×p . The latent factors from all</p><formula xml:id="formula_112">n subjects are U = (U 1 , . . . , U n ) ⊺ n×K .</formula><p>Then, the joint log-likelihood function can be written as follows:</p><formula xml:id="formula_113">L(Y | Γ, U, B, X) = 1 nq n i=1 q j=1 l ij (β j0 + γ ⊺ j U i + β ⊺ jc X c i ), (4.2)</formula><p>where the function</p><formula xml:id="formula_114">l ij (w ij ) = log p ij (Y ij |w ij ) is the individual log-likelihood function with w ij = β j0 + γ ⊺ j U i + β ⊺ jc X c i .</formula><p>We aim to obtain ( Γ, U, B) from maximizing the joint likelihood function L(Y | Γ, U, B, X).</p><p>While the estimators can be computed efficiently by maximizing the joint likelihood function through an alternating maximization algorithm <ref type="bibr" target="#b40">(Collins et al., 2002;</ref><ref type="bibr" target="#b32">Chen et al., 2019)</ref>, challenges emerge for performing statistical inference on the model parameters.</p><p>• One challenge concerns the model identifiability. Without additional constraints, the covariate effects are not identifiable due to the incorporation of covariates and their potential dependence on latent factors. The latent factors and factor loadings encounter similar identifiability issues as in traditional factor analysis <ref type="bibr" target="#b7">(Bai and Li , 2012;</ref><ref type="bibr" target="#b48">Fan et al., 2013)</ref>. Ensuring that the model is statistically identifiable is the fundamental prerequisite for achieving model reliability and making valid inferences <ref type="bibr" target="#b2">(Allman et al., 2009;</ref><ref type="bibr" target="#b62">Gu and Xu, 2020)</ref>.</p><p>• Another challenge arises from the nonlinearity of our proposed model. In the existing literature, most studies focus on the statistical inference for our proposed setting in the context of linear models <ref type="bibr" target="#b7">(Bai and Li , 2012;</ref><ref type="bibr" target="#b48">Fan et al., 2013;</ref><ref type="bibr">Wang et al., 2017)</ref>. On the other hand, settings with general log-likelihood function   <ref type="figure">B,</ref><ref type="figure">X</ref>). This causes the latent factors and factor loadings to be unidentifiable.</p><formula xml:id="formula_115">l ij (w ij ),</formula><formula xml:id="formula_116">(Y | Γ, U, B, X) = n i=1 q j=1 p ij (Y ij |w ij ).</formula><formula xml:id="formula_117">P (Y | Γ, U, B, X) = P (Y | Γ, Ū,</formula><p>Remark IV.3. Intuitively, the unidentifiable B = B -ΓA can be interpreted to include both direct and indirect effects of X on response Y. We take the intercept and covariate effect on the first item ( β 1 ) as an example and illustrate it in Figure <ref type="figure" target="#fig_17">4</ref>.2.</p><p>One part of β 1 is the direct effect from X onto Y (see the orange line in the left panel), whereas another part of β 1 may be explained through the latent factors U, as the latent factors are unobserved and there are potential correlations between latent factors and observed covariates. The latter part of β 1 can be considered as the indirect effect (see the blue line in the right panel). Identifiability Conditions. As described earlier, the correlation between the design matrix of covariates X and the latent factors U * results in the identifiability issue of B * . In the psychometrics literature, the intercept β * j0 is commonly referred to as the difficulty parameter, while β * jc represents the effects of observed covariates, namely DIF effects, on the response to item j <ref type="bibr" target="#b125">(Reckase, 2009;</ref><ref type="bibr" target="#b73">Holland and Wainer , 2012)</ref>.</p><formula xml:id="formula_118">X i Y i1 U i Y i2 Y i,q-1 Y iq … … β 1 β 2 β q-1 β q … γ 1 γ 2 γ q-1 γ q … X i Y i1 U i Y i2 Y i,q-1 Y iq … … β 1 β 2 β q-1 β q … γ 1 γ 2 γ q-1 γ q …</formula><p>The different scientific interpretations motivate us to develop different identifiability conditions for β * j0 and β * jc , respectively. Specifically, we propose a centering condition on U * to ensure the identifiability of the intercept β * j0 for all items j ∈ [q]. On the other hand, to identify the covariate effects β * jc , a natural idea is to impose the covariate effects β * jc for all items j ∈ [q] to be sparse, as shown in many regularized methods and item purification methods <ref type="bibr" target="#b25">(Candell and Drasgow , 1988;</ref><ref type="bibr" target="#b54">Fidalgo et al., 2000;</ref><ref type="bibr" target="#b13">Bauer et al., 2020;</ref><ref type="bibr" target="#b18">Belzak and Bauer , 2020)</ref>. In <ref type="bibr">Chen et al. (2023a)</ref>, an interpretable identifiability condition is proposed for selecting sparse covariate effects, yet this condition is specific to uni-dimensional covariates. Motivated by Chen et al.</p><p>(2023a), we propose the following minimal ℓ 1 condition applicable to general cases where the covariates are multi-dimensional. To better present the identifiability conditions, we write A = (a 0 , a 1 , . . . , a p * ) ∈ R K×p and define</p><formula xml:id="formula_119">A c = (a 1 , . . . , a p * ) ∈ R K×p *</formula><p>as the part applied to the covariate effects.</p><formula xml:id="formula_120">Condition IV.4. (i) n i=1 U * i = 0 K . (ii) q j=1 ∥β * jc ∥ 1 &lt; q j=1 ∥β * jc -A T c γ * j ∥ 1 for any A c ̸ = 0.</formula><p>Condition IV.4(i) assumes the latent abilities U * are centered to ensure the identifiability of the intercepts β * j0 's, which is commonly assumed in the item response theory literature <ref type="bibr" target="#b125">(Reckase, 2009)</ref>. Condition IV.4(ii) is motivated by practical applications. For instance, in educational testing, practitioners need to identify and remove biased test items, correspondingly, items with non-zero covariate effects (β * js ̸ = 0). In practice, most of the designed items are unbiased, and therefore, it is reasonable to assume that the majority of items have no covariate effects, that is, the covariate effects β * jc 's are sparse <ref type="bibr" target="#b73">(Holland and Wainer , 2012;</ref><ref type="bibr">Chen et al., 2023a)</ref>. Next, we present a sufficient and necessary condition for Condition IV.4(ii) to hold.</p><p>Proposition IV.5. Condition IV.4(ii) holds if and only if for any</p><formula xml:id="formula_121">v ∈ R K \ {0 K }, q j=1 v T γ * j I(β * js = 0) &gt; q j=1 sign(β * js )v T γ * j I(β * js ̸ = 0), ∀s ∈ [p * ]. (4.3)</formula><p>Remark IV.6. Proposition IV.5 implies that Condition IV.4(ii) holds when {j : and #{j : β * js &lt; 0} are comparable, then Condition IV.4(ii) holds even when less than q/2 items correspond to β * js = 0 and more than q/2 items correspond to β * js ̸ = 0.</p><formula xml:id="formula_122">β * js ̸ = 0} is separated into {j : β * js &gt; 0}</formula><p>Though assuming a "sparse" structure, our assumption here differs from existing high-dimensional literature. In high-dimensional regression models, the covariate coefficient when regressing the dependent variable on high-dimensional covariates, is often assumed to be sparse, with the proportion of the non-zero covariate coefficients asymptotically approaching zero. In our setting, Condition IV.4(ii) allows for relatively dense settings where the proportion of items with non-zero covariate effects is some positive constant.</p><p>To perform simultaneous estimation and inference on Γ * and U * , we consider the following identifiability conditions to address the second identifiability issue.</p><formula xml:id="formula_123">Condition IV.7. (i) (U * ) T U * is diagonal. (ii) (Γ * ) ⊺ Γ * is diagonal. (iii) n -1 (U * ) T U * = q -1 (Γ * ) ⊺ Γ * .</formula><p>Condition IV.7 is a set of widely used identifiability conditions in the factor analysis literature <ref type="bibr" target="#b6">(Bai , 2003;</ref><ref type="bibr" target="#b7">Bai and Li , 2012;</ref><ref type="bibr" target="#b146">Wang, 2022)</ref>. For practical and theoretical benefits, we impose Condition IV.7 to address the identifiability issue related to G. It is worth mentioning that this condition can be replaced by other identifiability conditions. For true parameters satisfying any identifiability condition, we can always find a transformation such that the transformed parameters satisfy our proposed Condi-tions IV.4-IV.7 and the proposed estimation method and theoretical results in the subsequent sections still apply, up to such a transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Joint Maximum Likelihood Estimation</head><p>In this section, we introduce a joint-likelihood-based estimation method for the covariate effects B, the latent factors U, and factor loadings Γ simultaneously. Incorporating Conditions IV.4-IV.7 into the estimation procedure, we obtain the maximum joint-likelihood-based estimators for ϕ * = (Γ * , U * , B * ) that satisfy the proposed identifiability conditions.</p><p>With Condition IV.4, we address the identifiability issue related to the transformation matrix A. Specifically, for any parameters ϕ = (Γ, U, B), there ex- Specifically, for steps t = 0, 1, . . ., we compute</p><formula xml:id="formula_124">ists a matrix A * = (a * 0 , A * c ) with A * c = argmin Ac∈R K×p * q j=1 ∥β jc -A T c γ j ∥ 1 and a * 0 = -n -1 n i=1 (U i + A * c X c i ) such</formula><formula xml:id="formula_125">Γ (t+1) , B (t+1) = argmin ∥Γ∥max≤D,∥B∥max≤D -L(Y | Γ, U (t) , B, X); U (t+1) = argmin ∥U∥max≤D -L(Y | Γ (t+1) , U, B (t+1) , X),</formula><p>until the quantity max{∥</p><formula xml:id="formula_126">Γ (t+1) -Γ (t) ∥ F , ∥ U (t+1) -U (t) ∥ F , ∥ B (t+1) -B (t) ∥ F } is less</formula><p>than some pre-specified tolerance value for convergence. We then estimate A c by minimizing the ℓ 1 -norm</p><formula xml:id="formula_127">A c = argmin Ac∈R K×p * q j=1 ∥ β jc -A T c γ j ∥ 1 . (4.5)</formula><p>Next, we estimate a 0 = -n -1 n i=1 ( U i + A c X c i ) and let A = ( a 0 , A c ). Given the estimators A, Γ, and B, we then construct</p><formula xml:id="formula_128">B * = B -Γ A and U = U + X A T</formula><p>such that Condition IV.4 holds.</p><p>Recall that Condition IV.7 addresses the identifiability issue related to the invertible matrix G. Specifically, for any parameters (Γ, U), there exists a matrix</p><formula xml:id="formula_129">G * such that Condition IV.7 holds for U * = (U + X(A * ) T )G * and Γ * = Γ(G * ) -T .</formula><p>Let U = diag(ϱ 1 , . . . , ϱ K ) be a diagonal matrix that contains the K eigenvalues of (nq) -1 (Γ T Γ) 1/2 (U + XA T ) T (U + XA T ) (Γ T Γ) 1/2 and let V be a matrix that contains its corresponding eigenvectors. We set G * = (q -1 Γ T Γ) 1/2 VU -1/4 . To further estimate Γ * and U * , we need to obtain an estimator for the invertible matrix G * .</p><p>Given the maximum likelihood estimators obtained in (4.4) and A in (4.5), we es-</p><formula xml:id="formula_130">timate G * via G = (q -1 Γ T Γ) 1/2 V U -1/4</formula><p>where U and V are matrices that contain the eigenvalues and eigenvectors of (nq</p><formula xml:id="formula_131">) -1 ( Γ T Γ) 1/2 ( U + X A T ) T ( U + X A T ) ( Γ T Γ) 1/2 ,</formula><p>respectively. With G and A, we now obtain the following transformed estimators that satisfy Condition IV.7:</p><formula xml:id="formula_132">Γ * = Γ( G T ) -1 and U * = ( U + X A T ) G.</formula><p>To quantify the uncertainty of the proposed estimators, we will show that the proposed estimators are asymptotically normally distributed. Specifically, in Theo-rem IV.16 of Section 4.4, we establish the asymptotic normality result for β * j , which allows us to make inference on the covariate effects β * j . Moreover, as the latent factors U * i and factor loadings γ * j often have important interpretations in domain sciences, we are also interested in the inference on parameters U * i and γ * j . In Theorem IV.16, we also derive the asymptotic distributions for estimators U * i and γ * j , providing inference results for parameters U * i and γ * j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Theoretical Results</head><p>We propose a novel framework to establish the estimation consistency and asymptotic normality for the proposed joint-likelihood-based estimators ϕ * = ( Γ * , U * , B * ) in Section 4.3. To establish the theoretical results for ϕ * , we impose the following regularity assumptions.</p><p>Assumption IV.8. There exist constants M &gt; 0, κ &gt; 0 such that:</p><formula xml:id="formula_133">(i) Σ * u = lim n→∞ n -1 (U * ) T U * exists and is positive definite. For i ∈ [n], ∥U * i ∥ 2 ≤ M .</formula><p>(ii) Σ * γ = lim q→∞ q -1 (Γ * ) T Γ * exists and is positive definite. For j</p><formula xml:id="formula_134">∈ [q], ∥γ * j ∥ 2 ≤ M . (iii) Σ x = lim n→∞ n -1 n i=1 X i X T i exists and 1/κ 2 ≤ λ min (Σ x ) ≤ λ max (Σ x ) ≤ κ 2 . For i ∈ [n], max i ∥X i ∥ ∞ ≤ M . (iv) Σ * ux = lim n→∞ n -1 n i=1 U * i X T i exists and ∥Σ * ux Σ -1 x ∥ 1,∞ ≤ M . The eigenval- ues of (Σ * u -Σ * ux Σ -1 x (Σ * ux ) T )Σ * γ are distinct.</formula><p>Assumptions IV.8 is commonly used in the factor analysis literature. In particular, Assumptions IV.8(i)-(ii) correspond to Assumptions A-B in <ref type="bibr" target="#b6">Bai (2003)</ref> under linear factor models, ensuring the compactness of the parameter space on U * and Γ * . Under nonlinear factor models, such conditions on compact parameter space are also commonly assumed <ref type="bibr" target="#b146">(Wang, 2022;</ref><ref type="bibr">Chen et al., 2023b)</ref>. Assumption IV.8(iii) is standard regularity conditions for the nonlinear setting that is needed to establish the concentration of the gradient and estimation error for the model parameters when p diverges. In addition, Assumption IV.8(iv) is a crucial identification condition; similar conditions have been imposed in the existing literature such as Assumption G in <ref type="bibr" target="#b6">Bai (2003)</ref> in the context of linear factor models and Assumption 6 in <ref type="bibr" target="#b146">Wang (2022)</ref> in the context of nonlinear factor models without covariates.</p><p>Assumption IV.9. For any i ∈ [n] and j ∈ [q], assume that l ij (•) is three times differentiable, and we denote the first, second, and third order derivatives of l ij (w ij ) with respect to w ij as l ′ ij (w ij ), l ′′ ij (w ij ), and l ′′′ ij (w ij ), respectively. There exist M &gt; 0 and</p><formula xml:id="formula_135">ξ ≥ 4 such that E(|l ′ ij (w ij )| ξ ) ≤ M and |l ′ ij (w ij )| is sub-exponential with ∥l ′ ij (w ij )∥ φ 1 ≤ M . Furthermore, we assume E{l ′ ij (w * ij )} = 0. Within a compact space of w ij , we have b L ≤ -l ′′ ij (w ij ) ≤ b U and |l ′′′ ij (w ij )| ≤ b U for b U &gt; b L &gt; 0.</formula><p>Assumption IV.9 assumes smoothness on the log-likelihood function l ij (w ij ). In particular, it assumes sub-exponential distributions and finite fourth-moments of the first order derivatives l ′ ij (w ij ). For commonly used linear or nonlinear factor models, the assumption is not restrictive and can be satisfied with a large ξ. For instance, consider the logistic model with</p><formula xml:id="formula_136">l ′ ij (w ij ) = Y ij -exp(w ij )/{1 + exp(w ij )}, we have |l ′ ij (w ij )</formula><p>| ≤ 1 and ξ can be taken as ∞. The boundedness conditions for l ′′ ij (w ij ) and l ′′′ ij (w ij ) are necessary to guarantee the convexity of the joint likelihood function.</p><p>In a special case of linear factor models, l ′′ ij (w ij ) is a constant and the boundedness conditions naturally hold. For popular nonlinear models such as logistic factor models, probit factor models, and Poisson factor models, the boundedness of l ′′ ij (w ij ) and l ′′′ ij (w ij ) can also be easily verified.</p><p>Assumption IV.10. For ξ specified in Assumption 2 and a sufficiently small ϵ &gt; 0,</p><p>we assume the scaling condition p n ∧ (pq) (nq) ϵ+3/ξ → 0, (4.6)</p><p>as n, q, p → ∞.</p><p>Assumption IV.10 is needed to ensure that the derivative of the likelihood function equals zero at the maximum likelihood estimator with high probability, a key property in the theoretical analysis. In particular, we need the estimation errors of all model parameters to converge to 0 uniformly with high probability. Such uniform convergence results involve delicate analysis of the convexity of the objective function, for which technically we need Assumption IV.10. For most of the popularly used generalized factor models, ξ can be taken as any large value as discussed above, thus (nq) ϵ+3/ξ is of a smaller order of n ∧ (pq), given a small ϵ. Specifically, Assumption IV.10 implies p = o(n 1/2 ∧ q) up to a small order term, an asymptotic regime that is reasonable for many educational assessments.</p><p>Next, we impose additional assumptions that are crucial to establishing the theoretical properties of the proposed estimators. One challenge for theoretical analysis is to handle the dependence between the latent factors U * and the design matrix X. To address this challenge, we employ the following transformed U 0 that are orthogonal with X, which plays an important role in establishing the theoretical results (see the Supplementary Material for details). In particular, for i ∈</p><p>[n], we let</p><formula xml:id="formula_137">U 0 i = (G ‡ ) T (U * i -A ‡ X i ). Here G ‡ = (q -1 (Γ * ) T Γ * ) 1/2 V * (U * ) -1/4 and A ‡ = (U * ) T X(X T X) -1 , where U * = diag(ϱ * 1 , . . . , ϱ * K ) with diagonal elements being the K eigenvalues of (nq) -1 ((Γ * ) T Γ * ) 1/2 (U * ) T (I n -P x )U * ((Γ * ) T Γ * ) 1/2 with P x =</formula><p>X(X T X) -1 X T and V * containing the matrix of corresponding eigenvectors. Under this transformation for U 0 i , we further define γ 0 j = (G ‡ ) -1 γ * j and β 0 j = β * j + (A ‡ ) T γ * j for j ∈ [q], and write</p><formula xml:id="formula_138">Z 0 i = ((U 0 i ) T X T i ) T and w 0 ij = (γ 0 j ) T U 0 i + (β 0 j ) T X i . These</formula><p>transformed parameters γ 0 j 's, U 0 i 's, and β 0 j 's give the same joint likelihood value as that of the true parameters γ * j 's, U * i 's and β * j 's, which facilitate our theoretical understanding of the joint-likelihood-based estimators.</p><p>Assumption IV.11. (i) For any j ∈ [q] and a, b ∈ R K+p with ∥a∥ 2 = 1 and</p><formula xml:id="formula_139">∥b∥ 2 = 1, -n -1 n i=1 l ′′ ij (w 0 ij )a T Z 0 i (Z 0 i ) T b p → a T Ψ 0 jz b with Ψ 0 jz positive definite and n -1/2 n i=1 l ′ ij (w 0 ij )a T (Ω 0 jz ) -1/2 Z 0 i d → N (0, 1). (ii) For any i ∈ [n], -q -1 q j=1 l ′′ ij (w 0 ij )γ 0 j (γ 0 j ) T p → Ψ 0 iγ for some positive definite matrix Ψ 0 iγ and q -1/2 q j=1 l ′ ij (w 0 ij )γ 0 j d → N (0, Ω 0 iγ ).</formula><p>Assumption IV.11 is a generalization of Assumption F(3)-( <ref type="formula">4</ref>) in <ref type="bibr" target="#b6">Bai (2003)</ref> for linear models to the nonlinear setting. Specifically, we need Assumption IV.11(i)</p><p>to derive the asymptotic distributions of the estimators β * j and γ * j , and Assumption IV.11(ii) is used for establishing the asymptotic distribution of U * i . Note that these assumptions are imposed on the log-likelihood derivative functions evaluated at the true parameters w 0 ij , Z 0 i , and γ 0 j . In general, for the popular generalized factor models, such assumptions hold with mild conditions. For example, under linear models, l ′ ij (w ij ) is the random error and l ′′ ij (w ij ) is a constant. Then Ψ 0 jz and Ψ 0 iγ naturally exist and are positive definite followed by Assumption IV.8. The limiting distributions of n -1/2 n i=1 l ′ ij (w 0 ij )Z 0 i and q -1/2 q j=1 l ′ ij (w 0 ij )γ 0 j can be derived by the central limit theorem under standard regularity conditions. Under logistic and probit models, l ′ ij (w ij ) and l ′′ ij (w ij ) are both finite inside a compact parameters space and similar arguments can be applied to show the validity of Assumption IV.11.</p><p>We present the following assumption to establish the theoretical properties of the transformed matrix A as defined in (4.5). In particular, we define A 0 = (G ‡ ) T A ‡ and write A 0 = (a 0 0 , . . . , a 0 p * ) T . Note that the estimation problem in (4.5) is related to the median regression problem with measurement errors. To understand the properties of this estimator, following existing M-estimation literature <ref type="bibr">(He and</ref><ref type="bibr">Shao, 1996, 2000)</ref>, we define ψ 0 js (a) = γ 0 j sign{β 0 js + (γ 0 j ) T (a -a 0 s )} and χ s (a) = q j=1 ψ 0 js (a) for j ∈ [q] and s ∈ [p * ]. We further define a perturbed version of ψ 0 js (a), denoted as ψ js (a, δ js ), as follows. For s ∈ [p * ],</p><formula xml:id="formula_140">ψ js (a, δ js ) = γ 0 j + δ js √ n [1:K] sign β 0 js + δ js √ n K+1 -(γ 0 j + δ js √ n [1:K] ) T (a -a 0 s ) ,</formula><p>where the perturbation</p><formula xml:id="formula_141">δ js =    I K 0 0 (1 (p) s ) T    - n i=1 l ′′ ij (w 0 ij )Z 0 i (Z 0 i ) T -1 √ n n i=1 l ′ ij (w 0 ij )Z 0 i ,</formula><p>is asymptotically normally distributed by Assumption IV.11. We define χ s (a) = q j=1 Eψ js (a, δ js ).</p><p>Assumption IV.12. For χ s (a), we assume that there exists some constant c &gt; 0</p><formula xml:id="formula_142">such that min a̸ =0 |q -1 χ s (a)| &gt; c holds for all s ∈ [p * ].</formula><p>Assume there exists a s0 for each</p><formula xml:id="formula_143">s ∈ [p * ] such that χ s (a s0 ) = 0 with p √ n∥α s0 ∥ → 0. In a neighbourhood of α s0 , χ s (a)</formula><p>has a nonsingular derivative such that {q -1 ∇ a χ s (α s0 )} -1 = O(1) and q -1 |∇ a χ s (a) -∇ a χ s (α s0 )| ≤ k|a -α s0 |. We assume ι nq,p := max ∥α s0 ∥, q -1 q j=1 ψ js (a s0 ,</p><formula xml:id="formula_144">δ js ) = o (p √ n) -1 .</formula><p>Assumption IV.12 is crucial in addressing the theoretical difficulties of establishing the consistent estimation for A 0 , a challenging problem related to median regression with weakly dependent measurement errors. In Assumption IV.12, we treat the minimizer of | q j=1 ψ(a, δ js )| as an M -estimator and adopt the Bahadur representation results in <ref type="bibr" target="#b70">He and Shao (1996)</ref> for the theoretical analysis. For an ideal case where δ js are independent and normally distributed with finite variances, which corresponds to the setting in median regression with measurement errors <ref type="bibr" target="#b69">(He and Liang, 2000)</ref>, these assumptions can be easily verified. Assumption IV.12 discusses beyond such an ideal case and covers general settings. In addition to independent and Gaussian measurement errors, this condition also accommodates the case when δ js are asymptotically normal and weakly dependent with finite variances, as implied by Assumption IV.11 and the conditional independence of Y ij .</p><p>We want to emphasize that Assumption IV.12 allows for both sparse and dense settings of the covariate effects. Consider an example of K = p = 1 and γ j = 1 for j ∈ [q]. Suppose β * js is zero for all j ∈ [q 1 ] and nonzero otherwise. Then this condition is satisfied as long as #{j : β * js &gt; 0} and #{j : β * js &lt; 0} are comparable, even when the sparsity level q 1 is small.</p><p>We now present our main theoretical results in the following theorem.</p><p>Theorem IV.13 (Average Consistency). Suppose the true parameters ϕ * = (Γ * , U * , B * ) satisfy Conditions IV.4-IV.7. Under Assumptions IV.8-IV.12, we have</p><formula xml:id="formula_145">q -1 ∥ B * -B * ∥ 2 F = O p p 2 log qp n + p log n q ; (4.7)</formula><p>if we further assume that p 3/2 (nq) ϵ+3/ξ (p 1/2 n -1/2 + q -1/2 ) = o(1), then we have</p><formula xml:id="formula_146">n -1 ∥ U * -U * ∥ 2 F = O p p log qp n + log n q ; (4.8) q -1 ∥ Γ * -Γ * ∥ 2 F = O p p log qp n + log n q .</formula><p>(4.9)</p><p>Theorem IV.13 presents the average convergence rates of ϕ * . Consider an oracle case with U * and Γ * known, the estimation of B * reduces to an M -estimation problem. For M -estimators under general parametric models, it can be shown that the optimal convergence rates in squared ℓ 2 -norm is O p (p/n) under the scaling condition p(log p) 3 /n → 0 <ref type="bibr" target="#b71">(He and Shao, 2000)</ref>. In terms of our average convergence rate on B * , the first term in (4.7), n -1 p 2 log(qp), approximately matches the convergence rate O p (p/n) up to a relatively small order term of p log(qp). The second term in (4.7), q -1 p log n, is mainly due to the estimation error for the latent factor U * . In educational applications, it is common to assume the number of subjects n is much larger than the number of items q. Under such a practical setting with n ≫ q and p relatively small, the term q -1 log n in (4.8) dominates in the derived convergence rate of U * , which matches with the optimal convergence rate O p (q -1 ) for factor models without covariates <ref type="bibr" target="#b7">(Bai and Li , 2012;</ref><ref type="bibr" target="#b146">Wang, 2022)</ref> up to a small order term.</p><p>Remark IV.14. The additional scaling condition p 3/2 (nq) ϵ+3/ξ (p 1/2 n -1/2 + q -1/2 ) = o(1) in Theorem IV.13 is used to handle the challenges related to the invertible matrix G that affects the theoretical properties of U * and Γ * . It is needed for establishing the estimation consistency of U * and Γ * but not for that of B * . With sufficiently large ξ and small ϵ, this assumption is approximately p = o(n 1/4 ∧ q 1/3 ) up to a small order term.</p><p>Remark IV.15. One challenge in establishing the estimation consistency for ϕ * arises from the unrestricted dependence structure between U * and X. If we consider the ideal case where the columns of U * and X are orthogonal, i.e., (U * ) T X = 0 K×p , then we can achieve comparable or superior convergence rates with less stringent assumptions. Specifically, with Assumptions IV.8-IV.10 only, we can obtain the same convergence rates for U * and Γ * as in (4.8) and (4.9), respectively. Moreover, with</p><p>Assumptions IV.8-IV.10, the average convergence rate for the consistent estimator of B * is O p (n -1 p log qp + q -1 log n), which is tighter than (4.7) by a factor of p.</p><p>With estimation consistency results established, we next derive the asymptotic normal distributions for the estimators, which enable us to perform statistical inference on the true parameters.</p><p>Theorem IV.16 (Asymptotic Normality). Suppose the true parameters ϕ * = (Γ * , U * , B * ) satisfy Conditions IV.4-IV.7. Write Ā0 and Ḡ ‡ as the probability limit of A 0 and G ‡ as n, q, p → ∞, respectively. Under Assumptions IV.8-IV.12, we have the asymptotic distributions as follows. If p 3/2 √ n(nq) 3/ξ (n -1 p log qp + q -1 log n) → 0, for any j ∈ [q] and a ∈ R p with ∥a∥ 2 = 1,</p><formula xml:id="formula_147">√ na T (Σ * β,j ) -1/2 ( β * j -β * j ) d → N (0, 1), (4.10) where Σ * β,j = (-( Ā0 ) T , I p )(Ψ 0 jz ) -1 Ω 0 jz (Ψ 0 jz ) -1 (-( Ā0 ) T , I p ) T</formula><p>, and for any j</p><formula xml:id="formula_148">∈ [q], √ n(Σ * γ,j ) -1/2 ( γ * j -γ * j ) d → N (0, I K ), (4.11)</formula><p>where</p><formula xml:id="formula_149">Σ * γ,j = Ḡ ‡ (I K , 0)(Ψ 0 jz ) -1 Ω 0 jz (Ψ 0 jz ) -1 (I K , 0) T ( Ḡ ‡ ) T . Furthermore, for any i ∈ [n], if q = O(n) and p 3/2 √ q(nq) 3/ξ (n -1 p log qp + q -1 log n) → 0, we have √ q(Σ * u,i ) -1/2 ( U * i -U * i ) d → N (0, I K ), (4.12) where Σ * u,i = ( Ḡ ‡ ) -T (Ψ 0 iγ ) -1 Ω 0 iγ (Ψ 0 iγ ) -1 ( Ḡ ‡ ) -1 .</formula><p>The asymptotic covariance matrices in Theorem IV.16 can be consistently estimated. Due to space limitations, we defer the construction of the estimators Σ * β,j , Σ * γ,j , and Σ * u,i to the Supplementary Material.</p><p>Corollary IV.17. Suppose the true parameters ϕ * = (Γ * , U * , B * ) satisfy Conditions IV.4-IV.7. Under Assumptions IV.8-IV.12, the asymptotic normality results Theorem IV.16 provides the asymptotic distributions for all estimators β * j 's, γ * j 's, and U * i 's. In particular, with the asymptotic distributions for the covariate effects and the estimators for asymptotic covariance matrices, we can perform hypothesis testing on any sub-vector of β * j , such as testing on a single entry or the entire vector.</p><p>These testing problems find wide practical applications in educational assessments and psychological measurements, where the practitioners are often interested in investigating whether the test items are biased for particular sets of covariates or even across all covariates. Specifically, as introduced in Section 4.2, we aim to test the covariate effect from the sth covariate to the jth response, i.e., perform hypothesis testing on single entry β * js for j ∈ [q] and s ∈ [p * ]. We reject the null hypothesis</p><formula xml:id="formula_150">β * js = 0 at significance level α if | √ n( σ * β,js ) -1 β * js | &gt; Φ -1 (1 -α/2)</formula><p>, where ( σ * β,js ) 2 is the (s + 1)-th diagonal entry in Σ * β,j . Empirically, with these inference results, we conduct simulation studies in Section 4.5 and real data analysis with PISA 2018 data in Section 4.6.</p><p>For the asymptotic normality of β * j , the condition p 3/2 √ n(nq) 3/ξ (n -1 p log qp + q -1 log n) → 0 together with Assumption IV.10 gives p = o{n 1/5 ∧ (q 2 /n) 1/3 } up to a small order term, and further implies n ≪ q 2 , which is consistent with established conditions in the existing factor analysis literature <ref type="bibr" target="#b7">(Bai and Li , 2012;</ref><ref type="bibr" target="#b146">Wang, 2022)</ref>.</p><p>For the asymptotic normality of U * i , the additional condition that q = O(n) is a reasonable assumption in educational applications where the number of items q is much smaller than the number of subjects n. In this case, the scaling conditions imply p = o{q 1/3 ∧ (n 2 /q) 1/5 } up to a small order term. Similarly for the asymptotic normality of γ * j , the proposed conditions give p = o{n 1/5 ∧ (q 2 /n) 1/3 } up to a small order term.</p><p>Remark IV.18. Similar to the discussion in Remark IV.15, the challenges arising from the unrestricted dependence between U * and X also affect the derivation of the asymptotic distributions for the proposed estimators. If we consider the ideal case with (U * ) T X = 0 K×p , we can establish the asymptotic normality for all individual estimators under Assumptions IV.8-IV.11 only and weaker scaling conditions. Specifically, when (U * ) T X = 0 K×p , the scaling condition becomes p √ n(nq) 3/ξ (n -1 p log qp + q -1 log n) → 0 for deriving asymptotic normality of β * j and γ * j , which is milder than that for (4.10) and (4.11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Simulation Study</head><p>In this section, we study the finite-sample performance of the proposed jointlikelihood-based estimator. We focus on the logistic latent factor model in (4.1) with</p><formula xml:id="formula_151">p ij (y | w ij ) = exp(w ij y)/{1 + exp(w ij )}, where w ij = (γ * j ) T U * i + (β * j ) T X i .</formula><p>The logistic latent factor model is commonly used in the context of educational assessment and is also referred to as the item response theory model <ref type="bibr" target="#b103">(Mellenbergh, 1994;</ref><ref type="bibr" target="#b67">Hambleton and Swaminathan, 2013)</ref>. We apply the proposed method to estimate B * and perform statistical inference on testing the null hypothesis β * js = 0.</p><p>We start with presenting the data generating process. We set the number of subjects n = {300, 500, 1000, 1500, 2000}, the number of items q = {100, 300, 500}, the covariate dimension p * = {5, 10, 30}, and the factor dimension K = 2, respectively. We jointly generate X c i and U * i from N (0, Σ) where Σ ij = τ |i-j| with τ ∈ {0, 0.2, 0.5, 0.7}. In addition, we set the loading matrix</p><formula xml:id="formula_152">Γ * [,k] = 1 (K) k ⊗ v k , where</formula><p>⊗ is the Kronecker product and v k is a (q/K)-dimensional vector with each entry generated independently and identically from Unif ). We refer the reader to the Supplementary Material for additional numerical results for p * = 10. Moreover, we observe similar results when we increase the test length q from q = 100 (top row) to q = 500 (bottom row) in Figures 4.3-4.6. In terms of the correlation between X and U * , we observe that while the power converges to one as we increase the sample size, the power decreases as the correlation τ increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Data Application</head><p>We </p><formula xml:id="formula_153">L obs (Y | Γ, U, B, X) = n i=1 j∈Q i l ij (γ ⊺ j U i + β ⊺ j X i )</formula><p>, where Q i defines the set of questions to which the responses from student i are observed. In this study, we include gender and 8 variables for school strata as covariates (p * = 9). These variables record whether the school is public, in an urban place, etc. After data preprocessing, we have n = 6063 students and q = 194 questions. Following the existing literature <ref type="bibr" target="#b125">(Reckase, 2009;</ref><ref type="bibr" target="#b104">Millsap, 2012)</ref>, we take K = 3 to interpret the three latent abilities measured by the math, reading, and science questions.</p><p>We apply the proposed method to estimate the effects of gender and school strata variables on students' responses. We obtain the estimators of the gender effect for each PISA question and construct the corresponding 95% confidence intervals. The constructed 95% confidence intervals for the gender coefficients are presented in Fig-ure 4.7. There are 10 questions highlighted in red as their estimated gender effect is statistically significant after the Bonferroni correction. Among the reading items, there is only one significant item and the corresponding confidence interval is below zero, indicating that this question is biased towards female test-takers, conditioning on the students' latent abilities. Most of the confidence intervals corresponding to the biased items in the math and science sections are above zero, indicating that these questions are biased towards male test-takers. In social science research, it is documented that female students typically score better than male students during reading tests, while male students often outperform female students during math and science tests <ref type="bibr" target="#b123">(Quinn and Cooc, 2015;</ref><ref type="bibr" target="#b12">Balart and Oosterveen, 2019)</ref>  To further illustrate the estimation results, Table <ref type="table">4</ref>.1 lists the p-values for testing the gender effect for each of the identified 10 significant questions, along with the proportions of female and male test-takers who answered each question correctly.</p><p>We can see that the signs of the estimated gender effect by our proposed method align with the disparities in the reported proportions between females and males. For example, the estimated gender effect corresponding to the item "CM496Q01S Cash</p><p>Withdrawal" is positive with a p-value of 2.77 × 10 -7 , implying that this question is statistically significantly biased towards male test-takers. This is consistent with the observation that in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Discussion</head><p>In this work, we study the covariate-adjusted generalized factor model that has wide interdisciplinary applications such as educational assessments and psychological measurements. In particular, new identifiability issues arise due to the incorporation of covariates in the model setup. To address the issues and identify the model parameters, we propose novel and interpretable conditions that are crucial for developing the estimation approach and inference results. With model identifiability guaranteed, we propose a computationally efficient joint-likelihood-based estimation method for model parameters. Theoretically, we obtain the estimation consistency and asymptotic normality for not only the covariate effects but also latent factors and factor loadings.</p><p>There are several future directions motivated by the proposed method. In this manuscript, we focus on the case in which p grows at a slower rate than the number of subjects n and the number of items q, a common setting in educational assessments.</p><p>It is interesting to further develop estimation and inference results under the highdimensional setting in which p is larger than n and q. Moreover, in this manuscript, we assume that the dimension of the latent factors K is fixed and known. One possible generalization is to allow K to grow with n and q. Intuitively, an increasing latent dimension K makes the identifiability and inference issues more challenging due to the increasing degree of freedom of the transformation matrix. With the theoretical results in this work, another interesting related problem is to further develop simultaneous inference on group-wise covariate coefficients over all q items, which we leave for future investigation. Finally, the model setup considered in this manuscript corresponds to the uniform DIF setting in psychometrics, where the effects of observed covariates on item responses are invariant across the latent factors <ref type="bibr" target="#b73">(Holland and Wainer , 2012)</ref>. Besides the uniform DIF setting, the non-uniform DIF setting, where the covariate effects may vary across the latent factors, also enjoys wide applica-</p><p>The proof of Lemma A.1 is presented in Section B.</p><p>Proof of Proposition II.3. We first prove the second part of Proposition II.3 that (A3 * ) is necessary for the identifiability of RegLCMs without covariates under (A1)-</p><formula xml:id="formula_154">(A2 * ). It is equivalent to show that if ψ 0 , • • • , ψ C-1 are not linearly independent,</formula><p>(η, Θ) are not identifiable. We prove it by the method of contradiction and assume the contrary that η are identifiable. Recall the definitions in Section 2.2, η = (η 0 , • • • , η C-1 ) T denotes the latent class membership probability, where η c = P (L = c)</p><formula xml:id="formula_155">for c = 0, • • • , C -1. And Ψ = (ψ 0 , • • • , ψ C-1</formula><p>) denotes the marginal probability matrix, where each entry ψ rc in ψ c corresponding to a response pattern r ∈ S ′ is</p><formula xml:id="formula_156">ψ rc = P (R = r | L = c) = J j=1 θ jr j c , c = 0, • • • , C -1.</formula><p>Based on the above definitions, we write the response probability vector as</p><formula xml:id="formula_157">(P (R = r) : r ∈ S ′ ) T = Ψ • η. (A.1)</formula><p>As we assume η are identifiable, there exist no η ′ ̸ = η such that</p><formula xml:id="formula_158">P (R = r | Ψ, η) = P (R = r | Ψ, η ′ ). According to (A.1), P (R = r | Ψ, η) = P (R = r | Ψ, η ′ ) implies Ψ • η = Ψ • η ′ .</formula><p>However, under the condition that ψ 0 , • • • , ψ C-1 are not linearly independent, there could exist η ′ ̸ = η such that Ψ • (η -η ′ ) = 0, and by the contradiction, (η, Θ) are not identifiable.</p><p>Next, we prove the first part of Proposition II.3, the necessity of (A4) for the identifiability of RegLCMs under (A1)-(A3). That is, if ϕ 0 , • • • , ϕ C-1 are not linearly independent, then (β, γ, λ) are not identifiable. This proof includes three steps.</p><p>Step 1 : we prove if ϕ 0 , • • • , ϕ C-1 are not linearly independent, then</p><formula xml:id="formula_159">ψ i 0 , • • • , ψ i C-1</formula><p>are not linearly independent for i = 1, . . . , N , where each ψ i c is an (S -1)-dimensional vector in which each element corresponds to a response pattern r = (r 1 , • • • , r J ) ∈ S ′ and is defined as</p><formula xml:id="formula_160">ψ i rc = P (R i = r | L i = c, x i , z i ).</formula><p>Equivalently, we need to prove if there exists subject i such that ψ i 0 , • • • , ψ i C-1 are linearly independent, then ϕ 0 , • • • , ϕ C-1 are linearly independent. We use similar techniques as in the Proof of Proposition 2 in Huang and Bandeen-Roche (2004). First, we associate the linear combinations of ϕ c 's with ψ c 's as follows. For any linear combination of ϕ c 's with coefficients a c 's, there exist b c 's and Y i such that the following equation holds,</p><formula xml:id="formula_161">C-1 c=0 a c ϕ c = C-1 c=0 b c ψ i c ⊙ Y i , (A.2)</formula><p>where ⊙ denotes the element-wise multiplication and</p><formula xml:id="formula_162">Y i = J j=1 1 exp(λ 1jr j z ij1 +•••+λ qjr j z ijq ) : r = (r 1 , . . . , r J ) ∈ S ′ T S×1 , b c = a c J j=1 1 + M j -1 s=1 exp(γ jsc + λ 1js z ij1 + • • • + λ qjs z ijq ) 1 + M j -1 s=1 e γ jsc . (A.3)</formula><p>Therefore to show ϕ c 's are linearly independent, we need to show that C-1 c=0 a c ϕ c = 0 implies a</p><formula xml:id="formula_163">0 = • • • = a C-1 = 0. Based on (A.2), we have C-1 c=0 a c ϕ c = 0 implies C-1 c=0 b c ψ i c = 0. Under the condition that ψ i 0 , . . . , ψ i C-1 are linearly independent, the equation C-1 c=0 b c ψ i c = b 0 ψ i 0 + • • • + b C-1 ψ i C-1 = 0 (A.4) implies b 0 = • • • = b C-1 = 0.</formula><p>And by (A.3), we have a 0 = • • • = a C-1 = 0. Hence, ϕ 0 , . . . , ϕ C-1 are linearly independent when ψ i 0 , . . . , ψ i C-1 are linearly independent and we complete the proof for Step 1.</p><p>Step 2 : We next introduce parameters ϵ i c 's and ω i jrc 's and show that they are not identifiable when ψ i 0 , • • • , ψ i C-1 are not linearly independent. By the similar arguments in proving the necessity of (A3 * ), we have (η i , Θ i ) are not identifiable when ψ i 0 , • • • , ψ i C-1 are not linearly independent for any subject i = 1, . . . , N . Recall in RegLCMs, (η i , Θ i ) are functionally dependent on the linear functions x T i β and γ jc + z T ij λ j , respectively. We follow the definitions of (η i , Θ i ) and (β, γ, λ) from (2.3) and (2.4) in Section 2.2.1 and let</p><formula xml:id="formula_164">ϵ i c = x T i β c = β 0c + β 1c x i1 + • • • + β pc x ip .</formula><p>for i = 1, . . . , N , c = 0, . . . , C -1. And</p><formula xml:id="formula_165">ω i jrc = γ jrc + z T ij λ jr = γ jrc + λ 1jr z ij1 + • • • + λ qjr z ijq .</formula><p>for i = 1, . . . , N , j = 1, . . . , J, r = 0, • • • , M j -1 and c = 0, . . . , C -1. Then according to Lemma A.1, ϵ i c 's and ω i jrc 's are not identifiable when (η i , Θ i ) are not identifiable. Hence, ϵ i c 's and ω i jrc 's are not identifiable when</p><formula xml:id="formula_166">ψ i 0 , • • • , ψ i C-1 are not</formula><p>linearly independent and we complete the proof for Step 2.</p><p>Step 3 : Lastly, we prove that (β, γ, λ) are not identifiable when ϵ i c 's and ω i jrc 's are not identifiable by the method of contradiction. Assume to the contrary that β are identifiable given ϵ i c 's and ω i jrc 's are not identifiable. By the definition of identifiability,</p><formula xml:id="formula_167">P (R | β * , γ, λ) = P (R | β ′ , γ, λ) implies that β * = β ′ .</formula><p>Because X has full column rank, according to the system of linear equations</p><formula xml:id="formula_168">ϵ =       ϵ 1 . . . ϵ N       =       1 x 11 • • • x 1p . . . . . . . . . . . . 1 x N 1 • • • x N p             β 00 • • • β 0(C-1) . . . . . . . . . β p0 • • • β p(C-1)       = Xβ, we have ϵ * = Xβ * equivalent to ϵ ′ = Xβ ′ . So for all subject i, P (R i | ϵ i * , γ, λ) = P (R i | ϵ i′ , γ, λ</formula><p>) would force ϵ i * = ϵ i′ , which contradicts the non-identifiability of ϵ i c 's.</p><p>Therefore β is not identifiable. Using similar techniques, we can prove γ, λ are not identifiable.</p><p>Combining the Step 1-3, we prove the first part of Proposition II.3, and thus complete the proof of Proposition II.3.</p><p>Proof of Proposition II.4. Theorem 4.4 (a) in <ref type="bibr" target="#b62">Gu and Xu (2020)</ref> showed that binaryresponse CDMs are not generically identifiable if some attribute is required by only one item. We adapt their proof of Theorem 4.4 (a) to establish that for polytomousresponse CDMs or RegCDMs, the parameters are not generically identifiable under (P 1) that some attribute is required by only one item. Consider polytomous-response CDMs first and let the Q-matrix to be</p><formula xml:id="formula_169">Q =    1 u 0 Q *    .</formula><p>This Q-matrix implies that α 1 is required by the first item only. For any (η, Θ),</p><formula xml:id="formula_170">we can construct ( η, Θ) ̸ = (η, Θ) such that P (R = r | η, Θ) = P (R = r | η, Θ),</formula><p>and hence we show that (η, Θ) are not identifiable. To better illustrate the idea, we next use α to replace c in all parameter subscripts, i.e. η α = η c and θ jrα = θ jrc given α T v = c. When j ̸ = 1, we let η c = ηc , θ jrc = θjrc for r = 0, . . . , M j -1 and c = 0, . . . , C -1. When j = 1, we denote α ′ = (α 2 , • • • , α K ) ∈ {0, 1} K-1 and for all r 1 = 0, . . . , M 1 -1, we let θ1r 1 (0,α ′ ) = θ 1r 1 (0,α ′ ) , and</p><formula xml:id="formula_171">θ1r 1 (1,α ′ ) = 1 E θ 1r 1 (1,α ′ ) + (1 - 1 E )θ 1r 1 (0,α ′ ) ,</formula><p>where E is a constant in a small neighborhood of 1 and E ̸ = 1. So we have θ1r</p><formula xml:id="formula_172">1 (1,α ′ ) ̸ = θ 1r 1 (1,α ′ ) . We also let η(0,α ′ ) = η (0,α ′ ) + (1 -E) • η (1,α ′ ) , η(1,α ′ ) = E • η (1,α ′ ) .</formula><p>Hence, we have</p><formula xml:id="formula_173">η(1,α ′ ) + η(0,α ′ ) = η (1,α ′ ) + η (0,α ′ ) , (A.5) θ1r 1 (1,α ′ ) η(1,α ′ ) + θ1r 1 (0,α ′ ) η(0,α ′ ) = θ 1r 1 (1,α ′ ) η (1,α ′ ) + θ 1r 1 (0,α ′ ) η (0,α ′ ) . (A.6) So for any r = (r 1 , • • • , r J ) ∈ S ′ , P (R = r | Ψ, η) = Ψ • η = α ′ ∈{0,1} K-1 α 1 ∈{0,1} j&gt;1 θI{R j =r j } jr j (α 1 ,α ′ ) η(α 1 ,α ′ ) [ θI{R 1 =r 1 } 1r 1 (1,α ′ ) η(1,α ′ ) + θI{R 1 =r 1 } jr 1 (0,α ′ ) η(0,α ′ ) ] =          α ′ ∈{0,1} K-1 α 1 ∈{0,1} j&gt;1 θI{R j =r j } jr j (α 1 ,α ′ ) η(α 1 ,α ′ ) [ θ1r 1 (1,α ′ ) η(1,α ′ ) + θ1r 1 (0,α ′ ) η(0,α ′ ) ], R 1 = r 1 α ′ ∈{0,1} K-1 α 1 ∈{0,1} j&gt;1 θI{R j =r j } jr j (α 1 ,α ′ ) η(α 1 ,α ′ ) [η (1,α ′ ) + η(0,α ′ ) ], R 1 ̸ = r 1 =          α ′ ∈{0,1} K-1 α 1 ∈{0,1} j&gt;1 θ I{R j =r j } jr j (α 1 ,α ′ ) η (α 1 ,α ′ ) [θ 1r 1 (1,α ′ ) η (1,α ′ ) + θ 1r 1 (0,α ′ ) η (0,α ′ ) ], R 1 = r 1 α ′ ∈{0,1} K-1 α 1 ∈{0,1} j&gt;1 θ I{R j =r j } jr j (α 1 ,α ′ ) η (α 1 ,α ′ ) [η (1,α ′ ) + η (0,α ′ ) ], R 1 ̸ = r 1 (A.7) = α ′ ∈{0,1} K-1 α 1 ∈{0,1} j&gt;1 θ I{R j =r j } jr j (α 1 ,α ′ ) η (α 1 ,α ′ ) [θ I{R 1 =r 1 } 1r 1 (1,α ′ ) η (1,α ′ ) + θ I{R 1 =r 1 } 1r 1 (0,α ′ ) η (0,α ′ ) ] = Ψ • η = P (R = r | Ψ, η).</formula><p>Equation (A.7) is derived based on (A.6) as well as the assumption that η c = ηc , θ jrc = θjrc for all j = 2, . . . , J, r = 0, . . . , M j -1 and c = 0, . . . , C -1. This proves (η, Θ) are not identifiable under (P 1) in Proposition II.4.</p><p>For polytomous-response RegCDMs, we have similar results by following the above proof. That is, (η i , Θ i ) are not identifiable under (P 1) for i = 1, . . . , N . Then following the same arguments in Step 2-3 from the Proof of Proposition II.3, we show that (β, γ, λ) in RegCDMs are not identifiable given (η i , Θ i ) are not identifiable.</p><p>Next we prove the remaining part, that is, the matrix Ψ in CDMs and the matrix Φ in RegCDMs have full column ranks under (P 2). Before presenting the proof, we introduce another probability matrix T -matrix of size S × C, where each row corresponds to one response pattern r ∈ S and each column corresponds to one latent class c = 0, . . . , C -1. Each entry of T -matrix is defined as</p><formula xml:id="formula_174">T rc = P (R ⪰ r | L = c),</formula><p>where ⪰ denotes that for any item j = 1, . . . , J, R j ≥ r j . According to a similar argument in Appendix Section 4.2 in Xu (2017), T -matrix has full column rank under the condition that the corresponding Q-matrix contains an identity submatrix I K .</p><p>There exists a relation between the two probability matrices, T -matrix and Ψ.</p><p>Because Ψ excludes a reference response pattern, its size is (S -1) × C. Denote RegCDMs, Φ has full column rank when Q-matrix contains an identity submatrix</p><formula xml:id="formula_175">Ψ ′ = (Ψ T , Ψ T ref ) T</formula><formula xml:id="formula_176">I K .</formula><p>Proof of Theorem II.5. Following the similar idea as in Huang and Bandeen-Roche (2004) page 15, we let f (R; η, Θ) to be the likelihood function, and</p><formula xml:id="formula_177">f (R; η, Θ) = r∈S P (R = r) I{R=r} , where S = × J j=1 {0, . . . , M j -1}. Let ξ = {η 1 , • • • , η C-1 , θ 110 , • • • , θ 1(M 1 -1)0 , • • • , θ J1(C-1) , • • • , θ J(M J -1)(C-1) }, so the Fisher information matrix is E ∂ log f ∂ξ ∂ log f ∂ξ T = E   r∈S I{R = r} P (R = r) ∂P (R = r) ∂ξ r∈S I{R = r} P (R = r) ∂P (R = r) ∂ξ T   = r∈S 1 P (R = r) ∂P (R = r) ∂ξ ∂P (R = r) ∂ξ T = J T          1 P (R=r 1 ) 0 • • • 0 0 1 P (R=r 2 ) • • • 0 . . . . . . . . . . . . 0 0 • • • 1 P (R=r S )          J.</formula><p>Hence the Fisher information matrix is non-singular if and only if J has full column rank. According to Theorem 1 of <ref type="bibr" target="#b128">Rothenberg (1971)</ref>, (η, Θ) are locally identifiable if and only if the Fisher information matrix is non-singular when the true values of (η, Θ) are regular point of the information matrix. Therefore (η, Θ) are locally identifiable if and only if the Jacobian matrix J has full column rank.</p><p>Proof of Theorem II.6. As introduced in Section 2.4, we consider a hypothetical subject with all covariates being zeros and denote its Jacobian matrix as J 0 . We use the following three steps to prove that (β, γ, λ) are identifiable if and only if J 0 has full column rank.</p><p>Step 1 : We first show that for subject i = 1, . . . , N , the Jacobian matrices J i , containing the derivatives of conditional response probabilities with respect to parameters in η i and Θ i , have full column rank if and only if J 0 has full column rank. This proof is adapted from the Proof of Proposition 1 in Huang and Bandeen-Roche (2004).</p><p>First, we need to set up a few notations. The Jacobian matrix J i is written as</p><formula xml:id="formula_178">J i = J i η 1 , • • • , J i η C-1 , J i θ 110 , • • • , J i θ 1(M 1 -1)0 , • • • , J i θ J1(C-1) , • • • , J i θ J(M J -1)(C-1) .</formula><p>Each entry in J i ηc is a partial derivative of response probability P (R = r) with respect to η i c at true value of η i c , which is computed to be</p><formula xml:id="formula_179">∂P (R = r) ∂η i c = J j=1 θ i jr j c - J j=1 θ i jr j 0 = ψ i rc -ψ i r0 .</formula><p>And each entry in J i θ jrc is a partial derivative of response probability P (R = r) with respect to θ i jrc at true value of θ i jrc , which is computed to be</p><formula xml:id="formula_180">∂P (R = r) ∂θ i jrc =                η i c d̸ =j θ i dr d c , if r j = r, -η i c d̸ =j θ i dr d c , if r j = 0, 0, otherwise.</formula><p>or summarized as</p><formula xml:id="formula_181">∂P (R = r) ∂θ i jrc = η i c ψ i rc ( I{r j = r} θ i jrc - I{r j = 0} θ i j0c</formula><p>).</p><p>In addition to J i , we also define the following two sets of vectors for this proof. Denote</p><formula xml:id="formula_182">J 0 = {ψ 0 0 , . . . , ψ 0 C-1 } ∪ {η 0 c I{r j = r}/θ 0 jrc ⊙ ψ 0 c : j = 1, . . . , J, r = 0, . . . , M j -1, c = 0, . . . , C -1} and J i = {ψ i 0 , . . . , ψ i C-1 } ∪ {η i c I{r j = r}/θ i jrc ⊙ ψ i c : j = 1, . . . , J, r = 0, . . . , M j -1, c = 0, . . . , C -1}</formula><p>, where I{r j = r} is a (S -1)-dimensional vector containing all I{r j = r} for r = (r 1 , . . . , r J ) ∈ S ′ . With the notations defined, we then introduce a useful lemma which simplify the arguments in proving the linear independence of the columns in J 0 and J i .</p><p>Lemma A.2. The Jacobian matrix J 0 has full column rank if and only if J 0 are linearly independent. The Jacobian matrix J i has full column rank if and only if J i are linearly independent.</p><p>The proof of Lemma A.2 is presented in Section B. According to Lemma A.2, to prove J i has full column rank if and only if J 0 has full column rank, we can equivalently show that J i are linearly independent if and only if J 0 are linearly independent. First, we associate the linear combinations of J i to that of J 0 as follows. For any linear combinations of J i with coefficients t i c , u i jrc , there exist t 0 c , u 0 jrc and W i such that the following equation holds</p><formula xml:id="formula_183">C-1 c=0 t i c ψ i c + J j=1 M j -1 r=0 C-1 c=0 u i jrc η i c I{r j = r} θ i jrc ⊙ ψ i c =   C-1 c=0 t 0 c ψ 0 c + J j=1 M j -1 r=0 C-1 c=0 u 0 jrc η 0 c I{r j = r} θ 0 jrc ⊙ ψ 0 c   ⊙ W i , (A.8)</formula><p>where</p><formula xml:id="formula_184">W i = J j=1 exp(λ 1jr j z ij1 + • • • + λ qjr j z ijq ) : r = (r 1 , . . . , r J ) ∈ S ′ T S×1 , t 0 c = t i c J j=1 1 + M j -1 s=1 e γ jsc 1 + M j -1 s=1 exp(γ jsc + λ 1js z ij1 + • • • + λ qjs z ijq ) , (A.9) u 0 jrc = u i jrc exp(β 1c x i1 + • • • + β pc x ip ) exp(λ 1jr j z ij1 + • • • + λ qjr j z ijq ) × {1 + C-1 l=1 e β 0l }{1 + M j -1 s=1 exp(γ jsc + λ 1js z ij1 + • • • + λ qjs z ijq )} {1 + C-1 l=1 exp(β 0l + β 1l x i1 + • • • + β pl x ip )}{1 + M j -1 s=1 e γ jsc } × J j=1 1 + M j -1 s=1 e γ jsc 1 + M j -1 s=1 exp(γ jsc + λ 1js z ij1 + • • • + λ qjs z ijq )</formula><p>.</p><p>(A.10)</p><p>The next two parts prove that J i are linearly independent if and only if J 0 are linearly independent in two directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part (i):</head><p>We prove J i are linearly independent if J 0 are linearly independent. To show J i are linearly independent, we need to show that <ref type="bibr">.11)</ref> implies t i c = 0 and u i jrc = 0. By (A.8), for any t i c , u i jrc such that (A.11) holds, we have</p><formula xml:id="formula_185">C-1 c=0 t i c ψ i c + J j=1 M j -1 r=0 C-1 c=0 u i jrc η i c I{r j = r} θ i jrc ⊙ ψ i c = 0, (A</formula><formula xml:id="formula_186">C-1 c=0 t 0 c ψ 0 c + J j=1 M j -1 r=0 C-1 c=0 u 0 jrc η 0 c ( I{r j = r} θ 0 jrc ) ⊙ ψ 0 c = 0.</formula><p>Under the condition that J 0 are linearly independent, t 0 c = 0 and u 0 jrc = 0. Then by (A.9) and (A.10), we have t i c = 0 and u i jrc = 0 for j = 1, . . . , J, r = 0, . . . , M j -1 and c = 0, . . . , C -1. So J i are linearly independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part (ii):</head><p>We prove J 0 are linearly independent if J i are linearly independent.</p><p>This part is similar to Part (i). To show J 0 are linearly independent, we need to show that</p><formula xml:id="formula_187">C-1 c=0 t 0 c ψ 0 c + J j=1 M j -1 r=0 C-1 c=0 u 0 jrc η 0 c I{r j = r} θ 0 jrc ⊙ ψ 0 c = 0 (A.12)</formula><p>implies t 0 c = 0 and u 0 jrc = 0. By (A.8), for any t 0 c , u 0 jrc such that (A.12) holds, we have</p><formula xml:id="formula_188">C-1 c=0 t i c ψ i c + J j=1 M j -1 r=0 C-1 c=0 u i jrc η i c ( I{r j = r} θ i jrc ) ⊙ ψ i c = 0.</formula><p>Under the condition that J i are linearly independent, t i c = 0 and u i jrc = 0, and hence t 0 c = 0 and u i jrc = 0 by (A.9) and (A.10), for j = 1, . . . , J, r = 0, . . . , M j -1 and c = 0, . . . , C -1. So J 0 are linearly independent.</p><p>Combining Part (i) and Part (ii), we show J i are linearly independent if and only if J 0 are linearly independent. And therefore J i has full column rank if and only if J 0 has full column rank.</p><p>Step 2 : We introduce (ϵ i , ω i ) and prove that they are identifiable if and only if J i has full column rank. By following similar arguments in the Proof of Theorem II.5, we have (η i , Θ i ) are identifiable if and only if J i has full column rank, for i = 1, . . . , N .</p><p>Next, we define (ϵ i , ω i ) and the remaining is to show that they are identifiable if and only if (η i , Θ i ) are identifiable. Following the same arguments as Step 2 in Proof of Proposition II.3, we let</p><formula xml:id="formula_189">ϵ i c = x T i β c = β 0c + β 1c x i1 + • • • + β pc x ip .</formula><p>for c = 0, . . . , C -1. And</p><formula xml:id="formula_190">ω i jrc = γ jrc + z T ij λ jr = γ jrc + λ 1jr z ij1 + • • • + λ qjr z ijq .</formula><p>for j = 1, . . . , J, r = 0, • • • , M j -1 and c = 0, . . . , C -1. Then according to Lemma A.1, (ϵ i , ω i ) are identifiable if and only if (η i , Θ i ) are identifiable. Hence the proof for Step 2 is complete.</p><p>Step 3 : The final step is to show (β, γ, λ) are identifiable if and only if (ϵ i , ω i ) are identifiable. We have shown that (β, γ, λ) are not identifiable when (ϵ i , ω i ) are not identifiable in the Proof of Proposition II.3. So all left to show is the necessary part that (β, γ, λ) are identifiable when (ϵ i , ω i ) are identifiable. We prove this result by the method of contradiction. Assuming the contrary that β is not identifiable, <ref type="bibr">γ, λ)</ref>. According to the system of linear equations</p><formula xml:id="formula_191">there exist β ̸ = β ′ such that P (R i | β, γ, λ) = P (R i | β ′ ,</formula><formula xml:id="formula_192">ϵ =       ϵ 1 . . . ϵ N       =       1 x 11 • • • x 1p . . . . . . . . . . . . 1 x N 1 • • • x N p             β 00 • • • β 0(C-1) . . . . . . . . . β p0 • • • β p(C-1)       = Xβ,</formula><p>and because the full rank X is an injective mapping, we have</p><formula xml:id="formula_193">β ̸ = β ′ implies that ϵ = Xβ is different from ϵ ′ = Xβ ′ for at least one ϵ i ̸ = ϵ ′i . However, since ϵ i 's are identifiable, there exist no ϵ i ̸ = ϵ ′i such that P (R i | ϵ i , ω i ) = P (R i | ϵ ′i , ω i ).</formula><p>By this contradiction, we prove β is identifiable. Using similar arguments, we can show γ, λ are also identifiable and hence complete the proof.</p><p>Combining Steps 1-3, we prove that (β, γ, λ) in RegCDMs are identifiable if and only if J 0 has full column rank under (A1)-(A3).</p><p>To prove the main results in Section 2.4, we next introduce other useful lemmas and corollaries from existing literatures. Lemma A.3 and Corollaries A.4-A.5 summarize the conditions for the global identifiability of general restricted latent class models proposed by <ref type="bibr" target="#b2">Allman et al. (2009)</ref>, which is based on the algebraic results in <ref type="bibr" target="#b86">Kruskal (1977)</ref>.</p><p>Before presenting these lemmas and corollaries, we introduce the decomposition of Ψ and some notation definitions. The decomposition of Ψ is similar as the decomposition of Φ defined in Section 2.4 in the main text. We divide the total of J items into three mutually exclusive item sets J 1 , J 2 and J 3 containing J 1 , J 2 and J 3 items respectively, with J 1 + J 2 + J 3 = J. For t = 1, 2 and 3, let S Jt be the set containing the response patterns from items in J t with cardinality of S Jt to be</p><formula xml:id="formula_194">κ t = |S Jt | = j∈Jt M j . The submatrix Ψ t has dimension κ t × C.</formula><p>The definition for the entries in Ψ t is the same as in (2.10), except that each row of Ψ t corresponds to one response patterns r ∈ S Jt while each row of Ψ corresponds to r ∈ S ′ .</p><p>Proof of Proposition II.10. In Proposition 5.1(b) of <ref type="bibr" target="#b62">Gu and Xu (2020)</ref>, the condition (C4 ′′ ) is sufficient for the generic identifiability of CDMs. So for RegCDMs, (η i , Θ i ) are generically identifiable under (C4 ′′ ) for i = 1, . . . , N . Based on the the similar arguments in Step 2-3 from the Proof of Theorem II.6, (β, γ, λ) in RegCDMs are generically identifiable given (η i , Θ i ) are generically identifiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proofs of Lemmas</head><p>Proof of Lemma A.1. For notational convenience, we use η, Θ, ϵ and ω to denote the parameters η i , Θ i , ϵ i and ω i of a general subject i. According to the definition of identifiability, (η, Θ) are identifiable means that there exist no (η, </p><formula xml:id="formula_195">Θ) ̸ = (η ′ , Θ ′ ) such that P (R = r | η, Θ) = P (R = r | η ′ , Θ ′ ).</formula><formula xml:id="formula_196">, • • • , η C-1 ) = (η ′ 0 , • • • , η ′ C-1 ) if and only if (ϵ 0 , • • • , ϵ C-1 ) = (ϵ ′ 0 , • • • , ϵ ′ C-1 ).</formula><p>First, we show that (η 0 ,</p><formula xml:id="formula_197">• • • , η C-1 ) = (η ′ 0 , • • • , η ′ C-1 ) implies (ϵ 0 , • • • , ϵ C-1 ) = (ϵ ′ 0 , • • • , ϵ ′ C-1</formula><p>). For c = 0, . . . , C -1, under the condition that</p><formula xml:id="formula_198">η c = e ϵc 1 + C-1 s=1 e ϵs = e ϵ ′ c 1 + C-1 s=1 e ϵ ′ s = η ′ c ,</formula><p>we can write</p><formula xml:id="formula_199">e δ = e ϵ 0 e ϵ ′ 0 = • • • = e ϵc e ϵ ′ c = • • • = e ϵ C-1 e ϵ ′ C-1 = 1 + C-1 s=1 e ϵs 1 + C-1 s=1 e ϵ ′ s</formula><p>, where e δ denotes the common ratio among all e ϵc /e ϵ ′ c . Hence</p><formula xml:id="formula_200">δ = ϵ c -ϵ ′ c , c = 0, • • • , C -1. (A.13)</formula><p>Substituting every ϵ ′ c with ϵ c -δ into the equation η 0 = η ′ 0 , we have</p><formula xml:id="formula_201">e ϵ 0 1 + C-1 s=1 e ϵs = e ϵ 0 -δ 1 + C-1 s=1 e ϵs-δ</formula><p>, Further simplifying the above equation gives</p><formula xml:id="formula_202">1 1 + C-1 s=1 e ϵs = 1 e δ + C-1 s=1 e ϵs</formula><p>, and then we have</p><formula xml:id="formula_203">e δ + C-1 s=1 e ϵs = 1 + C-1 s=1 e ϵs ,</formula><p>which has unique solution δ = 0. Taking δ = 0 back into (A.13), we have</p><formula xml:id="formula_204">ϵ c = ϵ ′ c for all c = 0, . . . , C -1. Therefore ϵ = (ϵ 0 , • • • , ϵ C-1 ) is equivalent to ϵ ′ = (ϵ ′ 0 , • • • , ϵ ′ C-1 ). Next we prove (ϵ 0 , • • • , ϵ C-1 ) = (ϵ ′ 0 , • • • , ϵ ′ C-1 ) implies (η 0 , • • • , η C-1 ) = (η ′ 0 , • • • , η ′ C-1 ). This part is straightforward as (ϵ 0 , • • • , ϵ C-1 ) = (ϵ ′ 0 , • • • , ϵ ′ C-1</formula><p>) implies that for any c = 0, . . . , C -1, we have exp(ϵ c )</p><formula xml:id="formula_205">1 + C-1 s=1 exp(ϵ s ) = exp(ϵ ′ c ) 1 + C-1 s=1 exp(ϵ ′ s )</formula><p>.</p><p>Equivalently, we show</p><formula xml:id="formula_206">η c = η ′ c for any c = 0, . . . , C -1. So (η 0 , • • • , η C-1 ) = (η ′ 0 , • • • , η ′ C-1</formula><p>). Combining the above arguments, we prove η = η ′ if and only if</p><formula xml:id="formula_207">ϵ = ϵ ′ .</formula><p>Similar arguments can be applied to show Θ = Θ ′ if and only if ω = ω ′ . Hence (η, Θ) are identifiable if and only if (ϵ, ω) are identifiable.</p><p>Proof of Lemma A.2. We prove the the first part, that is, J 0 has full column rank if and only if J 0 are linearly independent. The second part regarding J i can be similarly proved.</p><p>To show the linear independence of J 0 or J 0 , we need to establish the relationship 123 between the two linear combinations as follows. For any linear combinations of the columns in J 0 with coefficients h 0 c 's and l 0 jrc 's, there exist a 0 c 's and b 0 jrc 's such that the following equation holds.</p><formula xml:id="formula_208">C-1 c=1 h 0 c (ψ 0 c -ψ 0 0 ) + J j=1 M j -1 r=1 C-1 c=0 l 0 jrc η 0 c ( I{r j = r} θ 0 jrc - I{r j = 0} θ 0 j0c ) ⊙ ψ 0 c (A.14) = C-1 c=0 a 0 c ψ 0 c + J j=1 M j -1 r=0 C-1 c=0 b 0 jrc η 0 c ( I{r j = r} θ 0 jrc ) ⊙ ψ 0 c , (A.15)</formula><p>where</p><formula xml:id="formula_209">a 0 c =        h 0 c , if c ̸ = 0, -(h 0 1 + • • • + h 0 C-1 ), if c = 0, (A.16)</formula><p>and for any j = 1, . . . , J, c = 0, . . . , C -1,</p><formula xml:id="formula_210">b 0 jrc =        l 0 jrc , if r ̸ = 0, -(l 0 j1c + • • • + l 0 j(M j -1)c ), if r = 0.</formula><p>(A.17)</p><p>With the above relationship established, we next show that J 0 has full column rank if and only if J 0 are linearly independent. When J 0 are linearly independent, (A.15) = 0 implies a 0 c = 0 and b 0 jrc = 0, which further implies h 0 c = 0 and l 0 jrc = 0 by (A.16) and (A.17). So (A.14) = 0 implies h 0 c = 0 and l 0 jrc = 0. Hence, J 0 has full column ranks. Similarly, when J 0 has full column ranks, (A.14) = 0 implies h 0 c = 0 and l 0 jrc = 0 which further implies a 0 c = 0 and b 0 jrc = 0 by (A.16) and (A.17). So (A.15) = 0 implies a 0 c = 0 and b 0 jrc = 0. Hence, J 0 are linearly independent.</p><p>Proof of Lemma A.6. This proof is adapted from the Proof of Proposition 2 in Huang and <ref type="bibr" target="#b76">Bandeen-Roche (2004)</ref>. Before presenting the proof, we set up a few notations.</p><p>In Section 2.  <ref type="formula" target="#formula_2">B2</ref>) are shown to be necessary in Section 2.2 and assumed to hold. To prove (B3.a) is sufficient for the strict identifiability of (η i , Θ i ), we first need to show that for t = 1, 2 and 3, given Φ t has Kruskal rank I t , the equation O t ≥ I t holds, so that I 1 + I 2 + I 3 ≥ 2C + 2 from (B3.a) implies</p><formula xml:id="formula_211">O 1 + O 2 + O 3 ≥ 2C + 2. Then based on Corollary A.4 that (η i , Θ i ) are strictly identifiable under the condition that O 1 + O 2 + O 3 ≥ 2C + 2, we complete the proof of strict identifiability.</formula><p>The remaining part is to show O t ≥ I t for t = 1, 2 and 3. Without loss of generality, we only show O 1 ≥ I 1 , then O 2 ≥ I 2 and O 3 ≥ I 3 can be similarly proved. Under the condition that any set of I 1 columns in Φ 1 are linearly independent, ϕ 1σ(1) , • • • , ϕ 1σ(I 1 ) are linearly independent for any permutation σ on</p><formula xml:id="formula_212">{1, . . . , I 1 } such that {σ(1), σ(2), • • • , σ(I 1 )} ⊆ {0, • • • , C -1}. To show O t ≥</formula><p>I t , we need ψ 1σ(1) , • • • , ψ 1σ(I 1 ) to be linearly independent for any permutation set {σ(1), σ(2), • • • , σ(I 1 )}. The linear combinations of ϕ 1σ(1) , • • • , ϕ 1σ(I 1 ) can be associated with the linear combinations of ψ 1σ(1) , • • • , ψ 1σ(I 1 ) as follows. For any permutation σ and a σ(c) , there exists b σ(c) and Y i 1 such that</p><formula xml:id="formula_213">I 1 c=1 a σ(c) ψ i 1σ(c) = ( I 1 c=1 b σ(c) ϕ 1σ(c) ) ⊙ Y i 1 (A.18)</formula><p>where</p><formula xml:id="formula_214">Y i 1 = j∈J 1 exp(λ 1jr j z ij1 + • • • + λ qjr j z ijq ) : r = (r 1 , . . . , r J ) ∈ S J 1 κ 1 ×1 , b σ(c) = a σ(c) j∈J 1 1 + M j -1 s=1 e γ jsσ(c) 1 + M j -1 s=1 exp(γ jsσ(c) + λ 1js z ij1 + • • • + λ qjs z ijq ) . (A.19)</formula><p>To show ψ 1σ(1) , • • • , ψ 1σ(I 1 ) to be linearly independent, we need to show I 1 c=1 a σ(c) ψ i 1σ(c) = 0 implies a σ(c) = 0 for any σ. Based on (A.18), we have</p><formula xml:id="formula_215">I 1 c=1 a σ(c) ψ i 1σ(c) = 0 implies I 1 c=1 b σ(c) ϕ 1σ(c) = 0. Under the condition that ϕ 1σ(1) , • • • , ϕ 1σ(I 1 ) are linear independent, I 1 c=1 b σ(c) ϕ 1σ(c) = 0 implies b σ(1) = • • • = b σ(I 1 ) = 0. And by (A.19), a σ(1) = • • • = a σ(I 1 ) = 0. Hence ψ 1σ(1) , • • • , ψ 1σ(I 1 )</formula><p>are linearly independent for any σ.</p><p>Hence we show O 1 ≥ I 1 and complete the proof for strict identifiability.</p><p>For condition (B3.b), because each Ψ i t has row dimension κ t the same as Φ t does and we have min{C, κ 1 } + min{C, κ 2 } + min{C, κ 3 } ≥ 2C + 2, according to Corollary A.5, (η i , Θ i ) are generically identifiable under (B3.b) for all i = 1, . . . , N .</p><p>U follows the generalized linear model with the probability density (mass) function to be</p><formula xml:id="formula_216">f (y) = exp [{y(θD + v T Q + β T U ) -b(θD + v T Q + β T U )}/a(ϕ) + c(y, ϕ)] , (B.1)</formula><p>and the relationship between X and U is</p><formula xml:id="formula_217">X = W T U + E. (B.2)</formula><p>We let Z = (D, Q T , U T ) T to be a vector that includes all of the covariates and the unmeasured confounders, and let η T = (θ, v T , β T ) to be the corresponding parameters. For notational convenience, we also let M = (Q T , U T ) T and its coefficient</p><formula xml:id="formula_218">ζ = (v T , β T ) T , so η = (θ, ζ T ) T . We denote γ = (θ, v T ) T as the coefficient for covari- ates X, so η = (γ T , β T ) T .</formula><p>Throughout the appendix, we use an asterisk on the upper subscript to indicate the population parameters. We assume that the observed data {y i , X i } i=1,...,n and the unmeasured confounders {U i } i=1,...,n are realizations of (B.1) and (B.2). The noise for the factor model are denoted as {E i } i=1,...,n with E i = (E ij : j = 1, . . . , p).</p><p>In the proposed inferential procedure, we first obtain the maximum likelihood estimator for unmeasured confounders U i . With the unmeasured confounder estimators,</p><formula xml:id="formula_219">we let Żi = (D i , Q T i , U T i ) T , Ṁi = (X T i , U T i )</formula><p>T and get an initial lasso estimator</p><formula xml:id="formula_220">η = argmin η∈R (p+K) l(η) + λ∥γ∥ 1 , (B.3)</formula><p>where the loss function is the negative log-likelihood function defined as</p><formula xml:id="formula_221">l(η) = - 1 n n i=1 {y i η T Żi -b(η T Żi )}.</formula><p>The gradient ∇l(η) and Hessian ∇ 2 l(η) of the loss function are commonly used in our subsequent proofs, which are expressed as</p><formula xml:id="formula_222">∇l(η) = - 1 n n i=1 {y i -b ′ (η T Żi )} Żi ; ∇ 2 l(η) = 1 n n i=1 b ′′ (η T Żi ) Żi ŻT i .</formula><p>In constructing the debiased estimator, we define w * = I * θζ (I * ζζ ) -1 and τ * =</p><p>(1, -w * ) T , where</p><formula xml:id="formula_223">I * θζ = E[b ′′ {(η * ) T Z i }D i M T i ] and I * ζζ = E[b ′′ {(η * ) T Z i }M i M T i ].</formula><p>We denote two sub-vectors of w * as w * q = (w * 2 , . . . , w * q ) T and w * u = (w * p+1 , . . . , w * p+K ) T .</p><p>We define the estimator for the sparse vector w as w = argmin</p><formula xml:id="formula_224">w∈R (p+K-1) 1 2n n i=1 {w T ∇ ζζ l i ( θ, ζ)w -2w T ∇ ζθ l i ( θ, ζ)} + λ ′ ∥w∥ 1 , (B.4)</formula><p>where</p><formula xml:id="formula_225">l i (θ, ζ) = -y i (θD i + ζ T Ṁi ) + b(θD i + ζ T Ṁi ) is equivalent to l i (η)</formula><p>, the ith component of the loss function. Equivalently, the estimator w is obtained by</p><formula xml:id="formula_226">w = argmin w∈R (p+K-1) 1 2n n i=1 b ′′ ( θD i + ζ T Ṁi )(D i -w T Ṁi ) 2 + λ ′ ∥w∥ 1 .</formula><p>The estimator w is used in estimating the generalized decorrelated score function and the partial Fisher information matrix. Under null hypothesis H 0 : θ * = θ 0 , the two estimators are</p><formula xml:id="formula_227">S(θ 0 , ζ) = - 1 n n i=1 {y i -b ′ (θ 0 D i + v T Q i + β T U i )}(D i -w T Ṁi ); I θ|ζ = 1 n n i=1 b ′′ ( θD i + v T Q i + β T U i )D i (D i -w T Ṁi ).</formula><p>Our theoretical results are established under the asymptotic regime with n, p → ∞. Regarding the factor model, as stated in Assumption 1 in Section 3.4 of the main text, we do not assume the random errors E i to be identically distributed nor does the model assumption require the covariance of E i to be diagonal. Specifically, we assume for some large constant</p><formula xml:id="formula_228">C &gt; 0: (a) E(E ij ) = 0, E(E 8 ij ) ≤ C; (b) E(E ih E ij ) = τ i,hj</formula><p>with |τ i,hj | ≤ τ hj for some τ hj &gt; 0 and all i = 1, . . . , n, and p h=1 τ hj ≤ C for all j = 1, . . . , p. (c) E(E ij E sj ) = ρ is,j with |ρ is,j | ≤ ρ is for some ρ is &gt; 0 and all j = 1, . . . , p, and n -1 n i=1 n s=1 ρ is ≤ C. (d) For all j, q = 1, . . . , p,</p><formula xml:id="formula_229">E 1 √ n n i=1 [E ij E iq -E(E ij E iq )] 4 ≤ C.</formula><p>Regarding the loading matrix, we assume ∥W * j ∥ 2 ≤ C. There exist positive definite matrices Γ * and Υ * such that lim p→∞ p -1 W * (Σ * e ) -1 (W * ) T = Γ * and lim p→∞</p><formula xml:id="formula_230">p -1 p j=1 (σ * j ) -4 {(W * j ) T ⊗ (W * j ) T }(W * j ⊗ W * j ) = Υ * .</formula><p>In addition, we also assume a working identifiability condition that S u = I K and p -1 W * (Σ * e ) -1 (W * ) T is a diagonal matrix with distinct entries.</p><p>For the assumptions regarding the generalized linear model relating y and (X, U ), as mentioned in Assumption III.5 in Section 3.4 of the main text, we let λ min (I * ) ≥ κ,</p><p>where</p><formula xml:id="formula_231">I * = E[b ′′ {(η * ) T Z i }Z i Z T i</formula><p>] and κ is some constant. The unmeasured confounders, the covariates and coefficients parameters are assumed to be bounded, that is,</p><formula xml:id="formula_232">∥U i ∥ ∞ ≤ M , ∥X i ∥ ∞ ≤ M , ∥η * ∥ ∞ ≤ M , and |(w * q ) T Q i | ≤ M for some constant M &gt; 0. We let |y i -b ′ {(η * ) T Z i }| to be sub-exponential with ∥y i -b ′ {(η * ) T Z i }∥ φ 1 ≤ M .</formula><p>In addition, we assume</p><formula xml:id="formula_233">a 1 ≤ (η * ) T Z i ≤ a 2 , 0 ≤ |b ′ (t)| ≤ B with |b ′ (t 1 ) -b ′ (t)| ≤ B|(t 1 -t)b ′ (t)| and 0 ≤ b ′′ (t) ≤ B with |b ′′ (t 1 )-b ′′ (t)| ≤ B|t 1 -t|b ′′ (t) for constants a 1 , a 2 and B, where t ∈ [a 1 -ϵ, a 2 + ϵ] for ϵ &gt; 0 and sequence t 1 satisfying |t 1 -t| = o(1).</formula><p>With the notations and assumptions revisited, we next present the discussion for the working identifiability condition and then the proofs of Theorems III.7 and III.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Discussion on Working Identifiability Condition</head><p>In this section, we make a detailed clarification on the working identifiability condition in the main text. Specifically, we show that when the working identifiability condition does not hold, we can transform the model into an identifiable model and the transformed identifiable model has identical parameter of interest θ compared to that of the pre-transformed model.</p><p>When the identifiability condition is not satisfied, the estimators are not fully identifiable in the sense that, for any invertible matrix O, we have W = O T W and S u = (O -1 ) T S u O -1 to be valid maximum likelihood estimators. At W = O T W , we can substitute the expression for W into (3.4) to get the relationship between U i and U i as follows,</p><formula xml:id="formula_234">U i = ( W Σ -1 e W T ) -1 W Σ -1 e (X i -X) = (O T W Σ -1 e W T O) -1 O T W Σ -1 e (X i -X) = O -1 ( W Σ -1 e W T ) -1 (O -1 ) T O T W Σ -1 e (X i -X) = O -1 U i .</formula><p>Suppose W and U i are the asymptotically unbiased estimators for W * and U * i respectively. Here we want to clarify that we use S * u and U * i to differentiate the unmeasured confounders before the transformation from that of transformed model but this does not imply that S * u and U * i are population parameters. we have W and U i to be the asymptotically unbiased estimators for O T W * and O -1 U * i .</p><p>When the working identifiability condition does not hold, that is, we have S * u ̸ = I K , and/or p -1 W * Σ -1 e (W * ) T is not diagonal matrix with distinct entries, we can find an invertible matrix O to transform the true parameters to satisfy the assumption. Then we build a correspondence between the true model and the model corresponding to the transformed parameters, showing that the two models have parameter of interest θ to be identical.</p><p>The transformed parameters are</p><formula xml:id="formula_235">W r = O -1 2 O T 1 W * and U r i = O T 2 (O -1 1 )U * i , which</formula><p>are constructed in two steps. At first step, for any S * u ̸ = I K , we can find a matrix O 1 such that</p><formula xml:id="formula_236">n -1 n i=1 O -1 1 (U * i -Ū * )(U * i -Ū * ) T (O -1 1 ) T = O -1 1 S * u (O -1 1 ) T = I K .</formula><p>At next step, given the matrix p -1 W * Σ -1 e (W * ) T is symmetrical, there exists an orthogonal matrix O 2 whose columns correspond to the eigenvectors of p</p><formula xml:id="formula_237">-1 O T 1 W * Σ -1 e (O T 1 W * ) T as we could decompose this symmetric matrix into p -1 O T 1 W * Σ -1 e (O T 1 W * ) T = O 2 ΛO T 2</formula><p>where Λ has distinct eigenvalues in the diagonal.</p><p>We verify the transformed parameters satisfy the identifiability condition as fol-</p><formula xml:id="formula_238">lows. At W r = O -1 2 O T 1 W * and U r i = O T 2 (O -1 1 )U * i , we have S r u = n -1 n i=1 O T 2 O -1 1 (U * i -Ū * )(U * i -Ū * ) T (O -1 1 ) T O 2 = O T 2 I K O 2 = I K ,<label>and</label></formula><formula xml:id="formula_239">p -1 W r Σ -1 e (W r ) T = p -1 O -1 2 O T 1 W * Σ -1 e (W * ) T O 1 (O -1 2 ) T = O -1 2 O 2 ΛO T 2 (O -1 2 ) T = Λ,</formula><p>is a diagonal matrix with distinct entries.</p><p>We let O = O T 2 (O -1 1 ), so accordingly the parameters for the transformed factor model are U r = OU * and (W r ) T = (W * ) T O -1 . The relationship between the true model and the transformed model is as follows. The factor model structure corresponding to the true parameter is the same as the model corresponding to the transformed parameters as</p><formula xml:id="formula_240">X = (W r ) T U r + E = (W * ) T O -1 OU * + E = (W * ) T U * + E.</formula><p>The generalized linear framework according to the rotated true parameter is</p><formula xml:id="formula_241">f (y) = exp [y{θ r D + (v r ) T Q + (β r ) T U } -b{θ r D + (v r ) T Q + (β r ) T U }]/a(ϕ) + c(y, ϕ) ,</formula><p>whereas the framework according to true parameters is</p><formula xml:id="formula_242">f (y) = exp [y{θ * D + (v * ) T Q + (β * ) T U } -b{θ * D + (v * ) T Q + (β * ) T U }]/a(ϕ)</formula><p>+ c(y, ϕ) .</p><p>At β r = O -1 β * , θ r = θ * and v r = v * , the two frameworks are identical. That is, when the confounders are not identifiable, only the coefficient β will be affected accordingly; the parameter of interest will not change and thus the theoretical results on θ are not affected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Estimation of Dimension of Unmeasured Confounders</head><p>In the numerical studies of this paper, we use parallel analysis <ref type="bibr" target="#b74">(Horn, 1965)</ref> to estimate the dimension of unmeasured confounders. Because in factor analysis, parallel analysis is a popular approach to selecting the number of factors as it is accurate and easy to use <ref type="bibr" target="#b68">(Hayton et al., 2004;</ref><ref type="bibr" target="#b41">Costello and Osborne, 2005;</ref><ref type="bibr" target="#b21">Brown, 2015)</ref>. In the existing literature, several authors have conducted extensive simulation studies to assess the performance of parallel analysis relative to other existing approaches <ref type="bibr" target="#b159">(Zwick and Velicer , 1986;</ref><ref type="bibr" target="#b119">Peres-Neto et al., 2005)</ref>. They have shown that parallel analysis has better numerical performance in terms of selecting K than many existing approaches <ref type="bibr" target="#b159">(Zwick and Velicer , 1986;</ref><ref type="bibr" target="#b119">Peres-Neto et al., 2005)</ref>. Furthermore, parallel analysis is also a commonly used statistical tool for dimension reduction <ref type="bibr" target="#b96">(Lin et al., 2016)</ref>, multiple testing dependence <ref type="bibr" target="#b92">(Leek and Storey, 2008)</ref>, and finds wide appli-cations in other scientific disciplines including virology <ref type="bibr" target="#b122">(Quadeer et al., 2014)</ref> and genetic studies <ref type="bibr" target="#b91">(Leek and Storey, 2007)</ref>.</p><p>The implementation of parallel analysis is as follows. With our given matrix X n×p = (D, Q T ) T , we denote p columns in the design matrix as X 1 , . . . , X p , respectively. Then we repeatedly generate matrices X π 's where each matrix is generated by randomly permuting every column X j for j = 1, . . . , p. Next, we select the first factor when the top singular value of X is larger than a certain percentile of the top singular value of the permuted matrices X π 's. If the first factor is selected, we repeat this procedure to determine whether the second factor can be selected. The process is repeated until no more factor is selected. The main intuition of this approach is that the factor model is considered a summation of the signal (factors) and noise (random error). The permutation destroys the original signal structure and turns it into a matrix of random noise. Thus, identifying factors based on large singular values of X can be interpreted as selecting factors that are above the noise level.</p><p>Besides parallel analysis, there are various methods to estimate the dimension of unmeasured confounders such as scree plot <ref type="bibr" target="#b27">(Cattell , 1966)</ref>, which empirically chooses the elbow point in the plot of descending eigenvalues of factors; method based on cross validation <ref type="bibr" target="#b113">(Owen and Wang, 2016)</ref>, which uses random held-out matrices of data to choose the number of factors; method based on information criteria including AIC and BIC to select the number of factors in high-dimensional factor model <ref type="bibr" target="#b9">(Bai and Ng, 2002)</ref>; the eigenvalue ratio method <ref type="bibr" target="#b87">(Lam and Yao, 2012;</ref><ref type="bibr" target="#b0">Ahn and Horenstein, 2013)</ref>, which chooses K by K = arg max K≤K λ k (XX T )/λ k+1 (XX T ) where λ k (XX T ) denotes the k-th eigenvalue of XX T and K is a prespecified threshold which is often set to be K = p/2 in practice. Among these methods, the informationcriteria-based method and eigenvalue ratio method have the theoretical guarantees to be consistent under similar assumptions as Assumption 1 in our paper. These assumptions follow the common conditions in theoretical analysis for the approxi- III.5 and the scaling condition n, p → ∞, we have </p><formula xml:id="formula_243">(i) ∥∇l(η * )∥ ∞ = ∥n -1 n i=1 [y i -b ′ {(η * ) T Żi }] Żi ∥ ∞ = O p {n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 }; (ii) ∥(τ * ) T ∇ 2 l(η * ) -E η * {(τ * ) T ∇ 2 l(η * )}∥ ∞ = ∥n -1 n i=1 (τ * ) T b ′′ {(η * ) T Żi } Żi ŻT i -E η * [(τ * ) T b ′′ {(η * ) T Żi } Żi ŻT i ]∥ ∞ = O p {n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 }.</formula><formula xml:id="formula_244">∥n -1 n i=1 [y i -b ′ {(η * ) T Żi }]</formula><p>Żi ∥ ∞ , we instead apply Bernstein inequality which requires the uniform estimation bound of unmeasured confounders, that is,</p><formula xml:id="formula_245">max i ∥ U i -U i ∥ ∞ .</formula><p>We leave the detailed proof of Lemma B.1 in Appendix G.1.</p><p>We next obtain the upper bound for the estimation error of η. To this end, we define the first-order approximation to the loss difference as D( η, η * ) = ( ηη * ) T {∇l( η) -∇l(η * )}. We will obtain upper and lower bounds of D( η, η * ). As we will show, the ℓ 2 norm ∥ ηη * ∥ 2 is involved in both upper and lower bounds, combining which will give an inequality of ∥ ηη * ∥ 2 and thus result in a bounded estimation error of η.</p><p>Upper Bound for D( η, η * ): recall that η T = ( γ T , β T ) is a solution obtained from solving the convex optimization problem in (B.3). And thus, we have the following optimality conditions -∇l( γ) = λϱ and ∇l( β) = 0, where γ is a p × 1 vector with entries</p><formula xml:id="formula_246">ϱ j =      sign( γ j ), γ j ̸ = 0 [-1, 1], γ j = 0 , (j = 1, . . . p). This implies ∥∇l( η)∥ ∞ = ∥n -1 n i=1 Żi {y i -b ′ ( η T Żi )}∥ ∞ ≤ λ.</formula><p>Recall that we denote the support set for η * as S η = {j : η * j ̸ = 0}. We denote the difference between the estimator η and the true parameter η * as ∆ = ηη * , and its two sub-vectors are ∆ S = ( η j -η * j : j ∈ S η ) and ∆ S = ( η j -η * j : j / ∈ S η ). Similarly, we denote the sub-vectors of Żi corresponding to non-zero entries as Żi,S = { Żij : j ∈ S η } and that corresponding to zero entries as Żi, S = { Żij : j / ∈ S η }. With the notations introduced, the quadratic difference D( η, η * ) can then be written as</p><formula xml:id="formula_247">D( η, η * ) = ( η -η * ) T {∇l( η) -∇l(η * )} = - 1 n n i=1 ∆ T S Żi,S {y i -b ′ ( η T Żi )} - 1 n n i=1 ∆ T S Żi, S {y i -b ′ ( η T Żi )} -∆ T ∇l(η * ) ≤ λ∥ ∆ S ∥ 1 -λ∥ ∆ S ∥ 1 + ∥ ∆∥ 1 ∥∇l(η * )∥ ∞</formula><p>where the last inequality is by Hölder's inequality.</p><formula xml:id="formula_248">From Lemma B.1, we have ∥∇l(η * )∥ ∞ ≲ n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 . Since ∥ ∆∥ 1 ≤ ∥ ∆ S ∥ 1 + ∥ ∆ S ∥ 1 and by taking λ = 2c{n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 } where c &gt; 0 is some constant, we further have D( η, η * ) ≤ c log p n + log n p (3∥ ∆ S ∥ 1 -∥ ∆ S ∥ 1 ) ≤ 3cs 1/2 η log p n + log n p ∥ ∆∥ 2 , (B.5)</formula><p>where the last inequality is because</p><formula xml:id="formula_249">∥ ∆ S ∥ 1 ≤ s 1/2 η ∥ ∆ S ∥ 2 ≤ s 1/2 η ∥ ∆∥ 2 .</formula><p>Lower Bound for D( η, η * ): after establishing the upper bound for the quadratic difference, we next obtain a lower bound for it. Because l(η * ) is convex function,</p><formula xml:id="formula_250">D( η, η * ) ≥ 0. We have ∥ ∆ S ∥ 1 ≤ 3∥ ∆ S ∥ 1 from (B.5</formula><p>), and based on this result and the restricted strong convexity condition for generalized linear model in Proposition 1 of <ref type="bibr" target="#b100">Loh and Wainwright (2015)</ref>,</p><formula xml:id="formula_251">D( η, η * ) ≥ κ 2 ∥ ∆∥ 2 2 , (B.6)</formula><p>for some constant κ 2 &gt; 0.</p><p>Then combining the upper bound (B.5) and the lower bound (B.6) for D( η, η * ), we have</p><formula xml:id="formula_252">∥ ∆∥ 2 ≤ 3cs 1/2 η κ 2 log p n + log n p ,<label>(B.7)</label></formula><p>and so</p><formula xml:id="formula_253">∥ ∆∥ 1 ≤ ∥ ∆ S ∥ 1 + ∥ ∆ S ∥ 1 ≤ 4∥ ∆ S ∥ 1 ≤ 4s 1/2 η ∥ ∆ S ∥ 2 ≤ 4s 1/2 η ∥ ∆∥ 2 ≤ 12cs η κ 2 log p n + log n p ,</formula><p>which completes the first part proof.</p><formula xml:id="formula_254">Write ( δ T Ṁi ) 2 = ( w T Ṁi ) 2 -{(w * ) T Ṁi } 2 -2 w T Ṁi Ṁ T i w * + 2{(w * ) T</formula><p>Ṁi } 2 , then the above inequality can be rearranged as</p><formula xml:id="formula_255">1 2n n i=1 b ′′ ( η T Żi )( δ T Ṁi ) 2 ≤ 1 n n i=1 b ′′ ( η T Żi ){D i -(w * ) T Ṁi } Ṁ T i δ + λ ′ ∥w * ∥ 1 -λ ′ ∥ w∥ 1 . (B.11)</formula><p>The proof techniques are mostly motivated by <ref type="bibr" target="#b109">Ning and Liu (2017)</ref>. To present our proof, we define two quadratic difference terms as</p><formula xml:id="formula_256">Q( w, w * ) = ( w -w * ) T ∇ 2 l( η)( w - w * ) = n -1 n i=1 b ′′ ( η T Żi )( Ṁ T i δ) 2 and Q * ( w, w * ) = ( w -w * ) T ∇ 2 l(η * )( w -w * ) = n -1 n i=1 b ′′ {(η * ) T Żi }( Ṁ T i δ) 2 .</formula><p>The left hand side of the above inequality is Q( w, w * ), and next we investigate the upper bound for Q( w, w * ) in details.</p><p>For the right hand side of (B.11), we first consider λ ′ ∥w * ∥ 1 -λ ′ ∥ w∥ 1 . Recall that we denote the support set for w * as S w = {j : w * j ̸ = 0}. We also denote w * S = (w * j :</p><p>j ∈ S w ), w * S = (w * j : j / ∈ S w ), δ S = ( w j -w * j : j ∈ S w ) and δ S = ( w j -w * j : j / ∈ S w ).</p><p>So ∥ δ S ∥ 1 = ∥ w S ∥ 1 and ∥w * S ∥ 1 = 0. Therefore we have</p><formula xml:id="formula_257">λ ′ ∥w * ∥ 1 -λ ′ ∥ w∥ 1 = λ ′ ∥w * S ∥ 1 + λ ′ ∥w * S ∥ 1 -λ ′ ∥ w S ∥ 1 -λ ′ ∥ w S ∥ 1 = λ ′ ∥w * S ∥ 1 -λ ′ ∥ w S ∥ 1 -λ ′ ∥ w S ∥ 1 ≤ λ ′ ∥ δ S ∥ 1 -λ ′ ∥ δ S ∥ 1 . (B.12)</formula><p>And according to Lagranian duality theory, an equivalent problem for (B.4) is</p><formula xml:id="formula_258">w = argmin w ∥w∥ 1 s.t. 1 2n n i=1 {w T ∇ ζζ ℓ i ( η)w -2w T ∇ ζθ ℓ i ( η)} ≤ b 2 , for b &gt; 0. This gives ∥ w∥ 1 ≤ ∥w * ∥ 1 , which further results in ∥ δ S ∥ 1 ≤ ∥ δ S ∥ 1 from (B.12).</formula><p>We next consider the first term in (B.11). Denote</p><formula xml:id="formula_259">I 1 = n -1 n i=1 b ′′ ( η T Żi ){D i - (w * ) T</formula><p>Ṁi } Ṁ T i δ, which is a summation of two terms I 11 and I 12 denoted as</p><formula xml:id="formula_260">I 1 = 1 n n i=1 b ′′ {(η * ) T Żi }{D i -(w * ) T Ṁi } Ṁ T i δ + 1 n n i=1 [b ′′ ( η T Żi ) -b ′′ {(η * ) T Żi }]{D i -(w * ) T Ṁi } Ṁ T i δ = I 11 + I 12 .</formula><p>For I 11 , using a similar argument as in the proof of Lemma B.1, we have</p><formula xml:id="formula_261">∥n -1 n i=1 b ′′ {(η * ) T Żi } {D i -(w * ) T Ṁi } Ṁ T i ∥ ∞ ≲ n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 , hence |I 11 | ≤ ∥ δ∥ 1 ∥n -1 n i=1 b ′′ {(η * ) T Żi }{D i -(w * ) T Ṁi } Ṁ T i ∥ ∞ ≲ log p n + log n p ∥ δ∥ 1 . (B.13) For I 12 , by Assumption III.5 that |b ′′ (t 1 ) -b ′′ (t)| ≤ B|t 1 -t|b ′′ (t) with t 1 = η T Żi and t = (η * ) T</formula><p>Żi and applying Cauchy-Schwarz inequality, we have</p><formula xml:id="formula_262">|I 12 | ≤ n -1 n i=1 b ′′ {(η * ) T Żi }| η T Żi -(η * ) T Żi |{D i -(w * ) T Ṁi } Ṁ T i δ ≤ n -1 n i=1 b ′′ {(η * ) T Żi }( Ṁ T i δ) 2 1/2 n -1 n i=1 b ′′ {(η * ) T Żi }{( η -η * ) T Żi } 2 1/2 ≲ |Q * ( w, w * )| 1/2 s 1/2 η log p n + log n p , (B.14)</formula><p>where the last inequality is from (B.8) in Lemma B.3.</p><p>Combining the upper bounds in (B.12), (B.13) and (B.14) into (B.11), we have</p><formula xml:id="formula_263">Q( w, w * ) ≲ log p n + log n p ∥ δ∥ 1 +|Q * ( w, w * )| 1/2 s 1/2 η log p n + log n p +λ ′ ∥ δ S ∥ 1 -λ ′ ∥ δ S ∥ 1 . (B.15)</formula><p>The above upper bound of Q( w, w * ) involves the expression of Q * ( w, w * ). We next investigate the relation between Q( w, w * ) and Q * ( w, w * ). We apply Assumption III.5 again on the difference between Q( w, w * ) and Q * ( w, w * ) as we do for bounding the term I 12 , and have</p><formula xml:id="formula_264">|Q( w, w * ) -Q * ( w, w * )| = 1 n n i=1 [b ′′ ( η T Żi ) -b ′′ {(η * ) T Żi }]( Ṁ T i δ) 2 ≤ 1 n n i=1 b ′′ {(η * ) T Żi }( Ṁ T i δ) 2 ( η -η * ) T Żi ≤ Q * ( w, w * )∥ η -η * ∥ 1 max i=1,...,n ∥ Żi ∥ ∞ ≲ Q * ( w, w * )s η log p n + log n p M + 1 √ n + log n p ,</formula><p>where the last inequality is by the estimation consistency results of η in Appendix D.1 and by Proposition III.</p><formula xml:id="formula_265">6 that ∥ Żi ∥ ∞ = M + O p (n -1/2 + p -1/2 (log n) 1/2 ) and s η (n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 ) = o p (1).</formula><p>From the above result, we further apply triangular inequality and have</p><formula xml:id="formula_266">Q * ( w, w * ) 1 -s η log p n + log n p M + 1 √ n + log n p ≲ Q( w, w * ). (B.16)</formula><p>Combining the above result on Q * ( w, w * ) with (B.15) and taking λ ′ = 2{n -1/2</p><formula xml:id="formula_267">(log p) 1/2 +p -1/2 (log n) 1/2 }, we get Q( w, w * ) ≲ |Q( w, w * )| 1/2 s 1/2 η log p n + log n p + log p n + log n p (3∥ δ S ∥ 1 -∥ δ S ∥ 1 ). (B.17)</formula><p>The above result will be used to prove (B.10), which will later be used to derive the error bound ∥ δ∥ 1 . Consider the following two cases.</p><p>Case 1:</p><formula xml:id="formula_268">|Q( w, w * )| 1/2 ≲ s 1/2 η (n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 ). We have (B.10) naturally hold. Case 2: |Q( w, w * )| 1/2 ≳ s 1/2 η (n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2</formula><p>). Then we can have</p><formula xml:id="formula_269">Q( w, w * ) -|Q( w, w * )| 1/2 s 1/2 η log p n + log n p &gt; 0.</formula><p>combining this result with (B.17), we have 3∥ δ S ∥ 1 ≥ ∥ δ S ∥ 1 . We next use this cone condition to derive the lower bound for Q( w, w * ) for case 2.</p><p>Denote U T i = (0 p-1 , U T i -U T i ) p+K-1 as a vector including a vector of zeros and the differences between estimated unmeasured confounders and the true confounders.</p><p>Hence we have Ṁi = M i + U i . We establish the lower bounds for Q( w, w * )/∥ δ∥ 2 2 first.</p><formula xml:id="formula_270">Q( w, w * ) ∥ δ∥ 2 2 = δ T {n -1 n i=1 b ′′ ( η T Żi ) Ṁi Ṁ T i } δ ∥ δ∥ 2 2 = δ T {n -1 n i=1 b ′′ ( η T Żi )M i M T i } δ ∥ δ∥ 2 2 + 2 δ T {n -1 n i=1 b ′′ ( η T Żi )M i U T i } δ ∥ δ∥ 2 2 + δ T {n -1 n i=1 b ′′ ( η T Żi ) U i U T i } δ ∥ δ∥ 2 2 = Q 1 + Q 2 + Q 3 . (B.18)</formula><p>We next derive the bounds for the three terms in (B.18) separately. For Q 1 , we have </p><formula xml:id="formula_271">Q 1 = δ T [n -1 n i=1 b ′′ {(η * ) T Z i }M i M T i ] δ ∥ δ∥ 2 2 + 1 n n i=1 (M T i δ) 2 ∥ δ∥ 2 2 [b ′′ ( η T Żi ) -b ′′ {(η * ) T Z i }]. (B.</formula><formula xml:id="formula_272">|b ′′ ( η T Żi ) -b ′′ {(η * ) T Z i }| ≤ max i=1,...,n Bb ′′ {(η * ) T Z i }| η T Żi -(η * ) T Z i | ≤ max i=1,...,n Bb ′′ {(η * ) T Z i }| η T Żi -(η * ) T Żi + (η * ) T Żi -(η * ) T Z i | ≤ max i=1,...,n Bb ′′ {(η * ) T Z i }|( η -η * ) T Żi + (η * ) T ( Żi -Z i )|. ≤ max i=1,...,n Bb ′′ {(η * ) T Z i }(∥ η -η * ∥ 1 ∥ Żi ∥ ∞ + ∥η * ∥ 1 ∥ Żi -Z i ∥ ∞ ). (B.20) From Assumption III.5, b ′′ {(η * ) T Z i } ∈ [0, B].</formula><p>From the result for estimation consis-</p><formula xml:id="formula_273">tency of η in Appendix D.1, we have ∥ η -η * ∥ 1 ≲ s η (n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 ).</formula><p>In addition, we have</p><formula xml:id="formula_274">∥ Żi ∥ ∞ = M + O p (n -1/2 + p -1/2 (log n) 1/2</formula><p>). Based on these results and by the scaling condition n, p → ∞ and s η (n -1/2 (log p)</p><formula xml:id="formula_275">1/2 +p -1/2 (log n) 1/2 ) = o p (1), we have max i=1,...,n |b ′′ ( η T Żi ) -b ′′ {(η * ) T Z i }| = o p (1). (B.21)</formula><p>Hence we have</p><formula xml:id="formula_276">Q 1 ≥ δ T [n -1 n i=1 b ′′ {(η * ) T Z i }M i M T i ] δ 4∥ δ∥ 2 2 (B.22)</formula><p>with probability tending to 1 as the second term in (B.19) goes to 0. Recall that we denote</p><formula xml:id="formula_277">I * ζζ = E[b ′′ {(η * ) T Z i }M i M T i ]. Because δ T [n -1 n i=1 b ′′ {(η * ) T Z i }M i M T i ] δ ∥ δ∥ 2 2 = δ T I * ζζ δ ∥ δ∥ 2 2 + δ T |n -1 n i=1 b ′′ {(η * ) T Z i }M i M T i -I * ζζ | δ ∥ δ∥ 2 2 ≥ λ min (I * ζζ ) - δ T [n -1 n i=1 b ′′ {(η * ) T Z i }M i M T i -I * ζζ ] δ ∥ δ∥ 2 2 ≥ κ - ∥ δ∥ 2 1 ∥n -1 n i=1 b ′′ {(η * ) T Z i }M i M T i -I * ζζ ∥ max ∥ δ∥ 2 2 ,</formula><p>where the last inequality is by the definition and properties of eigenvalue and matrix operations. Recall that we have obtained the cone condition 3∥</p><formula xml:id="formula_278">δ S ∥ 1 ≥ ∥ δ S ∥ 1 , which results in ∥ δ∥ 2 1 ≤ 16∥ δ S ∥ 2 1 ≤ 16s w ∥ δ∥ 2 2 . Moreover, using Bernstein's inequality, it can be shown that ∥n -1 n i=1 b ′′ {(η * ) T Z i } M i M T i -I * ζζ ∥ max = O p {n -1/2 (log p) 1/2 }. As (s η ∨ s w )(n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 ) = o p (1), the term ∥ δ∥ 2 1 ∥n -1 n i=1 b ′′ {(η * ) T Z i }M i M T i -I ζζ ∥ max /∥ δ∥ 2 2 = o p (1)</formula><p>, which together with (B.22) shows Q 1 ≥ κ/4 with probability tending to 1.</p><p>For Q 2 , we can write it as</p><formula xml:id="formula_279">Q 2 = 2 δ T [n -1 n i=1 b ′′ ( η T Żi )M i U T i ] δ ∥ δ∥ 2 2 + 2 n n i=1 δ T M i U T i δ ∥ δ∥ 2 2 [b ′′ ( η T Żi ) -b ′′ {(η * ) T Z i }]. (B.23)</formula><p>Based on (B.21) and</p><formula xml:id="formula_280">δ T M i U T i δ ∥ δ∥ 2 2 ≤ ∥ δ∥ 2 1 ∥M i ∥ ∞ ∥ U i ∥ ∞ ∥ δ∥ 2 2 ≲ s w 1 √ n + log n p ,</formula><p>and under the scaling condition (</p><formula xml:id="formula_281">s η ∨ s w )(n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 ) = o p (1),</formula><p>we have the second term in (B.23) goes to 0 and thus</p><formula xml:id="formula_282">Q 2 ≥ δ T [n -1 n i=1 b ′′ {(η * ) T Z i }M i U T i ] δ 2∥ δ∥ 2 2 ≥ λ min (I * ζζ ) 2 - δ T [n -1 n i=1 b ′′ {(η * ) T Z i }M i U T i -I * ζζ ] δ 2∥ δ∥ 2 2 ≥ κ 2 - ∥ δ∥ 2 1 ∥n -1 n i=1 b ′′ {(η * ) T Z i }M i U T i -I * ζζ ∥ max ∥ δ∥ 2 2 .</formula><p>(B.24)</p><p>Because we have</p><formula xml:id="formula_283">∥n -1 n i=1 b ′′ {(η * ) T Z i }M i U T i -I * ζζ ∥ max ≤ ∥n -1 n i=1 b ′′ {(η * ) T Z i }M i U T i -E[b ′′ {(η * ) T Z i }M i U T i ]∥ max +∥E[b ′′ {(η * ) T Z i }M i U T i ] -I * ζζ ∥ max . ≲ log p n + log n p , (B.25)</formula><p>where the last inequality is from techniques similar to the proof of Lemma B.1 Condi-</p><formula xml:id="formula_284">tion (ii) that ∥n -1 n i=1 b ′′ {(η * ) T Z i }M i U T i -E[b ′′ {(η * ) T Z i }M i U T i ]∥ max = O p {n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 } and from Proposition III.6 that ∥E[b ′′ {(η * ) T Z i }M i U T i ] - I * ζζ ∥ max = O p {n -1/2 + p -1/2 (log n) 1/2 }. Because ∥ δ∥ 2 1 ≤ 16s w ∥ δ∥ 2 2 , under the scal- ing condition (s η ∨ s w ){n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 } = o p (1), combining (B.24)</formula><p>and (B.25) gives Q 2 ≥ κ/2 with probability tending to 1. Similarly, we have</p><formula xml:id="formula_285">Q 3 ≥ κ/4.</formula><p>Summarizing the above results, we have that with a high probability</p><formula xml:id="formula_286">Q 1 + Q 2 + Q 3 ≥ κ as n, p → ∞ and (s η ∨ s w ){n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 } = o p (1). Sub- stituting this result for the terms Q 1 , Q 2 and Q 3 into (B.18), and because ∥ δ S ∥ 2 1 ≤ s w ∥ δ S ∥ 2 2 ≤ s w ∥ δ∥ 2 2 , we have Q( w, w * ) ≥ κs -1 w ∥ δ S ∥ 2 1 . (B.26)</formula><p>Recall we have proved (B.17), which can be written as</p><formula xml:id="formula_287">Q( w, w * ) 1/2 Q( w, w * ) 1/2 -s 1/2 η log p n + log n p 1/2 ≲ log p n + log n p 1/2 (3∥ δ S ∥ 1 -∥ δ S ∥ 1 ) ≲ 3 log p n + log n p 1/2 ∥ δ S ∥ 1 .</formula><p>From (B.26), we have ∥ δ S ∥ 2 1 ≤ s w κ -1 Q( w, w * ) with high probability tending to 1.</p><p>Substitute this result into the above inequality, we have</p><formula xml:id="formula_288">Q( w, w * ) 1/2 Q( w, w * ) 1/2 -s 1/2 η log p n + log n p 1/2 ≲ 3s 1/2 w κ 1/2 log p n + log n p 1/2 Q( w, w * ) 1/2 .</formula><p>Cancelling the term Q( w, w * ) 1/2 , we have</p><formula xml:id="formula_289">Q( w, w * ) 1/2 -s 1/2 η log p n + log n p 1/2 ≲ 3s 1/2 w κ 1/2 log p n + log n p 1/2</formula><p>, which gives an upper bound for</p><formula xml:id="formula_290">Q( w, w * ) 1/2 that Q( w, w * ) 1/2 ≲ (s w ∨ s η ) 1/2 log p n + log n p 1/2 . (B.27)</formula><p>which completes the proof of (B.10) in case 2. Therefore, we prove (B.10) holds in both cases. In both cases 1 and 2, by replacing Q( w, w * ) with Q * ( w, w * ), we get (B.9) as</p><formula xml:id="formula_291">Q * ( w, w * ) = O p (s w ∨ s η ) log p n + log n p .</formula><p>To finally prove the estimation consistency of w, or equivalently, to derive the error bound ∥ δ∥ 1 , we also consider two situations. If cone condition 6∥ δ S ∥ 1 ≥ ∥ δ S ∥ 1 holds, we then apply similar techniques as in case 2 to derive (B.26). Therefore</p><formula xml:id="formula_292">∥ δ∥ 1 ≤ ∥ δ S ∥ 1 + ∥ δ S ∥ 1 ≤ 7∥ δ S ∥ 1 ≲ s 1/2 w Q( w, w * ) 1/2 ≲ s 1/2 w (s η ∨ s w ) 1/2 log p n + log n p 1/2 ≲ (s η ∨ s w ) log p n + log n p 1/2</formula><p>.</p><p>Otherwise, we have 6∥ δ S ∥ 1 ≤ ∥ δ S ∥ 1 . From (B.17), we have</p><formula xml:id="formula_293">Q( w, w * ) ≲ |Q( w, w * )| 1/2 s 1/2 η log p n + log n p + log p n + log n p (3∥ δ S ∥ 1 -∥ δ S ∥ 1 ). ≲ |Q( w, w * )| 1/2 s 1/2 η log p n + log n p - 1 2 log p n + log n p ∥ δ S ∥ 1 , which together with 6∥ δ S ∥ 1 ≤ ∥ δ S ∥ 1 gives ∥ δ∥ 1 ≤ 7 6 ∥ δ S ∥ 1 ≤ 7 3 log p n + log n p -1 |Q( w, w * )| 1/2 s 1/2 η log p n + log n p -Q( w, w * ) ≲ (s η ∨ s w ) log p n + log n p 1/2</formula><p>, where the last inequality is from (B.10).</p><p>In conclusion, under either 6∥</p><formula xml:id="formula_294">δ S ∥ 1 ≥ ∥ δ S ∥ 1 or 6∥ δ S ∥ 1 ≤ ∥ δ S ∥ 1 , we prove ∥ δ∥ 1 = O p (s w ∨ s η ) log p n + log n p 1/2 .</formula><p>B.5 Proof of Theorem III.8</p><p>Throughout the rest of appendix, we denote</p><formula xml:id="formula_295">H = ( W Σ -1 e W T ) -1 , H p = p × H = (p -1 W Σ -1 e W T ) -1 .</formula><p>From the Corollary S.1 in <ref type="bibr" target="#b8">Bai and Li (2016)</ref>, we have H p = O p (1) and H = O p (p -1 ).</p><p>These results will play an important role in the following proofs. We next introduce a few technical lemmas as tools in the proof of Theorem III.8.</p><p>Lemma B.5 (Smoothness of loss function). Suppose that Assumptions 1-III.5 hold.</p><p>With λ ≍ λ ′ ≍ n -1/2 (log p) 1/2 +p -1/2 (log n) 1/2 and (s w ∨s η ) (n -1/2 log p+p -1 n 1/2 log n)</p><p>= o p (1), the following conditions hold.</p><formula xml:id="formula_296">(iii) (τ * ) T {∇l( η) -∇l(η * ) -∇ 2 l(η * )( η -η * )} = o p (n -1/2 ); (iv) ( τ -τ * ) T {∇l( η) -∇l(η * )} = o p (n -1/2 ).</formula><p>Lemma B.5 shows the loss functions are smooth in the sense that they are secondorder differentiable around the true parameter value. These conditions hold for quadratic loss functions and non-quadratic functions given the function b(•) is properly constrained.</p><p>Lemma B.6 (Central limit theorem of score function). Under Assumptions 1-III.5,</p><formula xml:id="formula_297">with λ ≍ λ ′ ≍ n -1/2 (log p) 1/2 +p -1/2 (log n) 1/2 and (s w ∨s η ) (n -1/2 log p+p -1 n 1/2 log n) = o p (1), it holds that n 1/2 (τ * ) T ∇l(η * )(I * θ|ζ ) -1/2 → d N (0, 1). (B.28)</formula><p>Lemma B.6 implies that a linear combination of the gradient of loss function, in other words, the score function is asymptotically normal.</p><p>Lemma B.7 (Partial information estimator consistency). Suppose that Assumptions</p><formula xml:id="formula_298">1-III.5 hold. With λ ≍ λ ′ ≍ n -1/2 (log p) 1/2 +p -1/2 (log n) 1/2 , if n, p → ∞ and (s w ∨s η ) (n -1/2 log p + p -1 n 1/2 log n) = o p (1)</formula><p>, then the estimator for the partial information</p><formula xml:id="formula_299">I θ|ζ = ∇ 2 θθ l( η) -w∇ 2 ζθ l( η) is consistent, I θ|ζ -I * θ|ζ = o p (1).</formula><p>With these result, we now prove the asymptotic normality of the debiased estimator, which generalizes the proof of Theorem 3.2 in <ref type="bibr" target="#b109">Ning and Liu (2017)</ref> to the setting with unmeasured confounders.</p><p>The goal is to show the debiased estimator θ = θ -I -1 θ|ζ S( η) is asymptotically normal. First, by Lemma B.6, we have (B.28) hold. Therefore, it suffices to show that</p><formula xml:id="formula_300">n 1/2 |( θ -θ * )(I * θ|ζ ) 1/2 + (τ * ) T ∇l(η * )(I * θ|ζ ) -1/2 | = o p (1).</formula><p>which is equivalent to show</p><formula xml:id="formula_301">n 1/2 |( θ -θ * )I * θ|ζ + (τ * ) T ∇l(η * )| = o p (1).</formula><p>since I * θ|ζ is constant. Note that we have S( η) = τ T ∇l( η) by the definition of estimated decorrelated score function, we next decompose the left hand side of the above expression and apply triangular inequality as</p><formula xml:id="formula_302">n 1/2 |{ θ -I -1 θ|ζ S( η) -θ * }I * θ|ζ + (τ * ) T ∇l(η * )| = n 1/2 |( θ -θ * )I * θ|ζ -(τ * ) T ∇l( η) + (τ * ) T ∇l( η) -τ T ∇l( η) + τ T ∇l( η) -I * θ|ζ I -1 θ|ζ τ T ∇l( η) + (τ * ) T ∇l(η * )| ≤ n 1/2 |( θ -θ * )I * θ|ζ -(τ * ) T {∇l( η) -∇l(η * )}| +n 1/2 |( τ -τ * ) T ∇l( η)| + n 1/2 |(I * θ|ζ I -1 θ|ζ -1) τ T ∇l( η)| =: I 1 + I 2 + I 3 .</formula><p>By an application of Lemma B.5 condition (iii), we write I 1 as</p><formula xml:id="formula_303">I 1 = n 1/2 |( θ -θ * )I * θ|ζ -(τ * ) T {∇l( η) -∇l(η * )}| ≤ n 1/2 |( θ -θ * )I * θ|ζ -(τ * ) T ∇ 2 l(η * )( η -η * )| + o p (1) ≤ n 1/2 |( θ -θ * )I * θ|ζ -{∇ 2 θθ l(η * ) -(w * ) T ∇ 2 ζθ l(η * )}( θ -θ * ) -{∇ 2 θζ l(η * ) -w T ∇ 2 ζζ l(η * )}( ζ -ζ * )| + o p (1) ≤ n 1/2 ∥ η -η * ∥ 1 ∥T ∥ ∞ + o p (1)</formula><p>where the last inequality is by Hölder inequality with</p><formula xml:id="formula_304">T = {I * θ|ζ -∇ 2 θθ l(η * ) + (w * ) T ∇ 2 ζθ l(η * ), ∇ 2 θζ l(η * ) -(w * ) T ∇ 2 ζζ l(η * )} T . As a consequence of Lemma B.1 condition (ii), we have ∥T ∥ ∞ = O p {n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 }. In addition, Theorem III.7 gives ∥ η -η * ∥ 1 = O p {s η (n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 )}. Hence under the scaling condition of (s w ∨ s η ) (n -1/2 log p + p -1 n 1/2 log n) = o p (1), we have I 1 ≲ √ ns η log p n + log n p + o p (1) = o p (1). (B.29)</formula><p>By an application of Lemma B.5 condition (iv), we write I 2 as</p><formula xml:id="formula_305">I 2 = n 1/2 |( τ -τ * ) T ∇l( η)| ≤ n 1/2 |( τ -τ * ) T ∇l(η * )| + o p (1) ≤ n 1/2 ∥ τ -τ * ∥ 1 ∥∇l(η * )∥ ∞ + o p (1). (B.30) From Theorem III.7, we have ∥ τ -τ * ∥ 1 = ∥ w -w * ∥ 1 = O p {(s w ∨s η )(n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 )}. From Lemma B.1 Condition (i), we have ∥∇l(η * )∥ ∞ = O p {n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 }. Hence under the condition of (s w ∨ s η ) (n -1/2 log p + p -1 n 1/2 log n) = o p (1)</formula><p>, we have</p><formula xml:id="formula_306">I 2 ≲ √ n(s w ∨ s η ) log p n + log n p + o p (1) = o p (1). (B.31) For I 3 , since I θ|ζ -I * θ|ζ = o p (1) from Lemma B.7, we have I * θ|ζ I -1 θ|ζ = O p (1)</formula><p>. From (B.30) and τ * being fixed, the term</p><formula xml:id="formula_307">I 2 = o p (1) implies that n 1/2 | τ T ∇l( η)| = o p (1).</formula><p>Hence we have</p><formula xml:id="formula_308">I 3 = n 1/2 |(I * θ|ζ I -1 θ|ζ -1) τ T ∇l( η)| = o p (1). (B.32)</formula><p>Combining (B.29), (B.31) and (B.32), we obtain</p><formula xml:id="formula_309">n 1/2 |( θ -θ * )I * θ|ζ + (τ * ) T ∇l(η * )| ≤ I 1 + I 2 + I 3 = o p (1),</formula><p>which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Proof of Proposition III.6</head><p>To prove Proposition III.6, we show that the estimated unmeasured confounders are bounded with convergence guarantees. From the expression of the estimator for unmeasured confounders in (3.4) of the main text and recall that H = ( W Σ -1 e W T ) -1</p><p>and H p = p × H = (p -1 W Σ -1 e W T ) -1 , we apply triangular inequality and have max i=1,...,n</p><formula xml:id="formula_310">∥ U i -U i ∥ ∞ = max i=1,...,n ∥ H W Σ -1 e {(W * -W ) T U i + E i }∥ ∞ ≤ max i=1,...,n ∥ H W Σ -1 e (W * -W ) T U i ∥ ∞ + max i=1,...,n ∥ H W Σ -1 e E i ∥ ∞ . (B.33)</formula><p>For the first term in (B.33), we have</p><formula xml:id="formula_311">max i=1,...,n ∥ H W Σ -1 e (W * -W ) T U i ∥ ∞ ≤ ∥ H W Σ -1 e (W * -W ) T ∥ 1,∞ max i=1,...,n ∥U i ∥ 1 ≤ √ K∥ H W Σ -1 e (W * -W ) T ∥ F max i=1,...,n K∥U i ∥ ∞ .</formula><p>From Assumption III.5, we have ∥U i ∥ ∞ ≤ M for all i = 1, . . . , n and some constant M &gt; 0. For the norm ∥ H W Σ -1 e (W * -W ) T ∥ F , we apply the Cauchy-Schwarz inequality to bound the matrix norm</p><formula xml:id="formula_312">H W Σ -1 e (W * -W ) T F = H p 1 p p j=1 1 σ 2 j W j (W * j -W j ) T F ≤ ∥ H p ∥ F 1 p p j=1 1 σ 2 j ∥ W j ∥ 2 2 1/2 1 p p j=1 1 σ 2 j ∥W * j -W j ∥ 2 2 1/2 = O p 1 √ n + 1 p</formula><p>where the last equality follows because we have <ref type="bibr" target="#b8">Bai and Li (2016)</ref>. Therefore max i=1,...,n</p><formula xml:id="formula_313">∥ H p ∥ F = O p (1) from Corollary S.1(b), (p -1 p j=1 σ -2 j ∥ W j ∥ 2 2 ) 1/2 = O p (1) from Corollary S.1(a) and (p -1 p j=1 σ -2 j ∥W * j - W j ∥ 2 2 ) 1/2 = O p (n -1/2 + p -1 ) from Proposition 1 in</formula><formula xml:id="formula_314">∥ H W Σ -1 e (W * -W ) T U i ∥ ∞ = O p 1 √ n + 1 p . (B.34)</formula><p>For the second term in (B.33), we have max i=1,...,n</p><formula xml:id="formula_315">∥ H W Σ -1 e E i ∥ ∞ ≤ ∥ H p ∥ 1,∞ max i=1,...,n ∥p -1 W Σ -1 e E i ∥ 1 ≤ √ K∥ H p ∥ F max i=1,...,n K∥p -1 W Σ -1 e E i ∥ ∞ . (B.35)</formula><p>For p -1 W Σ -1 e E i , we express the K-dimensional vector as</p><formula xml:id="formula_316">1 p p j=1 σ -2 j W j E ij = 1 p p j=1 (σ * j ) -2 W * j E ij + 1 p p j=1 1 σ 2 j - 1 (σ * j ) 2 W * j E ij + 1 p p j=1 1 σ 2 j ( W j -W j )E ij . =: R 1 + R 2 + R 3 . (B.36)</formula><p>We next investigate the bound for the term</p><formula xml:id="formula_317">R 1 = p -1 p j=1 (σ * j ) -2 W * j E ij and then</formula><p>show the other two terms R 2 and R 3 are of smaller order and dominated by R 1 . From Assumption III.5, we have</p><formula xml:id="formula_318">∥U i ∥ ∞ ≤ M , ∥W * j ∥ 2 ≤ C and ∥X i ∥ ∞ ≤ M .</formula><p>We apply triangular inequality and have</p><formula xml:id="formula_319">∥E i ∥ ∞ = ∥X i -(W * ) T U i ∥ ∞ ≤ ∥X i ∥ ∞ + ∥(W * ) T U i ∥ ∞ ≤ M + max j=1,...,p ∥W * j ∥ 1 ∥U i ∥ 1 ≤ M + max j=1,...,p √ K∥W * j ∥ 2 K∥U i ∥ ∞ ≤ M (1 + CK 3/2 ),</formula><p>where the last three inequalities follow from matrix operation and properties of norms.</p><p>We have E ij to be sub-exponential random variable since E ij is bounded with</p><formula xml:id="formula_320">|E ij | ≤ M (1 + CK 3/2 ). Combining the bound for E ij with that |W * jk | ≤ ∥W * j ∥ ∞ ≤ ∥W * j ∥ 2 ≤ C and C -2 ≤ (σ * j ) -2 ≤ C 2 , we have |(σ * j ) -2 W * jk E ij | ≤ M C 3 (1 + CK 3/2 ), for j = 1, . . . , p and k = 1, . . . , K. So (σ * j ) -2 W * jk E ij is sub-Gaussian random variable</formula><p>and thus is sub-exponential. As E ij has mean zero, (σ * j ) -<ref type="foot" target="#foot_1">foot_1</ref> W * jk E ij has mean zero and by Bernstein inequality</p><formula xml:id="formula_321">P 1 p p j=1 (σ * j ) -2 W * jk E ij ≥ t ≤ 2 exp -C ′′ min t 2 M 2 C 6 (1 + CK 3/2 ) 2 , t M C 3 (1 + CK 3/2 ) p .</formula><p>Apply union bound inequality, we have</p><formula xml:id="formula_322">P   max i=1,...,n 1 p p j=1 (σ * j ) -2 W * j E ij ∞ ≥ t   ≤ 2nK exp -C ′′ min t 2 M 2 C 6 (1 + CK 3/2 ) 2 , t M C 3 (1 + CK 3/2 ) p ,</formula><p>where</p><formula xml:id="formula_323">C ′′ &gt; 0 is a constant. At t = M C 3 (1 + CK 3/2 )p -1/2 (log n) 1/2 , the inequality max i=1,...,n 1 p p j=1 (σ * j ) -2 W * j E ij ∞ ≤ M C 3 (1 + CK 3/2 ) log n p 1/2 , (B.37)</formula><p>holds with probability at least 1 -n -1 .</p><p>For the term R 2 , we apply Cauchy-Schwarz inequality and have</p><formula xml:id="formula_324">1 p p j=1 1 σ 2 j - 1 (σ * j ) 2 W * j E ij 2 ≤ 1 p p j=1 1 σ 2 j - 1 (σ * j ) 2 2 1/2 1 p p j=1 E ij (W * j ) ⊺ W * j E ij 1/2</formula><p>.</p><p>By Assumption 1(b), σ -2 j and (σ * j ) -2 are bounded in [C -2 , C 2 ] and as a result, we have 1 p</p><formula xml:id="formula_325">p j=1 1 σ 2 j - 1 (σ * j ) 2 and max i=1,...,n 1 p p j=1 1 σ 2 j - 1 (σ * j ) 2 W * j E ij ∞ ≤ max i=1,...,n 1 p p j=1 1 σ 2 j - 1 (σ * j ) 2 W * j E ij 2 ≤ 2C 2 max i=1,...,n 1 p p j=1 E ij (W * j ) ⊺ W * j E ij 1/2 . Because |W * jk W * jk ′ E 2 ij | ≤ M 2 C 2 (1 + CK 3/2</formula><p>) 2 and by Berstein inequality,</p><formula xml:id="formula_326">P 1 p p j=1 W * jk W * jk ′ E 2 ij ≥ t ≤ 2 exp -C ′′ min t 2 M 4 C 4 (1 + CK 3/2 ) 4 , t M 2 C 2 (1 + CK 3/2 ) 2 p .</formula><p>and then applying union bound gives</p><formula xml:id="formula_327">P max i=1,...,n 1 p p j=1 E ij (W * j ) ⊺ W * j E ij ≥ t ≤ 2nK 2 exp -C ′′ min t 2 M 4 C 4 (1 + CK 3/2 ) 4 , t M 2 C 2 (1 + CK 3/2 ) 2 p .</formula><p>From the above probability bound, we have max i=1,...,n {p</p><formula xml:id="formula_328">-1 p j=1 E ij (W * j ) ⊺ W * j E ij } = O p (p -1/2 (log n) 1/2 ) and max i=1,...,n 1 p p j=1 1 σ 2 j - 1 (σ * j ) 2 W * j E ij ∞ = O p log n p 1/4 (B.38)</formula><p>For the term R 3 , we apply Cauchy-Schwarz inequality and get</p><formula xml:id="formula_329">1 p p j=1 1 σ 2 j ( W j -W j )E ij 2 ≤ 1 p p j=1 1 σ 2 j ∥ W j -W j ∥ 2 2 1/2 1 p p j=1 1 σ 2 j E 2 ij 1/2 . (B.39)</formula><p>By Proposition 1 in <ref type="bibr" target="#b8">Bai and Li (2016)</ref>, the first term</p><formula xml:id="formula_330">1 p p j=1 1 σ 2 j ∥ W j -W j ∥ 2 2 1/2 = O p 1 √ n + 1 p .</formula><p>By a similar technique as in bounding R 1 and R 2 using the combination of Bernstein inequality and union bound, we can show max i=1,...,n (p</p><formula xml:id="formula_331">-1 p j=1 σ -2 j E 2 ij ) 1/2 = O p (p -1/4 (log n) 1/4</formula><p>), and therefore max i=1,...,n </p><formula xml:id="formula_332">1 p p j=1 1 σ 2 j ( W j -W j )E ij ∞ ≤ max i=1,...,n 1 p p j=1 1 σ 2 j ( W j -W j )E ij 2 ≤ 1 p p j=1 1 σ 2 j ∥ W j -W j ∥ 2 2 1/2 max i=1,...,n 1 p p j=1 1 σ 2 j E 2 ij 1/2 = O p (log n) 1/4 p 1/4 1 √ n + 1 p (B.</formula><formula xml:id="formula_333">1 p p j=1 σ -2 j W j E ij ∞ = O p log n p .</formula><p>In addition, from Corollary S.1 (a) in <ref type="bibr" target="#b8">Bai and Li (2016)</ref>, we have  </p><formula xml:id="formula_334">H p = (Γ * ) -1 + o p (1) with ∥(Γ * ) -1 ∥ sp = λ 1/2 max {(Γ * ) -T (Γ * ) -1 } = λ max {(Γ * ) -1 } being finite constant as (Γ * ) -</formula><formula xml:id="formula_335">∥ H W Σ -1 e E i ∥ ∞ ≲ K 3/2 λ max {(Γ * ) -1 }O p log n p (B.</formula><formula xml:id="formula_336">∥ U i -U i ∥ ∞ = O p 1 √ n + log n p . B.7 Proofs of Lemmas B.7.1 Proof of Lemma B.1</formula><p>Proof of Condition (i). To prove the Condition (i) in Lemma B.1, we need to show that</p><formula xml:id="formula_337">- 1 n n i=1 [y i -b ′ {(η * ) T Żi }] Żi ∞ = O p log p n 1/2 + log n p 1/2</formula><p>.</p><p>From Assumption III.5, we have</p><formula xml:id="formula_338">y i -b ′ {(η * ) T Z i } to be sub-exponential variable with mean 0 and ∥y i -b ′ {(η * ) T Z i }∥ φ 1 ≤ M . In addition, it is assumed that a 1 ≤ (η * ) T Z i ≤ a 2 and 0 ≤ b ′ {(η * ) T Z i } ≤ B. Denote max i ∥ U i -U * i ∥ ∞ = ∥ Żi -Z i ∥ ∞ = ϵ. From Proposition III.6, ϵ = O p (n -1/2 + p -1/2 (log n) 1/2 ). Since ∥ Żi -Z i ∥ ∞ = ϵ, it can be shown that |(η * ) T (Z i -Żi )| ≤ a 2 + s η M ϵ.</formula><p>To prove condition (i), we focus on finding the appropriate sequence of t such that variables and ∥y</p><formula xml:id="formula_339">i -b ′ {(η * ) T Z i }]U ik ∥ φ 1 ≤ 2M 2</formula><p>, by Bernstein inequality, we have</p><formula xml:id="formula_340">P 1 n n i=1 |y i -b ′ {(η * ) T Z i }]U ik | ≥ t 1 -δ 1 ≤ 2 exp -M ′′ min (t 1 -δ 1 ) 2 4M 4 , t 1 -δ 1 2M 2 n .</formula><p>Then applying union bound inequality, we have</p><formula xml:id="formula_341">P 1 n n i=1 [y i -b ′ {(η * ) T Z i }]U i ∞ ≥ t 1 -δ 1 ≤ 2K exp -M ′′ min (t 1 -δ 1 ) 2 4M 4 , t 1 -δ 1 2M 2 n , so at t 1 -δ 1 = M 2 n -1/2</formula><p>, the probability P 1 tends to 0. Using similar techniques, we can show that at t 2 -δ 2 = M ϵn -1/2 , the probability P 3 tends to 0; at t 3 -δ 3 = M 2 n -1/2 (log p) 1/2 , the probability P 5 tends to 0. Hence at</p><formula xml:id="formula_342">t = max{t 1 + t 2 , t 3 } = max M 2 √ n + BM (a 2 + s η M ϵ) + M ϵ √ n + Bϵ(a 2 + s η M ϵ), M 2 log p n + BM (a 2 + s η M ϵ) ,</formula><p>the probability (B.42) tends to 0. As t 3 dominates t 1 + t 2 in the above expression, we eventually have</p><formula xml:id="formula_343">1 n n i=1 [y i -b ′ {(η * ) T Żi }] Żi ∞ = O p log p n + log n p ,</formula><p>which completes the proof for condition (i).</p><p>Proof of Condition (ii). To show condition (ii) in Lemma B.1 holds, we use a similar technique as in proving condition (i) and decomposing the probability as follows. Recall that τ * = (1, -(w * ) T ) T and the two sub-vectors of w * are denoted where we denote M c = 2BM K∥w * ∥ ∞ for notational simplicity. By Bernstein inequality, we have</p><formula xml:id="formula_344">P 1 n n i=1 (w * u ) T b ′′ {(η * ) T Z i } U i X ij -E η * [(w * u ) T b ′′ {(η * ) T Z i } U i X ij ] ≥ t 4 -δ 4 ≤ 2 exp -M ′′ min (t 4 -δ 4 ) 2 M 2 c (M + ϵ) 2 , t 4 -δ 4 M 2 c (M + ϵ) 2 n .</formula><p>Then applying union bound inequality, we have</p><formula xml:id="formula_345">P 1 n n i=1 (w * u ) T b ′′ {(η * ) T Z i } U i X i -E η * [(w * u ) T b ′′ {(η * ) T Z i } U i X i ] ∞ ≥ t 4 -δ 4 ≤ 2p exp -M ′′ min (t 4 -δ 4 ) 2 M 2 c (M + ϵ) 2 , t 4 -δ 4 M 2 c (M + ϵ) 2 n , so at t 4 -δ 4 = M c (M + ϵ)n -1/2 (log p) 1/2</formula><p>, the probability R 1 tends to 0. Using similar techniques, we can show that at t 5 -δ 5 = 2BK∥w * ∥ ∞ (M + ϵ) 2 n -1/2 , the probability R 3 tends to 0; at t 6 -δ 6 = 4BM (M + ϵ)n -1/2 , the probability R 5 tends to 0; at t 7 -δ 7 = 4BM 2 n -1/2 (log p) 1/2 , the probability R 7 tends to 0. Hence at</p><formula xml:id="formula_346">t = max{t 4 , t 5 , t 6 , t 7 } = max M c (M + ϵ)n -1/2 (log p) 1/2 + 2BM (a 2 + s η M ϵ){K∥w * ∥ ∞ (M + ϵ)}, 2BK∥w * ∥ ∞ (M + ϵ) 2 n -1/2 + 2B(M + ϵ)(a 2 + s η M ϵ){K∥w * ∥ ∞ (M + ϵ)}, 4BM (M + ϵ)n -1/2 + 4BM (M + ϵ)(a 2 + s η M ϵ), 4BM 2 n -1/2 (log p) 1/2 + 4BM 2 (a 2 + s η M ϵ) } ,</formula><p>the probability (B.43) tends to 0. Since t 7 dominates t 4 , t 5 and t 6 , we have</p><formula xml:id="formula_347">1 n n i=1 (τ * ) T b ′′ {(η * ) T Żi } Żi ŻT i -E η * 1 n n i=1 (τ * ) T b ′′ {(η * ) T Żi } Żi ŻT i ∞ = O p log p n + log n p .</formula><p>This completes the proof for Condition (ii).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7.2 Proof of Lemma B.3</head><p>In this proof, we denote</p><formula xml:id="formula_348">H η = 1 n n i=1 ( η -η * ) T Żi b ′′ {(η * ) T Żi } ŻT i ( η -η * )</formula><p>We continue to use the notations and results defined in Appendix D.1. Recall we define D( η, η * ) = ( ηη * ) T {∇l( η) -∇l(η * )}. To show (B.8), we consider the difference of D( η, η * ) and H η and apply the mean value theorem with η = ξ η + (1 - s η (n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 ) and from the result derived from Proposition III.6</p><formula xml:id="formula_349">ξ)η * for ξ ∈ [0, 1] to get |D( η, η * ) -H η | = ( η -η * ) T {∇l( η) -∇l(η * )} - 1 n n i=1 ( η -η * ) T Żi b ′′ {(η * ) T Żi } ŻT i ( η -η * ) = ( η -η * ) T ∇ 2 l( η) - 1 n n i=1 b ′′ {(η * ) T Żi } Żi ŻT i ( η -η * ) = 1 n n i=1 [b ′′ ( η T Żi ) -b ′′ {(η * ) T Żi }]{ ŻT i ( η -η * )} 2 ≤ B 1 n n i=1 b ′′ {(η * ) T Żi }{ ŻT i ( η -η * )} 2 max i ( η -η * ) T Żi (B.44) ≲ B 1 n n i=1 b ′′ {(η * ) T Żi }{ ŻT i ( η -η * )} 2 ∥ η -η * ∥ 1 max i=1,...,n ∥ Żi ∥ ∞ ≲ H η s η log p n + log n p M + 1 √ n + log n p (B.</formula><formula xml:id="formula_350">that max i ∥ Żi ∥ ∞ = M + O p (n -1/2 + p -1/2 (log n) 1/2</formula><p>). Under the assumption that s η (n -1/2 (log p) 1/2 + p -1/2 (log n) 1/2 ) = o p (1), we have</p><formula xml:id="formula_351">H η 1 -s η log p n + log n p M + 1 √ n + log n p ≲ D( η, η * ) ≲ 9c 2 s η log p n + log n p ,</formula><p>where the last inequality is by combining (B.5) and (B.7). Therefore</p><formula xml:id="formula_352">H η = O p s η log p n + log n p .</formula><p>The two inequalities (B.9) and (B. As shown in the preliminaries, we have</p><formula xml:id="formula_353">∇l(η) = 1 n n i=1 {-y i + b ′ (η T Żi )} Żi , ∇ 2 l(η) = 1 n n i=1 {-y i + b ′′ (η T Żi )} Żi ŻT i .</formula><p>Proof of Condition (iii). For condition (iii), we apply mean value theorem with As n, p → ∞, under the scaling condition that (s w ∨s η )(n -1/2 log p+p -1/2 n 1/2 log n)</p><formula xml:id="formula_354">η = ξ η + (1 -ξ)η * for ξ ∈ [0, 1], the left hand side is equivalent to |(τ * ) T {∇l( η) -∇l(η * ) -∇ 2 l(η * )( η -η * )}| = |(τ * ) T {∇ 2 l( η) -∇ 2 l(η * )}( η -η * )| = 1 n n i=1 [b ′′ ( η T Żi ) -b ′′ {(η * ) T Żi }] ŻT i ( η -η * )(τ * ) T Żi ≤ B 1 n n i=1 ξ( η -η * ) T Żi b ′′ {(η * ) T Żi } ŻT i ( η -η * ) max i |(τ * ) T Żi | (B.</formula><p>= o p (1), we have condition (iii) holds as</p><formula xml:id="formula_355">√ n|(τ * ) T {∇l( η) -∇l(η * ) -∇ 2 l(η * )( η -η * )}| ≲ √ ns η log p n + log n p M + O p 1 √ n + log n p = o p (1).</formula><p>Proof of Condition (iv). We multiply n 1/2 to the left hand side of condition (iv)</p><p>and apply mean value theorem with η = ξ η + (1 -ξ)η * . Then we have According to the definitions in the preliminaries, we have</p><formula xml:id="formula_356">n 1/2 |( τ -τ * ) T {∇l( η) -∇l(η * )}| = n 1/2 ( τ -τ * ) T 1 n n i=1 b ′ ( η T Żi ) -b ′ {(η * ) T Żi )} Żi = n 1/2 1 n n i=1 b ′′ ( η T Żi )( τ -τ * ) T Żi ( η -η * ) T Żi ≤ n 1/2 1 n n i=1 b ′′ {(η * ) T Żi }{( w -w * ) T Ṁi } 2 1/2 × 1 n n i=1 b ′′ {(η * ) T Żi }{( η -η * ) T Żi } 2 1/2 (B.48) ≲ n 1/</formula><formula xml:id="formula_357">(τ * ) T ∇l(η * )(I * θ|ζ ) -1/2 = 1 n n i=1 (τ * ) T Żi [-y i + b ′ {(η * ) T Żi }](I * θ|ζ ) -1/2 . = 1 n n i=1 (τ * ) T Żi [-y i + b ′ {(η * ) T Z i }](I * θ|ζ ) -1/2 + 1 n n i=1 (τ * ) T Żi [b ′ {(η * ) T Żi } -b ′ {(η * ) T Z i }](I * θ|ζ ) -1/2 = 1 n n i=1 (τ * ) T Z i [-y i + b ′ {(η * ) T Z i }](I * θ|ζ ) -1/2 + 1 n n i=1 (w * u ) T ( U i -U i )[-y i + b ′ {(η * ) T Z i }](I * θ|ζ ) -1/2 + 1 n n i=1 (τ * ) T Z i [b ′ {(η * ) T Żi } -b ′ {(η * ) T Z i }](I * θ|ζ ) -1/2 + 1 n n i=1 (w * u ) T ( U i -U i )[b ′ {(η * ) T Żi } -b ′ {(η * ) T Z i }](I * θ|ζ ) -1/2 =: G 1 + G 2 + G 3 + G 4 Because |(τ * ) T Z i | is bounded, [-y i + b ′ {(η * ) T Z i }] is sub-exponential from As- sumption III.5, the term (τ * ) T Z i [-y i + b ′ {(η * ) T Z i }](I * θ|ζ ) -1/2</formula><p>are independent and has finite moments. We apply Berry-Esseen Theorem and show that G 1 → d N (0, 1) For G 2 , we apply similar techniques as in the proof of Lemma B.1 condition(i) by Bernstein's inequality and it can be verified that</p><formula xml:id="formula_358">G 2 = 1 n n i=1 (w * u ) T ( U i -U i )[-y i + b ′ {(η * ) T Z i }](I * θ|ζ ) -1/2 = O p 1 √ n + log n p .</formula><p>As a result of Proposition III.6, we have</p><formula xml:id="formula_359">G 3 = 1 n n i=1 (τ * ) T Z i [b ′ {(η * ) T Żi } -b ′ {(η * ) T Z i }](I * θ|ζ ) -1/2 = O p 1 √ n + log n p ,</formula><p>and</p><formula xml:id="formula_360">G 4 = 1 n n i=1 (w * u ) T ( U i -U i )[b ′ {(η * ) T Żi } -b ′ {(η * ) T Z i }](I * θ|ζ ) -1/2 = O p 1 n + log n p .</formula><p>Under the scaling condition that n, p → ∞ and (s w ∨s η )(n -1/2 log p+p -1 n 1/2 log n)</p><p>= o p (1), we show that G 2 → p 0, G 3 → p 0 and G 4 → p 0. Applying Slutsky's Theorem,</p><formula xml:id="formula_361">we have 1 n n i=1 (τ * ) T Żi [-y i + b ′ {(η * ) T Żi }](I * θ|ζ ) -1/2 → d N (0, 1).</formula><p>This completes the proof of central limit theorem for the score function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7.5 Proof of Lemma B.7</head><p>In this proof, we will show the partial information</p><formula xml:id="formula_362">I * θ|ζ = E[b ′′ {(η * ) T Z i }D i {D i -(w * ) T M i }],</formula><p>is consistently estimated by</p><formula xml:id="formula_363">I θ|ζ = 1 n n i=1 b ′′ ( η T Żi )D i (D i -w T Ṁi ).</formula><p>To see this, we write the difference between them as</p><formula xml:id="formula_364">I θ|ζ -I * θ|ζ = 1 n n i=1 b ′′ ( η T Żi )D 2 i -E[b ′′ {(η * ) T Z i }D 2 i ] + 1 n n i=1 b ′′ ( η T Żi ) w T Ṁi D i -E[b ′′ {(η * ) T Z i }(w * ) T M i D i ] = L 1 + L 2 .</formula><p>For L 1 , we decompose it into</p><formula xml:id="formula_365">L 1 = 1 n n i=1 b ′′ ( η T Żi )D 2 i -E[b ′′ {(η * ) T Z i }D 2 i ] = 1 n n i=1 b ′′ {(η * ) T Z i }D 2 i -E[b ′′ {(η * ) T Z i }D 2 i ] + 1 n n i=1 [b ′′ {(η * ) T Żi } -b ′′ {(η * ) T Z i }]D 2 i + 1 n n i=1 [b ′′ ( η T Żi ) -b ′′ {(η * ) T Żi }]D 2 i = L 11 + L 12 + L 13 .</formula><p>Applying similar techniques as in the proof of Lemma B.1 condition (ii) by Bernstein inequality, we can show that</p><formula xml:id="formula_366">L 11 = 1 n n i=1 b ′′ {(η * ) T Z i }D 2 i -E[b ′′ {(η * ) T Z i }D 2 i ] = O p 1 √ n .</formula><p>As a result of Proposition III.6,</p><formula xml:id="formula_367">L 12 = 1 n n i=1 [b ′′ {(η * ) T Żi } -b ′′ {(η * ) T Z i }]D 2 i = O p 1 √ n + log n p .</formula><p>Using the estimation consistency results in Theorem III.7, we have</p><formula xml:id="formula_368">L 13 = 1 n n i=1 [b ′′ ( η T Żi ) -b ′′ {(η * ) T Żi }]D 2 i = O p s η log p n + log n p M + 1 √ n + log n p</formula><p>Under the condition that n, p → ∞ and (s w ∨ s η ) (n -1/2 log p + p -1 n 1/2 log n) = o p (1), we have</p><formula xml:id="formula_369">L 1 = L 11 + L 12 + L 13 = o p (1). (B.51)</formula><p>For L 2 , we decompose it into</p><formula xml:id="formula_370">L 2 = 1 n n i=1 b ′′ ( η T Żi ) w T Ṁi D i -E[b ′′ {(η * ) T Z i }(w * ) T M i D i ] = 1 n n i=1 b ′′ ( η T Żi )( w -w * ) T Ṁi D i + 1 n n i=1 [b ′′ ( η T Żi ) -b ′′ {(η * ) T Żi }](w * ) T Ṁi D i + 1 n n i=1 b ′′ {(η * ) T Żi }(w * ) T Ṁi D i -E[b ′′ {(η * ) T Z i }(w * ) T M i D i ] = L 21 + L 22 + L 23 .</formula><p>We apply Hölder's inequality on L 21 and get</p><formula xml:id="formula_371">L 21 = 1 n n i=1 b ′′ ( η T Żi )( w -w * ) T Ṁi D i ≤ 1 n n i=1 b ′′ ( η T Żi ){( w -w * ) T Ṁi } 2 1/2 1 n n i=1 b ′′ ( η T Żi )D 2 i 1/2 ≲ (s η ∨ s w ) log p n + log n p 1/2 M + 1 √ n + log n p 1/2 (B.52)</formula><p>where the inequality (B.52) is from the the equation (B.10) in Lemma B.3 and similar arguments as in bounding L 12 + L 13 .</p><p>For L 22 , we apply Assumption III.5(3) that |b ′′ (t 1 ) -b ′′ (t)| ≤ B|t 1 -t|b ′′ (t) with t 1 = η T Żi and t = (η * ) T Żi . Then we apply Hölder's inequality and have For L 23 , we follow the similar arguments in bounding L 11 and get This completes the proof of Lemma B.7.</p><formula xml:id="formula_372">L 22 = 1 n n i=1 [b ′′ ( η T Żi ) -b ′′ {(η * ) T Żi }](w * ) T Ṁi D i ≤ 1 n n i=1 b ′′ {(η * ) T Żi }( η -η * ) T Żi (w * ) T Ṁi D i ≤ 1 n n i=1 b ′′ {(η * ) T Żi }{( η -η * ) T Żi } 2 1/2 × 1 n n i=1 b ′′ {(η * ) T Żi }{(w * ) T Ṁi } 2 D 2 i 1/2 ≲ s η log p n + log n p 1/2 M + 1 √ n + log n p 3/2 (B.</formula><formula xml:id="formula_373">L 23 = 1 n n i=1 b ′′ {(η * ) T Żi }(w * ) T Ṁi D i -E[b ′′ {(η * ) T Z i }(w * ) T M i D i ] ≲ 1 n + log n p . (B.54) Under the condition that n, p → ∞ and (s w ∨ s η ) (n -1/2 log p + p -1 n 1/2 log n) = o p<label>(</label></formula><p>umn of A ‡ is 0 K and that q -1 q j=1 [β <ref type="bibr">[,2:p]</ref> , as a result of Condition 1 in the main text.</p><formula xml:id="formula_374">0 j ] [2:p] -A T [,2:p] γ 0 j 1 = q -1 q j=1 ∥β * j +{(A ‡ [,2:p] ) T - A T [2:p] (G ‡ ) -1 }(γ * j ) T ∥ 1 is minimized when A [,2:p] = (G ‡ ) T A ‡</formula><p>Following the above discussion, we study the asymptotic properties of ϕ * by first examining the properties of the maximum likelihood estimator of ϕ 0 , which technically is more feasible due to the orthogonality of the transformed latent factors U 0 and the covariates X (Condition 1 ′ ). Then we use the obtained estimator for ϕ 0 to construct the estimators for the transformations A 0 and G 0 , by which we are able to further establish the asymptotic consistency and normality of ϕ * . In particular, the maximum likelihood estimator for ϕ 0 under the identifiability conditions 1 ′ -2 ′ , denoted as ϕ 0 , can be obtained as follows</p><formula xml:id="formula_375">ϕ 0 = arg min ϕ∈B(D) -L(Y|ϕ), (C.1) subject to U T X = 0 K×p and n -1 U T U = q -1 Γ T Γ = diagonal.</formula><p>With ϕ 0 obtained in (C.1), we construct estimators for the transformed parameters that satisfy the identifiability conditions IV.4-IV.7 as follows:</p><formula xml:id="formula_376">B * = B 0 - Γ 0 A 0 , Γ * = Γ 0 ( G 0T ) -1</formula><p>, and U * = ( U 0 + X A 0T )G 0 , where the transformation matrices are given as</p><formula xml:id="formula_377">A 0 = (0 K , A 0 c ) with A 0 c = arg min A q -1 q j=1 ∥ β 0 jc -A T γ 0 j ∥ 1 and G 0 = (q -1 ( Γ 0 ) T Γ 0 ) 1/2 V 0 ( U 0 ) -1/4</formula><p>. Here V 0 and U 0 contain the eigenvalues and eigenvectors of (nq</p><formula xml:id="formula_378">) -1 ( Γ 0 ) T Γ 0 1/2 ( U 0 ) T + A 0 X T U 0 + X( A 0 ) T ( Γ 0 ) T Γ 0 1/2 , re-</formula><p>spectively. We will show in the following proof that the estimators B * , Γ * , and U * obtained here are asymptotically equivalent to the target joint maximum likelihood estimators B * , Γ * , and U * . Thus, we can show the main results for ϕ * by establishing the consistency and asymptotic normality of the estimation for ϕ 0 given in (C.1) together with the consistency of the estimated transformation matrices A 0 and G 0 .</p><p>Our proof strategy is summarized as follows and also illustrated in the subsequent To study the theoretical properties of ϕ 0 in (C.1), we introduce the following equivalent reformulation of (C.1), which facilitates the theoretical analysis:</p><formula xml:id="formula_379">ϕ 0 = arg min ϕ∈B(D) L(Y|ϕ), (C.2)</formula><p>used in literature <ref type="bibr" target="#b1">(Aitchison and Silvey, 1958;</ref><ref type="bibr" target="#b131">Silvey, 1959;</ref><ref type="bibr" target="#b146">Wang, 2022;</ref><ref type="bibr" target="#b93">Li et al., 2023)</ref> We next verify an analogous version of Assumption IV.8 for ϕ 0 . Define Ḡ † as the probability limit of G ‡ . By Assumption IV.8, we know lim q→∞ q -1 (Γ 0 ) T Γ 0 = ( Ḡ ‡ ) -1 Σ * γ ( Ḡ ‡ ) -T is positive definite, which we denote by Σ 0 γ , and</p><formula xml:id="formula_380">lim n→∞ n -1 (Z 0 ) T Z 0 =    Σ * u -Σ * ux Σ -1 x Σ * xu 0 0 Σ x   </formula><p>is also positive definite, which we denote by Σ 0 z . Then we also have 1/(κ ′ ) 2 ≤ λ min (Σ 0 z ) ≤ λ max (Σ 0 z ) ≤ (κ ′ ) 2 for some κ ′ &gt; 0. We claim that for some M ′ &gt; 0,</p><formula xml:id="formula_381">max i ∥Z 0 i ∥ ∞ ≤ M ′ and max j ∥γ 0 j ∥ ∞ ≤ M ′ .</formula><p>The boundedness for γ 0 j can be easily verified since G ‡ converges and is of finite dimension. For</p><formula xml:id="formula_382">U 0 i = (G ‡ ) T (U * i -A ‡ X i ), since lim n→∞ A ‡ = Σ * ux Σ -1 x with ∥A ‡ X i ∥ ∞ ≤ ∥A ‡ ∥ ∞ ∥X i ∥ ∞ ≤ M 2 by Assump- tion IV.8, we can show max i ∥U 0 i ∥ ∞ ≤ M ′ . For simplicity we still write ∥U 0 i ∥ ∞ ≤ M , ∥γ 0 j ∥ ∞ ≤ M .</formula><p>We next study the theoretical properties of ϕ 0 with corresponding proofs presented in Section C.4.</p><p>Proposition C.1 (Average Consistency). Under Assumptions IV.8-IV.9 and p =</p><formula xml:id="formula_383">o(n∧q), n -1 ∥ U 0 -U 0 ∥ 2 F = O p (ζ -2 nq ), q -1 ∥ Γ 0 -Γ 0 ∥ 2 F = O p (ζ -2 nq ), and q -1 ∥ B 0 -B 0 ∥ 2 F = O p (ζ -2 nq ).</formula><p>The convergence rate of U 0 matches with the sharp convergence rates for the standard nonlinear factor model up to a small scale. Similar arguments apply to interpreting the convergence rate of Γ 0 . Our average convergence rate on B 0 has the first term n -1 p log(q) matching the convergence rates O p (p/n) under M -estimation with diverging p up to a logarithmic scale. The second term q -1 log n is mainly from the estimation for the latent factor U 0 .</p><p>These average consistency results are not sharp enough to guarantee that ϕ 0 lies in the interior of the parameter space, as the number of parameters increases to infinity.</p><p>However, in the following, we will show the strong convexity of the objective function inside a small region near the true parameter ϕ 0 . The average consistency restricts our estimation to this region, allowing us to apply the mean value theorem to the score function ∂ ϕ L.</p><p>Proposition C.2 (Local Convexity of L). Under Assumptions IV.8-IV.10, there exist some ϵ &gt; 0 and m such that</p><formula xml:id="formula_384">Pr min ϕ∈B(D), √ p∥D -1/2 q (ϕ-ϕ 0 )∥≤m λ min D 1/2 q H(ϕ)D 1/2 q ≥ ϵ → 1, (C.3)</formula><p>where</p><formula xml:id="formula_385">H(ϕ) = ∂ 2 ϕϕ L(Y|ϕ) = -∂ 2 ϕϕ L(Y|ϕ) -∂ 2 ϕϕ P (ϕ).</formula><p>This proposition shows that the eigenvalues of Hessian matrix for the loss function L(ϕ) can be bounded by ϵ from below with high probability, within a local region near ϕ 0 . This result, however, cannot directly lead to consistency for every parameter, as the number of parameters goes to infinity. For a more delicate result, we need to bound the infinity norm of the inverse of the Hessian matrix, which can further give the bound for ∥ ϕ 0 -ϕ 0 ∥ ∞ :</p><p>Proposition C.3 (Infinity Bound). Under Assumptions IV.8-IV.10, we have for any small ϵ, that</p><formula xml:id="formula_386">U v -U 0 v ∞ = O p p n ∧ (pq) (nq) 3/ξ+ϵ ; f v -f 0 v ∞ = O p √ p √ n ∧ q (nq) 3/ξ+ϵ .</formula><p>Under the scaling condition in Assumption IV.10 we know that ∥ ϕ 0 -ϕ 0 ∥ ∞ converges to 0 with high probability, and therefore each parameter of the estimation ϕ 0 converges to the that of ϕ 0 , which implies that ϕ 0 is an interior point of B(D) with high probability. This immediately implies the following:</p><p>Proposition C.4 (First Order Condition). Under Assumptions IV.8-IV.10, we have</p><formula xml:id="formula_387">∂ ϕ L(Y| ϕ) = 0 w.h.p.</formula><p>The above first order condition plays a foundational role in establishing the asymptotic normality of our estimation. This result is nontrivial in the sense that the dimension of the model parameters goes to infinity along with n, q, p. Also, the dimensions of factors, loadings, and the regression coefficients differ, not only in magnitude but also in their order. Based on the first order condition, we are able to derive the individual consistency rate for ϕ 0 :</p><p>Proposition C.5 (Individual Consistency). Under Assumptions IV.8-IV.10, we get for any j ∈</p><formula xml:id="formula_388">[q] ∥ γ 0 j -γ 0 j ∥ 2 = O p ζ -1 nq,p , ∥ β 0 j -β 0 j ∥ 2 = O p ζ -1 nq,p ,</formula><p>and for any i</p><formula xml:id="formula_389">∈ [n] ∥ U 0 i -U 0 i ∥ 2 = O p ( √ pζ -1 nq,p ).</formula><p>The convergence rates of individual estimators γ 0 j and β 0 j match with their corresponding average convergence rates in Proposition C.1. Comparing the individual convergence rates of U 0 i with the average rates of U 0 , the individual rate is higher than the corresponding average rate by a factor of √ p.</p><p>Proposition C.6 (Asymptotic Normality). Under Assumptions IV.8-IV.11, we have the asymptotic distributions for the constrained maximum likelihood estimators U 0 i as</p><formula xml:id="formula_390">√ q(Σ 0 u,i ) -1/2 ( U 0 i -U 0 i ) d → N (0 K , I K ) if p 3/2 √ q(nq) 3/ξ ζ -2 nq,p → 0, for all i ∈ [n]</formula><p>Then Σ 0 γ,j can be consistently estimated by the plug-in estimator as</p><formula xml:id="formula_391">Σ 0 γ,j = n n i=1 l ′′ ij U 0 i ( U 0 i ) T -1 n i=1 ( l ′ ij ) 2 U 0 i ( U 0 i ) T ( n i=1 l ′′ ij U 0 i ( U 0 i ) T -1 ,</formula><p>and for any a, b ∈ R p with ∥a∥ = ∥b∥ = 1, b T Σ 0 β,j a and Σ 0 γβ,j a can be consistently estimated by the plug-in estimator as</p><formula xml:id="formula_392">b T Σ 0 β,j a = nb T n i=1 l ′′ ij X i X T i -1 n i=1 ( l ′ ij ) 2 X i X T i ( n i=1 l ′′ ij X i X T i -1 a; Σ 0 γβ,j a = n n i=1 l ′′ ij U 0 i ( U 0 i ) T -1 n i=1 ( l ′ ij ) 2 U 0 i X T i ( n i=1 l ′′ ij X i X T i -1 a, with Σ 0 βγ,j = ( Σ 0 γβ,j ) T .</formula><p>Next based on ϕ 0 , recall that we have constructed the estimators as follows</p><formula xml:id="formula_393">B * = B 0 -Γ 0 A 0 ; Γ * = Γ 0 ( G 0 ) -T ; U * = { U 0 + X( A 0 ) T } G 0 . (C.4) We write ϕ * = {( f * v ) T , ( U * v ) T } T , where f * v = {( f * 1 ) T , • • • , ( f * q ) T } T with f * j = (( γ * j ) T , ( β * j ) T ) T and U * v = {( U * 1 ) T , • • • , ( U * n ) T } T .</formula><p>We then show that ϕ * is equivalent to the ϕ * obtained in the main file. First, it can be readily verified that ϕ * satisfies the identifiability conditions IV.4-IV.7 defined in the main text. As implies by the following result, A 0 and G 0 are consistent estimation of A 0 and G 0 = (G ‡ ) -1 .</p><p>Proposition C.8 (Consistency of transformation matrices). Under Assumption IV.8 -IV.11, A 0 and G 0 are consistent estimation of A 0 and G 0 := (G ‡ ) -1 such that</p><formula xml:id="formula_394">∥ A 0 -A 0 ∥ F =O p √ p ζ nq,p , ∥ G 0 -G 0 ∥ F =O p √ p ζ nq,p ∨ p 3/2 (nq) 3/ξ ζ 2 nq,p</formula><p>.</p><p>Further under Assumption IV.12, we have</p><formula xml:id="formula_395">∥ A 0 -A 0 ∥ F =O p √ p ζ nq,p ∧ √ pι nq,p ∨ p 3/2 (nq) 3/ξ ζ 2 nq,p , ∥ G 0 -G 0 ∥ F =O p pι nq,p ∨ p 3/2 (nq) 3/ξ ζ 2 nq,p</formula><p>.</p><p>In particular, under the condition p 3/2 ζ -1 nq (nq) 3/ξ → 0 and Assumption IV.12 that pn -1/2 ι nq,p → 0, we have</p><formula xml:id="formula_396">∥ A 0 -A 0 ∥ F , ∥ G 0 -G 0 ∥ F = o p ζ -1 nq,p .</formula><p>For infinity bound of f * v -f * v , by Assumption IV.10 and Proposition C.3, we first</p><formula xml:id="formula_397">have ∥ f 0 v -f 0 v ∥ ∞ = o p (1) . Then with ∥( A 0 -A 0 ) T ∥ ∞ ≤ √ K∥( A 0 -A 0 ) T ∥ F = o p (1) we know ∥ f * v -f * v ∥ ∞ = o p (1). Similarly we can show that ∥ U * v -U * v ∥ ∞ = o p (1).</formula><p>Therefore we conclude that ϕ * is also an interior point inside B(D). By the fact that ϕ * maximizes the joint log-likelihood inside B(D) and satisfies Conditions IV.4-IV.7, we conclude ϕ * = ϕ * asymptotically. In the following, we suppress the notation ϕ * and use ϕ * only. Note that these two sets of parameters agree in the intercept terms despite the transformation as under either set of identifiability conditions latent abilities are centralized.</p><p>Remark C.9. Establishing the consistency of A 0 is closely related to the median regression problem with measurement errors. A 0 's consistent rate O p ( √ pζ -1 nq ) can be derived using the individual rate obtained in Proposition C.5. This result, however, is not enough for establishing asymptotic properties of ϕ * . To derive a stringent convergence rate for A 0 , we extend the general results of Bahadur representations for M -estimators in the median regression framework <ref type="bibr" target="#b70">(He and Shao, 1996)</ref>. Compared to the results in <ref type="bibr" target="#b69">He and Liang (2000)</ref> with measurement errors being independent and following spherically symmetric distributions, our measurement errors, that is, β 0 j -β 0 j and γ 0 j -γ 0 j , are asymptotically Gaussian with weak dependence. It leaves a non-trivial problem to show the similar consistency results of A 0 given the weakly dependent errors. The estimator G 0 is also consistent and the convergence rate is dependent on the asymptotic results of ϕ 0 and also the estimation consistency of A 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Proof of Main Theorems and Corollary</head><p>The theoretical properties for ϕ 0 are established in Propositions From (C.4), the estimation for B * is affected by A 0 and not affected by G 0 . After obtaining the convergence rate ∥ A 0 -A 0 ∥ F , we write</p><formula xml:id="formula_398">B * -B * = B 0 -B 0 -( Γ 0 A 0 -Γ 0 A 0 ) = B 0 -B 0 -( Γ 0 -Γ 0 ) A 0 -Γ 0 ( A 0 -A 0 ),</formula><p>which, together with average consistency results in Proposition C.1 and Proposition C.8, implies that</p><formula xml:id="formula_399">∥ B * -B * ∥ F ≤ ∥ B 0 -B 0 ∥ F + ∥ Γ 0 -Γ 0 ∥ F (∥A 0 ∥ F + ∥ A 0 -A 0 ∥ F ) + ∥Γ 0 ∥ F ∥ A 0 -A 0 ∥ F = O p q 1/2 ζ -1 nq,p + q 1/2 ζ -1 nq,p ( √ p + A nq,p ) + q 1/2 A nq,p = O p q 1/2 p 1/2 ζ -1 nq,p ,</formula><p>where the last equality is because</p><formula xml:id="formula_400">∥ A 0 -A 0 ∥ F = O p (A nq,p ) = O p (p -1/2 n -1 ) under</formula><p>Assumption IV.12. Hence we have q</p><formula xml:id="formula_401">-1 ∥ B * -B * ∥ 2 F = O p pζ -2 nq,p .</formula><p>Next with the convergence rates ∥ G 0 -G 0 ∥ F , we derive the convergence rates ∥ Γ * -Γ * ∥ F and ∥ U * -U * ∥ F . Note that under Assumptions IV.10, IV.12, and the</p><formula xml:id="formula_402">scaling condition p 3/2 (nq) ϵ+3/ξ (p 1/2 n -1/2 + q -1/2 ) = o(1), ∥ G 0 -G 0 ∥ F = O p (G nq,p ) = o p (ζ -1 nq,p ).</formula><p>For ∥ Γ * -Γ * ∥ F , by (C.4), Proposition C.1 and Proposition C.8, we have</p><formula xml:id="formula_403">q -1/2 ∥ Γ * -Γ * ∥ F ≤q -1/2 Γ 0 ( G 0 ) -T -(G 0 ) -T F + q -1/2 ( Γ 0 -Γ 0 )(G 0 ) -T F =O p ζ -1 nq,p + G nq,p =O p ζ -1 nq,p .</formula><p>For ∥ U * -U * ∥, we first show by Assumption IV.8 and Proposition C.8 that</p><formula xml:id="formula_404">A 0 X i ≤ A 0 X i + A 0 -A 0 F X i ≤ (G 0 ) -T U * i + U 0 i + A 0 -A 0 F X i = O p (1), which also implies that ∥X( A 0 ) T ∥ = O p ( √ n). Then we have n -1/2 ∥ U * -U * ∥ F ≤ n -1/2 ∥( U 0 + X( A 0 ) T ) G 0 -(U 0 + X(A 0 ) T )G 0 ∥ F ≤n -1/2 ∥ U 0 + X( A 0 ) T ∥ F ∥ G 0 -G 0 ∥ F + n -1/2 ∥ U 0 + X( A 0 ) T -(U 0 + X(A 0 ) T )∥ F ∥G 0 ∥ F =O p (G nq,p + ζ -1 nq,p ) =O p ζ -1 nq,p .</formula><p>Remark C.10. For the individual rate β * j -β * j , we apply a similar technique and write</p><formula xml:id="formula_405">β * j -β * j = β 0 j -( A 0 ) T γ 0 j -(β 0 j -(A 0 ) T γ 0 j ) = β 0 j -β 0 j -( A 0 -A 0 ) T γ 0 j -(A 0 ) T ( γ 0 j -γ 0 j ).</formula><p>Therefore, we have</p><formula xml:id="formula_406">∥ β * j -β * j ∥ ≤ O p ζ -1 nq,p + A nq,p + p 1/2 ζ -1 nq,p = O p (p 1/2 ζ -1 nq,p ).</formula><p>For the individual rates of γ * j -γ * j and</p><formula xml:id="formula_407">U * i -U * i , we have γ * j -γ * j = ( G 0 ) -1 γ 0 j - (G 0 ) -1 γ 0 j . Therefore ∥ γ * j -γ * j ∥ ≤ ∥( G 0 ) -1 ∥∥ γ 0 j -γ 0 j ∥ + ∥( G 0 ) -1 -(G 0 ) -1 ∥∥γ 0 j ∥ = O p ζ -1 nq,p + G nq,p = O p ζ -1 nq,p .</formula><p>Similarly, by</p><formula xml:id="formula_408">U * i -U * i = ( G 0 ) T ( U 0 i + A 0 X i ) -(G 0 ) T (U 0 i + A 0 X i ), we conclude ∥ U * i -U * i ∥ ≤ ∥( G 0 ) T ∥∥ U 0 i + A 0 X i -(U 0 i + A 0 X i )∥ + ∥ G 0 -G 0 ∥∥U 0 i + A 0 X i ∥ = O p ζ -1 nq,p + G nq,p = O p ζ -1 nq,p .</formula><p>C.3.2 Proof of Theorem IV.16</p><p>From (C.4), we write</p><formula xml:id="formula_409">√ n( β * j -β * j ) = √ n{ β 0 j -( A 0 ) T γ 0 j -(β 0 j -(A 0 ) T γ 0 j )} = √ n{( β 0 j -β 0 j ) -(A 0 ) T ( γ 0 j -γ 0 j )} - √ n( A 0 -A 0 ) T γ 0 j . Because ∥ √ n( A 0 -A 0 ) T γ 0 j ∥ ∞ ≤ √ n∥ A 0 -A 0 ∥ ∞ ∥ γ 0 j ∥ ∞ ≲ √ n pι nq,p +p 3/2 (nq) 3/ξ ζ -2 nq,p</formula><p>→ 0, we further have</p><formula xml:id="formula_410">√ n( β * j -β * j ) = √ n{( β 0 j -β 0 j ) -(A 0 ) T ( γ 0 j -γ 0 j )} + o p (1).</formula><p>Based on the asymptotic normality results for f 0 j -f 0 j in (C.47), we have</p><formula xml:id="formula_411">√ na T (Σ * β,j ) -1/2 ( β * j -β * j ) d → N (0, 1),</formula><p>for any a ∈ R p with ∥a∥ = 1. Write Ā0 = lim n→∞ (U 0 ) T X(X T X) -1 as the probability limit of A 0 . Here Σ * β is defined as</p><formula xml:id="formula_412">Σ * β,j = -( Ā0 ) T I p (Ψ 0 jz ) -1 Ω 0 jz (Ψ 0 jz ) -1    -Ā0 I p    . Denote l ′ ij = l ′ ij ( γ 0⊺ j U 0 i + β 0⊺ j X i ) and l ′′ ij = l ′′ ij ( γ 0⊺ j U 0 i + β 0⊺ j X i ). Then b T Σ * β,j a can be consistently estimated by b T Σ * β,j a =b T -( A 0 ) T I p n -1 n i=1 l ′′ ij Z 0 i ( Z 0 i ) T -1 n -1 n i=1 ( l ′ ij ) 2 Z 0 i ( Z 0 i ) T n -1 n i=1 l ′′ ij Z 0 i ( Z 0 i ) T -1    -A 0 I p    a =b T n n i=1 l ′′ ij X i X T i -1 n i=1 ( l ′ ij ) 2 X i X T i n i=1 l ′′ ij X i X T i -1 -n( A 0 ) T n i=1 l ′′ ij U 0 i ( U 0 i ) T -1 n i=1 ( l ′ ij ) 2 U 0 i X T i n i=1 l ′′ ij X i X T i -1 -n n i=1 l ′′ ij X i X T i -1 n i=1 ( l ′ ij ) 2 X i ( U 0 i ) T n i=1 l ′′ ij U 0 i ( U 0 i ) T -1 A 0 + n( A 0 ) T n i=1 l ′′ ij U 0 i ( U 0 i ) T -1 n i=1 ( l ′ ij ) 2 U 0 i ( U 0 i ) T n i=1 l ′′ ij U 0 i ( U 0 i ) T -1 A 0 a,</formula><p>for any a, b ∈ R K+p with ∥a∥ = ∥b∥ = 1.</p><p>Furthermore, we write</p><formula xml:id="formula_413">√ n( γ * j -γ * j ) = √ n( G 0 ) -1 γ 0 j - √ n(G 0 ) -1 γ 0 j .</formula><p>Similarly, under the condition p √ nι nq,p = o(1) and p 3/2 n 1/2 (nq</p><formula xml:id="formula_414">) 3/ξ ζ -2 nq,p = o(1), we have √ n( G 0 -G 0 ) γ 0 j ∞ = o p (1)</formula><p>. By the asymptotic property of f 0 j -f 0 j in (C.47), we obtain that</p><formula xml:id="formula_415">√ n(Σ * γ,j ) -1/2 ( γ * j -γ * j ) d → N (0, I K ),</formula><p>where</p><formula xml:id="formula_416">Σ * γ,j = ( Ḡ0 ) -1 [(Ψ 0 jz ) -1 Ω 0 jz (Ψ 0 jz ) -1 ] [1:K,1:K] ( Ḡ0 ) -T</formula><p>, and can be consistently estimated by the plug-in estimator:</p><formula xml:id="formula_417">Σ * γ,j = ( G 0 ) -1 n n i=1 l ′′ ij U 0 i ( U 0 i ) T -1 ( n i=1 ( l ′ ij ) 2 U 0 i ( U 0 i ) T ( n i=1 l ′′ ij U 0 i ( U 0 i ) T -1 ( G 0 ) -T .</formula><p>Similarly for U * i -U * i , under p √ qι nq,p = o(1) when q = O(n) and p 3/2 q 1/2 (nq</p><formula xml:id="formula_418">) 3/ξ ζ -2 nq,p = o(1), ∥ A 0 -A 0 ∥ = o p q -1/2 and ∥ G 0 -G 0 ∥ = o p q -1/2 , which therefore implies √ q( U * i -U * i ) = √ q( G 0 ) T ( U 0 i + A 0 X i ) - √ q(G 0 ) T (U 0 i + A 0 X i ) = √ q(G 0 ) T { U 0 i -U 0 i + ( A 0 -A 0 )X i } + √ q( G 0 -G 0 ) T ( U i + A 0 X i ) = √ q(G 0 ) T ( U 0 i -U 0 i ) + o p (1).</formula><p>Hence,</p><formula xml:id="formula_419">√ q(Σ * u,i ) -1/2 ( U * i -U * i ) d → N (0, I K ),</formula><p>where Σ * u,i = ( Ḡ0 ) T (Ψ 0 iγ ) -1 Ω 0 iγ (Ψ 0 iγ ) -1 Ḡ0 , and can be estimated by the plug-in estimator:</p><formula xml:id="formula_420">Σ * u,i = ( G 0 ) T q q j=1 l ′′ ij γ 0 j ( γ 0 j ) T -1 q j=1 ( l ′ ij ) 2 γ 0 j ( γ 0 j ) T q j=1 l ′′ ij γ 0 j ( γ 0 j ) T -1 G 0 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3.3 Consistent Estimation for the Asymptotic Covariance Matrices in</head><p>Theorem IV.16</p><p>In this subsection, we present the estimators for the asymptotic covariance matrices in Theorem IV.16 based on the estimator ϕ 0 . For each j ∈ [q], for any a, b ∈ R p with ∥a∥ = ∥b∥ = 1, the scaled asymptotic covariance matrices of b T Σ * β,j a is consistently estimated by</p><formula xml:id="formula_421">b T Σ * β,j a =nb T n i=1 l ′′ ij X i X T i -1 n i=1 ( l ′ ij ) 2 X i X T i n i=1 l ′′ ij X i X T i -1 a -n( A 0 b) T n i=1 l ′′ ij U 0 i ( U 0 i ) T -1 n i=1 ( l ′ ij ) 2 U 0 i X T i n i=1 l ′′ ij X i X T i -1 a -nb T n i=1 l ′′ ij X i X T i -1 n i=1 ( l ′ ij ) 2 X i ( U 0 i ) T n i=1 l ′′ ij U 0 i ( U 0 i ) T -1 A 0 a + n( A 0 b) T n i=1 l ′′ ij U 0 i ( U 0 i ) T -1 n i=1 ( l ′ ij ) 2 U 0 i ( U 0 i ) T n i=1 l ′′ ij U 0 i ( U 0 i ) T -1 A 0 a, (C.5) and ∥ A 0 -A 0 ∥ F = O p ( √ pζ -1 nq,p ), we can show b T ( Σ * β,j )a = b T -( A 0 ) T I p Σ 0 f,j    -A 0 I p    a = b T -( A 0 ) T I p n -1 n i=1 l ′′ ij Z 0 i ( Z 0 i ) T -1 n -1 n i=1 ( l ′ ij ) 2 Z 0 i ( Z 0 i ) T a + b T n -1 n i=1 l ′′ ij Z 0 i ( Z 0 i ) T -1    -A 0 I p    a,</formula><p>consistently estimate b T Σ * β,j a for any a, b ∈ R K+p with ∥a∥ = ∥b∥ = 1. Similar arguments can be applied to show the consistency of Σ * γ,j and Σ * u,i . This completes the proof of Corollary IV.17.</p><p>At the end of this section, we prove the result of Proposition IV.5 in Main Text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition IV.5 in Main Text</head><p>Following the adjusted notation used in the Supplementary Material, here we use</p><formula xml:id="formula_422">β jc to denote (β j2 , • • • , β jp ) T . For s ∈ {2, • • • , p}, let L s (a) = q j=1 |β * js -a T γ * j |.</formula><p>We denote the directional gradient of L s as</p><formula xml:id="formula_423">∇ v L s (a) = q j=1 |v T γ * j |I(β * js = 0) + q j=1 sign(β * js )v T γ * j I(β * js ̸ = 0).</formula><p>'if' part: suppose (4.3) holds and for some A c ̸ = 0 we have</p><formula xml:id="formula_424">q j=1 ∥β * jc ∥ 1 ≥ q j=1 ∥β * jc -A T c γ * j ∥ 1 .</formula><p>Then there exists some s ∈ {2, • • • , p} such that q j=1 |β * js | ≥ q j=1 |β * js -a T s γ * j | and a s ̸ = 0, where a s is the (s -1)th column of A c . Then we know by the convexity of L s that ∇ as L s (0) ≤ 0, which contradicts (4.3) when taking v as -a s .</p><p>'only if' part: suppose (4.3) fails to hold for some v * and s ∈ {2, • • • , p}, which implies that ∇ v * L s (0) ≤ 0. We claim that there exists some ε such that L s (v * ε) ≤ L s (0). Note that we can find small ε such that when on the segment from 0 to v * ε, each term in L s (a) preserves its sign, and thus we always have ∇ v * L(a) ≤ 0 when a moves from 0 to v * ε. Taking the (s -1)th column of A c as v * ε and the other columns as 0s, then we have q j=1 ∥β *</p><formula xml:id="formula_425">jc ∥ 1 ≥ q j=1 ∥β * jc -A T c γ * j ∥ 1 , contradicting Condition 1(ii).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Proofs of Propositions</head><p>Before we prove the theoretical properties of the estimator ϕ 0 , we give the expressions for the derivatives of the objective function in the estimation problem (C.2). For notation convenience, we further define E</p><p>(n)</p><p>rl to be the indicator matrix of dimension n × n with the (r, l)th element being 1 and others being 0s. To better present the block-structure of the Hessian matrix, we define the indices: K(r) = (r -1)K + r and K(r, l) = (r -1)K + l, and define index sets:</p><formula xml:id="formula_426">K i = {(i -1)K + 1, • • • , iK} and P j = {(j -1)(K + p) + 1, • • • , j(K + p)}.</formula><p>The score function on ϕ 0 is written as a (qK + qp + nK)-dimensional vector:</p><formula xml:id="formula_427">S(ϕ 0 ) = - ∂ ∂ϕ (nq) -1 n i=1 q j=1 l ij (γ 0 j ) T U 0 i + (β 0 j ) T X i ,</formula><p>which can be written in two parts: S(ϕ) = (S f (ϕ 0 ) T , S U (ϕ 0 ) T ) T with each part given as:</p><formula xml:id="formula_428">[S f (ϕ 0 )] [P j ] = -(nq) -1 n i=1 l ′ ij (w 0 ij )Z 0 i , [S U (ϕ 0 )] [K i ] = -(nq) -1 q j=1 l ′ ij (w 0 ij )γ 0 j , where w 0 ij = (γ 0 j ) T U 0 i + (β 0 j ) T X i .</formula><p>Here it can be verified that the first order derivative of the penalty function on ϕ 0 is always zero.</p><p>Next, we compute the second order derivative of the objective function. The Hessian matrix is computed as a summation of three parts, H</p><formula xml:id="formula_429">(ϕ) = ∂ 2 ϕϕ L(ϕ) = H L (ϕ) + H R (ϕ) + H P (ϕ)</formula><p>, with the block form given as</p><formula xml:id="formula_430">H(ϕ) =    H f f ′ (ϕ) H f u ′ (ϕ) H uf ′ (ϕ) H uu ′ (ϕ)    ,</formula><p>where K+p) and H uu ′ (ϕ) ∈ R nK×nK . For H L , we have</p><formula xml:id="formula_431">H f f ′ (ϕ) ∈ R q(K+p)×q(</formula><formula xml:id="formula_432">H L (ϕ) =    H Lf f ′ (ϕ) H Lf u ′ (ϕ) H Luf ′ (ϕ) H Luu ′ (ϕ)   </formula><p>where the nonzero parts of the blocks are given as</p><formula xml:id="formula_433">H Lf f ′ [P j ,P j ] = - 1 nq n i=1 l ′′ ij (w ij )Z i Z T i , H Lf u ′ [P j ,K i ] = - 1 nq l ′′ ij (w ij )Z i γ T j , H Luu ′ [K i ,K i ] = - 1 nq q j=1 l ′′ ij (w ij )γ j γ T j , with H Luf ′ = H T Lf u ′ .</formula><p>The matrix H R is a complement term due to the chain rule in differentiation, which is written as</p><formula xml:id="formula_434">H R (ϕ) =    0 H Rf u ′ (ϕ) H Ruf ′ (ϕ) 0    , (C.8)</formula><p>where H Rf u ′ (ϕ) is of dimension q(K + p) × nK with each block of size (K + p) × K and the (i, j)-th block is given as</p><formula xml:id="formula_435">[H Rf u ′ (ϕ)] [P j ,K i ] = - 1 nq l ′ ij (w ij )    I K 0 p×K    H Ruf ′ (ϕ) is the transpose of H Rf u ′ .</formula><p>Here H L (ϕ) + H R (ϕ) is the Hessian matrix of -L(Y|ϕ). Then the last part H P (ϕ) is a matrix due to the differentiation of penalty term P (Γ, U). It can be derived that for r, l ̸ = h ∈ [K] and s ∈</p><formula xml:id="formula_436">[p], ∂ ϕ [( q j=1 γ 2 jr q - n i=1 U 2 ir n ) 2 ] = 4 q j=1 γ 2 jr q - n i=1 U 2 ir n D -1 q ν rr ; ∂ 2 ϕϕ [( q j=1 γ 2 jr q - n i=1 U 2 ir n ) 2 ] = 8D -1 q ν rr ν ⊺ rr D -1 q +4 q j=1 γ 2 jr q - n i=1 U 2 ir n D -1 q    I q ⊗ E (K+p) rr 0 0 I n ⊗ E (K) rr    ; (C.9) ∂ 2 ϕϕ ( q j=1 γ jh γ jl ) 2 = 2[u hl u T hl + ( q j=1 γ jr γ jl )D 1,hl ]; (C.10) ∂ 2 ϕϕ ( n i=1 U ih U il ) 2 = 2[u lh u T lh + ( n i=1 U ir U il )D 2,lh ]; (C.11) ∂ 2 ϕϕ ( n i=1 U ir X is ) 2 = 2b rs b T rs , (C.12)</formula><p>where for r, l ̸ = h ∈ [K], s ∈ [p], we have the following:</p><formula xml:id="formula_437">D q =    qI q(K+p) nI nK    ; ν rl =    Γ [,l] ⊗ 1 (K+p) r -U [,r] ⊗ 1 (K) l    ; b rs =    0 q(K+p) X [,s] ⊗ 1 (K) r    ; u hl =    Γ [,h] ⊗ 1 (K+p) l + Γ [,l] ⊗ 1 (K+p) h 0 nK    1 (l&lt;h) +    0 q(K+p) U [,h] ⊗ 1 (K) l + U [,l] ⊗ 1 (K) h    1 (h&lt;l) ; D 1,rl =    I q ⊗ (E (K+p) rl + E (K+p) lr ) I nK×nK    ; D 2,rl =    0 q(K+p)×q(K+p) I n ⊗ (E (K) rl + E (K) lr )    .</formula><p>Here ⊗ is the Kronecker product. By the identifiability conditions 1 ′ -2 ′ , we have</p><formula xml:id="formula_438">4 q j=1 (γ 0 jr ) 2 q - n i=1 (U 0 ir ) 2 n D -1 q    I q ⊗ E (K+p) rr 0 0 I n ⊗ E (K) rr    = 0; ( q j=1 γ 0 jh γ 0 jl )D 1,hl = 0; ( n i=1 U 0 ih U 0 il )D 2,lh = 0.</formula><p>So the third part H P (ϕ) on parameters ϕ 0 can be written as</p><formula xml:id="formula_439">H P (ϕ 0 ) =c K r=1 D -1 q ν 0 rr (ν 0 rr ) ⊺ D -1 q + K r=1 K l=r+1 D -1 q u 0 hl (u 0 hl ) T D -1 q + K r=1 K l=r+1 D -1 q u 0 lh (u 0 lh ) T D -1 q + K r=1 p s=1 D -1 q b 0 rs (b 0 rs ) T D -1 q . (C.13) Proof of Proposition C.1 Proof. Recall that w ij = γ T j U i + β T j X i .</formula><p>Here we further define θ ij = γ T j U i , in which way we have w ij = θ ij + β T j X i . Define Θ = UΓ T and let θ v = (θ 11 , . . . , θ 1q , θ 21 , . . . , θ 2q , . . . , θ n1 , . . . , θ nq ). For the new parameter system, we define ψ = (θ v , B v ) T and a bijective mapping between the two parameter spaces Φ 0 = {ϕ|P (ϕ) = 0} and</p><formula xml:id="formula_440">Ψ 0 = {ψ| P (ψ) = 0, rank(Θ) ≤ K} as Π : ϕ → vec(ΓU T ) T , B T v T , (C.15)</formula><p>with the corresponding penalty function defined as</p><formula xml:id="formula_441">P (Θ) = - c 2q q j=1 1 n n i=1 θ ij X i 2 .</formula><p>Here 0 &lt; c &lt; b L Obviously P (Θ 0 ) = 0 as n i=1 U 0 i X T i = 0 K×p . Since rank(Θ) in ψ 0 is no larger than K, we can give a unique rank-K decomposition for Θ = ΓU T where U ∈ R n×K , Γ ∈ R q×K and q -1 Γ T Γ = n -1 U T U are diagonal matrices, the uniqueness (up to a signed permutation) of which can be implied by the singular value decomposition. This induces the inversion of the mapping Π as Π -1 (ψ) = Π(Θ, B) = (Γ, U, B). Recall that we have argued that ϕ 0 can be expressed as arg min ϕ∈B(D)∩Φ 0 -L(ϕ), we define</p><formula xml:id="formula_442">ψ = arg min ψ∈ B(D)∩Ψ 0 -Ľ(ψ), (C.16) where Ľ(ψ) = -(nq) -1 n i=1 q j=1 l ij (θ ij + β T j X i ) and B(D) = {ψ : ∥ψ∥ ∞ ≤ D} For the objective function Ľ(Y|ψ), we claim that ϕ ∈ Φ 0 ∩ B(D) is a minimizer of L(Y|ϕ) if and only if Π(ϕ) ∈ Ψ 0 ∩ B(D) is a minimizer of Ľ(Y|ψ). The 'if'</formula><p>part is trivial. For the 'only if' part, suppose ϕ 1 is the minimizer of -L(Y|ϕ) and Ľ(Y|Π(ϕ 1 )) &lt; Ľ(Y|ψ 2 ) for some ψ 2 ∈ Ψ 0 ∩ Ω ψ . Then since ψ 2 ∈ Ψ 0 , we can get a rank-K decomposition for Θ 2 as Θ 2 = vec(Γ 2 U T 2 ). Then L(Y|U 2 , Γ 2 , B 2 ) &gt; L(Y|ψ 1 ), which is a contradiction. This implies Π arg min</p><formula xml:id="formula_443">ϕ∈Φ 0 ∩B(D) -L(Y|ϕ) = arg min ψ∈Ψ 0 ∩ B(D)</formula><p>-Ľ(Y|ψ), which implies that ψ = Π( ϕ 0 ). In the following, we use ψ 0 to denote the solution to (C.16) for clarity. We will denote ψ 0 = Π(ϕ 0 ) as the true parameter. Let</p><formula xml:id="formula_444">Θ 0 = Γ 0 ( U 0 ) T and Θ 0 = Γ 0 (U 0 ) T .</formula><p>The derivatives of Ľ(ψ) = -Ľ(ψ) -P (ψ) with respect to ψ will be denoted as Š(ψ) = ∂ ψ Ľ(Y|ψ), Šθ (ψ) = ∂ θ Ľ(Y|ψ), and Šβ (ψ) = ∂ β Ľ(Y|ψ). The Hessian matrix will be denoted as Ȟ</p><formula xml:id="formula_445">(ψ) = ∂ 2 ψ Ľ(ψ) = ȞL (ψ) + ȞP (ψ) with ȞL = ∂ 2 ψψ - n i=1 q j=1 l ij (θ ij +β T j X i ) and ȞP = ∂ 2 ψψ P (Θ). Expand the objective function Ľ(ψ) at ψ 0 , it follows that Ľ(Y| ψ 0 ) = Ľ(Y|ψ 0 ) + Š(ψ 0 ) T ( ψ 0 -ψ 0 ) + 1 2 Ď-1/2 q ( ψ 0 -ψ 0 ) T Ď1/2 q Ȟ( ψ) Ď1/2 q Ď-1/2 q ( ψ 0 -ψ 0 ) , (C.17)</formula><p>where ϕ is some points between ψ 0 and ψ 0 , and Ďq is a scaling matrix defined as</p><formula xml:id="formula_446">Ďq =    nqI nq qI q    .</formula><p>Because P ( Θ 0 ) ≤ 0 and P (Θ 0 ) = 0, we have Ľ(Y| ψ 0 ) + P ( Θ 0 ) ≥ Ľ(Y|ψ 0 ) + P (Θ 0 ) = Ľ(Y|ψ 0 ). Therefore Ľ(Y| ψ 0 ) ≥ Ľ(Y|ψ 0 ), which together with (C.17), Lemmas C.14, C.15 gives</p><formula xml:id="formula_447">0 ≲δ -1 nq log q nq ∥ Θ 0 -Θ 0 ∥ F + log pq n p q ∥ B 0 -B 0 ∥ F - γ 2 1 nq ∥ Θ 0 -Θ 0 ∥ 2 F + 1 q ∥ B 0 -B 0 ∥ 2 F , which implies with p &lt; q that 1 nq ∥ Θ 0 -Θ 0 ∥ 2 F ≤ O p log n q + p log q n , 1 q ∥ B 0 -B 0 ∥ 2 F ≤ O p log n q + p log q n .</formula><p>Here γ is some positive constant specified in Lemma C.14 that does not depend on the dimension.</p><p>Let ρ 0 1 , . . . , ρ 0 K be the singular values of n -1/2 q -1/2 U 0 (Γ 0 ) T and υ 0 1 , . . . , υ 0 K be the corresponding left-singular vectors. Let ρ 1 , . . . , ρ K be the singular values of n -1/2 q -1/2 U 0 Γ 0T and υ 1 , . . . , υ K be the corresponding left-singular vectors. From a variant version of Davis-Kahan Theorem (Yi Yu and Samworth, 2015), we have</p><formula xml:id="formula_448">∥ υ k -υ 0 k ∥ 2 ≤ √ 2∥n -1/2 q -1/2 U 0 ( Γ 0 ) T -n -1/2 q -1/2 U 0 (Γ 0 ) T ∥ F /η, ≤ √ 2n -1/2 q -1/2 ∥ U 0 ( Γ 0 ) T -U 0 (Γ 0 ) T ∥ F /η (C.18) where η = min{| ρ k-1 -ρ 0 k | ∧ | ρ k+1 -ρ 0 k | : k = 1, . . . , K}. By Weyl's inequality, for all k, | ρ k -ρ 0 k | ≤ (nq) -1/2 ∥ U 0 ( Γ 0 ) T -U 0 (Γ 0 ) T ∥ F = (nq) -1/2 ∥ Θ -Θ 0 ∥ F = O p (ζ -1 nq ).</formula><p>Under the Assumption IV.8 and p &lt; δ nq , η is bounded and bounded away from zero in probability and it follows from (C.18</p><formula xml:id="formula_449">) that ∥ υ k -υ 0 k ∥ 2 = O p (ζ -1 nq )</formula><p>. Under penalty function P (Γ, U), we have the k-th factor to be n ρ k υ k . Thus we have</p><formula xml:id="formula_450">∥ U 0 -U 0 ∥ F = ∥ U 0 v -U 0 v ∥ 2 ≤ √ n| ρ k -ρ 0 k |∥ υ k ∥ 2 + √ nρ k ∥ υ k -υ 0 k ∥ 2 = O p √ qζ -1 nq .</formula><p>Similarly we also have</p><formula xml:id="formula_451">∥ Γ 0 -Γ 0 ∥ F = O p √ qζ -1 nq .</formula><p>Proof of Proposition C.2</p><p>Proof. The strategy for this proof is that we will first show the positive definiteness of the scaled Hessian matrix at ϕ 0 , i.e. we first show λ min D</p><formula xml:id="formula_452">1/2 q H(ϕ 0 )D 1/2 q ≥ γ 0</formula><p>for some positive γ 0 . Then we prove the local convexity in the region B(D) ∩ {ϕ :</p><formula xml:id="formula_453">√ p∥D -1/2 p (ϕ -ϕ 0 )∥ ≤ m}.</formula><p>Positive definiteness at ϕ 0 . First, it can be verified that all the eigenvalues of</p><formula xml:id="formula_454">D 1/2 q H L D 1/2 q</formula><p>are all nonnegative. We also observe that</p><formula xml:id="formula_455">D 1/2 q H L D 1/2 q + cV 0 V T 0 = -D -1/2 q n i=1 q j=1 {l ′′ ij (w 0 ij ) + c}    1 (q) j ⊗ Z 0 i 1 (n) i ⊗ γ 0 j       1 (q) j ⊗ Z 0 i 1 (n) i ⊗ γ 0 j    T D -1/2 q + c    I q ⊗ cn -1 n i=1 Z 0 i (Z 0 i ) T I n ⊗ cq -1 q j=1 γ 0 j (γ 0 j ) T    +    [V 0 V T 0 ] [1:q(K+p),1:q(K+p)] [V 0 V T 0 ] [q(K+p)+1:q(K+p)+nK,q(K+p)+1:q(K+p)+nK]    . (C.19)</formula><p>By selecting 0 &lt; c &lt; b L , the first and third terms are positive semi-definite. In addition, as n, q goes to infinity, and by the transition of true parameters ϕ * to the working parameters ϕ 0 satisfying identifiability conditions 1 ′ -2 ′ , we have q -1 q j=1 γ 0 j (γ 0 j ) T and n</p><formula xml:id="formula_456">-1 n i=1 U 0 i (U 0 i ) T converge to the diagonal matrix of eigenvalues of {(Σ 0 u ) 1/2 Σ 0 γ (Σ 0 u ) 1/2 } 1/2 . Besides, we also have n -1 n i=1 X i X T i converges to Σ x with λ min (Σ x ) ≥</formula><p>column vectors of N . The matrix N (ϕ 0 ) is an invertible matrix when set on ϕ 0 as τ h ̸ = τ l for any h ̸ = l when n, q are large enough where τ r converges to the rth</p><formula xml:id="formula_457">diagonal element in {(Σ 0 u ) 1/2 Σ 0 γ (Σ 0 u ) 1/2 } 1/2</formula><p>. By Assumption IV.8, the diagonal values in this matrix are distinct. We also define the following residual vectors:</p><formula xml:id="formula_458">ν (R) hl =D -1/2 q ν hl -V 0 N D -1/2 q ν hl ; b (R) ks =D -1/2 q b ks -V 0 N D -1/2 q b ks = D -1/2 q    Γ [,k] ⊗ 1 (K+p) K+s 0 nK    , and R = R 1 , R 2 , where R 1 = 0 K 2 +Kp , ν (R) 12 , • • • , ν (R) 1D , ν (R) 21 , 0 K 2 +Kp , ν (R) 23 , • • • , ν (R) KK-1 , 0 K 2 +Kp ; R 2 = b (R) 1 , • • • , b (R) p , b (R) 21 , • • • , b (R) qp .</formula><p>Here all column vectors of R 1 and R 2 are orthogonal to span(V 0 ). Next on ϕ 0 we have</p><formula xml:id="formula_459">D 1/2 q H P (ϕ 0 )D 1/2 q =cD -1/2 q K r=1 ν 0 rr (ν 0 rr ) T + h̸ =l u 0 hl (u 0 hl ) T + K r=1 p s=1 b 0 rs (b 0 rs ) T D -1/2 q =c(V 0 (ϕ 0 )N (ϕ 0 ) + R(ϕ 0 ))(V 0 (ϕ 0 )N (ϕ 0 ) + R(ϕ 0 )) T =cV 0 (ϕ 0 )N (ϕ 0 )N (ϕ 0 ) T V 0 (ϕ 0 ) T + c R 1 (ϕ 0 ) R 2 (ϕ 0 ) N (ϕ 0 ) T V 0 (ϕ 0 ) T + cV 0 (ϕ 0 )N (ϕ 0 )    R 1 (ϕ 0 ) T R 2 (ϕ 0 ) T    + cR 2 (ϕ 0 )R 1 (ϕ 0 ) T + cR 2 (ϕ 0 )R 2 (ϕ 0 ) T .</formula><p>Define by V the space spanned by the column vectors of V 0 , namely, the null space of</p><formula xml:id="formula_460">D 1/2 q H L D 1/2</formula><p>q . Then for any z, ∥z∥ = 1, let v ∈ V and u ∈ V ⊥ such that z = αv + βu and ∥v∥ = ∥u∥ = 1. Still, we have α 2 + β 2 = 1. Denote n 0 = N (z) = N (v) and to H P , that is,</p><formula xml:id="formula_461">z T D 1/2 q H P D 1/2 q z = (V 0 N + R)(V 0 N + R) T (C.27) + 2cz T K r=1 (n -1 n i=1 U 2 ir -q -1 q j=1 γ 2 jr )    I n+q ⊗ E (K+p) rr I n ⊗ E (K) rr    z + cz T D -1/2 q r&lt;l ( n i=1 U ih U il )D 1,rl + ( q j=1 γ jh γ jl )D 2,rl D -1/2 q z. (C.28)</formula><p>Notice that P (ϕ) = 0 does not necessarily hold for the general ϕ ∈ B(D), which results in extra terms in (C.28). Because √ p∥D -1/2 q (ϕ -ϕ 0 )∥ ≤ m and by Cauchy-Schwarz Inequality, we have</p><formula xml:id="formula_462">1 n n i=1 U 2 ir -(U 0 ir ) 2 ≤ 2n -1/2 ∥ U 0 -U 0 ∥n -1/2 ∥U 0 ∥ + n -1 ∥ U 0 -U 0 ∥ ≤ 2mn -1/2 ∥U 0 ∥/p + m 2 /p 2 ; (C.29)</formula><p>Similarly, we have</p><formula xml:id="formula_463">1 q q j=1 γ 2 jr - 1 q q j=1 (γ jr ) 2 ≤ 2mq -1/2 ∥Γ 0 ∥/p + m 2 /p 2 ; (C.30) 1 n n h&lt;l U ih U il ≤ 2mn -1/2 ∥U 0 ∥/p + m 2 /p 2 ; (C.31) 1 q q h&lt;l γ jh γ jl ≤ 2mq -1/2 ∥Γ 0 ∥/p + m 2 /p 2 . (C.32)</formula><p>From (C.29)-(C.32), the two terms in (C.28) can be bounded as follows</p><formula xml:id="formula_464">|(C.28)| ≤ cK(4K + 3)(mn -1/2 ∥U 0 ∥/p + mq -1/2 ∥Γ 0 ∥/p + m 2 /p 2 ) ≲ m p . (C.33) ∥ U 0 v -U G v ∥ ∞ .</formula><p>Then we conclude that ϕ is the maximizer of the objective function L(ϕ) inside this space with ∂ ϕ ∥D 1/2 q S(ϕ)∥ ζ = 0 w.h.p., which implies that S( ϕ) = 0, w.h.p. Recall that ϕ 0 is a global maximizer of the objective function in B(D), we conclude that ϕ = ϕ 0 and thus we proved Proposition C.3 and C.4</p><p>Proof. Expanding S( ϕ) at ϕ 0 using mean value theorem, we have</p><formula xml:id="formula_465">ϕ -ϕ 0 = M( ϕ) S( ϕ) -S(ϕ 0 ) , (C.36)</formula><p>where M is defined as</p><formula xml:id="formula_466">M(ϕ) = 1 0 H ϕ 0 + s(ϕ -ϕ 0 ) ds -1</formula><p>, and M(ϕ) defined as 1 0 H ϕ 0 + s(ϕ -ϕ 0 ) ds. We proceed to show ϕ lies in the interior of {ϕ :</p><formula xml:id="formula_467">√ p D -1/2 q (ϕ -ϕ 0 ) ≤ m}. By reorganizing the expression (C.36), we have D -1/2 q ( ϕ -ϕ 0 ) = D 1/2 q M( ϕ)D 1/2 q -1 D 1/2 q S<label>(</label></formula><p>ϕ) -S(ϕ 0 ) . Further, according to Proposition C.13, Lemma C.17 and Assumption IV.10, we have Proof. By Proposition C.4 we know that S f ( ϕ 0 ) = 0 w.h.p. We can use the integral form of mean value theorem to expand S f ( ϕ 0 )] [P j ] as follows:</p><formula xml:id="formula_468">D -1/2 q ( ϕ -ϕ 0 ) ≤ D 1/2 q S( ϕ) -S(ϕ 0 ) ≤ (n + pq) 1/2-1/ζ D 1/2 q S( ϕ) -S(ϕ 0 ) ζ ≤ 2(n + pq) 1/2-1/ζ D 1/2 q S(ϕ 0 ) ζ ≤ O p p √ pq ∧ n ϵ nq = o p<label>(</label></formula><formula xml:id="formula_469">0 =[S f j ( ϕ 0 )] [P j ] = -(nq) -1 n i=1 Z 0 i l ′ ij (γ 0 j ) T U 0 i + (β 0 j ) T X i (C.38) -(nq) -1 n i=1 [0,1] l ′′ ij (f 0 j ) T Z 0 i + s( f 0 j -f 0 j ) T Z 0 i Z 0 i ( Z 0 i ) T ( f 0 j -f 0 j )ds (C.39)</formula><p>For (C.39), according to Assumption IV.9(iii), we have</p><formula xml:id="formula_470">∥(C.39)∥ ≥ λ min b L q -1 (n -1 n i=1 Z 0 i ( Z 0 i ) T ) ∥ f 0 j -f 0 j ∥ ≳ q -1 ∥ f 0 j -f 0 j ∥.</formula><p>Before we discuss (C.38), for notational simplicity, here we denote b ′ ( U</p><formula xml:id="formula_471">0 i ) = b ′ (γ 0 j ) T U 0 i + (β 0 j ) T X i and b ′ (U 0 i ) = b ′ (γ 0 j ) T U 0 i + (β 0 j ) T X i . We next decom- pose (C.38) into ∥(C.38)∥ = (nq) -1 n i=1 Z 0 i Y ij -b ′ ( U 0 i ) = (nq) -1 n i=1 Z 0 i {Y ij -b ′ (U 0 i )} + (Y ij -b ′ (U 0 i ))( Z 0 i -Z 0 i ) + Z 0 i b ′ (U 0 i ) -b ′ ( U 0 i ) + b ′ (U 0 i ) -b ′ ( U 0 i ) ( Z 0 i -Z 0 i ) ≤ q -1 n -1 n i=1 Z 0 i l ′ ij (w 0 ij ) + n -1 n i=1 l ′ ij (w 0 ij )( U 0 i -U 0 i ) + n -1 n i=1 l ′′ ij ( w ij )Z 0 i U 0 i -U 0 i T γ 0 j + n -1 n i=1 l ′′ ij ( w ij ) U 0 i -U 0 i U 0 i -U 0 i T γ 0 j , (C.40)</formula><p>where w ij lies in the segment between w 0 ij and (γ 0 j ) T U i + (β 0 j ) T X i . For the first term we have ∥n -1 n i=1 Z 0 i l ′ ij (w 0 ij )∥ = pn -1 log qp from Proposition C.13. For the second term, by Assumption IV.9, we have</p><formula xml:id="formula_472">n i=1 [l ′ ij (w 0 ij )] 2 = O p (n 1/2 ) and therefore n -1 n i=1 l ′ ij (w 0 ij )( U 0 i -U 0 i ) ≤ O p n -1/2 ∥ U 0 -U 0 = O p p log qp n + log n q .</formula><p>For the third term we note that n i=1 U 0 i X T i = n i=1 U 0 i X T i = 0 K×p . Then together with Assumption IV.8 and Proposition C.1,</p><formula xml:id="formula_473">n -1 n i=1 l ′′ ij ( w ij )Z 0 i U 0 i -U 0 i T γ 0 j ≲ n -1 n i=1 Z 0 i U 0 i -U 0 i T ≤ n -1/2 U 0 -U 0 n -1/2 ∥U 0 ∥ ≤ O p p log qp n + log n q .</formula><p>For the fourth term by Proposition C.1 it can be bounded by pn -1 log(pq) + q -1 log n.</p><p>Then we conclude that (C.40) ≤ O p 1 q p log qp n + log n q .</p><p>Therefore, (C.38)+(C.39) = 0 implies that for any j ∈</p><formula xml:id="formula_474">[q] ∥ f 0 j -f 0 j ∥ = O p p log qp n + log n q , (C.41)</formula><p>which gives that for any j ∈</p><formula xml:id="formula_475">[q] ∥ γ 0 j -γ 0 j ∥ = O p p log qp n + log n q , ∥ β 0 j -β 0 j ∥ = O p p log qp n + log n q . for i ∈ [n], j ∈ [q], r ∈ [K], s ∈ [p]</formula><p>and ϕ ♭ in the segment between ϕ 0 and ϕ 0 . Invert the matrix H(ϕ 0 ) we have</p><formula xml:id="formula_476">f 0 j -f 0 j = -[H -1 (ϕ 0 )S(ϕ 0 )] [P j ] - 1 2 [H -1 (ϕ 0 )R] [P j ] ; (C.45) U 0 i -U 0 i = -[H -1 (ϕ 0 )S(ϕ 0 )] [q(K+p)+K i ] - 1 2 [H -1 (ϕ 0 )R] [q(K+p)+K i ] . (C.46)</formula><p>We use the following results to estimate the terms in (C.45) and (C.46):</p><p>Proposition C.12 Under Assumption IV.8-IV.11, we have</p><formula xml:id="formula_477">(i) [H -1 (ϕ 0 )S(ϕ 0 )] [P j ] -[H -1 Lf f ′ (ϕ 0 )S f (ϕ 0 )] [P j ] = O p p(nq) 3/ξ √ nq ϵ nq ; [H -1 (ϕ 0 )R] [P j ] = O p p(nq) 3/ξ ζ 2 nq . (ii) [H -1 (ϕ 0 )S(ϕ 0 )] [q(K+p)+K i ] -[H -1 Luu ′ (ϕ 0 )S u (ϕ 0 )] [K i ] = O p p 3/2 (nq) 3/ξ √ nq ϵ nq ; [H -1 (ϕ 0 )R] [q(K+p)+K i ] = O p p 3/2 (nq) 3/ξ ζ 2 nq .</formula><p>Proposition C.12 implies that the matrix H is nearly block-diagonal even though the dimension of some blocks is not fixed. Under certain scaling conditions, the diagonal blocks are of a higher order than the off-diagonal parts. These results lead to close approximations between [H -1 (ϕ 0 )S(ϕ 0 )] [P j ] and its expanded term</p><formula xml:id="formula_478">[H -1 Lf f ′ (ϕ 0 )S f (ϕ 0 )] [P j ] . Also the remainder term [H -1 (ϕ 0 )R] [P j ] can be shown to O p (p 2 ζ -2</formula><p>nq ), which is asymptotically negligible under the scaling condition.</p><p>According to Assumption IV.11(i), for any a, b ∈ R p+K with ∥a∥</p><formula xml:id="formula_479">= ∥b∥ = 1, b T [qH Lf f ′ (ϕ 0 )] [P j ,P j ] a = n -1 b T n i=1 l ′′ ij (w 0 ij )Z 0 i Z 0 i a p → b T Ψ 0 jz a and b T (Ω 0 jz ) -1/2 [n 1/2 qS f (ϕ 0 )] [P j ] = n -1/2 b T (Ω 0 jz ) -1/2 n i=1 l ′ ij (w 0 ij )Z 0 i d → N (0, 1). Because H Lf f ′ (ϕ 0 ) is block-diagonal, we have for any a ∈ R K+p with ∥a∥ = 1, √ na T (Σ 0 f,j ) -1/2 [H -1 Lf f ′ (ϕ 0 )S f (ϕ 0 )] [P j ] d → N (0, 1).</formula><p>Expanding (C.45), we have</p><formula xml:id="formula_480">√ n( f 0 j -f 0 j ) = - √ n[H -1 Lf f ′ (ϕ 0 )S f (ϕ 0 )] [P j ] - √ n{[H -1 (ϕ 0 )S(ϕ 0 )] [P j ] -[H -1 Lf f ′ (ϕ 0 )S f (ϕ 0 )] [P j ] } - √ n 2 [H -1 (ϕ 0 )R] [P j ] .</formula><p>Under the scaling condition that √ np(nq) 3/ξ /ζ 2 nq,p → 0, the small-order terms in Proposition C.12 (i) can be omitted. Hence we have the following asymptotic distribution</p><formula xml:id="formula_481">√ na T (Σ 0 f,j ) -1/2 ( f 0 j -f 0 j ) → N (0, 1), if p √ n(nq) 3/ξ ζ -2 nq,p → 0. (C.47)</formula><p>As a result, for a ∈ R K+p with ∥a∥ = 1</p><formula xml:id="formula_482">√ na T (Σ 0 f,j ) -1/2 ( β 0 j -β 0 j ) d → N (0, 1). Here Σ 0 f,j = (Ψ 0 jz ) -1 Ω 0 jz (Ψ 0 jz ) -1 .</formula><p>Similarly for U 0 i -U 0 i , we can expand (C.46) as</p><formula xml:id="formula_483">√ q( U 0 i -U 0 i ) = - √ q[H -1 Luu ′ (ϕ 0 )S u (ϕ 0 )] [K i ] - √ q [H -1 (ϕ 0 )S(ϕ 0 )] [q(K+p)+K i ] -[H -1 Luu ′ (ϕ 0 )S u (ϕ 0 )] [K i ] - √ q 2 [H -1 (ϕ 0 )R] [q(K+p)+K i ] . According to Assumption IV.11(ii) that [nH Luu ′ (ϕ 0 )] [K i ] = -q -1 q j=1 l ′′ ij γ 0 j (γ 0 j ) T p → Ψ 0 iγ and [nq 1/2 S u (ϕ 0 )] [K i ] = q -1/2 q j=1 l ′ ij γ 0 j d → N (0, Ω 0 iγ ), we have √ q[H -1 Luu ′ (ϕ 0 )S u (ϕ 0 )] [K i ] d → N (0, (Ψ 0 iγ ) -1 Ω 0 iγ (Ψ 0 iγ ) -1 ).</formula><p>Under the scaling condition that √ qp 3/2 (nq) 3/ξ /ζ 2 nq,p → 0, the small-order terms in Proposition C.12 (2) are negligible, so we have</p><formula xml:id="formula_484">√ q(Σ 0 u,i ) -1/2 ( U 0 i -U 0 i ) d → N (0, I K ), if p 3/2 √ q(nq) 3/ξ ζ -2 nq,p → 0. where Σ 0 u,i = (Ψ 0 iγ ) -1 Ω 0 iγ (Ψ 0 iγ ) -1 .</formula><p>The asymptotic variances Σ 0 u,i and b T Σ 0 f,j a can be consistently estimated by</p><formula xml:id="formula_485">Σ 0 u,i = q q j=1 l ′′ ij γ 0 j ( γ 0 j ) T -1 q j=1 ( l ′ ij ) 2 γ 0 j ( γ 0 j ) T ( q j=1 l ′′ ij γ 0 j ( γ 0 j ) T -1 ; b T Σ 0 f,j a = nb T n i=1 l ′′ ij Z 0 i ( Z 0 i ) T -1 n i=1 ( l ′ ij ) 2 Z 0 i ( Z 0 i ) T ( n i=1 l ′′ ij Z 0 i ( Z 0 i ) T -1 a,</formula><p>for any a, b ∈ R K+p with ∥a∥ = ∥b∥ = 1, and the consistency of Σ u,i and Σ f,j can be shown by Assumption IV.9 and Proposition C.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition C.8</head><p>Proof. We start with proving the consistency of estimator A 0 . We first define for</p><formula xml:id="formula_486">s = 2, • • • , p that L 0 s (a) = q -1 ∥B 0 [,s] -Γ 0 (a -a 0 s )∥ 1 = q -1 q j=1 |β 0 js -(γ 0 j ) T (a - a 0 s )| and let a 0 s = argmin a L 0 s (a)</formula><p>. Similarly, we let the loss function L s (a) to be</p><formula xml:id="formula_487">L s (a) = q -1 ∥ B 0 [,s] -Γ 0 (a -a 0 s )∥ 1 = q -1 q j=1 | β 0 js -( γ 0 j ) T (a -a 0 s )|. We denote a 0 s -a 0 s = argmin a L s (a).</formula><p>Preliminary Convergence of a 0 s . From the convergence rates for ϕ 0 , we have</p><formula xml:id="formula_488">| β 0 js -β 0 js | ≤ ∥ β 0 j -β 0 j ∥ = O p (ζ -1 nq,p ) and ∥ γ 0 j -γ 0 j ∥ = O p (ζ -1 nq,p ), then for any a, we have | L s (a)-L 0 s (a)| ≤ q -1 q j=1 | β 0 js -β 0 js |+∥ γ 0 j -γ 0 j ∥∥(a-a 0 s )∥ = O p (1+∥(a-a 0 s )∥)ζ -1 nq,p .</formula><p>(C.48)</p><p>As 0 = arg min a L 0 s (a) is unique, for any v, ∥∇ v L 0 s (0)∥ &gt; c for some c &gt; 0. Also since L 0 s is convex, we have ∥∇ v L 0 s (0 + tv)∥ &gt; c ′ . For any a s ̸ = 0, we have for certain</p><formula xml:id="formula_489">t that L 0 s (a s ) = L 0 s (0) + ∥∇ v L 0 s (tv)∥∥a s ∥ ≥ L 0 s (0) + c∥a s ∥. (C.49) By definition we have L s ( a 0 s -a 0 s ) ≤ L s (0). Then by taking a s = a 0 s -a 0 s in (C.49) we have L s (0) -L 0 s (0) ≥ L s ( a 0 s -a 0 s ) -L 0 s (0) ≥ c∥ a s -a 0 s ∥ -| L s ( a 0 s -a 0 s ) -L 0 s ( a s -a 0 s )|.</formula><p>As a result, we have by (C.48) that</p><formula xml:id="formula_490">c∥ a 0 s -a 0 s ∥ ≤ | L s ( a 0 s -a 0 s ) -L 0 s ( a 0 s -a 0 s )| + L s (0) -L 0 s (0) ≲ 1 + ∥ a 0 s -a 0 s ∥ ζ -1 nq,p . Therefore we have ∥ a 0 s -a 0 s ∥ = O p (ζ -1 nq,p )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and by definition we know</head><formula xml:id="formula_491">A 0 = ( a 0 1 , • • • , a 0 p ),</formula><p>from which we conclude that</p><formula xml:id="formula_492">∥ A 0 -A 0 ∥ F = O p (p 1/2 ζ -1 nq,p ).</formula><p>Refined convergence rates. We next obtain a more stringent rate for A 0 . We write L s (a) as</p><formula xml:id="formula_493">L s (a) =q -1 q j=1 β 0 js -( γ 0 j ) T (a -a 0 s ) =q -1 q j=1 β 0 js -β 0 js + β 0 js + (γ 0 j + γ 0 j -γ 0 j ) T (a -a 0 s )</formula><p>By (C.45) in Proposition C.6 and Proposition C.12, we can write the first order derivative of L s (a) using ψ defined in the main text as</p><formula xml:id="formula_494">∂ a L s (a) = q -1 q j=1 ψ δ js , a + O p p(nq) 3/ξ ζ -2 nq,p .</formula><p>We next introduce the following lemma extended from Corollary 2.2 in <ref type="bibr" target="#b70">He and Shao (1996)</ref>.</p><p>Lemma C.11 <ref type="bibr" target="#b70">(He and Shao, 1996)</ref>. Suppose there exists α s0 such that χ s (α s0 ) = q j=1 Eψ(δ js , α s0 ) = 0 with ∥α s0 ∥ ≤ O p (v q ) for some sequence v q = o p (q -1/2 ) as q → ∞. In a neighbourhood of α s0 , χ s (a) has a nonsingular derivative D q such that {D q (α s0 )} -1 = O(q -1 ) and |D q (a) -D q (α s0 )| ≤ kq|a -α s0 |. Moreover, assume q -1 q j=1 ψ(δ js , α s0 ) = O p (v q ). Let α s = arg min a q j=1 ψ(δ js , a), then we have ∥ α s ∥ = O p (v q ). By Assumption IV.12, the conditions in Lemma C.11 are satisfied with rate ι nq,p , and therefore the minimizer of q -1 q j=1 ψ(δ js , a) denoted as a ψ satisfies ∥ a ψ ∥ = O p (ι nq,p ). Then since a 0 s -a 0 s is the minimizer of L s (a), it also minimizes ∂ a L s (a) .</p><p>Then by a similar argument as in proving the preliminary convergence rates, we</p><formula xml:id="formula_495">have ∥ a 0 s -a 0 s -a ψ ∥ ≲ | L s (a) -q -1 q j=1 ψ(δ js , a)| = O p (p(nq) 3/ξ ζ -2 nq,p ). Therefore, ∥ a 0 s -a 0 s ∥ = O p (ι nq,p ∨ p(nq) 3/ξ ζ -2 nq,p ), which implies ∥ A 0 -A 0 ∥ F = O p p 1/2 ι nq,p ∨ p 3/2 (nq) 3/ξ ζ -2 nq,p . (C.50)</formula><p>We next show the consistency rate of G 0 . First, we bound ∥(q -1 ( Γ </p><formula xml:id="formula_496">0 ) T Γ 0 ) 1/2 -(q -1 (Γ 0 ) T Γ 0 ) 1/2 ∥ F . Write q -1 ( Γ 0 ) T Γ 0 -q -1 (Γ 0 ) T Γ 0 (C.51) = q -1 q j=1 γ 0 j ( γ 0 j ) T -γ 0 j (γ 0 j ) T = q -1 q j=1 ( γ 0 j -γ 0 j )( γ 0 j -γ 0 j ) T + γ 0 j ( γ 0 j -γ 0 j ) T + ( γ 0 j -γ 0 j )(γ 0 j ) 0T . (C.</formula><formula xml:id="formula_497">( γ 0 j -γ 0 j )(γ 0 j ) T = q j=1 n i=1 l ′′ ij (w 0 ij )Z 0 i (Z 0 i ) T -1 n i=1 l ′ ij (w 0 ij )Z 0 i (γ 0 j ) T + O p pq(nq) 3/ξ ζ 2 nq,p =O p pq n ϵ nq ∨ pq(nq) 3/ξ ζ 2 nq,p<label>.</label></formula><p>Plugging this estimate to the last two terms in (C.52) we have</p><formula xml:id="formula_498">∥q -1 ( Γ 0 ) T Γ 0 -q -1 (Γ 0 ) T Γ 0 ∥ F = O p p/(nq)ϵ nq ∨ p(nq) 3/ξ ζ -2 nq,p . (C.53)</formula><p>Similarly from (C.46), (ii) of Proposition C.12, we have</p><formula xml:id="formula_499">q i=1 ( U 0 i -U 0 i )U 0 i T = O p p/(nq)ϵ nq ∨ p 3/2 (nq) 3/ξ nζ -2 nq,p . By Proposition C.1 we know ∥ U 0 -U 0 ∥ = O p (n 1/2 ζ -1 nq,p ). Then by ( U 0 ) T X = (U 0 ) T X = 0 K×p , we have ( U 0 +X( A 0 ) T ) T ( U 0 + X( A 0 ) T ) -(U 0 + X(A 0 ) T ) T (U 0 + X(A 0 ) T ) = ( U 0 ) T U 0 -(U 0 ) T U 0 + ( A 0 ) T (X T X) A 0 -(A 0 ) T (X T X)A 0 = n i=1 ( U 0 i -U 0 i )( U 0 i -U 0 i ) T + U 0 i ( U 0 i -U 0 i ) T + ( U 0 i -U 0 i )(U 0 i ) T + ( A 0 -A 0 ) T (X T X) A 0 -(A 0 ) T (X T X)( A 0 -A 0 ),</formula><p>together with the convergence rate of A 0 , we have </p><formula xml:id="formula_500">n -1 ( U 0 + X( A 0 ) T ) T ( U 0 + X( A 0 ) T ) -n -1 (U 0 + X(A 0 ) T ) T (U 0 + X(A 0 ) T ) = O p √ p ζ nq,p ∨ p 3/2 (nq) 3/ξ ζ 2 nq,p<label>(</label></formula><formula xml:id="formula_501">∥ G 0 -G 0 ∥ F = O p √ p ζ nq,p ∨ p 3/2 (nq) 3/ξ ζ 2 nq,p . (C.55)</formula><p>The consistency rate under Assumption IV.12 can be similarly derived for G 0 based on (C.50).</p><p>Proposition C.12. Under Assumption IV.8-IV.11, we have</p><formula xml:id="formula_502">(i) [H -1 (ϕ 0 )S(ϕ 0 )] [P j ] -[H -1 Lf f ′ (ϕ 0 )S f (ϕ 0 )] [P j ] = O p p(nq) 3/ξ √ nq ϵ nq ; (C.56) [H -1 (ϕ 0 )R] [P j ] = O p p(nq) 3/ξ ζ 2 nq . (C.57) (ii) [H -1 (ϕ 0 )S(ϕ 0 )] [q(K+p)+K i ] -[H -1 Luu ′ (ϕ 0 )S u (ϕ 0 )] [K i ] = O p p 3/2 (nq) 3/ξ √ nq ϵ nq ; (C.58) [H -1 (ϕ 0 )R] [q(K+p)+K i ] = O p p 3/2 (nq) 3/ξ ζ 2 nq . (C.59)</formula><p>Proof. We write the Hessian matrix on ϕ 0 as</p><formula xml:id="formula_503">H(ϕ 0 ) =    H f f ′ H f u ′ H uf ′ H uu ′    , with H f f ′ = H Lf f ′ + H Rf f ′ + Λ 1 Λ T 1 , H f u ′ = H Lf u ′ + Λ 1 Λ T 2 , H uf ′ = H Luf ′ + Λ 2 Λ T 1 , H uu ′ = H Luu ′ + H Ruu ′ + Λ 2 Λ T 2 .</formula><p>We omit the Hessian matrix's dependence on ϕ 0 because in this proof we only need estimates of the Hessian matrix on ϕ 0 . By Sherman-Morrison-Woodbury formula, we get the exact expression for the blocks in</p><formula xml:id="formula_504">H(ϕ 0 ) -1 =    H f f ′ H f u ′ H uf ′ H uu ′    , as H f f ′ = H -1 f f ′ + H -1 f f ′ H f u ′ H -u H uf ′ H -1 f f ′ , H f u ′ = H f f ′ H f u ′ H -1 uu ′ , H uu ′ = H -1 uu ′ + H -1 uu ′ H uf ′ H -f H f u ′ H -1 uu ′ and H uf ′ = H uu ′ H uf ′ H -1 f f ′ . Here we define H -f = (H f f ′ - H f u ′ H -1 uu ′ H uf ′ ) -1 and H -u = (H uu ′ -H uf ′ H -1 f f ′ H f u ′ ) -1</formula><p>. By Proposition C.2 we have l 2 estimates for H -f and H -u as</p><formula xml:id="formula_505">∥H -f ∥ = O p (q), ∥H -u ∥ = O p (n)</formula><p>Then jth block of the first row of H -1 (ϕ 0 )S(ϕ 0 ) is given as</p><formula xml:id="formula_506">H -1 (ϕ 0 )S(ϕ 0 ) [P j ] = H -1 f f ′ S f [P j ] (C.60a) -H -1 f f ′ H f u ′ H -u H uf ′ H -1 f f ′ S f [P j ] (C.60b) + H -1 f f ′ H f u ′ H -1 uu ′ S u [P j ] (C.60c) -H -1 f f ′ H f u ′ H -u H uf ′ H -1 f f ′ H f u ′ H -1 uu ′ S u [P j ] . (C.60d)</formula><p>and the jth block of the first row of H -1 (ϕ 0 )R is given as</p><formula xml:id="formula_507">H -1 (ϕ 0 )R [P j ] = H -1 f f ′ R f [P j ] (C.61a) -H -1 f f ′ H f u ′ H -u H uf ′ H -1 f f ′ R f [P j ] (C.61b) + H -1 f f ′ H f u ′ H -1 uu ′ R u [P j ] (C.61c) -H -1 f f ′ H f u ′ H -u H uf ′ H -1 f f ′ H f u ′ H -1 uu ′ R u [P j ] ,<label>(C.61d)</label></formula><p>Here all blocks in H are taken on ϕ 0 . For (C.60a), write</p><formula xml:id="formula_508">H -1 f f ′ = H Lf f ′ + Λ 1 Λ T 1 -1 = H -1 Lf f ′ -H -1 Lf f ′ Λ 1 I K 2 + Λ T 1 H Lf f ′ Λ 1 -1 Λ T 1 H -1 Lf f ′ .</formula><p>By from (i) and (vii) of Lemma C.16 and (i) of Lemma C.18, </p><formula xml:id="formula_509">H -1 Lf f ′ Λ 1 I K 2 + Λ T 1 H Lf f ′ Λ 1 -1 Λ T 1 H -1 Lf f ′ S f [P j ] = [H -1 Lf f ′ ] [P j ,P j ] [Λ 1 ] [P j ,] I K 2 + Λ T 1 H -1 Lf f ′ Λ 1 -1 Λ T 1 H -1 Lf f ′ S f ≤ O p p nq ϵ nq , which implies ∥(C.60a)-H -1 Lf f ′ S f [P j ] ∥ ≤ O p p/(nq</formula><formula xml:id="formula_510">C.60b) ≤ H -1 f f ′ H f u ′ [P j ,] H -u H uf ′ H -1 f f ′ S f ≤O p p(nq) 2/ξ √ nq ϵ nq</formula><p>For (C.60c), by (ii), (iii) of Lemma C.16 and (iv), (vi) of Lemma C.18 we have</p><formula xml:id="formula_511">(C.60c) = H -1 Lf f ′ [P j ,P j ] H f u ′ H -1 uu ′ S u [P j ] + H -1 Lf f ′ [P j ,P j ] Λ 1 [P j ,] I K 2 + Λ T 1 H Lf f ′ Λ 1 -1 Λ T 1 H -1 Lf f ′ H f u ′ H -1 uu ′ S u =O p p √ nq (nq) 1/ξ ϵ nq .</formula><p>And for (C.60d), by (i), (iii), (v) of Lemma C.16 and (vi) of Lemma C.18, we have</p><formula xml:id="formula_512">(C.60d) = H -1 f f ′ H f u ′ [P j ,] H -u H uf ′ H -1 f f ′ H f u ′ H -1 uu ′ S u ≤ O p √ p(nq) 3/ξ √ nq ϵ nq .</formula><p>Combining these together, we have</p><formula xml:id="formula_513">[H -1 (ϕ 0 )S(ϕ 0 )] [P j ] -[H -1 Lf f ′ (ϕ 0 )S f (ϕ 0 )] [P j ] = O p p(nq) 3/ξ √ nq ϵ nq .</formula><p>For (C.61a), the first term can be given as </p><formula xml:id="formula_514">H -1 f f ′ R f [P j ] ≤ H -1 Lf f ′ [P j ,P j ] R f [P j ] + H -1 Lf f ′ [P j ,P j ] [Λ 1 ] [P j ,] I K 2 + Λ T 1 H Lf f ′ Λ 1 -1 Λ T 1 H -1 Lf f ′ R f By (i),<label>(</label></formula><formula xml:id="formula_515">C.61b) ≤ [H -1 f f ′ H f u ′ ] [P j ,] H -u H uf ′ H -1 f f ′ R f = O p p(nq) 2/ξ ζ 2 nq</formula><p>For third part (C.61c), by (ii), (v) of Lemma C.16 and Lemma C.12 we have</p><formula xml:id="formula_516">(C.61c) = H -1 f f ′ H f u ′ ] [P j ,] H -1 uu ′ R u ≤ O p p(nq) 1/ξ ζ 2 nq,p</formula><p>For fourth part (C.61d), similar by (i), (ii), (iii), (v) of Lemma C.16 and Lemma C.12, we have</p><formula xml:id="formula_517">(C.61d) ≤ H -1 f f ′ H f u ′ [P j ,] H -u H uf ′ H -1 f f ′ H f u ′ H -1 uu ′ R u = O p p(nq) 3/ξ ζ 2 nq .</formula><p>Then H -1 (ϕ 0 )R [P j ] can be bounded by O p p(nq) 3/ξ ζ -2 nq,p .</p><p>For the ith block of H -1 (ϕ 0 )S(ϕ 0 ), it can be written as</p><formula xml:id="formula_518">H -1 (ϕ 0 )S(ϕ 0 ) [K i ] = H -1 uu ′ S u [K i ] (C.62a) -H -1 uu ′ H uf ′ H -f H f u ′ H -1 uu ′ S u [K i ] (C.62b) + H -1 uu ′ H uf ′ H -1 f f ′ S f [K i ] (C.62c) -H -1 uu ′ H uf ′ H -f H f u ′ H -1 uu ′ H uf ′ H -1 f f ′ S f [K i ]</formula><p>.</p><p>(C.62d) and similarly the ith block of second row of H -1 (ϕ 0 )R is given as</p><formula xml:id="formula_519">H -1 (ϕ 0 )R [K i ] = H -1 uu ′ R u [K i ] (C.63a) -H -1 uu ′ H uf ′ H -f H f u ′ H -1 uu ′ R u [K i ] (C.63b) + H -1 uu ′ H uf ′ H -1 f f ′ R f [K i ] (C.63c) -H -1 uu ′ H uf ′ H -f H f u ′ H -1 uu ′ H uf ′ H -1 f f ′ R f [K i ] , (C.63d)</formula><p>Here all blocks in H are taken on ϕ 0 . For (C.62a), similar to the procedure of bounding (C.60a), by (ii) and (viii) of Lemma C.16 and (v) of Lemma C.18 we have </p><formula xml:id="formula_520">(C.62a) -H Luu ′ [K i ,K i ] [S u ] [K i ] ≤ H -1 Luu ′ Λ 2 I K 2 +Kp + Λ T 2 H Luu ′ Λ 2 -1 Λ T 2 H -1 Luu ′ S u [K i ] = [H -1 Luu ′ ] [K i ,K i ] [Λ 2 ] [K i ,] I K 2 +Kp + Λ T 2 H -1 Luu ′ Λ 2 -1 Λ T 2 H -1 Luu ′ S u ≤ O p p √ nq (nq)</formula><formula xml:id="formula_521">≤ H -1 Luu ′ [K i ,K i ] H uf ′ H -1 f f ′ S f [K i ] + H -1 Luu ′ [K i ,K i ] Λ 2 [K i ,] I K 2 +Kp + Λ T 2 H Luu ′ Λ 2 -1 Λ T 2 H -1 Luu ′ H uf ′ H -1 f f ′ S f ≤O p p 3/2</formula><p>√ nq (nq) 1/ξ ϵ nq .</p><p>Finally for (C.62d), by (ii), (iv), (vi) of Lemma C.16 and (iii) of Lemma C.18, we have</p><formula xml:id="formula_522">∥(C.62d)∥ ≤ H -1 uu ′ H uf ′ [K i ,] H -f H f u ′ H -1 uu ′ H uf ′ H -1 f f ′ S f = O p p 3/2 (nq) 3/ξ √ nq ϵ nq .</formula><p>For the first term (C.63a), similar with (C.61a), by (ii), (viii) of Lemma C.16 and Lemma C.12, we have (C.63a)</p><formula xml:id="formula_523">≤ [H -1 Luu ′ ] [K i ,K i ] [Λ 2 ] [K i ,] I K 2 +Kp + Λ T 2 H -1 Luu ′ Λ 2 -1 Λ T 2 H -1 Luu ′ R u = O p p 3/2 ζ 2 nq .</formula><p>For the second term (C.63b), similar with (C.61b), by (ii), (iii), (vi) of Lemma C.16</p><p>and Lemma C.12, we have</p><formula xml:id="formula_524">∥(C.63b)∥ ≤ H -1 uu ′ H uf ′ [K i ,] H -f H f u ′ H -1 uu ′ R u ≤ H -1 uu ′ H uf ′ [K i ,] H -f H f u ′ H -1 uu ′ R u =O p p 3/2 (nq) 2/ξ ζ 2 nq .</formula><p>For the third term (C.63c), similar to (C.61c), by (i), (vi) of Lemma C.16 and Lemma C.12, we have</p><formula xml:id="formula_525">(C.63c) ≤ H -1 uu ′ H uf ′ [K i ,] H -1 f f ′ R f = O p p 3/2 (nq) 1/ξ ζ 2 nq .</formula><p>For the fourth term in (C.63d), similar to (C.61d), by (i), (ii), (iii), (vi) of Lemma C.16</p><p>and Lemma C.12, we have</p><formula xml:id="formula_526">∥(C.63d)∥ ≤ H -1 uu ′ H uf ′ [K i ,] H -f H f u ′ H -1 uu ′ H uf ′ H -1 f f ′ R f = O p p 3/2 (nq) 3/ξ ζ 2 nq .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Proof of Auxiliary Lemmas</head><p>Lemma C.13 (First-order concentration). Under Assumptions IV.8-IV.9, we have estimates for the first order derivatives on ϕ 0 as:</p><formula xml:id="formula_527">∥n 1/2 S U (ϕ 0 )∥ 2 = O p log n q ; ∥q 1/2 S f (ϕ 0 v )∥ 2 = O p p log qp n .</formula><p>Scaled by matrix D p we can write</p><formula xml:id="formula_528">∥D 1/2 q S(ϕ 0 )∥ 2 = O p ζ -1 nq,p ,</formula><p>or ∥D 1/2 q S(ϕ 0 )∥ ζ ≤ O p (p 1/2 (nq) ϵ-1/2 ) when ζ &gt; 1/ϵ.</p><p>Proof. Assumption IV.9 in the main text imposes regularity conditions for parameters ϕ 0 that U 0 i and γ 0 j are bounded for i ∈ [n] and j ∈ [q]. Based on the assumptions, we have ∥l ′ ij U 0 ir ∥ φ 1 ≤ 2M 2 , for j = 1, . . . , q and r = 1, . . . , K.</p><p>The variable l ′ ij U 0 ir is sub-exponential with ∥l ′ ij U 0 ir ∥ φ 1 ≤ 2M 2 and E(l ′ ij U 0 ir ) = 0.</p><p>By the definition of sub-exponential variable, there exists a universal constant c &gt; 0 such that, for any 0 &lt; ν &lt; (2cM 2 ) -1 , the following inequality holds: Under ν = ν * , (C.64) ≤ exp{-nq 2 t 2 /(8c 2 M 2 )}. If ν * falls outside of (0, (2cM 2 ) -1 ), then (C.64) is minimized at ν = (2cM 2 ) -1 . So (C.64) ≤ exp{(2nc -nqt)/(2cM 2 )}.</p><formula xml:id="formula_529">E[exp{νl ′ ij (w ij )U 0 ir }] ≤ exp{2ν 2 c 2 M 2 },</formula><p>In conclusion,</p><formula xml:id="formula_530">P |(nq) -1 n i=1 l ′ ij (w ij )U 0 ir | ≥ t ≤ 2 exp -min nq 2 t 2 8c 2 M 2 , nqt 2cM 2 .</formula><p>By the union bound inequality, we have</p><formula xml:id="formula_531">P max j ∥(nq) -1 n i=1 l ′ ij (w ij )U 0 i ∥ ∞ ≥ t ≤ 2qK exp -min nq 2 t 2 8c 2 M 2 , nqt 2cM 2 .</formula><p>Take t = cM (log q) 1/2 n -1/2 q -1 , and we have</p><formula xml:id="formula_532">∥S γ (ϕ 0 )∥ ∞ = max j ∥(nq) -1 n i=1 l ′ ij (w ij )U 0 i ∥ ∞ = O p log q nq 2 ,</formula><p>which further leads to ∥q 1/2 S γ (ϕ 0 )∥ 2 = O p log q n . (C.65)</p><p>Using similar techniques, we have ∥S U (ϕ 0 )∥ ∞ = O p log n n 2 q , and ∥n 1/2 S U (ϕ 0 )∥ 2 = O p log n q .</p><p>(C.66)</p><p>For ∥q 1/2 S β (ϕ 0 )∥ 2 , because</p><formula xml:id="formula_533">P max j ∥(nq) -1 n i=1 l ′ ij (w ij )X i ∥ ∞ ≥ t ≤ 2qp exp -min nq 2 t 2 c 2 M 2 , nqt 2cM 2 .</formula><p>Take t = cM (log qp) 1/2 n -1/2 q -1 , and we have where the non-zero parts of blocks of ȞL are given as ȞLθθ ′ (ψ) [(j-1)q+i,(j-1)q+i] = -(nq) -1 l ′′ ij (θ ij + β T j X i );</p><formula xml:id="formula_534">∥S β (ϕ 0 )∥ ∞ = max j ∥(nq) -1 n i=1 l ′ ij (w ij )X i ∥ ∞ = O p</formula><p>ȞLθβ ′ (ψ) [(j-1)q+i,(j-1)p+1:jp] = -(nq) -1 l ′′ ij (θ ij + β T j X i )X i ;</p><p>ȞLββ ′ (ψ) [(j-1)p+1:jp,(j-1)p+1:jp] = -(nq) -1</p><formula xml:id="formula_535">n i=1 l ′′ ij (θ ij + β T j X i )X i X T i .</formula><p>First note that Ȟj :=    nq ȞLθθ ′ (ψ) [((j-1)q+1):jq,((j-1)q+1):jq] n 1/2 q ȞLθβ ′ (ψ) [((j-1)q+1):jq,(j-1)p+1:jp] n 1/2 q ȞLθβ ′ (ψ) [(j-1)p+1:jp,((j-1)q+1):jq] q ȞLββ ′ (ψ) [(j-1)p+1:jp,(j-1)p+1:jp]</p><formula xml:id="formula_536">   + c    n -1 XX T -n -1/2 X -n -1/2 X T I p    = n i=1 (-l ′′ ij (w ij ) -c)    1 (n) i n -1/2 X i       1 (n) i n -1/2 X i    T + c    I n + n -1 XX T n -1 X T X + I p    .</formula><p>We let c &lt; b l and the first term is positive definite. For the second term, since ∥n -1 XX T ∥ max = O(p/n), there exist some γ ′ &lt; 1 such that λ min I n + n -1 XX T &gt; γ ′ .</p><p>For the lower-right part, by Assumption IV.8, we also have λ min I p + n -1 X T X &gt; γ ′ .</p><p>Furthermore, the zero eigenvalues for Ď1/2 q ȞL Ď1/2 q has multiplicity pq with corresponding eigenvectors:</p><formula xml:id="formula_537">νjs = √ q Ď-1/2 q      1 (q) j ⊗    X [,s] -1 (p) s         , j ∈ [q], s ∈ [p].</formula><p>Note that the eigenvectors do not depend on the parameters. Here 1 (q) j is a qdimensional indicator vector, 1 where ν(1) js = n -1/2 ((1</p><formula xml:id="formula_538">(q) j ) T ⊗ X T [s,] , 0 T pq ) T . Further define ν(2) js = 0 T nq ,<label>(1 (qp)</label></formula><p>(j-1)p+s ) T . Then νjs = ν(1) js -ν(2) js .</p><p>Suppose V = span Ď1/2 q νjs : j = 1, • • • , q, s = 1, • • • , p is the null space of Ď1/2 q ȞL Ď1/2 q . Here V is a constant space that does not depend on the parameters (but depends on the covariates X i ). For any w ∈ R nq+qp ∥w∥ = 1, let v ∈ V and u ∈ V⊥ such that w = αv + βu and ∥v∥ = ∥u∥ = 1. So we have α 2 + β 2 = 1.</p><p>Let v = q j=1 p s=1 λ js νjs . The coefficients {λ js } j,s are unique as { νjs } are linear independent. We also have v T νjs = λ js + n -1 X T [,s] p r=1 λ jr X [,r] := λ js + λ js (X) and v T ν(2) js = -λ js . Here λ js (X) is the sth column of (λ j1 , • • • , λ jp ) T n -1 X T X. Then Suppose on the contrary that for any ϵ 0 &gt; 0 we can select a set of parameters such that the above lower bound for w T Ď1/2 q Ȟ(ψ) Ď1/2 q w is smaller than ϵ 0 . Since all there terms are positive, we have Lemma C.15. Under Assumption IV.9, the first order derivatives of Ľ on ψ 0 has (i) ∥H -1 Lf f ′ ∥ = O p (q), ∥H -1 f f ′ ∥ = O p (q), and ∥H -1 f f ′ ∥ 1 = O p ( √ pq).</p><p>(ii) ∥H -1 Luu ′ ∥ = O p (n), ∥H -1 uu ′ ∥ = O p (n), and ∥H -1 uu ′ ∥ 1 = O p ( √ pn).</p><p>(iii) max i [H uf ′ ] [K i ,] = O p (p 1/2 + (nq) 1/ξ )n -1 q -1/2 , ∥H uf ′ ∥ 1 = O p (nq) 1/ξ q -1 , and ∥H uf ′ ∥ = O p (nq) -1/2+1/ξ .</p><p>(iv) max j [H f u ′ ] [P j ,] = O p (nq) 1/ξ n -1 q -1/2 , ∥H f u ′ ∥ 1 = O p (p + (nq)) 1/ξ n -1 , and</p><formula xml:id="formula_539">∥H f u ′ ∥ = O p (nq) -1/2+1/ξ . (v) max j [H -1 f f ′ H f u ′ ] [P j ,] = O p (nq) 1/ξ n -1/2 . (vi) max i [H -1 uu ′ H uf ′ ] [K i ,] = O p (p 1/2 + (nq) 1/ξ )q -1/2 .</formula><p>(vii) ∥[Λ 1 ] [P j ,•] ∥ = O(q -1 ) and ∥Λ 1 ∥ = O(q -1/2 ).</p><formula xml:id="formula_540">(viii) ∥[Λ 2 ] [K i ,•] ∥ = O(p 1/2 n -1</formula><p>) and ∥Λ 2 ∥ = O(n -1/2 ).</p><p>Here we omitted the dependence on ϕ 0 , also in the following proof.</p><p>Proof. By the definition of Λ 1 in section C.1, it is obvious that ∥[Λ 1 ] [P j ,•] ∥ = O(q -1 ), ∥Λ T 1 ∥ 1 = O(q -1 ), ∥Λ 1 ∥ 1 = O(1), ∥Λ 1 ∥ = O(q -1/2 );</p><p>(C.77)</p><p>Which implies (vii). For Λ 2 , first it is obvious that ∥[Λ 2 ] [K i ,1:K 2 ] ∥ = O(n -1 ) and</p><formula xml:id="formula_541">∥[Λ 2 ] [K i ,1:K 2 +1:K 2 +Kp] ∥ = O(p 1/2 n -1</formula><p>). Also note that</p><formula xml:id="formula_542">∥[Λ 2 ] [,K 2 +1:K 2 +Kp] ∥ = λ max n -1 X T X ⊗ I K = O(1).</formula><p>Then the bounds for Λ 2 are given as</p><formula xml:id="formula_543">∥[Λ 2 ] [K i ,•] ∥ = O p (p 1/2 n -1</formula><p>), ∥Λ T 2 ∥ 1 = O p (pn -1 ), ∥Λ 2 ∥ 1 = O p (1), ∥Λ 2 ∥ = O p (n -1/2 ), (C.78) which implies and (viii).</p><p>Next for (i), because H Lf f ′ is block diagonal, by Assumptions IV.8-IV.9, we have <ref type="bibr">(C.79)</ref> and also for the l 1 -norm, we have</p><formula xml:id="formula_544">∥H -1 Lf f ′ ∥ = max j -(nq) -1 n i=1 l ′′ ij Z i Z T i -1 ≤ q b L max j n -1 n i=1 Z i Z T i -1 = O p (q),</formula><formula xml:id="formula_545">H -1 Lf f ′ 1 = (nq) -1 n i=1 l ′′ ij Z i Z T i -1 1 ≤ √ q (nq) -1 n i=1 l ′′ ij Z i Z T i -1 = O p ( √ pq).</formula><p>(C.80) <ref type="formula" target="#formula_538">1</ref>), the l 1 -norm for (I K 2 + Λ T 1 H -1 Lf f ′ Λ 1 ) -1 can be bounded by</p><formula xml:id="formula_546">Since Λ T 1 H -1 Lf f ′ Λ 1 is positive semi-definite and ∥Λ T 1 H -1 Lf f ′ Λ 1 ∥ ≤ ∥Λ 1 ∥ 2 ∥H -1 Lf f ′ ∥ ≤ O(</formula><formula xml:id="formula_547">(I K 2 + Λ T 1 H -1 Lf f ′ Λ 1 ) -1 1 ≤ K (I K(K+1)/2 + Λ T 1 H -1 Lf f ′ Λ 1 ) -1 ≤ O p (1).</formula><p>Next for H f f ′ = H Lf f ′ + Λ 1 Λ T 1 , we use Woodbury identity to give</p><formula xml:id="formula_548">H -1 f f ′ = H -1 Lf f ′ -H -1 Lf f ′ Λ 1 I K 2 + Λ T 1 H -1 Lf f ′ Λ 1 -1 Λ T 1 H -1 Lf f ′ = H -1 Lf f ′ + H -1 Lf f ′ Λ 1 I K 2 + Λ T 1 H -1 Lf f ′ Λ 1 -1 Λ T 1 H -1</formula><p>Lf f ′ ≤O p (q), (C.81)</p><p>For the l 1 estimate of H -1 f f ′ , by (C.77),</p><formula xml:id="formula_549">H -1 Lf f ′ Λ 1 I K 2 + Λ T 1 H -1 Lf f ′ Λ 1 -1 Λ T 1 H -1 Lf f ′ 1 = max j H -1 Lf f ′ Λ 1 I K 2 + Λ T 1 H -1 Lf f ′ Λ 1 -1 [Λ 1 ] T [P j ,] [H -1 Lf f ′ ] [P j ,P j ] 1 ≤ √ pq max j H -1 Lf f ′ Λ 1 I K 2 + Λ T 1 H -1 Lf f ′ Λ 1 -1 [Λ 1 ] T [P j ,]</formula><p>[H -1 Lf f ′ ] [P j ,P j ]</p><p>= O p ( √ pq).</p><p>Together with ∥H -1 Lf f ′ ∥ 1 = O p ( √ pq) we have H -1 Lf f ′ 1 = O p ( √ pq).</p><p>For (ii), similarly by (ii) of Assumption IV.8 and Assumption IV.9, we have the bound for the l 2 -and l 1 -norm of H Luu ′ given as</p><formula xml:id="formula_550">H -1 Luu ′ ≤ max i (nq) -1 q j=1 l ′′ ij (w ij )γ j γ T j -1 ≤ √ Knq b L q j=1 γ j γ T j -1 = O p (n),</formula><p>and</p><formula xml:id="formula_551">H -1 Luu ′ 1 ≤ √ K max i (nq) -1 q j=1 l ′′ ij γ j γ T j -1 ≤ √ Knq b L q j=1 γ j γ T j -1 = O p (n).</formula><p>We use Woodbury identity to get</p><formula xml:id="formula_552">H -1 uu ′ = H -1 Luu ′ -H -1 Luu ′ Λ 2 (I K 2 +Kp + Λ T 2 H -1 Luu ′ Λ 2 ) -1 Λ T 2 H -1</formula><p>Luu ′ . By (C.78), we know ∥Λ T 2 H -1 Luu ′ Λ 2 ∥ ≤ ∥H -1 Luu ′ ∥∥λ 2 ∥ 2 ≤ O(1). Therefore we have estimate (I K 2 +Kp + Λ T 2 H -1 Luu ′ Λ 2 ) -1 = O p (1), which implies</p><formula xml:id="formula_553">H -1 Luu ′ Λ 2 I K 2 +Kp + Λ T 2 H -1 Luu ′ Λ 2 -1 Λ T 2 H -1 Luu ′ ≤ H -1 Luu ′ Λ 2 -I K 2 +Kp + Λ T 2 H -1 Luu ′ Λ 2 -1 Λ T 2 H -1 Luu ′ = O p (n),</formula><p>and</p><formula xml:id="formula_554">H -1 Luu ′ Λ 2 I K 2 +Kp + Λ T 2 H -1 Luu ′ Λ 2 -1 Λ T 2 H -1 Luu ′ 1 ≤ max i H -1 Luu ′ Λ 2 I K 2 +Kp + Λ T 2 H -1 Luu ′ Λ 2 -1 [Λ 2 ] T [K i ,] [H -1 Luu ′ ] [K i ,K i ] 1 ≤ √ n max i H -1 Luu ′ Λ 2 I K 2 +Kp + Λ T 2 H -1 Luu ′ Λ 2 -1 [Λ 2 ] T [K i ,] [H -1 Luu ′ ] [K i ,K i ] = O p ( √ pn).</formula><p>For H Luu ′ , similarly with (i), by Assumption IV.8 we have</p><formula xml:id="formula_555">H -1 Luu ′ 1 ≤ √ K max i (nq) -1 ( q j=1 l ′′ ij γ j γ T j ) -1 ≤ √ Knq b L ( q j=1 γ j γ T j ) -1 = O p (n).</formula><p>Therefore the l 2 -norm of H -1 uu ′ can be bounded by</p><formula xml:id="formula_556">H -1 uu ′ ≤ H -1 Luu ′ + H -1 Luu ′ Λ 2 I K 2 + Λ T 2 H -1 Luu ′ Λ 2 -1 Λ T 2 H -1 Luu ′ = O p (n),</formula><p>and its l 1 -norm can be bounded by</p><formula xml:id="formula_557">H -1 uu ′ 1 ≤ H -1 Luu ′ 1 + H -1 Luu ′ Λ 2 I K 2 + Λ T 2 H -1 Luu ′ Λ 2 -1 Λ T 2 H -1 Luu ′ 1 = O p ( √ pn).</formula><p>Next we prove (iii) and (iv). For the off-diagonal blocks, note</p><formula xml:id="formula_558">[H f u ′ ] [P j ,K i ] = -(nq) -1 l ′′ ij γ j Z T i -(nq) -1 l ′ ij    I k 0 (K+p)×K    + [Λ 1 ] [P j ,] [Λ 2 ] [,K i ] ,</formula><p>For the first term, we have from Assumption IV.9 that (nq) 1/ξ )n -1 q -1 ), max j ∥[H f u ′ ] [P j ,] ∥ = O p ((nq) 1/ξ n -1/2 q -1 ), ∥[</p><formula xml:id="formula_559">H uf ′ ] [K i ,] ∥ 1 = O p ((nq) 1/ξ n -1 q -1 ), max i ∥[H uf ′ ] [K i ,] ∥ = O p ( √ p + (nq) 1/ξ )n -1 q -1/2 ) and ∥H uf ′ ∥ = ∥H f u ′ ∥ =</formula><p>O p (nq) -1/2+1/ξ . Finally (v) and (vi) are obtained by</p><formula xml:id="formula_560">max j [H -1 f f ′ H f u ′ ] [P j ,] ≤ max j [H -1 Lf f ′ H f u ′ ] [P j ,] + max j [H -1 Lf f ′ Λ 1 I K 2 + Λ T 1 H Lf f ′ Λ 1 -1 Λ T 1 H -1 Lf f ′ H f u ′ ] [P j ,] ≤ max j [H -1 Lf f ′ ] [P j ,P j ] [H f u ′ ] [P j ,] + max j [H -1 Lf f ′ ] [P j ,P j ] [Λ 1 ] [P j ,] I K 2 + Λ T 1 H Lf f ′ Λ 1 -1 Λ T 1 H -1 Lf f ′ H f u ′</formula><p>=O p (nq) 1/ξ n -1/2 , from (i), (iv) and (vi) and</p><formula xml:id="formula_561">max i [H -1 uu ′ H uf ′ ] [K i ,] ≤ max j [H -1 Luu ′ H uf ′ ] [K i ,] + max j [H -1 Luu ′ Λ 2 I K 2 +Kp + Λ T 2 H Luu ′ Λ 2 -1 Λ T 2 H -1 Luu ′ H uf ′ ] [K i ,] ≤ max j [H -1 Luu ′ ] [K i ,K i ] [H uf ′ ] [K i ,] + max j [H -1 Luu ′ ] [K i ,K i ] [Λ 2 ] [K i ,] I K 2 +Kp + Λ T 2 H Luu ′ Λ 2 -1 Λ T 2 H -1 Luu ′ H uf ′</formula><p>=O p ( √ p + (nq) 1/ξ )q -1/2 , from (ii), (iii), and (vii), respectively.</p><p>Lemma C.17. Recall that we have defined that M(ϕ) = [ 1 0 H ϕ 0 +s(ϕ-ϕ 0 ) ds] -1 . Use the notation in the proof of Proposition C.4, denote M = M( ϕ) and express it in the following block form:</p><formula xml:id="formula_562">M =    M f f ′ M f u ′ M uf ′ M uu ′    .</formula><p>Under Asssumptions IV.8-IV.11, there exist some ϵ &gt; 0 and m such that q M(ϕ)D 1/2 q ≥ min s λ min D 1/2 q H(ϕ 0 + s(ϕ -ϕ 0 ))D 1/2 q -1 .</p><p>Next we will focus on the second part. We first show the estimates are valid using the bound established in Lemma C.16. For now, we focus our discussion on ϕ 0 and its dependence will be omitted. Again use the notation in the proof of Proposition C.12</p><formula xml:id="formula_563">H(ϕ 0 ) -1 =    H f f ′ H f u ′ H uf ′ H uu ′   </formula><p>Also recall we have defined the Schur complement</p><formula xml:id="formula_564">H -f = (H f f ′ -H f u ′ H -1 uu ′ H uf ′ ) -1 and H -u = (H uu ′ -H uf ′ H -1 f f ′ H f u ′ ) -1 with ∥H -f ∥ = O p (q)</formula><p>and ∥H -u ∥ = O p (n). For the upper-left block H f f ′ , by Woodbury formula, we have</p><formula xml:id="formula_565">H f f ′ = H -1 f f ′ + H -1 f f ′ H f u ′ H -u H uf ′ H -1 f f ′ . By (i) of Lemma C.16 we know H -1 f f ′ ∞ = H -1 f f ′ 1 =</formula><p>O p (p 1/2 q) and by (i), (v) of Lemma C.16 we have</p><formula xml:id="formula_566">H -1 f f ′ H f u ′ H -u H uf ′ H -1 f f ′ ∞ ≤ max j [H -1 f f ′ H f u ] [P j ,] H -u H uf ′ H -1 f f ′ ∞ ≤(pq) 1/2 max j [H -1 f f ′ H f u ′ ] [P j ,] 2 ∥H -u ∥∥H uf ′ ∥∥H -1 f f ′ ∥ =O p √ p(nq) 2/ξ q .</formula><p>So we conclude that</p><formula xml:id="formula_567">H f f ′ ∞ ≤ O p √ pq(nq) 2/ξ .</formula><p>For the lower-right block H uu ′ , we have </p><formula xml:id="formula_568">H uu ′ = H -1 uu ′ + H -1 uu ′ H uf ′ H -f H f u ′ H -</formula><formula xml:id="formula_569">H -1 uu ′ H uf ′ H -f H f u ′ H -1 uu ′ ∞ = max i [H -1 uu ′ H uf ′ ] [K i ,] H -f H f u ′ H -1 uu ′ ∞ ≤ √ n max i [H -1 uu ′ H uf ′ ] [K i ,] H -f H f u ′ H -1 uu ′</formula><p>≤O p ( √ p + (nq) 1/ξ )(nq) 1/ξ n So we conclude that H uu ′ ∞ ≤ O p √ pn(nq) 2/ξ .</p><p>For the off-diagonal term H f u ′ , again by the Sherman-Morrison-Woodbury formula,</p><formula xml:id="formula_570">H f u ′ = H f f ′ H f u ′ H -1 uu ′ = H -1 f f ′ H f u ′ H -1 uu ′ + H -1 f f ′ H f u ′ H -u H uf ′ H -1 f f ′ H f u ′ H -1 uu ′ .</formula><p>By (i) (iii) and (v) of Lemma C.16 we have for the first term that</p><formula xml:id="formula_571">H -1 f f ′ H f u ′ H -1 uu ′ ∞ = max j [H -1 f f ′ H f u ′ H -1 uu ′ ] [P j ,] ≤ √ n max j [H -1 f f ′ H f u ′ ] [P j ,] H -1 uu ′ =O p √ p(nq) 1/ξ n ,</formula><p>and for the second term</p><formula xml:id="formula_572">H -1 f f ′ H f u ′ H -u H uf ′ H -1 f f ′ H f u ′ H -1 uu ′ ∞ ≤ max j [H -1 f f ′ H f u ′ ] [P j ,] H -u H uf ′ H -1 f f ′ H f u ′ H -1 uu ′ ∞ ≤ √ n max j [H -1 f f ′ H f u ′ ] [P j ,] H -u H uf ′ H -1 f f ′ H f u ′ H -1 uu ′ ≤ √ n max j [H -1 f f ′ H f u ′ ] [P j ,] H -u H uf ′ H -1 f f ′ H f u ′ H -1 uu ′ ≤ O p √ p(nq) 3/ξ n .</formula><p>Then we obtain H f u ′ = O p ( √ p(nq) 3/ξ √ n) For the off-diagonal term H uf ′ , we expand it similarly by the formula of Sherman-Morrison-Woodbury formula as</p><formula xml:id="formula_573">H uf ′ = H uu ′ H uf ′ H -1 f f ′ = H -1 uu ′ H uf ′ H -1 f f ′ + H -1 uu ′ H uf ′ H -f H f u ′ H -1 uu ′ H uf ′ H -1 f f ′ .</formula><p>By (ii) (vi) and (vi) of Lemma C.16 the first term can be estimated by</p><formula xml:id="formula_574">H -1 uu ′ H uf ′ H -1 f f ′ ∞ ≤ H -1 uu ′ H uf ′ H -1 f f ′ ∞ ≤ max i [H -1 uu ′ H uf ′ ] [K i ,] H -1 f f ′ ∞ ≤ √ pq max i [H -1 uu ′ H uf ′ ] [K i ,] H -1 f f ′ = O p ( √ p + (nq) 1/ξ )q ,</formula><p>and the second term can be similarly calculated as</p><formula xml:id="formula_575">H -1 uu ′ H uf ′ H -f H f u ′ H -1 uu ′ H uf ′ H -1 f f ′ ∞ ≤ max i [H -1 uu ′ H uf ′ ] [K i ,] H -f H f u ′ H -1 uu ′ H uf ′ H -1 f f ′ ∞ ≤ √ pq max i [H -1 uu ′ H uf ′ ] [K i ,] H -f H f u ′ H -1 uu ′ H uf ′ H -1 f f ′ ≤ √ pq max i [H -1 uu ′ H uf ′ ] [K i ,] H -f H f u ′ H -1 uu ′ H uf ′ H -1 f f ′ ≤ O p √ p( √ p + (nq) 1/ξ )(nq) 2/ξ q .</formula><p>Therefore we conclude that the infinity bound for H uf ′ is O p (p(nq) 3/ξ q).</p><p>Till here we have proved the estimates in infinity norm for the blocks of Hessian matrix on ϕ 0 . It suffices to prove that the estimates in Lemma C.16 are still valid when H(ϕ 0 ) is replaced with M( ϕ). Still, from now we omit the dependence on ϕ unless otherwise specified. Specifically, we merely need to verify the following hold:</p><p>(i') ∥M -1 Lf f ′ ∥ = O p (q), ∥M -1 f f ′ ∥ = O p (q), and ∥M -1 f f ′ ∥ 1 = O p ( √ pq).</p><p>(ii') ∥M -1 Luu ′ ∥ = O p (n), ∥M -1 uu ′ ∥ = O p (n), and ∥H -1 uu ′ ∥ 1 = O p ( √ pn).</p><p>(iii') max i [M uf ′ ] [K i ,] = O p (p 1/2 + (nq) 1/ξ )n -1 q -1/2 , ∥M uf ′ ∥ 1 = O p (nq) 1/ξ q -1 , and ∥M uf ′ ∥ = O p (nq) -1/2+1/ξ .</p><p>(iv') max j [M f u ′ ] [P j ,] = O p (nq) 1/ξ n -1 q -1/2 , ∥M f u ′ ∥ 1 = O p (p + (nq)) 1/ξ n -1 , and</p><formula xml:id="formula_576">∥M f u ′ ∥ = O p (nq) -1/2+1/ξ . (v') ∥[ Λ 1 ] [P j ,•] ∥ = O p (q -1 ) and ∥ Λ 1 ∥ = O p (q -1/2 ). (vi') ∥[ Λ 2 ] [K i ,•] ∥ = O p (p 1/2 n -1 ) and ∥ Λ 2 ∥ = O p (n -1/2 ).</formula><p>First, it can be verified that inside this region, the residuals terms in the Hessian of penalty functions, i.e., the last two terms in (C.13), can be incorporated into these </p><formula xml:id="formula_577">=    M f f ′ M f u ′ M uf ′ M uu ′   </formula><p>where</p><formula xml:id="formula_578">M f f ′ = 1 0 H Lf f ′ (s) -Λ 1 (s)Λ T 1 (s) ds; M f u ′ = 1 0 H Lf u ′ (s) + H Rf u ′ (s) -Λ 1 (s)Λ T 2 (s) ds; M uf ′ = 1 0 H Luf ′ (s) + H Ruf ′ (s) -Λ 2 (s)Λ T 1 (s) ds; M uu ′ = 1 0</formula><p>H Luu ′ (s) -Λ 2 (s)Λ T 2 (s) ds.</p><p>For the upper-left block M f f ′ , since Λ 1 (s) is linear with respect to s, we get</p><formula xml:id="formula_579">M f f ′ = M Lf f ′ -Λ 1 Λ T , where M Lf f ′ = 1 0 H Lf f ′<label>(</label></formula><p>s)ds and Λ 1 = Λ 1 (1/2), (Λ 1 (1) -Λ 1 (0))/(2 √ 3) . Then agian by Woodbury identity</p><formula xml:id="formula_580">M -1 f f ′ = M -1 Lf f ′ -M -1 Lf f ′ Λ 1 I 2K 2 + Λ T 1 M -1 Lf f ′ Λ 1 -1 Λ T 1 M -1</formula><p>Lf f ′ Similar to (vii) in Lemma C.16, we have</p><formula xml:id="formula_581">∥[ Λ 1 ] [P j ,•] ∥ = O p (q -1 ), ∥ Λ T 1 ∥ 1 = O p (q -1 ), ∥ Λ 1 ∥ 1 = O p (1), ∥ Λ 1 ∥ = O p (q -1/2 ).</formula><p>This implies (v'). Next for M Lf f ′ = </p><formula xml:id="formula_582">Z 0 i (Z 0 i ) T -(nq) -1 max j,s n i=1 l ′′ ij (w 0 ij )Z 0 i (Z 0 i ) T -l ′′ ij (w ij (s))Z i (s)Z i (s) T ≳ 1 q , (C.84)</formula><p>where c is selected to be small enough. Then we conclude that as ρ min (M Lf f ′ ) ≥ O p (q -1 ). Similar to Lemma C.16, we obtain M -1</p><p>Lf f ′ = O p (q) and M -1 f f ′ 1 = O p ( √ pq), which is (i'). For M uu ′ , we first compute by Λ 2 = Λ 2 (1/2), (Λ 2 (1) -Λ 2 (0))/(2 √ 3) followed by a similar argument in proving (vii) of Lemma C.16 that</p><formula xml:id="formula_583">∥[ Λ 2 ] [K i ,•] ∥ = O p ( √ pn -1 ), ∥ Λ T 2 ∥ 1 = O p (pn -1 ), ∥ Λ 2 ∥ 1 = O p (1), ∥ Λ 2 ∥ = O p (n -1/2 ).</formula><p>This implies (vi'). For M Luu ′ = 1 0 H Luu ′ (s)ds, it is block-diagonal and when ϕ ∈ B(D), √ p D -1/2 q ( ϕϕ 0 ) ≤ c, we have by a similar approach of computing (C.84) that ρ min (M Luu ′ ) ≥ O p (n -1 ). Similar to Lemma C.16, we obtain M -1 Luu ′ 1 = O p (n) and M -1 uu ′ 1 = O p (n), which implies (ii'). For the off-diagonal blocks, define</p><formula xml:id="formula_584">M -f = (M f f ′ -M f u ′ M -1 uu ′ M uf ′ ) -1 and M -u = (M uu ′ -M uf ′ M -1 f f ′ M f u ′ ) -1</formula><p>. By Proposition C.2 and ρ min (D 1/2 q MD 1/2 q ) ≥ min s ρ min (D 1/2 q H(s)D So we proved (i).</p><p>For (ii), note that H uf ′ can be written as the sum of three parts:</p><formula xml:id="formula_585">H Luf ′ + H Ruf ′ + Λ 2 Λ T 1 , [H uf ′ H -1 f f ′ S f ] [K i ] can be bounded by ∥[H uf ′ H -1 f f ′ S f ] [K i ] ∥ ≤ [H Luf ′ H -1 f f ′ S f ] [K i ,] + [H Ruf ′ H -1 f f ′ S f ] [K i ,] + [Λ 2 Λ T 1 H -1 f f ′ S f ] [K i ,] ,</formula><p>with</p><formula xml:id="formula_586">H -1 f f ′ = H -1 Lf f -H -1 Lf f Λ 1 (I K 2 + Λ T 1 H -1 Lf f ′ Λ 1 ) -1 Λ T 1 H -1 Lf f ′ .</formula><p>Then for the first term, by Assumptions IV.8-IV.9, we have</p><formula xml:id="formula_587">[H Luf ′ H -1 Lf f ′ S f ] [(i-1)K+r] = (nq) -1 q j=1 l ′′ ij (w 0 ij )γ 0 jr Z 0 i T n t=1 l ′′ tj (w 0 tj )Z 0 t Z 0 t T -1 n t=1 l ′ tj (w 0 tj )Z 0 t ≤ 1 nq q j=1 n t=1 l ′ tj (w 0 tj )b 2 U λ min n s=1 Z 0 s (Z 0 s ) T -1 ∥Z t ∥∥Z 0 i ∥|γ jr |</formula><p>= O p pn -3/2 q -1/2 ϵ nq , (C.85)</p><p>and by (i), (iii), (vii) of Lemma C.16 and the previous bound for</p><formula xml:id="formula_588">∥Λ T 1 H -1 Lf f ′ S f ∥, [H Luf ′ H -1 Lf f Λ 1 (I K 2 + Λ T 1 H -1 Lf f ′ Λ 1 ) -1 Λ T 1 H -1 Lf f ′ S f ] [K i ,] ≤ [H Luf ′ ] [K i ,] H -1 Lf f Λ 1 (I K 2 + Λ T 1 H -1 Lf f ′ Λ 1 ) -1 Λ T 1 H -1 Lf f ′ S f</formula><p>= O p p(nq) 1/ξ n -3/2 q -1/2 ϵ nq . q -1 γ jr γ jl D -1 q   </p><formula xml:id="formula_589">I q ⊗ (E (K+p) rl + E (K+p) lr ) 0 nK×nK    D -1 q =cD -1 q ∂ γ jr ν r ν T r + ∂ γ jr l&lt;r u rl u T rl + r&lt;l u lr u T lr D -1 q c q γ jr D -1 q    I q ⊗ E (K+p) rr I n ⊗ E (K) rr    cq -1 D -1 q    l̸ =r γ jl I q ⊗ (E (K+p) rl + E (K+p) lr ) 0 nK×nK    D -1 q .</formula><p>Then δϕ T ∂ γ jr H P (ϕ ♭ )δϕ can be bounded as For R U , Then derivative with respect to U i of δϕ v H(ϕ ♭ )δϕ v is given as And for ∂ U ir H P , similar with ∂ γ jr H P , we compute</p><formula xml:id="formula_590">δϕ v T ∂ U ir H(ϕ ♭ )δϕ v = -(nq) -1 q j=1 γ ♭ jr l ′′′ ij (w ♭ ij ) δf T j Z ♭ i 2 + 2l ′′ ij (w ♭ ij )δγ jr δf T j Z ♭ i (C.91a) + q j=1 γ ♭ jr l ′′′ ij (w ♭ ij ) δU T i γ ♭ j δf T j Z ♭ i + l ′′ ij (w ♭ ij ) δU T i γ ♭ j δγ jr (C.91b) + q j=1 γ ♭ jr l ′′′ ij (w ♭ ij ) δU T i γ ♭</formula><formula xml:id="formula_591">∂ U ir H P = -D -1 q c K r=1 ∂ U ir ν r ν T r + c h&lt;l ∂ U ir u hl u T hl D -1 q - c 2 n -1 n i=1 ∂ U ir (U 2 ir ))D -1 q    I q ⊗ E (K+p) rr I n ⊗ E (K) rr    -c l̸ =r ∂ U ir n i=1 n -1 U ir U il D -1 q    I q ⊗ (E (K+p) rl + E (K+p) lr ) 0 nK×nK    D -1 q = -cD -1 q ∂ U ir ν r ν T r + ∂ U ir r&lt;l u rl u T rl + l&lt;r u lr u T lr D -1 q - c q U ir D -1 q    I q ⊗ E (K+p) rr I n ⊗ E (K) rr    -cq -1 D -1 q    0 q(K+p)×q(K+p) l̸ =r U il I n ⊗ (E (K) rl + E (K) lr )    D -1 q .</formula><p>Then similarly δϕ </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure</head><label></label><figDesc>Figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>of the proposed model in (4.1). The subscript i indicates the ith subject, out of n independent subjects. The response variable Y ij can be discrete or continuous. . . . . . . . . . . . . . . 4.2 The direct effects (orange solid line in the left panel) and the indirect effects (blue solid line in the right panel) for item 1. . . . . . . . . 4.3 Powers and type I errors under sparse setting at p * = 5. Red Circles ( ) denote correlation parameter τ = 0. Green triangles ( ) represent the case τ = 0.2. Blue squares ( ) indicate τ = 0.5. Purple crosses ( ) represent the τ = 0.7. . . . . . . . . . . . . . . . . . . . 4.4 Powers and type I errors under sparse setting at p * = 30. Red Circles ( ) denote correlation parameter τ = 0. Green triangles ( ) represent the case τ = 0.2. Blue squares ( ) indicate τ = 0.5. Purple crosses ( ) represent the τ = 0.7. . . . . . . . . . . . . . . . . . . . 4.5 Powers and type I errors under dense setting at p * = 5. Red Circles ( ) denote correlation parameter τ = 0. Green triangles ( ) represent the case τ = 0.2. Blue squares ( ) indicate τ = 0.5. Purple crosses ( ) represent the τ = 0.7. . . . . . . . . . . . . . . . . . . . 4.6 Powers and type I errors under dense setting at p * = 30. Red Circles ( ) denote correlation parameter τ = 0. Green triangles ( ) represent the case τ = 0.2. Blue squares ( ) indicate τ = 0.5. Purple crosses ( ) represent the τ = 0.7. . . . . . . . . . . . . . . . . . . . 4.7 Confidence intervals for the effect of gender covariate on each PISA question using Taipei data. Red intervals correspond to confidence intervals for questions with significant gender bias after Bonferroni correction. (For illustration purposes, we omit the confidence intervals with the upper bounds exceeding 6 and the lower bounds below -6 in this figure). . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.8 Confidence intervals for the effect of school stratum covariate on each PISA question. Red intervals correspond to confidence intervals for questions with significant school stratum bias after Bonferroni correction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix B.1 Coverage and length of the confidence interval under logistic regression with prespecified K = 2, 4, 5 and 10, averaged over 300 replications, with varying p and fixed n = 500 (top), and with varying n and fixed p = 600 (bottom). Black dashlines indicate the 0.95 level. Orange dashed lines ( ) represent the results at K = 2. Green dotted lines ( ) represent the results at K = 4. Blue two-dashed lines ( ) represent the results at K = 5. Purple solid lines ( ) represent the results at K = 10. . . . . . . . . . . . . . . . . . . . . B.2 Coverage and length of the confidence interval under linear model with prespecified K = 2, 4, 5 and 10, averaged over 300 replications, with varying p and fixed n = 500 (top), and with varying n and fixed p = 600 (bottom). Black dashlines indicate the 0.95 level. Orange CHAPTER I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>; Reboussin et al. (2008); Bakk et al. (2013); Park et al. (2018). The related estimation problems have also received great interests from researchers in psychometrics field, such as estimating the covariate coefficients Petersen et al. (2012), adjusting for the bias in the estimation Bakk et al. (2013), and estimating the number of latent classes Huang (2005); Pan and Huang (2014).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>are only necessary but not sufficient for the local identifiability of latent class models. Our argument borrows ideas from recent developments in the identifiability of the Cognitive Diagnosis Models (CDMs), a special family of restricted latent class models. Besides, the results in Huang and Bandeen-Roche (2004) only concern the local identifiability but not the global identifiability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, in the conditional probability model (2.1), the regression parameters (β) are latent class specific. In the conditional probability model (2.2), we allow the intercept parameters (γ) dependent on the latent class, response level and item indices, while the regression coefficients parameters (λ) dependent on response level and item indices but not the latent class membership, which, as pointed in Huang and Bandeen-Roche (2004), is a logical assumption to prevent possible misclassification by adjusting for the associated covariates. The following two assumptions proposed by Huang and Bandeen-Roche (2004) hold for all RegLCMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>binary latent attributes respectively and A denotes the attribute pattern space. The vector α represents a unique latent profile with the kth entry α k = 1 implying the mastery of the subject on the kth latent attribute and α k = 0 implying his deficiency of it. The number of latent classes is C = |A| = 2 K . For notational convenience, we follow the idea in Culpepper (2019) by introducing a tool vector</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(2017),<ref type="bibr" target="#b153">Xu and Shang (2018)</ref>,<ref type="bibr" target="#b61">Gu and Xu (2019)</ref>,<ref type="bibr" target="#b62">Gu and Xu (2020)</ref>, etc. For restricted latent class models with polytomous responses, Culpepper (2019), Fang et al. (2019), Chen et al. (2020a), and Gu and Xu (2020) proposed the identifiability conditions based on the Q-matrix for polytomous restricted latent models. The above research focus on the identifiability of the general or restricted latent class models with no covariate. As for the identifiability of latent class models containing covariates, Huang and Bandeen-Roche (2004) generalized the result of Goodman (1974) and derived local identifiability conditions for RegLCMs. Under the setting of RegLCMs, denote S ′ as the response pattern space S with a reference pattern removed (e.g., 0), and the number of distinct response patterns in S ′ is then S -1. Define Φ = (ϕ c ; c = 0, . . . , C -1) (S-1)×C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>consider a TIMSS 2007 4th Grade dataset, which was previously studied in Park and Lee (2014) and is accessible from the R package "CDM" George et al. (2016); Robitzsch et al. (2020). The dataset contains N = 698 Austrian 4th grade students'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Example III. 1 (</head><label>1</label><figDesc>Logistic Regression). Let y ∈ {0, 1} be a binary variable. Given covariates D, Q, and unmeasured confounders U , the response y follows the logistic regression model with ϕ = 1, a(ϕ) = 1, b(t) = log{1 + exp(t)}, and c(y, ϕ) = 0. Example III.2 (Poisson Regression). Let y ∈ {0, 1, 2, . . .} be a discrete variable. Given covariates D, Q, and unmeasured confounders U , the response y follows the Poisson regression model with ϕ = 1, a(ϕ) = 1, b(t) = exp(t), and c(y, ϕ) = -log(y!).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>is a challenging and intriguing problem. There are various related models in the literature. For instance, to address the difficulty of model selection when p ≫ n, Paul et al. (2008) assumed that y and X are connected via a low-dimensional latent variable model: y = β T U +ε and X = W T U +E, where the latent factors are associated with response and covariates are only used to infer the latent factors. However, the response and covariates are not directly associated. Different from the latent variable model in Paul et al. (2008), Bai and Ng (2006) considered an additional low-dimensional covariate G to the latent variable model, that is expressed as y = β T U +ϱ T G+ε and X = W T U +E. Besides, there are other factor-adjusted models that can be extended to adjust hidden confounding. For example, Fan et al. (2020) studied the highdimensional model selection problem when covariates are highly correlated. As most commonly used model selection methods may fail with highly-correlated covariates, they used a factor model to reduce the dependency among covariates and proposed a factor-adjusted regularized model section method. Fan et al. (2020) considered a generalized linear model between response and covariate, which together with the factor model forms a similar model framework as ours. However, the problem they studied is fundamentally different than our problem. They did not assume hidden confounding and the factor model is only used to identify a low rank part of highlycorrelated covariates whereas in our problem, we focus on the regression problem with unmeasured confounders associated with response and covariates. Similar factoradjusted methods have also been studied in other settings (e.g., Gagnon-Bartsch and Speed , 2011; Wang and Blei, 2019); however, as noted in Ćevid et al. (2020), related theoretical justifications are still underdeveloped in the literature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>from the approach of<ref type="bibr" target="#b65">Guo et al. (2022)</ref> that implicitly adjusts for the hidden confounding effects,<ref type="bibr" target="#b51">Fan et al. (2023)</ref> proposed to first use the principal component method to estimate unmeasured confounders and then construct the bias-corrected estimatorfor ∥(θ, v T )∥ ∞ ,which involves the decomposition of the estimation error relying on the linear form of the response and uses the projection of the response onto the factor space. The motivation of Fan et al. (2023) originated from Fan et al. (2020). When covariates are highly correlated, the leading factors are likely to have extra impacts on the response. So they augmented the factor into the sparse linear regression model between response and covariates, which is written as (3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>ζ) as the generalized decorrelated score function, where ∇ θ l(θ, ζ) and ∇ ζ l(θ, ζ) are the partial derivatives of the loss function with respect to θ and ζ, respectively. Different from the existing definition of the decorrelated score function in Ning and Liu (2017), the generalized decorrelated score function takes into account the effects induced by the unmeasured confounders. Specifically, in the presence of unmeasured confounders, the generalized decorrelated score function is uncorrelated with the score function corresponding to the nuisance covariates as well as the unmeasured confounders, i.e., E{S(θ, ζ) T ∇ ζ l(θ, ζ)} = I θζ -I θζ I -1ζζ I ζζ = 0. The debiased estimator of θ is constructed by solving for θ from the first-order approximation of the generalized decorrelated score function S( θ, ζ) + I θ|ζ ( θ -θ) = 0. From the first-order approximation equation, we see that to establish the debiased estimator θ, we need to construct two estimators S( θ, ζ) and I θ|ζ , and the key is to estimate w.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 3 Figure 3</head><label>33</label><figDesc>Figure 3.1: Coverage and length of the confidence interval under linear regression, averaged over 300 replications, with varying p and fixed n = 500 (top), and with varying n and fixed p = 600 (bottom). Black dashlines indicate the 0.95 level. Purple solid lines ( ) are our proposed method. Blue two-dashed lines ( ) represent the oracle case. Green dotted lines ( ) indicate the naive method. Orange dashed lines ( ) represent the doubly debiased lasso method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>lengths. Under the new loading matrix, the results for linear regression model are shown in Fig. 3.3 and the results for logistic regression model are shown in Fig. 3.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 3 Figure 3</head><label>33</label><figDesc>Figure 3.3: Coverage and length of the confidence interval under linear regression with new loading matrix, averaged over 300 replications, with varying p and fixed n = 500 (top), and with varying n and fixed p = 600 (bottom). Black dashlines indicate the 0.95 level. Purple solid lines ( ) are our proposed method. Blue two-dashed lines ( ) represent the oracle case. Green dotted lines ( ) indicate the naive method. Orange dashed lines( ) represent the doubly debiased lasso method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Figure 3.5: Confidence interval estimation using proposed method under logistic regression for the stimulation groups of PAM (top), PIC (middle) and LPS (bottom). Purple intervals indicate confidence intervals that do not cover zero. Red intervals represent confidence intervals that are among purple intervals and significant after Bonferroni Correction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>For any symmetric matrix M, let λ min (M) and λ max (M) be the smallest and largest eigenvalues of M, respectively. For any matrix A = (a ij ) n×l , let ∥A∥ ∞,1 = max j=1,...,l n i=1 |a ij | be the maximum absolute column sum, ∥A∥ 1,∞ = max i=1,...,n l j=1 |a ij | be the maximum of the absolute row sum, ∥A∥ max = max i,j |a ij | be the maximum of the absolute matrix entry, ∥A∥ F = ( n i=1 l j=1 |a ij | 2 ) 1/2 be the Frobenius norm of A, and ∥A∥ = λ max (A T A) be the spectral norm of A. Let ∥ • ∥ φ 1 be sub-exponential norm. Define the notation A v = vec(A) ∈ R nl to indicate the vectorized matrix A ∈ R n×l . Finally, we denote ⊗ as the Kronecker product.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 4</head><label>4</label><figDesc>Figure 4.1: A schematic diagram of the proposed model in (4.1). The subscript i indicates the ith subject, out of n independent subjects. The response variable Y ij can be discrete or continuous.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>The model parameters are identifiable if and only if for any response Y, there does not exist (Γ, U, B) ̸ = ( Γ, U, B) such that P (Y | Γ, U, B, X) = P (Y | Γ, U, B, X). The first issue concerning the identifiability of B and U is that for any (Γ, U, B) and any transformation matrix A, there exist Γ = Γ, U = U + XA T , and B = B -ΓA such that P (Y | Γ, U, B, X) = P (Y | Γ, U, B, X). This identifiability issue leads to the indeterminacy of the covariate effects and latent factors. The second issue is related to the identifiability of U and Γ. For any ( Γ, U, B) and any invertible matrix G, there exist Γ = Γ(G T ) -1 , Ū = UG, and B = B such that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 4 . 2 :</head><label>42</label><figDesc>Figure 4.2: The direct effects (orange solid line in the left panel) and the indirect effects (blue solid line in the right panel) for item 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>and {j : β * js &lt; 0} in a balanced way. With diversified signs of β * js , Proposition IV.5 holds when a considerable proportion of test items have no covariate effect (β * js ̸ = 0). For example, when γ * j = m1 (k) K with m &gt; 0, Condition IV.4(ii) holds if and only if q j=1 |m|{-I(β * js /m &gt; 0) + I(β * js /m ≤ 0)} &gt; 0 and q j=1 |m|{-I(β * js /m ≥ 0) + I(β * js /m &lt; 0)} &lt; 0. With slightly more than q/2 items correspond to β * js = 0, Condition IV.4(ii) holds. Moreover, if #{j : β * js &gt; 0}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>that the transformed matrices U * = U + X(A * ) T and B * = B -ΓA * satisfy Condition IV.4. The transformation idea naturally leads to the following estimation methodology for B * . To estimate B * and U * that satisfy Condition IV.4, we first obtain the maximum likelihood estimator ϕ = ( Γ, U, B) byϕ = argmin ϕ∈B(D) -L(Y | ϕ, X),(4.4)where the parameter space B(D) is given as B(D) = {ϕ : ∥ϕ∥ max ≤ D} for some large constant D. To solve (4.4), we employ an alternating minimization algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>( 4 .</head><label>4</label><figDesc>10)-(4.12) hold when Σ * β,j , Σ * γ,j , and Σ * u,i are substituted with their estimators Σ * β,j in (C.5), Σ * γ,j in (C.6), and Σ * u,i in (C.7) of the Supplementary Material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 4 Figure 4 . 4 :Figure 4 . 5 :Figure 4 . 6 :</head><label>4444546</label><figDesc>Figure 4.3: Powers and type I errors under sparse setting at p * = 5. Red Circles ( ) denote correlation parameter τ = 0. Green triangles ( ) represent the case τ = 0.2. Blue squares ( ) indicate τ = 0.5. Purple crosses ( ) represent the τ = 0.7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 4 . 7 :</head><label>47</label><figDesc>Figure 4.7: Confidence intervals for the effect of gender covariate on each PISA question using Taipei data. Red intervals correspond to confidence intervals for questions with significant gender bias after Bonferroni correction. (For illustration purposes, we omit the confidence intervals with the upper bounds exceeding 6 and the lower bounds below -6 in this figure).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 4 . 8 :</head><label>48</label><figDesc>Figure 4.8: Confidence intervals for the effect of school stratum covariate on each PISA question. Red intervals correspond to confidence intervals for questions with significant school stratum bias after Bonferroni correction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>where Ψ ref is the row corresponding to the reference pattern. And Ψ ref is linearly dependent on the rows in Ψ because r∈S P (R = r | L = c) = 1. So Ψ has full column rank if and only if Ψ ′ has full column rank. Further, Ψ ′ has full column rank if and only if T -matrix has full column rank, because Ψ ′ is bijectively corresponding to T -matrix according to their definitions. In conclusion, Ψ in the CDMs has full column rank when Q-matrix contains an identity submatrix I K . According to the Proof of Proposition 2 in Huang and Bandeen-Roche (2004), the matrix Φ has full column rank when the matrix Ψ has full column rank. So for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure</head><label></label><figDesc>Fig. B.2.From the results, we find that the overestimation of K appears not to affect the asymptotic normality results but the underestimation of K can influence the asymptotic distribution of debiased estimation and further affects confidence interval estimation. Intuitively, as long as the corresponding linear combinations of the true underlying factors U in the considered models can be well approximated by those of the estimated U , the developed inference results for θ * would still hold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head></head><label></label><figDesc>Lemma B.1 shows that there exist sub-exponential type of bounds on the gradient and linear combination of the Hessian of the loss functions. This is motivated by Assumption 3.2 in<ref type="bibr" target="#b109">Ning and Liu (2017)</ref>. They construct the loss function based on the observed covariates whereas our results are established upon the fact that the gradient and the hessian of the loss function involve not only observed covariates but also the estimated unmeasured confounders.Remark B.2. As the decomposition techniques commonly used in linear models may not be applicable in generalized linear model settings, it is necessary to establish stronger and more general intermediate results as the foundation of our theoretical analysis. For instance, different from linear models, where average estimation consistency on unmeasured confounders suffices, stronger uniform estimation consistency is necessary for the generalized linear framework. Specifically, in<ref type="bibr" target="#b51">Fan et al. (2023)</ref>, the linear model form and projection-based techniques enable the reduction of the gradientmax-norm into ∥ E T ( y -Eγ * )∥ ∞ , where γ * = (θ * , (v * ) T ) T and y = (I n -n -1 U U T )yis the residual of the response y after projecting in onto space of U . To show the concentration of the gradient, a key step is to upper bound∥(U * ) T Eϕ∥ ∞ with ∥ϕ∥ 2 = 1 by ∥( E -E) T (U H T -U )H -T ϕ∥ ∞ and ∥E T (U H T -U )H -T ϕ∥ ∞ ,where they apply Cauchy-Schwartz inequality and use the Frobenius norm of estimated unmeasured confounders ∥(U H T -U )∥ F . Here H K×K is some transformation matrix and with a slight abuse of notation, we use E ∈ R n×p , U ∈ R n×K and y ∈ R n to denote the matrix (vector) form of random error, unmeasured confounders, and responses. However in the generalized linear model, to derive the bound for gradient max-norm to be</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head></head><label></label><figDesc>45) where the inequality (B.44) is based on Assumption III.5(3) that |b ′′ (t 1 ) -b ′′ (t)| ≤ B|t 1 -t|b ′′ (t) with t 1 = η T Żi and t = (η * ) T Żi . The last inequality (B.45) is from the estimation error bound in the proof of Theorem III.7 that ∥ ηη * ∥ 1 ≲</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head></head><label></label><figDesc>10) are shown to hold in Appendix D.2. Hence the proof for Lemma B.3 is complete. B.7.3 Proof of Lemma B.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head></head><label></label><figDesc>53) where the inequality (B.53) is from the results (B.8) in Lemma B.3 and from similar arguments as in bounding L 11 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head></head><label></label><figDesc>1) and (B.52), (B.53), (B.54), we get L 2 = L 21 + L 22 + L 23 = o p (1) (B.55) Combining (B.51) and (B.55), we have I θ|ζ -I * θ|ζ = o p (1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head></head><label></label><figDesc>figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head></head><label></label><figDesc>show ϕ lies in the interior of B(D). Again by Proposition C.13 and Proof of Proposition C.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head></head><label></label><figDesc>C.54) Finally combine (C.53), (C.54) and by Lemma C.20 we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head></head><label></label><figDesc>′ ij (w ij )U 0 ir }] ≤ exp{2nc 2 M 2 ν 2 -nqtν}. (C.64)Next we will minimize (C.64) by minimizing the quadratic expression of ν in the exponent. Differentiating the quadratic term with respect to ν and set it to 0 gives the optimizerν * = qt 4c 2 M 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head></head><label></label><figDesc>14. Under Assumptions IV.8-IV.9, there exist some γ &gt; 0 such that The Hessian matrix will be given by Ȟ(ψ) = ȞL (ψ) + ȞP (ψ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head></head><label></label><figDesc>js ) T , (C.69)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>.</head><label></label><figDesc>It suffices to prove that there exist some γ &gt; 0 that does not depend on n, q,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head></head><label></label><figDesc>, by noting that { ν(2) js } j is a set of orthogonal indicators, we know ϵ 0 small enough, (C.72) and (C.73) will contradict (C.70). This contradiction implies (C.71).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head></head><label></label><figDesc>Lf u ′ ] [P j ,] 2 = O p (n -1/2 q -1 ), which also implies ∥H Lf u′ ∥ = O p ((nq) -1/2 ). For the second term, by Assumption IV.9we have max i |l ′ ij (w ij )| = O p (n 1/ξ ), max j |l ′ ij (w ij )| = O p (q 1/ξ ), max i,j |l ′ ij (w ij )| = O p ((nq) 1/ξ). Together with (C.77) and (C.78), we have ∥[H f u ′ ] [P j ,] ∥ 1 = O p ((p +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head></head><label></label><figDesc>of l ∞ -norm of M can be estimated as follows: The first part (C.3) is proved by Proposition C.2 and λ min D 1/2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head>1 0 H</head><label>0</label><figDesc>Lf f ′ (s)ds, it is block-diagonal and when ϕ ∈ B(D), √ p D -1/2 q ( ϕϕ 0 ) ≤ c for some small enough c, as ϕ lies in the line segment between ϕ and ϕ 0 . Under Assumptions IV.8-IV.9, with p = o( δ nq ),ρ min (M Lf f ′ ) = min j ρ min [M Lf f ′ ] [P j ,P j ] ≥ min j,0≤s≤1 ρ min [H Lf f ′ ] [P j ,P j ]≥b L ρ min ( nq) -1 n i=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head></head><label></label><figDesc>have ∥M -f ∥ = O p (q) and ∥M -u ∥ = O p (n). By similar argument in Lemma C.16, we know ∥M f u ′ ∥ = ∥M uf ′ ∥ = O p ((nq) -1/2 ). For the off-diagonal blocks, since inside B(D) ∩ √ p D -1/2 q ( ϕϕ 0 ) ≤ m, max i,j l ′ ij (w ij (s)) -l ′ ij (w 0 ij) is bounded by Assumption IV.9. The bounds forH Lf u ′ and [λ 1 ] [P j ,] [Λ [,K i ]can be derived similarly as in Lemma C.16. Therefore we get (iii') and similarly, we can derive (iv').</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head>.</head><label></label><figDesc>85) and (C.86) together imply ∥[H Luf′ H -1 f f ′ S f ] [K i ] ∥ = O p p(nq) 1/ξ n -3/2 q -1/2 ϵ nq . for r = s + K with s ∈ [p]. We first give an estimate for (C.89a) and (C.90a). Note that for any r ∈ [K + p], j ∈ [q], similar to estimating (C.84), ϕ 0 ) ≤ m}. Therefore by the results from Proposition C.1 and Proposition C.5, we have(C.89a) =O p q -1 ∥δf j ∥ 2 = O p 1 qζ 2 nq (C.89b) =O p (nq) -1 √ p∥δf j ∥ 90a) =O p q -1 ∥δf j ∥ 2 = O p 1 qζ 2 nq (C.90b) =O p (nq) -1 √ p∥δf j ∥ Combining these estimates gives ∥δϕ T ∂ f j H L (ϕ ♭ )δϕ∥ = O p pq -1 ζ -2nq,p . Next we com-pute ∂ γ jr H P from (C.13):∂ γ jr H P =D -1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head></head><label></label><figDesc>δϕ T ∂ γ jr H P (ϕ ♭ )δϕ jl , which implies that δϕ T ∂ γ jr H P (ϕ ♭ )δϕ = O p q -1 ζ -2 nq using (C.29)-(C.32) and ϕ ♭ ∈ B(D) ∩ {ϕ : √ p∥D -1/2 q (ϕ -ϕ 0 ) ≤ m}. Therefore with δϕ T ∂ β js H P (ϕ ♭ )δϕ = 0, we know [R f ] [P j ] = O p pq -1 ζ -2nq,p and consequently R f = O p pq -1/2 ζ -2 nq,p .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><head></head><label></label><figDesc>the estimates (C.89a)-(C.89c), we have(C.91a) =O p p(nq) 91b) =O p (nq) -1 ( √ p(nq) -1 ∥δU i ∥ q j=1 ∥δf j ∥ + (nq) -1 ∥δU i ∥ 91c) =O p n -1 ∥δU i ∥ 2 = O p p nζ 2 nq .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE OF CONTENTS</head><label>OF</label><figDesc>ACKNOWLEDGEMENTS . . . . . . . . . . . . . . . . . . . . . . . . . . ii LIST OF FIGURES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii</figDesc><table><row><cell>LIST OF FIGURES</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>2.1 and Section 2.2.2, we give a review of the existing studies. The identifiability conditions for general latent class models have been extensively investigated in the existing literature. In particular,<ref type="bibr" target="#b102">McHugh (1956)</ref> studied the latent class models with binary responses and proposed the sufficient local identifiability condi-</figDesc><table /><note><p><p><p>tions. Extending McHugh's work,</p><ref type="bibr" target="#b60">Goodman (1974)</ref> </p>presented a fundamental method to determine the local identifiability of the general latent class models with polytomous responses, stating that if the Jacobian matrix that contains the derivatives of response probability vector with respect to the parameters has full column rank, then the parameters are locally identifiable. This condition is intuitively straightforward but empirically nontrivial to apply. When the number of latent class C or the number of possible responses to items M j increases, the dimension of the Jacobian matrix would increase at a faster rate. Moreover, this method could only guarantee local identifiability for the general latent class models but leave the global identifiability undiscussed.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Theorem II.7 is sufficient to guarantee the strict identifiability for RegLCMs, including RegCDMs. Compared with the local identifiability conditions in Huang and</figDesc><table><row><cell>Bandeen-Roche (2004), Theorem II.7 keeps (A1)-(A3) and replaces the condition</cell></row><row><cell>(A4) regarding the column rank of Φ with a stronger condition (C4) regarding the</cell></row><row><cell>Kruskal ranks of the decomposed matrices from Φ. This condition is based on the</cell></row><row><cell>algebraic result in Kruskal (1977). We next present identifiability conditions tailored</cell></row><row><cell>to RegCDMs.</cell></row><row><cell>Theorem II.7 (Strict Identifiability for RegLCMs). Continue with the notation defi-</cell></row><row><cell>nitions in Section 2.2.3. For RegLCMs with polytomous responses, assume (A1)-(A3)</cell></row><row><cell>are true, (β, γ, λ) are strictly identifiable if the following condition holds.</cell></row></table><note><p><p><p>{1, • • • , κ t } with cardinality κ t = j∈Jt M j to be the number of response patterns for this set. And each variable T t is used to construct a κ t ×C submatrix Φ t , where its row indices arise from the response patterns corresponding to T t . The linear independence condition is then regarding to the Kruskal ranks of Φ t rather than normal column rank of Φ, where for any matrix Φ t , its Kruskal rank I t is the smallest number of columns of Φ t that are linearly dependent.</p>(C4) The matrix Φ can be decomposed into Φ 1 , Φ 2 and Φ 3 with Kruskal ranks I 1 , I 2 and I 3 respectively. And I 1 + I 2 + I 3 ≥ 2C + 2.</p>Proposition II.8 (Strict Identifiability for RegCDMs). For RegCDMs with polytomous responses, if we replace the condition (C4) in Theorem II.7 with the following condition, then (β, γ, λ) are strictly identifiable.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc><ref type="bibr" target="#b146">Wang, 2022;</ref> Chen et al., 2023b). However, they either focus only on the overall consistency properties of the estimation or do not incorporate covariates into the models.The rest of the paper is organized as follows. In Section 4.2, we introduce the model setup of the covariate-adjusted generalized factor model. Section 4.3 discusses the associated identifiability issues and further presents the proposed identifiability conditions and estimation method. Section 4.4 establishes the theoretical properties for not only the covariate effects but also the latent factors and factor loadings. In Section 4.5, we perform extensive numerical studies to illustrate the performance of the proposed estimation method and the validity of the theoretical results. In Section 4.6, we analyze an educational testing dataset from Programme for International</figDesc><table><row><cell>In a concurrent work, motivated by applications in single-cell omics, Du et al. (2023)</cell></row><row><cell>considered a generalized linear factor model with covariates and studied its inference</cell></row><row><cell>theory, where the latent factors are used as surrogate variables to control for un-</cell></row><row><cell>measured confounding. However, they imposed relatively stringent assumptions on</cell></row><row><cell>the sparsity of covariate effects and the dimension of covariates, and their theoretical</cell></row><row><cell>and further incorporate these conditions into the development of a computationally</cell></row><row><cell>efficient likelihood-based estimation method. Under these identifiability conditions,</cell></row><row><cell>we develop new techniques to address the aforementioned theoretical challenges and</cell></row><row><cell>obtain estimation consistency and asymptotic normality for covariate effects under</cell></row><row><cell>a practical yet challenging asymptotic regime. Furthermore, building upon these</cell></row><row><cell>results, we establish estimation consistency and provide valid inference results for</cell></row></table><note><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p>, regularized estimation methods</p><ref type="bibr" target="#b13">(Bauer et al., 2020;</ref><ref type="bibr" target="#b18">Belzak and Bauer , 2020;</ref><ref type="bibr" target="#b145">Wang et al., 2023)</ref></p>, item pair functioning methods</p><ref type="bibr" target="#b14">(Bechger and Maris, 2015)</ref></p>, and many other non-anchor-based methods have been proposed. However, these non-anchor-based methods do not provide valid statistical inference guarantees for testing the covariate effects. It remains an open problem to perform statistical inference on the covariate effects and the latent factors in educational assessments.</p>To address this open problem, we study the statistical estimation and inference for a general family of covariate-adjusted nonlinear factor models, which include the popular factor models for binary, count, continuous, and mixed-type data that commonly occur in educational assessments. The nonlinear model setting poses great challenges for estimation and statistical inference. Despite recent progress in the factor analysis literature, most existing studies focus on estimation and inference under linear factor models</p><ref type="bibr" target="#b133">(Stock and Watson, 2002;</ref><ref type="bibr" target="#b7">Bai and Li , 2012;</ref><ref type="bibr" target="#b48">Fan et al., 2013)</ref> </p>and covariateadjusted linear factor models</p><ref type="bibr" target="#b92">(Leek and Storey, 2008;</ref> Wang et al., 2017;<ref type="bibr" target="#b59">Gerard and Stephens, 2020;</ref><ref type="bibr" target="#b19">Bing et al., 2023)</ref></p>. The techniques employed in linear factor model settings are not applicable here due to the nonlinearity inherent in the general models under consideration. Recently, several researchers have also investigated the param-eter estimation and inference for generalized linear factor models</p><ref type="bibr" target="#b32">(Chen et al., 2019;</ref>    </p>results also rely on data-splitting. Moreover,</p><ref type="bibr" target="#b47">Du et al. (2023)</ref> </p>focused primarily on statistical inference on the covariate effects, while that on factors and loadings was unexplored, which is often of great interest in educational assessments. Establishing inference results for covariate effects and latent factors simultaneously under nonlinear models remains an open and challenging problem, due to the identifiability issue from the incorporation of covariates and the nonlinearity issue in the considered general models.</p>To overcome these issues, we develop a novel framework for performing statistical inference on all model parameters and latent factors under a general family of covariate-adjusted generalized factor models. Specifically, we propose a set of interpretable and practical identifiability conditions for identifying the model parameters, factor loadings and latent factors that are often of scientific interest, advancing our theoretical understanding of nonlinear latent factor models.</p>Student Assessment (PISA) and identify test items that may lead to potential bias among different test-takers. We conclude by providing some potential future directions in Section 4.7. The proofs for the theoretical results presented in the paper, along with additional simulation results, are included in a separate Supplementary Material.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc><ref type="bibr" target="#b7">Bai and Li, 2012;</ref><ref type="bibr" target="#b152">Xu, 2017)</ref>. The proposed model in (4.1) has two major identifiability issues. The first issue is that the proposed model remains unchanged after certain linear transformations of both B and U, causing the covariate effects together with the intercepts, represented by B, and the latent factors, denoted by U, to be unidentifiable. The second issue is that the model is invariant after an invertible</figDesc><table><row><cell>4.3 Method</cell></row><row><cell>4.3.1 Model Identifiability</cell></row><row><cell>Identifiability issues commonly occur in latent variable models (Allman et al.,</cell></row><row><cell>2009;</cell></row><row><cell>Motivated by these challenges, we propose interpretable and practical identifi-</cell></row><row><cell>ability conditions in Section 4.3.1. We then incorporate these conditions into the</cell></row><row><cell>joint-likelihood-based estimation method in Section 4.3.2. Furthermore, we intro-</cell></row><row><cell>duce a novel inference framework for performing statistical inference on β j , γ j , and</cell></row><row><cell>U i in Section 4.4.</cell></row></table><note><p><p><p><p>including covariate-adjusted logistic and probit factor models, are less investigated. Common techniques for linear models are not applicable to the considered general nonlinear model setting. transformation of both U and Γ as in the linear factor models</p><ref type="bibr" target="#b7">(Bai and Li , 2012;</ref><ref type="bibr" target="#b48">Fan et al., 2013)</ref></p>, causing the latent factors U and factor loadings Γ to be undetermined.</p>Under the model setup in (4.1), we define the joint probability distribution of responses to be P</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>apply our proposed method to analyze the Programme for International Stu-2018 used the computer-based assessment mode and the assessment lasted two hours for each student, with test items mainly evaluating students' proficiency in mathematics, reading, and science domains. A total of 930 minutes of test items were used</figDesc><table><row><cell>and each student took different combinations of the test items. In addition to the</cell></row><row><cell>assessment questions, background questionnaires were provided to collect students'</cell></row><row><cell>information.</cell></row><row><cell>In this study, we focus on PISA 2018 data from Taipei. The observed responses</cell></row><row><cell>are binary, indicating whether students' responses to the test items are correct, and</cell></row><row><cell>dent Assessment (PISA) 2018 data 2 . PISA is a worldwide testing program that</cell></row><row><cell>compares the academic performances of 15-year-old students across many countries</cell></row></table><note><p><p><p><p>(OECD, 2019)</p>. More than 600,000 students from 79 countries/economies, representing a population of 31 million 15-year-olds, participated in this program. The PISA 2 The data can be downloaded from: https://www.oecd.org/pisa/data/2018database/ we use the popular item response theory model with the logit link (i.e., logistic latent factor model;</p><ref type="bibr" target="#b125">Reckase, 2009)</ref></p>. Due to the block design nature of the large-scale assessment, each student was only assigned to a subset of the test items, and for the Taipei data, 86% response matrix is unobserved. Note that this missingness can be considered as conditionally independent of the responses given the students' characteristics. Our proposed method and inference results naturally accommodate such missing data and can be directly applied. Specifically, to accommodate the incomplete responses, we can modify the joint log-likelihood function in (4.2) into</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Table4.1, 58.44% of male students correctly answered this question, which exceeds the proportion of females, 51.29%.Besides gender effects, we estimate the effects of school strata on the students' response and present the point and interval estimation results in the left panel of Figure4.8. All the detected biased questions are from math and science sections, with 6 questions for significant effects of whether attending public school and 5 questions for whether residing in rural areas. To further investigate the importance of controlling for the latent ability factors, we compare results from our proposed method with the latent factors, to the results from directly regressing responses on covariates without latent factors. From the right panel of Figure4.8, we can see that without conditioning on the latent factors, there are excessive items detected for the covariate of whether the school is public or private. On the other hand, there are no biased items detected if we only apply generalized linear regression to estimate the effect of the covariate of whether the school is in rural areas.</figDesc><table><row><cell></cell><cell>Public</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Public -without latent variable</cell><cell></cell></row><row><cell></cell><cell>Math</cell><cell>Reading</cell><cell cols="2">Science</cell><cell></cell><cell>2</cell><cell>Math</cell><cell>Reading</cell><cell>Science</cell></row><row><cell>Public School Effect Estimator</cell><cell>-4 0 4</cell><cell></cell><cell></cell><cell></cell><cell>Public School Effect Estimator</cell><cell>-1 0 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell></row><row><cell></cell><cell></cell><cell cols="2">PISA Questions for TAP</cell><cell></cell><cell></cell><cell></cell><cell cols="3">PISA Questions for TAP</cell></row><row><cell></cell><cell>Rural</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Rural -without latent variable</cell><cell></cell></row><row><cell></cell><cell>Math</cell><cell>Reading</cell><cell cols="2">Science</cell><cell></cell><cell>2</cell><cell>Math</cell><cell>Reading</cell><cell>Science</cell></row><row><cell>Rural Region Effect Estimator</cell><cell>-4 0 4</cell><cell></cell><cell></cell><cell></cell><cell>Rural Region Effect Estimator</cell><cell>-1 0 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell></row><row><cell></cell><cell></cell><cell cols="2">PISA Questions for TAP</cell><cell></cell><cell></cell><cell></cell><cell cols="3">PISA Questions for TAP</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>, where each Ψ i t has Kruskal rank O t and the same row dimension κ t as Φ t . Denote the columns in Φ t to be ϕ t0 , • • • , ϕ t(C-1) and the columns in Ψ i t to</figDesc><table><row><cell>Ψ i 1 , Ψ i 2 and Ψ i 3 be ψ i t0 , • • • , ψ i t(C-1) . Conditions (B1) and (</cell></row></table><note><p>4, Φ can be decomposed into Φ 1 , Φ 2 and Φ 3 , where each Φ t has Kruskal rank I t and row dimension κ t . And in Appendix A.1, Ψ i can be decomposed into</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>19)For the second term in (B.19), consider Assumption III.5 that |b ′′ (t 1 ) -b ′′ (t)| ≤ B|t 1 -t|b ′′ (t) by letting t 1 = η T Żi and t = (η * ) T</figDesc><table><row><cell>Żi and applying Cauchy-Schwarz</cell></row><row><cell>inequality, we have</cell></row><row><cell>max i=1,...,n</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>1 is symmetric and positive definite. Substituting these results into (B.35), we</figDesc><table><row><cell>have</cell></row><row><cell>max i=1,...,n</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>(B.46)  is based on Assumption III.5(4) that |b ′′ (t 1 ) -b ′′ (t)| ≤ B|t 1 -t|b ′′ (t) with t 1 = η T Żi and t = (η * ) T Żi . The last inequality (B.47) is from the (B.8) of Lemma B.3 and from the results of Proposition III.6.</figDesc><table><row><cell cols="2">where the inequality</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>46)</cell></row><row><cell>≲ s η</cell><cell>log p n</cell><cell>+</cell><cell>log n p</cell><cell>M + O p</cell><cell>1 √ n</cell><cell>+</cell><cell>log n p</cell><cell>(B.47)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>) inLemma B.3 and (B.50) is from the condition (s w ∨ s η )(n -1/2 log p + p -1/2 n 1/2 log n) = o p (1). This completes the proof of Lemma B.5.</figDesc><table><row><cell>2 s η ≲ n 1/2 (s w ∨ s η ) log p n where (B.48) is by applying Cauchy-Schwarz inequality, (B.49) is from the (B.8) + log n p 1/2 (s η ∨ s w ) log p n + log n p 1/2 (B.49) log p n + log n p = o p (1), (B.50) and (B.9B.7.4 Proof of Lemma B.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head></head><label></label><figDesc>C.1-C.6 and the estimation consistency for the transformation A 0 and G 0 are provided in Proposition C.8. These results together with the transformation technique in (C.4) are used to derive theoretical properties of ϕ * . Before we start, we denote the bound in √ nι nq,p → 0, we have A nq,p , G nq,p = o(ζ -1 nq,p ).</figDesc><table><row><cell cols="2">Proposition C.8 as</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A nq,p =</cell><cell>√ p ζ nq,p</cell><cell>∧ p 1/2 ι nq,p ∨</cell><cell>p 3/2 (nq) 3/ξ ζ 2 nq,p</cell><cell>, G nq,p = pι nq,p ∨</cell><cell>p 3/2 (nq) 3/ξ nq,p ζ 2</cell><cell>.</cell></row><row><cell cols="3">Here when p 3/2 ζ -1 nq (nq) 3/ξ → 0 and p</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">C.3.1 Proof of Theorem IV.13</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head></head><label></label><figDesc>vii) of Lemma C.16 and Lemma C.12, the first term in bounded by O</figDesc><table><row><cell>(iv), (v) of Lemma C.16 and Proposition C.12 we know</cell></row><row><cell>(</cell></row></table><note><p>p (pζ -2 nq,p ) and the second term is bounded by O p (pζ -2 nq,p ). For the second part (C.61b), by (i),</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head></head><label></label><figDesc>1/ξ ϵ nq .For (C.62b), by (ii), (iv), (v) of Lemma C.16 and (vi) of Lemma C.18 we have∥(C.62b)∥ ≤ H -1 uu ′ H uf ′ [K i ,] H -f H f u ′ H -1 uu ′ S u ≤ O p</figDesc><table><row><cell>p 3/2 (nq) 2/ξ √ nq</cell><cell>ϵ nq</cell></row><row><cell cols="2">For (C.62c), by Lemma (ii), (vi) of Lemma C.18 and (iv), (v) of Lemma C.16 we have</cell></row><row><cell>(C.62c)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head></head><label></label><figDesc>1 uu ′ . By (ii) of Lemma C.16 we know H -1 uu ′ ∞ = H -1 uu ′ 1 = O p (√pn) and by (ii), (vi) of Lemma C.16 we have</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head></head><label></label><figDesc>estimates. Here Λ 1 and Λ 2 are defined in the following. DefineH Lf f ′ (s), H Lf u ′ (s), H Luf ′ (s), H Luu ′ (s), H Rf u ′ (s), H Ruf ′ (s) and Λ(s) as H Lf f ′ (ϕ), H Lf u ′ (ϕ), H Luf ′ (ϕ), H Luu ′ (ϕ), H Rf u ′ (ϕ), H Ruf ′ (ϕ)and Λ(ϕ) on ϕ 0 + s( ϕϕ 0 ). Here ϕ is defined in Proposition C.4. Then M</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head></head><label></label><figDesc>T ∂ U ir H P (ϕ ♭ )δϕ can be bounded as followsδϕ T ∂ U ir H P (ϕ ♭ )δϕ</figDesc><table><row><cell>≤</cell><cell>2c n</cell><cell cols="5">|δU ir |</cell><cell>1 q</cell><cell></cell><cell>n i=1</cell><cell>γ ♭ jr δγ jr +</cell><cell>1 n</cell><cell>n i=1</cell><cell>U ♭ ir δU ir</cell></row><row><cell></cell><cell>+</cell><cell cols="3">2c n</cell><cell cols="2">l̸ =r</cell><cell cols="4">|δU il |</cell><cell>1 q</cell><cell>n i=1</cell><cell>(γ ♭ jl δU ir + U ♭ ir δγ jl ) +</cell><cell>1 n</cell><cell>n i=1</cell><cell>(U ♭ il δU ir + U ♭ ir δU il )</cell></row><row><cell></cell><cell>+</cell><cell cols="2">c n</cell><cell cols="3">|U ♭ ir |</cell><cell cols="2">1 q</cell><cell cols="2">q j=1</cell><cell>δγ 2 jr +</cell><cell>1 n</cell><cell>n i=1</cell><cell>δU 2 ir</cell></row><row><cell></cell><cell cols="2">+ 2</cell><cell cols="3">1 n 2</cell><cell cols="2">r̸ =l</cell><cell cols="3">|γ ♭ jl |</cell><cell>1 n</cell><cell>n i=1</cell><cell>δU ih δU</cell></row></table><note><p>il , which implies that δϕ T ∂ U ir H P (ϕ ♭ )δϕ = O p n -1 ζ -2 nq using (C.29)-(C.32) with ϕ ♭ ∈</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>OECD: Organisation for Economic Co-operation and Development</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>1/2 ≤ 2C 2 .</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>First and foremost, I would like to express my deepest gratitude to my advisor <rs type="person">Prof. Gongjun Xu</rs>. Without his invaluable mentorship throughout my PhD journey,</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>tions in educational assessments <ref type="bibr" target="#b73">(Holland and Wainer , 2012;</ref><ref type="bibr" target="#b145">Wang et al., 2023)</ref>. For instance, the advantage of certain demographic groups on exam questions can change along the continuum of the assessed latent skills and abilities <ref type="bibr" target="#b145">(Wang et al., 2023)</ref>. In the future, it is also an interesting direction to extend our considered model setup to accommodate the non-uniform DIF setting and develop the estimation approach and theoretical results under such generalized models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDICES Appendix of Chapter 2 A.1 Proofs of Propositions and Theorems</head><p>In this section, we first introduce a lemma adapted from Proposition 3 in Huang and <ref type="bibr" target="#b76">Bandeen-Roche (2004)</ref>, which is an important tool in later proofs to associate the identifiability of parameters (β, γ, λ) with the identifiability of (η i , Θ i ) = {η i c , θ i jrc : j = 1, . . . , J, r = 0, . . . , M j -1, c = 0, . . . , C -1}, for i = 1, . . . , N .</p><p>Lemma A.1. For any subject i = 1, . . . , N , we define transformed variables (ϵ i , ω i ) = {ϵ c , ω jrc : j = 1, . . . , J, r = 0, . . . , M j -1, c = 0, . . . , C -1} such that (η i , Θ i ) and (ϵ i , ω i ) are related through the following equations,</p><p>, c = 0, . . . , C -1;</p><p>, j = 1, . . . , J; r = 0, . . . , M j -1; c = 0, . . . , C -1.</p><p>Then (η i , Θ i ) are identifiable if and only if (ϵ i , ω i ) are identifiable.</p><p>Lemma A.3. <ref type="bibr" target="#b86">Kruskal (1977)</ref> For t = 1, 2 and 3, denote O t = rank K (Ψ t ) as the Kruskal rank of Ψ t , where Ψ t is a decomposed matrix of Ψ.</p><p>If</p><p>then Ψ 1 , Ψ 2 and Ψ 3 uniquely determines the decomposition of Ψ up to simultaneous permutation and rescaling of columns.</p><p>Corollary A.4. <ref type="bibr" target="#b2">Allman et al. (2009)</ref> Consider the restricted latent class models with C classes. For t = 1, 2 and 3, let Ψ t denote a decomposed matrix of Ψ and O t denote its Kruskal rank. If</p><p>then the parameters of the model are uniquely identifiable, up to label swapping.</p><p>Corollary A.5. <ref type="bibr" target="#b2">Allman et al. (2009)</ref> Continue with the setting in Corollary A.4. For t = 1, 2, 3, let Ψ t denote a decomposed matrix of Ψ and κ t denote its row dimension.</p><p>If min{C, κ 1 } + min{C, κ 2 } + min{C, κ 3 } ≥ 2C + 2,</p><p>Then the parameters of the restricted latent class models are generically identifiable up to label swapping.</p><p>Combining all these results as well as Proposition 2 in Huang and Bandeen-Roche</p><p>(2004), we present Lemma A.6, which is the key in the proof of Theorem II.7.</p><p>Lemma A.6. For polytomous-response RegLCMs, (η i , Θ i ) are strictly identifiable if (B1) ,(B2) and (B3.a) hold, and are generically identifiable if (B1), (B2) and (B3.b) hold.</p><p>(B1) S J &gt; CJ(S -1) + C;</p><p>(B2) All β, γ, λ are free parameters and all x i , z i are finite;</p><p>(B3) The matrix Φ can be decomposed into Φ 1 , Φ 2 , Φ 3 , with Kruskal rank of each Φ t to be I t and the dimension of each Φ t to be κ t × C. We have either (B3.a) I 1 + I 2 + I 3 ≥ 2C + 2; or (B3.b) min{C, κ 1 } + min{C, κ 2 } + min{C, κ 3 } ≥ 2C + 2.</p><p>The proof of Lemma A.6 is provided in Section B.</p><p>Proof of Theorem II.7. From condition (C4), the Kruskal rank I t of Φ t fulfill the arithmetic condition of (B3.a) in Lemma A.6. Given (A1) and (A2), (B1) and (B2) also hold. According to Lemma A.6, the RegLCMs are strictly identifiable at (η i , Θ i ) for i = 1, . . . , N . Following the similar arguments in Step 2-3 from the Proof of Theorem II.6, we show that (β, γ, λ) in RegLCMs are identifiable given (η i , Θ i ) are identifiable under (A3). Hence we complete the proof.</p><p>Proof of Proposition II.8. As mentioned in Section 2.4, (C4 * ) is the sufficient condition for the identifiability of general restricted latent class models with binary responses according to Theorem 1 in <ref type="bibr" target="#b152">Xu (2017)</ref>. This condition is further extended to restricted latent class models with polytomous responses by Theorem 2 in <ref type="bibr" target="#b42">Culpepper (2019)</ref>. So for RegCDMs, (η i , Θ i ) are strictly identifiable given (C4 * ) for i = 1, . . . , N . Then based on the the similar arguments in Step 2-3 from the Proof of Theorem <ref type="bibr">II.6,</ref><ref type="bibr">(β,</ref><ref type="bibr">γ,</ref><ref type="bibr">λ)</ref> in RegCDMs are identifiable given (η i , Θ i ) are identifiable.</p><p>Proof of Theorem II.9. For t = 1, 2 and 3, the decomposed matrix Φ t and the decomposed matrix Ψ t have the same row dimension κ t . So given (C4 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix of Chapter 3 B.1 Preliminaries</head><p>We start with introducing some notations and definitions. For a vector r = (r 1 , . . . , r l ) T , we let S r = {j : r j ̸ = 0}, ∥r∥ ∞ = max j=1,...,l |r j |, ∥r∥ q = ( l j=1 |r j | q ) 1/q for q ≥ 1 and s r = card(S r ). For a matrix A = (a ij ) n×l , let ∥A∥ ∞,1 = max j=1,...,l n i=1 |a ij | to be the maximum absolute column sum, ∥A∥ 1,∞ = max i=1,...,n l j=1 |a ij | to be the maximum of the absolute row sum, ∥A∥ max = max i,j |a ij | to be the maximum of the matrix entry, λ min (A) and λ max (A) to be the smallest and largest eigenvalues of</p><p>to be the Frobenius norm of A. For sequences {a n } and {b n }, we write a n ≲ b n if there exists a constant C &gt; 0 such that a n ≤ Cb n for all n, and a n ≍ b n if a n ≲ b n and b n ≲ a n . For any sub-exponential random variable Y 1 , we define the sub-exponential norm as</p><p>For any sub-Gaussian random variable Y 2 , we define the sub-Gaussian norm as</p><p>Next, we give a review of our model framework. We assume the response y given the covariate of interest D, the nuisance covariates Q and the unmeasured confounders Before proving these results, we first introduce some lemmas that will be used in the proofs.</p><p>Lemma B.1 (Concentration of the Gradient and Hessian). Under Assumptions 1-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.2 Estimation Consistency of w</head><p>Before we present the proof for the estimation consistency of the estimator w, we introduce additional results established based on the estimation consistency of η and used in proving the estimation consistency of w. To prove the consistency of w, we denote δ = w -w * and establish the estimation error bound for ∥ δ∥ 1 . Recall that w is a solution to (B.4), that is</p><p>By definition, we have</p><p>the following probability tends to 0.</p><p>=: P 1 + P 2 + P 3 + P 4 + P 5 + P 6 , (B.42)</p><p>, then it can be verified that the probability P 2 , P 4 and P 6 tends to 0 under</p><p>Next, we look at P 1 , P 3 and P 5 to determine t 1 , t 2 and t 3 separately.</p><p>For P 1 , as [y i -b ′ {(η * ) T Z i }]U ik is independent mean 0 sub-exponential random as w * q = (w * 2 , . . . , w * q ) T and w * u = (w * p+1 , . . . , w * p+K ) T . Denote τ * q = (1, -(w * q ) T ) T =</p><p>(1, -w * 2 , . . . , -w * q ) T .</p><p>where t = max{t 4 , t 5 , t 6 , t 7 }.</p><p>Similarly as in proof of condition (i), we let max</p><p>We let</p><p>Then we determine t 4 , t 5 , t 6 and t 7 such that the probability (B.43) tends to 0.</p><p>mean 0 sub-exponential random variables and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix of Chapter 4</head><p>This Supplementary Material provides proofs of the theoretical results in the main text and additional simulation results. It is organized as follows: Section C.1 presents our proof strategy and provides a roadmap for the theoretical analysis. Section C.2 introduces a constrained maximum likelihood estimator and discusses its theoretical properties. Section C.3 contains the proofs of the main theorems and the corollary.</p><p>Sections C.4 and C.5 present the proofs for the propositions introduced in Section C.2 and technical lemmas, respectively. Finally, Section C.6 presents additional simulation results.</p><p>We begin with introducing notations and expressions used throughout the subsequent theoretical proofs. For any integer</p><p>to be the y-dimensional vector with x-th entry to be 1 and all other entries to be 0. For any symmetric matrix M, let λ min (M) and λ max (M) be the smallest and largest eigenvalues of M. For any matrix A = (a ij ) n×l , let ∥A∥ 1 = max j=1,...,l n i=1 |a ij | be the maximum absolute column sum, ∥A∥ ∞ = max i=1,...,n l j=1 |a ij | be the maximum of the absolute row sum, ∥A∥ max = max i,j |a ij | be the maximum of the absolute matrix entry, for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Define the notation</head><p>the sub-matrix of A with entries in rows indexed by S 1 and columns indexed by S 2 . When S 1 = [n], we omit S 1 and write</p><p>. Denote "w.h.p." as the abbreviation for "with high probability approaching 1". For notation simplicity, we define the following expressions frequently used in the proofs: δ nq = min{n 1/2 , q 1/2 }, ϵ nq = (nq) ϵ with sufficiently small ϵ &gt; 0, and</p><p>.</p><p>Throughout this Supplementary Material, for the statement convenience in the proof, we slightly adjust the notation and re-index the covariate effect coefficient as</p><p>T with the first component β j1 being the intercept and others (β j2 , . . . , β jp ) being the covariate effects, namely DIF effects in psychometrics <ref type="bibr" target="#b73">(Holland and Wainer , 2012)</ref>. Accordingly, we clarify that X ∈ R n×p is a design matrix with the first column being all ones and that each X i ∈ R p incorporates entry-one in the vector.</p><p>We further define</p><p>be the assembled vector of all parameters, where</p><p>T are the assembled vector of all f j 's and all U i 's, respectively. The true parameters that satisfy our proposed identifiability conditions IV.4-IV.7 are denoted </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Proof Strategy and Framework</head><p>In this section, we outline the proof strategy for the theorems discussed in the main text. Directly establishing the theoretical properties of the estimator ϕ * is challenging due to the potentially complicated correlations between the latent variables U * i and the observed covariates X i . Following the discussions in the main text (before Assumption IV.11), we employ the following transformed U 0 that are orthogonal with X, which plays an important role in establishing the theoretical results.</p><p>As defined in the main text,</p><p>where</p><p>containing the matrix of corresponding eigenvectors. We let</p><p>These transformed parameters ϕ 0 give the same joint likelihood as that of the true parameters ϕ * , which facilitate our theoretical understanding of the joint-likelihoodbased estimators. In particular, these transformed parameters ϕ 0 can be readily shown to satisfy the following identifiability conditions:</p><p>Similarly, we can also establish the transformation from ϕ 0 to the true parameters ϕ * as follows. Specifically, we can show the true parameters</p><p>Here V 0 and U 0 contain the eigenvalues and eigenvectors of (nq</p><p>where we define L(Y|ϕ) = -L(Y|ϕ) -P (Γ, U) as the loss function with</p><p>The ndiag (q -1 Γ ⊺ Γ) is the upper triangular matrix consisting of the nondiagonal elements of q -1 Γ ⊺ Γ and ndiag(n -1 U ⊺ U) is defined similarly.</p><p>We emphasize that under any choice of c &gt; 0, minimizing (C.2) is equivalent to minimizing (C.1) subject to Condition 1 ′ and Condition 2 ′ . It can be verified that for any ϕ, we have the penalty term P (Γ, U) = 0 if identifiability conditions 1 ′ -2 ′ hold and the penalty term P (ϕ) &lt; 0 otherwise. For any A ∈ R K×p and invertible G ∈ R K×K , ϕ(G, A) consisting of (U + XA T )G, ΓG -T , and B -ΓA gives the same log-likelihood as that of ϕ. Among the equivalence class of estimators that maximize the log-likelihood function L(Y|ϕ), we choose the solution to our problem to be the one that satisfies the identifiability conditions, which uniquely exists. Therefore, our solution leads to P ( ϕ 0 ) = 0 whereas all other solutions do not satisfy the proposed identifiability conditions as a result of negative penalty terms. Hence we conclude that the estimator from minimizing the objective function L(Y|ϕ) is equivalent to that obtained by maximizing L(Y|ϕ) under identifiability conditions 1 ′ -2 ′ . We highlight that the choice of any positive c shall yield the same estimation results and that we set c to be smaller than b L just for convenience in our theoretical analysis. The similar idea of introducing the regularization term into the joint likelihood function and formulating the constrained maximum likelihood estimation has been extensively and f 0 j as</p><p>for any a ∈ R K+p with ∥a∥ 2 = 1, where the asymptotic covariance matrices are defined as</p><p>) -1 a and can be consistently estimated by</p><p>Remark C.7. Proposition C.6 provides the asymptotic distributions for all individual estimators under Conditions 1 ′ -2 ′ . For the asymptotic distributions of β 0 j and γ 0 j , the scaling conditions imply p = o(n 1/4 ∧ q/ √ n) up to small order term. For the asymptotic normality of U 0 i , the scaling condition implies p = o{q 1/3 ∧ (n 2 /q) 1/5 } up to small order term. These asymptotic results lay the foundation for deriving the asymptotic distributions for ϕ * .</p><p>If we further write Σ 0 f,j in a 2 × 2 block defined as follows:</p><p>and the asymptotic covariance matrices of Σ * γ,j can be consistently estimated by</p><p>For each i ∈ [n], the asymptotic covariance matrices of Σ * u,i can be consistently estimated by</p><p>Next we prove Corollary IV.17 that the above estimators are consistent.</p><p>Proof of Corollary IV.17</p><p>Recall that we denote</p><p>on the individual consistency results for γ 0 j , β 0 j , and U 0 i in Proposition C.5, we</p><p>. By the continuity of functions l ′ ij (•) and l ′′ ij (•) from Assumption IV.9, we obtain</p><p>Such V p is the low-rank decomposed matrix of the Hessian matrix of the penalty term P (Γ, U). We assemble those decomposed vectors into Λ = (Λ T 1 , Λ T 2 ) T as follows:</p><p>Here the entries in the last q(K + p) × Kp part of Λ 1 are zeros. We also denote</p><p>Then the matrix H P (ϕ 0 ) can be expressed as</p><p>Here we write Λ 0 to denote Λ(ϕ 0 ) for short.</p><p>Moreover, we further define the following:</p><p>and let V 0 ∈ R (q(K+p)+nK)×(K 2 +Kp) to be</p><p>It can be verified that the column vectors of V 0 form the null space of the Hessian matrix under regularization conditions, and can be approximated by V p , which leads to local convexity of the objective function in some regime.</p><p>κ 2 and n -1 n i=1 U 0 i X T i = 0. Hence, we know that</p><p>for some 0 &lt; γ &lt; min{c, b L }. (C.19) also implies that there are a total of K 2 + Kp</p><p>with corresponding eigenvectors:</p><p>Therefore, the column vectors of V 0 form a non-degenerate basis of the null space of</p><p>The inner products of these vectors are given as follows:</p><p>Here the first K 2 column vectors are orthogonal to each other and to the last Kp column vectors. But the last Kp column vectors are not orthogonal to each other.</p><p>However, since rank(X) = p, these vectors are still linear independent and form a basis for the null space.</p><p>To further bound the eigenvalues of D</p><p>q , we define function N : R q(K+p)+nK → R K 2 +Kp that maps any x ∈ R nK+qK+qp to its projection on the null space of D</p><p>under basis V 0 (ϕ 0 ). Since this matrix is full-column rank, i.e., the column vectors are linearly independent as shown above, the projection is unique. Specifically,</p><p>T where N i is the ith coordinates corresponding to vector</p><p>. We first observe that at ϕ 0 , we have for</p><p>Therefore, we get the projection of first K 2 column vectors of V p given by</p><p>For the projection of</p><p>ω ks is orthogonal to all columns vectors of V 0 .</p><p>Then the projection on span</p><p>ω ks and therefore</p><p>Define N the assembled coordinates, i.e., assemble the coordinates under V 0 as the</p><p>Next we prove (C.22) has positive upper-bound independent of n and q by contradiction. Suppose for any ϵ &gt; 0 we have (C.22</p><p>Then by (C.22) &lt; ϵ we know that each term in (C.22) will be smaller than ϵ, which implies</p><p>Then plug in (C.25) and (C.26) we have</p><p>which is a contradiction. Then by boundedness of γ 0 j , U 0 i and Lemma C.15, we have</p><p>To sum up, we get λ min D</p><p>Local convexity in a neighborhood of ϕ 0 . For arbitrary ϕ ∈ B(D) ∩ {ϕ : √ p∥D -1/2 q (ϕ -ϕ 0 )∥ ≤ m}, we prove for any z, ∥z∥ = 1,</p><p>has positive upper bound. Note here we consider a neighborhood region of {ϕ : √ p∥D -1/2 q (ϕ -ϕ 0 )∥ ≤ m} where it has been proved in Proposition C.1 that the constrained MLEs will fall into this region with any small m when p ≲ δ nq .</p><p>For ϕ ∈ B(D), we omit ϕ in the notations including V 0 (ϕ), N (ϕ), R(ϕ) and other vectors dependent on ϕ for simplicity. We first focus on the expression related</p><p>and by Assumption IV.9 and the mean value theorem, we have</p><p>where the last equality is from ϕ ∈ B(D) ∩ {ϕ : √ p∥D</p><p>Therefore combining above results with the estimates for H R on ϕ 0 , we have</p><p>Lastly, we write the remaining terms in z T D</p><p>Here</p><p>r (ϕ 0 ) are previously defined vectors on parameter ϕ 0 and n v (ϕ 0 ) =</p><p>we conclude that inside this restricted region,</p><p>Also, by</p><p>bound for small enough m.</p><p>Combining the above results with (C.33) and (C.34), we complete the proof for Proposition C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition C.3 and C.4</head><p>Suppose ϕ is the solution to arg min</p><p>The idea is that we first show that ϕ is the interior point of the space B(D) ∩ ϕ :</p><p>Lemma C.17, we have</p><p>under Assumption IV.10. Similarly, we have</p><p>Hence, we show ϕ, the solution to (C.35), to be an interior point of the specified space B(D) ∩ ϕ :</p><p>By the definition of ϕ, we have ∂ ϕ ∥D (ϕ-ϕ 0 ) ≤ m w.h.p.. Hence, we conclude that ϕ = ϕ 0 and therefore S( ϕ 0 ) = 0 w.h.p..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarly, we can use the expansion of [S</head><p>For the first term, we use similar arguments as in bounding ∥(C.39)∥:</p><p>Because we have ∥q -1 q j=1 γ 0 j l ′ ij (w 0 ij )∥ = q -1 log n from Proposition C.13, similar to the approach of estimating (C.39), we have</p><p>By (C.42) + (C.43) = 0, we show that for any i ∈</p><p>The results in Proposition C.1 and C.5 can be summarized as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition C.6</head><p>To establish the asymptotic distribution for the estimators, we expand the first order condition S( ϕ 0 ) to high orders as follows</p><p>following estimates:</p><p>Proof. Define L θ as q × n matrix with ∂ θ ij l ij (θ ij + (β j ) T X i ) in the j-th row and i-th column. By Assumption IV.9 we know that entries of L(ψ 0 ) are independent and have a finite fourth moment, therefore by the concentration inequality of Lata la (2005),</p><p>we have</p><p>Next for the first order derivatives of Ľ on ψ 0 with respect to θ:</p><p>And for the first order derivatives of Ľ on ψ 0 with respect to β</p><p>Then the estimation results for the first order derivative can be derived directly from Lemma C.18. Under Assumptions IV.8-IV.11, we have estimates on ϕ 0 as follows:</p><p>Proof. For (i), since Λ 1 = (Λ 1• , 0 q(K+p)×Kp ), we only need to prove first K 2 entries of Λ T 1 H -1 Lf f ′ S f can be bounded by O p √ pn -1/2 δ -1 nq ϵ nq . By (ii) of Assumption IV.11, for all r ∈ [K], the K(r)th element can be bounded as</p><p>Here the term ϵ nq arises from using Bernstein bound as {l ′ ij } i,j are independent with zero mean. We suppress the commonly used log terms and replace them with ϵ nq for brevity. and for h ̸ = l ∈ [K], the K(h, l)th element can be bounded as</p><p>=O p pn -3/2 q -1/2 ϵ nq .</p><p>Similar with the estimates (C.86) we have</p><p>and consequently, we have</p><p>All these together imply (ii) and (iii).</p><p>Next for (iv), since Λ 2 ∈ R nK×K 2 +Kp , we show that each element can be bounded by O p √ pn -1/2 q -1/2 ϵ nq . By Proposition C.13 and (ii) of Lemma C.16, for all r ∈ [K],</p><p>the K(r)th element can be bounded as</p><p>and similarly for all h ̸ = l ∈ [K], the K(h, l)th element can be bounded as</p><p>for all r ∈ [K], s ∈ [p], the {K 2 + (r -1)p + s}th element can be bounded as</p><p>Together we obtain (iv). For (v) and (vi), it suffices to show that each term on the right side of the following</p><p>can be bounded by O p pn -1/2 q -3/2 (nq) 1ξ ϵ nq , which are followed by the technique of proving (ii) and (iii), (ii), (iv), (viii) of Lemma C.16.</p><p>Lemma C.19. Under Assumptions IV.8-IV.11, the residuals defined in the proof of Proposition C.6 can be bounded as follows:</p><p>Proof. Recall we have assumed that the individual log-likelihood function is three times differentiable, i.e., l ′′′ ij is continuous. In the following we denote δυ = υ -υ 0 ; here υ can be ϕ, U, Γ, β. Then derivative with respect to f jr of δϕ T H(ϕ ♭ )δϕ is given, with ϕ ♭ on the segment between ϕ and ϕ 0 , as</p><p>and </p><p>Proof. Since A and B are of finite dimension, we have ∥</p><p>Let γ be the eigen-gap of AB. By Weyl's theorem, we know that when ν is small enough, the eigen-gap of A B is larger than γ/2. Similarly if we denote γ L = min λ min (A), λ min (B) &gt; 0 and γ U = max λ max (A), λ max (B) &gt; 0, then we have min λ min ( A), λ min ( B) &gt; γ L /2 and max λ max ( A), λ max ( B) &lt; 2γ U when ν exceeds some threshold. Next we discuss only when ν exceeds these two thresholds.</p><p>Next let the singular value decomposition (SVD) of A be U 1 Υ 1 U T 1 , and the singular value decomposition of A 1/2 BA 1/2 be U 2 Υ 2 U T 2 with A 1/2 defined as U 1 Υ 1/2 1 U T 1 . Then</p><p>is the unique solution up to a permutation of Υ 2 . The uniqueness is a result of the eigenvalues of AB being different, which implies that there exists a unique order for the diagonal entries of Υ 2 , and thus U 2 is uniquely determined.</p><p>Similarly we write the SVD of A and A 1/2 B A 1/2 as U 1 Υ 1 U T 1 and U 2 Υ 2 U T 2 . Therefore G can be uniquely expressed as A -1/2 U 2 Υ -1/4 . The existence of A -1/2 is implied by λ min ( A) &gt; γ L /2, λ max ( A) &lt; γ U /2 and uniqueness is implied by the fact that the eigen-gap of A B is larger than γ/2.</p><p>Implied by ∥ A -A∥ = O p (ν) and that λ max ( A), λ max (A), λ min ( A), λ max (A) are bounded in (2γ U , γ L /2), we have </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Addional Simulation Results</head><p>In Section 5 of the main text, we present simulation results at p * = 5 and p * = 30, across all considered settings. This section provides additional simulation results at BIBLIOGRAPHY</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Eigenvalue ratio test for the number of factors</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Horenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1203" to="1227" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Maximum-Likelihood Estimation of Parameters Subject to Restraints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Aitchison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Silvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="813" to="828" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Identifiability of parameters in latent structure models with many observed variables</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Allman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rhodes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6A</biblScope>
			<biblScope unit="page" from="3099" to="3132" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Asymptotic Normal Distribution of Estimators in Factor Analysis under General Conditions</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Amemiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="759" to="771" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Serum Amyloid A3 is required for normal weight and immunometabolic function in mice</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ather</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Poynter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-concordant analysis for logistic regression</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="384" to="414" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inferential theory for factor models of large dimensions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="135" to="171" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical analysis of factor models of high dimension</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="436" to="465" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation and inference for approximate factor models of high dimension</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of Economics and Statistics</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="298" to="309" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Determining the number of factors in approximate factor models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="191" to="221" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Confidence intervals for diffusion index forecasts and inference for factor-augmented regressions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1133" to="1150" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Estimating the association between latent class membership and external variables using bias-adjusted three-step approaches</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bakk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Tekle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Vermunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological Methodology</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="272" to="311" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Females show more sustained performance during test-taking than males</title>
		<author>
			<persName><forename type="first">P</forename><surname>Balart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oosterveen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3798</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simplifying the assessment of measurement invariance over multiple background variables: Using regularized moderated nonlinear factor analysis to detect differential item functioning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Belzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Cole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="55" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A statistical test for differential item pair functioning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Bechger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Maris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="317" to="340" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sparse models and methods for optimal instruments with an application to eminent domain</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2369" to="2429" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inference on treatment effects after selection among high-dimensional controls</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of Economic Studies</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="608" to="650" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Post-selection inference for generalized linear models with many controls</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Business &amp; Economic Statistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="606" to="619" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving the assessment of measurement invariance: Using regularization to select anchor items and identify differential item functioning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Belzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="673" to="690" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inference in high-dimensional multivariate response regression with hidden variables</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Some latent trait models and their use in inferring an examinee&apos;s ability</title>
		<author>
			<persName><forename type="first">A</forename><surname>Birnbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Theories of Mental Test Scores</title>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Lord</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Novick</surname></persName>
		</editor>
		<meeting><address><addrLine>Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1968">1968</date>
			<biblScope unit="page" from="395" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Brown</surname></persName>
		</author>
		<title level="m">Confirmatory factor analysis for applied research</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Guilford publications</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High-dimensional statistics with a view toward applications in biology</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="255" to="278" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A review of instrumental variable estimators for mendelian randomization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Methods in Medical Research</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2333" to="2355" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Statistical inference for high-dimensional generalized linear models with binary outcomes</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">542</biblScope>
			<biblScope unit="page" from="1319" to="1332" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An iterative procedure for linking metrics and assessing item bias in item response theory</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Candell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Drasgow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="260" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High-dimensional sparse factor modeling: applications in gene expression genomics</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Nevins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">484</biblScope>
			<biblScope unit="page" from="1438" to="1456" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The scree test for the number of factors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Cattell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate behavioral research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="276" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spectral deconfounding via perturbed sparse linear models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ćevid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">232</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Introducing the general polytomous diagnosis modeling framework</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1474</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Determining the number of factors in high-dimensional generalized latent factor models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="769" to="782" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Statistical analysis of Q-matrix based diagnostic classification models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">510</biblScope>
			<biblScope unit="page" from="850" to="866" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint maximum likelihood estimation for highdimensional exploratory item factor analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="124" to="146" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A sparse latent class model for cognitive diagnosis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Culpepper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="153" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Structured latent factor analysis for large-scale data: Identifiability, estimability, and their implications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">532</biblScope>
			<biblScope unit="page" from="1756" to="1770" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DIF statistical inference without knowing anchoring items</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="1097" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Statistical inference for noisy incomplete binary matrix</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">95</biblScope>
			<biblScope unit="page" from="1" to="66" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Double/debiased machine learning for treatment and structural parameters</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chetverikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Demirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duflo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Newey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Econometrics Journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="C68" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Latent structure analysis of a set of multidimensional contingency tables</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Clogg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">388</biblScope>
			<biblScope unit="page" from="762" to="771" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Latent class and latent transition analysis: With applications in the social, behavioral, and health sciences</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Lanza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">718</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A generalization of principal components analysis to the exponential family</title>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="617" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Best practices in exploratory factor analysis: Four recommendations for getting the most from your analysis, Practical assessment, research, and evaluation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Osborne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An exploratory diagnostic model for ordinal responses with binary attributes: Identifiability and estimation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Culpepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="921" to="940" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Concomitant-variable latent-class models</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Dayton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Macready</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">401</biblScope>
			<biblScope unit="page" from="173" to="178" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The generalized DINA model framework</title>
		<author>
			<persName><forename type="first">J</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="179" to="199" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploring the sensitivity of horn&apos;s parallel analysis to the distributional form of random data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dinno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="362" to="388" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Permutation methods for factor analysis and PCA</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dobriban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2824" to="2847" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Simultaneous inference for generalized linear models with unmeasured confounders</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roeder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.07261</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Large covariance estimation by thresholding principal orthogonal complements</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mincheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="603" to="680" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Challenges of Big Data analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Science Review</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="293" to="314" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Factor-adjusted regularized model selection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">216</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="85" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Are latent factor regression and sparse regression adequate?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On the identifiability of diagnostic classification models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="40" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The Impact of Residual and Unmeasured Confounding in Epidemiologic Studies: A Simulation Study</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Davey</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A C</forename><surname>Sterne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Epidemiology</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="646" to="655" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Effects of amount of DIF, test length, and purification type on robustness and power of Mantel-Haenszel procedures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fidalgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mellenbergh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Muñiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods of Psychological Research Online</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="43" to="53" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Constrained latent class models: Theory and applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Formann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="111" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Testing for associations between loci and environmental gradients using latent factor mixed models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Frichot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Schoville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>François</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Biology and Evolution</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1687" to="1699" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Using control genes to correct for unwanted variation in microarray data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gagnon-Bartsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Speed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="539" to="552" />
			<date type="published" when="2011">2011</date>
			<pubPlace>Oxford, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The R package CDM for cognitive diagnosis models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robitzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Groß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ünlü</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Empirical bayes shrinkage and false discovery rate estimation, allowing for unwanted variation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gerard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="32" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Exploratory latent structure analysis using both identifiable and unidentifiable models</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="231" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The sufficient and necessary condition for the identifiability and estimability of the DINA model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="468" to="483" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Partial identifiability of restricted latent class models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2082" to="2107" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Sufficient and necessary conditions for the identifiability of the Q-matrix</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="449" to="472" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Confidence intervals for causal effects with invalid instruments by using two-stage hard thresholding with voting</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="793" to="815" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Doubly debiased lasso: High-dimensional inference under hidden confounding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ćevid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1320" to="1347" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Non-uniqueness in probabilistic numerical identification of bacteria</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gyllenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reilink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verlaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Probability</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="542" to="548" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Hambleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Swaminathan</surname></persName>
		</author>
		<title level="m">Item response theory: Principles and applications</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Factor retention decisions in exploratory factor analysis: A tutorial on parallel analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hayton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Scarpello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational research methods</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="205" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Quantile regression estimates for a class of linear and partially linear errors-in-variables models</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="page" from="129" to="140" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A general Bahadur representation of M-estimators and its application to linear regression with nonstochastic designs</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2608" to="2630" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">On parameters of increasing dimensions</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="120" to="135" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Defining a family of cognitive diagnosis models using log-linear models with latent variables</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Henson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Templin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Willse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="210" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wainer</surname></persName>
		</author>
		<title level="m">Differential item functioning</title>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A rationale and test for the number of factors in factor analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="185" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Selecting the number of classes under latent class regression: a factor analytic analogue</title>
		<author>
			<persName><forename type="first">G.-H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="325" to="345" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Building an identifiable latent class model with covariate effects on underlying and measured variables</title>
		<author>
			<persName><forename type="first">G.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bandeen-Roche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">RSAD2 is necessary for mouse dendritic cell maturation via the IRF7-mediated signaling pathway</title>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Death &amp; Disease</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">823</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Semiparametric efficiency bounds for highdimensional models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jankova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van De Geer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2336" to="2359" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Confidence intervals and hypothesis testing for high-dimensional regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Javanmard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2869" to="2909" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Cognitive assessment models with few assumptions, and connections with nonparametric item response theory</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Junker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sijtsma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="258" to="272" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Instrumental variables estimation with some invalid instruments and its application to mendelian randomization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">513</biblScope>
			<biblScope unit="page" from="132" to="144" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Factor models and variable selection in highdimensional regression analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kneip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sarda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2410" to="2447" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Koopmans</surname></persName>
		</author>
		<title level="m">Statistical Inference in Dynamic Economic Models</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1950">1950</date>
			<biblScope unit="page">460</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">The identification of structural characteristics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Koopmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Reiersol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="181" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A framework for anchor methods and an iterative forward approach for DIF detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeileis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strobl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="83" to="103" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Kruskal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="138" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Factor modeling for high-dimensional time series: inference for the number of factors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="694" to="726" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Estimation of latent factors for highdimensional time series</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bathia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="901" to="918" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Some estimates of norms of random matrices</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lata La</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1273" to="1282" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Batch effect removal methods for microarray gene expression data integration: a survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lazar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="469" to="490" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Capturing heterogeneity in gene expression studies by surrogate variable analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Storey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS genetics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">161</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">A general framework for multiple testing dependence</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Storey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">48</biblScope>
			<biblScope unit="page" from="718" to="718" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.06605</idno>
		<title level="m">Statistical inference on latent space models for network data</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semiparametric analysis of the additive risk model</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="61" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">High-dimensional sparse additive hazards regression</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>American Statistical Association</publisher>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="247" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Simultaneous dimension reduction and adjustment for confounding variation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">51</biblScope>
			<biblScope unit="page" from="662" to="676" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<author>
			<persName><forename type="first">Linda</forename><forename type="middle">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T L</forename></persName>
		</author>
		<title level="m">Latent Class and Latent Transition Analysis: With Applications in the Social, Behavioral, and Health Sciences</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Correction for hidden confounders in the genetic analysis of gene expression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Listgarten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Schadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">38</biblScope>
			<biblScope unit="page">470</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Generalized factor model for ultrahigh dimensional correlated variables with mixed types</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">542</biblScope>
			<biblScope unit="page" from="1385" to="1401" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</title>
		<author>
			<persName><forename type="first">P.-L</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="559" to="616" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Global and simultaneous hypothesis testing for high-dimensional logistic regression models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">534</biblScope>
			<biblScope unit="page" from="984" to="998" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Efficient estimation and local identification in latent class analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Mchugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="331" to="347" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Generalized linear item response theory</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mellenbergh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">300</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Millsap</surname></persName>
		</author>
		<title level="m">Statistical approaches to measurement invariance</title>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Deficient IL-12p70 secretion by dendritic cells based on IL12B promoter genotype</title>
		<author>
			<persName><forename type="first">J</forename><surname>Müller-Berghaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Klüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Morahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schadendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genes &amp; Immunity</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="431" to="434" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Discrete-time survival mixture analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Masyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational and Behavioral statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="58" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Muthén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
		<title level="m">Mplus user&apos;s guide: Statistical analysis with latent variables, user&apos;s guide</title>
		<imprint>
			<publisher>Muthén &amp; Muthén</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">IL1B gene polymorphisms influence the course and severity of inflammatory bowel disease</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nemetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Immunogenetics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="527" to="531" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">A general theory of hypothesis tests and confidence regions for sparse high dimensional models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PISA 2018 Assessment and Analytical Framework</title>
		<meeting><address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<publisher>PISA, OECD Publishing</publisher>
			<date type="published" when="2017">2017. 2019</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">308</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Identifiability of latent class models with covariates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1343" to="1360" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">High-dimensional inference for generalized linear models with hidden confounding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">296</biblScope>
			<biblScope unit="page" from="1" to="61" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Statistical inference for covariateadjusted and interpretable generalized factor model with applications to testing fairness</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Submitted</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bi-Cross-Validation for Factor Analysis</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="119" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Bayesian inferences of latent class models with an unknown number of classes</title>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="621" to="646" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">An extension of the DINA model using covariates: Examining factors affecting response probability and latent classification</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="376" to="390" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Explanatory cognitive diagnostic models: Incorporating latent and observed predictors</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="376" to="392" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Preconditioning&quot; for feature selection and regression in high-dimensional problems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1595" to="1618" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Regularized multivariate regression for identifying master predictors with application to integrative genomics study of breast cancer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bergamaschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Pollack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="77" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">How many principal components? stopping rules for determining the number of non-trivial axes revisited</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Peres-Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Somers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="974" to="997" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Predicting latent class scores for subsequent analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bandeen-Roche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Budtz-Jørgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Groes</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="244" to="262" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Principal components analysis corrects for stratification in genome-wide association studies</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Plenge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Weinblatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Shadick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Genetics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="904" to="909" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Statistical linkage analysis of substitutions in patient-derived sequences of genotype 1a hepatitis c virus nonstructural protein 3 exposes targets for immunogen design</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Quadeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Louie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-M</forename><surname>Hsing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Virology</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="7628" to="7644" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Science achievement gaps by gender and race/ethnicity in elementary and middle school: Trends and predictors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cooc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Researcher</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="336" to="346" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Locally dependent latent class models with covariates: an application to under-age drinking in the USA</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Reboussin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Ip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wolfson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series A (Statistics in Society)</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="877" to="897" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Asymptotic normality and optimalities in estimation of large Gaussian graphical models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Reckase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multidimensional Item Response Theory</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009. 2015</date>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="991" to="1026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">A nonlinear mixed model framework for item response theory</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rijmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tuerlinckx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">De</forename><surname>Boeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuppens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">185</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Robitzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ünlü</surname></persName>
		</author>
		<title level="m">CDM: Cognitive Diagnosis Modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>R package version 7.5-15</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Identification in parametric models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Rothenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="577" to="591" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Single-cell RNA-seq reveals dynamic paracrine control of cellular variation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Shalek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">510</biblScope>
			<biblScope unit="issue">7505</biblScope>
			<biblScope unit="page" from="363" to="369" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Statistical inference for high-dimensional models via recursive online-score estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">535</biblScope>
			<biblScope unit="page" from="1307" to="1318" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">The lagrangian multiplier test</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Silvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="407" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Dynamic factor models, factor-augmented vector autoregressions, and structural vector autoregressions in macroeconomics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Watson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Elsevier</publisher>
			<biblScope unit="page" from="415" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Forecasting using principal components from a large number of predictors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="issue">460</biblScope>
			<biblScope unit="page" from="1167" to="1179" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Item response theory with covariates (IRT-C) assessing item recovery and differential item functioning for the threeparameter logistic model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Vermunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="42" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Measurement of psychological disorders using cognitive diagnosis models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Templin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Henson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="305" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Use of item response theory in the study of group differences in trace lines</title>
		<author>
			<persName><forename type="first">D</forename><surname>Thissen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Test validity</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Wainer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Braun</surname></persName>
		</editor>
		<meeting><address><addrLine>Mahwah, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum Associates, Inc</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="147" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Test organization, insurance firm settle bias suit</title>
		<author>
			<persName><forename type="first">T</forename><surname>Toch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
	<note>Education Week</note>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">IL6 modulates the immune status of the tumor microenvironment to facilitate metastatic colonization of colorectal cancer cells</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Toyoshima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Immunology Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Item-focussed trees for the identification of items in differential item functioning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="727" to="750" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">On asymptotically optimal confidence regions and tests for high-dimensional models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van De Geer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ritov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dezeure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1166" to="1202" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Estimating the concomitant-variable latent-class model with the EM algorithm</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G M</forename><surname>Van Der Heijden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dessens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bockenholt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational and Behavioral Statistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="215" to="229" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">A general diagnostic model applied to language testing data</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Von Davier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="287" to="307" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<title level="m">High-Dimensional Statistics: A Non-Asymptotic Viewpoint</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Using lasso and adaptive lasso to identify DIF in multidimensional 2PL models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="387" to="407" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation and inference for high dimensional generalized factor models with application to factor-augmented regressions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">229</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="180" to="200" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Confounder adjustment in multiple hypothesis testing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1863" to="1894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Asymptotics of empirical eigenstructure for high dimensional spiked covariance</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of statistics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1342" to="1374" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">The blessings of multiple causes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">528</biblScope>
			<biblScope unit="page" from="1574" to="1596" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">On the use of the lasso for instrumental variables estimation with some invalid instruments</title>
		<author>
			<persName><forename type="first">F</forename><surname>Windmeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farbmacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">527</biblScope>
			<biblScope unit="page" from="1339" to="1350" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Nested partially latent class models for dependent binary data; estimating disease etiology</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deloria-Knoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Zeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="200" to="213" />
			<date type="published" when="2017">2017</date>
			<pubPlace>Oxford, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Identifiability of restricted latent class models with binary responses</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="675" to="707" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Identifying latent structures in restricted latent class models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">523</biblScope>
			<biblScope unit="page" from="1284" to="1295" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Identifiability of diagnostic classification models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="625" to="649" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">A useful variant of the davis-kahan theorem for statisticians</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Samworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="315" to="323" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Confidence intervals for low dimensional parameters in high dimensional linear models</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="217" to="242" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Simultaneous inference for high-dimensional linear models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="757" to="768" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">On high-dimensional constrained maximum likelihood inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">529</biblScope>
			<biblScope unit="page" from="217" to="230" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Comparison of five rules for determining the number of components to retain</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Zwick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Velicer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="432" to="442" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
