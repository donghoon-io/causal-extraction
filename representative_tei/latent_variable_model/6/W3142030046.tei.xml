<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EVALUATING EFFECT, COMPOSITE, AND CAUSAL INDICATORS IN STRUCTURAL EQUATION MODELS 1</title>
				<funder ref="#_WVeBKxX">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Kenneth</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
							<email>bollen@unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Carolina Population Center</orgName>
								<orgName type="department" key="dep2">Department of Sociology</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<addrLine>Chapel Hill</addrLine>
									<postCode>27599-3210</postCode>
									<region>NC</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EVALUATING EFFECT, COMPOSITE, AND CAUSAL INDICATORS IN STRUCTURAL EQUATION MODELS 1</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T17:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Causal indicators</term>
					<term>effect indicators</term>
					<term>formative indicators</term>
					<term>reflective indicators</term>
					<term>measurement</term>
					<term>validity</term>
					<term>structural equation models</term>
					<term>scale construction</term>
					<term>composites</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although the literature on alternatives to effect indicators is growing, there has been little attention given to evaluating causal and composite (formative) indicators. This paper provides an overview of this topic by contrasting ways of assessing the validity of effect and causal indicators in structural equation models (SEMs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It also draws a distinction between composite (formative) indicators and causal indicators and argues that validity is most relevant to the latter. Sound validity assessment of indicators is dependent on having an adequate overall model fit and on the relative stability of the parameter estimates for the latent variable and indicators as they appear in different models.</head><p>If the overall fit and stability of estimates are adequate, then a researcher can assess validity using the unstandardized and standardized validity coefficients and the unique validity variance estimate. With multiple causal indicators or with effect indicators influenced by multiple latent variables, collinearity diagnostics are useful. These results are illustrated with a number of correctly and incorrectly specified hypothetical models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction 1</head><p>After years of neglect, alternative measures to effect indicators are finally receiving more attention in a variety of social and behavioral science disciplines. As consideration of causal, formative, and composite indicators becomes more common so do questions about how to analyze them. Some research on identification and estimation of causal indicator models is available (e.g., Bollen 1989, pp. 311-313,331;  Bollen and Davis 2009a; Diamantopoulos et al. 2008;  MacCallum and Browne 1993). But the validity, selection, and elimination of causal, composite, or formative indicators have received scant attention. The focus of this paper is on these issues. Some authors have broached these topics in the course of writing about causal or formative indicators (e.g., Bollen  1989, pp. 222-223; Bollen and Lennox 1991; Diamantopoulos  et al. 2008; Jarvis et al. 2003; Petter et al. 2007). One clear message from these works is that researchers should not use effect indicator selection tools such as item-total correlations or Cronbach's alpha when evaluating other types of indicators.</p><p>Another common theme is the greater importance of individual causal indicators in measuring a concept versus the interchangeability of effect indicators. For instance, Bollen  and Lennox (1991, p. 307-308) argue that we can sample effect indicators, but we need a "census" of causal indicators. Jarvis et al. (2003, p. 202) state that "dropping a causal indicator may omit a unique part of the composite latent construct and change the meaning of the variable." Although these discussions are helpful, our fields would benefit from further discussion.</p><p>The purpose of this paper is to present ideas on evaluating the validity of causal, composite, or formative indicators in measurement models. This task is facilitated by considering the selection and elimination of effect indicators in that the similarities and differences between these traditional and nontraditional indicators are illuminating. A division is also made between models whose only possible flaw is that one or more of the indicators are not a measure of the intended concept and a second set of models where more serious structural misspecifications are present. But the task is the same: evaluating the validity of indicators.</p><p>The next section presents background information to clarify my terminology, the role played by theory, a summary table of validity checks, and my position on when to eliminate indicators. Next are sections that examine these different indicators in models that are correctly specified, followed by a section on these indicators in incorrectly specified models. A concluding section summarizes the discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background Information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Terminology</head><p>Structural equation models (SEMs) and concerns about measurement cover a broad range of disciplines. The diversity of disciplines and groups writing on SEMs has resulted in a diversity of terms with sometimes the same term applied in different ways. For this reason I clarify my basic terminology and assumptions.</p><p>Concepts are the starting point in measurement. Concepts refer to ideas that have some unity or something in common. The meaning of a concept is spelled out in a theoretical definition. The dimensionality of a concept is the number of distinct components that it encompasses. Each component is analytically separate from other components, but it is not easily subdivided. The theoretical definition should make clear the number of dimensions in a concept. Each dimension is represented by a single latent variable in a SEM, so if a concept is unidimensional we need a single latent variable; if it is bidimensional, we will need two latent variables.</p><p>SEMs are sets of equations that encapsulate the relationships among the latent variables, observed variables, and error variables. Parameters such as coefficients, variances, and covariances from the same model are estimable in different ways (e.g., ML, PLS) with the different estimators having different properties. So the model and estimation method are distinct and should not be confused.</p><p>Indicators are observed variables that measure a latent variable. I use indicator and measure to mean the same thing. One primary distinction is whether the indicator is influenced by the latent variable or vice versa. Effect (or reflective) indicators depend on the latent variable. Causal indicators affect the latent variable. Blalock (1964) appears to be the first social or behavioral scientist to make the distinction between causal and effect indicators. The term formative indicator came much later than causal indicators and had a meaning more restrictive than causal indicators. Fornell and  Bookstein (1982, p. 292) defined formative indicators as when constructs are conceived as explanatory combinations of indicators (such as "population change" or "marketing mix") that are determined by a combination of variables, their indicators should be formative.</p><p>As this quote illustrates, an important difference between causal indicators and formative indicators is that formative indicators completely determine the "latent" variable. Crudely put, the R-square for the composite variable is 1.0 since it is a linear combination of the formative indicators. Causal indicators have no such restriction.</p><p>Another distinction is that causal indicators should have conceptual unity in that all the variables should correspond to the definition of the concept whereas formative indicators are largely variables that define a convenient composite variable where conceptual unity is not a requirement. That is, the composite variable with its formative indicators might just be a useful summary device for the effect of several variables on other variables (e.g., see Heise's [1972] sheaf coefficient). In other situations, researchers' use of the term formative indicators departs from its original meaning and they appear to mean the same as causal indicators.</p><p>To avoid the ambiguity of the term formative indicator, I use composite indicator to refer to an indicator that is part of a linear composite variable (Grace and Bollen 2006). Although composite indicators might share similarities, they need not have conceptual unity. In other words, I use composite indicators to refer to the original use and sometimes current use of formative indicators. Causal indicators differ from the composite indicators in that causal indicators tap a unidimensional concept (or dimension of a concept) and the latent variable that represents the concept is not completely determined by the causal indicators. That is, an error term affects the latent variable as well. As I will explain later, whether an indicator is a composite or causal indicator has implications for evaluating validity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Necessity of Theory</head><p>Theory enters measurement throughout the process. We need theory to define a concept and to pick out its dimensions. We need theory to develop or to select indicators that match the theoretical definition and its dimensions. Theory also enters in determining whether the indicator influences the latent variable or vice versa. Each relation specified between a latent variable and an indicator is a theoretical hypothesis. Empirical estimates and tests aid in assessing whether the set of relations are consistent with the data. However, empirical consistency between the data and model alone is insufficient to support the validity of measures. There must be theory behind the relations to make the validity argument credible.</p><p>On the other hand, theory might suggest that an indicator is valid, but the empirical estimates linking the latent variable and indicator might be weak to zero. If this occurs in a wellfitting model, these results cast doubt on the validity of the measure even if theory suggests its validity. The burden would be on the researcher to explain why a valid measure is not related to its latent variable. It could be that the measure is valid, but that the model is misspecified in a way that leads to low values of the validity statistics. This explanation is most convincing if the researcher is specific on the nature of the misspecifications and why these problems would lead to low validity estimates. Although I will not repeatedly discuss this in the remaining sections, I assume throughout that there is measurement theory to support the models and the relations between the latent variables and indicators within them. The empirical estimation and tests are intended to evaluate these ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Validity Checks</head><p>I define the validity of a measure as follows: "The validity of a measure x i of ξ i is the magnitude of the direct structural relation between ξ i and x i " (Bollen 1989, p. 197). Causal indicators occur when the observed indicators have a direct effect on the latent variable whereas effect indicators are indicators where the latent variable has a direct effect on the indicator.</p><p>Table <ref type="table" target="#tab_0">1</ref> gives a summary of the validity assessment that I would recommend (Bollen 1989, pp. 197-206). The first step is to compare the definition of the concept (or dimension of the concept) that is the object of measurement to the indicator and to check whether the indicator corresponds to this definition. This is a type of face validity check in that the researcher assesses whether it is sensible to view the indicator as matching the idea embodied by the concept.</p><p>The second check in Table <ref type="table" target="#tab_0">1</ref> is to examine the overall fit of the model. Assuming that the researcher is using an estimator such as ML that provides a test of whether the model adequately reproduces the variances, covariances, and means of the observed variable, a chi-square test statistic and various fit indexes are available to help in assessing model fit. The fit indexes to choose and the weight to give to them is subject to controversy and this is an issue that cannot be easily resolved. However, the researcher will need to make the judgment as to whether model fit is adequate enough to use the other validity checks that are specific to the individual indicators.</p><p>Another aspect of assessing measurement validity is having the factor and measures embedded in a fuller model with additional causes and consequences of the main latent variable. Part of assessing validity is determining whether a measure and latent variable relate to other distinct latent variables as expected. Sometimes this is called external validity. For instance, there often are variables thought to be determinants or consequences of a specific latent variable. If I estimate a model with this full set of relationships and I find the expected effects, then this is further support for the validity of the latent variable and its measures. Failure to find such relationships undermines validity even if the preceding validity measures are satisfactory. This idea is closely related to construct validity where part of the validity assessment is done by evaluating if a construct relates to other constructs as predicted by theory or substantive considerations. When such variables are available, they enhance the validation process.</p><p>If the researcher is satisfied with model fit, the remaining validity checks bear examination. Using the definition of validity from the previous section, the factor loadings (λ) for effect indicators and the regression coefficients (γ) for causal indicators provide the unstandardized validity coefficient.</p><p>Here the researcher makes sure that the coefficients have the correct sign and looks for their statistical and substantive significance. The standardized validity coefficients are treated similarly except that these coefficients give the expected standard deviation shift in dependent variable for a one standard deviation shift in the other variable net of other influences on the dependent variable.</p><p>The unique validity variance in the last row gives the variance in the dependent variable uniquely attributable to a variable. If we have an effect indicator, it gives the explained variance uniquely attributable to the latent variable that it measures. If we have a causal indicator, it gives the uniquely attributable variance in the latent variable due to the causal indicator. When a single latent variable influences the effect indicator, the unique validity variance is the same as the usual R² for an indicator. Typically there are two or more causal indicators, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indicator Elimination</head><p>Viewpoints on when an indicator should be eliminated vary with one extreme answering "never" and the other suggesting whenever an indicator is found inadequate. I would place myself between these two endpoints. If a definition of a concept (or a dimension of it) leads a researcher to include an indicator as a measure of it, this is strong a priori evidence to keep it. On the other hand, if repeated empirical tests find a statistically and substantively weak relation of the indicator to its purported latent variable, then it is difficult to maintain the validity of the indicator. The distinction between causal and effect indicators has relevance here. Effect indicators of the same latent variable that have the same explained variance are in some sense interchangeable. Although more indicators are generally better than fewer, which indicators are used is less important.</p><p>With causal indicators, removal of an indicator can be more consequential. The causal indicators help to determine the latent variable, so that removal of a causal indicator might change the nature of the latent variable. But here also if the coefficient of the causal indicator is not significant or is the wrong sign, it is possible that you have an invalid causal indicator. Decisions on whether to eliminate indicators must be made taking account of the theoretical appropriateness of the indicator and its empirical performance in the researcher's and the studies of others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indicators in Structurally Correct Models</head><p>We can analytically separate the task of indicator evaluation under two broad conditions. The first condition is when a researcher has correctly identified the dimensionality and the latent variables that represent a concept. Furthermore, most if not all of the indicators selected to measure the concept are correct in that there is a direct relationship between the indicator and the latent variable that represents the concept. The goal is to find which indicators do not adequately measure the concept. I refer to these as structurally correct models in that these are essentially valid models where the only possible flaw is that some indicators might have low to near zero validity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Five Effect Indicators, One Latent Variable</head><p>The equations of the measurement model 2 on which I focus are 2 These equations describe the hypothesized relation between the indicators and their latent variable without assuming that any particular estimator (e.g., ML or PLS) is being used.</p><formula xml:id="formula_0">y 1 = α 1 + λ 1 η + g 1 y 2 = α 2 + λ 2 η + g 2 y 3 = α 3 + λ 3 η + g 3</formula><p>(1)</p><formula xml:id="formula_1">y 4 = α 4 + λ 4 η + g 4 y 5 = α 5 + λ 5 η + g 5</formula><p>where y j is the j th effect indicator of η, α j is the intercept in the measurement equation, λ j is the factor loading or regression coefficient of the effect of the latent factor on y j , η is the factor or latent variable that we hope to measure, and g j is the unique factor that contains all other influences on y j besides η. I assume that E(g j ) = 0, COV (g j , g j ) = 0, and that COV(g j , η) = 0 for i, j = 1, 2, 3,4,5 and i … j. This is equivalent to a single factor model with five indicators and uncorrelated unique factors (errors). <ref type="bibr">3</ref> The path diagram is in Figure <ref type="figure" target="#fig_1">1</ref>.</p><p>Suppose that I am not sure that all five measures are valid.</p><p>How can I examine their validity and discard any that are not valid? Assume that I am certain of the validity of at least one of these measures, say y 1 , so I scale the latent η to this indicator by setting λ 1 = 1 and α 1 = 0 leading to y 1 = η + g 1 .</p><p>Scaling the latent variable is sufficient to identify this model; that is, it is possible to find unique values for the remaining intercepts, factor loadings, and other parameters in this model. <ref type="bibr">4</ref> (Without scaling the latent variable, the model is underidentified.)</p><p>Referring to Table <ref type="table" target="#tab_0">1</ref>, the first check for the remaining indicators is whether they correspond to the definition of the concept that is represented by η. If they do not, then this is sufficient grounds to eliminate an indicator. Assuming that they do match the definition, then the next step is to assess whether the model represented in equation ( <ref type="formula">1</ref>) fits the data. This requires that we estimate the model with a procedure that provides overall fit statistics. For instance, if the ML estimator is appropriate, then a likelihood ratio chi-square test is available to compare the hypothesized model to a saturated model. Adjustments to this chi-square test are readily available when the data exhibit significant nonnormality. There also are additional fit indexes that can supplement the chisquare test. If the fit is not adequate, then the researcher will need to find alternative specifications that might be better suited to the data. If the fit is judged to be good, then the other means of assessing validity are turned to next.</p><p>Employing a consistent estimator (e.g., maximum likelihood, two-stage least squares) of the parameters in this model, I can begin to assess the validity of the indicators. <ref type="bibr">5</ref> The factor loadings ( ) for effect indicators provide the unstandardized  λ validity coefficient. Here the researcher makes sure that the coefficients have the correct sign and looks for their statistical and substantive significance. It provides the expected difference in the indicator (y j ) given a one unit difference in the latent variable (η). If a measure is valid, then I expect that the loading will be large in a substantive sense and that its estimate is statistically significant as well. Invalidity would be suggested by the opposite: a substantively small or statistically insignificant factor loading. For instance, suppose that the scaling indicator variable, y 1 , has a range from 0 to 100 as does y 2 . If y 2 has a coefficient (λ 2 ) of 0.1, then its validity appears much less than y 1 whose coefficient is 1.0.</p><p>Closely related is the standardized validity coefficient which gives the expected number of standard deviations that the indicator (y j ) differs for a one standard deviation difference in the latent variable (η). The unique validity variance provides the variance in the measure that is uniquely attributable to a specific latent variable. Assuming that only η directly affects y j , the unique validity variance is equivalent to the R-square for the indicator (y j ). This in turn is the amount of variation in the indicator explained by the latent variable. Finally, the collinearity analysis is not relevant for this example because only a single latent variable affects the measure so there is no correlation of the latent variable with anything else. This would be an issue if, say, there were two correlated latent variables influencing the same measure.</p><p>Another aspect of assessing measurement validity is having the factor and measures embedded in a fuller model with additional causes and consequences of η. In this hypothetical example and sometimes in practice, we do not have additional latent variables for external validity checks but this would be possible if this measurement structure were embedded in a larger model.</p><p>Returning to the primary task of indicator selection in the effect indicator model in Figure <ref type="figure" target="#fig_1">1</ref>, I can now provide some guidance. First, each indicator should have a sufficiently values of the model as the sample size goes to infinity. A related concept is asymptotically unbiased. Asymptotically unbiased means that in large samples the expected value of the estimator is the population parameter. The usual maximum likelihood estimator that I use in this paper has these two properties. Partial least squares (PLS)-sometimes called a component-based SEM estimator-does not, so this makes it difficult to use PLS to make validity assessments. That is, if you have biased estimates of parameters in the model, using these to guide validity assessments could be misleading. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Five Causal Indicators, One Latent Variable</head><p>In this section I move from effect indicators to causal indicators. The equation corresponding to causal indicators is</p><formula xml:id="formula_2">η = α η + γ η1 x 1 + γ η2 x 2 + γ η3 x 3 + γ η4 x 4 + γ η5 x 5 + ζ (2)</formula><p>where α η is the intercept, x j is a causal indicator, γ ηj is the coefficient of x j and its effect on η, and ζ is the error or disturbance with E(ζ) = 0 and COV(x j , ζ) = 0 for j = 1 to 5. Figure <ref type="figure">2</ref> is a path diagram of the equation.</p><p>The presence of the error (ζ) in equation ( <ref type="formula">2</ref>) is in recognition that other variables not in the model are likely to determine the values of the latent variable. It seems unlikely that we could identify and measure all of the variables that go into the latent variable and the error collects together all of those omitted causes.</p><p>An important difference from the effect indicator model in equation ( <ref type="formula">1</ref>) is that the causal indicator model in ( <ref type="formula">2</ref>) is not identified. This means that without additional information I cannot find unique values for the parameters in the model and hence cannot assess indicator validity.</p><p>To overcome this obstacle I assume that I have two effect indicators added to this model. Now instead of equation (2) alone, I also have</p><formula xml:id="formula_3">y 1 = α 1 + λ 1 η + g 1 (3) y 2 = α 2 + λ 2 η + g 2</formula><p>with the same assumptions made as I did for equation (1). These two effect indicators are sufficient to identify the causal indicator model once I scale the latent variable (e.g., λ 1 = 1, α 1 = 0). I now turn to assessing the validity of the causal indicators. <ref type="bibr">7</ref> Referring to Table 1 again, our starting point is the same as with effect indicators: Do the causal indicators correspond to the definition of the concept? The causal indicators should be initially chosen for their face validity in that on the face of it they seem to tap the concept (or dimension of a concept) that is being measured. If not, then they should not be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2. Five Causal Indicators, One Latent Variable</head><p>Our second validity criterion is to check the overall fit of the model. The model as specified is overidentified, so using an estimator like ML we can test its fit compared to the saturated model and supplement this with other fit indexes. A poor fit means we need to stop to determine the source of the problem before going further. A good fit suggests that we move on to the other validity checks.</p><p>The definition of validity given earlier refers to the direct structural relation between the measure and the latent variable. In the causal indicator model, the unstandardized validity coefficient is γ ηj and the standardized validity coefficient is its standardized counterpart 8 and these provide evidence relevant to the validity of the causal indicators. A researcher should examine the sign, magnitude, and statistical significance of these coefficients. In addition, the unique validity variance reveals the incremental variance explained by each causal indicator. A valid causal indicator should have a strong effect on η as gauged by these standards.</p><p>It is impossible to give exact numerical values for cutoff for each of these validity measures since the minimum validity acceptable would depend on several factors such as the findings from prior studies, the degree to which collinearity prevents crisp estimates of effects, the difficulty of measuring the latent variable, etc. However, a coefficient of a causal indicator with the wrong sign or that is not statistically significant would appear to be invalid and a candidate for exclusion. Alternatively, a causal indicator with a correct sign and statistically significant coefficient with an increase of the squared multiple correlation by 10 percent or more has support for its validity.</p><p>Before eliminating a causal indicator, a researcher must consider whether the allegedly invalid causal indicator suffers from high multicollinearity with the other causal indicators. Since η has more than one determinant, our validity assessment is more complicated than the previous effect indicator example in that we need to assess the impact of each x j separately from the other causal indicators. If x j were uncorrelated (or nearly so) with all other causal indicators, then the task would be easier in that I would not attribute to one causal indicator the effects of another. But when the causal indicators are highly correlated, there is the potential for the wellknown multiple regression problem of multicollinearity: An explanatory variable (a causal indicator) is highly correlated with the other explanatory variables (causal indicators) and it is more difficult to estimate its unique effects. This difficulty is reflected in larger standard errors than if there were lower collinearity and these larger standard errors imply less precise (more variable) coefficient estimates and validity coefficients. In the situation of high multicollinearity, a competing explanation for low unstandardized and standardized validity coefficients and low unique validity variance is that the causal indicator was so highly associated with the other causal indicators that estimates of distinct impact were not possible.</p><p>One factor that makes this collinearity issue less worrisome for causal indicators is that such indicators need not correlate (Bollen 1984). This means in general we would not expect high degrees of collinearity among causal indicators, although it is possible. Fortunately, there are regression diagnostics available to assess when collinearity is severe or mild (e.g., Belsley et al. 1980) and many of these are adaptable to the causal indicator model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Are effect indicators immune from the collinearity problem?</head><p>The brief answer is "no." We did not face this issue in equation (1) because each effect indicator was only influenced by a single latent variable. But sometimes effect indicators are depended on by two or more latent variables and if these 8 More specifically, the standardized validity coefficient is .</p><formula xml:id="formula_4">( ) ( ) γ η η j s d x s d j . . . .      </formula><p>latent variables are highly correlated, the same collinearity issue might emerge. If an effect indicator has factor complexity of one, then the collinearity issue is absent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Five Composite Indicators, One Composite</head><p>The distinction between composite (formative) and causal indicators is subtle. I represent the composite indicator model for five indicators below:</p><formula xml:id="formula_5">C = α C + γ C1 x 1 + γ C2 x 2 + γ C3 x 3 + γ C4 x 4 + γ C5 x 5 (4)</formula><p>This equation shares with equation ( <ref type="formula">2</ref>) that the indicators (x's) help set the value of the left-hand side variable, but the differences from causal indicators are more important than this similarity. For one thing, the equation with composite indicators has no error (or disturbance) term since the composite indicators completely determine the composite. The composite variable C rather than the latent variable η is on the left hand side of the equation to reflect that we are dealing with a linear composite variable (C) rather than a latent variable (η).</p><p>At least as important, but less obvious, is that composite indicators need not share unidimensional conceptual unity. That is, composite indicators might be combined into a composite as a way to conveniently summarize the effect of several variables that do not tap the same concept although they may share a similar "theme." For instance, age, sex, gender, and race effects could be summarized in a demographic composite even though in this context demographic is not seen as a unidimensional latent variable. Indeed, another researcher might have a different set of demographic variables to form a demographic composite.</p><p>A troubling aspect of composite indicators not sharing unidimensional conceptual unity is whether it even makes sense to discuss the validity of these variables. Using either the preceding definition of validity (The validity of a measure x i of ξ j is the magnitude of the direct structural relation between ξ j and x i ) or the less formal definition of whether a variable measures what it is suppose to measure, if the composite indicators are just convenient ingredients to form a linear composite variable, it is not clear that assessments of validity apply. In other words, if we have no unidimensional theoretical concept in mind, how can we ask whether the composite indicators measure it? With composite indicators and a composite variable, the goal might not be to measure a scientific concept, but more to have a summary of the effects of several variables.</p><p>Even if the unidimensional concept criterion is satisfied by the compositive indicators, it seems unlikely that there are many situations where an error term would be absent as in (4). This would mean that the latent variable that represents the unidimensional concept is an exact linear function of its indicators, which would seem to be a rarity.<ref type="foot" target="#foot_6">foot_6</ref> Given that the concept of validity makes little sense for arbitrary composites formed from composite indicators, I will not discuss the validity measures for composite indicators.</p><p>Many researchers gloss over the distinction between composite indicators and causal indicators that I make here. <ref type="bibr">10</ref> This distinction likely underlies some of the controversies in this area. For instance, if composite indicators form a composite without having unidimensional conceptual unity, then their coefficients could well be unstable as the composite is allowed to influence different outcome variables. Returning to the demographic composite, we would not expect the effects of age, race, and gender to be the same regardless of the outcome variable. On the other hand, if these are causal indicators intended to tap a single latent variable, then the coefficients of the causal indicators should be stable across different outcomes. This could explain some of the differences expressed in Howell et al. (2007) versus Bollen (2007) where the former is referring to composite (formative) indicators and the latter refers to causal indicators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of Structurally Correct Models</head><p>When we have "clean" models in which the researcher has essentially the correct specification and the only question is whether one or more of either the effect or causal indicators are valid, the task of indicator assessment is simplified. We can use the regression coefficients from the causal indicator to the latent variable (as in ( <ref type="formula">2</ref>)) or from the latent variable to the effect indicator (as in ( <ref type="formula">1</ref>)) to form the unstandardized and standardized validity coefficients and test their statistical significance to assess measurement validity. And we can use the unique validity variance as another measure of an indicator's validity. Low or statistically insignificant values on these measures are symptoms of low validity and suggest potential removal of such measures. Moderate to high values reinforce their validity. Exact cutoff values for indicators do not exist since their performance must be assessed in the context of what prior research has found under similar conditions.</p><p>One complication with causal indicators is that we cannot identify or estimate any of the validity measures in a model when only causal indicators are present. The latent variable with causal indicators must emit at least one path to identify and estimate the unstandardized validity coefficients and it must emit at least two paths to identify and estimate the standardized validity measures (Bollen and Davis 2009b).</p><p>The main remaining difficulty in indicator selection occurs with causal indicators or with effect indicators that depend on more than one collinear latent variable. In either of these cases, high collinearity might make it difficult to obtain sharp estimates of the distinct effects of the explanatory variables.</p><p>In practice, causal indicators are often not highly collinear and effect indicators often depend on one latent variable, so the collinearity issue is less prevalent. Finally, composite indicators as an arbitrary linear combination of indicators are not subject to validity analysis in that the composite need not represent a unidimensional concept so the question of validity is not appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indicators in Structurally Misspecified Models</head><p>The discussion to this point has focused on models that are structurally correctly specified. Real world modeling is more complex and makes the task of indicator selection more complicated. I turn now to some of these issues. As before, I treat effect indicator models first and then turn to causal indicator models to facilitate understanding the similarities and differences between these models. The effect indicators are included to contrast the similarities and differences between causal and effect indicators. For the reasons given in the last subsection, I do not discuss composite indicators further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Five Effect Indicators, Two Latent Variables</head><p>Suppose we once again have five effect indicators, but now have two latent variables in the true model. The equations are</p><formula xml:id="formula_6">y 1 = η 1 + g 1 y 2 = α 2 + λ 21 η 1 + g 2 y 3 = α 3 + λ 31 η 1 + λ 32 η 2 + g 3</formula><p>(5)</p><formula xml:id="formula_7">y 4 = α 4 + λ 42 η 2 + g 4 y 5 = η 2 + g 5</formula><p>where I make the usual assumptions that the unique factors (g j ) have means of zero and are uncorrelated with the latent variables and with each other. Figure <ref type="figure">3a</ref> is a path diagram of this model. In practice, researchers do not know the true generating model and are likely to make specification mistakes. Figures 3b to 3d are three examples of misspecified models. In Figure <ref type="figure">3b</ref>, I mistakenly assume a single factor for all five indicators; Figure <ref type="figure">3c</ref> also assumes a single factor but only uses y 1 , y 3 , and y 5 as indicators; and Figure <ref type="figure">3d</ref> has a single factor but takes y 1 , y 2 , and y 3 as the only indicators.</p><p>Simulating data that conforms to Figure <ref type="figure">3a</ref> (λ 11 = 1, λ 21 = .8, λ 31 = .6, λ 32 = .5, λ 42 = .8,</p><formula xml:id="formula_8">λ 52 = 1, α 1 = 0, V(g j ) = 1, V(η k ) = 1, COV(η 1 , η 2 ) = .</formula><p>6, N = 1000), I estimated the factor loadings and R-squares for the models that matched Figures 3a to 3d using the ML estimator. These results are reported in Table <ref type="table">2</ref>. The first columns with the results for the true model in Figure <ref type="figure">3a</ref> show estimates that are within two standard errors of the population values. The low chi-square and high pvalue and the other fit indices consistently point toward a good fitting model. I could interpret the validity measures for this correct version of the model as I illustrated with the other correct models in the first part of the paper, but will not do so.</p><p>Rather I examine what happens in the incorrect models.</p><p>Figure <ref type="figure">3b</ref> mistakenly has one factor rather than two. All the factor loadings remain statistically and substantively significant, which might lead a researcher to view these as valid measures. But we know that y 4 and y 5 are invalid measures of the first factor (η 1 ) and that y 3 measures both η 2 and η 1 , although the model only permits η 1 to have an impact.</p><p>In applied settings we are not privileged to the knowledge of the true model, so how can we know that there is a validity issue with some of the measures in Figure <ref type="figure">3b</ref>? This is where the previous discussion about overall model fit is relevant.</p><p>The chi-square of 70.8 with 5 df is highly statistically significant for the model in Figure <ref type="figure">3b</ref>. The Relative Noncentrality Index (RNI) and Incremental Fit Index (IFI) pass conventional standards, but the Tucker-Lewis Index (TLI) and Bayesian Information Criterion (BIC) point toward an unacceptable fit. <ref type="bibr">11</ref> The overall poor fit for some of the measures serves as a cautionary note about structural misspecifications and the misspecification creates uncertainty about the accuracy of the validity statistics.</p><p>The models in Figures <ref type="figure">3c</ref> and<ref type="figure">3d</ref> provide additional signs of trouble and potential invalidity. If these five indicators truly measured a single factor, then I would expect that fitting a single factor to a subset of the five indicators would lead to the same factor loadings and other parameter estimates for any parameters shared between the models. Any differences in factor loadings for the same variable should be due to just sampling fluctuations. In Figure <ref type="figure">3c</ref>, I keep the assumption of a single factor, but only use y 1 , y 3 , and y 5 as indicators. Figure <ref type="figure">3d</ref> also is a single factor, but I only use y 1 , y 2 , and y 3 . Contrasting the estimates from the models that correspond to Figure <ref type="figure">3c</ref> and 3d reveals a large difference in the factor loading estimates for y 3 (1.418 versus 0.897). Furthermore the R-squares for y 1 go from 0.30 to 0.47 and from 0.65 to 0.41 for y 3 . Finding differences this large raises questions about the model specification and cast doubt on any estimates of validity that use these model estimates as a basis. This illustrates that using different indicators in the model can provide evidence of an invalid structure and should deter the analyst from using the estimates to make validity assessments.</p><p>To minimize errors in indicator selection when working with effect indicators, a researcher should have a model with good overall fit and one in which the coefficients do not exhibit large changes when effect indicators of the same latent variable are added or subtracted from the model. Our misspecified models in Figures 3b to 3d did not exhibit these characteristics and the result was that at least some of the validity estimates were seriously misleading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Five Causal Indicators, Two Latent Variables</head><p>In this subsection, I have a model with five causal indicators that influence two latent variables according to the following equations:</p><formula xml:id="formula_9">η 1 = α η + γ η1 x 1 + γ η2 x 2 + γ η3 x 3 + ζ 1 (6) η 2 = α η + γ η1 x 3 + γ η4 x 4 + γ η5 x 5 + ζ 2</formula><p>where the errors (ζ's) have a mean of zero and are uncorrelated with the causal indicators (x's) and with each other. Note that is the only causal indicator to influence both η's. The model is set up to parallel the situation of effect indicators, which are linked to two different latent variables. With no effect indicators both equations are underidentified. Suppose that I have one effect indicator for each latent variable <ref type="formula">7</ref>)</p><formula xml:id="formula_10">y 1 = η 1 + g 1 (</formula><formula xml:id="formula_11">y 2 = η 2 + g 2</formula><p>where the unique factors (g's) have means of zero and are uncorrelated with the latent variables (η's), but correlated with each other. Figure <ref type="figure">4a</ref> is a path diagram of this model. Not knowing the true model, in practice researchers are likely to formulate models with at least some errors. Figure <ref type="figure">4b</ref> is an example where a researcher incorrectly assumes a single latent variable (η) for the five causal indicators and that the latent variable has two effect indicators, y 1 and y 2 . Figure <ref type="figure">4c</ref> is a model where once again a single latent variable is incorrectly used, but only one effect indicator (y 1 ) is available. Figure <ref type="figure">4d</ref> is the same setup except that only y 2 is available as an effect indicator.</p><p>Simulating data for the model in Figure <ref type="figure">4a</ref> (λ 11 = 1, λ 22 = 1, γ 11 = .5, γ 12 = .7, γ 13 = .2, γ 23 = .3, γ 25 = .7, α j = 0, V(g j ) = 1, COV(g 1 , g 2 ) = .6, V(ζ k ) = 1, N = 1000), I estimated each of the models in Figures 4a to 4d using the ML estimator. The results are reported in Table <ref type="table">3</ref>. The model in Figure <ref type="figure">4a</ref> is not identified. More specifically, we cannot separately identify the error variances for the η's and the error variances of the g's. Had I two effect indicators each for η 1 and η 2 , then this would not be necessary since the original model would be identified.<ref type="foot" target="#foot_9">foot_9</ref> But to illustrate how to proceed in the situation of a single effect indicator for each latent variable, I do not assume additional effect indicators. In Figure <ref type="figure">4a</ref> the coefficients of the causal indicators are identified and I can estimate them by using the partially reduced form model as described in Bollen and Davis (2009a). <ref type="bibr">13</ref> A similar situation holds for the models in Figures <ref type="figure">4c</ref> and<ref type="figure">4d</ref> and I also estimate these using the partially reduced form versions of these models.</p><p>The column of Table <ref type="table">3</ref> that corresponds to Figure <ref type="figure">4a</ref> shows that the estimates of the identified parameters are all within +/-2 times the standard errors of the true parameters. In addition, the likelihood ratio chi-square and the fit indices all indicate an excellent fitting model. Since the γ's are identified and estimated, I have the unstandardized validity coefficients to gauge the validity of each causal indicator as a measure of either η 1 or η 2 . The low to moderate association among the causal indicators does not create collinearity problems that prevent reasonable estimates of validity. The statistical significance and magnitude of coefficients reveal that x 1 to x 2 are valid measures of η 1 . Similarly, x 3 to x 4 are valid measures of η 2 . However, the lack of identification of the variances of ζ's (error variances of the η's) means that I cannot estimate the standardized validity coefficients for the causal indicators. (I could estimate the incremental validity variance by comparing the explained variance with and without each causal indicator.) Fortunately, the results for the coefficients are sufficient to highlight the validity of the causal indicators. Furthermore, if I modified the model to allow all five causal indicators to influence both η 1 and η 2 , then I also could establish that x 4 and x 5 are invalid measures of η 1 and that x 1 and x 2 are invalid measures of η 2 .  <ref type="table">3</ref> that corresponds to it shows all causal indicators with statistically significant coefficients. Their nonnegligible magnitude suggests that these measures are valid. Indeed, the R-square for η* suggests that these causal indicators collectively explain nearly 90 percent of the variance in the latent variable. It is only when I note the large and statistically significant chi-square and other fit indices that doubt is cast on the validity assessment. The model's fit is not good. This raises questions about the validity analysis since structural misspecifications in the model could distort these estimates. A comparison of the results for Figure <ref type="figure">4b</ref> to those for the true Figure <ref type="figure">4a</ref> model reveals the problem of incorrectly assuming a single latent variable when two are present.</p><p>Figures <ref type="figure">4c</ref> and<ref type="figure">4d</ref> represent a case where a researcher might have different single effect indicators with which to estimate a model. Table <ref type="table">3</ref> has columns that represent the results of estimating these models. Interestingly, the estimates that correspond to the model in Figure <ref type="figure">4c</ref> correspond to estimates of the impact of these causal indicators on η 1 while the estimates that correspond to the model in Figure <ref type="figure">4d</ref> are the estimates of the impact of these same causal indicators but on η12 . This implies that as long as a researcher recognizes that the effect indicators (y 1 and y 2 ) correspond to different latent variables, then it is possible to estimate the unstandardized validity coefficients and determine which causal indicator is valid for which latent variable. I tested the single latent variable model in the column that corresponds to Figure <ref type="figure">4b</ref>. I found a poor fit as is expected given that the effect indicators actually measure two different latent variables. A good fit would be more likely if they did tap the same latent variable. Some researchers have been puzzled by estimates of the impact of causal indicators that differ depending on the effect indicator (or other outcome variable) in the analysis. They have interpreted this as suggesting that this is a special problem with causal indicators. However, this is more of a problem with indicators not measuring the same latent variable. An analogous problem was found with effect indicators in the models represented in Figure <ref type="figure">3b</ref> to 3d. In Figure <ref type="figure">3b</ref>, I mistakenly assumed that all five effect indicators measure a single latent variable and the factor loadings differ from the true model with two factors. In Figures <ref type="figure">3c</ref> and<ref type="figure">3d</ref>, the factor loadings differ depending on which three effect indicators are used, a symptom of the incorrect number of latent variables, not of an inherent problem with effect indicators. So assuming a single latent variable when there are more and having indicators that tap different latent variables can lead to different assessments of validity whether a researcher is evaluating causal or effect indicators. For a discussion of this issue, see Howell et al. (2007), Bagozzi (2007), and Bollen  (2007) for contrasting views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Indicator selection and elimination begins with clear measurement theory that defines a concept, its dimensions, and the necessary latent variables to represent them. The definitions also guide the selection of indicators and specifying whether the measures are causal or effect indicators. <ref type="bibr">14</ref> Once these relations are specified in SEM, a researcher can begin to assess the validity of indicators. An important step in the empirical analysis is determining whether the measurement model is an adequate description of the data using statistics such as the likelihood ratio chi-square test that compares the model fit to a saturated model. Additional fit indices often supplement the chi-square test. In addition, if the estimates are relatively stable using different effect indicators for the same latent variable, this is supportive of the model specification. We would not expect such stability when using a subset of causal indicators. Embedding this measurement structure in a larger model and checking whether the coefficients change beyond sampling fluctuations is another check on validity. If the results differ, this raises doubt about the specification.</p><p>If the fit of the model is adequate, then the researcher can examine the unstandardized and standardized validity coefficients and the unique validity variance measures to gauge the degree of validity. The analyst needs to consider the substantive and statistical significance of these estimates. For causal indicators or effect indicators that depend on two or more latent variables, it would be wise to perform collinearity diagnostics. All of these statistics provide information on the validity of the indicators. Indicators with low validity coefficients and low unique validity variance (in the absence of high collinearity) are ones that are candidates for elimination even if theory initially supported their use. This is true whether they are causal or effect indicators. On the other hand, statistically significant and substantively large unstandardized and standardized validity coefficients with moderate to high unique validity variance support the validity of an indicator.</p><p>What makes these assessments most difficult is deciding whether the model as a whole is adequate for the data. In many SEMs with large samples there is considerable statistical power to detect even minor mistakes in the model specification. In practice, there will be ambiguity in assessing overall model fit. This in turn leaves open the possibility of a structural misspecification that is sufficient to alter the parameter estimates vital to assessing indicator validity. Thus we must recognize and be willing to live with the fact that mistakes are possible where an invalid indicator is retained or a valid one discarded. It is this fact that reinforces the need for a theoretical basis to back up our specifications and empirical tests.</p><p>Finally, if composite (formative) indicators are combined to be a convenient summary of a set of variables, then the issue of validity is not present since there is not a claim that these indicators share conceptual unity. With the composite (formative) indicators, there is no expectation of stable coefficients across different dependent variables that the composite might predict. This distinction between causal indicators and composite (formative) indicators is not typically made and when ignored can contribute to controversy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>It will help our understanding of the validity of causal indicators if we first present the better-known effect indicator model and then contrast it with causal indicators. To make the discussion more concrete, I consider five indicators and one latent variable. In one situation I have effect indicators, in another I have causal indicators, and finally I have composite indicators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Five Effect Indicators, One Latent Variable</figDesc><graphic coords="6,237.66,86.04,136.86,119.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure</head><label></label><figDesc>Figure 4a is the true model and it is reassuring to have accurate estimates of the validity of the causal indicators. But</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,144.00,86.04,324.06,247.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,144.00,86.04,324.06,237.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Validity Assessment for Causal and Effect Indicators Validity Check What to Check What to Look For</head><label>1</label><figDesc></figDesc><table><row><cell>Definition of Concept</cell><cell>Indicator</cell><cell>Indicator corresponds to definition</cell></row><row><cell>Overall Model Fit</cell><cell>Chi-square and fit indexes</cell><cell>High p-value and acceptable fit indexes</cell></row><row><cell>External Validity</cell><cell>Relation to other latent variables</cell><cell>Relations consistent with theory</cell></row><row><cell>Unstandardized Validity Coefficients</cell><cell>Coefficients (λ, γ)</cell><cell>Correct sign, statistical and substantive</cell></row><row><cell></cell><cell></cell><cell>significance</cell></row><row><cell>Standardized Validity Coefficients</cell><cell>Coefficients (standardized λ, γ)</cell><cell>Correct sign, statistical and substantive</cell></row><row><cell></cell><cell></cell><cell>significance</cell></row><row><cell>Unique Validity Variance</cell><cell>Incremental R²</cell><cell>Explained variance added</cell></row></table><note><p><p>so in this situation the unique validity variance is the additional explained variance due to a specific causal indicator once all other variables enter the latent variable equation.</p>I will say more about these validity checks in the other sections.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Detmar Straub was the accepting senior editor for this paper. Thomas Stafford served as the associate editor.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Here, as with the causal indicator and composite indicator model, we can estimate a measurement model and need not form indexes prior to analysis.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>If the scaling indicator has zero validity, I would expect estimation problems and even if estimates are obtained, the R-square for this indicator would be near zero.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>A consistent estimator is an estimator that converges on the true parameter</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>This is a point where the estimator employed is relevant in that some estimators are less sensitive to structural misspecifications than others. For example, the ML estimator is generally more sensitive to structural misspecifications than the latent variable two stage least squares estimator.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>The model becomes a MIMIC model<ref type="bibr" target="#b16">(Jöreskog and Goldberger 1975)</ref> when these two effect indicators are added.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>Linear identities might be present where this formative formulation makes sense. For instance, Profit = (Sales Price -Cost) or Population Size = Births -Deaths +/-Net Migration are identities that require no error term assuming accurate measures.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7"><p>My earlier writings have not made this distinction clear.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8"><p>Other fit indexes could be used, but in my experience these represent a useful variety of measures that perform well.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_9"><p>This assumes that the errors of the new indicators are uncorrelated.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_10"><p>This strategy leads the variances of the errors in the y's to be the sum of the variances of the equation and indicator disturbances (e.g., VAR(g 1 + ζ 1 )).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_11"><p>The study by<ref type="bibr" target="#b9">Bollen and Ting (2000)</ref> has an empirical test to help determine whether a subset or all indicators are causal or effect indicators.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>I gratefully acknowledge the support of <rs type="funder">NSF</rs> <rs type="grantNumber">SES 0617276</rs>. My thanks to <rs type="person">Shawn Bauldry</rs> for research assistance and to the reviewers and editors for helpful comments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_WVeBKxX">
					<idno type="grant-number">SES 0617276</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Copyright of MIS Quarterly is the property of MIS Quarterly &amp; The Society for Information Management and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.</p><p>View publication stats View publication stats</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the Meaning of Formative Measurement and How it Differs from Reflective Measurement</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Bagozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="229" to="237" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Belsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Welch</surname></persName>
		</author>
		<title level="m">Regression Diagnostics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Blalock</surname></persName>
		</author>
		<title level="m">Causal Inferences in Nonexperimental Research</title>
		<meeting><address><addrLine>Chapel Hill, NC</addrLine></address></meeting>
		<imprint>
			<publisher>University of North Carolina Press</publisher>
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiple Indicators: Internal Consistency or No Necessary Relationship</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantity and Quality</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="377" to="385" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Structural Equations with Latent Variables</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interpretational Confounding Is Due to Misspecification, Not to Type of Indicator: Comment on Howell, Breivik, and Wilcox</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="228" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Causal Indicator Models: Identification, Estimation, and Testing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="498" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Two Rules of Identification for Structural Equation Models</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="523" to="536" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conventional Wisdom on Measurement: A Structural Equation Perspective</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lennox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="305" to="314" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Tetrad Test for Causal Indicators</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="22" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Advancing Formative Measurement Models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Diamantopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Business Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1203" to="1218" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Comparative Analysis of Two Structural Equation Models: LISREL and PLS Applied to Market Data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fornell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Bookstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Second Generation of Multivariate Analysis</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Fornell</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Praeger</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="289" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Representing General Theoretical Concepts in Structural Equation Models: The Role of Composite Variables</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Grace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmental and Ecological Statistics</title>
		<imprint>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="191" to="213" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Employing Nominal Variables, Induced Variables, and Block Variables in Path Analyses</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Heise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological Methods and Research</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="173" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reconsidering Formative Measurement</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Breivik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Wilcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="218" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Critical Review of Construct Indicators and Measurement Model Misspecification in Marketing and Consumer Research</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jarvis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mackenzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Podsakoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Consumer Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="218" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Estimation of a Model with Multiple Indicators and Multiple Causes of a Single Latent Variable</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Jöreskog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">351</biblScope>
			<biblScope unit="page" from="631" to="639" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Use of Causal Indicators in Covariance Structure Models: Some Practical Issues</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Maccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="533" to="541" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Specifying Formative Constructs in Information Systems Research</title>
		<author>
			<persName><forename type="first">S</forename><surname>Petter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIS Quarterly</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="623" to="656" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
