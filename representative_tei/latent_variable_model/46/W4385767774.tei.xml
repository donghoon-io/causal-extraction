<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IID-GAN: an IID Sampling Perspective for Regularizing Mode Collapse</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
							<email>yanglily@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liangliang</forename><surname>Shi</surname></persName>
							<email>shiliangliang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
							<email>yanjunchi@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IID-GAN: an IID Sampling Perspective for Regularizing Mode Collapse</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite its success, generative adversarial networks (GANs) still suffer from mode collapse, i.e., the generator can only map latent variables to a partial set of modes in the target distribution. In this paper, we analyze and seek to regularize this issue with an independent and identically distributed (IID) sampling perspective and emphasize that holding the IID property referring to the target distribution for generation can naturally avoid mode collapse. This is based on the basic IID assumption for real data in machine learning. However, though the source samples {z} obey IID, the generations {G(z)} may not necessarily be IID sampling from the target distribution. Based on this observation, considering a necessary condition of IID generation that the inverse samples from target data should also be IID in the source distribution, we propose a new loss to encourage the closeness between inverse samples of real data and the Gaussian source in latent space to regularize the generation to be IID from the target distribution. Experiments on both synthetic and real-world data show the effectiveness of our model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For generative models, the training data (in target space) is often assumed to be IID sampled from an unknown implicit distribution. Deep generative models often try to construct a mapping from a known distribution in source space (e.g. Gaussian distribution) to the implicit target distribution. Among popular generative schemes, e.g. Variational Auto-Encoder (VAEs) <ref type="bibr" target="#b8">[Kingma and Welling, 2014]</ref>, Generative Flow models <ref type="bibr">[Rezende and Mohamed, 2015]</ref> and Generative Adversarial Networks (GANs) <ref type="bibr" target="#b4">[Goodfellow et al., 2014]</ref>, GAN is widely recognized for its excellence in generating images with high resolution. Mode collapse is one of the standing issues in GAN, a phenomenon that the generator tends to get stuck in a subset of modes while excluding other parts of the target distribution <ref type="bibr" target="#b11">[Liu et al., 2020;</ref><ref type="bibr">Yang et al., 2019]</ref>, leading to poor generation diversity.</p><p>Efforts have been made to address mode collapse along two branches: 1) Achieve a better convergence between the generated distribution and the target (real data) distribution <ref type="bibr">[Gul-rajani et al., 2017;</ref><ref type="bibr" target="#b15">Metz et al., 2017]</ref>. Though the distribution convergence is recognized as the ultimate goal of generation <ref type="bibr" target="#b4">[Goodfellow et al., 2014]</ref>, the implicitness of the two distributions makes the desire can only be achieved by utilizing the sampling data. Thus the goal of distribution convergence can be somehow vague and imprecise. 2) Penalize the similarity of the generated images <ref type="bibr" target="#b3">[Elfeki et al., 2019;</ref><ref type="bibr" target="#b11">Mao et al., 2019]</ref> or apply multiple generator/discriminators <ref type="bibr" target="#b10">[Liu and Tuzel, 2016;</ref><ref type="bibr" target="#b13">Nguyen et al., 2017]</ref>. These methods promote the generation diversity in a more direct manner but often lack rigorous theoretical guarantees.</p><p>Existing mainstream GAN methods <ref type="bibr" target="#b4">[Goodfellow et al., 2014;</ref><ref type="bibr" target="#b0">Arjovsky et al., 2017;</ref><ref type="bibr" target="#b4">Gulrajani et al., 2017;</ref><ref type="bibr" target="#b12">Miyato et al., 2018]</ref> focus on achieving the identical generated distribution as the real one. <ref type="bibr" target="#b4">[Goodfellow et al., 2014]</ref> shows that if given enough capacity and training time, the generated distribution converges to real distribution through GAN's optimization. However, due to the limited capacity of the networks (e.g. catastrophic forgetting) and the implicitness of the two distributions, the convergence only holds in theory. With the short-term guidance in training, most of the methods <ref type="bibr" target="#b4">[Goodfellow et al., 2014;</ref><ref type="bibr" target="#b0">Arjovsky et al., 2017;</ref><ref type="bibr" target="#b4">Gulrajani et al., 2017;</ref><ref type="bibr" target="#b12">Miyato et al., 2018]</ref> tend to sample-wise boost the generation quality i.e. increase the probability density of the generated sample obeying the target distribution. Taking GAN as an example, discriminator D distinguishes whether every generation is sampled from the identical target distribution, and generator G tends to generate samples with high discriminative confidence of the current D. While in VAEbased or Flow-based approaches <ref type="bibr" target="#b8">[Kingma and Welling, 2014;</ref><ref type="bibr">Rezende and Mohamed, 2015]</ref>, log-likelihood is applied to increase the density of every generation. These methods rarely study the coupled relation between the generated samples to constitute a distribution, and each step the optimized sample is pulled to the center of the distribution to maintain high probability density, while the characteristics of the overall distribution e.g. variance are not explicitly controlled. This leads them to merely improve the quality of the individual generated samples, trivializing to maintain diversity as real data does. This may stem from the dilemma that a single sample can hardly estimate a distribution through nets, unless the nets possess unrestricted capability and do not exhibit catastrophic forgetting <ref type="bibr" target="#b11">[McCloskey and Cohen, 1989]</ref>.</p><p>In this paper, rather than study the relation between distribu-tions, we directly optimize the sampling relation between generated sample batches and the target distribution. We first make a basic yet under-exploited (in GAN literature) observation that datasets for generation are assumed to be independent and identically distributed (IID) sampled from an unknown target distribution (i.e. real distribution). We aim to achieve that the generated samples are independently sampled from the identical target distribution. If the latents {z} are IID sampled from the source, it is guaranteed that the generations {G(z)} are IID samples from the generated distribution, but not necessarily IID from the target. Note the independence here is considered from the perspective of statistical test, which needs consistency over samples {x 1 , x 2 , • • • , x n } and the target distribution (or between the latents {z 1 , z 2 , • • • , z n } and source distribution). Sample batches can contain relatively more complete features characterizing a distribution, improving optimization efficiency.</p><p>To address mode collapse and get IID generation, we first define mode completeness as the key requirement of the ideal mapping from the source to the target. As the target distribution is implicit and we can only obtain its finite IID samples, we propose a weaker/necessary condition<ref type="foot" target="#foot_1">foot_1</ref> for IID generation based on an inverse mapping: if the real data are IID samples from target distribution, then their inverses in the source space are also IID. To achieve IID property, a common and straightforward idea is to drive the distribution of the overall inverse samples close to a standard Gaussian by certain measures. This idea differs from existing inversionbased methods <ref type="bibr" target="#b15">[Srivastava et al., 2017;</ref><ref type="bibr">Donahue et al., 2017;</ref><ref type="bibr" target="#b17">Yu et al., 2019]</ref> which learn the relationship between the individual latent sample z and real data x through the discriminator, yet the relationship among {z} is not fully exploited. The highlights of the paper are:</p><p>1) We take an IID perspective to mode collapse in GAN. The requirement for an ideal generator is proposed that it should satisfy the so-called mode completeness by Def. 1. Limited by the finiteness of real samples, we resort to a necessary condition of mode completeness in Prop. 1, which requires the IID property of the inverse samples from the real data.</p><p>2) Guided by Prop. 1, a regularizer is devised to avoid mode collapse. We enforce the inverse sample batch to be distributed close to a standard Gaussian by Wasserstein distance, termed as Gaussian consistency loss (see <ref type="bibr">Sec. 4.3)</ref>. QQ-plot, Shapiro-Wilk and Kolmogorov-Smirnov statistics are used to test the IID property of the inverse Gaussian samples.</p><p>3) We show that IID-GAN outperforms baselines by different metrics on synthetic data w.r.t. the number of covered modes, quality and reverse KL divergence. Our regularization technique also performs competitively on natural images. Unsupervised disentanglement feature learning and conditional generation are also investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Since its debut <ref type="bibr" target="#b4">[Goodfellow et al., 2014]</ref>, subsequent works of GANs have emerged to improve training stability and generation quality. However, issues still remain for GANs, and mode collapse is one of the most common challenges. In this section, we introduce methods that are directly or indirectly dedicated to address mode collapse.</p><p>Improving training behavior. The promotion of distribution convergence can naturally avoid mode collapse. Unrolled <ref type="bibr">GAN [Metz et al., 2017]</ref> presents a surrogate objective to train the generator, along with an unrolled optimization of the discriminator to improve training stability. Wasserstein GANs <ref type="bibr" target="#b0">[Arjovsky et al., 2017]</ref> and its variants e.g. <ref type="bibr" target="#b4">[Gulrajani et al., 2017]</ref> modify the optimization objective to stabilize training. Although these methods all claim to improve the convergence between the generated and real distribution, the convergence focus more on generation quality that they tend to align every generated sample closer to the real distribution but focus less on the inter-sample relation.</p><p>Enforcing to capture diverse modes. Many other methods address mode collapse by penalizing similarity. GDPP <ref type="bibr" target="#b3">[Elfeki et al., 2019]</ref> applies the determinantal point process theory, which imposes a penalty on the discriminator to enforce the convergence of the covariance matrix between the features of generated samples and real data. <ref type="bibr">BuresGAN [De Meulemeester et al., 2021]</ref> matches the discriminator's last layer output through Bures distance. AdvLatGAN <ref type="bibr" target="#b10">[Li et al., 2022]</ref> mines sample pairs that tend to collapse and regularize them to maintain diversity. All these methods are concerned with the relationship among generated results. However, these methods lack stability guarantees for diverse generation, as they ignore that the generated data shall statistically maintain the same diversity as the real distribution.</p><p>Multiple generators and discriminators. Involving multiple generators to achieve wider coverage over the real distribution can also benefit diverse generation. In <ref type="bibr" target="#b10">[Liu and Tuzel, 2016]</ref>, two coupled generators are trained with parameter sharing to learn the real distribution. The multi-agent system MAD-GAN <ref type="bibr" target="#b4">[Ghosh et al., 2018]</ref> involves multiple generators along with one discriminator, and it implicitly encourages each generator to learn a partial set of modes. On the other hand, multiple discriminators are used in <ref type="bibr" target="#b3">[Durugkar et al., 2017]</ref> as an ensemble. <ref type="bibr">AMAT [Mangalam and Garg, 2021]</ref> introduce a novel training procedure that adaptively spawns additional discriminators to remember previous modes of generation. These methods directly seek to utilize more networks to capture more modes, but omit to reach the essence of mode collapse.</p><p>Learn the representations by inverse mapping. Bi-GAN <ref type="bibr">[Donahue et al., 2017]</ref>, <ref type="bibr">VEEGAN [Srivastava et al., 2017]</ref>, VAE-based models <ref type="bibr" target="#b8">[Kingma and Welling, 2014]</ref> and recently proposed <ref type="bibr">MGGAN [Bang and Shim, 2021]</ref>, Dist-GAN <ref type="bibr" target="#b16">[Tran et al., 2018]</ref> involve encoding networks (inverse mapping) of the generator to encourage the convergence between the inverse distribution of real data and source distribution. Flow- <ref type="bibr">GAN [Grover et al., 2018]</ref> utilizes a flow-based generator that allows for invertibility. However, it requires very special network design that can significantly limit the network's expressive power and inference speed.</p><p>However, similar to the previous discussion about methods improving training behavior, the inverse mapping procedure ignores the IID requirement, i.e., they merely sample-wise boost the generation quality (e.g. the probability of the sample locating in real data distribution) and fail to explicitly control the coupled relation of samples to constitute a distribution. For example, VEEGAN seeks to discriminate real/fake (z, x) pairs by the discriminator while the loss is enforced to each pair. As the networks exhibit limited capacity e.g. catastrophic forgetting, the discriminator cannot accurately characterize the implicit distribution through a sequence of single sample learning, which hinders the convergence between the inverse and source distributions as shown in Fig. <ref type="figure" target="#fig_3">3</ref>. Under the guidance of IID generation, we take one step further to reveal the impact of IID informed inverse mapping on solving mode collapse. Specifically, Prop. 1 calls for the entire inverse sample batch {G -1 (x)} to be IID sampled from Gaussian, which requires considering the entire sample set, regularizing the optimization with more samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries and Motivation</head><p>Fig. <ref type="figure">1a</ref> presents a basic interpretation of mode collapse on top of the mapping between the source and target spaces. To address mode collapse, Fig. <ref type="figure">1b</ref> shows a probability measure requirement for source and target space samples (i.e. probability measures for set-set pairs in the two spaces should be equal). We first define mode completeness corresponding to the ideal generative mapping between two probability measures: Definition 1. (Mode Completeness) The probability measures α and β are defined in the source space A and the target space B, respectively. The generative mapping G : A → B is defined as mode completeness from α to β if G satisfies:</p><formula xml:id="formula_0">β(S) = α(z ∈ A : G(z) ∈ S}),<label>(1)</label></formula><p>where S ⊂ B is an arbitrary set in the the target space.</p><p>Eq. 1 defines the same operation of the push-forward operator <ref type="bibr" target="#b14">[Peyré et al., 2019]</ref> β = G # α in optimal transportation. So the mode completeness can also be interpreted as regularizing the mapping G such that β = G # α, and the operator G # implies that G pushes forward the mass of α to β <ref type="bibr" target="#b14">[Peyré et al., 2019]</ref>. Based on Def. 1, mode collapse can be naturally avoided because the values of the probability measures given the corresponding sets based on G are equal (i.e. α({z i }) = β({T (z i )})). Then given the equal probability measures, when z i are IID sampled from α, the generations {G(z i )} are IID samples from β.</p><p>Mode completeness β = G # α can hardly be completely achieved since β is complex and implicit. Considering the basic assumption that the real training data are IID sampled from β, if we assume the existence of G -1 : B → A such that z = G -1 (G(z)) and x = G(G -1 (x)) for any z ∈ A and x ∈ B, then we can obtain a necessary condition for mode completeness. Such a necessary condition for IID generation is formalized Proposition 1: Proposition 1. (IID for Inverse of Target Samples) If the mapping G satisfies mode completeness from the source probability measure α to the target measure β and its inverse G -1 exists, then given IID samples {x (i) } n i=1 from β, their inverses {G -1 (x (i) )} n i=1 can be viewed as IID samples from α. To prove Proposition 1, we first provide a proof for Lemma 1, which represents a more general scenario. Lemma 1. Assume the existence of mapping T which satisfies T # α = β and the existence of its inverse T -1 . Let ρ be the joint probability measure of n independent copies of β, denoted as β 1 , • • • , β n and π is the joint probability measure of n independent copies of α, denoted as</p><formula xml:id="formula_1">α 1 , • • • , α n . Then we have T# π = ρ, where T is the concatenation of n mapping T s that satisfies T (x 1 , • • • , x n ) = [T (x 1 ), • • • , T (x n )].</formula><p>Then we can obtain that π is the joint probability measure with n independent probability measure α 1 , • • • , α n .</p><p>Proof. Given n measurable set S i ⊂ B and S = S 1 × S 2 × • • • × S n , with the independence between {β i } n i=1 , we obtain:</p><formula xml:id="formula_2">ρ(S) = β 1 (S 1 )β 2 (S 2 ) • • • β n (S n )<label>(2)</label></formula><p>Since T# π = ρ and T # α = β, we know that ρ(S) = π( T -1 (S)) and β(S i ) = α(T -1 (S i )). Then we have:</p><formula xml:id="formula_3">π( T -1 (S)) = α 1 (T -1 (S 1 )) • • • α n (T -1 (S n ))<label>(3)</label></formula><p>which indicates that π is the joint probability measure with</p><formula xml:id="formula_4">independent α 1 , • • • , α n .</formula><p>The proof of Proposition 1 can be easily obtained from Lemma 1 by setting n measurable sets as</p><formula xml:id="formula_5">S 1 = {x 1 }, • • • , S n = {x n } where x 1 , • • • , x n are n IID samples from target distribution. According to Eq. 3, we obtain α(T -1 (x 1 )) • • • α(T -1 (x n )) = π( T -1 ({x 1 , • • • , x n }))</formula><p>(4) which implies that {T -1 (x (i) )} n i=1 can be viewed as n independent samples from source distribution.</p><p>Since it is commonly assumed that the real data {x (i) } n i=1 are independently sampled from an unknown distribution β, their inverses {G -1 (x (i) )} n i=1 can be viewed as IID samples from a known distribution α. To satisfy mode completeness, we regularize the inverse samples of real data to be closer to the IID samples from α. Since we can hardly obtain a strict inverse mapping G -1 , we calls for an approximation of G -1 as F , which is obtained by penalizing:</p><formula xml:id="formula_6">z = F (G(z))</formula><p>x = G(F (x)).</p><p>(5)</p><p>4 The Proposed Approach</p><p>Our loss mainly consists of three parts as shown in Fig. <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adversarial Learning Term: GAN Loss</head><p>The vanilla <ref type="bibr">GAN [Goodfellow et al., 2014]</ref> consists of a discriminator D : R d → R and a generator G : R M → R d , which are typically embodied by deep neural networks. Given the empirical distribution p(x), D distinguishes whether the generated sample is from real data, while G models the mapping from Gaussian sample z to a target space sample. The objective V (G, D) is optimized for the discriminator and generator by solving the min-max:</p><formula xml:id="formula_7">E x∼pr(x) [log(D(x))] + E z∼p(z) [log(1 -D(G(z)))] (6)</formula><p>The first term denotes the probability expectation of x coming from the real data distribution p(x) and the second term involves the input distribution p(z), which is embodied in this paper as a standard multi-dimensional (M -D) Gaussian distribution N (z; 0, I). I ∈ R M ×M denotes the identity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reconstruction Term: Cycle-Consistency Loss</head><p>Prop. 1 calls for the existence of the generator's inverse, i.e. G -1 . To achieve the approximation for the holding of z = F (G(z)) and x = G(F (x)) where F is also a networkbased mapping, a cycle-consistency loss can be an effective solution <ref type="bibr" target="#b15">[Srivastava et al., 2017;</ref><ref type="bibr">Donahue et al., 2017]</ref>:</p><formula xml:id="formula_8">L re (G, F ) = E z z -F (G(z)) 1 to achieve inverse mapping +λ E x x -G(F (x)) 1</formula><p>to guarantee the existence (7) Here λ = d/M is the dimension ratio of x to z. The first term promotes F to be the inverse of G, which takes the reconstruction loss as the expectation of the cost for auto-encoding noise vectors <ref type="bibr" target="#b15">[Srivastava et al., 2017]</ref>. The second term promotes F (x) ∈ R M , which makes z = F (x) possible to be sampled from Gaussian. Then for each x, we can obtain the corresponding z in R M satisfying G(z) = x, which guarantees the existence of the real samples' inverses in source space. Note that it does not offer any help to avoid mode imbalance, since the imbalance refers to the overall distribution rather than the individual data points studied here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Regularizer: Inverse Distribution Consistency</head><p>Recall the necessary condition for IID generation proposed in Prop. 1: assume that the given real data {x (i) } n i=1 are IID sampled from p(x), then the inverse samples of the real data will be independent and obey the same distribution p(z) (i.e. IID samples from source distribution). Thus, given a batch of real data, {F (x (i) )} n i=1 should be closer to independent samples from standard Gaussian p(z).</p><p>M -D Gaussian consistency loss. Suppose the inverse samples {z (i) } n i=1 ∈ R M of real data obey the Gaussian distribution N (z; µ, Σ) in latent space, then the maximum likelihood estimation can be formulated as:</p><formula xml:id="formula_9">μ = 1 n n i=1 z(i) , Σ = 1 n n i=1 z(i) -μ z(i) -μ<label>(8</label></formula><p>) where μ ∈ R M , and Σ ∈ R M ×M is the estimated covariance. Our goal is to make the Gaussian q(z) = N (z; μ, Σ) closer to the standard Gaussian p(z) = N (z; 0, I), based on which we can view {z (i) } n i=1 as IID samples from standard Gaussian p(z). To this end, we introduce a distribution closeness loss L Gau . For example, it can be specified as the square of Wasserstein distance of two Gaussians:</p><formula xml:id="formula_10">L Gau = μ 2 + trace( Σ + I -2 Σ1/2 ) (9)</formula><p>The two Gaussian distributions q(z) and p(z) can also be evaluated by static divergence, e.g., p-norm, KL-divergence. More details about loss selections are presented in Appendix A.</p><p>Decoupling M -D Gaussian into M 1-D Gaussians. For large M and small batch size, the training may potentially suffer from the curse of dimensionality for estimating Σ. So we also devise a simplified consistency loss by decoupling the M -D Gaussian loss to the sum of the 1-D Gaussian losses of M ones. This design introduces an assumption on the covariance that the non-diagonal values are all zero. Equivalently, the inverse samples {z (i) } n i=1 follow M 1-D Gaussian N (z; µ, σ) and then we can get the estimation μ and σ.</p><p>Our goal is to make the 1-D Gaussian distribution q(z j ) closer to the standard 1-D Gaussian distribution in each dimension j. Similar to the M -D case, we can design the Gaussian loss with Wasserstein distance between two Gaussian distributions, and summing over M dimensions:</p><formula xml:id="formula_11">L Gau = M m=1 μ2 m + ( σm -1) 2 (10)</formula><p>Throughout the rest of this paper, we directly call it the Gaussian loss, omitting the term consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">IID-GAN: an Overview</head><p>By inputting samples from p(z) together with the corresponding inverse samples {z} from the real data, we optimize G, D, and F by adversarial learning to push {z} closer to the sampling results of p(z). Then we can obtain the final loss as:</p><formula xml:id="formula_12">min G,F max D V (G, D) + λ re L re (G, F ) + λ Gau L Gau (F ) (11)</formula><p>where L Gau (F ) can be specified according to different distances or divergences and λ re , λ Gau are weight parameters, which will be discussed in detail in the experiment section.</p><p>Remarks for Conditional Generation. IID-GAN can be extended to the conditioned case <ref type="bibr" target="#b12">[Mirza and Osindero, 2014]</ref> when the label of the real data c is known. As for conditional IID-GAN, (z, c) are the inputs of the generator G and the real x is the input of F for classification. The Gaussian loss is used to maintain the independence condition and learn the diversity of hidden features (e.g. the thickness for MNIST).</p><p>Remarks for the Disentanglement View. Many previous studies <ref type="bibr" target="#b6">[Higgins et al., 2017;</ref><ref type="bibr" target="#b8">Kim and Mnih, 2018]</ref> learn the unsupervised disentanglement representation with the assumption of independent factors, i.e. q(z) = M j=1 q(z j ). Our work presents a new viewpoint with an M -D Gaussian guarantee. When q(z) approximates an M -D standard Gaussian, it is obvious that z is independent for different dimensions. However, varying z i can not disentangle different modes as shown in the second column of Fig. <ref type="figure" target="#fig_3">3</ref>. We observe that representing the data in polar coordinates and varying the polar angle and diameter can achieve better performance for unsupervised disentangle representations.</p><p>IID testing for Gaussian. The regularization assumes a non-standard Gaussian and enforces the consistency of two Gaussians, which raises the doubt: Is the consistency useful for IID? We apply mathematical statistics here to test the Gaussian IID property. Specifically, we adopt the QQ-plot, Shapiro-Wilk test (SW) <ref type="bibr">[Shapiro and Wilk, 1965]</ref>, Kolmogorov-Smirnov test (KS) <ref type="bibr">[Gosset, 1987]</ref> to show whether the samples are IID sampled from standard Gaussian. The results are shown in Fig. <ref type="figure">5</ref> and Table <ref type="table" target="#tab_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Discussion</head><p>Experiments are conducted on a single RTX 3090. Synthetic data results are performed on GeForce RTX 2080Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiments on Synthetic Datasets</head><p>Since the distribution is known for synthetic data, mode collapse can be directly measured. In line with <ref type="bibr" target="#b15">[Metz et al., 2017]</ref>, we simulate two synthetic datasets: i) Ring dataset. It consists of a mixture of 8 2-D Gaussians p(z) with mean {(2 cos (iπ/4), 2 cos (iπ/4))} 8 i=1 and standard deviation 0.001. 12.5K samples are simulated from each Gaussian (100K samples in total). ii) Grid dataset. It consists of a mixture of 25 2-D isotropic Gaussians i.e. p(z) with mean {(2i, 2j)} 2 i,j=-2 and standard deviation 0.0025. 4K samples are simulated from each Gaussian (100K samples in total).</p><p>Metrics and network architecture. Following <ref type="bibr" target="#b15">[Metz et al., 2017;</ref><ref type="bibr" target="#b3">Elfeki et al., 2019]</ref>, we adopt the number of covered modes, generation quality 2 and reverse KL divergence as the 2 We follow <ref type="bibr" target="#b2">[De Meulemeester et al., 2021]</ref>: if the generated data 24.2 ± 1.2 83.4 ± 2.9 0.26 ± 0.20 Unrolled GAN 6.4 ± 2.2 98.6 ± 0.5 0.42 ± 0.53 8.2 ± 1.7 98.7 ± 0.6 1.27 ± 0.17 VEEGAN 5.4 ± 1.2 38.8 ± 16.7 0.40 ± 0.10 20.0 ± 2.6 85.0 ± 5.9 0.41 ± 0.10 IID-GAN (1-D) 8.0 ± 0.0 97.3 ± 0.6 0.18 ± 0.06 25.0 ± 0.0 97.8 ± 0.49 0.32 ± 0.09 IID-GAN (M -D) 8.0 ± 0.0 99.0 ± 0.2 0.17 ± 0.06 25.0 ± 0.0 98.0 ± 0.4 0.26 ± 0.12 Table <ref type="table">1</ref>: Results for Ring and Grid synthetic datasets.</p><p>evaluation metrics. Since in the experiment, each mode shares the same number of real samples, one can calculate the reverse KL divergence between the generated distribution and the real one <ref type="bibr" target="#b13">[Nguyen et al., 2017]</ref>. Note the reverse KL divergence is not strictly defined, as m i=1 p i &lt; 1 (there exist invalid generated points), thus it allows being negative. We adopt network architectures consisting of three linear layers with hidden dimensions of 100, 200, and 100, along with ReLU activation, for both the generator and discriminator.</p><p>Quantitative results. The comparison involve vanilla <ref type="bibr">GAN [Goodfellow et al., 2014]</ref>, <ref type="bibr">BiGAN [Donahue et al., 2017]</ref>, Unrolled <ref type="bibr">GAN [Metz et al., 2017]</ref> and VEEGAN <ref type="bibr" target="#b15">[Srivastava et al., 2017]</ref> on Ring and Grid datasets in Table <ref type="table">1</ref>. IID-GAN variants are the only methods covering all modes on both datasets and IID-GAN (M -D) achieves the best quality and RKL performance compared to other peer methods.</p><p>Visualization with Gaussian inverse samples. Models involving inverse mappings (i.e. encoders) allow the visualization for the inverses of real samples. Fig. <ref type="figure" target="#fig_3">3</ref> present the visualization results including the inverse from real data, source Gaussian samples and generated results of VAE [Kingma and Welling, 2014], VEEGAN, BiGAN and IID-GAN variants. As shown in the left four columns, VAE can cause the overlap of the inverse samples z, which may lead to bad generations (white points). BiGAN and VEEGAN learn the relations between z and x rather than the relation among the samples {G -1 (x)}, which leads to the failure of constructing 2D inverse Gaussian samples as shown in the first column. For IID-GAN, as shown in the fifth column for IID-GAN (M-D), the inverse samples are very similar to the Gaussian samples, which show the effectiveness of the regularization.</p><p>Visualization of generated distribution. Fig. <ref type="figure">4</ref> shows the generation results of IID-GAN and peer methods. IID-GAN is within 3 times std of the Gaussian, consider it a valid generation (otherwise bad). The resulting ratio is used as the generation quality. significantly outperforms other peer methods in terms of mode coverage and better fits the target distribution. IID test for inverse samples. We support IID test for Gaussian distribution on Ring dataset in Table <ref type="table" target="#tab_0">2</ref>. Shapiro-Wilk (SW) and Kolmogorov-Smirnov (KS) Statics are calculated to verify IID property. In Fig. <ref type="figure">5</ref>, QQ-plot are supported for IID test. Compared to other peer methods, IID-GAN's plots are closer to the red diagonal, implying that the inverse samples of IID-GAN are closer to Gaussian.</p><p>Ablation study for Gaussian consistency loss. The right four columns of Fig. <ref type="figure" target="#fig_3">3</ref> compare the results of 1-D, M -D and without Gaussian consistency loss in Ring datasets. For 1-D loss's inverse samples in source space, the yellow sample region in the red box leads to a tendency to mode collapse, while the generated modes in the M -D case are more uniformly scaled and more effective for solving mode collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments on Real-world Data</head><p>The experimented image datasets include <ref type="bibr">MNIST [LeCun et al., 1998</ref>], Stacked <ref type="bibr">MNIST [Metz et al., 2017]</ref>, <ref type="bibr">CIFAR-10 [Krizhevsky et al., 2009]</ref>, <ref type="bibr">STL-10 [Coates et al., 2011]</ref>, LSUN <ref type="bibr" target="#b17">[Yu et al., 2015]</ref> and CELEBA <ref type="bibr" target="#b10">[Liu et al., 2015]</ref>. We adopt different architectures to evaluate our model,    is defined as the ratio of high-confidence samples evaluated by a trained classifier. 1-D and M -D IID-GAN can lead to less mode imbalance (which is evaluated by KL divergence) and higher quality than VAE, while VEEGAN is unstable and often fails to successfully generate in case of bad initialization.</p><p>Quantitative results. (i) Table <ref type="table" target="#tab_1">3</ref> show the #Mode, KL divergence and FID results on Stacked MNIST. The KL divergence calculation is based on the classification results by the classifier proposed by <ref type="bibr" target="#b2">[Dieng et al., 2019]</ref>. The architecture is DCGAN-based in line with <ref type="bibr" target="#b15">[Radford et al., 2016]</ref>. (ii) Table 4 shows the experimental results of <ref type="bibr">CIFAR-10 [Krizhevsky et al., 2009] and</ref><ref type="bibr">STL-10 [Coates et al., 2011]</ref>. We adopt JSD, IS, FID as the metrics, including <ref type="bibr">UnrolledGAN [Metz et al., 2017]</ref>, <ref type="bibr">VEEGAN [Srivastava et al., 2017]</ref>, MAD-GAN <ref type="bibr" target="#b4">[Ghosh et al., 2018]</ref>, <ref type="bibr">GDPP [Elfeki et al., 2019]</ref> and <ref type="bibr">BuresGAN [De Meulemeester et al., 2021]</ref> for comparison. To fit with advanced GAN achievements, we adopt SNGAN <ref type="bibr" target="#b12">[Miyato et al., 2018]</ref> as the backbone. (iii) We also evaluate on larger scale datasets e.g. CELEBA <ref type="bibr" target="#b10">[Liu et al., 2015]</ref> and LSUN Church <ref type="bibr" target="#b17">[Yu et al., 2015]</ref> with more recently proposed works pursuing diversity for comparison including Dist-GAN <ref type="bibr" target="#b16">[Tran et al., 2018]</ref>, <ref type="bibr">MGGAN [Bang and Shim, 2021]</ref> and <ref type="bibr">AMAT [Mangalam and Garg, 2021]</ref>. We adopt FID, JSD as the evaluation metrics. The results are presented in Table <ref type="table" target="#tab_3">5</ref>. (iv) We evaluate IID-GAN with StyleGAN2-ada <ref type="bibr" target="#b7">[Karras et al., 2020]</ref>    Results on conditional generation and disentanglement. Fig. <ref type="figure">7</ref> shows the results for conditional generation on CIFAR-10. The conditional IID-GANs are the most stable and robust model, and maintain the most diverse generation for each given category. Fig. <ref type="figure" target="#fig_6">8</ref> shows the visualization results in Cartesian and Polar Coordinate on MNIST. To better visualize the distribution, we use a 2-D latent space for generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>To address GAN's long-standing mode collapse issue, we provide an IID sampling perspective to analyze the generation behavior and offer our new methodology guidance, which is orthogonal to existing literature. The proposed IID-GAN shows its effectiveness on both synthetic and real-world datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Mapping from source to target: (a) Left: high-density samples can only be mapped to part of real data i.e. mode collapse; Right: only part of real samples' inverses lie in high-density sampling region, and the inverses outside the region can hardly be sampled, leading to mode collapse. (b) Assuming the existence of the inverse of mapping G(•), to solve mode collapse and get IID generation, the probability measures of the set G -1 (S) and S should be equal for any set S in target space. See Fig. 3 for experimental examples.</figDesc><graphic coords="3,55.07,54.00,157.95,138.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: Generator G maps M -D Gaussian samples to targets and F inverts the target back to a source sample obeying M -D Gaussian.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>6 ± 0.5 98.8 ± 0.6 0.92 ± 0.11 18.4 ± 1.6 98.0 ± 0.4 0.75 ± 0.25 BiGAN 6.8 ± 1.0 38.6 ± 9.5 0.43 ± 0.18</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example for 2-D source (z) to 2-D target (x) generation and inverse adopting 8-mode Ring dataset as the real training set. Left 4 columns are the results of VAE, BiGAN, VEEGAN, while the right 4 columns are the results of IID-GAN under different Gaussian consistency losses as detailed in Sec. 4.3 after training 24K batches. For each half part, column 1 and column 5 show the inverse of the real target data, column 2 and column 6 show the sampled z from Gaussian in source space. Source points are in nine colors (8 'modes' + 1 'bad') according to their generation's mode in the target space. The pie charts show the ratio of valid generation points in different modes.</figDesc><graphic coords="6,129.95,56.86,166.75,133.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 7 :</head><label>57</label><figDesc>Figure 5: QQ-plot of standard 2-D Gaussian over two dimensions. The closer to the diagonal, the closer to the Gaussian distribution.</figDesc><graphic coords="6,450.99,390.96,106.00,71.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>backbone on 64×64 resized AFHQ, LSUN via FID and KID metrics for same training iterations, i.e. 10M images trained (measured in real images shown to the discriminator</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Disentanglement by uniformly sampling on MNIST.</figDesc><graphic coords="7,455.28,222.67,77.07,77.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>SW and KS Static evaluation for IID test on Ring.</figDesc><table><row><cell>Model</cell><cell cols="3">Stacked MNIST</cell></row><row><cell></cell><cell cols="2">#Mode↑ KL↓</cell><cell>FID↓</cell></row><row><cell>GAN</cell><cell>392.0</cell><cell cols="2">8.012 97.788</cell></row><row><cell>VEEGAN</cell><cell>761.8</cell><cell cols="2">2.173 86.689</cell></row><row><cell>PACGAN</cell><cell>992.0</cell><cell cols="2">0.277 117.128</cell></row><row><cell>IID-GAN (1-D)</cell><cell>996.4</cell><cell cols="2">0.152 86.911</cell></row><row><cell>IID-GAN (M -D)</cell><cell>999.7</cell><cell cols="2">0.101 69.675</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Results on Stacked MNIST. 83±0.33 7.12±0.07 26.94±0.94 5.23±0.36 8.39±0.05 37.25±0.32 MAD-GAN 7.53±0.78 7.00±0.02 29.24±0.36 9.15±0.43 7.99±0.03 40.03±0.10 GDPP 5.25±0.29 7.23±0.06 26.90±0.15 4.18±0.19 8.36±0.02 38.12±0.22 BuresGAN 6.19±0.72 7.22±0.01 28.28±0.28 4.33±0.19 8.17±0.03 39.22±0.39 IID-GAN 2.95±0.22 7.35±0.03 25.71±0.28 3.11±0.25 8.43±0.04 36.16±0.51</figDesc><table><row><cell>Model</cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell>STL-10</cell><cell></cell></row><row><cell>JSD×10 2 ↓</cell><cell>IS↑</cell><cell>FID↓</cell><cell>JSD×10 2 ↓</cell><cell>IS↑</cell><cell>FID↓</cell></row><row><cell cols="3">Unrolled GAN 3.23±0.36 7.28±0.13 27.07±0.74</cell><cell cols="3">3.32±0.27 8.28±0.03 38.79±0.56</cell></row><row><cell>VEEGAN 4.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Results on CIFAR-10 and STL-10.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Evaluation (calculated at 100K steps of training) on realworld CELEBA and LSUN, using SNGAN as backbone.following StyleGAN2-ada) and show its positive effect.</figDesc><table><row><cell>Method</cell><cell cols="2">AFHQ Cat</cell><cell cols="2">LSUN Church</cell></row><row><cell></cell><cell>FID↓</cell><cell>KID×10 3 ↓</cell><cell>FID↓</cell><cell>KID×10 3 ↓</cell></row><row><cell cols="3">StyleGAN2-ada 6.386±0.503 1.077±0.113</cell><cell cols="2">3.530±0.021 1.415±0.008</cell></row><row><cell>IID-GAN</cell><cell cols="2">5.373±0.142 0.897±0.126</cell><cell cols="2">3.208±0.111 1.317±0.095</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Evaluation on StyleGAN2-ada backbone.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Note that it will become a necessary and sufficient condition if there are an infinite amount of real data.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Gaussian Loss with Different Divergence</head><p>In this paper, Gaussian Consistency loss involves three different forms to reduce mode collapse:</p><p>1) p-norm for the difference of mean and variance. To evaluate the divergence of two Gaussian distributions N (z; 0, I) and N (z; μ, Σ), we first calculate the difference of the parameters of Gaussian with p-norm:</p><p>2) Wasserstein distance. Given two M -D Gaussains p(z) and q(z), the 2-Wasserstein distance is: W 2 (p(z), q(z)) = 0 if and only if μ = 0 and Σ = I.</p><p>3) KL divergence. Given M -D Gaussians p(z) and q(z), the KL divergence KL(p(z), q(z)) can be specified as:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Network Architectures for Real Data</head><p>The architectures for CIFAR-10 are presented in Table <ref type="table">7</ref> and<ref type="table">8</ref>. The architectures for CELEBA and LSUN are presented in Table <ref type="table">9</ref> and<ref type="table">10</ref>. The inverse mapping primarily follows the design of discriminator, with only the dimension of the output layer changed to match that of the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Training Details for Real Data</head><p>MNIST. We set the loss weights as (λ re , λ Gau ) = (0.5, 0.1). Following <ref type="bibr" target="#b2">[Dieng et al., 2019]</ref>, we use the 10-category classifier to divide the generated images into 11 categories. If the highest probability of the generated image's prediction is smaller than 0.75, we treat it as a bad generation and classify it to the bad class. Otherwise, its label is determined according to the highest probability. Latent space dimension is set as 2. We train the models for 300 epochs. StackedMNIST.. The StackedMNIST covers 1,000 known modes, as constructed by stacking three randomly sampled MNIST images along the RGB channels in line with the practice in <ref type="bibr" target="#b15">[Metz et al., 2017]</ref>. We follow <ref type="bibr" target="#b15">[Srivastava et al., 2017]</ref> to evaluate the number of covered modes and divergence between the real and generation distributions. The weights are set as (λ re , λ Gau ) = (3, 3). Latent space dimension is set as 100. We train the models for 50 epochs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution Statement</head><p>Yang Li and Liangliang Shi contribute equally to this work. Junchi Yan is the correspondence author. The work was partly supported by NSFC (62222607, U19B2035).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bang and Shim, 2021] Duhyeon Bang and Hyunjung Shim. Mggan: Solving mode collapse using manifold-guided training</title>
		<author>
			<persName><surname>Arjovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2021</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName><surname>Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The bures metric for generative adversarial networks</title>
		<author>
			<persName><forename type="first">De</forename><surname>Meulemeester</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04302</idno>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
		<editor>
			<persName><forename type="first">Philipp</forename><surname>Donahue</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Trevor</forename><surname>Krähenbühl</surname></persName>
		</editor>
		<editor>
			<persName><surname>Darrell</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2021. 2021. 2019. 2019. 2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gdpp: Learning diverse generations using determinantal point processes</title>
		<author>
			<persName><surname>Durugkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<editor>
			<persName><forename type="first">Mohamed</forename><surname>Elfeki</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Morgane</forename><surname>Riviére</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2019. 2019</date>
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gosset, 1987] Eric Gosset. A three-dimensional extended kolmogorov-smirnov test as a useful tool in astronomy</title>
		<author>
			<persName><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="1987">2018. 2018. 2014. 2014. 1987. 2018. 2018. 2017</date>
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="page" from="258" to="264" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><surname>Heusel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName><surname>Karras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12104" to="12114" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Mnih</forename><forename type="middle">;</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<publisher>Kingma and Welling</publisher>
			<date type="published" when="2009">2018. 2018. 2014. 2014. 2009. 2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech Report</note>
	<note>Disentangling by factorising. ICML</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>Ziwei Liu</publisher>
			<date type="published" when="2015-12">2022. 2022. 2016. 2016. 2015. December 2015</date>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mangalam and Garg, 2021] Karttikeya Mangalam and Rohin Garg. Overcoming mode collapse with adaptive multi adversarial training</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.14406</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Ben Poole, David</addrLine></address></meeting>
		<imprint>
			<publisher>Luke Metz</publisher>
			<date type="published" when="1989">2020. 2020. 2021. 2019. 2019. 1989. 1989. 2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Osindero ; Takeru</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiki</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichi</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<idno>arXiv:1802.05957</idno>
	</analytic>
	<monogr>
		<title level="m">Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets</title>
		<editor>
			<persName><surname>Miyato</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014">2014. 2014. 2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual discriminator generative adversarial nets</title>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Computational optimal transport: With applications to data science</title>
		<author>
			<persName><surname>Peyré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="355" to="607" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shapiro and Wilk, 1965] Samuel Sanford Shapiro and Martin B Wilk. An analysis of variance test for normality (complete samples)</title>
		<author>
			<persName><surname>Radford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12462</idno>
	</analytic>
	<monogr>
		<title level="m">On gans and gmms</title>
		<editor>
			<persName><forename type="first">Lazar</forename><surname>Srivastava</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chris</forename><surname>Valkov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Russell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Charles</forename><surname>Gutmann</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sutton</surname></persName>
		</editor>
		<imprint>
			<publisher>Richardson and Weiss</publisher>
			<date type="published" when="1965">2016. 2016. 2015. 2018. 2018. 2017. 1965. 2017. 2017</date>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="591" to="611" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Diversitysensitive conditional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Tran</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2019. 2019</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lsun: Construction of a largescale image dataset using deep learning with humans in the loop</title>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015">2015. 2015. 2019. 2019</date>
			<biblScope unit="page" from="4206" to="4212" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Vaegan: A collaborative filtering framework based on adversarial variational autoencoders</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
