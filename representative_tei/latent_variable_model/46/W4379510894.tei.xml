<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Multimodal Dynamical Variational Autoencoder for Audiovisual Speech Representation Learning ⋆</title>
				<funder ref="#_QaQx77b">
					<orgName type="full">EC</orgName>
				</funder>
				<funder>
					<orgName type="full">Randstad</orgName>
				</funder>
				<funder ref="#_2fpF2W3 #_KthBdc9 #_g5AqR3V">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-02-20">20 Feb 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Samir</forename><surname>Sadok</surname></persName>
							<email>samir.sadok@centralesupelec.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">CentraleSupélec</orgName>
								<orgName type="laboratory" key="lab2">IETR UMR CNRS 6164</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Leglaive</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">CentraleSupélec</orgName>
								<orgName type="laboratory" key="lab2">IETR UMR CNRS 6164</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Girin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">GIPSA-lab</orgName>
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Grenoble-INP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xavier</forename><surname>Alameda-Pineda</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Inria</orgName>
								<orgName type="laboratory">LJK</orgName>
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Renaud</forename><surname>Séguier</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">CentraleSupélec</orgName>
								<orgName type="laboratory" key="lab2">IETR UMR CNRS 6164</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Multimodal Dynamical Variational Autoencoder for Audiovisual Speech Representation Learning ⋆</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-02-20">20 Feb 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.03582v3[cs.SD]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep generative modeling</term>
					<term>disentangled representation learning</term>
					<term>variational autoencoder</term>
					<term>multimodal and dynamical data</term>
					<term>audiovisual speech processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High-dimensional data such as natural images or speech signals exhibit some form of regularity, preventing their dimensions from varying independently. This suggests that there exists a lower dimensional latent representation from which the high-dimensional observed data were generated. Uncovering the hidden explanatory features of complex data is the goal of representation learning, and deep latent variable generative models have emerged as promising unsupervised approaches. In particular, the variational autoencoder (VAE) which is equipped with both a generative and an inference model allows for the analysis, transformation, and generation of various types of data. Over the past few years, the VAE has been extended to deal with data that are either multimodal or dynamical (i.e., sequential). In this paper, we present a multimodal and dynamical VAE (MDVAE) applied to unsupervised audiovisual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how audiovisual speech latent factors are encoded in the latent space of MDVAE. These experiments include manipulating audiovisual speech, audiovisual facial image denoising, and audiovisual speech emotion recognition. The results show that MDVAE effectively combines the audio and visual information in its latent space. They also show that the learned static representation of audiovisual speech can be used for emotion recognition with few labeled data, and with better accuracy compared with unimodal baselines and a state-of-the-art supervised model based on an audiovisual transformer architecture.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction and related work</head><p>The world around us is represented by a multitude of different modalities <ref type="bibr" target="#b30">(Lazarus, 1976)</ref>. A single event can be observed from different perspectives, and combining these different views can provide a complete understanding of what is happening. For instance, speech in human interactions is a multimodal process where the audio and visual modalities carry complementary verbal and nonverbal information. By capturing the correlations between different modalities, we can reduce uncertainty and better understand a phenomenon <ref type="bibr">(Bengio et al., 2013)</ref>. Combining complementary sources of information from heterogeneous modalities is a challenging task, for which machine and deep learning techniques have shown their efficiency. In particular, the flexibility and versatility of deep neural networks allow them to efficiently learn from heterogeneous data to solve a given task <ref type="bibr" target="#b46">(Ramachandram &amp; Taylor, 2017;</ref><ref type="bibr">Baltrušaitis et al., 2018)</ref>.</p><p>The rapid development of artificial intelligence technology and hardware acceleration has led to a shift towards multimodal processing <ref type="bibr" target="#b46">(Ramachandram &amp; Taylor, 2017)</ref>, which aims to enhance machine perception by integrating various data types. With the explosion of digital content and communication, audiovisual speech processing has become increasingly important for a range of applications, such as speech recognition <ref type="bibr" target="#b0">(Afouras et al., 2018;</ref><ref type="bibr" target="#b43">Petridis et al., 2018;</ref><ref type="bibr" target="#b19">Hori et al., 2019)</ref>, speaker identification <ref type="bibr" target="#b50">(Roth et al., 2020)</ref>, and emotion recognition <ref type="bibr" target="#b66">(Wu et al., 2014;</ref><ref type="bibr" target="#b41">Noroozi et al., 2017;</ref><ref type="bibr" target="#b53">Schoneveld et al., 2021)</ref>. However, in tasks such as emotion recognition, the limited availability of labeled data remains a significant challenge. As a result, researchers are investigating unsupervised or weakly supervised methods to learn effective audiovisual speech representations. This is extremely promising in problem settings involving a large amount of unlabeled data but limited labeled data.</p><p>Deep generative models <ref type="bibr" target="#b26">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b48">Rezende et al., 2014;</ref><ref type="bibr" target="#b17">Goodfellow et al., 2014)</ref> have recently become very successful for unsupervised learning of latent representations from high-dimensional and structured data such as images, audio, and text. Learning meaningful representations is essential not only for synthesizing data but also for data analysis and transformation. For a learned representation to be effective, it should capture high-level characteristics that are invariant to small and local changes in the input data, and it should be as disentangled as possible for explainability. Furthermore, hierarchical and disentangled generative models have demonstrated their efficacy to solve downstream learning tasks <ref type="bibr" target="#b63">(Van Steenkiste et al., 2019;</ref><ref type="bibr">Bengio et al., 2013)</ref>. Variants of generative models have recently led to considerable progress in disentangled representation learning, particularly with the variational autoencoder (VAE) <ref type="bibr" target="#b26">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b48">Rezende et al., 2014)</ref>.</p><p>The VAE considers that an observed high-dimensional data vector x is generated by a low-dimensional latent vector z. The generative process is characterized by the joint distribution p θ (x, z) = p θ (x|z)p(z), where p(z) is the prior distribution over the latent variable and p θ (x|z) is the conditional likelihood that characterizes how the observed data is generated from the latent variable. In the VAE, the conditional likelihood is parameterized by a neural network called the decoder, whose parameters are denoted by θ. The VAE also comes with an inference model q ϕ (z|x), which approximates the intractable posterior distribution of the latent variable. This inference model is parametrized by a second neural network, called the encoder, whose parameters are denoted by ϕ. The inference model allows us to extract the latent variable z from an observed data vector x, and the generative model allows us to generate x from z. The parameters of both the inference (i.e., encoder) and generative (i.e., decoder) models are efficiently and jointly learned by maximizing a lower bound of the training data log-likelihood called the evidence lower-bound (ELBO), which is central in variational methods for inference and learning in probabilistic graphical models <ref type="bibr" target="#b40">(Neal &amp; Hinton, 1998;</ref><ref type="bibr" target="#b22">Jordan et al., 1999)</ref>.</p><p>The VAE enables deep unsupervised representation learning in a Bayesian framework. Typically, a standard Gaussian distribution is chosen for the prior distribution over the latent variable: p(z) = N (z; 0, I). This choice encourages the independence of the different dimensions in the learned representation, and it is considered a key factor contributing to VAE's potential for disentanglement. However, it has been observed that vanilla VAEs exhibit limited disentanglement capability, especially when dealing with complex datasets. To address this challenge, substantial efforts have been made to enhance disentanglement by introducing implicit or explicit inductive biases in the model and/or in the learning algorithm. Early methods for disentanglement using VAEs focused on modifying the evidence lower bound objective function <ref type="bibr" target="#b17">(Higgins et al., 2016;</ref><ref type="bibr" target="#b8">Chen et al., 2018;</ref><ref type="bibr" target="#b24">Kim &amp; Mnih, 2018)</ref>. Since unsupervised disentanglement in a generative model is impossible without incorporating inductive biases on both models and data <ref type="bibr" target="#b36">(Locatello et al., 2019)</ref>, new approaches are oriented towards weakly-supervised <ref type="bibr" target="#b37">(Locatello et al., 2020;</ref><ref type="bibr" target="#b51">Sadok et al., 2023)</ref> or semi-supervised learning <ref type="bibr" target="#b28">(Klys et al., 2018)</ref>. Because of their flexibility in modeling complex data, VAEs have been extended to various types of data, including multimodal or sequential data.</p><p>VAEs have gained significant interest in modeling multimodal data due to their several advantages compared to other generative models, especially generative adversarial networks (GANs) <ref type="bibr" target="#b17">(Goodfellow et al., 2014)</ref>. VAEs are equipped with encoder and decoder models, resulting in a more stable and faster training process than GANs, which makes them well-suited for multimodal generative modeling <ref type="bibr" target="#b57">(Suzuki &amp; Matsuo, 2022)</ref>. Several approaches have been developed to learn a joint latent space for multiple heterogeneous input data. For instance, PoE-VAE <ref type="bibr">(Wu &amp; Goodman, 2018)</ref> adopts the product of experts (PoE) <ref type="bibr" target="#b18">(Hinton, 2002)</ref> to model the posterior distribution of multimodal data while MoE-VAE <ref type="bibr" target="#b54">(Shi et al., 2019)</ref> uses a mixture of experts (MoE). Another approach <ref type="bibr" target="#b56">(Sutter et al., 2021)</ref> combines these methods for improved data reconstruction. Nevertheless, limitations have been demonstrated and formalized for these methods <ref type="bibr" target="#b11">(Daunhawer et al., 2021)</ref>. For example, multimodal VAE models often produce lower-quality reconstructions compared to unimodal VAE models, particularly for complex data. To address this issue and achieve better inference, many multimodal generative models now use hierarchical approaches to disentangle joint information from modality-specific information <ref type="bibr" target="#b21">(Hsu &amp; Glass, 2018;</ref><ref type="bibr" target="#b32">Lee &amp; Pavlovic, 2020;</ref><ref type="bibr" target="#b55">Sutter et al., 2020)</ref>.</p><p>Another area where VAE models have seen significant progress is in the modeling of sequential data, where the latent and/or observed variables evolve over time. Dynamical VAEs (DVAEs) <ref type="bibr" target="#b16">(Girin et al., 2021)</ref> aim to tackle high-dimensional complex data exhibiting temporal or spatial correlations using deep dynamical Bayesian networks. Recurrent neural networks are often used for this purpose, and a wide range of methods have been developed that differ in their inference and generative model structures. These DVAE models have two points in common when modeling sequential data: (i) unsupervised training is preserved, and (ii) the structure of the VAE is maintained; this means that the inference and generative models are jointly learned by maximizing a lower bound of the log-marginal </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variable notation Definition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T, t</head><p>Sequence length and time/frame index x (a) ∈ R da×T Observed audio data sequence</p><formula xml:id="formula_0">x (v) ∈ R dv×T Observed visual data sequence w ∈ R w Latent static audiovisual vector z (av) ∈ R lav×T Latent dynamical audiovisual vectors z (a) ∈ R la×T Latent dynamical audio vectors z (v) ∈ R lv×T</formula><p>Latent dynamical visual vectors z = {z (a) , z (v) , z (av) , w} Set of all latent variables x = {x (a) , x (v) } Set of all observations likelihood (ELBO). Of particular interest to the present paper is the disentangled sequential autoencoder (DSAE) <ref type="bibr" target="#b33">(Li &amp; Mandt, 2018)</ref>, which separates dynamical from static latent information. While many extensions of the VAE have been proposed to handle either multimodal or sequential data, none have been able to process both types of data simultaneously. This paper presents a novel approach for modeling multimodal and sequential data in a single framework, specifically applied to audiovisual speech data. We propose the first unsupervised generative model of multimodal and sequential data, to learn a hierarchical latent space that separates static from dynamical information and modalitycommon from modality-specific information. The proposed model, called Multimodal Dynamical VAE (MDVAE), is trained on an expressive audiovisual speech database and evaluated on three tasks: the transformation of audiovisual speech data, audiovisual facial image denoising, and audiovisual speech emotion recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Multimodal Dynamical VAE</head><p>This section presents the design and architecture of MD-VAE. Initially, we motivate the structure of the MDVAE latent space from the perspective of audiovisual speech generative modeling. Subsequently, we formalize the MDVAE generative and inference models. Finally, we introduce a two-stage training approach for unsupervised learning of the MDVAE model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Motivation and notations</head><p>Our goal is to model emotional audiovisual speech at the utterance level, where a single speaker speaks and expresses a single emotion. Let {x (a) , x (v) } denote the observed audiovisual speech data, where x (a) ∈ R da×T is a sequence of audio features of dimension d a , x (v) ∈ R dv×T is a sequence of observed visual features of dimension d v , and T is the sequence length. For the audio speech, features are extracted from the power spectrogram of the signal, and for the visual speech, features are extracted from the pre-processed face images. The feature extraction process will be further discussed below.</p><p>To motivate the structure of the generative model in MDVAE, let us reason about the latent factors involved in generating an emotional audiovisual speech sequence. First, we have the speaker's identity and global emotional state that correspond to static and audiovisual latent factors. Indeed, these do not evolve with time at the utterance level, and they are shared between the two modalities as defined from both vocal and visual attributes (e.g., the average pitch and timbre of the voice and the visual appearance). Second, we have dynamical latent factors that are shared between the two modalities, so audiovisual factors that vary with time. This typically corresponds to the phonemic information carried by the movements of the speech articulators that are visible in the visual modality, namely the jaw and lips. Finally, we have dynamical latent factors that are specific to each modality. Visual-only dynamical factors include, for instance, facial movements that are not related to the mouth region and the head pose. Audio-only dynamical factors include the pitch variations, induced by the vibration of the vocal folds, and the phonemic information carried by the tongue movements, which is another important speech articulator that is not visible in the visual modality.</p><p>This analysis of the latent factors involved in the generative process of emotional audiovisual speech suggests structuring the latent space of the MDVAE model by introducing the following latent variables: w ∈ R w is a static latent variable assumed to encode audiovisual information that does not evolve with time; z (av) ∈ R lav×T is a dynamical (i.e., sequential) latent variable assumed to encode audiovisual information that evolves with time; z (a) ∈ R la×T is a dynamical latent variable assumed to encode audio-only information; z (v) ∈ R lv×T is a dynamical latent variable assumed to encode visual-only information. A time/frame index t ∈ {1, 2, ..., T } is added in subscript of dynamical variables to denote one particular frame within a sequence (i.e., x</p><formula xml:id="formula_1">(a) t , x (v) t , z (a) t , z (v) t , z (av) t</formula><p>). The above notations are summarized in Table <ref type="table" target="#tab_0">1</ref>. Note that for the specific modeling of audiovisual speech data, it is not particularly relevant to introduce static latent variables that are specific to each modality. Indeed, as above-described, the static information is here assumed to encode the speaker's identity and global emotional state, which are fundamentally multimodal factors. Nevertheless, for other types of data, we can readily envision using two distinct static latent variables for each modality. The methodology developed in the subsequent sections could be straightforwardly extended to this case.</p><p>In summary, the MDVAE model is a generative model of audiovisual speech data {x (a) , x (v) } that involves four different latent variables {w, z (av) , z (a) , z (v) }. In the latent space of MDVAE, we can dissociate the latent factors that are static (w) from those that are dynamic (z (av) , z (v) , z (a) ), and we can dissociate the latent factors that are shared between the modalities (w, z (av) ) from those that are specific to each modality (z (a) , z (v) ). Note that a study by Gao and Shinkareva <ref type="bibr" target="#b14">(Gao &amp; Shinkareva, 2021)</ref> recently showed that the human brain distinguishes between modality-common and modality-specific information for affective processing in a multimodal context. In the MDVAE model, we also introduce temporal modeling on top of this dichotomy regarding modality-common vs modality-specific information. Our objective is to learn a multimodal and dynamical VAE than can disentangle the above-mentioned latent factors in an unsupervised manner for the analysis and transformation of emotional audiovisual speech data. In the next subsections, we detail the generative and inference models of MDVAE and its two-stage training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Generative model</head><p>The generative model of MDVAE is represented as a Bayesian network in Figure <ref type="figure" target="#fig_0">1</ref>, which also corresponds to the following factorization of the joint distribution of the observed and latent variables: a) |w, z (av) , z (a) p θ x (v) |w, z (av) , z (v)   × p (w) p θ z (av) p θ z (a) p θ z (v) , (1) where x = {x (a) , x (v) }, z = {z (a) , z (v) , z (av) , w}, and</p><formula xml:id="formula_2">p θ (x, z) = p θ x (</formula><formula xml:id="formula_3">p θ x (a) |w, z (av) , z (a) = T t=1 p θ x (a) t |w, z (av) t , z (a) t ; (2) p θ x (v) |w, z (av) , z (v) = T t=1 p θ x (v) t |w, z (av) t , z (v) t ; (3) p(z (av) ) = T t=1 p θ z (av) t |z (av) 1:t-1 ; (4) p(z (a) ) = T t=1 p θ z (a) t |z (a) 1:t-1 ; (5) p(z (v) ) = T t=1 p θ z (v) t |z (v) 1:t-1 .<label>(6)</label></formula><p>Equation (2) (resp. equation ( <ref type="formula">3</ref>)) indicates that, at time index t, the observed audio (resp. visual) speech  ), and the audio-only (resp. visual-only) dynamical latent variable at time index t (z</p><formula xml:id="formula_4">(a) t , resp. z (v) t ).</formula><p>In particular, we see that w is involved in the generation of the complete audiovisual speech sequence (x (a) , x (v) ). All latent variables are assumed independent, and the autoregressive structure of the priors for the dynamical variables in equations ( <ref type="formula">4</ref>)-( <ref type="formula" target="#formula_3">6</ref>) is inspired by DSAE <ref type="bibr" target="#b33">(Li &amp; Mandt, 2018)</ref>. Following standard DVAEs <ref type="bibr" target="#b16">(Girin et al., 2021)</ref>, each conditional distribution that appears in a product over the time indices in equations ( <ref type="formula">2</ref>)-( <ref type="formula" target="#formula_3">6</ref>) is modeled as a Gaussian with a diagonal covariance, and its parameters are provided by deep neural networks (decoders) that take as input the variables after the conditioning bars. For the distributions in equations ( <ref type="formula">2</ref>)-(3), the variance coefficients are fixed to one, while for the distributions in equations ( <ref type="formula">4</ref>)-( <ref type="formula" target="#formula_3">6</ref>), the variance coefficients are learned. Standard feed-forward fully-connected neural networks can be used for parametrizing the conditional distributions over the observed audiovisual speech variables. The autoregressive structure of the priors over the latent dynamical variables requires the use of RNNs. Finally, the prior over the static latent variable w is a Gaussian with zero mean and identity covariance matrix. More details about the decoder network architectures can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Inference model</head><p>As in the standard VAE, the exact posterior distribution of the latent variables in the MDVAE model is intractable, we thus need to define an inference model q ϕ (z|x) ≈ p θ (z|x). However, it is not because the exact posterior distribution is intractable that we cannot look at the structure of the exact posterior dependencies. Actually, using the Bayesian network of the model, the chain rule of probabilities, and D-separation <ref type="bibr" target="#b15">(Geiger et al., 1990;</ref><ref type="bibr" target="#b6">Bishop &amp; Nasrabadi, 2006)</ref>, it is possible to analyze how the observed and latent variables depend on each other in the exact posterior, and define an inference model with the same dependencies. An extensive discussion of D-separation in the context of DVAEs can be found in <ref type="bibr" target="#b16">(Girin et al., 2021)</ref>. The Bayesian network corresponding to our MDVAE model is represented in Figure <ref type="figure" target="#fig_0">1</ref>. For this model, it is relevant to factorize the inference model as follows:</p><p>q ϕ (z|x) = q ϕ w|x (a) , x (v) q ϕ z (av) |x (a) , x (v) , w × q ϕ z (a) |x (a) , z (av) , w q ϕ z (v) |x (v) , z (av) , w , (7) where</p><formula xml:id="formula_5">q ϕ z (av) |x (a) , x (v) , w = T t=1 q ϕ z (av) t | z (av) 1:t-1 , x (a) t:T , x (v) t:T , w ; (8) q ϕ z (a) |x (a) , z (av) , w = T t=1 q ϕ z (a) t | z (a) 1:t-1 , x (a) t:T , z (av) t , w ; (9) q ϕ z (v) |x (v) , z (av) , w = T t=1 q ϕ z (v) t | z (v) 1:t-1 , x (v) t:T , z (av) t , w . (<label>10</label></formula><formula xml:id="formula_6">)</formula><p>This factorization is consistent with the exact posterior dependencies between the latent and observed variables, i.e., no approximation was made as we followed the principle of D-separation. However, to lighten the inference model architecture, we choose to omit the non-causal dependencies on the observations in equation ( <ref type="formula">8</ref>), equation ( <ref type="formula">9</ref>) and equation <ref type="bibr">(10)</ref>. In these equations, we thus replace x t , and the equalities become approximations. In this inference model, q ϕ w|x (a) , x (v) and each conditional distribution that appears in a product over the time indices in equations ( <ref type="formula">8</ref>)-( <ref type="formula" target="#formula_5">10</ref>) is modeled as a Gaussian with a diagonal covariance, and its parameters (mean vector and variance coefficients) are provided by deep neural networks (encoders) that take as input the variables after the conditioning bars. In practice, the MD-VAE encoder can be decomposed into four sub-encoders, each dedicated to the inference of a specific latent variable. Distinct conditioning variables are concatenated at the input of these sub-encoders depending on the structure of the corresponding inference model. For instance, when inferring w we concatenate x (a) and x (v) along the feature dimension. More details about the encoder network architectures can be found in Appendix A.</p><p>The probabilistic graphical model of MDVAE during inference is represented in Figure <ref type="figure" target="#fig_1">2</ref>, corresponding to the factorization in equation ( <ref type="formula">7</ref>). It can be interpreted as follows: First, we infer the static audiovisual latent variable w from the observed audiovisual speech sequence, which corresponds to the computation of q ϕ (w|x (a) , x (v) ). Next, we infer the audiovisual dynamical latent variable z (av)  from the previously inferred variable w and the observed audiovisual speech, which corresponds to the computation of q ϕ (z (av) |x (a) , x (v) , w). Indeed, we need the static audiovisual information to infer the dynamical audiovisual information from the audiovisual speech observations. Finally, we infer the audio-only (resp. visual-only) dynamical latent variables z (a) (resp. z (v) ) from the audio (resp. visual) speech observations x (a) (resp. x (v) ) and the previously inferred audiovisual latent variables w and z (av) , which corresponds to the computation of q ϕ (z (a) |x (a) , z (av) , w) (resp. q ϕ (z (v) |x (v) , z (av) , w)). This is logical, as to infer the latent information that is specific to one modality, we require the observations of that modality and also the latent information that is shared with the other modality, which is captured by w and z (av) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Training</head><p>As in standard (D)VAEs <ref type="bibr" target="#b26">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b48">Rezende et al., 2014;</ref><ref type="bibr" target="#b16">Girin et al., 2021)</ref>, learning the MD-VAE generative and inference model parameters consists in maximizing the evidence lower-bound (ELBO):</p><formula xml:id="formula_7">L(θ, ϕ) = E q ϕ (z|x) [ln p θ (x | z)] -D KL (q ϕ (z | x) ∥ p θ (z)) . (<label>11</label></formula><formula xml:id="formula_8">)</formula><p>where D KL is the Kullback-Leibler divergence, defined by</p><formula xml:id="formula_9">D KL (q ∥ p) = E q [ln q -ln p].</formula><p>The first term in equation ( <ref type="formula" target="#formula_7">11</ref>) is the reconstruction accuracy term, which aims to maximize la data log-likelihood over a training dataset. These input and output data can take any form, including raw images for the visual modality and speech power spectra for the audio modality, or can be replaced by any representation from another pre-trained model. The second term is the latent space regularization term, which encourages the latent variables to conform to the prior distribution. Using equations ( <ref type="formula">1</ref>) and ( <ref type="formula">7</ref>), the ELBO can be further developed as follows: v)  .</p><formula xml:id="formula_10">L(θ, ϕ) = E q ϕ (z|x) ln p θ x (a) , x (v) | w, z (av) , z (a) , z (v) -D KL (q ϕ (w|x (a) , x (v) ) ∥ p θ (w)) -E q ϕ (z|x) D KL q ϕ z (av) |x (a) , x (v) , w ∥ p θ z (av) -E q ϕ (z|x) D KL q ϕ z (a) |x (a) , w, z (av) ∥ p θ z (a) -E q ϕ (z|x) D KL q ϕ z (v) |x (v) , w, z (av) ∥ p θ z (</formula><p>(12)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Two-stage training</head><p>Unlike GANs <ref type="bibr" target="#b17">(Goodfellow et al., 2014)</ref>, VAEs often produce poor reconstructions that lack realism, and this also affects the generation of new data. Improving the quality of VAE reconstruction or generation is an active area of research. One issue with VAE is that using an information bottleneck in combination with a pixel-wise reconstruction error can result in blurry, unrealistic images. This problem also exists with the audio modality, where VAEgenerated sound is often unnatural, mainly when using a time-frequency representation. To address this problem, several solutions have been proposed. One approach is to combine VAEs and GANs, where the discriminator replaces the standard reconstruction error and provides improved realism <ref type="bibr" target="#b29">(Larsen et al., 2016)</ref>. Another solution is to build </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDVAE decoder</head><p>Step 1</p><p>Step 1</p><p>Step 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VQ-VAE decoder</head><p>Step 2</p><p>Step 2 a hierarchical VAE, with a more complex structure for the latent space <ref type="bibr" target="#b61">(Vahdat &amp; Kautz, 2020)</ref>. Other methods incorporate regularization techniques, such as using a perceptual loss for the image modality, to ensure that VAE outputs have similar deep features to their corresponding inputs <ref type="bibr" target="#b45">(Pihlgren et al., 2020;</ref><ref type="bibr" target="#b20">Hou et al., 2019)</ref>. In this work, we focus on using a vector quantized VAE (VQ-VAE) model <ref type="bibr" target="#b62">(Van Den Oord et al., 2017)</ref>, which is a deterministic autoencoder with a discrete latent space. In the VQ-VAE, the continuous latent vector provided by the encoder is quantized using a discrete codebook before being fed to the decoder network. The codebook is jointly learned with the network architecture. The VQ-VAE model has been shown to produce higher-quality generations than VAEs or GANs <ref type="bibr" target="#b47">(Razavi et al., 2019)</ref>. Therefore, as illustrated in Figure <ref type="figure" target="#fig_5">3</ref>, we propose a two-stage training approach of the MDVAE model to improve its reconstruction and generation quality.</p><p>The first stage involves learning a VQ-VAE model independently on the visual and audio modalities and without temporal modeling. The training procedure of the VQ-VAEs, including the loss functions, is the same as originally proposed in <ref type="bibr" target="#b62">(Van Den Oord et al., 2017)</ref>, using an exponential moving average for the codebook updates. The VQ-VAE loss function includes a reconstruction term, which corresponds to the pixel-wise mean squared error for the visual modality and to the Itakura-Saito divergence <ref type="bibr" target="#b13">(Févotte et al., 2009)</ref> for the audio modality. In the second stage, we learn the MDVAE model on the continuous representations obtained from the pre-trained VQ-VAE encoders before quantization, instead of working directly on the raw audiovisual speech data. The disentanglement between static versus dynamic and modality-specific versus audiovisual latent factors occurs during this second training stage. This is because the VQ-VAEs are learned independently on each modality and without temporal modeling. To reconstruct the data, the continuous representations from the MDVAE are quantized and decoded by the pre-trained VQ-VAE decoders. This approach will be referred to as VQ-MDVAE in the following.</p><p>The first stage of this two-stage approach can be seen as learning audiovisual speech features in an unsupervised manner using a VQ-VAE. This feature extraction procedure is pseudo-invertible, as we can go from the raw data to the features with the VQ-VAE encoder and from the features to the raw data with the VQ-VAE decoder.</p><p>Beyond the reconstruction quality of the audiovisual speech data, another interest of the proposed two-stage training procedure is that it allows us to distribute the GPU memory usage between the two training stages, which is particularly useful when working with limited computational resources. During the first training stage, we train small models (fully convolutional VQ-VAEs) on high-dimensional data, corresponding to the raw audio and visual speech data. Processing modalities and time frames independently along with defining models of reasonable size allows us to efficiently manage the GPU memory to create large batches containing the raw audio or visual speech data. During the second training stage, we train a large model (MDVAE) on low-dimensional data, corresponding to the compressed audio and visual latent representations provided by the pre-trained VQ-VAEs before quantization. The effective compression of the audiovisual speech data using the VQ-VAEs allows us to increase the model capacity for the second training stage, which is required by the complexity of the learning task, and at the same time to keep a sufficiently large batch size. In summary, the two-stage training strategy decomposes the difficult problem of learning from high-dimensional multimodal and sequential data into two smaller sub-problems. The first sub-problem deals with compressing the data and ensuring a good reconstruction quality, and the second sub-problem deals with modality fusion and temporal modeling. Overall, this two-stage training leads to good reconstruction quality, efficient memory management, and accelerated training speed. Note that from a practical perspective, one could learn the VQ-VAE and MDVAE models jointly, from scratch, given sufficient computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments on audiovisual speech</head><p>This section presents three sets of experiments conducted with the VQ-MDVAE model for audiovisual speech processing. First, we analyze qualitatively and quantitatively the learned representations by manipulating audiovisual speech sequences in the MDVAE latent space. Second, we explore the use of the VQ-MDVAE model for audiovisual facial image denoising, showing that the model effectively exploits the audio modality to reconstruct facial images where the mouth region is corrupted. Finally, we show that using the static audiovisual latent representation learned by the VQ-MDVAE model leads to state-of-the-art results for audiovisual speech emotion recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Expressive audiovisual speech dataset</head><p>The VQ-MDVAE model is trained on the multi-view emotional audiovisual dataset (MEAD) <ref type="bibr" target="#b63">(Wang et al., 2020)</ref>. It contains talking faces comprising 60 actors and actresses speaking with eight different emotions at three levels of intensity. We keep only the frontal view for the visual modality. 75%, 15%, and 10% of the dataset are used respectively for the training, validation, and test, with different speakers in each split. This corresponds to approximately 25h, 5h, and 3h of audiovisual speech, respectively. For the visual modality, face images in the MEAD dataset are cropped, resized to a 64x64 resolution, and aligned using Openface <ref type="bibr">(Baltrušaitis et al., 2016)</ref>. For the audio modality, power spectrograms are computed using the short-time Fourier transform (STFT). The STFT parameters are chosen such that the audio frame rate is equal to the visual frame rate (30 fps), which leads to an STFT analysis window length of 64 ms (1024 samples at 16 kHz) and a hop size of 52.08% of the window length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training VQ-MDVAE</head><p>The architecture of the MDVAE and VQ-VAE models are described in detail in Appendix A. This section only provides an overview of the training pipeline.</p><p>The pre-processed facial images and the power spectrograms are used to train the visual and audio VQ-VAEs, respectively. The two VQ-VAEs do not include any temporal model, i.e., the audio and visual frames of an audiovisual speech sequence are processed independently. The VQ-VAE for the visual modality takes as input and outputs an RGB image of dimension 64 × 64 × 3. This image is mapped by the encoder to a latent representation corresponding to a 2D grid of 8 × 8 codebook vectors of dimension 32. The visual codebook contains a total number of 512 vectors. The VQ-VAE for the audio modality takes as input and outputs a speech power spectrum of dimension 513. This power spectrum is mapped by the encoder to a latent representation corresponding to a 1D grid of 64 codebook vectors of dimension 8. The audio codebook contains a total number of 128 vectors. The VQ-VAEs consist of convolutional layers for both the visual and audio modalities. Since the quantization operation is non-differentiable, the codebooks for each modality are learned using the stop gradient trick <ref type="bibr" target="#b62">(Van Den Oord et al., 2017)</ref>.</p><p>The audio and visual observed data x (a) ∈ R da×T and x (v) ∈ R dv×T that are used to train the MDVAE model are taken from the flattened output of the pre-trained and frozen VQ-VAE encoders before quantization, with d a = 512 (64 × 8) and d v = 2048 (8 × 8 × 32). The sequence length is fixed to T = 30 for training. The MDVAE model is composed of dense and recurrent layers. The dimensions of the latent variables in the VQ-MDVAE model are as follows: the static latent vector (w ∈ R w ) has a dimension of w = 84, the audiovisual dynamical latent vectors (z (av) ∈ R lav×T ) have a dimension of l av = 16, and both the audio and visual dynamical latent vectors (z (v) ∈ R lv×T , z (a) ∈ R la×T ) have a dimension of l v = l a = 8. The models are trained using the Adam optimizer <ref type="bibr" target="#b27">(Kingma &amp; Ba, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analysis-resynthesis</head><p>We first present the results of an analysis-resynthesis process on the audiovisual speech data. The analysis step involves performing inference on audiovisual speech sequences that were not seen during training to obtain the latent vectors, while the resynthesis step involves generating the sequence from the obtained latent vectors without any modification, with the goal of faithfully reconstructing the input sequence. Methods For this experiment, we compare MDVAE and VQ-MDVAE to VQ-VAE <ref type="bibr" target="#b62">(Van Den Oord et al., 2017)</ref> and DSAE <ref type="bibr" target="#b33">(Li &amp; Mandt, 2018)</ref>, which are unimodal generative models. DSAE also includes a temporal model that separates sequential information from static information. The VQ-VAE does not include any temporal model. The VQ-VAE and DSAE are both trained separately on the audio and visual modalities. For a fair comparison, we consider the original DSAE and its improved version VQ-DSAE obtained by training the model in two stages like VQ-MDVAE (see Section 2.5). This experimental comparison therefore corresponds to an ablation study: If we take VQ-MDVAE and remove the multimodal modeling we obtain VQ-DSAE. If we further remove the temporal model we obtain VQ-VAE. It will also allow us to assess the impact of the proposed two-stage training process on both DSAE and MDVAE. Evaluation metrics The average quality performance for the speech and visual modalities is evaluated using the MEAD test dataset. Four metrics are used to assess the quality of the resynthesized audio speech data:</p><p>• The Short-Time Objective Intelligibility (STOI) measure is an intrusive metric (i.e., it requires the original reference speech signal) that assesses how intelligible the resynthesized speech is <ref type="bibr" target="#b59">(Taal et al., 2010)</ref>;</p><p>• The Perceptual Evaluation of Speech Quality (PESQ) measure is an intrusive metric that evaluates the perceived quality of the resynthesized speech <ref type="bibr" target="#b49">(Rix et al., 2001)</ref>. It accounts for factors like distortion, noise, and other artifacts that can affect the overall perceived quality;</p><p>• The Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) is an intrusive metric defined as the power ratio between the original speech signal and the distortion caused by the resynthesis process <ref type="bibr" target="#b31">(Le Roux et al., 2019)</ref>; it is made invariant to signal amplitude rescaling;</p><p>• MOSnet is a learning-based non-intrusive metric that predicts human-rated quality scores for speech <ref type="bibr" target="#b35">(Lo et al., 2019)</ref>.</p><p>Four metrics are also used to assess the quality of the resynthesized visual data:</p><p>• The Mean Square Error (MSE) computes the average squared difference between the pixel values of the original and resynthesized visual data;</p><p>• The Peak Signal-to-Noise Ratio (PSNR) considers both the image fidelity and the level of noise or distortion introduced during resynthesis;</p><p>• The Spatial Correlation Coefficient (SCC) evaluates how well the structures and patterns in the images match using the correlation;</p><p>• The Structural Similarity Index Measure (SSIM) assesses the structural similarity between the original and resynthesized images. It takes into account luminance, contrast, and structure, providing a comprehensive measure of image quality <ref type="bibr" target="#b64">(Wang et al., 2004)</ref>.</p><p>Discussions Tables <ref type="table" target="#tab_4">2</ref> and<ref type="table" target="#tab_2">3</ref> respectively show the reconstruction quality of the audio and visual modalities for this analysis-resynthesis experiment. The proposed VQ-MDVAE method outperforms MDVAE alone, as evidenced by the improvement of 0.03, 0.47, 1.19, and 4.64 for STOI, PESQ, MOSnet, and SI-SDR, respectively, for the audio modality. Similarly, for the visual modality, VQ-MDVAE yields a gain of 6.5, 0.1, and 0.26 for PSNR, SCC, and SSIM, respectively. These results validate the proposed two-step training approach, demonstrating a significant improvement in reconstruction quality. This is confirmed when comparing the results of DSAE with those of VQ-DSAE.</p><p>In addition, for both modalities, MDVAE and VQ-MDVAE outperform DSAE and VQ-DSAE, respectively. However, the proposed method (VQ-MDVAE) shows a decrease in reconstruction quality compared to using the VQ-VAE alone, especially for the PESQ metric. This can be attributed to the fact that the VQ-MDVAE, with its temporal dependencies, acts as a temporal filter. Despite this, we can leverage these temporal dependencies and the hierarchy provided by the MDVAE model for other applications, as discussed in the following sections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Analysis-transformation-synthesis</head><p>This section aims to analyze the latent representations learned by the MDVAE model. We want to study what high-level characteristics of audiovisual speech are encoded in the different latent variables of the model. The experiments involve exchanging latent variables between a sequence named (A) and sequences named (B) through an analysis-transformation-synthesis process. The analysis step involves performing inference separately on two audiovisual speech sequences (A) and (B). Then, the values of certain latent variables from (A) are replaced with the values of the same latent variable from (B). Finally, the output sequence is reconstructed from the combined set of latent variables. The resulting sequence is expected to be a mixed sequence whose features correspond to sequence (A) for the unmodified latent variables and sequence (B) for the modified latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Qualitative results</head><p>Visual modality Figure <ref type="figure" target="#fig_6">4</ref> illustrates visual sequences generated using the analysis-transformation-synthesis method, each accompanied by two curves representing the intensity of two facial action units (AUs), namely jaw drop (AU 26 ) and eyes closed (AU 43 ), plotted as a function of the frame index. AUs are the smallest components of facial expression, involving coordinated contractions of facial muscles that produce recognizable and measurable changes in the face <ref type="bibr" target="#b12">(Ekman &amp; Friesen, 1978)</ref>. These AUs were extracted from the visual sequences using Py-Feat <ref type="bibr" target="#b39">(Muhammod et al., 2019)</ref>. The top two sequences depict original visual sequences of different subjects exhibiting varying facial expressions. Conversely, the bottom sequences display the results when the variable w values are swapped between the two original sequences. We observe that the bottom-left sequence has the same facial movements as the top-left sequence, but the speaker identity is that of the top-right sequence. The curves of AU 43 and AU26   for the bottom-left sequence are similar to those of the top-left sequence. A noticeable blink of the eyes occurs between frames 26 and 28, which is depicted by a peak in the AU 43 curve. Similarly, the bottom-right sequence has the same facial movements as the top-right sequence, but the speaker identity is that of the top-left sequence. This disentanglement of dynamic facial movements from static speaker identity reveals that w encodes the visual identity of the speaker, among other information.</p><p>Figure <ref type="figure" target="#fig_7">5</ref> illustrates what other latent variables encode using the analysis-transformation-synthesis method. The figure shows three sequences of visual data, labeled as sequence (a) in the green box and sequences (b) and (c) in the blue box. First, two sequences on the left are reconstructed by combining z (av) of sequence (a) with w and z (v)  of sequences <ref type="bibr">(b)</ref> and <ref type="bibr">(c)</ref>. The speaker identity of sequences <ref type="bibr">(b)</ref> and (c) is preserved in the output sequences, but the movement of the lips follows that of sequence (a). This shows that z (av) encodes the lip movement. Second, two other sequences on the right are reconstructed by combining z (v) of sequence (a) with w and z (av) of sequences <ref type="bibr">(b)</ref> and <ref type="bibr">(c)</ref>. The speaker identity and the movement of the lips of sequences <ref type="bibr">(b)</ref> and <ref type="bibr">(c)</ref> are preserved in the output  sequences, but the movement of the eyes and eyelids (e.g., the blink of the eyes, as seen in the red rectangle) follows that of sequence (a). This indicates that z (v) encodes eye and eyelid movements. It also appears that the head orientation in the bottom right output sequence is different from that of the original sequence (c), which was not the case for the bottom left output sequence. This indicates that z (v) also encodes the head pose. From this example, we can also confirm that w encodes the speaker's identity.</p><p>Figure <ref type="figure" target="#fig_8">6</ref> shows that w also encodes the global emotional state. Each line in the figure is a reconstruction created by combining the dynamical latent variables of the sequence labeled as neutral in terms of emotion (first row) with w of other sequences of the same person labeled with different emotions (from top to bottom: fear, sad, surprised, angry, and happy). The emotion changes between the different rows, but the visual dynamics remain the same as in the row, indicating that the static audiovisual variable w encodes both the identity and the global emotion in the input sequence. Audio modality As for the visual modality, Figure <ref type="figure" target="#fig_10">7</ref> illustrates audio sequences (speech power spectrograms) generated using the analysis-transformation-synthesis method. In this figure, sequence (a) (green box) represents the power spectrogram and the pitch contour of a speech signal spoken by a male speaker, and sequence (b) (blue box) represents the power spectrogram and the pitch contour of a speech signal spoken by a female speaker. The pitch contour is extracted using CREPE <ref type="bibr" target="#b24">(Kim et al., 2018)</ref>. The generated spectrogram (1) (top left) is derived from w of sequence (a), and the dynamical latent variables z (av) , z (a) of sequence <ref type="bibr">(b)</ref>. Comparing the resulting spectrogram with that of sequence <ref type="bibr">(b)</ref>, we can deduce that they have the same phonemic structure, but the pitch has been shifted downwards, as can be seen from the pitch contour and the spacing between the harmonics. Similarly, the reconstructed spectrogram (2) (bottom left) is derived from w of sequence <ref type="bibr">(b)</ref>, and the dynamical latent variables z (av) , z (a)  are from the sequence (a). Here, we notice that the pitch shifts upwards while preserving the phonemic structure of sequence (a). Therefore, the static latent variable w encodes the average pitch value related to the speaker's identity. The generated spectrograms (3) (top right) and ( <ref type="formula">4</ref>) (bottom right) reveal that the dynamical latent variables z (a) and z (av) have distinct roles in capturing the phonemic content. Specifically, z (a) predominantly captures the high frequency, while z (av) encodes the low frequency, which also corresponds to the lower formants. This finding is noteworthy as research has shown that the lower formants are highly correlated with the lip configuration <ref type="bibr" target="#b1">(Arnela et al., 2016)</ref>. Moreover, it is particularly interesting that the two correlated factors (lower formants and lip movements) are found in the same latent dynamical variable, z (av) , especially since the MDVAE was trained in an unsupervised manner. Additional qualitative results Additional qualitative results, such as audiovisual animations, analysis-transformationsynthesis, interpolation on the static latent space, and au- diovisual speech generation conditioned on specific latent variables, can be found at <ref type="url" target="https://samsad35.github.io/site-mdvae/">https://samsad35.github.io/ site-mdvae/</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">Quantitative Results</head><p>The aim of this section is to complement the above qualitative analysis with quantitative metrics by measuring the ability of the VQ-MDVAE model to modify facial and vocal attributes through manipulations of the different latent variables. Experimental setup and metrics The evaluation protocol for this experiment involves using a sequence (labeled as (A)) and 50 other sequences selected randomly from the test dataset (labeled as (B)). The protocol is based on the analysis-transformation-synthesis framework described in Section 3.4. It involves reconstructing sequences (B) using one of the latent variables (among {w, z (av) , z (a) , z (v) }) taken from the sequence (A) and comparing audio and visual attributes extracted from the output sequences to the same attributes extracted from the original sequence (A). This comparison is done using the mean absolute error (MAE) and the Pearson correlation coefficient (PCC). If the MAE metric (resp. the PCC metric) is low (resp. high) for the swapping of a given latent variable, it indicates that the attribute was transferred from the sequence (A) to sequences (B); the swapped variable thus encodes the attribute. For the visual modality, the attributes being considered include the action units (ranging from 0 (not activated) to 1 (very activated)), the angle of the gaze, and the head pose. These factors are estimated using Py-Feat <ref type="bibr" target="#b39">(Muhammod et al., 2019)</ref> and Openface <ref type="bibr">(Baltrušaitis et al., 2016)</ref>. For the audio modality, we consider the first two formant frequencies (in Hz) and the pitch (in Hz), estimated using Praat <ref type="bibr" target="#b7">(Boersma &amp; Weenink, 2021)</ref> and CREPE <ref type="bibr" target="#b24">(Kim et al., 2018)</ref>. Note that all these attributes are time-varying. The PCC is computed after centering the data (by subtracting the time average of the factor), which is not the case for the MAE. Therefore, contrary to the MAE, the PCC will not be affected by a time-invariant shift of the attribute. Discussion Figure <ref type="figure" target="#fig_11">8</ref> presents the average results obtained  by repeating the protocol 50 times, i.e., using 50 different sequences (A). From this figure, we draw four main conclusions. First, the action units related to the lips and jaw (lip press AU 24 , lip parts AU 25 , jaw drop AU 26 , and lip suction AU 28 ) and the first two formant frequencies all show high PCC values and low MAE values when performing transformations with the latent variable z (av) . It indicates that this audiovisual dynamical latent variable plays a significant role in globally controlling these factors. This is very interesting, considering that the lips and jaw are two important speech articulators whose movement induces variations of the shape of the vocal tract and thus also variations of the formant frequencies (the resonance frequencies of the vocal tract). The VQ-MDVAE model thus managed to encode highly-correlated visual and audio factors in the same audiovisual dynamical latent variable. Secondly, the pitch factor shows a high PCC value when manipulating the dynamical audiovisual latent variable z (av) . However, it shows a low MAE value when manipulating the static audiovisual latent variable w. This indicates that w encodes the average pitch value while z (av) captures the temporal variation of the pitch around this center value (we remind that the PCC is computed from centered data but not the MAE). The fluctuations in pitch around the average value are encoded in the audiovisual latent variable z (av) rather than the audio-specific one z (a) . This finding is supported by a recent study <ref type="bibr" target="#b5">(Berry et al., 2022)</ref> that demonstrates a significant correlation between pitch and the lowering of the jaw. Then, the action unit associated with the closing of the eyes (AU 43 ), the angle of the gaze as well as the pose of the head show a high PCC and low MAE when manipulating the visual dynamical latent variable z (v) . This suggests that z (v) plays a significant role in globally controlling the movement of the eyelids, the gaze, and the head movements. These factors are indeed much less correlated with the audio than the lip and jaw movements, which explains why they are encoded in the visual dynamical latent variable z (v) and not in the audiovisual dynamical latent variable z (av) . Finally, action units such as the inner brow raiser (AU 01 ), outer brow raiser (AU 02 ), upper lid raiser (AU 05 ), cheek raiser (AU 06 ), and lid tightener (AU 07 ) on one side, and nose wrinkler (AU 09 ), nasolabial deepener (AU 11 ), lip corner puller (AU 12 ), and dimpler (AU 14 ) on the other side, show high PCC values with respect to z (v)  and z (av) , respectively, but low MAE values with respect to w. We argue that this result is related to the encoding of the speaker's emotional state in the latent space of the VQ-MDVAE model. Indeed, we have shown qualitatively that the static audiovisual latent variable w encodes the global emotional state of a speaker, which explains why it also encodes the average activation level (as indicated by the low MAE values) of the above-mentioned action units that are important for emotions. In contrast, the dynamical latent variables z (av) and z (v) capture the temporal variations around this average value (as indicated by the high PCC values). As an illustration, we can think of an audiovisual speech utterance spoken by a happy speaker. The global emotional state (happy) would be encoded in w, leading to high constant average values of the cheek raiser (AU 06 ) and lip corner puller (AU 12 ) action units, and these values would be modulated temporally by the movement of the speech articulators, as encoded in z (av) .</p><p>In Section 3.4.1, we showed qualitatively that the static audiovisual latent variable w encodes the speaker's identity and global emotion. This paragraph aims to quantify this with two complementary approaches. The first approach operates in the reconstructed image space at the output of the VQ-MDVAE model, while the second approach operates in the latent space of the model. To investigate the emotions in the VQ-MDVAE output images, we randomly select an audiovisual sequence (A) from the test data that is labeled with a specific emotion. We then perturb the dynamical latent variables of (A) by replacing them with those of sequences (B) whose emotions are different from that of sequence (A), while keeping the static audiovisual latent variable w of sequence (A) unchanged. We evaluate the performance of an emotion classification model (ResMaskNet <ref type="bibr" target="#b44">(Pham et al., 2021)</ref>) on the VQ-MDVAE output images produced by this experiment and repeat the process 120 times for each emotion. The results are summarized in a confusion matrix shown in Figure <ref type="figure" target="#fig_13">9a</ref>. This matrix is mainly diagonal, indicating that as long as the static audiovisual latent variable w is not changed, the overall emotion is not changed. This is consistent with the discussion in the previous paragraph, where w was shown to control the average value of certain action units. In the second approach, we use the latent variable of the VQ-MDVAE model to recognize emotions and identities using a Support Vector Machine (SVM) classifier. The training and test datasets for the SVM comprised 70% and 30% of the combined test and validation data from the MEAD dataset, respectively. The dataset consisted of 11 speakers and included eight emotions. The performance accuracy for both classification tasks is shown in Figure <ref type="figure" target="#fig_13">9b</ref>, for different latent variables used as input to the classifier. The results show that emotional and identity information are encoded in the static audiovisual latent variable w, with 98% and 100% correct classification, respectively. Ap-pendix B contains visualizations of the static latent space, while the aforementioned companion website provides qualitative results of interpolations on w, demonstrating how we can modify the emotion within an audiovisual speech sequence without altering the identity, and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Audiovisual facial image denoising</head><p>Experimental set-up This section focuses on denoising audiovisual facial videos. The denoising approach consists of encoding and decoding corrupted visual speech sequences with autoencoder-based models (see next paragraph) pretrained on the clean MEAD dataset. We intentionally introduced two types of perturbations, strategically located around the eyes and mouth. Specifically, we chose to perturb the sequences using centered isotropic Gaussian noise, and we studied the impact of different levels of noise variance. Our analysis was performed on sequences consisting of ten images, where only the six central images were corrupted. Methods In this experiment, we compare the performance of VQ-MDVAE, which uses both audio and visual modalities and includes a hierarchical temporal model, with three other models: VQ-VAE <ref type="bibr" target="#b62">(Van Den Oord et al., 2017)</ref>, a unimodal model only trained on the visual modality and without temporal modeling; DSAE <ref type="bibr" target="#b33">(Li &amp; Mandt, 2018)</ref>, a unimodal model only trained on the visual modality and with the same temporal hierarchical model as the proposed VQ-MDVAE; and JointMVAE <ref type="bibr" target="#b58">(Suzuki et al., 2016)</ref>, a multimodal model without temporal modeling. To ensure a fair comparison, we trained the DSAE and JointMVAE models in two stages, similar to the VQ-MDVAE model. It is important to mention that the VQ-VAE used in this experiment is identical to the one used in VQ-MDVAE, VQ-DSAE, and VQ-JointMVAE. Metrics To evaluate the denoising performance, we consider again the PSNR and SSIM metrics. These are calculated on the corrupted region of the image, and provide a quantitative measure of the quality and similarity of the denoised image compared to the original. The higher the PSNR and SSIM values, the better the denoising performance. Discussion Figures 10 and 11 present the qualitative and quantitative results for the denoising experiment, respectively. The mean and standard deviation of the metrics computed over 200 test sequences for the mouth and eyes corruptions are shown in Figures <ref type="figure" target="#fig_18">11a</ref> and<ref type="figure" target="#fig_18">11b</ref>, respectively. Overall, the VQ-MDVAE, VQ-JointMVAE, and VQ-DSAE models outperform the VQ-VAE for both types of perturbations. In the case of mouth corruption, the VQ-MDVAE and VQ-JointMVAE models perform better than the unimodal VQ-DSAE model, demonstrating the benefit of multimodal modeling. These models use the audio modality to denoise the mouth, resulting in a notable 7 dB increase in PSNR at a variance of 10 for VQ-MDVAE compared to VQ-DSAE. As expected, the audio modality is less useful for denoising the eyes, resulting in a smaller advantage for multimodal models in this case. In fact, the     PSNR improvement with VQ-MDVAE for the corruption of the eyes is only 3 dB compared to the unimodal VQ-DSAE model. It can also be seen that VQ-MDVAE consistently outperforms VQ-JointMVAE, which shows the benefit of temporal modeling in multimodal models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Audiovisual speech emotion recognition</head><p>This section presents emotion recognition experiments based on the static audiovisual representation w learned by VQ-MDVAE in an unsupervised manner. We consider two problems: estimating the emotion category and the emotional intensity level. Experimental set-up We assess the effectiveness of the proposed model on two different datasets: MEAD <ref type="bibr" target="#b63">(Wang et al., 2020)</ref> and RAVDESS <ref type="bibr" target="#b34">(Livingstone &amp; Russo, 2018)</ref>. The MEAD dataset was presented in Section 3.1. The RAVDESS dataset contains 1440 audio files that were recorded by 24 professional actors, with each file labeled with one of eight different emotions: neutral, calm, happy, sad, angry, fearful, disgusted, or surprised. We conduct two types of evaluations to measure performance. The first evaluation involves recognizing emotions and their intensity levels in the case where individuals can be seen during the training phase (person-dependent evaluation). For this evaluation, we randomly divide the dataset into 70% training data and 30% testing data. The second evaluation involves recognizing emotions and their intensity levels in the case where individuals are not seen during the training phase (person-independent evaluation). To perform this evaluation, we use a 5-fold cross-validation approach to separate the speakers' identities between the training and evaluation sets. Through these evaluations, we are able to assess the ability of the models to detect emotions and their intensity levels in both person-dependent and person-independent scenarios using two different datasets. Methods We compare the performance of VQ-MDVAE with several methods from the literature. First, the VQ-DSAE-audio and VQ-DSAE-visual models, which correspond to the VQ-DSAE model already discussed in the previous experiments, here trained either on the audio modality or on the visual modality. We remind that VQ-DSAE is an improved version of DSAE <ref type="bibr" target="#b33">(Li &amp; Mandt, 2018)</ref> that uses the 2-stage training process proposed in the present paper. VQ-MDVAE can be seen as a multimodal extension of VQ-DSAE because both methods share the same hierarchical temporal model, including a static and a dynamical latent variable. Comparing VQ-MDVAE with the two VQ-DSAE models will thus allow us to fairly assess the benefit of a multimodal approach to emotion recognition. Second, the wav2vec model <ref type="bibr" target="#b52">(Schneider et al., 2019)</ref>, which is a self-supervised unimodal representation learning approach. Wav2vec is trained on the audio speech signals of the Librispeech dataset <ref type="bibr" target="#b42">(Panayotov et al., 2015)</ref>, which includes 960 hours of unlabeled speech data, with 2338 different speakers. Finally, we also include in this experiment two state-of-the-art supervised multimodal approaches <ref type="bibr" target="#b9">(Chumachenko et al., 2022;</ref><ref type="bibr" target="#b60">Tsai et al., 2019)</ref>, which are based on an audiovisual transformer architecture. The method of <ref type="bibr" target="#b9">Chumachenko et al. (2022)</ref> will be referred to as "AV transformer". It also relies on transfer learning using EfficientFace <ref type="bibr" target="#b67">(Zhao et al., 2021)</ref>, a model pre-trained on AffectNet <ref type="bibr" target="#b38">(Mollahosseini et al., 2017)</ref>, the largest dataset of in-the-wild facial images labeled in emotions. The method of <ref type="bibr" target="#b60">Tsai et al. (2019)</ref> will be referred to as "MULT" for multimodal transformer.</p><p>AV transformer and MULT are fully supervised, trained, and evaluated on RAVDESS. This contrasts with wav2vec, VQ-DSAE-audio, VQ-DSAE-visual, and VQ-MDVAE, which are pre-trained in a self-supervised or unsupervised manner and then used as frozen feature extractors to train a small classification model on top of the extracted representation of (audiovisual) speech. For VQ-DSAE-audio, VQ-DSAEvisual, and VQ-MDVAE, only the global latent variable (w) is fed to the classifier. For wav2vec, a temporal meanpooling layer is added before the classifier as in <ref type="bibr" target="#b43">(Pepino et al., 2021)</ref>. Depending on the feature extraction method and evaluation configuration (person independent or dependent), we consider different classification models: a simple multinomial logistic regression (MLR) implemented with a single linear layer followed by a softmax activation function, or a multilayer perceptron (referred to as MLP) with two hidden layers followed by a linear layer and a softmax activation function. In the person-dependent setting, we explore a third approach (referred to as DA + MLR) that involves transforming the test data using an unsupervised domain adaptation method (DA) before classification with the MLR model. Unsupervised domain adaptation is here used to compensate for the domain shift due to the fact that speakers are different in the training and testing sets. This is further discussed below. Discussion We start by comparing VQ-MDVAE with its two unimodal counterparts, VQ-DSAE-audio and VQ-DSAE-visual, for the emotion category classification task on the MEAD dataset. In Figure <ref type="figure" target="#fig_19">12</ref>, we show the classification accuracy as a function of the amount of labeled training data used to train the MLR classification model. VQ-MDVAE and the VQ-DSAE models are all pre-trained in an unsupervised manner on the MEAD dataset. Using the exact same experimental protocol, we observe that when using 100% of the labeled data the VQ-MDVAE model outperforms its two unimodal counterparts by about 50% of accuracy, which clearly demonstrates the interest of a multimodal approach to emotion recognition from latent representations learned with dynamical VAEs. Another interesting observation is that we need less than 10% of the labeled data to reach 90% of the maximal performance of the VQ-MDVAE model.</p><p>Table <ref type="table" target="#tab_3">4</ref> compares the emotion category and intensity level classification performance of the proposed VQ-MDVAE method and the previously mentioned methods from the literature. We report the accuracy (in %), defined as the ratio of correctly predicted instances to the total number of instances, and the F1-score (in %), defined as the harmonic mean of the precision and recall. For the person-dependent evaluation ("PD" section of the table), VQ-MDVAE demonstrates superior performance in recognizing emotion categories (resp. emotion levels) on the MEAD dataset, outperforming VQ-DSAE-audio by 57.8% (resp. 35.2%), VQ-DSAE-visual by 47.6% (resp. 40.6%), and wav2vec by 22% (resp. 28.5%) of accuracy. On the RAVDESS dataset, it can be observed that VQ-MDVAE pre-trained on MEAD and finetuned on RAVDESS (in an unsupervised manner) outperforms the fully-supervised state-of-the-art method <ref type="bibr" target="#b9">(Chumachenko et al., 2022)</ref> (AV transformer) by 0.2% of accuracy and 1.0% of F1-score. Note that AV transformer cannot be trained simultaneously on MEAD and RAVDESS because the emotion labels in these two datasets are different. On the contrary, the proposed VQ-MDVAE model can be pre-trained on any emotional audiovisual speech dataset, precisely because it is unsupervised. The learned representation can then be used to train a supervised classification model. This evaluation confirms that the static audiovisual latent variable w learned by the proposed VQ-MDVAE is an effective representation for audiovisual speech emotion recognition. Indeed, as shown in Figure <ref type="figure">B</ref>.14 of Appendix B, emotion categories and levels form distinct clusters in the static audiovisual latent space of the VQ-MDVAE model.</p><p>For the person-independent evaluation, we only compare VQ-MDVAE, wav2vec, MULT and AV transformer, as the person-dependent evaluation showed that VQ-MDVAE outperforms its two unimodal counterparts based on VQ-DSAE. Compared with the person-dependent setting, we  <ref type="bibr" target="#b65">(Wei et al., 2018;</ref><ref type="bibr" target="#b23">Kim &amp; Song, 2022)</ref>. To adapt our model to a new domain, we use optimal transport to map the probability distribution of the source domain (w of seen identities) to that of the target domain (w of unseen identities). This is accomplished by minimizing the earth mover's distance between the two distributions <ref type="bibr" target="#b10">(Courty et al., 2017)</ref>. By finding an optimal transport plan, we can transfer knowledge from the source domain to the target domain in an unsupervised manner (i.e., emotion labels are not used), resulting in a large improvement in accuracy for both the wav2vec and VQ-MDVAE models compared to when no domain adaptation is performed: +9.9% and +7.9% for emotion category classification on the MEAD and RAVDESS datasets with the VQ-MDVAE model (using the finetuned model for RAVDESS), and +2.6% and +2.1% with the wav2vec model. It can also be seen that the MLR linear classification model with domain adaptation is more effective than the MLP non-linear classification approach. Finally, for emotion category classification on RAVDESS, we see that the proposed VQ-MDVAE (finetuned) with domain adaptation and MLR outperforms the state-of-the-art fully-supervised AV transformer and MULT methods by 0.1% and 2.7% of accuracy. This is particularly interesting considering that most of the proposed model parameters have been learned in an unsupervised manner. Indeed, only the MLR classification model, which includes 680 (84 × 8 + 8) trainable parameters, is learned using labeled emotional audiovisual speech data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>Deep generative modeling is a powerful unsupervised learning paradigm that can be applied to many different types of data. In this paper, we proposed the VQ-MDVAE model to learn structured and interpretable representations of multimodal sequential data. A key to learn a meaningful representation in the proposed approach is to structure the latent space into different latent variables that disentangle static, dynamical, modality-specific and modality-common information. By defining appropriate probabilistic dependencies between the observed data and the latent variables, we were able to learn structured and interpretable representations in an unsupervised manner. Trained on an expressive audiovisual speech dataset, the same VQ-MDVAE model was used to address several tasks in audiovisual speech processing. This versatility contrasts with task-specific supervised models. The experiments have shown that the VQ-MDVAE model effectively combines the audio and visual information in static (w) and dynamical (z (av) ) audiovisual latent variables, while characteristics specific to each individual modality are encoded in dynamical modality-specific latent variables (z (a) and z (v) ). Indeed, we have shown that lip and jaw movements can be synthesized by transferring z (av) from one sequence to another, while preserving the speaker's identity, emotional state, and visual-only facial movements. For denoising, we have shown that the audio modality provides robustness with respect to the corruption of the visual modality on the mouth region. Finally, we proposed to use the static audiovisual latent variable w for emotion recognition. This approach was shown to be effective with only a few labeled data, and it obtained much better accuracy than unimodal baselines. Experimental results have also shown that the proposed unsupervised representation learning approach outperforms state-of-the-art fully-supervised emotion recognition methods based on an audiovisual transformer.</p><p>Unfortunately, the two modalities are not always available in audiovisual speech processing. For instance, the audio modality might be missing due to highly intrusive noise, and the visual modality might be missing due to low-lighting conditions. A robust multimodal information retrieval system should be able to handle such a situation where some modalities are temporarily missing. In the current configuration of the MDVAE model, the proposed approach relies on both modalities for inference of the latent variables. Nevertheless, MDVAE could be extended to accommodate single-modality inference using the "subsampled training" approach proposed in <ref type="bibr">(Wu &amp; Goodman, 2018)</ref>, or maybe using the multimodal masking strategies proposed in <ref type="bibr" target="#b2">(Bachmann et al., 2022)</ref>. Moreover, being able to infer all latent variables from one single modality would allow the model to be used for cross-modality generation, i.e., generating one modality given another.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: MDVAE generative probabilistic graphical model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: MDVAE inference probabilistic graphical model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>generated from the audiovisual static latent variable (w), the audiovisual dynamical latent variable at time index t (z (av) t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The overall architecture of VQ-MDVAE. During the first step of the training process, we learn a VQ-VAE independently on each modality, without any temporal modeling. During the second step of the training process, we learn the MDVAE model on the latent representation provided by the frozen VQ-VAE encoders, before quantization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visual sequences generated using the analysis-transformation-synthesis experiment. The top two sequences depict original image sequences of two distinct individuals, while the bottom two sequences were generated by swapping the latent variable w between the two original sequences.</figDesc><graphic coords="9,300.33,179.73,176.59,52.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: This figure demonstrates the qualitative significance of each latent space for visual data using the analysis-transformation-synthesis experiment. The sequences in the yellow box (left) were generated using z (av) from sequence (a) and z(v) , w from sequences(b)  and(c). The sequences in the red box (right) were generated using z(v) from the sequence (a), and z (av) , w from sequences(b)  and(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The first row represents a sequence of face images for an individual whose emotion is neutral. The rows below are generated with VQ-MDVAE, keeping all the dynamical latent variables of the first sequence and replacing the static latent variable with that of sequences from the same person but with different emotions (from top to bottom: fear, sad, surprised, angry, and happy).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Frequency</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Audio spectrograms generated from between sequence (a) in green and sequence (b) in blue. The spectrograms (1), (2), (3), and (4) are synthesized by swapping latent variables between sequence (a) and sequence(b). The black dotted line corresponds to the pitch contour.</figDesc><graphic coords="10,55.02,230.25,106.12,105.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Relationship between the audio/visual attributes and the latent variables of VQ-MDVAE. (left) Pearson correlation coefficient (PCC), (right) mean absolute error (MAE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(a) Confusion matrix for emotion classification on the VQ-MDVAE output images after perturbation of the dynamical latent variables. Emotion recognition Person identity recognition (b) Performance of emotion and person identity recognition for each latent variable of the VQ-MDVAE model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Analysis of the latent variables of the VQ-MDVAE model in terms of emotion and person identity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Corruption of the eyes region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>of the mouth region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Qualitative comparison of the denoising results. From top to bottom: perturbed sequences; sequences reconstructed with VQ-VAE; sequences reconstructed with DSAE; sequences reconstructed with VQ-JointMVAE; and sequences reconstructed with VQ-MDVAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>(a) Corruption of the mouth region. (b) Corruption of the eyes region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: (For better visibility, please zoom in.) Quantitative results of audiovisual facial image denoising. (a) PSNR (left) and SSIM (right) are plotted as a function of the noise variance when the noise is applied to the mouth region. (b) PSNR (left) and SSIM (right) are plotted as a function of the noise variance when the noise is applied to the eyes region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Accuracy for emotion category classification as a function of the amount of labeled data used to train the MLR classification model on the MEAD dataset in the person-dependent evaluation setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>Figure A.13: (Better zoom in) The overall architecture of the MDVAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>Figure B.14a shows visualizations obtained with PCA and ISOMAP for one single speaker in the MEAD dataset, and the colors indicate the emotion labels. It can be seen that different emotions form different clusters and the neutral emotion is approximately in the middle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>Figure B.14b corresponds to the same visualization but the colors now indicate the emotion intensity levels. It can be seen that for each emotion, the intensity level increases continuously from the middle to the outside of the emotion cluster. Finally, Figure B.14c shows the identity clusters for six different speakers (left figure) and the emotion clusters for two speakers (right figure), both obtained using PCA. 3D visualizations are available on the companion website, along with other dimension reduction methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of the notations.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Speech performance of the MDVAE model tested in the analysis-resynthesis experiment. The STOI, PESQ, and MOSnet scores are averaged over the test subset of the MEAD dataset.</figDesc><table><row><cell>Method</cell><cell>STOI ↑</cell><cell>PESQ ↑</cell><cell>MOSnet ↑</cell><cell>SI-SDR ↑</cell></row><row><cell>VQ-VAE-audio</cell><cell cols="4">0.91 ±0.02 3.49 ±0.25 3.60 ±0.15 6.67 ±1.18</cell></row><row><cell>DSAE-audio</cell><cell cols="4">0.79 ±0.05 2.10 ±0.31 1.88 ±0.30 -1.20 ±1.58</cell></row><row><cell>MDVAE</cell><cell cols="4">0.82 ±0.03 2.43 ±0.28 2.35 ±0.18 2.21 ±1.30</cell></row><row><cell cols="5">VQ-DSAE-audio 0.84 ±0.03 2.12 ±0.24 3.05 ±0.20 6.12 ±1.10</cell></row><row><cell>VQ-MDVAE</cell><cell cols="4">0.85 ±0.04 2.90 ±0.23 3.54 ±0.20 6.85 ±1.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Visual performance of the MDVAE model tested in the analysisresynthesis experiment. The MSE, PSNR, SCC and SSIM scores are averaged over the test subset of the MEAD dataset.</figDesc><table><row><cell>Method</cell><cell>MSE ↓</cell><cell>PSNR ↑</cell><cell>SCC ↑</cell><cell>SSIM ↑</cell></row><row><cell>VQ-VAE-visual</cell><cell cols="4">0.0016 ±0.0002 27.2 ±0.70 0.70 ±0.01 0.85 ±0.01</cell></row><row><cell>DSAE-visual</cell><cell>0.023 ±0.03</cell><cell cols="3">15.8 ±2.9 0.58 ±0.07 0.47 ±0.03</cell></row><row><cell>MDVAE</cell><cell>0.010 ±0.008</cell><cell cols="3">20.3 ±1.3 0.62 ±0.03 0.58 ±0.03</cell></row><row><cell cols="5">VQ-DSAE-visual 0.0018 ±0.0005 25.3 ±1.23 0.70 ±0.01 0.82 ±0.04</cell></row><row><cell>VQ-MDVAE</cell><cell cols="4">0.0017 ±0.0007 26.8 ±0.72 0.72 ±0.01 0.84 ±0.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Accuracy (%) and F1-score (%) results of emotion category and intensity level recognition in the person-dependent (PD) and person-independent (PI) evaluation settings for the MEAD and RAVDESS datasets. The best scores are in bold and second best scores are underlined. For the VQ-MDVAE model evaluated on RAVDESS, two scores are reported. The first one corresponds to VQ-MDVAE trained on MEAD only, and the second one to the same model fine-tuned (in an unsupervised manner) on RAVDESS.</figDesc><table><row><cell></cell><cell></cell><cell>Model</cell><cell>MEAD</cell><cell cols="3">Emotion category RAVDESS</cell><cell cols="4">Emotion intensity level MEAD RAVDESS</cell></row><row><cell></cell><cell>classification</cell><cell>representation</cell><cell cols="2">Accuracy F1-score</cell><cell>Accuracy</cell><cell>F1-score</cell><cell cols="2">Accuracy F1-score</cell><cell>Accuracy</cell><cell>F1-score</cell></row><row><cell></cell><cell></cell><cell>VQ-DSAE-audio (Li &amp; Mandt, 2018)</cell><cell>40.4</cell><cell>39.3</cell><cell>-</cell><cell>-</cell><cell>48.7</cell><cell>45.7</cell><cell>-</cell><cell>-</cell></row><row><cell>PD</cell><cell>MLR</cell><cell>VQ-DSAE-visual (Li &amp; Mandt, 2018) wav2vec (Schneider et al., 2019)</cell><cell>50.6 76.2</cell><cell>51.1 75.0</cell><cell>-74.3</cell><cell>-75.5</cell><cell>43.3 55.0</cell><cell>44.2 54.6</cell><cell>-76.5</cell><cell>-76.3</cell></row><row><cell></cell><cell></cell><cell>VQ-MDVAE (our)</cell><cell>98.2</cell><cell>98.3</cell><cell cols="2">81.9 / 89.4 82.9 / 89.6</cell><cell>83.9</cell><cell>83.1</cell><cell cols="2">78.0 / 80.1 77.2 / 79.8</cell></row><row><cell></cell><cell cols="2">AV transformer (Chumachenko et al., 2022)</cell><cell>-</cell><cell>-</cell><cell>89.2</cell><cell>88.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>MLR</cell><cell>wav2vec (Schneider et al., 2019) VQ-MDVAE (our)</cell><cell>68.4 73.2</cell><cell>64.5 72.5</cell><cell cols="2">69.5 68.8 / 71.4 68.5 / 70.5 68.6</cell><cell>51.8 63.8</cell><cell>50.3 61.7</cell><cell cols="2">76.6 73.8 / 77.2 75.7 / 77.6 75.6</cell></row><row><cell>PI</cell><cell>MLP</cell><cell>wav2vec (Schneider et al., 2019) VQ-MDVAE (our)</cell><cell>70.9 80.0</cell><cell>70.8 80.5</cell><cell cols="2">70.2 77.5 / 78.7 78.0 / 78.1 70.6</cell><cell>53.7 71.5</cell><cell>53.9 72.2</cell><cell cols="2">76.6 77.4 / 77.4 77.6 / 77.7 76.3</cell></row><row><cell></cell><cell>DA + MLR</cell><cell>wav2vec (Schneider et al., 2019) VQ-MDVAE (our)</cell><cell>71.0 83.1</cell><cell>69.9 82.2</cell><cell cols="2">71.6 78.1 / 79.3 78.0 / 80.7 71.2</cell><cell>53.5 77.5</cell><cell>52.9 78.0</cell><cell cols="2">76.8 78.1 / 79.0 78.5 / 79.1 76.5</cell></row><row><cell></cell><cell></cell><cell>MULT (Tsai et al., 2019)</cell><cell>-</cell><cell>-</cell><cell>76.6</cell><cell>77.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">AV transformer (Chumachenko et al., 2022)</cell><cell>-</cell><cell>-</cell><cell>79.2</cell><cell>78.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">observe in the "PI" section of Table 4 a decrease in perfor-</cell><cell>factors</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">mance for all methods using an MLR classification model.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">For VQ-MDVAE, this decline can be analyzed through vi-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">sual representations of the static audiovisual latent space, as</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">shown in Figure B.14 of Appendix B. This figure highlights</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">the hierarchical structure of the static latent audiovisual</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">space in terms of identity, emotion, and intensity level. In</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">this structure, identities are represented by clusters, each</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">of which is made up of several emotion clusters. These</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">clusters represent eight distinct emotions distributed in a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">range of intensity levels from weak to strong. As a result,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">each identity is associated with its own representation of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">emotions, which means that the emotion clusters differ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">from one identity to another. By incorporating the identity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">information as in the previous evaluation approach, we</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">can more accurately classify the emotion categories. Con-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">sequently, a simple linear model (MLR) is sufficient for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">classifying both the emotions and their levels. To improve</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">generalization to test data where speakers were not seen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">during training, we propose two solutions. First, we im-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">prove the classification model by replacing the linear MLR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">classifier by a non-linear MLP classifier, which results in a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">substantial increase in accuracy for the VQ-MDVAE model:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">+6.8% and +7.3% for emotion category classification on the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">MEAD and RAVDESS datasets, respectively (using the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">finetuned model for RAVDESS). We observe a similar trend</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">with the wav2vec + MLP model, which leads to an improve-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">ment in performance compared to using the MLR classifier.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Second, we keep the MLR classification model but apply</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">unsupervised domain adaptation to the test data using an</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">optimal transport approach (Courty et al., 2017). Domain</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">adaptation has been shown to be effective when dealing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">with domain shifts caused by unknown transformations,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">such as changes in identity, gender, age, ethnicity, or other</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table A .</head><label>A</label><figDesc>6: The architecture of the VQ-VAE-audio.Appendix B. Visualization of the MDVAE static latent space2D visualizations of the static latent space of the MD-VAE are obtained using dimension reduction methods.</figDesc><table><row><cell></cell><cell>Layer</cell><cell>Activation</cell><cell>Output dim</cell></row><row><cell>Input</cell><cell>-</cell><cell>-</cell><cell>1 × 513</cell></row><row><cell></cell><cell>Conv1D(1, 16, 4, 2, 1)</cell><cell>Tanh</cell><cell>16 × 256</cell></row><row><cell></cell><cell>Conv1D(16, 32, 4, 2, 1)</cell><cell>Tanh</cell><cell>32 × 128</cell></row><row><cell>Encoder</cell><cell>Conv1D(32, 32, 3, 2, 1)</cell><cell>Tanh</cell><cell>32 × 64</cell></row><row><cell></cell><cell>1 × Residual Stack</cell><cell>Tanh</cell><cell>32 × 64</cell></row><row><cell></cell><cell>Conv1D(32, 8, 1, 1)</cell><cell>-</cell><cell>8 × 64</cell></row><row><cell></cell><cell>ConvT1D(8, 32, 1, 1)</cell><cell>-</cell><cell>32 × 64</cell></row><row><cell></cell><cell>1 × Residual Stack (T)</cell><cell>Tanh</cell><cell>32 × 64</cell></row><row><cell>Decoder</cell><cell>ConvT1D(32, 32, 3, 2, 1)</cell><cell>Tanh</cell><cell>32 × 128</cell></row><row><cell></cell><cell>ConvT1D(32, 16, 4, 2, 1)</cell><cell>Tanh</cell><cell>16 × 256</cell></row><row><cell></cell><cell>ConvT1D(16, 1, 4, 2, 0)</cell><cell>-</cell><cell>1 × 513</cell></row><row><cell cols="4">Conv1D(in_channel, out_channel, kernel_size, stride, padding)</cell></row><row><cell cols="4">Residual Stack (T) = { 2 × Conv(T)1D(32, 32, 3, 1, 1)}</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>⋆ This work was supported by <rs type="funder">Randstad</rs> corporate research chair, by <rs type="funder">ANR</rs>-<rs type="projectName">3IA</rs> <rs type="projectName">MIAI</rs> (<rs type="grantNumber">ANR-19-P3IA-0003</rs>), by <rs type="funder">ANR</rs>-<rs type="projectName">JCJC ML3RI</rs> (<rs type="grantNumber">ANR-19-CE33-0008-01</rs>), and by <rs type="funder">H2020</rs> <rs type="projectName">SPRING</rs> (funded by <rs type="funder">EC</rs> under GA #<rs type="grantNumber">871245</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_2fpF2W3">
					<orgName type="project" subtype="full">3IA</orgName>
				</org>
				<org type="funded-project" xml:id="_KthBdc9">
					<idno type="grant-number">ANR-19-P3IA-0003</idno>
					<orgName type="project" subtype="full">MIAI</orgName>
				</org>
				<org type="funded-project" xml:id="_g5AqR3V">
					<idno type="grant-number">ANR-19-CE33-0008-01</idno>
					<orgName type="project" subtype="full">JCJC ML3RI</orgName>
				</org>
				<org type="funded-project" xml:id="_QaQx77b">
					<idno type="grant-number">871245</idno>
					<orgName type="project" subtype="full">SPRING</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix A. The detailed architecture of the vector quantized MDVAE</p><p>This section details the architecture of the VQ-MDVAE model, starting with the VQ-VAE and then the MDVAE. The VQ-VAE developed for audio or images consists of three parts: (i) an encoder that maps an image to a sequence of continuous latent variables, referred to as the intermediate representation in the paper, (ii) a shared codebook that is used to quantize these continuous latent vectors to a set of discrete latent variables (each vector is replaced with the nearest vector from the codebook), and (iii) a decoder that maps the indices of the vectors from the codebook back to an image. The architectures of the visual and audio VQ-VAEs are described in tables A.5 and A.6, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A.2. MDVAE</head><p>MDVAE is decomposed into two models: (i) the first is the inference model (encoder), which is further decomposed into four inferences for each latent variable, represented by Gaussian distributions whose parameters are determined via a neural network. The prior distributions for the dynamic latent variables are also trained, except for the static latent space, where the prior is assumed to be a standard normal distribution. (ii) The second part is composed of two decoders, one for the visual modality and the other for the audio modality. Structured only with linear layers and non-linear activation functions, the input of these two decoders are the concatenation of w, z</p><p>for the visual and audio modalities, respectively. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep audio-visual speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Influence of lips on the production of vowels based on finite element simulations and experiments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arnela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Blandin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dabbaghchian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Guasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alías</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pelorson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Hirtum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Engwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="2852" to="2859" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimae: Multi-modal multi-task masked autoencoders</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bachmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mizrahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European (a) Visualization of the emotion clusters for a single speaker using PCA (left) and ISOMAP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>right</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">PCA (left) and ISOMAP</title>
		<imprint/>
	</monogr>
	<note>right</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Visualization of the identity clusters for six speakers using PCA (left) and visualization of the emotion clusters for two speakers using PCA (right)</title>
		<imprint/>
	</monogr>
	<note>Figure B.14: 2D visualizations of the static latent space</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Correlated expression of the body, face, and voice during character portrayal in actors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lewin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Praat: doing phonetics by computer</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boersma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weenink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2021. 2011</date>
		</imprint>
	</monogr>
	<note>computer program. Version, 5 , 74</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-attention fusion for audiovisual emotion recognition with incomplete data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chumachenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iosifidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gabbouj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2822" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint distribution optimal transportation for domain adaptation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the limitations of multimodal vaes</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daunhawer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chin-Cheong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Palumbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Vogt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Facial action coding system</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>Environmental Psychology &amp; Nonverbal Behavior</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization with the itakura-saito divergence: With application to music analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bertin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Durrieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="793" to="830" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modality-general and modalityspecific audiovisual valence processing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Shinkareva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cortex</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="127" to="137" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identifying independence in bayesian networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="507" to="534" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamical variational autoencoders: A comprehensive review</title>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hueber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="175" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014. 2016</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
	<note>Generative adversarial nets. beta-vae: Learning basic visual concepts with a constrained variational framework</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end audio visual scene-aware dialog using multimodal attention-based video features</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alamri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cartillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2352" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving variational autoencoder with deep feature consistent and generative adversarial training</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">341</biblScope>
			<biblScope unit="page" from="183" to="194" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Disentangling by partitioning: A representation learning framework for multimodal sensory data</title>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11264</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An introduction to variational methods for graphical models. Machine learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="183" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimal transport-based identity matching for identity-invariant facial expression recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Disentangling by factorising</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2649" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Crepe: A convolutional representation for pitch estimation</title>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="161" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Represen-tations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning latent subspaces in variational autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multimodal therapy. Handbook of Psychotherapy Integration</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Lazarus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<biblScope unit="page">105</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sdr-half-baked or well done?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Private-shared disentangled multimodal vae for learning of hybrid latent representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13024</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Disentangled sequential autoencoder</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02991</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">196391</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Mosnet: Deep learning based objective assessment for voice conversion</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-M</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08352</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4114" to="4124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weakly-supervised disentanglement without compromises</title>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6348" to="6359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pyfeat: a python-based effective feature generation tool for dna, rna and protein sequences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Muhammod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Md Farid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shatabda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dehzangi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3831" to="3833" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A view of the em algorithm that justifies incremental, sparse, and other variants</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in graphical models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Audio-visual emotion recognition in video clips</title>
		<author>
			<persName><forename type="first">F</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marjanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Njegus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Anbarjafari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="60" to="75" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Emotion recognition from speech using wav2vec 2.0 embeddings</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pepino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2021. 2018</date>
			<biblScope unit="page" from="6548" to="6552" />
		</imprint>
	</monogr>
	<note>End-to-end audiovisual speech recognition. ICASSP</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Facial expression recognition using residual masking network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4513" to="4519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improving image autoencoder embeddings with perceptual loss</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Pihlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sandin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep multimodal learning: A survey on recent advances and trends</title>
		<author>
			<persName><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramachandram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="96" to="108" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech, and signal processing. Proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Ava active speaker: An audio-visual dataset for active speaker detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stopczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4492" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning and controlling the source-filter representation of speech with a variational autoencoder</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sadok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Séguier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3465" to="3469" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Leveraging recent advances in deep learning for audio-visual emotion recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Schoneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Othmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abdelkawy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Variational mixture-of-experts autoencoders for multi-modal deep generative models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multimodal generative learning utilizing jensen-shannon-divergence</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Daunhawer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vogt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6100" to="6110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Generalized multimodal elbo</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Daunhawer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Vogt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A survey of multimodal deep generative models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Robotics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="261" to="278" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Joint multimodal learning with deep generative models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01891</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A short-time objective intelligibility measure for time-frequency weighted noisy speech</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="4214" to="4217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multimodal transformer for unaligned multimodal language sequences</title>
		<author>
			<persName><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference</title>
		<meeting>the conference</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">6558</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Nvae: A deep hierarchical variational autoencoder</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="19667" to="19679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Are disentangled representations helpful for abstract visual reasoning? Advances in Neural Information Processing Systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019. 2020</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="700" to="717" />
		</imprint>
	</monogr>
	<note>Mead: A large-scale audio-visual dataset for emotional talking-face generation</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with regularized optimal transport for multimodal 2d+ 3d facial expression recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="31" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Survey on audiovisual emotion recognition: databases, features, and data fusion strategies</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Goodman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014">2014. 2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
	<note>Multimodal generative models for scalable weakly-supervised learning</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Robust lightweight facial expression recognition network with label distribution training</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on artificial intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3510" to="3519" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
