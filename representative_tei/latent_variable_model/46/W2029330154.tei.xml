<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interaction Analysis using Switching Structured Autoregressive Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Siracusa</surname></persName>
							<email>siracusa@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Fisher</surname><genName>III</genName></persName>
							<email>fisher@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Interaction Analysis using Switching Structured Autoregressive Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper explores modeling the dependency structure among multiple vector time-series. We focus on a large classes of structures which yield efficient and tractable exact inference. Specifically, we use directed trees and forests to model causal interactions among time-series. These models are incorporated in a dynamic setting in which a latent variable indexes evolving structures. We demonstrate the utility of the method by analyzing the interaction of multiple moving objects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Consider a scene in which multiple objects are moving around in an environment. Given measurements of their position over time we wish to identify any interactions among them and describe how this interaction changes over time. We treat this task of labeling interaction as a problem of inference of dependency structures in a model for multiple time-series. Here the statistical dependence is of primary interest and the underlying parameters of the model are treated as nuisances. We show how to perform exact Bayesian inference on directed tree and forest structured vector autoregressive models. While the number of directed trees and forests is super-exponential in the number of time-series, the Matrix-Tree theorem allows one to calculate sums of multiplicative and additive functions on directed edges in polynomial time. This, combined with a conjugate prior allows for efficient and exact calculation of posterior event probabilities.</p><p>We examine the utility of our model within a dynamic setting. An evolving latent variable is introduced to index changing structure over time. We present empirical results on two data sets evaluating the interaction of multiple moving objects.</p><p>Spectral methods for point estimation of graphical models over stationary time-series is considered in <ref type="bibr" target="#b1">[2]</ref>. The related work of <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> consider alternative models in which structure is changing over time. The methodology here differs in that while we restrict ourselves to directed models, we marginalize over parameters generating a tractable posterior over structures. These models as well as the model proposed here fall into the general class of multinets <ref type="bibr" target="#b6">[7]</ref> or Contingent Bayesian Networks <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. STRUCTURED VECTOR AUTOREGRESSIVE MODEL</head><p>Here, we present a structured vector autoregressive model for multivariate time-series. This model is an rth order Markov model with additional structural constraints. We begin by introducing some notation for the purpose of explicitly denoting individual time-series, past values of individual time-series as well as sets thereof.</p><p>Consider N time-series / data streams and let x v t be a random vector representing the value of the vth time-series at time t. The r past values of time-series v is defined as</p><formula xml:id="formula_0">x v t . That is, x v t is a stacked vector of x v t-1 through x v t-r</formula><p>. As we will be explicit on the model order, r is suppressed in the notation, x v t , for brevity. Furthermore, the vector x S t indexed by set S is x</p><formula xml:id="formula_1">S(1) t ,..., x S(m) t</formula><p>stacked in a vector where |S| = m (e.g. x v,u t is x v t and x u t stacked). The random variables denoting the present and past of all timeseries at time t are defined as X t and X t respectively.</p><p>Multiple time points can be indexed by a vector t = [t 1 , t 2 , ..., t T ] such that X t = X t1 , ..., X tT . Note that the collection of past values, X t , can be formed from X t and a set C containing values not available in X t (initial conditions). Given a directed structure Ē, a set of parameters Θ and C, the model is rth order Markov:</p><formula xml:id="formula_2">p X t | Ē, Θ, C = T i=1 p X t(i) | X t(i) , Ē, Θ .<label>(1)</label></formula><p>In order to simplify notation we will drop the C when it is clear from the context. Ē is a directed structure on N nodes/vertices defining the factorization of the model:</p><formula xml:id="formula_3">p X t | Ē, Θ = T i=1 N v=1 p x v t(i) | x v,pa(v, Ē) t(i) , Θ v|pa(v, Ē)<label>(2)</label></formula><p>where pa v, Ē returns the parents of vertex v given the structure Ē. We will drop the Ē and use pa (v) when it is clear from the context. Each time-series v at time t is dependent on its own past x v t as well as the past of its parent set S = pa (v), x S t . Note that we use Θ v|S rather than the more explicit notation Θ v|v,S to represent the parameters of this relationship for brevity. In general we refer to this class of models as rth order temporal interaction models, TIM(r). In this paper we focus on the special case in which Ē describes a directed tree or forest and Θ parameterizes a linear Gaussian relationships. We refer to such models as directed tree or forest structured vector autoregressive models, SVARM(r).</p><formula xml:id="formula_4">x 1 t+1 x 1 t-1 x 1 t x 2 t x 3 t x 3 t-1 x 2 t-1 x 2 t+1 x 3 t+1 (a)</formula><p>Figure <ref type="figure" target="#fig_0">1</ref>(a) illustrates a SVARM(1) for three time-series with Ē describing a directed tree with the second timeseries as the root. Here pa (1) = pa (3) = 2 and pa (2) = ∅. Figure <ref type="figure" target="#fig_0">1(b)</ref> shows an alternative and more compact view for this model in which a single node represents a time-series/data stream over all time. We refer to this as the interaction graph and use diamond shaped nodes to emphasize it is not meant to be interpreted as a directed Bayesian network, though there is a one to one mapping between these graphs. A directed edge from u to v in the interaction graph implies a directed edge from x u t and x v t in the SVARM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CONJUGATE PRIOR</head><p>In this section we introduce a conjugate prior on the parameters, θ, and structure, Ē of the SVARM(r). The conjugacy of the prior yields a simple form for posterior updates. Tractable calculation of the exact posterior depends on the ability to efficiently evaluate a partition function which sums over a super-exponential number of structures. We show that directed trees and forests are classes of structures for which this can be done in polynomial time. This allows one to efficiently calculate the posterior on structure given data, while integrating out parameters.</p><p>The prior we use is related to those presented in <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b9">[10]</ref>. It factors as</p><formula xml:id="formula_5">p 0 Ē, Θ = p 0 Ē p 0 Θ| Ē .<label>(3)</label></formula><p>The parameters are assumed to be independent and modular given a structure Ē and hyper-parameters Γ. That is, they factorize according to the edges in Ē:</p><formula xml:id="formula_6">p 0 Θ| Ē = N v=1 p 0 Θ v|pa(v) |Γ<label>(4)</label></formula><p>and p 0 Θ v|S |Γ is the same for all structures Ē for which S is the parent set of v. Thus, for each time-series v one needs to specify a parameter prior for all potential parent sets. Given this finite set of priors for each v one is able to supply a full prior on parameters for any given structure. We place a prior on directed structures which has the form:</p><formula xml:id="formula_7">p 0 Ē = 1 Z(β) N v=1 β pa(v),v<label>(5)</label></formula><p>where the partition function Z(β) ensures proper normalization. Each scalar hyper-parameter β S,v can be interpreted as a weight on the parent set S for v and the prior is simply a proportional to the product of these weights.</p><p>Let D = {X 1 , ..., X T } be a set of T complete observations. We use the notation</p><formula xml:id="formula_8">D S = {x S 1 , ..., x S T } to denote observations of a set of time-series. D t = X t and D u t = x u t . D = { X 1 , .</formula><p>.., X T } can be formed using D and past information in C. Given data D, the posterior on parameters given structure has the form:</p><formula xml:id="formula_9">p Θ| Ē, D = N v=1 p Θ v|pa(v) |D v , D pa(v) , Γ<label>(6)</label></formula><p>That is, the prior on parameters given a structure is fully conjugate. If one choses a conjugate prior for each Θ v|S in Equation 4 the posterior has the same form and can be updated by sufficient statistic calculations from the data.</p><p>In addition, the posterior on structure is:</p><formula xml:id="formula_10">p Ē|D = 1 Z(β • W ) N v=1 β pa(v),v W pa(v),v<label>(7)</label></formula><p>where • is an element wise / Hadamard product and</p><formula xml:id="formula_11">W S,v = p D v | D v,S , Θ v|S p 0 Θ v|S |Γ dΘ v|S (8) is the evidence, p D v | D v,S</formula><p>, for time-series v given the time-series of its parent set defined by S. That is, the prior is updated by modifying β with a set of evidence weights W . The proof follows from the fact that the prior on structure factorizes in the same manner as Equation <ref type="formula" target="#formula_3">2</ref>.</p><p>In this paper, we assume continuous observations and a linear gaussian model with parameters Θ v|S . We model p 0 Θ v|S |Γ to be a matrix-normal-inverse-Wishart distribution with hyper-paramters Γ. This yields efficient updates for Equation <ref type="formula" target="#formula_9">6</ref>and W v|S will be the evaluation of a Matrix-T distribution <ref type="bibr" target="#b10">[11]</ref>. However, tractable calculate ion of Equation 7 requires one to evaluate Z(β) efficiently. We show this can be done for directed trees and forests in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Directed Trees and Forests</head><p>In general there are 2 N -1 possible parent sets for each v. This yields a super-exponential number of possible structures, 2 N 2 -N , implying Z(β) requires summing over a super-exponential number of terms. In this paper we focus on two particular subclasses of these structures, while still super-exponential in number, that can be reasoned over in polynomial time.</p><p>The first class, directed trees (spanning arborescences), yield structures in which all the time-series are dependent on each other with no temporal feedback. That is, directed trees restrict the interaction graph to be spanning and acyclic with no more than a single parent for each timeseries. Thus, in the directed tree case, β is simply an N ×N matrix and each hyper-parameter β u,v =u can be interpreted as a weight on the edge u → v, and β ∅,v = β v,v is a weight on a node being a root (having no parents). The edge set corresponding to the nonzero entries of β form a support graph. We assume this support graph is connected and contains at least one directed tree.</p><p>While there are N N -1 possible directed trees on N nodes, the Matrix Tree Theorem allows one to calculate Z(β) in polynomial time. This theorem was used by Meila and Jaakkola <ref type="bibr" target="#b8">[9]</ref> for reasoning over undirected trees. The undirected version of theorem is a special case of the often rediscovered real valued directed version developed by Kirchhoff <ref type="bibr" target="#b11">[12]</ref>. The theorem allows one to calculate the weighted sum over all directed trees rooted at r, Z r (β) via:</p><formula xml:id="formula_12">Z r (β) = Ē rooted at r u→v β u,v = Cof r,r Q(β)<label>(9)</label></formula><p>where Q(β) is the Kirchhoff matrix with its u, v entry defined as:</p><formula xml:id="formula_13">Quv (β) = -β u,v 1 ≤ u = v ≤ N N u ′ =1 β u ′ ,v 1 ≤ u = v ≤ N<label>(10)</label></formula><p>and Cof i,j (M ) is the i, j cofactor of matrix M . Cof i,j Q(β) is invariant to i and gives the sum over all weighted trees rooted at j. A proof can be found in <ref type="bibr" target="#b12">[13]</ref>. By summing over all N possible roots one obtains</p><formula xml:id="formula_14">Z(β) = N v=1 β v,v Z v (β).<label>(11)</label></formula><p>Thus, a straight forward implementation yields O(N 4 ) time for calculating the partition function. However, as pointed out in <ref type="bibr" target="#b13">[14]</ref>, using the invariance of Cof i,j Q(β) allows for O(N 3 ) time computation of Z(β). That is, Z(β) can be calculated by replacing any row of the matrix Q(β) with β 1,1 , ..., β N,N and taking its determinant. Directed forests, remove the fully connected assumption and can have multiple roots. There are (N + 1) (N -1) directed forest for N nodes, but Z(β) can still be calculated in O(N 3 ) time <ref type="bibr" target="#b13">[14]</ref>. Some intuition as to why this is true is that any directed forest can be turned into a directed tree by the addition of one virtual super root node which has no parents and connects to all the roots in the forest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Event Probabilities and Expectations</head><p>The ability to compute the partition function and conjugacy of the prior allows one to calculate a wide variety of useful prior and/or posterior event probabilities. For example, the probability of a particular edge being present is:</p><formula xml:id="formula_15">p (I u→v = 1) = E [I u→v ] = 1 - Z(β -(u→v) ) Z(β)<label>(12)</label></formula><p>where I u→v is an indicator variable that has value 1 when the edge u → v is present. β -(u→v) is β with all elements involving edge from u to v set to zero. Using the same approach one can calculate the joint edge appearance probability of one set of edges conditioned on another set. Similarly one can calculate the probability a timeseries/node has no parents (is a root):</p><formula xml:id="formula_16">p (I v is a root ) = Z(β -edgesin(v) ) Z(β) ,<label>(13)</label></formula><p>where edgesin(v) and return the set of all edges into time-series/node v. β -e indicates all elements of β which involve any edge in the set e are zero. Similarly one can use β -edgesout(v) to calculate the probability of a time-series having no children (is a leaf).</p><p>The indicator variables used in the examples above can be expressed as general multiplicative functions of the form g( Ē) = N v=1 g pa(v),v . The expected value of a general multiplicative function can be calculated by:</p><formula xml:id="formula_17">E g( Ē) = Z(β • g) Z(β) .<label>(14)</label></formula><p>Note that variance or other higher order moments of multiplicative functions can also calculated in this manner. Additionally, one can calculate the expectation of additive functions of the form</p><formula xml:id="formula_18">f ( Ē) = N v=1 f pa(v),v . For directed trees E f ( Ē) = N r=1 Z r (β) Z(β) tr M r,r Q(β • f ) M r,r Q(β) -1<label>(15)</label></formula><p>where M i,j (M ) is the matrix M with its ith row and jth column removed. A proof follows that of <ref type="bibr" target="#b8">[9]</ref> substituting in the directed tree partition function in place of the undirected version. A similar form is obtained for directed forests. Additive functions allow calculation of quantities such as the expected number of children of a particular time-series/node. </p><formula xml:id="formula_19">K z 1 z 2 z 3 X 1 X 2 X 3 X 0 Θ k Ēk π α β Γ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sampling directed trees and forests</head><p>Given data, we have shown how to calculate the posterior p Ē|D and various event probabilities. Another important task is sampling from this posterior. Random sampling of directed spanning trees (and forests with some modification) is a well studied problem. This paper uses Wilson's random walk based algorithm <ref type="bibr" target="#b14">[15]</ref>. If a point estimate of structure is desired, the maximum a posteriori structure can also be found. The Chu-Liu/Edmonds/Bock <ref type="bibr" target="#b15">[16]</ref> algorithm can be used for both directed trees and forests</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DYNAMIC SWITCHING STRUCTURES</head><p>The SVARM(r) model is stationary and assumes a single Ē and parameters Θ over all time. Here, we introduce a switching SVARM, SSVARM(r,K), which allows structure to change over time. Let z t be a hidden state at time t which indexes a specific structure,E zt , and parameters, Θ zt . Figure <ref type="figure" target="#fig_1">2</ref> shows a first order model. Given a set of K structures E = { Ē1 , ..., ĒK }, parameters Θ = {π 0 , π 1 , ..π k , Θ 1 , ..., Θ K } and an observations over the time period t = [1, ..., T ]:</p><formula xml:id="formula_20">p(X t , z t |E, Θ) = p(z t )p(X t |z t , E, Θ) = T t=1 p(z t |π zt-1 )p(X t | X t , E zt , Θ zt ) (16)</formula><p>where z 0 = 0, the transition distribution is multinomial p (z t |π zt-1 ) = Mult (z t ; π zt-1 ) and is given a Dirichlet prior p 0 (π) = K k=1 Dir π k ; α k 1 , ..., α k K . Exact inference on this model is complex due to the fact there are K T possible state sequences. Thus, we turn to a Monte Carlo Markov chain (MCMC) approach in which samples are drawn from this model using a Gibbs sampler. The sampler has three main steps. Step 1 samples the state sequence give previous estimate of structures and parameters. This is done efficiently with backward message passing followed by forward sampling. This step can be modified when initializing to sample the state sequence from its transition prior. During step 2 the sampled counts of state transitions are noted and transition probabilities are then sampled given these counts. In Step 3 a vector t k is formed with all the time points with z t = k. The structure and parameters are then sampled given X t k for each k. It is important to note that given a state sequence, one can efficiently calculate exact event probabilities and posterior over structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section we present two illustrative experiments focusing on the calculation of posterior event probabilities. Each experiment analyzes the interaction of tracked moving objects. Specifically, we are interested in quantifying uncertainty in the dependence structure among time-series rather than obtaining point estimates. When analyzing data we do not assume there is a "true/correct structure" one would like to discover. Our goal is to obtain a full characterization of the posterior uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Interacting People</head><p>We begin by analyzing the motion of two individuals in an enclosed environment. Individuals are instructed to switch between moving independently and following each other. A simple tracker outputs the (x, y) position of each individual over all time and ground-truth indicating when interaction occurred was created.</p><p>In <ref type="bibr" target="#b5">[6]</ref> a dependence test was performed on this data using a hidden factorization Markov model (HFactMM). An HFactMM allows switching between latent states each of which having a known/specified fixed structure. For this data two states were used and the structures were set to be independent in state 1 and fully connected/dependent in state 2. The dependence test was performed by finding the parameters that maximized the likelihood of the data assuming these fixed structures and then Viterbi decoding was performed to segment the sequence. A probability of error of .035 was obtained when comparing to the groundtruth.</p><p>Here, we will not assume known structures. We place a uniform prior on all directed forests and weak matrixnormal-inverse-Wishart prior on parameters (small degrees of freedom, and larger variance). Using a two state, first order model, SSVARM(1,2), we generate 100 Gibbs samples. Burn in was quick and required approximately 15 iterations of the sampler.</p><p>Figure <ref type="figure" target="#fig_2">3</ref>(a) summarizes the results. The ground truth state sequence is shown on top. The individuals start out being independent and then randomly switch between that and the following behavior. The 100 sampled state sequences are shown below the ground truth ranked by the log probability of the data given the sampled parameters and structures, with the top being the most likely. The state sequence labels were permuted to give a consistent coloring with the ground truth segmentation. The side of the plot shows the normalized Hamming distance of the best mapping to the ground truth. Note that these samples indicated a very sharp posterior. Almost all of the samples give a very consistent segmentation of the data and have an average Hamming distance of .05. This distance can be interpreted as the probability of error if we were only concerned with segmenting the data. However, Figure <ref type="figure" target="#fig_2">3</ref>(a) only shows the sampled state sequence. Figure <ref type="figure" target="#fig_2">3</ref>(b) shows a more detailed breakdown of the sampled model with the highest log probability. The top row shows the sampled state sequence. The second row represents the posterior probability of the structure for each state depicted as weighted interaction graphs. In these graphs edge color represents the posterior probability of that edge. Node color represents the probability a timeseries has no parents/is a root (white = 0.0, black = 1.0). Recall that while the state sequence is MCMC sampled, we obtain an exact posterior conditioned on this sequence. We see that for this data the posterior is very sharp. The posterior for the first state is sharp around independent structure. The second state has a strong posterior on the second time-series/individual influencing the first. This is consistent with the fact that first person follows the second when they are interacting. The Hamming distance for this particular state sequence is .03. That is, we obtain comparable results to those obtained in <ref type="bibr" target="#b5">[6]</ref> without fixing the two possible structures a priori.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Follow The Leader</head><p>Next we explore a dataset comprised of recordings of three individuals playing a simple interactive computer game. Each player's computer mouse controls a specific dot/marker on their screen. All players can see three dots on their screen representing the other players in game and themselves. The individuals are instructed to play a game of follow the leader. One player is designated the leader. The leader moves his or her dot randomly around the screen while the other players are instructed to follow. The designated leader changes throughout the game. That is, a fourth person observing the game tells the players when to switch leaders. Thus, the latent variable indicating the change of leader is observable, and consequently, nominal ground-truth is available by which to evaluate performance.</p><p>We begin by using SSVARM(1,3) with directed trees to analyze this sequence. That is, we will use the fact there are only three players and knowledge that directed trees may sufficiently describe the interaction among players. A uniform prior on structures and equivalently weak prior on parameters is used. A weak self biased prior on the state transition distribution is imposed with a bias towards self transition.</p><p>Given the data and the prior model, 100 samples of the structure, parameters and the hidden state sequence are obtained with a Gibbs sampler. Burn in required approximately 60 iterations. A detailed view of the results can be found in Figure <ref type="figure" target="#fig_3">4</ref>(a) in the same format as shown in Figure <ref type="figure" target="#fig_2">3</ref>(a). The ground-truth state sequence is shown on top with players taking turns being the leader in order. The sampled state sequences show some posterior uncertainty. Each sample falls within two general categories. The top third of the samples match the ground truth closely, the bottom two thirds suggests a consistent alternative explanation.</p><p>Again, the state labels alone simply provide a segmentation. Given this segmentation we look at the posterior on structure to analyze the interaction among the players. Figures <ref type="figure" target="#fig_3">4(b</ref>) and 4(c) show a more detailed breakdown of two sampled models. Figure <ref type="figure" target="#fig_3">4</ref>(b) is a sample with low Hamming distance and high log probability. Notice that the posterior on structure for each state is basically a delta function on three distinct structures. These structures agree with our intuition in that each root is consistent with who was designated as the leader and the followers are conditionally independent given the root. Figure <ref type="figure" target="#fig_3">4(c</ref>) is a sample with a mid-range Hamming distance (ranked 34 out the 100 samples). It has errors consistent with the majority of sequences shown in 4(a). The confusion between the first and third state is most noticeably reflected in posterior uncertainty in the structure for state 3.</p><p>While the above analysis assumed three states, consistent with our knowledge of the ground truth, Figure <ref type="figure" target="#fig_3">4(c)</ref> gives evidence for additional modes/states. That is, for each phase of the game a better model may be a mixture of processes each with similar structure but different parameters. We repeat the experiment using K = 6 states. Figure <ref type="figure" target="#fig_3">4</ref>(d) is a sample from this model.</p><p>The second row shows the occurrence of the learned states. Interestingly, state 4 indicates uncertainty in the structure. However, this state is never used and thus its posterior remains uniform. The remaining structures are consistent with the ground truth indicating who is the leader with little uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We have presented a framework for Bayesian inference of the dependency structure among multiple vector timeseries. Assuming linear Gaussian dynamics we presented a structure autoregressive model in which directed trees and forests were used to describe causal interactions among time-series. We showed how a super-exponential number of structures can be reasoned over in polynomial time and how exact posterior event probabilities can be obtained. By introducing an evolving latent variable to index these models we allowed for dynamically switching structures. We demonstrated the utility of our method by analyzing the interaction of moving objects.</p><p>Here we have focused on acyclic structures in which each time-series has at most one parent. However, the framework presented is general and can be used for any class of directed structures. Tractable calculation of exact posterior probabilities is dependent on the ability to efficiently evaluate the partition function Z(β). Other structures for which Z(β) can be tractably calculated are discussed in <ref type="bibr" target="#b16">[17]</ref>.</p><p>The SSVARM(r,K) model used in this paper assumes a known number of states, K, and that structures can be revisited over time. Such assumptions were reasonable in our applications of interest. However, the SVARM(r) can easily be embedded into alternative dynamic models in which the number of states are unknown such as those presented in <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b17">[18]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example directed tree SVARM(1) model (a) and corresponding interaction graph (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The structure of a SSVARM(1,K)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Results for 2 person interaction experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Results for follow the leader experiment.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This research was partially supported by the Air Force Office of Scientific Research via Grant FA9550-06-1-0324 and the Army Research Office via grant W911NF-06-1-0076.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An experimental study of apparent behavior</title>
		<author>
			<persName><forename type="first">F</forename><surname>Heider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<date type="published" when="1944">1944</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning graphical models for stationary time series</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="2189" to="2199" />
		</imprint>
	</monogr>
	<note>in Trans. on signal processing</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic bayesian multinets</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Conditional chow-liu tree structures for modeling discrete-valued vector time series</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirshner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robertson</surname></persName>
		</author>
		<idno>UCI-ICS</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling changing dependency structure in multivariate time series</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inferring dynamic dependency with applications to link analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Siracusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asilomar Conference on Signals, Systems, and Computers</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge representation and inference in similarity networks and bayesian multinets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Integlligence</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="45" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Approximate inference for infinite contingent bayesian networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Milch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Marthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolobov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>in AIStats</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tractable bayesian learning of tree belief networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Computing</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="77" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Being bayesian about network structure. a bayesian approach to structure discovery in bayesian networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="95" to="125" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harrison</surname></persName>
		</author>
		<title level="m">Bayesian Forecasting and Dynamic Models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ueber die auflsung der gleichungen, auf welche man bei der untersuchung der linearen vertheilung galvanischer strme gefhrt wird</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kirchhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annalen der Physik und Chemie</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="497" to="508" />
			<date type="published" when="1847">1847</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Tutte</surname></persName>
		</author>
		<title level="m">Graph Theory</title>
		<imprint>
			<publisher>Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structured prediction models via the matrix-tree theorem</title>
		<author>
			<persName><forename type="first">T</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generating random spanning trees more quickly than the cover time</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Computing</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="296" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the shortest arborescence of a directed graph</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Sinica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1396" to="1400" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tractable bayesian inference of time-series dependence structure</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Siracusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LIDS Tech Report</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical dirichlet processes</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">476</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
