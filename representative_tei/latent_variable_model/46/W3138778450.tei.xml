<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A bumpy journey: exploring deep Gaussian mixture models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Margot</forename><surname>Selosse</surname></persName>
							<email>margot.selosse@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Isobel</forename><forename type="middle">Claire</forename><surname>Gormley</surname></persName>
							<email>claire.gormley@ucd.ie</email>
						</author>
						<author>
							<persName><forename type="first">Julien</forename><surname>Jacques</surname></persName>
							<email>julien.jacques@univ-lyon2.fr</email>
						</author>
						<author>
							<persName><forename type="first">Christophe</forename><surname>Biernacki</surname></persName>
							<email>christophe.biernacki@inria.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Université de Lyon</orgName>
								<address>
									<addrLine>Lyon 2</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematics and Statistics</orgName>
								<orgName type="institution">ERIC</orgName>
								<address>
									<addrLine>UR3083 5 Avenue Pierre Mendès France</addrLine>
									<postCode>69500</postCode>
									<settlement>Bron</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University College</orgName>
								<address>
									<addrLine>Dublin Dublin 4 Ireland</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Université de Lyon</orgName>
								<address>
									<addrLine>Lyon 2</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">ERIC</orgName>
								<address>
									<addrLine>UR3083 5 Avenue Pierre Mendès France</addrLine>
									<postCode>69500</postCode>
									<settlement>Bron</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Université de Lille</orgName>
								<address>
									<addrLine>INRIA 40, av. Halley -Bât A -Park</addrLine>
									<postCode>59650</postCode>
									<settlement>Plaza Villeneuve d&apos;Ascq</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A bumpy journey: exploring deep Gaussian mixture models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The deep Gaussian mixture model (DGMM) is a framework directly inspired by the finite mixture of factor analysers model (MFA) and the deep learning architecture composed of multiple layers. The MFA is a generative model that considers a data point as arising from a latent variable (termed the score) which is sampled from a standard multivariate Gaussian distribution and then transformed linearly. The linear transformation matrix (termed the loading matrix) is specific to a component in the finite mixture. The DGMM consists of stacking MFA layers, in the sense that the latent scores are no longer assumed to be drawn from a standard Gaussian, but rather are drawn from a mixture of factor analysers model. Thus the latent scores are at one point considered to be the input of an MFA and also to have latent scores themselves. The latent scores of the DGMM's last layer only are considered to be drawn from a standard multivariate Gaussian distribution. In recent years, the DGMM has gained prominence in the literature: intuitively, this model should be able to capture complex distributions more precisely than a simple Gaussian mixture model. We show in this work that while the DGMM is an original and novel idea, in certain cases it is challenging to infer its parameters. In addition, we give some insights to the probable reasons of this difficulty. Experimental results are provided on github: <ref type="url" target="https://github.com/ansubmissions/ICBINB">https://github.com/ansubmissions/ICBINB</ref>, alongside an R package that implements the algorithm and a number of ready-to-run R scripts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>such that v ig = 1 indicates that x i is generated by the gth factor analyser. The generative process of this model is as follows:</p><formula xml:id="formula_0">v i ∼ M(1, (π 1 , . . . , π G )), z i ∼ N (0, I R ), u i ∼ N (0, ψ g ), x i = η g + Λ g z i + u i ,</formula><p>where g is the index of the element of v i that is equal to 1, Λ g is a J × R matrix and η g ∈ R J is the mean vector of the gth factor analysis model. In addition, ψ g is a diagonal covariance matrix of dimension J, and π = (π 1 , . . . , π G ) are the mixing proportions. The latent variable v i = (v i1 , . . . , v iG ) represents the partitions for observation i and p(v ig = 1) = π g . The marginal density of x i is then a Gaussian mixture model (GMM):</p><formula xml:id="formula_1">f (x i ; θ) = G g=1 π g f g (x i ; η g , Σ g ),</formula><p>where θ = (η g , Λ g , ψ g , π g ) g∈{1,...,G} denotes all parameters and:</p><p>• f g (.; η g , Σ g ) is the Gaussian probability density function for component g with mean η g and covariance matrix Σ g and</p><formula xml:id="formula_2">• Σ g = Λ g Λ T g + ψ g .</formula><p>Hence, the observed-data log-likelihood of N samples x = (x 1 , . . . , x N ) is given by:</p><formula xml:id="formula_3">l o (θ; x) = N i=1 log G g=1 π g f g (x i ; η g , Σ g ).</formula><p>2 Deep Gaussian Mixture Models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Coupling Deep Learning and Gaussian Mixture Models</head><p>One of the earliest papers to define a model that stacks MFA layers in a deep learning fashion is that of <ref type="bibr" target="#b5">[6]</ref> where the authors refer to the model as the "Deep Mixture of Factor Analysers". In this model, the layers are not fully-connected and the architecture is a tree: a neuron receives an input from only one neuron of the previous layer. In <ref type="bibr" target="#b6">[7]</ref>, the authors define a DGMM model with fully-connected layers, but the matrices of weights Λ are assumed to be square. In other words, there is no dimension reduction between each MFA layer, which leads to important unidentifiability issues. In the model proposed by <ref type="bibr" target="#b8">[9]</ref>, instead of using the MFA model as a layer, the authors suggest using a mixture of factor analysers model with common factor loadings (MCFA) <ref type="bibr" target="#b0">[1]</ref>: the matrices Λ g of the same layer are therefore assumed to be equal for all g. Finally, <ref type="bibr" target="#b7">[8]</ref> defines the deep Gaussian mixture model (DGMM) with dimension reduction at each layer and provides an EM algorithm for the inference of the parameters. This present work aims at investigating the statistical efficiency of this algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Definition of the Deep Gaussian Mixture Model</head><p>This section relies on <ref type="bibr" target="#b7">[8]</ref>. Let us assume that there are L layers and that the data x = (x i ) i∈{1,...,N } are generated as follows:</p><formula xml:id="formula_4">x i = z (0) i = η (1) g1 + Λ (1) g1 z (1) i + u<label>(1)</label></formula><p>i with prob. π (1)  g1 , g 1 ∈ {1, . . . , G (1) } z</p><formula xml:id="formula_5">(1) i = η (2) g2 + Λ (2) g2 z (2) i + u (2) i with prob. π (2) g2 , g 2 ∈ {1, . . . , G (2) } . . . z (l) i = η (l+1) g l+1 + Λ (l+1) g l+1 z (l+1) i + u (l+1) i with prob. π (l+1) g l+1 , g l+1 ∈ {1, . . . , G (l+1) }<label>(1)</label></formula><p>. . .</p><formula xml:id="formula_6">z (L-1) i = η (L) g L + Λ (L) g L z (L) i + u (L) i</formula><p>with prob. π (L) g L , g L ∈ {1, . . . , G (L) }, where:</p><formula xml:id="formula_7">• z (L) i</formula><p>is a vector of size R (L) and is assumed to be drawn from the Gaussian distribution</p><formula xml:id="formula_8">N (0, I R (L) ), • u (l)</formula><p>i is a specific random error that follows a Gaussian distribution with expectation 0 and covariance matrix ψ (l)</p><formula xml:id="formula_9">g l , • η (l) g l is a vector of length R (l) , • Λ (l) g l is a matrix of dimension R (l-1) × R (l) , • J &gt; . . . &gt; R (l-1) &gt; R (l) for all l.</formula><p>The vectors u (l) i are assumed to be independent of the scores z (l) i . From these considerations, we see that at each layer l, the conditional distribution of z</p><formula xml:id="formula_10">(l) i given z (l+1) i</formula><p>is a mixture of Gaussian distributions such that:</p><formula xml:id="formula_11">f (z (l) i |z (l+1) i ; θ) = G (l+1) g l+1 =1 π g l+1 N (η (l+1) g l+1 + Λ (l+1) g l+1 z (l+1) i</formula><p>, ψ (l+1) g l+1 ).</p><p>(</p><formula xml:id="formula_12">)<label>2</label></formula><p>Figure <ref type="figure" target="#fig_0">1</ref> represents the graphical model of this DGMM in the case of L = 2. Therefore, the DGMM has parameters θ = (η (l)</p><formula xml:id="formula_13">g l , Λ<label>(l)</label></formula><p>g l , ψ</p><p>(l)</p><formula xml:id="formula_14">g l , π<label>(l)</label></formula><p>g l ) l∈{1,...,L};g l ∈{1,...,G (l) } . It also has the latent variables v (l) and z (l) where v (l) corresponds to the partition of the lth layer.</p><p>In addition, it is important to notice that the marginal probability distribution f (x i ; θ) is a GMM where each component corresponds to one of the possible paths of the network. Therefore, by noting G as the set of all possible paths through the network, the DGMM can be written:</p><formula xml:id="formula_15">f (x i ; θ) = g∈G π g N (µ g , Σ g ),<label>(3)</label></formula><p>where g = (g 1 , . . . , g L ) is one of the possible paths of the network,</p><formula xml:id="formula_16">π g = L l=1 π (l) g l ,<label>(4)</label></formula><formula xml:id="formula_17">µ g = η (1) g1 + Λ (1) g1 η (2) g2 + Λ (2) g2 . . . (η (L-1) g L-1 + Λ (L-1) g L-1 η (L) g L ) and<label>(5)</label></formula><formula xml:id="formula_18">Σ g = Λ (1) g1 Λ (2) g2 . . . (Λ (L) g L Λ (L) g L T + ψ (L) g L ) . . . Λ (2) g2 T + ψ (2) g2 Λ (1) g1 T + ψ (1) g1 .<label>(6)</label></formula><p>We refer to the GMM of Equation (3) as the "global GMM" of the DGMM so as not to confuse it with the GMMs of the layers as expressed in Equation ( <ref type="formula" target="#formula_12">2</ref>). In addition, the number of possible paths through the network is denoted as G. In fact, Equation ( <ref type="formula" target="#formula_15">3</ref>) can be generalised to all layers: the marginal distributions of all the scores z (l)</p><p>i are Gaussian mixtures. So, by noting G(l) as all the possible paths from the lth layer, and by integrating out the latent variables of the layers that follow the lth layer, we have:</p><formula xml:id="formula_19">f (z (l) i ; θ) = g(l) ∈ G(l) πg (l) N ( μ(l+1) g(l) , Σ(l+1) g(l) ),<label>(7)</label></formula><p>where g(l) = (g l+1 , . . . , g L ) is a possible path of the network from the lth layer,</p><formula xml:id="formula_20">πg (l) = L l =l+1 π g l ,<label>(8)</label></formula><formula xml:id="formula_21">μg (l) = η (l+1) g l+1 + Λ (l+1) g l+1 η (l+2) g l+2 + Λ (l+2) g l+2 . . . (η (L-1) g L-1 + Λ (L-1) g L-1 η (L)</formula><p>g L ) and ( <ref type="formula">9</ref>)</p><formula xml:id="formula_22">Σg (l) = Λ (l+1) g l+1 Λ (l+2) g l+2 . . . (Λ (L) g L Λ (L) g L T +ψ (L) g L ) . . . Λ (l+2) g l+2 T +ψ (l+2) g l+2 Λ (l+1) g l+1 T +ψ (l+1) g l+1 . (10)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inference of the model</head><p>Similar to MFA, the DGMM has parameters and latent variables to estimate. Therefore, the EM algorithm <ref type="bibr" target="#b1">[2]</ref> is a candidate to optimise the log-likelihood function with respect to the parameters and latent variables. However, in the MFA model, the computation of the expectation of the complete data log-likelihood does not involve p(z; θ) since z is assumed to be drawn from the standard Gaussian distribution N (0, I). For the DGMM, unless l = L, z</p><p>l is assumed to be drawn from an MFA with parameters (π (l+1)</p><formula xml:id="formula_24">g l+1 , η (l+1) g l+1 , Λ (l+1) g l+1 , ψ<label>(l+1)</label></formula><p>g l+1 ) g l+1 ∈{1,...,G (l+1) } . Thus, the parameters of the model are involved in the computation of p(z (l) ; θ) which makes it difficult to optimise the auxiliary function with all the layers at the same time. A solution to this issue is to perform the optimisation layer by layer in iterations of the EM algorithm. In this case, when we optimise the auxiliary function with respect to the parameters of layer l, the fact that p(z (l) ; θ) depends on the parameters of layer l + 1 is not a problem and an EM algorithm can be performed. We refer to <ref type="bibr" target="#b7">[8]</ref> for further details about the EM algorithm steps. Finally, choosing the number of clusters for each layer can be done with a model selection criterion such as the BIC <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Interpreting the inferred partitions under the DGMM</head><p>There are two main ways to use the inferred partitions of the DGMM. The first way focuses on the first layer of the network, and then have a look at the partitions v (1) . In this case, the number of clusters is equal to G (1) and the distribution of the observations of a cluster is assumed not to be Gaussian since the latent scores z (1) are assumed to be drawn through an MFA, as detailed in Equation ( <ref type="formula" target="#formula_12">2</ref>). The second way of using the DGMM is to observe the clusters of the global GMM of Equation <ref type="bibr" target="#b2">(3)</ref>. In this case, the number of clusters is equal to G (1) × . . . × G (L) , and the observations of a cluster are now assumed to follow a Gaussian distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments with the Deep Gaussian Mixture Model</head><p>We simulate data via the generative process of the DGMM with known parameters and partitions. Then, we run the EM algorithm to see if it estimates the parameters and partitions well. Because the number of parameters is large, we focus on the Adjust Rand Index (ARI), which gives an indicator of the performances of the algorithm on the estimation of the partitions and hence on the estimation of the parameters too. In these experiments we start by focusing on the partitions of the first layer, and then we focus on the partitions of the global GMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Settings for the simulated data set</head><p>For the data set x, we have N = 4, 000 and J = 17. The network was built with three layers (L = 3) and G (1) = 3, G (2) = 3 and G (3) = 2. In addition, the latent scores dimensions were set to R (1) = 11, R (2) = 6 and R (2) = 2 respectively; note this DGMM is identified in a factorial sense.. The parameters π (l) g were set to 1/G (l) for all g and for all l. The parameters (η</p><formula xml:id="formula_25">(l) g , Λ (l) g , ψ (l)</formula><p>g ) g,l were sampled from Gaussian distributions whose parameters can be found in the supplementary material files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results on the first layer</head><p>In this section, we focus on the partitions of the first layer v (1) . We run 20 times the DGMM EM algorithm and the classic GMM by using the R package mclust <ref type="bibr" target="#b4">[5]</ref>. For the DGMM runs, we fix G (2) and G (3) to their ground-truth values and vary G (1) from 2 to 6. In addition, the algorithm is run with random starting values. For the GMM runs, we vary the number of components G from 2 to 6. Table <ref type="table">1</ref> shows for each run the number of clusters with the best BIC value. We see that mclust was not able to find the true number of clusters. This result is expected since the clusters of the first layer are not supposed to be Gaussian: mclust overfits the data by finding more clusters than there truly are. However, the DGMM seems to yield the same behaviour, which is not expected since its first layer is supposed to capture non-Gaussian distributions. DGMM 6 6 6 5 6 4 4 5 5 6 5 6 5 6 5 6 6 6 6 5 mclust 6 6 6 6 6 6 6 6 6 6 5 6 6 6 6 6 6 5 5 6</p><p>Table <ref type="table">1</ref>: Number of clusters chosen with BIC for each run of the DGMM and mclust.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> represents the ARI of the runs with the best BIC value. We see that the DGMM yields better results than mclust. However, these results are not satisfying because the data was generated through the DGMM generative process: one would hope that at least some of the runs should achieve an ARI equal to 1. Let us note that similar behaviour was observed across a large number of data sets generated with a range of settings of the DGMM architecture but for brevity only one such is presented here. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results on the global GMM</head><p>For every experiment described below, the number of clusters (G (l) ) l are fixed to their true value. The DGMM's inference algorithm is run 20 times with random starting values.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> shows the resulting ARI. The DGMM yields poor results since the highest ARI value achieved is 0.67 even though the data set was generated through the DGMM's generative process.</p><p>In Figure <ref type="figure" target="#fig_3">4</ref>, we plot the BIC values over the iterations of the EM algorithm for a single random initialisation; the BIC reaches a plateau, meaning that the EM algorithm achieves at least a local maximum of the likelihood.   If the EM algorithm is able to find local maxima, but fails to find the right partitions, it might mean that there are many local maxima. To verify this, we run the EM algorithm a hundred times and compute the ARI of each resulting clustering with respect to every other resulting clustering. The mutual ARI are plotted in Figure <ref type="figure" target="#fig_4">5</ref> (left and right), where we see that none of the mutual ARI values are 1 (except the ARI of a result with respect to itself). This means that the EM algorithm found 100 different local maxima leading to very different partitions.</p><p>The last result we detail regards another unexpected behaviour of the DGMM's inference algorithm.</p><p>When we list the resulting partition for each observation on each layer, we can see that all the clusters of every layer are represented. However, when we list the resulting partitions of the global GMM as in Table <ref type="table" target="#tab_0">2</ref>, we see that not all the clusters are represented. This means that even if each neuron of each layer has observations, not every combination of neurons of the three layers does. Therefore, there is a severe problem of empty clusters at the global GMM's level.</p><p>Finally, let us note that the package mclust was not able to find a solution for G = l G (l) = 18. From this result, the DGMM can be seen as a candidate when the number of clusters is high, even if the estimation of partitions is not perfect. We see that the latent scores (z (l) ) l do not seem to be estimated well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Possible reasons for the poor results</head><p>There are several possible reasons for the poor results described in the previous section. First of all, the MFA model already suffers from unindentifiability issues that could worsen with the multilayer architecture of the DGMM. The most common solutions to tackle the unidentifiablity issues are:</p><formula xml:id="formula_26">• To constrain Λ (l)</formula><p>g so that it is orthonormal and to order the columns by decreasing variance of the corresponding latent scores.</p><formula xml:id="formula_27">• To constrain Λ (l) g T ψ (l) g Λ (l)</formula><p>g to be diagonal.</p><formula xml:id="formula_28">• To constrain Λ (l)</formula><p>g so that it is a lower triangular full rank matrix with diagonal elements strictly positive.</p><p>• To choose an informative rotation matrix. Many heuristic methods try to find rotation matrices P that can be used to modify Λ (l) g (and the latent factors) so as to try to increase the interpretability.</p><p>While we already tried to apply these constraints (results not detailed in this document), we did not obtained better results regarding the partitions and parameters estimation.</p><p>Second, the DGMM has many latent variables, which can be a severe problem when it comes to estimation. Indeed, in the context of the configuration of Section 3.1, the latent scores alone represent N × (R (1) + R (2) + R (3) ) = 4, 000 × (11 + 6 + 2) = 76, 000 continuous values to estimate, which is very high. This large number of latent variables is for us one of the most important obstacles to correct estimation. To support this point, Table <ref type="table" target="#tab_1">3</ref> shows the ARI that the EM algorithm achieved at each layer. At the first layer, the ARIs are globally higher than the other layers and can even be equal to 1. However, the ARI for the other layers worsens with the depth: the second layer's higher ARI value is 0.51 and the third layer's ARI value are all near to zero. This probably means that the latent scores are poorly estimated since the DGMM of layer 1 depends on the data x, the DGMM of layer 2 depends on the scores z (1) and the DGMM of layer 3 depends on the scores z (2) . A solution to bypass the latent score estimation is to use another kind of algorithm, such as the Newton-Raphson algorithm and its variants. However, we encountered different problems when using this kind of algorithm such as computational errors due to matrix inversions. Finally, we have also observed that even though the clusters of each layer do not necessarily become empty, the clusters of the global GMM do so, which is also a problem for estimation of the partition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In theory, the DGMM is a powerful model that can estimate complex distributions by stacking MFA layers in a deep learning fashion. We have seen in this Section 3.3 that this algorithm is able to provide an estimation of partitions and parameters in cases where mclust is not able to do so, for instance when the number of clusters is high. However, this estimation is not correct and we have seen that the EM algorithm suffers from drawbacks and can struggle to find the global maxima.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical model of an example of the DGMM.</figDesc><graphic coords="3,216.90,300.30,178.20,176.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ARI of the DGMM's first layer and of the mclust result.</figDesc><graphic coords="5,216.90,443.48,178.19,127.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Boxplot of the ARI for the global GMM of the DGMM using random initialisations.</figDesc><graphic coords="6,114.73,72.00,121.17,121.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: BIC values over the iterations of the EM algorithm using a single random initialisation.</figDesc><graphic coords="6,263.23,72.02,228.10,132.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Mutual ARI of 100 runs of the EM algorithm. None of them gets the same final partition.</figDesc><graphic coords="6,108.00,270.66,396.02,211.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>The non-empty clusters that are represented in the global GMM for the 20 runs.</figDesc><table><row><cell>Cluster numbers 1, 2, 4, 5, 6, 8, 10, 11, 12, 13, 14, 18 1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16 1, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 16, 17</cell><cell cols="3">ARI of layer 1 ARI of layer 2 ARI of layer 3 0.18 0.5 0 0.17 0.17 0.01 0.2 0.25 0.01</cell></row><row><cell>2, 3, 4, 5, 6, 7, 11, 12, 13, 14, 15, 16, 18</cell><cell>0.45</cell><cell>0.19</cell><cell>0</cell></row><row><cell>1, 2, 3, 5, 6, 7, 9, 10, 12, 14, 15, 16, 18</cell><cell>0.32</cell><cell>0.29</cell><cell>0.01</cell></row><row><cell>1, 2, 4, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17, 18</cell><cell>0.38</cell><cell>0.09</cell><cell>0</cell></row><row><cell>1, 2, 5, 6, 8, 10, 11, 12, 14, 15, 17 2, 3, 4, 5, 6, 9, 11, 12, 13, 14, 16, 18 2, 3, 4, 7, 9, 10, 11, 12, 13, 15, 16, 17, 18 1, 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 17, 18 1, 2, 3, 5, 6, 7, 9, 10, 11, 14, 15, 16, 18</cell><cell>0.43 0.4 0.55 0.11 0.17 0.17</cell><cell>0.07 0.22 0.17 0.24 0.37 0.19</cell><cell>0 0 0.01 0.01 0 0</cell></row><row><cell>2, 3, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16</cell><cell>1</cell><cell>0.08</cell><cell>0.01</cell></row><row><cell>1, 2, 4, 5, 6, 7, 8, 10, 11, 13, 14, 15, 16, 17, 18</cell><cell>0.19</cell><cell>0.42</cell><cell>0.01</cell></row><row><cell>1, 2, 3, 4, 5, 8, 9, 10, 11, 12, 14, 17, 18</cell><cell>0.5</cell><cell>0.51</cell><cell>0</cell></row><row><cell>1, 2, 4, 5, 8, 9, 10, 11, 13, 14, 17, 18 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 17 1, 2, 3, 4, 6, 7, 9, 10, 11, 12, 15, 16, 17 1, 4, 5, 8, 10, 13, 14, 15, 17, 18 1, 3, 4, 7, 8, 9, 10, 11, 12, 13, 15, 17, 18</cell><cell>0.22 0.4 0.29 0.55 0.52</cell><cell>0.51 0.21 0.09 0.07 0.21</cell><cell>0 0 0 0 0</cell></row><row><cell>1, 5, 7, 8, 9, 10, 14, 16, 17, 18</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>ARI results for the GMM of each layer.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>We would like to warmly thank <rs type="person">Cinzia Viroli</rs> and <rs type="person">Geoff McLachlan</rs> for their helpful discussions from which the work benefitted.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixtures of factor analyzers with common factor loadings: Applications to the clustering and visualization of high-dimensional data</title>
		<author>
			<persName><forename type="first">Jangsun</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lloyd</forename><surname>Flack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1298" to="1309" />
			<date type="published" when="2010-07">07 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of he Royal Statistical Society, series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modelling high-dimensional data by mixtures of factor analyzers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Bean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="388" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Estimating the dimension of a model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">mclust 5: clustering, classification and density estimation using Gaussian finite mixture models</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Scrucca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The R Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="205" to="233" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep mixtures of factor analysers</title>
		<author>
			<persName><forename type="first">Yichuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML&apos;12</title>
		<meeting>the 29th International Coference on International Conference on Machine Learning, ICML&apos;12<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1123" to="1130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Factoring variations in natural images with deep Gaussian mixture models</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3518" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Gaussian mixture models</title>
		<author>
			<persName><forename type="first">Cinzia</forename><surname>Viroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="51" />
			<date type="published" when="2019-01">Jan 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep mixtures of factor analyzers with common loadings: A novel deep generative approach to clustering</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing -24th International Conference</title>
		<meeting><address><addrLine>Guangzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-11-14">2017. November 14-18, 2017. 2017</date>
			<biblScope unit="page" from="709" to="719" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
