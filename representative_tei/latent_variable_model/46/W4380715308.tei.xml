<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised speech enhancement with deep dynamical generative speech and noise models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-06-13">13 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaoyu</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria Grenoble Rhône-Alpes</orgName>
								<orgName type="institution">Univ. Grenoble Alpes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Leglaive</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">CentraleSupélec</orgName>
								<orgName type="institution">IETR (UMR CNRS</orgName>
								<address>
									<postCode>6164)</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Girin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Grenoble-INP</orgName>
								<orgName type="institution" key="instit4">GIPSA-lab</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xavier</forename><surname>Alameda-Pineda</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria Grenoble Rhône-Alpes</orgName>
								<orgName type="institution">Univ. Grenoble Alpes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised speech enhancement with deep dynamical generative speech and noise models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-13">13 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2306.07820v1[eess.AS]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised speech enhancement</term>
					<term>dynamical variational autoencoders</term>
					<term>deep dynamical generative model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work builds on a previous work on unsupervised speech enhancement using a dynamical variational autoencoder (DVAE) as the clean speech model and non-negative matrix factorization (NMF) as the noise model. We propose to replace the NMF noise model with a deep dynamical generative model (DDGM) depending either on the DVAE latent variables, or on the noisy observations, or on both. This DDGM can be trained in three configurations: noise-agnostic, noise-dependent and noise adaptation after noise-dependent training. Experimental results show that the proposed method achieves competitive performance compared to state-of-the-art unsupervised speech enhancement methods, while the noise-dependent training configuration yields a much more time-efficient inference process.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Speech enhancement is a fundamental task of speech processing that aims at recovering the clean speech signal from a noisy audio recording <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref>. In recent years, methods based on deep neural networks (DNNs) have greatly advanced research in this field. The most widely-used approach is a direct supervised mapping from the noisy speech signal to either the clean speech target or a denoising mask, see a review in <ref type="bibr" target="#b3">[3]</ref>. While this approach has shown impressive results, it also has limitations: it requires a huge amount of paired noisy-clean speech data for training and can show poor generalization to noise types and acoustic conditions unseen during training. Another type of supervised method resort to generative adversarial networks (GANs) to learn a conditional distribution of the clean speech signal given the noisy speech <ref type="bibr" target="#b4">[4,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b6">6]</ref>. Recently, methods based on diffusion models were proposed in, e.g., <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8]</ref>. A diffusion model transforms a clean signal into a noisy one by adding noise step by step, and speech enhancement is obtained by applying the inverse diffusion process conditioned on the input noisy speech signal. These methods show good generalization capability, but still require a large amount of paired data for training and are quite slow at inference.</p><p>Unsupervised speech enhancement methods were recently developed to improve the performance of models on unseen noise types. Here, the models do not use parallel clean-noisy data for training. Instead, they use either non-parallel noisyclean data (i.e., the clean and noisy samples do not correspond) <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10]</ref>, or clean data only <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13]</ref>, or noisy data only This research was supported by ANR-3IA MIAI (ANR-19-P3IA-0003), ANR-JCJC ML3RI (ANR-19-CE33-0008-01), H2020 SPRING (funded by EC under GA #871245). <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17]</ref>. Unsupervised speech enhancement methods can be further divided into noise-dependent (ND) and noiseagnostic (NA) methods <ref type="bibr" target="#b13">[13]</ref>. ND methods use noise or noisy speech training samples to learn some noise characteristics. In contrast, NA methods only use clean speech signals for training and the noise characteristics are estimated at test time for each noisy speech sequence to process. A typical unsupervised NA approach uses a pre-trained variational autoencoder (VAE) as a prior distribution of the clean speech signal and a nonnegative matrix factorization (NMF) model for the noise variance <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19]</ref>. The NMF parameters and the VAE latent vector are estimated at test time from the noisy signal and combined to build a denoising Wiener filter. Further developments in this general line were proposed in, e.g., <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref>.</p><p>Recently, <ref type="bibr" target="#b13">[13]</ref> proposed to replace the VAE by a dynamical variational autoencoder (DVAE) <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25]</ref>, yielding better clean speech modeling by considering the temporal dependencies across successive spectrogram frames. The algorithm proposed in <ref type="bibr" target="#b13">[13]</ref> was shown to achieve very competitive performance even when compared to supervised approaches. However, this algorithm has two main drawbacks. First, the NMF may be a too simple model for many real-world noise signals, which are poorly described in the spectrogram domain as a nonnegative linear combination of a few spectral templates. Second, at test time, the inference algorithm, which must be run on each noisy sequence independently, is very time-consuming.</p><p>In this work, we aim at both increasing the modeling power of the noise model and accelerating the inference process. To this aim, we build on <ref type="bibr" target="#b13">[13]</ref> and propose to replace the NMF noise model with a deep dynamical generative model (DDGM), which is a general class of dynamical models for the generation of sequential data based on DNNs. <ref type="foot" target="#foot_0">1</ref> We implement and test the DDGM noise model with dependencies either on: the DVAE latent variables (LV), or the noisy observations (NO), or both (NOLV). Moreover, these three variants are implemented and tested in both ND (using a large noisy speech dataset) or NA configurations. Even further, the models trained in ND configuration can then be fine-tuned on each noisy speech test sequence to get adapted to specific noise types in the test set (i.e., ND followed by noise adaptation). Experimental results show that the proposed method obtains performance that is comparable to that of <ref type="bibr" target="#b13">[13]</ref>, while in the ND configuration, it requires much less computation time during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed method</head><p>In this section, we present our unsupervised speech enhancement method using DDGM-based speech and noise models.  f =1 ∈ C F is the short-time spectrum at time frame t, and f denotes the frequency bin. Let z1:T ∈ R L×T denote the associated latent vector sequence, with L ≪ F the latent dimension. As reported in <ref type="bibr" target="#b13">[13]</ref>, among several tested DVAE models, the recurrent variational autoencoder (RVAE) model <ref type="bibr" target="#b26">[26]</ref> worked best on the speech enhancement task. So, we also use this model in this work. The RVAE generative model is defined as:</p><formula xml:id="formula_0">p θs (s1:T , z1:T ) = T t=1 p θs (st|z1:t)p(zt).<label>(1)</label></formula><p>For each time frame t, and conditionally to z1:t, st is assumed to follow a circularly-symmetric zero-mean complex Gaussian distribution <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref>, p θs (st|z1:t) = Nc st; 0, diag(v θs,t ) . In this work, all covariance matrices are assumed diagonal and are represented by the vector of diagonal entries. Here, v θs,t ∈ R F + is a function of z1:t and is modeled with the RVAE decoder.</p><p>The latent vector zt is assumed to follow a standard Gaussian prior distribution, p(zt) = N (zt; 0, I).</p><p>The inference model, i.e. the approximated posterior distribution of RVAE, is defined as <ref type="bibr" target="#b26">[26]</ref>:</p><formula xml:id="formula_1">q ϕz (z1:T |s1:T ) = T t=1 q ϕz (zt|z1:t-1, st:T ),<label>(2)</label></formula><p>with zt assumed to follow a (real-valued) Gaussian distribution, q ϕz (zt|z1:t-1, st:T ) = N zt; µ ϕz,t , diag(v ϕz,t ) , where µ ϕz,t ∈ R L and v ϕz,t ∈ R L + are both a function of z1:t-1 and st:T , which is modeled with the RVAE encoder. <ref type="foot" target="#foot_1">2</ref>The RVAE model is pre-trained on a clean speech dataset by maximizing the evidence lower bound (ELBO) <ref type="bibr" target="#b26">[26]</ref>:</p><formula xml:id="formula_2">L(θs, ϕz; s1:T ) = - T t=1 Eq ϕz dIS(|st| 2 , v θs,t ) + DKL q ϕz (zt|z1:t-1, st:T )||p(zt) ,<label>(3)</label></formula><p>where modulus and exponentiation are element-wise, dIS(•, •) is the Itakura-Saito divergence <ref type="bibr" target="#b27">[27]</ref> and DKL(•||•) is the Kullback-Leibler divergence (KLD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">DDGM-based noise model</head><p>Let x1:T = {xt} T t=1 ∈ C F ×T and n1:T = {nt} T t=1 ∈ C F ×T denote respectively the complex-valued STFT spectrogram of the noisy speech and the noise, which is assumed additive:</p><formula xml:id="formula_3">x1:T = s1:T + n1:T . (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>At each time frame t, nt is assumed to follow a circularlysymmetric zero-mean complex Gaussian distribution:</p><formula xml:id="formula_5">p θn (nt) = Nc nt; 0, diag(v θn,t ) ,<label>(5)</label></formula><p>where v θn,t ∈ R F + is the noise variance vector. In several previous works, v θn,t was modeled with NMF, i.e. factorized into the product of two low-rank non-negative matrices. In this work, we model v θn,t with a DDGM. We propose three different noise model dependencies: (i) DVAE latent variables (LV), in which v θn,t is a function of the whole sequence of the DVAE latent vectors, i.e. v θn,t = v θn,t (z1:T ); (ii) noisy observations (NO), in which v θn,t is a function of all the past values of the noisy speech, i.e. v θn,t = v θn,t (x1:t-1); and (iii) both noisy observations and DVAE latent variables (NOLV), in which v θn,t is a function of all the past values of the noisy speech as well as the past and present values of the DVAE latent vectors, i.e. v θn,t = v θn,t (x1:t-1, z1:t). For clarity of presentation, let pt denote the input of the noise model, i.e. pt = z1:T in LV, pt = x1:t-1 in NO, and pt = {x1:t-1, z1:t} in NOLV. For all model dependencies (NO, LV, or NOLV), the noise variance v θn,t is a function of pt that is implemented by a DNN.</p><p>Applying the chain rule and taking into account the conditional dependencies, the generative model over the set of variables {x1:T , s1:T , z1:T } is given by: p θ (x1:T , s1:T , z1:T ) = T t=1 p θn (xt|st, pt)p θs (st|z1:t)p(zt), <ref type="bibr" target="#b6">(6)</ref> where</p><formula xml:id="formula_6">p θn (xt|st, pt) = Nc xt; st, diag(v θn,t (pt))<label>(7)</label></formula><p>is deduced from ( <ref type="formula" target="#formula_3">4</ref>) and ( <ref type="formula" target="#formula_5">5</ref>), p θs (st|z1:t) and p(zt) are defined in Section 2.1, and θ = θs ∪ θn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Speech enhancement with the inference model</head><p>The posterior distribution corresponding to the generative model ( <ref type="formula">6</ref>) factorizes as follows:</p><formula xml:id="formula_7">p θ (s1:T , z1:T |x1:T ) = T t=1 p θ (st|z1:t, xt, pt)p θ (zt|z1:t-1, x1:T ).<label>(8</label></formula><p>) For each time frame t, p θ (st|z1:t, xt, pt) can be computed in closed form as a complex Gaussian distribution p θ (st|z1:t, xt, pt) = Nc(st; µ θ,t , diag(v θ,t )), with</p><formula xml:id="formula_8">µ θ,t = v θs,t (z1:t) v θs,t (z1:t) + v θn,t (pt) xt,<label>(9)</label></formula><formula xml:id="formula_9">v θ,t = v θs,t (z1:t)v θn,t (pt) v θs,t (z1:t) + v θn,t (pt) ,<label>(10)</label></formula><p>where vector multiplication and division are element-wise. Eq. ( <ref type="formula" target="#formula_8">9</ref>) provides the clean speech signal minimum mean squared error (MMSE) estimate, which corresponds to the Wiener filter output. The distribution p θ (zt|z1:t-1, x1:T ) is intractable and cannot be used directly to recursively provide the z1:T estimate. We thus approximate it with the RVAE inference model (defined in Section 2.1):</p><formula xml:id="formula_10">p θ (zt|z1:t-1, x1:T ) ≈ q ϕz (zt|z1:t-1, xt:T ).<label>(11)</label></formula><p>Here, the RVAE encoder, pre-trained on a clean speech signal dataset, takes as input the noisy speech signal, and must thus be adapted to such kind of input (see the next sub-section). In the following, we inject ( <ref type="formula" target="#formula_10">11</ref>) into <ref type="bibr" target="#b8">(8)</ref>, and the resulting approximate joint posterior is denoted by p θ,ϕz (s1:T , z1:T |x1:T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Model optimization</head><p>We recall that the parameters {θs, ϕz} are learned by pretraining the RVAE on a clean speech dataset. θs is then fixed during the speech enhancement stage, whereas ϕz has to be fine-tuned on the noisy signal(s), and we also have to estimate the noise model parameters θn. As is usually done in variational inference algorithms, the parameters are optimized by maximizing the ELBO, which is here defined as: </p><formula xml:id="formula_11">L(</formula><p>Given the factorizations ( <ref type="formula">6</ref>) and ( <ref type="formula" target="#formula_7">8</ref>), and the fact that all involved distributions are Gaussian, ( <ref type="formula" target="#formula_12">12</ref>) can be developed as:</p><formula xml:id="formula_13">L(θn, ϕz; x1:T ) = - T t=1</formula><p>Eq ϕz dIS(|xt| 2 , v θs,t + v θn,t )</p><formula xml:id="formula_14">+ DKL q ϕz (zt|z1:t-1, xt:T )||p(zt) .<label>(13)</label></formula><p>As mentioned before, the model can be trained in either NA or ND configuration. When trained in NA configuration, the parameters {θn, ϕz} are estimated directly from the noisy speech sequence to be enhanced. This is done by optimizing the ELBO (13) independently on each single noisy speech sequence for a certain number of iterations. Afterwards, the clean speech estimate is computed with (9), using the optimal parameters and latent vectors sampled from the encoder. This configuration allows the model to adapt to the specific noise patterns of each test sequence, without the need for any prior knowledge or training data on the noise type. This makes it suitable for scenarios where the noise type is unknown. When trained in ND configuration, the model parameters are estimated by optimizing the ELBO (13) on a large noisy speech training set using stochastic gradient descent (SGD) optimization (we recall that no parallel noisy-clean data is thus used). Then at test time, the clean speech is computed using ( <ref type="formula" target="#formula_8">9</ref>) with a single forward pass of the model on the noisy test sequence. This results in a much more time-efficient inference than methods based on an NMF noise model, while still achieving competitive performance. A schematic view of the proposed method is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets and pre-processing</head><p>We used two datasets to evaluate the proposed method: the WSJ0-QUT dataset introduced in <ref type="bibr" target="#b26">[26]</ref> and reused in <ref type="bibr" target="#b13">[13]</ref>, and the publicly available VoiceBank-DEMAND (VB-DMD) dataset <ref type="bibr" target="#b29">[29]</ref>. WSJ0-QUT is obtained by mixing clean signals from the Wall Street Journal (WSJ0) dataset <ref type="bibr" target="#b30">[30]</ref> with various types of noise signals from the QUT-NOISE dataset <ref type="bibr" target="#b31">[31]</ref> with three different signal-to-noise ratio (SNR) values: -5, 0 and 5 dB. It contains 12,765 utterances from 101 speakers, 1,026 utterances from 10 speakers and 651 utterances from 8 speakers for model training, validation and test, respectively. VB-DMD is obtained by mixing clean signals from the VoiceBank (VB) corpus <ref type="bibr" target="#b32">[32]</ref> with ten types of noise from the DEMAND noise dataset <ref type="bibr" target="#b33">[33]</ref>. Following <ref type="bibr" target="#b17">[17]</ref>, we used 10,802 utterances from 26 speakers for training, 770 utterances from 2 other speakers for validation, and 824 utterances from 2 other speakers for test. The SNR values used for the training set are 15, 10, 5 and 0 dB, while the SNR values used for the test set are 17.5, 12.5, 7.5, and 2.5 dB. For each dataset, we first pre-trained the RVAE model on the clean speech dataset, i.e. WSJ0 or VB; then we estimated the noise model parameters using the noisy speech data, either in the NA or ND configuration (see Section 2.4 ).</p><p>Before being input into the neural networks, the audio signals are pre-processed as follows. We compute the STFT with a 64-ms sine window (1,024 samples) and a 75%-overlap (256-sample shift), resulting in a sequence of 513-dimensional discrete Fourier coefficients (for positive frequencies). The squared modulus of the STFT coefficients is computed afterwards. For the RVAE pre-training and the speech enhancement model trained in ND configuration, we first use a voice activity detection threshold of 30 dB to remove silence portions at the beginning and the end of the signals, and rescale the waveforms in [-1, 1] before computing the STFT coefficients. And we also split the training utterances into smaller sequences of length T = 100 frames. At test time, the model is evaluated on the complete noisy test utterances, which can be of variable length. The speech enhancement model in NA configuration is trained and evaluated directly on each single complete noisy test utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation details and training settings</head><p>The RVAE architecture closely follows the one used in <ref type="bibr" target="#b13">[13]</ref>, with the exception of replacing the bidirectional LSTM (BLSTM) layers in both the encoder and decoder with standard LSTM layers, since we use here the causal version of RVAE <ref type="bibr" target="#b26">[26]</ref>. The latent vector dimension was set to L = 16.</p><p>The NO noise model is implemented using an LSTM layer that takes as input at time t the past noisy speech vectors x1:t-1, followed by a multi-layer perceptron (MLP) with a tanh activation function, except for the output layer, which is linear, and which provides the noise log-variance vector log v θn,t (pt). The architecture of the NOLV and LV noise model are similar to that of the NO noise model, except that the NOLV model uses two LSTM layers, one to encode information from the past noisy speech vectors x1:t-1 and another one to process the past and present latent vectors z1:t, and the LV noise model uses a single BLSTM layer to encode information from the complete latent vector sequence z1:T . For all training processes, we used the Adam optimizer <ref type="bibr" target="#b34">[34]</ref> with parameters β1 = 0.9, β2 = 0.99, ϵ = 10 -9 . For RVAE pre-training and ND training configuration, we decayed the learning rate (from 5 × 10 -4 to 10 -8 ) with a cosine annealing scheduler <ref type="bibr" target="#b35">[35]</ref>. The models are trained in maximum 500 epochs and the validation set is used to select the best models. During the RVAE pre-training, we applied linear warm-up to the KL term in (3) during the first 20 epochs <ref type="bibr" target="#b36">[36]</ref>.</p><p>Table <ref type="table">1</ref>: Speech enhancement results. S stands for supervised, U-NA stands for unsupervised noise-agnostic and U-ND stands for unsupervised noise-dependent, U-NDA stands for U-ND training followed by noise adaptation fine-tuning. Except for the RTF, the baselines scores are taken from the corresponding papers. The best scores are in bold and the second best scores are underlined. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Baselines and evaluation metrics</head><p>We compare our method with both supervised and unsupervised speech enhancement baselines. For supervised baselines, we considered Open-Unmix (UMX) <ref type="bibr" target="#b37">[37]</ref> and MetricGAN+ <ref type="bibr" target="#b6">[6]</ref>, which are BLSTM-based methods, and CDiffuSE <ref type="bibr" target="#b7">[7]</ref> and SGMSE+ <ref type="bibr" target="#b8">[8]</ref>, which are diffusion-based methods. For unsupervised baselines, we compared to MetricGAN-U <ref type="bibr" target="#b17">[17]</ref>, NyTT <ref type="bibr" target="#b16">[16]</ref>, and RVAE-VEM <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b13">13]</ref>.</p><p>As for the speech enhancement performance metrics, we used the scale-invariant signal-to-distortion ratio (SI-SDR) <ref type="bibr" target="#b38">[38]</ref> in dB, the perceptual evaluation of speech quality (PESQ) score <ref type="bibr" target="#b39">[39]</ref> (in [-0.5, 4.5]), and the extended short-time objective intelligibility (ESTOI) score <ref type="bibr" target="#b40">[40]</ref> (in [0, 1]). We also evaluated the computational efficiency of the inference (denoising algorithm) for RVAE-VEM, SGMSE+ and the proposed method (in different configurations) using the average real-time factor (RTF), which is the time required to process 1 second of audio. 3   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Experimental results</head><p>The speech enhancement results are reported in Table <ref type="table">1</ref>. When tested in the NA configuration, our method has a better performance than the unsupervised baselines on both datasets in terms of the SI-SDR. The results are actually comparable to those of the supervised methods (although this can depend on the metrics), even though we have never used pairs of aligned noisyclean speech data for training. This shows the ability of the proposed model to adapt to the noise characteristics in the NA configuration. More specifically, in the NA configuration, the RVAE-NO model performs slightly better than the other two models on the WSJ0-QUT dataset while the three noise model variants (NO, NOLV, LV) lead to very similar performance on the VB-DMD dataset. However, the RVAE-NO model and RVAE-NOLV model have a slight drop of performance when trained in ND configuration. This may be due to the mismatch between train and test data in this ND configuration (different types of noise being used for training and test). In contrast, the RVAE-LV model reveals very robust when used in the ND con- 3 All of the RTF values are computed on NVIDIA Quadro RTX 4000 GPU, in a machine with an Intel(R) Xeon(R) W-2145 CPU @ 3.70GHz and averaged on 10 sequences. figuration. This may be because estimating the noise variance only from the latent vectors, without using the noisy speech vectors, helps to alleviate the training/test data mismatch issue.</p><p>As for the computational cost, in the NA configuration, the RTF mainly depends on the number of iterations run on each test sequence. It can be seen from Table <ref type="table">1</ref> that, in general, achieving good performance in the NA configuration is at the price of very high RTF values. In contrast, in the ND configuration, the inference process only requires a single forward pass of the trained model, resulting in a much lower RTF value of 0.02 for all of the proposed model variants. Due to the timeconsuming inverse diffusion process, the state-of-the-art supervised baseline SGMSE+ has an RTF value that is much higher than the proposed unsupervised model in the ND configuration, while the two methods have similar performance in terms of SI-SDR (however, the RTF of SGMSE+ remains much lower than the proposed model in the NA configuration).</p><p>Finally, after being trained in the ND configuration, the proposed model can further be fine-tuned on each noisy test sequence, just like in the NA configuration. This new 'hybrid' mode is referred to as NDA in Table <ref type="table">1</ref>. We found that on the VB-DMD dataset, after just a few iterations of fine-tuning, the performance of all the three model variants were greatly improved (over the ND configuration). This is also true on WSJ0-QUT, but at the price of more fine-tuning iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We presented a new unsupervised speech enhancement model that uses a DDGM for both speech and noise. We tested three different dependencies for the noise model (NO, NOLV, LV), as well as three 'training/testing' configurations (NA, ND, and ND + noise adaptation). Experiments show that in the NA configuration, our model outperforms several unsupervised baselines (including RVAE+NMF), and competes well with the supervised baselines. In the ND configuration, our model provides a very fast inference process with only minimal performance degradation (especially for RVAE-LV). Furthermore, the ND + noise adaptation configuration enables the model to adapt to specific noise types and further improve performance, with much less iterations than in the NA configuration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schematic view of the proposed speech enhancement method.</figDesc><graphic coords="2,57.60,73.99,481.88,68.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2. 1 .</head><label>1</label><figDesc>Clean speech modeling with an RVAE We work in the short-time Fourier transform (STFT) domain. Let s1:T = {st} T t=1 ∈ C F ×T denote the STFT spectrogram of the clean speech. Each vector st = {s t,f } F</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>θn, ϕz; x1:T ) = Ep θ,ϕz log p θ (x1:T , s1:T , z1:T ) p θ,ϕz (s1:T , z1:T |x1:T ) .</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that the DVAE used for clean speech modeling also belongs to the DDGM family, hence the paper title.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In practice, the squared modulus of the s t:T entries are send to the encoder input instead of the complex-valued STFT coefficients.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Speech Enhancement</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
		<title level="m">Speech enhancement: Theory and practice</title>
		<imprint>
			<publisher>CRC press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1702" to="1726" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SEGAN: Speech enhancement generative adversarial network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MetricGAN: Generative adversarial networks based black-box metric scores optimization for speech enhancement</title>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML<address><addrLine>Long Beach, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MetricGAN+: An improved version of Met-ricGAN for speech enhancement</title>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-A</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W V</forename><surname>Plantinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conditional diffusion probabilistic model for speech enhancement</title>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Speech enhancement and dereverberation with diffusion-based generative models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Welker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Lemercier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.05830</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A parallel-data-free speech enhancement method using multi-objective learning cycle-consistent generative adversarial network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1826" to="1838" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CycleGAN-based non-parallel speech enhancement with an adaptive attention-in-attention mechanism</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. APSIPA</title>
		<meeting>APSIPA<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical speech enhancement based on probabilistic integration of variational autoencoder and non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Itoyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yoshii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Calgary, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A variance modeling framework based on variational autoencoders for speech enhancement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MLSP</title>
		<meeting>MLSP<address><addrLine>Aalborg, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised speech enhancement using dynamical variational autoencoders</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2993" to="3007" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving deep speech denoising by noisy2noisy signal mapping</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alamdari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Azarang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Acoustics</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page">107631</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speech denoising without clean training data: a noise2noise approach</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tambwekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Manohara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Noisytarget training: A training strategy for DNN-based speech enhancement without clean speech</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fujimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yatabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miyazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EUSIPCO, virtual conf</title>
		<meeting>EUSIPCO, virtual conf</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MetricGAN-U: Unsupervised speech enhancement / dereverberation based only on noisy / reverberated speech</title>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A statistically principled and computationally efficient approach to speech enhancement using variational autoencoders</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast and efficient speech enhancement with variational autoencoders</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP, 2023</title>
		<meeting>ICASSP, 2023</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Speech enhancement with variational autoencoders and alphastable distributions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Guided variational autoencoder for speech enhancement with a supervised classifier</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carbajal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised multichannel speech enhancement with variational autoencoders and non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A bayesian permutation training deep representation learning method for speech enhancement with variational autoencoder</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Højvang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="381" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamical variational autoencoders: A comprehensive review</title>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hueber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="175" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A benchmark of dynamical variational autoencoders applied to speech spectrogram modeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hueber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A recurrent variational autoencoder for speech enhancement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization with the Itakura-Saito divergence: With application to music analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bertin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Durrieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comp</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="793" to="830" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gaussian processes for underdetermined source separation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3155" to="3167" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Investigating RNN-based speech enhancement methods for noiserobust text-to-speech</title>
		<author>
			<persName><forename type="first">C</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SSW</title>
		<meeting>SSW<address><addrLine>Sunnyvale, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">CSR-I (WSJ0) Sennheiser LDC93S6B</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pallett</surname></persName>
		</author>
		<ptr target="https://catalog.ldc.upenn.edu/LDC93S6B" />
	</analytic>
	<monogr>
		<title level="m">Philadelphia: Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The QUT-NOISE-SRE protocol for the evaluation of noisy speaker recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanagasundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Voice Bank corpus: Design, collection and data analysis of a large regional accent speech database</title>
		<author>
			<persName><forename type="first">C</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COCOSDA</title>
		<meeting>COCOSDA<address><addrLine>Gurgaon, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The diverse environments multi-channel acoustic noise database (DEMAND): A database of multichannel environmental noise recordings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thiemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="3591" to="3591" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ladder variational autoencoders</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Open-Unmix -A reference implementation for music source separation</title>
		<author>
			<persName><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Open Source Software</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">41</biblScope>
			<biblScope unit="page">1667</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SDR -Half-baked or well done?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ) -A new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of time-frequency weighted noisy speech</title>
		<author>
			<persName><forename type="first">C</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
