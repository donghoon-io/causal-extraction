<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLASSIFYING NON-GAUSSIAN AND MIXED DATA SETS IN THEIR NATURAL PARAMETER SPACE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cécile</forename><surname>Levasseur</surname></persName>
							<email>clevasseur@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Jacobs School of Engineering ‡ Department of Mathematics</orgName>
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">San Diego University of Utah</orgName>
								<address>
									<addrLine>La Jolla</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Uwe</forename><forename type="middle">F</forename><surname>Mayer</surname></persName>
							<email>mayer@math.utah.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ken</forename><surname>Kreutz-Delgado</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Jacobs School of Engineering ‡ Department of Mathematics</orgName>
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">San Diego University of Utah</orgName>
								<address>
									<addrLine>La Jolla</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Salt Lake City</orgName>
								<address>
									<region>UT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CLASSIFYING NON-GAUSSIAN AND MIXED DATA SETS IN THEIR NATURAL PARAMETER SPACE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of both supervised and unsupervised classification for multidimensional data that are nongaussian and of mixed types (continuous and/or discrete). An important subclass of graphical model techniques called Generalized Linear Statistics (GLS) is used to capture the underlying statistical structure of these complex data. GLS exploits the properties of exponential family distributions, which are assumed to describe the data components, and constrains latent variables to a lower dimensional parameter subspace. Based on the latent variable information, classification is performed in the natural parameter subspace with classical statistical techniques. The benefits of decision making in parameter space is illustrated with examples of categorical data text categorization and mixed-type data classification. As a text document preprocessing tool, an extension from binary to categorical data of the conditional mutual information maximization based feature selection algorithm is presented.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The complexity of data generally comes from the possible existence of an extremely large number of components and from the fact that the components are often of mixed types, i.e., some components might be continuous (with different underlying distributions) and some components might be discrete (categorical, count or Boolean). This is typically the case in drug discovery, health care, or fraud detection.</p><p>Graphical models, also referred to as Bayesian Networks when their graph is directed, are a powerful tool to encode and exploit the underlying statistical structure of complex data sets <ref type="bibr">[5]</ref>. The Generalized Linear Statistics (GLS) framework represents a subclass of graphical model techniques and includes as special cases multivariate probabilistic systems such as Principal Component Analysis (PCA), Generalized Linear Models (GLMs) and factor analysis <ref type="bibr" target="#b7">[7]</ref>. It This research was partially supported by NSF grant No. CCF-0830612.</p><p>is equivalent to a computationally tractable mixed exponential families data-type hierarchical Bayes graphical model with latent variables constrained to a low-dimensional parameter subspace. The use of exponential family distributions allows the data components to have different parametric forms and exploits the division between data space and parameter space specific to exponential families. In addition to giving a generative model that can be fit to the data, it offers the advantage that problems can be attacked in a latent variable parameter subspace that is a continuous, Euclidean space, even when data are categorical or of mixed types.</p><p>Although a variety of techniques exists for performing inference on graphical models, it is, in general, very difficult to learn the parameters which constitute the model, even if it is assumed that the graph structure is known. The main goal of this paper is to demonstrate our ability to learn a generative GLS graphical model that captures the statistical structure of the data, to then use this knowledge to gain insight into the problem domain, and perform effective classification. The text categorization and classification problems shown in the paper serve this purpose as examples illustrating the benefits of making decisions in parameter space rather than in data space as done with more classical approaches. Support Vector Machines as well make decisions in a non-data space. However, although often promising the highest accuracy, this technique will not generally provide any better understanding of the data. An advantage of learning a generative model of the data as done with GLS is that generating synthetic data for the purposes of developing and training classifiers with the same statistical structure as the original data becomes possible. This is particularly useful in cases where data are very difficult or expensive to obtain, and when the original data are proprietary and cannot be directly used for publication purposes in open literature.</p><p>In this paper, we first review the GLS framework and show how natural it is for non-gaussian data of mixed types. Then we demonstrate the utility of this approach with experiments on real data sets, where classification in parameter space outperforms classification in data space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">GENERALIZED LINEAR STATISTICS</head><p>The Generalized Linear Statistics framework is based on the hierarchical Bayes graphical model for hidden or latent variables shown in Figure <ref type="figure">1 [7]</ref>.</p><formula xml:id="formula_0">a 1 a 2 a q • • • θ 1 θ 2 θ 3 θ d • • • ¡ ¡ ¡ e e e s z ¡ ¡ ¡ e e e q e e e $ $ $ $ $ $ $ $ $ $ $ W A % x 1 x 2 x 3 x d • • • c c c c</formula><p>Fig. <ref type="figure">1</ref>. Graphical model for the GLS framework.</p><p>The row vector x = [x 1 , . . . , x d ] ∈ R d consists of observed features of mixed data instances in a d-dimensional space. It is assumed that instances can be drawn from populations having class-conditional probability density functions</p><formula xml:id="formula_1">p(x|θ) = p 1 (x 1 |θ 1 ) • . . . • p d (x d |θ d ),<label>(1)</label></formula><p>where, when conditioned on the random parameter vector θ = [θ 1 , . . . , θ d ] ∈ R d , the components of x are independent. The subscript i on p i (•|•) serves to indicate that the marginal densities can all be different, allowing for the possibility of x containing categorical, discrete, and continuous valued components. Also, the marginal densities are each assumed to be one-parameter exponential family densities, and θ i is taken to be the natural parameter (or some simple bijective function of it) of the exponential family density</p><formula xml:id="formula_2">p i . Each component density p i (x i |θ i ) in (1) for x i ∈ X i , i = 1, . . . , d, is of the form p(x i |θ i ) = exp θ i x i -G(θ i ) ,</formula><p>where G(•) is the cumulant generating function defined as</p><formula xml:id="formula_3">G(θ i ) = log X i exp θ i x i ν(dx i ),</formula><p>with ν(•) a σ-finite measure that generates the exponential family. It can be shown that</p><formula xml:id="formula_4">G(θ) = d i=1 G(θ i ) [7]</formula><p>. It is further assumed that θ can be written as</p><formula xml:id="formula_5">θ = aV + b</formula><p>(2) with the hidden or latent variable a = [a 1 , . . . , a q ] ∈ R q random and unknown with q &lt; d (and ideally q d), V ∈ R q×d and b ∈ R d deterministic and unknown. The latent variable a in some way explains part (or all) of the random behavior of the observed variables.</p><p>The maximum likelihood identification of the blind random effect model</p><formula xml:id="formula_6">p(x) = p(x|θ)π(θ)dθ = d i=1 p i (x i |θ i )π(θ)dθ,<label>(3)</label></formula><p>with π(θ) the probability density function of θ, is quite a difficult problem. It corresponds to identifying π(θ), which, under the condition θ = aV + b, corresponds to identifying the matrix V, the vector b, and a density function on the random effect a via a maximization of the likelihood function p(X) with respect to V, b, and the random effect density function, where</p><formula xml:id="formula_7">p(X) = n k=1 p x[k] = n k=1 p x[k]|θ π(θ)dθ, (4)</formula><p>and X is the (n × d) observation matrix</p><formula xml:id="formula_8">X =     </formula><p>x <ref type="bibr" target="#b1">[1]</ref> x <ref type="bibr" target="#b2">[2]</ref> . . .</p><formula xml:id="formula_9">x[n]      =      x 1 [1] . . . x d [1] x 1 [2] . . . x d [2] . . . . . . . . . x 1 [n] . . . x d [n]      .</formula><p>This difficulty can be avoided by Non-Parametric Maximum Likelihood (NPML) estimation of the random effect distribution, concurrently with the structural model parameters. The NPML estimate is known to be a discrete distribution on a finite number of support points <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8]</ref>. As shown in <ref type="bibr" target="#b7">[7]</ref>, the NPML approach yields unknown point-mass support points ā[l], point-mass probability estimates π l , and the linear predictor</p><formula xml:id="formula_10">θ[l] = ā[l]V + b for l = 1, . . . , m, m ≤ n. The single-sample likelihood (3) then becomes p(x) = m l=1 p x| θ[l] π l = m l=1 p x| ā[l]V + b π l</formula><p>and the data likelihood ( <ref type="formula">4</ref>) is equal to</p><formula xml:id="formula_11">p(X)= n k=1 m l=1 p x[k]| θ[l] π l = n k=1 m l=1 p x[k]| ā[l]V+b π l .</formula><p>The data likelihood is thus approximately the likelihood of a finite mixture of exponential family densities with unknown mixture proportions or point-mass probability estimates π l and unknown point-mass support points ā[l], with the linear predictor θ[l] in the lth mixture component. The combined problem of maximum likelihood estimation of the parameters V, b, the point-mass support points ā[l] and the point-mass probability estimates π l , l = 1, . . . , m, can be attacked by either using the Expectation-Maximization algorithm <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b1">1]</ref>, as done in particular in the Semi-Parametric Principal Component Analysis technique <ref type="bibr" target="#b11">[11]</ref>, or by simply considering the special case of uniform pointmass probabilities, i.e., π l = 1/m ∀l, for which the number of support points equals the number of data samples. It was demonstrated in <ref type="bibr" target="#b7">[7]</ref> that this special uniform case corresponds to the exponential Principal Component Analysis technique <ref type="bibr" target="#b2">[2]</ref>. We are using this special case in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CLASSIFYING IN PARAMETER SPACE: REAL DATA EXPERIMENTAL RESULTS</head><p>The data sets used in this work are from the UC Irvine machine learning repository <ref type="bibr">[14]</ref>. For each data set we do as follows. For text categorization examples, data preprocessing is needed, including a dictionary learning step. Then, for each data set, a low-dimensional latent variable subspace is identified in parameter space using GLS. In data space, classical Principal Component Analysis selects a lower dimensional subspace. Finally, classification is performed on both subspaces and performances are compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Text Categorization</head><p>The Twenty Newsgroups and the Reuters-21578 data sets account for most of the experimental work in text categorization, one example of information retrieval tasks. Text categorization is the activity of labeling natural language texts with thematic categories from a predefined set <ref type="bibr" target="#b13">[13]</ref>.</p><p>It has been acknowledged by the text categorization community that words seem to work well as features of a document for many classification tasks. In addition, it is usually assumed that the ordering of the words in a document does not matter. Hence, a document can be represented as a vector for which each distinct word is a feature <ref type="bibr" target="#b9">[9]</ref>. There are two ways to characterize the value of each feature that are commonly used in the literature: Boolean and tf ×idf weighting schemes. In Boolean weighting, the weight of a word is 1 if the word appears in the document and 0 otherwise. We choose to characterize the value of each feature by using the tf ×idf scheme as recently more commonly used for document representation <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13]</ref>. The tf ×idf weight is a statistical measure used to evaluate how important a word (or term) is to a corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. The term frequency tf is the number of times a specific word occurs in a specific document. The document frequency df is the number of documents in which the specific word occurs at least once. The inverse document frequency idf is calculated from the document frequency, yielding the tf ×idf weight w i for feature i:</p><formula xml:id="formula_12">w i = tf i • idf i = tf i • log ((total # of documents)/df i ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Twenty Newsgroups data set</head><p>The Twenty Newsgroups data set consists of Usenet articles collected from twenty different newsgroups. Each newsgroup contains 1000 articles. We consider the three following newsgroups: sci.med, comp.sys.mac.hardware and comp.sys.ibm.pc.hardware. We decide on a text categorization problem with two distinct classes, the first class con-sisting of the newsgroup sci.med and the second class consisting of the two other newsgroups.</p><p>Following the text document representation preprocessing steps described in Figure <ref type="figure">2</ref>, we first choose to discard all header fields such as Cc, Bcc, Message-ID, as well as the Subject field (this step is called parsing). Case-folding is performed by converting all the characters into lower-case. We use a stop list, i.e., a list of words that will not be taken into account. Indeed, there are words such as pronouns, prepositions and conjunctions which are encountered very frequently but carry no useful information about the content of the document. We used a stop list commonly used in the literature, <ref type="url" target="ftp://ftp.cs.cornell.edu/pub/smart/english.stop">ftp://ftp.cs.cornell.edu/pub/smart/english.stop</ref>. It consists of 571 stop-words and yields a drastic reduction in the number of features. Then, some simple stemming is performed, such as removing the third person and plural "s". In addition to removing very frequent words with the stop list, we remove words appearing less than 10 times in the corpus. The tf ×idf weighting scheme is then used and we choose to bin the weights and work with integer valued weights (5 bins are selected), i.e., categorical features.</p><p>Modified dictionary learning: Last, we construct a dictionary, and hence reduce the dimensionality of the feature space. There are various methods commonly applied for dimensionality reduction in document categorization <ref type="bibr" target="#b9">[9]</ref>. We choose a conditional mutual information based approach to select a dictionary of d = 150 words. We modify the binary feature selection with conditional mutual information algorithm proposed in <ref type="bibr" target="#b4">[4]</ref> to fit a categorical feature. The feature selection algorithm proposed in <ref type="bibr" target="#b4">[4]</ref> is based on the conditional mutual information maximization criterion and selects features that maximize both the information about the class and the independence between features. The modification from binary to categorical is simple: following the definition of entropy and mutual information shown in <ref type="bibr" target="#b4">[4]</ref>, the summations are changed from summing over two values to summing over the total number of bins values.</p><p>We use this data set leaving out a randomly selected 40% of the instances of each class to use as a test set. The training set then consists of 1764 instances and the test set 1236. The dictionary is learned using the training set only.</p><p>Classification effectiveness is often measured in terms of precision and recall in the text categorization community <ref type="bibr" target="#b13">[13]</ref>. Precision with respect to a class C i (π i ) is defined as the probability that, if a random document is classified under C i , this decision is correct. Recall with respect to a class C i (ρ i ) is defined as the probability that, if a random document ought to be classified under C i , this decision is taken. These probabilities are estimated in terms of the contingency table for C i on a given test set as follows: with respect to C i (documents correctly deemed to belong to class C i ), false positives with respect to C i (documents incorrectly deemed to belong to class C i ), and false negatives with respect to C i (documents incorrectly deemed not to belong to class C i ). Then, the F 1 measure combines precision and recall, attributing equal importance to π and ρ:</p><formula xml:id="formula_13">π i = T P i T P i + F P i and ρ i = T P i T P i + F N i ,</formula><formula xml:id="formula_14">F 1 = 2 • πρ π + ρ .</formula><p>When effectiveness is computed for several classes, the results for individual classes can be averaged in two ways: microaveraging, where π and ρ are obtained by summing over all individual classes (the subscript "µ" indicates microaveraging), and macroaveraging, where π and ρ are first evaluated "locally" for each class and then "globally" by averaging over the results of the different classes (the subscript "M " indicates macroaveraging) <ref type="bibr" target="#b13">[13]</ref>.</p><p>Supervised text categorization: Table <ref type="table" target="#tab_2">1</ref> compares classification performances on (a) the q-dimensional latent variable subspace learned with GLS using a Binomial distribution assumption and (b) the q-dimensional classical PCA subspace learned in data space in terms of precision, recall and F 1 measure, for several values of q. The classifier is a simple linear discriminant. The classification performances are often very similar, at times at the advantage of GLS (q = 4 and 10), at other times at the advantage of classical PCA.</p><p>Unsupervised text categorization: The K-means algorithm is used to cluster the training documents into two distinct classes. Based on this clustering information, a linear discriminant is learned on the training documents and used to classify the test documents. Figure <ref type="figure" target="#fig_0">3</ref> presents the corresponding ROC curve for this unsupervised approach performed on both the GLS parameter subspace and the classical PCA data subspace (q = 2). The performance is best when the unsupervised approach is used on the GLS subspace rather than on the classical PCA subspace. In this example, even though it is of interest, we did not further investigate the impact of the value for q on the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Reuters-21578 data set</head><p>The Reuters-21578 text categorization test collection Distribution 1.0 is considered as the standard benchmark for automatic document organization systems and consists of documents that appeared on the Reuters newswire in 1987. This corpus contains 21578 documents assigned to 135 different economic subject categories called topics. The topics are not disjoint. For the training test division of the data, the "Modified Apte" (ModApte) split is used. We reduce the size of the training test sets by only considering the ten topics that have the highest number of training documents as commonly done in the literature <ref type="bibr" target="#b13">[13]</ref>. These topics are given in Table <ref type="table" target="#tab_1">2</ref> and yield a training set of 6490 documents and a test set of 2545 documents. They cover almost all of the data, hence, researchers are able to restrict their work to them and still capture the essence of the data set.</p><p>The data are preprocessed as done for the previous data set: parsing, case-folding, elimination of stopwords, stemming by using Porter's stemming algorithm commonly used for word stemming in English <ref type="bibr" target="#b10">[10]</ref>, elimination of words that appear less than 20 times in the corpus, tf×idf weighting. Then, we choose to bin the weights and work with integer valued weights (5 bins are selected), i.e., categorical features. A dictionary of d = 50 words is learned using the following approach. The dictionary is learned on the training set only and built independently for each of the ten classes. Feature selection was incremental. First we do a backward selection to 300 features with linear regression. From these 300 features, we use a logistic regression with a number of iterations reduced down to 5 for convergence, and do a backward selection down to 100 features. Finally, we do a standard full-convergence logistic regression from those 100 features down to 50 features. Table <ref type="table" target="#tab_3">3</ref> compares classification performances micro-and macroaveraged over the top ten categories of the Reuters-21578 data set using a linear discriminant classifier on (a) the latent q-dimensional variable subspace learned with GLS using a Binomial distribution assumption and (b) the classical PCA q-dimensional subspace learned in data space. Microaveraging and macroaveraging methods give quite different results: the linear discriminant classifier performs better based on the GLS information than on classical PCA information when the macroaveraging method is used, while microaveraging emphasizes how similar the two results are. It is known that the ability of a classifier to behave well on categories with few positive training instances will be highlighted by macroaveraging compared to microaveraging <ref type="bibr" target="#b13">[13]</ref>. The linear discriminant classifier based on GLS information performs very well for the categories with fewer positive training instances yielding a better macroaveraged performance than the microaveraged one, cf. Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Abalone data set</head><p>The task is to predict the age of an abalone based on physical measurements. The Abalone data set consists of 4177 instances with 8 attributes. The problem can be seen as a classification problem aiming to distinguish three classes (number of rings = 1 to 8, number of rings = 9 to 10, number of rings = 11 and higher). We use this data set leaving out a randomly selected 40% of the instances to use as a test set (2506 training points and 1671 test points). Attribute 1 (sex, defined as infant, male or female) is the only noncontinuous attribute. We choose to model this attribute with a Binomial distribution, hence choosing a Gaussian-Binomial mixeddata assumption. Table <ref type="table" target="#tab_4">4</ref> compares micro-and macroaveraged classification performances using a linear discriminant classifier on (a) the latent q-dimensional variable subspace learned with GLS using a mixed Gaussian-Binomial distri-bution assumption and (b) the classical PCA q-dimensional subspace learned in data space. Performances are best when classification is performed on the GLS parameter subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>As with Bayesian Networks in general, the strength of the Generalized Linear Statistics framework is that it offers important insight into the underlying statistical structure of complex data, both creating a generative model of the data and making effective classification decisions possible. The benefits of making decisions in parameter space rather than in data space as done with more classical approaches have been illustrated with examples of Binomial data supervised and unsupervised text categorization and Gaussian-Binomial mixed-data supervised classification. However, one noticeable weakness of the framework is its running time. In addition, for the text categorization situation, the conditional mutual information maximization based feature selection algorithm was modified to fit categorical data. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Twenty Newsgroups data set: ROC curve for the proposed unsupervised text categorization technique performed on the low-dimensional subspace learned by (a) GLS (solid line) and (b) classical PCA (dashed line) (q = 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The ten topics with the highest number of training documents in the Reuters-21578 data set with the number of their documents in the training and test sets.</figDesc><table><row><cell>topics</cell><cell cols="2">training set test set</cell></row><row><cell>earn</cell><cell>2877</cell><cell>1087</cell></row><row><cell>acq</cell><cell>1650</cell><cell>719</cell></row><row><cell>money-fx</cell><cell>538</cell><cell>179</cell></row><row><cell>grain</cell><cell>433</cell><cell>149</cell></row><row><cell>crude</cell><cell>389</cell><cell>189</cell></row><row><cell>trade</cell><cell>369</cell><cell>118</cell></row><row><cell>interest</cell><cell>347</cell><cell>131</cell></row><row><cell>wheat</cell><cell>212</cell><cell>71</cell></row><row><cell>ship</cell><cell>197</cell><cell>89</cell></row><row><cell>corn</cell><cell>181</cell><cell>56</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Twenty Newsgroups data set: linear discriminant classification performances on the q-dimensional latent variable space learned with classical PCA and GLS with a Binomial distribution (1236 test instances).PCA -Precision PCA -Recall PCA -F 1 GLS -Precision GLS -Recall GLS -F 1</figDesc><table><row><cell>q = 1</cell><cell>0.5045</cell><cell>0.8149</cell><cell>0.6232</cell><cell>0.3677</cell><cell>0.6603</cell><cell>0.4744</cell></row><row><cell>q = 2</cell><cell>0.7843</cell><cell>0.9351</cell><cell>0.8531</cell><cell>0.7844</cell><cell>0.8918</cell><cell>0.8346</cell></row><row><cell>q = 3</cell><cell>0.9388</cell><cell>0.8846</cell><cell>0.9109</cell><cell>0.8641</cell><cell>0.8558</cell><cell>0.8599</cell></row><row><cell>q = 4</cell><cell>0.9389</cell><cell>0.8870</cell><cell>0.9122</cell><cell>0.8830</cell><cell>0.9615</cell><cell>0.9206</cell></row><row><cell>q = 5</cell><cell>0.9038</cell><cell>0.9712</cell><cell>0.9363</cell><cell>0.8931</cell><cell>0.9639</cell><cell>0.9272</cell></row><row><cell>q = 6</cell><cell>0.9038</cell><cell>0.9712</cell><cell>0.9363</cell><cell>0.8914</cell><cell>0.9663</cell><cell>0.9273</cell></row><row><cell>q = 8</cell><cell>0.9040</cell><cell>0.9736</cell><cell>0.9375</cell><cell>0.8813</cell><cell>0.9639</cell><cell>0.9208</cell></row><row><cell>q = 10</cell><cell>0.8904</cell><cell>0.9760</cell><cell>0.9312</cell><cell>0.9691</cell><cell>0.9038</cell><cell>0.9353</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Reuters-21578 data set: linear discriminant classification performances (microaveraged in (a) and macroaveraged in (b)) on the q-dimensional latent variable space learned with classical PCA and GLS with a Binomial distribution. (a) Microaveraged performances PCA -Precision µ PCA -Recall µ PCA -F µ 1 GLS -Precision µ GLS -Recall µ GLS -F µ</figDesc><table><row><cell>1</cell></row></table><note><p>M PCA -Recall M PCA -F M 1 GLS -Precision M GLS -Recall M GLS -F M</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Abalone data set: linear discriminant classification performances (microaveraged in (a) and macroaveraged in (b)) on the q-dimensional latent variable space learned with classical PCA and GLS with a Gaussian-Binomial mixed distribution.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">(a) Microaveraged performances</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">PCA -Precision µ PCA -Recall µ PCA -F µ 1</cell><cell cols="3">GLS -Precision µ GLS -Recall µ GLS -F µ 1</cell></row><row><cell>q = 1</cell><cell>0.5036</cell><cell>0.7120</cell><cell>0.5899</cell><cell>0.5043</cell><cell>0.7409</cell><cell>0.6001</cell></row><row><cell>q = 2</cell><cell>0.5085</cell><cell>0.7337</cell><cell>0.6007</cell><cell>0.5178</cell><cell>0.7385</cell><cell>0.6088</cell></row><row><cell></cell><cell></cell><cell cols="3">(b) Macroaveraged performances</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">PCA -Precision 1</cell></row><row><cell>q = 1</cell><cell>0.5204</cell><cell>0.7126</cell><cell>0.5952</cell><cell>0.5208</cell><cell>0.7415</cell><cell>0.6058</cell></row><row><cell>q = 2</cell><cell>0.5242</cell><cell>0.7335</cell><cell>0.6062</cell><cell>0.5337</cell><cell>0.7380</cell><cell>0.6141</cell></row></table><note><p>M PCA -Recall M PCA -F M 1 GLS -Precision M GLS -Recall M GLS -F M</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Boehning</surname></persName>
		</author>
		<title level="m">Computer-Assited Analysis of Mixtures and Applications: Meta-Analysis, Disease Mapping, and Others</title>
		<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A generalization of principal component analysis to the exponential family</title>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Statistical Soc., Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast binary feature selection with conditional mutual information</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1531" to="1555" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<title level="m">Graphical Models: Foundations of Neural Computation</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nonparametric Maximum Likelihood Estimation of a Mixing Distribution</title>
		<author>
			<persName><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. American Statistical Assoc</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">364</biblScope>
			<biblScope unit="page" from="805" to="811" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unifying viewpoint of some clustering techniques using Bregman divergences and extensions to mixed data sets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Levasseur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Burdge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreutz-Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">F</forename><surname>Mayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. 1st IEEE Int&apos;l Workshop on Data Mining and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="56" to="63" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The geometry of mixture likelihoods: a general theory</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Lindsay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="94" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Supervised and unsupervised machine learning techniques for text document categorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Özgür</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Istanbul, Turkey</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Eng., Bogazici University</orgName>
		</respStmt>
	</monogr>
	<note>master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An algorithm for suffix stripping</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-parametric exponential family PCA</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sajama</surname></persName>
		</author>
		<author>
			<persName><surname>Orlitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<title level="m">Introduction to Modern Information Retrieval</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
