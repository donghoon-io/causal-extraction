<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IMAGE DECOMPOSITION AND CLASSIFICATION THROUGH A GENERATIVE MODEL</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-02-09">9 Feb 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Houpu</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical and Aerospace Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Malcolm</forename><surname>Regan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">North Carolina State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical and Aerospace Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IMAGE DECOMPOSITION AND CLASSIFICATION THROUGH A GENERATIVE MODEL</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-02-09">9 Feb 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1902.03361v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generative model</term>
					<term>classification</term>
					<term>adversarial defense</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We demonstrate in this paper that a generative model can be designed to perform classification tasks under challenging settings, including adversarial attacks and input distribution shifts. Specifically, we propose a conditional variational autoencoder that learns both the decomposition of inputs and the distributions of the resulting components. During test, we jointly optimize the latent variables of the generator and the relaxed component labels to find the best match between the given input and the output of the generator. The model demonstrates promising performance at recognizing overlapping components from the multiMNIST dataset, and novel component combinations from a traffic sign dataset. Experiments also show that the proposed model achieves high robustness on MNIST and NORB datasets, in particular for high-strength gradient attacks and non-gradient attacks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Neural network architectures have been developed to achieve human-level performance on standard vision tasks <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref>. However, it is acknowledged that feedforward networks have difficulty at generalization under input distribution shifts, e.g., novel object sets <ref type="bibr" target="#b3">[3]</ref> and objects with overlaps <ref type="bibr" target="#b4">[4]</ref>. Furthermore, studies have shown that networks, even with high standard test accuracy, can suffer from imperceptible adversarial attacks <ref type="bibr" target="#b5">[5]</ref>. While neither distribution shifting or adversarial attacks are common cases in standard test environments for image classifiers <ref type="bibr">[6,</ref><ref type="bibr" target="#b7">7]</ref>, the demonstrated risk of existing models have raised concerns over their real-world applications (e.g., autonomous driving and security surveillance) where failed classification for a short amount of time can be catastrophic. These concerns have led to the question of whether semantic attributes or physical components of the inputs are truly understood by feedforward, albeit deep, networks <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref>. Indeed, evidence showed that state-of-the-art classification models have a drastically different accuracy changing pattern than human beings in classifying image sequences with diminishing details <ref type="bibr" target="#b8">[8]</ref>, suggesting that the two This concern over model generalizability and robustness is intrinsic to classifiers that perform bottom-up signal processing. Alternative models that integrate bottom-up processing with top-down reasoning through recursive inference have been studied <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10]</ref>. Of particular interest are recursive compositional models (e.g., AND-OR templates <ref type="bibr" target="#b10">[10]</ref>) that learn to match deformable objects and infer graph states for detection and recognition. These models take advantage of highly structured generators, e.g., by explicitly modeling object edges and surfaces. However, the intrinsic tradeoff between model and computational complexity (for both model learning and recognition/classification) may hamper their application to general inputs for which the recognition or classification tasks depend on a richer set of features. With the rapid advance in generative models (e.g. GAN <ref type="bibr" target="#b11">[11]</ref>, Pix-elCNN <ref type="bibr" target="#b12">[12]</ref>, and VAE <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref>), it is tempting to investigate top-down classification mechanisms that incorporate more flexible generators than compositional models.</p><p>To this end, we present in this paper a classification algorithm where input images are classified by minimizing the difference between the target image and the output of a generator. We choose Conditional Variational Autoencoder (c-VAE) as the backbone of our model due to its simplicity and good convergence property. Following the argument that attribute recognition is key to object classification <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>, we focus on the classification of attributes, or more specifically, the existence of object components from the input image, while assuming that the follow-up mapping from these components to the object class is established. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, the model is built on a variational autoencoder whose decoder is composed of separate sub-networks, where each component is associated with a sub-network. Experiments show that the proposed model demonstrates promising performance at recognizing overlapping components from the multiMNIST dataset, and novel component combinations from a traffic sign dataset. The model also achieves high robustness on MNIST and NORB datasets, in particular for high-strength gradient attacks and non-gradient attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Conditional generative models have been heavily investigated in the literature. For example, VAE structure <ref type="bibr" target="#b13">[13]</ref> is extended into c-VAE <ref type="bibr" target="#b14">[14]</ref> by including attribute variables as extra inputs. Besides, the latent variables are disentangled into two sub-networks in <ref type="bibr" target="#b14">[14]</ref> to perform foreground and background separation. In our work, we further extended the c-VAE idea to have more sub-networks where each sub-network corresponds to a component. And instead of generation or segmentation, we investigate the possibility to use the conditional generative model to perform robust classification.</p><p>The utility of generative model as a defense mechanism has been explored very recently. For example, defense GAN <ref type="bibr" target="#b17">[17]</ref> finds the closest generation from a GAN to a given input image, and feeds the generated image to a vanilla classifier for prediction. Similar to our approach, Schott et al. <ref type="bibr" target="#b18">[18]</ref> chose VAE to perform image generation and used the reconstruction error as a metric to perform classification directly, without feedforward classifier involved. Compared with <ref type="bibr" target="#b18">[18]</ref>, the proposed model has an optimized network structure and classification workflow, which makes it able to learn object decomposition and handle more tasks like novel or overlapping object recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Customized conditional variational autoencoder</head><p>We customize a conditional variational autoencoder to learn the decomposition of object components. Let x r and y be the input images and their corresponding components, respectively. A binary component vector y encodes the existence of components within x r . It is assumed that the maximum amount of possible components in the dataset is n. Let z = Enc(x r , θ) ∈ R p be the latent variable generated from the encoder Enc(•, θ) with parameters θ, and x f (z, y, φ) = n i y i Dec i (z i , φ i ) be the generated images from the set of decoders Dec i (•, φ i ) with parameters φ i .</p><formula xml:id="formula_0">z i ∈ R p/n</formula><p>is a segment of z corresponding to the ith component. Following the formulation of variational autoencoders, we define our training loss as:</p><formula xml:id="formula_1">E {x r ,y}∼D ||x f (z, y, φ) -x r || 2 2 + C n i y T i KL(z i ) (1)</formula><p>where KL(z i ) is the sub-network-wise KL divergence of the distribution of z i from a standard normal distribution. In the presented experiments, Adam optimizer with a learning rate of 0.001 is used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Classification</head><p>Based on the learned generative model, the classification process can be cast into an inverse problem, where we jointly find z * and y * that minimize the difference between the generated image x f and the target image x r in the image space:</p><formula xml:id="formula_2">z * , y * = argmin z∈R p ,y∈{0,1} n f (z, y, x r ), f (z, y, x r ) ≡ x f (z, y, φ) -x r 2 2 .</formula><p>(2)</p><p>Note that Eq. ( <ref type="formula">2</ref>) is a combinatorial problem, since y is a binary vector. To make this problem feasible, we relax component labels to be continuous within [0, 1], so that the optimization problem becomes differentiable and can be solved efficiently through gradient descent and back-propagation. In addition, a L 1 penalty is introduced to regularize non-zero components with weight c. The regularization is needed so that a component will be correctly discarded (i.e., sub-network outputs suppressed) if its addition to the generated image does not improve the reconstruction quality significantly. Lastly, it is also found that minimizing the image-wise difference after a sigmoid transformation over the reconstruction further improves the defense performance. With these modifications, the classifier solves the following problem:</p><formula xml:id="formula_3">z * , y * = argmin z∈R p ,y∈[0,1] n F (z, y, x r ) F (z, y, x r ) = Sig(x f (z, y, φ)) -Sig(x r ) 2 2 + c y 1 ,<label>(3)</label></formula><p>where Sig(x) = sigmoid β • x + b , and β, b are hyperparameters.</p><p>Upon convergence, z * and y * obtained via Eq. ( <ref type="formula" target="#formula_3">3</ref>) will be processed to derive the final classification result using the following logic flow: First, if the lowest reconstruction loss found is larger than a pre-set threshold l, the input image will be classified as noise. This step is able to filter out adversarial attacks using non-gradient methods such as Neuroevolution of Augmenting Topologies (NEAT) <ref type="bibr" target="#b19">[19]</ref>. After this initial filtering, we apply two thresholds y l and y u (y l &lt; y u ) to y * : an element y * i is set to 0 if y * i &lt; y l , and 1 if y * i &gt; y u . For the remaining elements of y * between y l and y u , we will enumerate over all binary combinations of this subset to generate a candidate set Y for fine-tuning the prediction. Specifically, we compute z † (y) = argmin z∈R p f (z, y, x r ) for y ∈ Y and set the classification result as y † = argmin y∈Y f (z † (y), y, x r ). All hyper-parameters (β,b,l,y l , and y u ) are determined based on a validation set, and are therefore dataset dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation details</head><p>The proposed model will be tested on MNIST, NORB <ref type="bibr" target="#b20">[20]</ref>, multiMNIST <ref type="bibr" target="#b4">[4]</ref>, and a Traffic Sign (TS) dataset. For NORB, similar to <ref type="bibr" target="#b4">[4]</ref>, we downscale the image resolution to 48-by-48. The multiMNIST dataset is synthesized by stacking two MNIST images into an image of resolution 36-by-36, which will result in 80% overlap between two digits on average. Every image label in multiMNIST has 2 ones and 8 zeros. A visualization of the multiMNIST dataset can be found in Fig. <ref type="figure" target="#fig_3">4</ref>. The TS dataset is prepared with affine transformation to mimic traffic signs at different view angles. There are four different types of images in the dataset: "left turn", "right turn", "no left turn", and "no right turn". These images are in a resolution of 64-by-64, with examples visualized in Fig. <ref type="figure" target="#fig_2">3</ref>. There are four components for TS images to represent the components: circle, slash, left arrow, and right arrow. For example, a "No right turn" image has label [1, 1, 0, 1] since it has circle, slash, and right arrow.</p><p>The number of sub-networks is equal to the number of total components in the dataset, which is 10 for MNIST and multiMNIST, 5 for NORB, and 4 for TS. Once the network has been trained, it is expected to decompose the input object into different components, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. To demonstrate the efficacy of the learning of decomposition, we visualize the latent space of each sub-networks trained on different datasets in Fig. <ref type="figure" target="#fig_1">2</ref>. One observation from the results is that the model is able to remove data redundancy automatically, which can be seen from the TS dataset: To recall, the first component of a TS image represents the existence of a circle. Since the circle exists in all TS images, the learned generative model combines the circle into the generation of the arrows and leaves the first sub-network blank. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS AND DISCUSSION</head><p>In this section, we demonstrate the efficacy of our method through a set of challenging experiments. These include classifying overlapping and novel objects, and defense against both gradient-based (FGM <ref type="bibr" target="#b5">[5]</ref>) and non-gradient based (NEAT <ref type="bibr" target="#b19">[19]</ref>) adversarial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Classification of novel and overlapping objects</head><p>We first train our model on the TS dataset by only using "left turn", "right turn" and "no right turn" images for training. After the model is trained, its performance is evaluated on "No left turn" images. This experiment is set up in a way that the network is asked to recognize novel objects not included in the training set by making use of the learned decomposition of image elements.</p><p>A traditional feedforward CNN classifier will completely fail under this setting, where 99% of the "no left turn" images are classified as "left turn". This is due to the fact that the classifier will associate the "left arrow" feature with the "left turn" category during the training phase, and this correlation strongly affects the prediction during the testing phase. In contrast, our proposed model learns to decompose the TS dataset into components after training (as shown in Fig. <ref type="figure" target="#fig_1">2b</ref>), and by combining these components together, it correctly recognizes 95% of all novel testing images. The classification result is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. We now demonstrate that the proposed model is able to handle objects with large overlap using the multiMNIST dataset. After training, the proposed model learns the decomposition of individual digits successfully as shown in Fig. <ref type="figure" target="#fig_1">2a</ref>. Applying the learned model to classifying the test dataset leads to a classification accuracy of 65.6%. Successful and failed test samples and their classification results are shown in Fig. <ref type="figure" target="#fig_3">4</ref>. While the accuracy on multiMNIST is lower than that of a CapsNet <ref type="bibr" target="#b4">[4]</ref>(95%), investigation of the model performance shows that many of the misclassified samples are truly difficult to be separated even for human beings. It should also be noted that the accuracy of CapsNet is resulted from 60 million training data, while our model successfully decomposes the learns the component-wise latent spaces with only 128k data points. We expect improved classification accuracy by increasing the capacity of the generative model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Defending adversarial attacks</head><p>We test the performance of our model under both gradientand non-gradient-based adversarial attacks in this experiment. FGM and NEAT are chosen as the attack methods. NEAT performs evolution on Compositional Pattern Producing Networks (CPPN) <ref type="bibr" target="#b21">[21]</ref>, each of which represents an image. The fitness of the evolution is defined as 1 -||ŷ -y target ||, where ŷ is the classification result of a CPPN image by the source classification model and y target is the target class for the attack. If an adversarial image is misclassified with over 90% confidence, the attack is considered successful and this adversarial image will be used for test.</p><p>We first compare our method with the binarization defense <ref type="bibr" target="#b22">[22]</ref> under FGM attack in Tab.1. The performance of binarization drops quickly with increasing attack magnitudes, while the proposed model is able to maintain high accuracy even under relatively high attack magnitude. To further examine the cases where our model fails, we visualized adversarial MNIST samples with = 0.4 in Fig. <ref type="figure" target="#fig_4">5a</ref>, along with correctly classified samples. We note that many of the misclassified images are hard to be recognized even for human beings. The comparison on NEAT attacks is shown in Fig. <ref type="figure" target="#fig_4">5b</ref>. The baseline classifier is fooled completely by the adversarial images with consistently high confidence for the targeted labels (shown in the first row of Fig. <ref type="figure" target="#fig_4">5b</ref>). Even after binarization, many NEAT images can still be misclassified with high confidence (shown in the second row of Fig. <ref type="figure" target="#fig_4">5b</ref>). In contrast, the proposed method is able to identify 90.6% of these attacks. This is because the generative model is not trained on these highly structured CPPN patterns, and is not able to reconstruct them during the test. Due to the high reconstruction error, the proposed model will conclude that these input images are out of the data distribution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion</head><p>While the presented experiments show promise of the proposed classification method, a deeper investigation is needed to better characterize the applicability and limitations of the proposed method to more complicated datasets (e.g., CI-FAR10 and ImageNet) under more sophisticated settings (e.g. PGD adversarial attack <ref type="bibr" target="#b23">[23]</ref>). Further improvements can be made to compress the generative model in order to accelerate the optimization during classification. Generative models with the capability to create fine-grained details should be studied and incorporated to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this paper, we investigate the utility of a tailored conditional variational autoencoder as a classifier, and test its generalizability and robustness under challenging tasks. Results show that the proposed training and classification formulations lead to promising performance: First, the model can recognize overlapping objects and novel component combinations that do not exist in the training phase. Second, our model is able to defend against adversarial attacks well, in particular under higher attack magnitudes and under none-gradient attacks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic of the proposed generative model with subnetworks for component-wise generation. Each sub-network is gated by a binary component label y i . Classification is done through optimizing z and y to match the generated image with the input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Sampling the latent space of each generator trained on (a) multiMNIST, (b) TS, and (c) NORB. Each sub-network is dedicated to one component. Samples of multiMNIST training images can be found in Fig.4.</figDesc><graphic coords="3,54.43,536.86,243.78,112.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Classification result on novel objects. Left: convergence of component labels during the classification process, with ±3 standard deviation over all "no left turn" test images. Right: the inputs (top), their sub-network generations (middle), and the optimal final generations (bottom).</figDesc><graphic coords="3,316.43,415.41,241.34,92.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples of classification results on multiMNIST. First row: input test images. Second row: predicted images. L and P denotes ground truth and predicted labels, respectively.</figDesc><graphic coords="4,55.64,143.34,241.34,72.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Sample adversarial images. Left (first and second row): FGM adversarial images and the generated images.L and P denotes ground truth and predicted labels, respectively. Right (from top to bottom row): CPPN adversarial images, images after binarization, and generation found by proposed method. Classification confidence of the feed-forward classifier are shown under the images.</figDesc><graphic coords="4,316.43,178.91,241.34,74.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparisons</figDesc><table><row><cell></cell><cell cols="5">between baseline (binarization) and</cell></row><row><cell cols="6">the proposed method on robustness under FGM attacks with</cell></row><row><cell cols="2">increasing perturbation levels.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell></row><row><cell cols="6">MNIST baseline 0.97 0.95 0.90 0.76 0.51</cell></row><row><cell cols="6">MNIST proposed 0.95 0.87 0.91 0.87 0.82</cell></row><row><cell>NORB</cell><cell cols="3">baseline 59.8 18.3 2.8</cell><cell>0.8</cell><cell>0.2</cell></row><row><cell cols="6">NORB proposed 87.1 61.9 40.1 24.6 18.8</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Invariant object recognition in the visual system with novel views of 3d objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edmund</forename><forename type="middle">T</forename><surname>Stringer</surname></persName>
		</author>
		<author>
			<persName><surname>Rolls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2585" to="2596" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Practical black-box attacks against deep learning systems using adversarial examples</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02697</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Standard detectors aren&apos;t (currently) fooled by physical adversarial stop signs</title>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hussein</forename><surname>Sibai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Fabry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03337</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Atoms of recognition in human and computer vision</title>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liav</forename><surname>Assif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Harari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2744" to="2749" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rapid inference on a novel and/or graph for object detection, segmentation and parsing</title>
		<author>
			<persName><forename type="first">Yuanhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A generative vision model that trains with high data efficiency and breaks text-based captchas</title>
		<author>
			<persName><forename type="first">W</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lehrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Laan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Marthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="page">2612</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05517</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="776" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName><forename type="first">Hannes</forename><surname>Christoph H Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Defense-gan: Protecting classifiers against adversarial attacks using generative models</title>
		<author>
			<persName><forename type="first">Pouya</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06605</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Towards the first adversarially robust neural network model on mnist</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">104</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evolving neural networks through augmenting topologies</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risto</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="127" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Feature squeezing: Detecting adversarial examples in deep neural networks</title>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01155</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
