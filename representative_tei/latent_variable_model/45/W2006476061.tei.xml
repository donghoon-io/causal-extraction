<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Integrated Statistical Model for Multimedia Evidence Combination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Sheng</forename><surname>Gao</surname></persName>
							<email>gaosheng@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joo-Hwee</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qibin</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<addrLine>21 Heng Mui Keng Terrace</addrLine>
									<postCode>119613</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Integrated Statistical Model for Multimedia Evidence Combination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Systems]: INFORMATION STORAGE AND RETRIEVAL Algorithms</term>
					<term>Management</term>
					<term>Theory Semantic concept detection</term>
					<term>average precision</term>
					<term>evidence fusion</term>
					<term>model-based fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given rich content-based features of multimedia (e.g., visual, text, or audio) followed by various detectors (e.g., SVM, Adaboost, HMM or GMM, etc), can we find an efficient approach to combine these evidences? In the paper, we address this issue by proposing an Integrated Statistical Model (ISM) to combine diverse evidences extracted from the domain knowledge of detectors, the intrinsic structure of modality distribution and interconcept association. The ISM provides a unified framework for evidence fusion, owning the following unique advantages: 1) the intrinsic modes in the modality distribution are discovered and modeled by the generative model; 2) each mode is a partial description of structure of the modality and the mode configuration, i.e. a set of modes, is a new representation of the document content; 3) the mode discrimination is automatically learned; 4) prior knowledge such as the detector correlation and inter-concept relation can be explicitly described and integrated. More importantly, an efficient pseudo-EM algorithm is realized for training the statistical model. The learning algorithm relaxes the computation cost due to the normalized factor and latent variables in graphical model. We evaluate the system performance on multimedia semantic concept detection with the TRECVID 2005 development dataset, in terms of efficiency and capacity. Our experimental results demonstrate that the ISM fusion outperforms the SVM based discriminative fusion method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Multimedia document contains rich (e.g. information carried in multiple channels such as visual, textual and audio) and diverse (e.g. visual appearance has a lot of variations for the same semantic concept) information. It is far from reaching the right features for multimedia indexing, especially for visual indexing <ref type="bibr" target="#b4">[5]</ref>. Even if the features are rightly chosen, we still face the problem of finding suitable machine learning tools for multimedia semantic concept detection and information access and retrieval. There are so many tools (e.g. parametric or non-parametric models, generative or discriminative models, etc.) available to address the problem in hand. It is very challenging to find the right ones. Thus, in practice, the choice is based on the experiments or experiences learned from other researchers. Rich and diverse information in multimedia document teaches us that no single solution would exist so far. Successful systems in TRECVID always extract various features from the visual (e.g. color, texture, edge, etc), textual (e.g. tf-idf, name entity, etc.) or audio (e.g. Mel Frequency Cepstral Coefficients (MFCC), pitch, Fast Fourier Transform (FFT), etc.) signals, and build various types of detectors (e.g. Support Vector Machine (SVM), AdaBoost, Hidden Markov Model (HMM), Gaussian Mixture Model (GMM), etc.) <ref type="foot" target="#foot_0">1</ref> . Then the outputs of these detectors are combined to obtain the final decision. For instance, 110 detectors are built based on the features extracted from the visual and textual modalities and are combined to improve semantic concept detection in <ref type="bibr" target="#b3">[4]</ref>.</p><p>Therefore, the evidence combination is a critical step in multimedia content classification. For simplicity, we discuss the evidence fusion in the context of semantic concept detection in the paper. The evidences may be extracted using the detectors which are trained on the different visual, textual or audio features using the suitable machine learning algorithms or they may be the prior knowledge on the feature discrimination power, the association strength among the semantic concepts, etc.</p><p>For example, if we need to detect N c semantic concepts and there are N d types of detectors trained for each concept, the task of the fusion model is to efficiently combine the N c *N d detector outputs to boost the performance of concept detection. Many different approaches, i.e. the non-parametric method or the parametric method, have been presented to address the issue.</p><p>The non-parametric method, e.g. CombSUM, CombMAX, does not need training samples to build a fusion model <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref>. It is an adhoc method for easy usage. On the contrary, the parametric method needs training samples to estimate the fusion model. It may treat the N c *N d outputs as a new representation of multimedia document. Then the supervised learning algorithms are exploited, e.g. graphical model <ref type="bibr" target="#b6">[7]</ref>, SVM <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13]</ref>, MC MFoM <ref type="bibr" target="#b15">[16]</ref>, etc. In TRECVID 1  <ref type="bibr" target="#b24">[25]</ref>, the SVM-based discriminative fusion model is the dominant approach. In practice, it may be preferred to cluster the detector outputs into a few groups according to some prior knowledge. In this case, the fusion will be completed using multiple stages. For example, if we have 3 detectors built on color histograms in RGB, HSV and LUV spaces respectively and 2 detectors built on texture features such as Gabor filter and gray-level co-occurrence, it may be better to have the former 3 outputs in one group and the others in another group. Then the fusion is first carried out in each group and the outputs of the 2 groups are further combined. The domain knowledge can guide the design of groups. Sometimes, the unsupervised learning approaches such as PCA and ICA can be employed to discover the groups <ref type="bibr" target="#b16">[17]</ref>. Following <ref type="bibr" target="#b16">[17]</ref>, we use the term modality to refer to each group.</p><p>Besides the evidences from the detector outputs and the domain knowledge of detectors, another source of evidence is the interconcept association, i.e. the performance of detecting one concept can be boosted by detecting other concepts. For example, detecting the concept outdoor will help detecting the concept animal because animal frequently plays in the outdoor. Animal is the boosted concept while outdoor is the boosting concept. Many works have been carried out to combine this contextual information <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>To use the inter-concept relation in the fusion stage, graphical model with various model structures (e.g. restricted Boltzmann machine, conditional random field, markov random field, etc.) is extensively employed. The power of the contextual evidence depends on many issues, e.g. the performance of boosting concepts, the association strength between the boosting concept and the boosted concept, etc. To select the strong boosting concepts, an active context-based concept fusion is proposed in <ref type="bibr" target="#b19">[20]</ref> and it is further incorporated into the boosted conditional random fields in <ref type="bibr" target="#b8">[9]</ref>. The experiments on TRECVID dataset report obvious performance improvement due to the combination of the inter-concept relation. These works empirically show that the evidence of the concept associations can enhance the concept detection. The issue is that the computation cost is high for the graphical model.</p><p>In addition, the existing approaches lack the capacity to unify the various types of evidences. For example, in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20]</ref>, the models are designed to fuse the inter-concept association where each concept has one detector output. It is an issue whether they would work well when each concept has multiple detectors. They also ignore the evidence of the correlation among detectors. Other works such as <ref type="bibr" target="#b16">[17]</ref> utilize the correlation. However, the interconcept relation is missed. All the approaches have no capacity to discover and incorporate the intrinsic statistical distribution of the modality, whose efficiency to improve the combination of multiple search engines is demonstrated in <ref type="bibr" target="#b11">[12]</ref> through modeling the score distribution of the search engine output. In <ref type="bibr" target="#b11">[12]</ref>, the model is an exponential distribution for the non-relevant documents and a normal distribution for the relevant documents.</p><p>In the paper, an Integrated Statistical Model (ISM) is presented to address the challenging research issue of combining multiple evidences extracted from the detector correlation, the modality distribution and inter-concept association. The ISM provides a unified framework to combine evidences with the following unique features: 1) the intrinsic modes in the modality distribution are discovered and modeled by the generative model; 2) each mode is a partial description of structure in the modality distribution while the mode configuration, i.e. a set of modes, can be used to represent the document; 3) the mode discrimination is automatically learned; 4) the prior knowledge such as the modality correlation and inter-concept relation is explicitly described and integrated. Further, we develop an efficient pseudo-EM algorithm for training the statistical model. It relaxes the computation cost due to the normalized factor and latent variables in the graphical model <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20]</ref>. We study and evaluate the proposed fusion model on the task of semantic concept detection using the development set in TRECVID 2005.</p><p>The paper is organized as follows. In the next section, the major components of ISM are discussed in detail. Then the pseudo-EM algorithm for estimating the ISM parameters is presented in Section 3. Experiments and analyses are given in Section 4. Finally, concluding remarks are presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OUTLINE OF ISM FRAMEWORK</head><p>In this section, we will first give a brief overview of the proposed ISM framework. The learning algorithm will be further discussed in Section 3. Figure <ref type="figure">1</ref> depicts the key components of the ISM model. X is the detected evidence, i.e. output scores of N c *N d concept detectors and E is the prior knowledge including the correlation in X and the inter-concept association. In the following, we introduce each component from the bottom to the top as shown in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1 key components of the Integrated Statistical Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Predict Latent Modes</head><p>Firstly we introduce a few terms which will be used throughout the paper in the context of evidence combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modality:</head><p>Assuming that a set of detectors are built for the concept detection. The output value of the detector is used to determine whether the concept is present or absent. Usually the output is random and its value is a real number. We refer to the modality as the random variable as well as the corresponding detector. Sometimes we concatenate the output values from some detectors into a vector for decision. In this scenario the modality refers to the random vector as well as the corresponding vectors. We use modality value for a real value of random variable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(c|X,E)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predict latent modes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mode:</head><p>The modality may contain rich structures, each of which may be described by some parametric statistical distributions. The mode refers to one partial structure as well as its corresponding parametric distribution. For example, the mode here is modeled by a single Gaussian distribution with the mean and covariance and the modality with the 2-mixture Gaussian components consists of 2 modes.</p><p>Mode configuration: It is a vector whose dimension equals to the number of modalities. Each element in the vector is the most representative mode identity for the observed modality value.</p><p>The modality distribution contains information that can improve the ranking performance. It is studied in <ref type="bibr" target="#b11">[12]</ref>, where the modality distribution is modeled by two component models, one is Gaussian component for relevant documents and the other is Poisson component for irrelevant documents. Then the documents are rescored using the learned models. Rather than explicitly modeling the modality distribution as in <ref type="bibr" target="#b11">[12]</ref>, we model the modes in the paper. All modes work together to render an approximate image of the corresponding modality. The mode models are unknown and mode configuration is hidden. To learn the mode models and mode configuration, the generative and discriminative approaches are employed. Not limited to the Gaussian distribution for mode models, other generative models can also be used. However, it is not studied in the paper.</p><p>When the mode models are available, the observed modality values are mapped to its corresponding mode configuration. The mode configuration is treated as a symbolic representation of the modality values. The further decision can be carried out on it. It is much different from the traditional fusion models, where only the original modality value is used while the deep structure of modality distribution is ignored. In the next section, we will use a toy example to demonstrate the power of the mode models to classification and ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Toy example</head><p>Figure <ref type="figure" target="#fig_1">2</ref> illustrates a toy problem for 2 categories, i.e. positive and negative classes. 6 samples are used: 4 negative samples and 2 positive samples. One detector is used to score the 6 samples. The corresponding output scores are shown in the figure: circle points for negative samples and plus points for positive samples. The positive scores are located in the middle of the negative scores. With any threshold, there is always classification error occurred. If the threshold is set to zero, the error rate is 0.33 with the 2 rightmost negative samples, i.e. 0.6 and 0.8, wrongly classified.</p><p>However, perfectly correct classification could be obtained if the mode models were known. In this example, one mode is enough for characterizing the modality distribution. The curve of the Gaussian mode model (mean: 0.23, standard derivation: 0.42) is plotted in the figure (blue curve). Measured by the Euclidean distance, the distances between the raw scores of samples and the mean of mode model are 0.001 and 0.034 for the 2 positive samples, respectively. Correspondingly, they are 0.40, 0.19, 0.13 and 0.32 for the 4 negative samples (from the left to the right), respectively. Now the 6 samples can be correctly classified if the threshold (such as 0.035) is used. Using the new scores, the 6 samples are correctly ranked.</p><p>This example clearly demonstrates the usefulness of the modality modes, despite that it is just a toy problem. With the statistics of the modes, the raw modality values will be transformed into a new space where good ranking would be observed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Predict modality modes</head><p>To predict the mode identity, a set of mode models are built. Each modality will have K modes to characterize its distribution. Like the toy example above, a single Gaussian distribution with the mean and variance is used for modeling the mode. The k-th mode is denoted as ( ) ( )</p><formula xml:id="formula_0">, k k k f x N x μ = ∑ .</formula><p>Here x is the modality value.</p><p>The predicted probability to assign the k-th mode to x is calculated as, ( )</p><formula xml:id="formula_1">( ) ( ) k f x P k x Z x η =<label>(1)</label></formula><p>where ( ) ( )</p><formula xml:id="formula_2">1 K i i Z x f x η = = ∑</formula><p>and η is a smoothing constant. Then the mode with the maximal probability, ( )</p><formula xml:id="formula_3">h x , is assigned to x as ( ) [ ] ( ) 1, arg max k K h x P k x ∈ = (2).</formula><p>However, the question is that the modes are unknown and they are hidden in the modality samples. There is no prior knowledge of the correct assignments between the modality value and the mode identity. Thus, the supervised learning approaches are infeasible. Fortunately, our aim is to use the mode as the intermediate representation rather than to discover the meaningful modality modes. Therefore, the unsupervised learning algorithms, e.g. the k-means clustering, are employed. In the ISM fusion model, the k-means clustering algorithm is used to initialize the mode models. Then the mode models are updated in the E-step in the iterative pseudo-EM algorithm developed for learning ISM model (detailed in Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Co-occurrence Mode Feature Extraction</head><p>When the mode models of all modalities are available, the mode configuration can be found according to Eq. ( <ref type="formula">2</ref>). Assuming that there are M modalities each having K modes, the modality values are</p><formula xml:id="formula_4">[ ] { } , 1, i X x i M = ∈ and the corresponding mode configuration is [ ] { } , 1, i H h i M = ∈</formula><p>, where i h is the mode identity of i x . This configuration gives a symbolic description of the document. Each mode will function as a word likewise in a text document. After mapping the modality value using the mode models, a document represented in the continuous feature space is tokenized using a set of modes. Thus, the document-level features such as tf-idf, unigram or bigram become available like in text categorization and text information retrieval <ref type="bibr" target="#b21">[22]</ref>. In this paper, the unigram feature, similar to that adopted in text categorization <ref type="bibr" target="#b20">[21]</ref>, is extracted. It is defined as,</p><formula xml:id="formula_5">( ) ( ) ( ) , , # , , ,<label>, 0,</label></formula><formula xml:id="formula_6">q c q c q I w i fc y f I y Z q c otherwise ⎧ ⋅ = ⎪ = ⎨ ⎪ ⎩<label>(3)</label></formula><p>Here q is one mode identity of M*N*K modality modes (N: the number of concepts). I is a document belonging to the concept y.</p><p>, q c w is a weight measuring the association degree between the mode q and the concept c (to be detailed in Section 2.4). It comes from the prior knowledge of the inter-concept association. <ref type="figure">,</ref><ref type="figure">q c</ref> f ⋅ is a feature extractor designed for the mode q and the concept c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( )</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( )</head><p>, Z q c is a normalization factor so that the sum of features is equal to 1, i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( )</head><p>, , , 1</p><formula xml:id="formula_7">q c q c f I y = ∑<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Predict Concept Probability</head><p>From the co-occurrence mode features (see Eq. ( <ref type="formula" target="#formula_6">3</ref>)), we can train the concept models to predict the probability assigned to a concept. The maximum entropy (ME) approach is applied to model the concepts in the paper <ref type="bibr" target="#b1">[2]</ref>. When the ME models have been trained, they are used to predict the probability assigned to the concept c according to the observed evidence X. It is calculated as, (</p><p>,</p><formula xml:id="formula_9">q c q c q c P c I f I c Z I θ λ θ = ⋅ ∑<label>(5) where ( ) ( ) ( ) , , , , e x p</label></formula><p>,</p><formula xml:id="formula_10">q c q c c q c Z I f I c θ λ = ⋅ ∑ ∑ is the normalization factor and { } , q c θ λ = is the parameter set of concept models. , q c</formula><p>λ is a weight coefficient of the feature extracted in Eq. (3). Eq. ( <ref type="formula" target="#formula_9">5</ref>) is concept dependent. Hereafter, we use the term concept model to refer to it. In the context of classification, the document is assigned to the concept * c which has the maximal probability according to Eq. ( <ref type="formula" target="#formula_9">5</ref>). </p><p>The model parameters θ can be trained through maximizing the likelihood on the training samples. Efficient algorithms such as generalized iterative scaling (GIS) or improved iterative scaling (IIS) are developed for estimating the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Prior Knowledge</head><p>The prior knowledge includes the relations among the detectors and the association between the semantic concepts. The former helps to cluster the detectors into groups to obtain the modalities.</p><p>In the paper, a group or modality only contains the detectors built for one concept. For example, if a concept, saying A, has a set of detectors {A}. Similarly, the concept B has a set of detectors {B}.</p><p>Grouping the detectors is only carried out in {A} or {B} separately. And it is not allowed to cluster the element in {A} and the elements in {B} into one group. This constraint keeps each modality to have one unique concept identity, which is shared by all its modes. It facilitates the definition of the weights between the modality mode and the concept, i.e.</p><p>, q c w , in Eq. (3). The weight , q c w is set to be equal to the association strength between the concept identity assigned to q and the concept c.</p><p>The pair-wise association strength is adopted in the paper. The degree of association strength is estimated from the training samples. For example, the association strength, , ' c c w , between the concept c and another concept c' is calculated as,</p><formula xml:id="formula_12">( ) ( ) , ' # , ' # c c c c w c = (7)</formula><p>where #(c, c') is the number of documents relevant to both c and c' in the training set and #(c) is the number of documents only relevant to c. Eq. ( <ref type="formula">7</ref>) is the measurement of conditional probability of c' on c. Higher the value is, stronger the association between c' and c is. The strength of c with itself is defined to be 1. For example, the association strength between the concept airplane and outdoor is 0.84 and 0.67 between the airplane and sky. But it is zero between airplane and animal or building (estimated from the training set based on TRECVID'05 development set. See section 4 for details.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Discussions</head><p>So far, the key components have been explained. We would like to stress that the ISM unifies these components rather than sequentially combining them. In the above, the mode configuration is deterministic for simplifying the discussion. This induces a simple bottom-up structure. Once the mode models are learned, they are not affected by the concept models estimated in the component predict concept probability. It is not optimal. The good one is to integrate the bottom-up and the top-down methods, i.e. firstly, the mode models (Eq. ( <ref type="formula" target="#formula_1">1</ref>)) are estimated from the observed modality values as well as the concept model parameters in Eq. ( <ref type="formula" target="#formula_9">5</ref>) in the bottom-up manner; secondly, the learned models are used to predict the concept probability, which are further feedback to the bottom so that the mode models are updated using the top-down manner. These procedures make it impossible to learn the ISM model using the traditional algorithms. In the next section, we will present an efficient learning algorithm to train the ISM and to use it to infer the concept identity assigned to the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">LEARNING AND INFERENCE</head><p>We assume that there are M modalities according to the domain knowledge of detectors, denoted by</p><formula xml:id="formula_13">[ ] { } , 1, i X x i M = ∈</formula><p>. Each modality gets the values in the multidimensional space. Correspondingly, the dimensions for M modalities are denoted as</p><formula xml:id="formula_14">[ ] { } , 1, i D d i M = ∈ . i</formula><p>d is the number of detectors assigned to the i-th modality, i.e. the dimension of the i-th modality. Thus, a document I is represented in the M modality space by a set of vectors, say ( )</p><formula xml:id="formula_15">1 2</formula><p>, , , M I x x x = L</p><p>. Sometimes a few modalities are missed due to many reasons, e.g. there are no detector outputs for these modalities or the detectors are not used. In this case, these modalities are skipped in learning and inference. To learn the ISM for detecting the concept C, a training set, ∑ being the parameters for the mode q of the modality m and 2) the mode weights, i.e.</p><formula xml:id="formula_16">( ) { } { } , ,<label>1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{ }</head><p>, q y m θ λ = , , q y m λ for the mode q of the modality m and class y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Objective Function</head><p>In the ISM, there is a variable ( )</p><formula xml:id="formula_17">1 2 , , , M H h h h = L</formula><p>to describe the mapping between the observed modality values and the mode identities. If it is deterministic, learning is easy. However, it is hidden and random. In the next, we will derive an objective function for efficient optimization.</p><p>Firstly, we see the calculation of log-likelihood to predict the class y, given the ISM. It is calculated as,</p><formula xml:id="formula_18">I P y H I φ θ φ θ = ∑<label>( ) ( ) ( ) log , , log , , , H P y</label></formula><p>It is the sum over all possible mode configurations H.</p><p>For M modalities each having K modes, there will be M K configurations. It is impossible to compute Eq. ( <ref type="formula" target="#formula_19">8</ref>) in practice.</p><p>Even if it were possible, there would be some other challenges to find a computable model for the joint distribution of the class and the hidden variables, i.e. ( ) , , , P y H I φ θ . Here we seek an approximate computational model to solve the problem.</p><p>According to the Bayesian rule and Jensen's inequality, we can factorize the joint distribution in Eq. ( <ref type="formula" target="#formula_19">8</ref>) and find its lower bound, (</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P y I P H I P y H P H I P y H</head><formula xml:id="formula_21">φ θ φ θ φ θ φ θ = ≥ ∑ ∑ (9)</formula><p>The sum in the second line in Eq. ( <ref type="formula">9</ref>) is the lower bound of Eq. ( <ref type="formula" target="#formula_19">8</ref>) (note that <ref type="bibr">( )</ref> , , P H I φ θ is independent of θ and ( )</p><p>, , P y H φ θ is independent of φ ). Rather than computing Eq. ( <ref type="formula" target="#formula_19">8</ref>), we use its lower-bound to approximate it, i.e, (</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H P y I P H I P y H</head><formula xml:id="formula_23">φ θ φ θ ≈ ∑<label>(10)</label></formula><p>The first term on the right hand side (RHS) is the predicted probability of one mode configuration given the observed modality features and the mode models. The second term explains how much probability the class y can be predicted from a fixed configuration given the concept models.</p><p>With the assumption that the modalities occur independently, the first term on the RHS in Eq. ( <ref type="formula" target="#formula_23">10</ref>) is factorized to be, (</p><p>, ,</p><formula xml:id="formula_25">i i P H I P h x φ φ = ∏<label>(11)</label></formula><p>where ( )</p><formula xml:id="formula_26">, i i</formula><p>P h x φ is the probability assigned to the mode i by the mode predictors. It is calculated from Eq. ( <ref type="formula" target="#formula_1">1</ref>).</p><p>Substituting Eq. ( <ref type="formula" target="#formula_9">5</ref>) into Eq. ( <ref type="formula" target="#formula_23">10</ref>), the overall likelihood in the training set S is, ( Eq. ( <ref type="formula">12</ref>) is still difficult for optimization due to the nonlinear term, ( ) log , Z I θ . We further approximate it using its upper bound, i.e., ( ) ( )</p><formula xml:id="formula_28">log , 1 , Z H Z H θ θ - ≥ -<label>(13) and, ( ) ( ) ( ) , , , , , e x p</label></formula><formula xml:id="formula_29">q c q c y qc f H y Z H f f θ λ - ≥ - ⋅ ∑ ∑<label>(14) where ( ) , ,</label></formula><p>,</p><formula xml:id="formula_30">q c q c f f H y = ∑</formula><p>. It is a constant and is equal to 1 in the paper (see Eq. ( <ref type="formula" target="#formula_7">4</ref>)).</p><p>Substituting Eqs. <ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref> into Eq. ( <ref type="formula">12</ref>), we can obtain the lower bound of Eq. ( <ref type="formula">12</ref>), i.e.,</p><p>p low q c q c I y H q c q c q c I H y q c</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S PI y PHI f H y f H y P I P HI f f</head><formula xml:id="formula_32">φ θ φ λ φ λ Γ = + - ⋅ ∑ ∑ ∑ ∑ ∑ ∑ ∑ % %<label>(15)</label></formula><p>In the equation,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( )</head><p>, P H I φ is factorized as in Eq. ( <ref type="formula" target="#formula_25">11</ref>), ( )</p><formula xml:id="formula_33">, , q c</formula><p>f H y is a linear function that is calculated through simply counting the number of occurrences of the mode in the document, and</p><p>( ) , exp q c f λ ⋅ only depends on one term. Thus, it can be efficiently optimized using the following pseudo-EM algorithm. This lower-bound function is the objective function for learning the ISM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pseudo-EM Algorithm</head><p>The ISM parameters are solved through maximizing the objective function of Eq. <ref type="bibr" target="#b14">(15)</ref>. Since the mode model parameters φ and concept model parameters θ are intertwined, we seek an iterative algorithm, i.e. the pseudo-EM algorithm, to find the solution. In the M-step, the mode models are fixed so that the concept models,θ , are found for maximizing Eq. <ref type="bibr" target="#b14">(15)</ref>. By allowing the gradients of the objective function over θ to be zero, we will find that θ has a closed solution (see Eqs. 16 (a-c)). In the E-step, θ is fixed so that φ is solved by maximizing Eq. ( <ref type="formula" target="#formula_32">15</ref>). However, φ is not analytic and the gradient descent algorithm is applied to find a local solution (see Eq. ( <ref type="formula">17</ref>)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3 Pseudo-EM algorithm to estimate the ISM</head><p>In the M-step, the concept model parameters,θ , are calculated as,</p><formula xml:id="formula_34">( ) ( ) ( ) ( ) , , log m q m I c q y m q I P I P c I o c y P I o δ λ = ∑ ∑ ∑ % % % (16a)</formula><p>where</p><formula xml:id="formula_35">[1, ] m M ∈ , [1, ] q K ∈ , { } 1, 0 y ∈ , (</formula><p>)</p><formula xml:id="formula_36">, m m m q I P h q x o Z φ = = (16b) and, , m I q q m Z o = ∑ (16c) ( ) , c y δ is an indicator function, which is 1 if c is equal to y and 0 otherwise.</formula><p>In the E-step, φ is found using the gradient descent algorithm, ( )</p><formula xml:id="formula_37">1 , low t t S δ φ θ φ φ α δφ + Γ = + (17)</formula><p>where t φ is the estimate of φ at the t-th iteration and α is a constant to control the learning rate. For a specific mode model, it is easy to deduce their particular gradient functions. Thus, the details are skipped here. Note that the variances are updated in the log-domain to avoid overflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ranking with ISM</head><p>Once the ISM is learned, we can use it for classification or ranking. Thus, we need to calculate the log-likelihood in Eq. ( <ref type="formula" target="#formula_19">8</ref>). Again its lower-bound is used for approximation. The approximated log-likelihood, L y , for the class y is computed as,</p><formula xml:id="formula_38">* c T T y y c L O O = Λ ⋅ - Λ ⋅ ∑ (18a)</formula><p>where, ( ) </p><formula xml:id="formula_39">1 1 1, ,<label>1, , 1, , , , , , , , , , , ,</label></formula><formula xml:id="formula_40">λ λ λ λ λ λ Λ = L L L L L (18b) ( ) * 1* 1 * * * * * 1, ,<label>1, , 1, , , , , , , , , , , , c</label></formula><formula xml:id="formula_41">T m m M M c K c c K c c K c λ λ λ λ λ λ Λ = L L L L L (18c) ( ) * , , exp m m q c q c λ λ = (18d) ( ) 1 1 1 1 1</formula><p>, , , , , , , , , ,</p><formula xml:id="formula_42">T m m M M K K K O o o o o o o = L L L L L (18e)</formula><p>The computation is trivial. When the likelihood is known for all classes, the document will be assigned to the class having the maximal value.</p><p>To use the ISM for ranking, the likelihood ratio or the loglikelihood difference between the positive class and the negative class is used. The log-likelihood difference is calculated as,</p><formula xml:id="formula_43">1 0 R L L = -<label>(19)</label></formula><p>.</p><p>The documents are ranked according to the decreasing score R.</p><p>Higher the value R is, higher the rank is assigned to the corresponding document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS AND ANALYSES</head><p>We evaluate the presented fusion model on the task of semantic concept detection using the development dataset in TRECVID 2005. The dataset has 137 MPEG news videos. We randomly split the videos into three sets, i.e. 70% (96 videos, ~30,000 shots) for training, 15% (20 videos, ~7,000 shots) for validation, and 15% (21 videos, ~6,700 shots) for evaluation. 39 semantic concepts, officially used in TRECVID, are listed in Table <ref type="table" target="#tab_0">1</ref>. For each type of features, one SVM classifier (SVM) <ref type="bibr" target="#b23">[24]</ref> or one linear discriminative function (LDF) classifier is trained. LDF is trained using the ROC optimization algorithm <ref type="bibr" target="#b22">[23]</ref>. Table <ref type="table" target="#tab_1">2</ref> lists the details of 8 classifiers, i.e. the identity number of a classifier (Column ID), feature type (Column Feature) and classifier type (Column Classifier). For each concept, these 8 classifiers are trained. In total there are 312 detector scores available for fusion.</p><p>The performance metric of the concept detection is the average precision (AP) at the top-2000 retrieved shots and the system performance is measured by the mean average precision (MAP) over 39 concepts. This is the official NIST evaluation metric.</p><p>1. Initialization a) k-means clustering for initializing modality mode models φ . b) θ is set to zero. 2. M-step: Calculate concept models θ when φ is fixed.</p><p>3. E-step: Update mode models φ using the gradient descent algorithm when θ is fixed.</p><p>4. Stop until the predefined criterion reaches, i.e. the maximal iterative number or the relative increment of objective function is less than the threshold. Otherwise, go to (2).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with SVM</head><p>The benchmark system is based on the SVM discriminative model fusion. It has been demonstrated successful in TRECVID. In the first experiment, we build the benchmark system, SVM1, by only combining the concept-specific classifiers, i.e. 8 classifiers, for detecting a particular concept, and do not consider the effects of other concepts. We carefully tune the SVM configuration, i.e. the kernel type and parameters of the kernel, for each concept on the validation set. Then the configuration having the highest AP value on the validation set is used to train the final SVM model and the learned SVM fusion model is evaluated on the evaluation set.</p><p>Correspondingly, we also train an ISM-based system, ISM1, where each classifier is treated as one modality. The ISM is tuned as follows: first we train the ISM using 3 difference mode numbers, i.e. 2, 4 and 8, for each modality and 10 iterations to select the mode number having the highest AP value on the validation set. Then the ISM with the selected mode number is trained in 30 iterations. Each iteration generates an ISM model, from which the model having the highest AP value on the validation set is chosen for testing on the evaluation set. All other constant parameters, e.g. η ,α , in the ISM learning algorithm are empirically set based on one concept. Then they are used for all other concepts. These experiment results are shown in Figure <ref type="figure" target="#fig_6">4</ref>.</p><p>The MAP value of ISM fusion is 0.239 over 39 concepts on the evaluation set. Comparing with 0.223, the MAP value of the SVM-based fusion, we have obtained a relative improvement of 7.2%. The ISM system outperforms the SVM system among 27 out of 39 concepts. Both systems are better than the performance of the best individual detector. In our experiment, the best individual detector is observed for the SVM system trained on the HSV feature. Its MAP value is 0.201.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effect of Inter-concept Association</head><p>The second experiment evaluates the effect of the inter-concept association. The association strength between the concepts is estimated from the training samples and calculated according to Eq. ( <ref type="formula">7</ref>). The 312 classifier outputs are used, each detector being treated as one modality. As comparison, the benchmark system is still the SVM-based fusion model trained on a 312-dimensional feature vector. Both systems are tuned using the same way to the above experiment. The two systems are denoted as SVM2 and ISM2, respectively. Figure <ref type="figure" target="#fig_7">5</ref> illustrates the AP values for all concepts. The MAP value of the SVM system, SVM2, on 39 concepts is only 0.204, which is worse than the SVM1 system with the MAP value 0.223. In contrast, the ISM system, ISM2, obtains 5.4% relative improvement of the MAP value when compared with the ISM1 system. Its MAP value reaches 0.252. The further analysis on each concept reveals that incorporating the inter-concept association has indeed enhanced the detection performance for 26 out of 39 concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of Grouping Detectors</head><p>The above experiments treat each detector as one modality. Now we will study the effect of grouping some detectors into one modality. Here only the results on ISM are reported. We base on the ISM2 system and group some detectors into one modality. For simplicity, we will only study one grouping method, i.e. grouping 8 detectors from the same concept into one modality. Thus there are 39 modalities to be used to train the third ISM system, ISM3.</p><p>The comparison of AP values between the ISM3 system and the ISM2 system is depicted in Figure <ref type="figure" target="#fig_8">6</ref>. The ISM3 system has the MAP value 0.236. It is worse than the ISM2 system with the MAP value 0.252. It suggests that this way of grouping detectors does not improve system performance. Among 39 concepts, the ISM3 system only has 12 concepts which perform better than the ISM2. For some concepts, grouping greatly deteriorates the ranking performance, e.g. the concept court (ID=10) whose AP value is reduced to 0.064 from 0.244. Perhaps there are other grouping schemes that may perform better, which may not be easy to identify. The knowledge of detector correlation does not seem to be as powerful as that of the inter-concept association. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Modality Distribution</head><p>As discussed in Section 2, each modality has its distinct distribution characterized by a set of mode models in the ISM. In the section, we study how the learned mode models fit the empirical estimation of the modality distribution from the training samples, and also empirically analyze and visualize the relation between the modes and the classes. The experiments of the ISM1 system are chosen for illustration, where 8 modalities are used and each classifier is treated as one modality (see Table <ref type="table" target="#tab_1">2</ref> for the IDs of the classifiers or modalities). Due to the limited space, we only select 2 modalities, i.e. Modality 1 (GCC feature based SVM) and Modality 6 (GCC feature based LDF) for the concept airplane. Its Modality has 2 modes in the ISM1 system.</p><p>First, we compare the empirical histogram of modality values estimated from the training samples with the predicted histogram by the mode models. They are shown in Figure <ref type="figure">7</ref> for the Modality 1 and Figure <ref type="figure">8</ref> for the Modality 6. It is found that the prediction performs better for the modality 6, i.e., the LDF output, than the Modality 1, i.e. SVM output. It may be that the SVM score has a much smaller variance than the LDF. In the future, we will seek other generative models to fit the different modality distribution. In addition, from Figure <ref type="figure">7</ref> and Figure <ref type="figure">8</ref>, we observe that there is an obvious relation between the empirical histogram and the mode model. The two mode models respectively fit into two different kinds of empirical histograms.</p><p>To build a link between the modes and the classes, we draw the empirical histograms of the modality values for the positive and negative class separately and depict them with the predicted histograms by the two mode models. The curves are illustrated in Figure <ref type="figure">9</ref> for Modality 1 and Figure <ref type="figure" target="#fig_10">10</ref> for Modality 6. Obviously, each mode can be highly correlated with one class. For example, in Figure <ref type="figure">9</ref>, Mode 2 fits well with the negative class while Mode 1 fits with the positive class. The similar case is found in Figure <ref type="figure" target="#fig_10">10</ref>. It means that for the Modality 1, its Mode 1 is discriminative for the positive class while the Mode 2 is discriminative for the negative class. Thus, the former should have higher weight for the positive class than that for the negative. The property is exemplified in Figure <ref type="figure">11</ref> through analyzing the mode weights. Similarly, we can draw conclusions for other modality modes.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis of Mode Weights</head><p>Now we analyze the learned weights in the concept model of Eq. ( <ref type="formula" target="#formula_9">5</ref>). These weights measure the contribution of the modes to the concept. If its absolute value is high, the mode should be important and discriminative for the concept. Otherwise, the mode contribution to predict the concept is small. Here two concepts, airplane and flag-us, are selected as examples. The ISM models used in the ISM1 are chosen for illustration. The weights are plotted in Figure <ref type="figure">11</ref> for airplane and Figure <ref type="figure" target="#fig_1">12</ref> for flag-us, respectively. In the selected ISM models, each modality has 2 modes. Thus there are 16 weights. In the mode index in the 2 figures, Mode 1 and 2 belong to one modality. Similarly, Mode 3 and 4 are in another modality. The same rule is applied for others modes.</p><p>From the two figures, the different patterns of mode weights are observed for the two classes. For the two modes in one modality, it is often seen that one mode has a high weight for the positive class while another has a high weight for the negative class. It implies that in each modality, some modes will dominate in the positive class while others dominate in the negative class. However, it is also found that the mode weights in a modality may be almost equal. For the concept airplane (see Figure <ref type="figure">11</ref>), the weights of Mode 9 are zeros for both the negative and positive classes and are very close for Mode 10. Similarly, for the concept flag-us (see Figure <ref type="figure" target="#fig_1">12</ref>), the weights of Mode 9 and 10 are also close to each other for both classes. That implies that the two modes have fewer discriminative information for prediction and the corresponding modality may have lower capacity to distinguish the positive class from the negative.</p><p>We re-examine the corresponding detector performance of the modality and find that it is the SVM classifier using the GLCM feature. The AP value is 0.0015 for airplane. It performs worst in all 8 detectors for the concept. For flag-us, its AP value is 0.033 ranked at the middle in all 8 detectors. The similar observation is found for other concepts. These findings may be used to predict and select the discriminative detectors for fusion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In the paper, a framework, i.e. Integrated Statistical Model (ISM), is presented for combining rich evidences extracted from the domain knowledge of detectors, the intrinsic structure of modality distribution and inter-concept association. The ISM provides a unified framework for evidence fusion. Its efficiency and capacity are evaluated on semantic concept detection using the development dataset of TRECVID 2005. We compare the ISM fusion with the SVM-based discriminative fusion. Significant improvement is obtained. Through analyzing the histogram of modality values and the learned mode weights, we find that the</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>It refers to the statistical distribution of modality values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 a</head><label>2</label><figDesc>Figure 2 a toy example to illustrate the mode importance of the modality (Circle points: negative samples. Plus points: positive samples. Blue curve: fitting Gaussian curve from the samples)</figDesc><graphic coords="3,355.20,179.04,141.36,105.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>y is the annotation for the document I, which is 1 if I is relevant to C ( i.e. the positive class) and 0 (i.e. the negative class) otherwise. The model parameters to be estimated include 1) the mean and covariance (diagonal here) of the mode models,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4</head><label>4</label><figDesc>Figure 4 Performance comparison between the ISM and the benchmark system on the evaluation set (X-axis: the concept ID. Y-Axis: the AP value. Red bar: system ISM1. White bar: system SVM1.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5</head><label>5</label><figDesc>Figure 5 Effect of inter-concept association on the performance for the ISM and SVM systems on the evaluation set (X-axis: the concept ID. Y-Axis: the AP value. Red bar: ISM2 system. White bar: SVM2 system.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6</head><label>6</label><figDesc>Figure 6 Effect of grouping detectors on the performance on the evaluation set for the ISM systems (X-axis: the concept ID. Y-Axis: the AP value. Red bar: ISM2 system. White bar: ISM3 system.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 Figure 8 Figure 9</head><label>789</label><figDesc>Figure 7 Comparison between the empirical histogram (Blue curve marked with Empirical) and predicted histogram (Red curve marked with Predicted) by the mode models on the training set (Concept: airplane. Modality: 1. X-axis: the modality values. Y-axis: the probability of samples whose modality values are in the interval.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10</head><label>10</label><figDesc>Figure 10 Illustration of empirical histograms for the positive and negative classes and predicted histograms by the mode models, respectively, on the training set (Concept: airplane. Modality: 6. X-axis: the modality values. Y-axis: the probability of samples whose modality values are in the interval. Empirical_P: the empirical histogram of the positive class. Empirical_N: the empirical histogram of the negative class. Mode_1: the histogram predicted by the Mode 1. Mode_2: the histogram predicted by the Mode 2.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 Figure 12</head><label>1112</label><figDesc>Figure 11 Learned mode weights for the positive and the negative classes (Concept: airplane. X-axis: the mode ID. Yaxis: the weight coefficient. Red bar: negative class. White bar: positive class.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 Semantic concepts in TRECVID'05</head><label>1</label><figDesc></figDesc><table><row><cell>ID Concept</cell><cell>ID Concept</cell><cell>ID Concept</cell></row><row><cell>1 Airplane</cell><cell>14 Explosion_Fire</cell><cell>27 Police_Security</cell></row><row><cell>2 Animal</cell><cell>15 Face</cell><cell>28 Prisoner</cell></row><row><cell>3 Boat_Ship</cell><cell>16 Flag-US</cell><cell>29 Road</cell></row><row><cell>4 Building</cell><cell cols="2">17 Government-Leader 30 Sky</cell></row><row><cell>5 Bus</cell><cell>18 Maps</cell><cell>31 Snow</cell></row><row><cell>6 Car</cell><cell>19 Meeting</cell><cell>32 Sports</cell></row><row><cell>7 Charts</cell><cell>20 Military</cell><cell>33 Studio</cell></row><row><cell>8 Computer_TV-</cell><cell>21 Mountain</cell><cell>34 Truck</cell></row><row><cell>screen</cell><cell></cell><cell></cell></row><row><cell cols="2">9 Corporate-Leader 22 Natural-Disaster r</cell><cell>35 Urban</cell></row><row><cell>10 Court</cell><cell>23 Office</cell><cell>36 Vegetation</cell></row><row><cell>11 Crowd</cell><cell>24 Outdoor</cell><cell>37 Walking_Running</cell></row><row><cell>12 Desert</cell><cell>25 People-Marching</cell><cell>38 Waterscape_Waterfront</cell></row><row><cell>13 Entertainment</cell><cell>26 Person</cell><cell>39 Weather</cell></row><row><cell cols="3">The features used to build the concept detectors are shown below:</cell></row><row><cell cols="3">Global color correlogram (GCC) in HSV space: 324-</cell></row><row><cell>dimension.</cell><cell></cell><cell></cell></row><row><cell cols="3">Co-occurrence texture extracted from global gray-level</cell></row><row><cell cols="3">co-occurrence matrix (GLCM): 64-dimension.</cell></row><row><cell cols="3">3-D global color histogram in HSV (HSV): 162-dimension.</cell></row><row><cell cols="3">3-D global color histogram in RGB (RGB): 125-dimension.</cell></row><row><cell cols="3">3-D global color histogram in LAB (LAB): 125-dimension.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 Detailed description of classifiers</head><label>2</label><figDesc></figDesc><table><row><cell>ID</cell><cell>Feature</cell><cell>Classifier</cell><cell>ID</cell><cell>Feature</cell><cell>Classifier</cell></row><row><cell>1</cell><cell>GCC</cell><cell>SVM</cell><cell>5</cell><cell>GLCM</cell><cell>SVM</cell></row><row><cell>2</cell><cell>HSV</cell><cell>SVM</cell><cell>6</cell><cell>GCC</cell><cell>LDF</cell></row><row><cell>3</cell><cell>LAB</cell><cell>SVM</cell><cell>7</cell><cell>HSV</cell><cell>LDF</cell></row><row><cell>4</cell><cell>RGB</cell><cell>SVM</cell><cell>8</cell><cell>GLCM</cell><cell>LDF</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www-nlpir.nist.gov/projects/trecvid/</p></note>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>modes characterize the structure of the modality distribution and they have different power to discriminate the positive class from the negative. However, we also find that the predicted histogram by the learned mode models does not fit well for some modalities. In future, we will exploit other generative models rather than the Gaussian distribution and study their efficiency.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">IBM research TRECVID-2005 video retrieval system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of TRECVID&apos;05 Workshop</title>
		<meeting>of TRECVID&apos;05 Workshop</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to natural language processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="71" />
			<date type="published" when="1996-03">Mar. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The use of the area under the ROC curve in the evaluation of machine learning algorithms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1145" to="1159" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Intelligent multimedia group of Tsinghua University at TRECVID 2006</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of TRECVID 2006 Workshop</title>
		<meeting>of TRECVID 2006 Workshop</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recent advances and challenges of semantic image/video search</title>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prof. of ICASSP</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A comparison of score, rank and probability-based fusion methods for video shot retrieval</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mc Donald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prof. of CIVR</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<title level="m">CMU Informedia&apos;s TRECVID 2005 skirmishes. Proc. of TRECVID&apos;05 Workshop</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative model fusion for semantic concept detection and annotation in video</title>
		<author>
			<persName><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Multimedia&apos;03</title>
		<meeting>of ACM Multimedia&apos;03</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Context-based concept fusion with boosted conditional random fields</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP&apos;07</title>
		<meeting>of ICASSP&apos;07</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analyses of multiple evidence combination</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR&apos;97</title>
		<meeting>of SIGIR&apos;97</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probabilistic semantic video indexing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Naphade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS&apos;00</title>
		<meeting>of NIPS&apos;00</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using models of score distributions in information retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Workshop on Language Modeling and Information Retrieval</title>
		<meeting>of Workshop on Language Modeling and Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimedia semantic indexing using model vectors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICME&apos;03</title>
		<meeting>of ICME&apos;03</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combing multiple classifiers by averaging or by multiplying</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1475" to="1485" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Normalized classifier fusion for semantic visual concept detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP&apos;03</title>
		<meeting>ICIP&apos;03</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative fusion approach for automatic image annotation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MMSP&apos;05</title>
		<meeting>of MMSP&apos;05</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimal multimodal fusion for multimedia data analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Multimedia&apos;04</title>
		<meeting>of ACM Multimedia&apos;04</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The combination limit in multimedia retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Multimedia&apos; 03</title>
		<meeting>of ACM Multimedia&apos; 03</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A comparative study of evidence combination strategies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yavlinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP&apos;04</title>
		<meeting>of ICASSP&apos;04</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Active Context-based concept fusion with partial user labels</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICIP&apos;06</title>
		<meeting>of ICIP&apos;06</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using maximum entropy for text classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI Workshop on Machine Learning for Information Filtering</title>
		<meeting>of IJCAI Workshop on Machine Learning for Information Filtering</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<title level="m">Modern information retrieval</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classifier optimization for multimedia semantic concept detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">B</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICME&apos;06</title>
		<meeting>of ICME&apos;06</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning to classify text using support vector machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Kluwer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Dissertation</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<title level="m">Proc. of TRECVID&apos;05 Workshop</title>
		<meeting>of TRECVID&apos;05 Workshop</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
