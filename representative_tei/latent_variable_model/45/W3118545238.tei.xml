<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Domain Latent Modulation for Variational Transfer Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinyong</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Science</orgName>
								<orgName type="institution">University of Otago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeremiah</forename><forename type="middle">D</forename><surname>Deng</surname></persName>
							<email>jeremiah.deng@otago.ac.nz</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Science</orgName>
								<orgName type="institution">University of Otago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Cranefield</surname></persName>
							<email>stephen.cranefield@otago.ac.nz</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Science</orgName>
								<orgName type="institution">University of Otago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuejie</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Science</orgName>
								<orgName type="institution">University of Otago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Domain Latent Modulation for Variational Transfer Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a cross-domain latent modulation mechanism within a variational autoencoders (VAE) framework to enable improved transfer learning. Our key idea is to procure deep representations from one data domain and use it as perturbation to the reparameterization of the latent variable in another domain. Specifically, deep representations of the source and target domains are first extracted by a unified inference model and aligned by employing gradient reversal. Second, the learned deep representations are cross-modulated to the latent encoding of the alternate domain. The consistency between the reconstruction from the modulated latent encoding and the generation using deep representation samples is then enforced in order to produce inter-class alignment in the latent space. We apply the proposed model to a number of transfer learning tasks including unsupervised domain adaptation and image-toimage translation. Experimental results show that our model gives competitive performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In machine learning, one can rarely directly apply a pretrained model to a new dataset or a new task, as the performance of a learned model often plunges significantly for the new data which may have significant sampling bias or even belong to different distributions. Transfer learning can help us utilize the learned knowledge from a previous domain (the 'source') to improve performance on a related domain or task (the 'target') <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>From the perspective of probabilistic modeling <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b33">34]</ref>, the key challenge in achieving cross-domain transfer is to learn a joint distribution of data from different domains. Once the joint distribution is learned, it can be used to generate the marginal distribution of the individual domains <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23]</ref>. Under the variational inference scenario, an inferred joint distribution is often applied to the latent space. Due to the coupling theory, inferring the joint distribution from the marginal distributions of different domains is a highly ill-posed problem <ref type="bibr" target="#b20">[21]</ref>. To address this problem, UNIT <ref type="bibr" target="#b22">[23]</ref> makes an assumption that there is a shared latent space for the two domains. Usually, this can be achieved by applying the adversarial strategy to the domains' latent spaces. Another line of research focuses on the use of a complex prior to improve the representation performance for the input data <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b11">12]</ref>. However, the previous works neglect the role of the generation process for the latent space which could be helpful for cross-domain transfer scenarios.</p><p>In this paper, we propose a novel latent space reparameterization method, and employ a generative process to cater for the cross-domain transferability. Specifically, we incorporate a cross-domain component into the reparameterization transformation, which builds the connection between the variational representations and domain features in a cross-domain manner. The generated transfer latent space is further tuned by domain-level adversarial alignment and domain consistency between images obtained through reconstruction and generations. We apply our model to the homogeneous transfer scenarios, such as unsupervised domain adaptation and image-to-image translation. The experimental results show the efficiency of our model.</p><p>The rest of the paper is organized as follows. In Section 2, some related work is briefly reviewed. In Section 3, we outline the overall structure of our proposed model and develop the learning metrics with defined losses. The experiments are presented and discussed in Section 4. We conclude our work in Section 5, indicating our plan of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Latent space manipulation: As discussed above, for a joint distribution, manipulation of the latent space is common <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22]</ref> for the cross-domain adaptation situations. One approach focuses on a shared latent space, where the latent encodings are regarded as common representations for inputs across domains. Some adversarial strategy is usually used to pool them together so that the representations are less domain-dependent. For the variational approach, works in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b14">15]</ref> adopt complex priors for multi-modal latent representations, while other works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b21">22]</ref> still assume arXiv:2012.11727v1 [cs.LG] 21 Dec 2020 a standard Gaussian prior. Another aproach is to use disentangled latent representations where the latent encoding is divided into some defined parts (e.g. style and content parts), then the model learns separated representations and swaps them for the transfer <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref>. Our method is different from these approaches. In our model, learned auxiliary deep representation is used to generate perturbations to the latent space through a modified reparameterization using variational information from the counterpart domain. It helps generate cross-domain image translation. The transfer is carried out by a reparameterization transformation, using statistical moments retaining specific information for one domain, and deep representation providing information from another domain.</p><p>Varied Homogeneous transfer tasks: The manipulation on the latent space is often interwoven with the homogeneous image transfer together, such as unsupervised domain adaptation and image translation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b4">5]</ref>. In the domain separation networks <ref type="bibr" target="#b3">[4]</ref>, separate encoding modules are employed to extract the invariant representation and domainspecific representations from the domains respectively, with the domain-invariant representations being used for the domain adaptation. References <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b10">11]</ref> transfer the target images into source-like images for domain adaptation. References <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b12">13]</ref> map the inputs of different domains to a single shared latent space, but cycle consistency is required for the completeness of the latent space. The UFDN <ref type="bibr" target="#b21">[22]</ref> utilizes a unified encoder to extract the multi-domain images to a shared latent space, and the latent domain-invariant coding is manipulated for image translation between different domains.</p><p>In contrast, we adopt the pixel-level adaptation between domains from the cross-domain generation, but the proposed model can also be used at the feature-level due to the latent space alignment. Our model also has a unified inference model, but the consistency is imposed in a straightforward way, with reduced computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem setting</head><p>Let X ⊂ R d be a d-dimensional data space, and X = {x 1 , x 2 , . . . , x n } ∈ X the sample set with marginal distribution p(X). The source domain is denoted by a tuple (X s , p(X s )), and the target domain by (X t , p(X t )). In our paper, we consider the homogeneous transfer with domain shift, i.e. X s ≈ X t , but p(X s ) = p(X t ). For the unsupervised pixel-level domain adaptation scenario, the label set is Y = {y 1 , y 2 , . . . y n } ∈ Y (Y is the label space), and a task T = p(Y |X) is considered too. However, only the source domain's label set Y s is available during transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Transfer Latent Space</head><p>As any given marginal distribution can be yielded by an infinite number of joint distributions, we need to build an inference framework with some constraints. Under the variational autoencoder (VAE) framework, the latent space is one of the manipulation targets. We propose the transfer latent space as follows.</p><p>Definition 1. Transfer Latent Space Z. Let x s ∈ X s , x t ∈ X t be the domain samples. Let us have a map f that extracts domain information Ω and a feature representation h given an input x:</p><formula xml:id="formula_0">f : x -→ (Ω, h), x ∈ X s ∪ X t .</formula><p>Suppose we construct a transfer map G that generates a latent variable z from Ω and h with domain crossovers:</p><formula xml:id="formula_1">zst = G(Ω s , h t ), zts = G(Ω t , h s ).</formula><p>The joint space formed by zst and zts samples is defined as a transfer latent space, denoted by Z.</p><p>The transfer latent space is intended to become a "mixer" for the two domains, as the resulted latent variables are under cross-domain influences. Hence the transfer latent space can be regarded as a generalization of the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Framework</head><p>Our framework is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. In our framework, we build the cross-domain generation by a unified inference model E φ (•) (as an implementation of the map f ) and a generative model for the desired domain D θ (•), e.g., the source domain in our model. A discriminator Ξ is utilized for the adversarial training. We use the terms "inference model" and "encoder" for E φ (•), and "generative model" and "decoder" for D θ (•) interchangeably.</p><p>As discussed in section 3.2, under the variational framework, the domain information Ω (here we remove the domain subscript for simplicity) is usually the pair (µ, σ). Let h be the flattened activations of the last convolution layer in E u . Then, following the treatment in <ref type="bibr" target="#b15">[16]</ref>, µ and σ can be obtained by µ = W µ h + b µ and σ = W σ h + b σ , where W µ , W σ , b µ , b σ are the weights and biases for µ and σ.</p><p>From our observations, both shallow (e.g. PCA features) and deep representations can be used to obtain domain information h, in our end-to-end model we use the latter. We choose the high-level activation of the last convolutional layer, i.e., h = sigmoid(W h h + b h ) as the deep representation <ref type="bibr" target="#b43">[44]</ref>, where W h , b h are the weights and biases for the deep abstractions.</p><p>Having obtained the domain information Ω and deep representation h, a natural choice for the transfer map G is through reparameterization. Here we propose a modified reparameterization trick to give the sampling from the transfer latent space as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Encoder</head><note type="other">Deep Decoder</note><formula xml:id="formula_2">zst = G((µ s , σ s ), h t ) = µ s + σ s (γ 1 h t + γ 2 ),<label>(1)</label></formula><p>and</p><formula xml:id="formula_3">zts = G((µ t , σ t ), h s ) = µ t + σ t (γ 1 h s + γ 2 ),<label>(2)</label></formula><p>where h s (h t ) is the sample of the deep representation space H s (H t ); µ s and σ s (µ t and σ t ) are the mean and standard deviation of the approximate posterior for the source (target) domain; γ 1 , γ 2 &gt; 0 are trade-off hyperparameter to balance the deep feature modulation and the standard Gaussian noise ; and stands for the element-wise product of vectors. Therefore, the auxiliary noise in VAE resampling is now a weighted sum of a deep representation from the other domain and Gaussian noise, different from the standard VAE framework. Because the modified reparameterization allows a domain's deep representation to get modulated into another domain's latent representation, we call our model "Cross-Domain Latent Modulation", or CDLM for short. Now we have obtained the transfer encodings by a unified encoder. Following the probabilistic encoder analysis <ref type="bibr" target="#b15">[16]</ref>, a shared inference model confines the latent variables into the same latent space. But this cannot guarantee them to be aligned. To pull the domains close, an adversarial strategy <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref> should be used for the alignment. The gradient reversal layer <ref type="bibr" target="#b6">[7]</ref> is used in our model, by which adversarial learning is introduced to learn transferable features that are robust to domain shift. The adversarial alignment between H s and H t is for domain-level.</p><p>Furthermore, for better interpretation of the modulated reparameterization, let h t ∼ p(H t ) = N (µ ht , σ ht ). Then for zst , we have zst ∼ N (z st ; µ st , σ 2 st I). For the i-th element of the distribution moments are given as follows:</p><formula xml:id="formula_4">µ i st = E{µ i s + σ i s (γ 1 h i t + γ 2 i )} = µ i s + γ 1 σ i s µ i ht .</formula><p>(3)</p><formula xml:id="formula_5">Var(z i st ) = E{(σ i s (γ 1 h i t + γ 2 i ) -γ 1 σ i s µ i ht ) 2 } = (σ i s ) 2 [γ 2 1 (σ i ht ) 2 + γ 2 2 ].<label>(4)</label></formula><p>Therefore, the µ i st and σ i st are</p><formula xml:id="formula_6">µ i st = µ i s + γ 1 σ i s µ i ht σ i st = σ i s γ 2 1 (σ i ht ) 2 + γ 2 2 .</formula><p>(</p><formula xml:id="formula_7">)<label>5</label></formula><p>Here, it is reasonable to assume µ ht ≈ µ t and σ ht ≈ σ t when the training is finished. With a practical setting of γ 1 γ 2 , and in effect σ i s = 1, Eq. ( <ref type="formula" target="#formula_7">5</ref>) can be further simplified to µ i st = µ i s + γ 1 µ i t , and σ i st = γ 1 σ i s σ i t . Then we can see that µ st can be regarded as a location shift of µ s under the influence of µ t , which helps reduce the domain gap; σ st can be taken as a recoloring of σ s under the influence from the target. The formulation of zts can be similarly interpreted. These modulated encodings are hence constructed in a cross-domain manner.</p><p>Next, we apply the consistency constraint to the transfer latent space with modulation for further inter-class alignment. It has been found that consistency constraints preserve class-discriminative information <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b7">8</ref>]. For our model, the consistency is applied to the reconstructions from modulated encodings and the corresponding generations from deep representations. Let D θ (•) be the generative model for domain image generation from the transfer latent space. The consistency requirements are</p><formula xml:id="formula_8">D θ (z st ) = D θ (γ 1 h s + γ 2 ) D θ (z ts ) = D θ (γ 1 h t + γ 2 ),<label>(6)</label></formula><p>where D θ (z st ) is the reconstruction of the source ( x st ), D θ (z ts ) is for the target ( x ts ). D θ (•) can also function as a generative model, generating</p><formula xml:id="formula_9">x s = D θ (γ 1 h s + γ 2 ) and x t = D θ (γ 1 h t + γ 2 )</formula><p>for the source and target domain respectively. Also, the consistencies can guide the encoder to learn the representations from both domains. Finally, a desired marginalized decoder, e.g. the source decoder, is trained to map the target images to be sourcelike. We render the target's structured generation x ts for the test mode. For this end, we do not need the source to be taken into account for the test. That means a test image from the target domain x i t first passes through the inference model and obtains its deep feature h i t . Then it is fed into the generation model to generate an image with source style but keep its own class. That is to make the marginal distribution p( x i ts ) ≈ p(x j s ), but keep its class y i t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning</head><p>Our goal is to update the variational parameters to learn a joint distribution and the generation parameters for the desired marginal distribution. Since the latent variables are generated with inputs from both domains, we have a modified formulation adapted from the plain VAE:</p><formula xml:id="formula_10">log p(x s , x t ) -KL(q φ (z|x s , x t ) p(z|x s , x t )) = E[log p(x s , x t |z)]</formula><p>-KL(q φ (z|x s , x t ) p(z)), <ref type="bibr" target="#b6">(7)</ref> where KL(•) is the Kullback-Leibler divergence, and the transfer latent variable z can be either zst or zts . Minimizing KL(q φ (z|x s , x t ) p(z|x s , x t )) is equivalent to maximizing the variational evidence lower bound (ELBO) L(x s , x t , θ, φ): <ref type="bibr" target="#b7">(8)</ref> where the first term corresponds to the reconstruction cost (L Rec ), and the second term is the K-L divergence between the learned latent probability and the prior (specified as N (0, I)) (L KL ). Considering the reconstruction of x s , and the K-L divergence for both zst and zts , we have</p><formula xml:id="formula_11">L(x s , x t , θ, φ) = E qφ [log p θ (x s , x t |z)] -KL(q φ (z|x s , x t ) p(z)),</formula><formula xml:id="formula_12">L(x s , x t , θ, φ) = E z∼q(zst|xs,xt) [log p θ (x s |z st )] -KL(log q φ (z st |x s , x t ) p(z)) -KL(log q φ (z ts |x t , x s ) p(z)).<label>(9)</label></formula><p>To align the deep representations of the source and target domains, an adversarial strategy is employed to regularize the model. The loss function is given by</p><formula xml:id="formula_13">L adv = E hs∼p(hs|xs) [log Ξ(h s )] + E ht∼p(ht|xt) [log(1 -Ξ(h t ))],<label>(10)</label></formula><p>where Ξ(•) is the discriminator to predict from which domain the deep representation feature is.</p><p>From the analysis in Section 3.2, we can introduce a pairwise consistency between the reconstruction and the generation for the source and the target in an unsupervised manner respectively. The consistencies regularization improve the inter-class alignment. For the consistency loss L c , both the l<ref type="foot" target="#foot_0">foot_0</ref> and l 2 -norm penalty can be used to regularize the decoder. Here we simply use MSE. Let L s c and L t c be the consistency for the domains respectively. L c is given as a combination of these two components, weighted by two coefficients β 1 and β 2 , respectively:</p><formula xml:id="formula_14">L c = β 1 L s c + β 2 L t c = β 1 (E z∼q(z|xs,xt) [log p( x s |z st )] -E hs∼p(hs|xs) [log p( x s |h s )]) 2 + β 2 (E z∼q(z|xs,xt) [log p( x ts |z ts )] -E ht∼p(ht|xt) [log p( x ts |h t )]) 2 .</formula><p>(11) Then, the variational parameters φ and generation parameters θ are updated by the following rules:</p><formula xml:id="formula_15">φ ← φ -η 1 ∇(L adv + λ 1 L KL + λ 2 L Rec ) θ ← θ -η 2 ∇(L Rec + L c ),<label>(12)</label></formula><p>where η 1 , η 2 are the learning rates. Note, that only data from the desired domain (the source) are used to train the reconstruction loss. The KL items approximate the transfer latent space to their prior. Hyperparameters λ 1 , λ 2 are used to balance the discriminator loss and reconstruction loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conducted extensive evaluations of CDLM in two homogeneous transfer scenarios including unsupervised domain adaptation and image-to-image translation. During the experiments, our model was implemented using Tensor-Flow <ref type="bibr" target="#b0">[1]</ref>. The structures of the encoder and the decoder adopt those of UNIT <ref type="bibr" target="#b22">[23]</ref> which perform well for image translation tasks. A two-layer fully connected MLP was used for the discriminator. SGD with momentum was used for updating the variational parameters, and Adam for updating generation parameters. The batch size was set to 64. During the experiments, we set γ 1 = 1.0, γ 2 = 0.1, λ 1 = λ 2 = 0.0001, β 1 = 0.1 and β 2 = 0.01. For the datasets, we considered a few popular benchmarks, including MNIST <ref type="bibr" target="#b18">[19]</ref>, MNSITM <ref type="bibr" target="#b6">[7]</ref>, USPS <ref type="bibr" target="#b17">[18]</ref>, Fashion-MNIST <ref type="bibr" target="#b42">[43]</ref>, Linemod <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">42]</ref>, Zap50K-shoes <ref type="bibr" target="#b44">[45]</ref> and CelebA <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We have evaluated our model on a variety of benchmark datasets. They are described as follows. MNIST: MNIST handwritten dataset <ref type="bibr" target="#b18">[19]</ref> is a very popular machine learning dataset. It has a training set of 60,000 binary images, and a test set of 10,000. There are 10 classes in the dataset. In our experiments, we use the standard split of the dataset. MNISTM <ref type="bibr" target="#b6">[7]</ref> is a modified version for the MNIST, with random RGB background cropped from the Berkeley Segmentation Dataset 1 . Linemod 3D images Following the protocol of <ref type="bibr" target="#b2">[3]</ref>, we render the LineMod <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">42]</ref> for the adaptation between synthetic 3D images (source) and real images (target). The objects with different poses are located at the center of the images. The synthetic 3D images render a black background and a variety of complex indoor environments for real images. We use the RGB images only, not the depth images. CelebA: CelebA <ref type="bibr" target="#b24">[25]</ref> is a large celebrities face image dataset. It contains more than 200k images annotated with 40 facial attributes. We select 50K images randomly, then transform them to sketch images followed the protocol of <ref type="bibr" target="#b23">[24]</ref>. The original and sketched images are used for translation. UT-Zap50K-shoes: This dataset <ref type="bibr" target="#b44">[45]</ref> contains 50K shoes images with 4 different classes. During the translation, we get the edges produced by canny detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Unsupervised Domain Adaptation</head><p>We applied our model to unsupervised domain adaptation, adapting a classifier trained using labelled samples in the source domain to classify samples in the target domain. For this scenario, only the labels of the source images were available during training. We chose DANN <ref type="bibr" target="#b6">[7]</ref> as the baseline, but also compared our model with the state-of-the-art domain adaptation methods: Conditional Domain Adaptation Network (CDAN) <ref type="bibr" target="#b26">[27]</ref>, Pixel-level Domain Adaptation (PixelDA) <ref type="bibr" target="#b2">[3]</ref>, Unsupervised Image-to-Image translation (UNIT) <ref type="bibr" target="#b22">[23]</ref>, Cycle-Consistent Adversarial Domain Adaption (CyCADA) <ref type="bibr" target="#b10">[11]</ref>, and Generate to Adapt (GtA) <ref type="bibr" target="#b32">[33]</ref>. We also used source-and target-only training as the lower and upper bound respectively, following the practice in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Quantitative Results</head><p>The performance of domain adaptation for the different tasks is shown in Table <ref type="table" target="#tab_0">1</ref>. There are 4 scenarios and 7 tasks. Each scenario has bidirectional tasks for adaptation except LineMod. For LineMod, it is adapted from synthetic 3D image to real objects. For the same adaptation task, we cite the accuracy from the corresponding references, otherwise the accuracies for some tasks are obtained by training the open-source code provided by authors with suggested optimal parameters, for fair comparison.</p><p>From Table <ref type="table" target="#tab_0">1</ref> we can see that the our method has a higher advantage compared with the baseline and the sourceonly accuracy, a little lower than the target-only accuracy from both adaptation directions. In comparison with other models, our model has a better performance for most of tasks. The CDLM has a higher adaptation accuracy for the scenarios with seemingly larger domain gap, such as MNIST→MNISTM and Fashion→FashionM. For the 3D scenario, the performance of our model is a little lower than PixelDA <ref type="bibr" target="#b2">[3]</ref>, but outperforms all the other compared methods. In PixelDA, the input is not only source image but also depth image pairs. It might be helpful for the generation. Besides, we visualize the t-SNE <ref type="bibr" target="#b37">[38]</ref> for latent encodings (z st , zts ) w.r.t the source and the target, respectively. Fig. <ref type="figure" target="#fig_5">4</ref> is the visualization for task MNISTM→MNIST and MNIST→USPS, and it shows that both are aligned well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Qualitative Results</head><p>Our model can give the visualization of the adaptation. Fig. <ref type="figure" target="#fig_2">2</ref> is the visualization for the digits and Fashion adaptation respectively. For the scenario of MNIST and USPS, the gener-  ation for the task USPS→MNIST is shown in Fig. <ref type="figure" target="#fig_2">2a</ref>. The target MNIST is transferred to the source USPS style well, meanwhile it keeps the correspondent content (label). For example, the digit '1' in MNIST become more leaned and '9' more flatten. Also in Fig. <ref type="figure" target="#fig_2">2b</ref>, the target USPS becomes the MNIST style. For the scenario of MNIST→MNISTM, our proposed model can remove and add the noise background well for adaptation.</p><p>For the scenario of Fashion, the fashion items have more complicated texture and content. In addition, the noisy backgrounds pollute the items randomly, for example, different parts of a cloth are filled with various colors. For visualizations, specifically, Fig. <ref type="figure" target="#fig_2">2e</ref> is for the task Fashion → FashionM. The proposed model can remove the noisy background and maintain the content. On the other hand, Fig. <ref type="figure" target="#fig_2">2f</ref> shows that the original Fashion images are added with similar noisy background as the source. This is promising for a   better adaptation performance. For Linemod3D, the real objects images with different backgrounds are transferred to the synthetic 3D images with black background. Due to the 3D style, the generation of the target gives different poses. For example in Fig. <ref type="figure" target="#fig_3">3</ref>, different poses of the iron object are obtained for different trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Cross-Domain Image Mapping</head><p>The proposed model also can be used for the crossdomain image mapping.</p><p>Fig. <ref type="figure" target="#fig_6">5</ref> gives a demonstration of the image style translation. Specifically, for "shoes" and "edges" in Fig. <ref type="figure" target="#fig_6">5a</ref> and<ref type="figure" target="#fig_6">5c</ref>, we can see that the proposed model can translate "edges" to its counterpart quite well. The translation is stochastic -an "edge" pattern can be used to generate "shoes" in different colors with different trials. For the more challenging "face" and "sketch" translations, the proposed model also performs well. The generations have some variations compared with the original images. In general, our method can generate realistic translated images. However, we find that compared with the translation from sketches to real images, the reverse task seems harder. For example, when a face image is given, the generated sketch loses some details. The reason may be the low-level feature is neglected when the deep feature acts as the condition.</p><p>For further evaluation, quantitative performance is evalu- ated for image mapping. SSIM <ref type="bibr" target="#b39">[40]</ref>, MSE, and PSNR are used for the evaluation. The results are shown in Table <ref type="table" target="#tab_1">2</ref>. We can see that our model outperforms E-CDRD <ref type="bibr" target="#b23">[24]</ref>, which learns a disentangled latent encoding for the source and the target for domain adaptation. Meanwhile, it matches the performance of StarGAN <ref type="bibr" target="#b4">[5]</ref>, which is designed for multidomain image translation. The result shows that our model can map cross-domain images well compared to these prior works. In addition, we also conduct the classification to evaluate the translation performance. We take shoes as an example which are labeled to 4 different classes. The recognition accuracy of our proposed model for task shoes→edge is 0.953, which is higher than the results of PixelDA (0.921) and UNIT (0.916) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Model Analysis</head><p>The effect of encoder settings -depth and different γ 1 , γ 2 : In our model, the deep features are utilized to crossmodulate the transfer latent encoding. Therefore the deep feature is an important factor in our framework and is influenced by the depth of the encoder. During the experiments, we use MNIST → USPS and Fashion → FashionM as the evaluation tasks. For the first one, they have different content, but with the same background. The second task is a totally different scenario, the images have the same content but different background. The outputs of different encoder layers (k &gt; 3) are used for the experiments. As the result (Table <ref type="table" target="#tab_2">3</ref>) shows, a higher accuracy is achieved when more layers are used to extract the deep representations. The accuracy gain of the task MNIST → USPS is lower than that of Fashion → FashionM. This is expected as features extracted by higher layers would normally eliminate lower-level variations between domains, such as change of background and illumination in the images.</p><p>For γ 1 , γ 2 , we fixed the last convolutional layer for the deep representations and evaluate different values. From Table <ref type="table" target="#tab_3">4</ref>, we can see that the performance drops down significantly with a smaller γ 1 compared with γ 2 , and increased with a larger γ 1 . The performance seems to be stabilized when γ 1 is greater than 0.9 while γ 2 remains 0.1. Following the standard VAE, we keep the noise (γ 2 = 0) in the evaluations. Meanwhile, our model works well even when </p><formula xml:id="formula_16">= 2(1 -2 )</formula><p>, where is the generalization error of a binary classifier (e.g. kernel SVM) trained to distinguish the input's domain (source or target). Following the protocol of <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32]</ref>, we calculate the A-distance on four adaptation tasks under the scenarios of Raw features, DANN features, and CDLM features respectively. The results are show in Fig. <ref type="figure" target="#fig_7">6</ref>. We observe that both the DANN and CDLM reduce the domain discrepancy compared with the Raw images scenario, and the A-distance of CDLM is smaller than the DANN's. This demonstrates that it is harder to distinguish the source and the target by the CDLM generations. Convergence: We also conduct the convergence experiment with training error on task MNSIT-USPS to evaluate our model. As shown in the Fig. <ref type="figure" target="#fig_8">7</ref>, our model has a better convergence than DANN, thought there are some oscillations at the beginning of the training. In addition, the error of CDLM is lower that the DANN, which demonstrate that CDLM has a better adaptation performance. This is consistent with the adaptation performance in Table <ref type="table" target="#tab_0">1</ref>.</p><p>The effect of unsupervised consistency metrics: In our model, two unsupervised consistency metrics are added for generation in good effects. The adaptation accuracy is used for evaluation. Table <ref type="table" target="#tab_4">5</ref> is the results for the four different connects outputs generated from the h t and zts only for the target, which improves the performance slightly. Meanwhile, we can see that the L s c loss boosts the accuracy for adaptation significantly, which connects the two domains with the generations by the h. Finally, the scenario with both L s c and L t c gives the best performance in all four tasks. It bridges both the h and z between the two domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented a novel variational crossdomain transfer learning model with cross modulation of deep representations from different domains. A shared transfer latent space is introduced, and the reparameterization transformation is modified to enforce the connection between domains. Evaluations carried out in unsupervised domain adaptation and image translation tasks demonstrate our model's competitive performance. Its effectiveness is also clearly shown in visual assessment of the adapted images, as well as in the alignment of the latent information as revealed by visualization using t-SNE. Overall, competitive performance has been achieved by our model despite its relative simplicity.</p><p>For future work, we intend to further improve our variational transfer learning framework and use it for heterogeneous, multi-domain transfer tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architectural view of the proposed model. It encourages an image from target domain (blue hexagon) to be transformed to a corresponding image in the source domain (black hexagon). The transfer latent distributions p(zts|xt, hs) and p(zst|xs, ht) are learned which are used to generate corresponding images by the desired decoder. The deep representations are integrated into the reparameterization transformation with standard Gaussian auxiliary noise. Blue lines are for the target domain and black ones are for the source domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization for the adaptations. 6 different tasks are illustrated. For each task, the first row shows target images and the second row shows the adapted images with source-like style.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Linemod 3D Synthetic→Real. For a query image (on the left), different adaptation images (to the right) with various poses can be generated.</figDesc><graphic coords="6,295.87,203.02,137.70,137.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: t-SNE visualization of cross-domain latent encodings zst , zts . The zst are in blue, and the zts in red.</figDesc><graphic coords="6,415.68,202.72,141.39,138.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of cross-domain image mapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: A-distances comparison for four tasks.</figDesc><graphic coords="8,74.13,389.22,195.94,146.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Convergence of CDLM compared with DANN.</figDesc><graphic coords="8,331.10,173.95,202.68,152.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Mean classification accuracy comparison. The "source only" row is the accuracy for target without domain adaptation training only on the source. The "target only" is the accuracy of the full adaptation training on the target. For each source-target task the best performance is in bold</figDesc><table><row><cell>Source</cell><cell>MNIST</cell><cell>USPS</cell><cell>MNIST</cell><cell>MNISTM</cell><cell>Fashion</cell><cell>Fashion-M</cell><cell>Linemod 3D</cell></row><row><cell>Target</cell><cell>USPS</cell><cell cols="2">MNIST MNISTM</cell><cell>MNIST</cell><cell>Fashion-M</cell><cell>Fashion</cell><cell>Linemod Real</cell></row><row><cell>Source Only</cell><cell>0.634</cell><cell>0.625</cell><cell>0.561</cell><cell>0.633</cell><cell>0.527</cell><cell>0.612</cell><cell>0.632</cell></row><row><cell>DANN [7]</cell><cell>0.774</cell><cell>0.833</cell><cell>0.766</cell><cell>0.851</cell><cell>0.765</cell><cell>0.822</cell><cell>0.832</cell></row><row><cell>CyCADA [11]</cell><cell>0.956</cell><cell>0.965</cell><cell>0.921</cell><cell>0.943</cell><cell>0.874</cell><cell>0.915</cell><cell>0.960</cell></row><row><cell>GtA [33]</cell><cell>0.953</cell><cell>0.908</cell><cell>0.917</cell><cell>0.932</cell><cell>0.855</cell><cell>0.893</cell><cell>0.930</cell></row><row><cell>CDAN [27]</cell><cell>0.956</cell><cell>0.980</cell><cell>0.862</cell><cell>0.902</cell><cell>0.875</cell><cell>0.891</cell><cell>0.936</cell></row><row><cell>PixelDA [3]</cell><cell>0.959</cell><cell>0.942</cell><cell>0.982</cell><cell>0.922</cell><cell>0.805</cell><cell>0.762</cell><cell>0.998</cell></row><row><cell>UNIT [23]</cell><cell>0.960</cell><cell>0.951</cell><cell>0.920</cell><cell>0.932</cell><cell>0.796</cell><cell>0.805</cell><cell>0.964</cell></row><row><cell>CDLM ( x ts )</cell><cell>0.961</cell><cell>0.983</cell><cell>0.987</cell><cell>0.962</cell><cell>0.913</cell><cell>0.922</cell><cell>0.984</cell></row><row><cell>Target Only</cell><cell>0.980</cell><cell>0.985</cell><cell>0.983</cell><cell>0.985</cell><cell>0.920</cell><cell>0.942</cell><cell>0.998</cell></row></table><note><p><p><p><p><p><p><p><p><p>USPS: USPS is a handwritten zip digits datasets</p><ref type="bibr" target="#b17">[18]</ref></p>. It contains 9298 binary images</p>(16 × 16)</p>, 7291 of which are used as the training set, while the remaining 2007 are used as the test set. The USPS samples are resized to 28 × 28, the same as MNIST. Fashion: Fashion</p><ref type="bibr" target="#b42">[43]</ref> </p>contains 60,000 images for training, and 10,000 for testing. All the images are grayscale, 28 × 28 in size space. In addition, following the protocol in</p><ref type="bibr" target="#b6">[7]</ref></p>, we add random noise to the Fashion images to generate the FashionM dataset, with random RGB background cropped from the Berkeley Segmentation Dataset.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance for image mapping.</figDesc><table><row><cell>Models</cell><cell>"Sketch" to "Face" SSIM MSE PSNR</cell></row><row><cell cols="2">E-CDRD [24] 0.6229 0.0207 16.86</cell></row><row><cell cols="2">StarGAN [5] 0.8026 0.0142 19.04</cell></row><row><cell>CDLM</cell><cell>0.7961 0.0140 19.89</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Adaptation accuracy with different layer depth for Tasks MNIST → USPS and Fashion → FashionM.</figDesc><table><row><cell>Tasks/Layers</cell><cell cols="2">Conv4 Conv5 Convlast</cell></row><row><cell>MNIST → USPS</cell><cell>0.954 0.956</cell><cell>0.961</cell></row><row><cell cols="2">Fashion → FashionM 0.890 0.905</cell><cell>0.913</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Adaptation accuracy with different (γ1, γ2) for Tasks MNIST → USPS and Fashion → FashionM.</figDesc><table><row><cell>Tasks / (γ1, γ2)</cell><cell cols="5">(0.1,1.0) (0.5,0.5) (0.9,0.1) (1.0,0.1) (1.0, 0)</cell></row><row><cell>MNIST → USPS</cell><cell>0.320</cell><cell>0.723</cell><cell>0.961</cell><cell>0.961</cell><cell>0.961</cell></row><row><cell>Fashion → FashionM</cell><cell>0.226</cell><cell>0.513</cell><cell>0.912</cell><cell>0.913</cell><cell>0.913</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Evaluation on the effect of unsupervised consistency metrics. The recognition accuracy is shown for four tasks in the unsupervised domain adaptation scenario. Our model is on the last row with both the L s c and L t c , which achieves the best performance. These results suggest the deep representation plays a crucial role in the cross-domain modulation. A-Distance: In a theoretical analysis of the domain discrepancy<ref type="bibr" target="#b1">[2]</ref>, Ben-David et al. suggests that A-distance can be used as a measure for the domain discrepancy. As the exact A-distance is intractable, a proxy is defined as dA</figDesc><table><row><cell>Model/Tasks</cell><cell cols="4">MNIST→USPS USPS→MNIST Fashion→FashionM FashionM→Fashion</cell></row><row><cell>CDLM w/o L c</cell><cell>0.635</cell><cell>0.683</cell><cell>0.646</cell><cell>0.672</cell></row><row><cell>CDLM+L t c CDLM+L s c CDLM+L s c +L t c</cell><cell>0.689 0.951 0.961</cell><cell>0.695 0.980 0.983</cell><cell>0.682 0.912 0.913</cell><cell>0.691 0.915 0.922</cell></row><row><cell>γ 2 = 0.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>URL https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Mart</forename><surname>Abadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467[cs.DC],2016.4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><forename type="middle">Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><surname>Stargan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dual swap disentangling</title>
		<author>
			<persName><forename type="first">Zunlei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anxiang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenglong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5894" to="5904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2005">2016. 3, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep reconstructionclassification networks for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Muhammad Ghifary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<editor>
			<persName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image-to-image translation for cross-domain disentanglement</title>
		<author>
			<persName><forename type="first">Abel</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1294" to="1305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradient response maps for real-time detection of texture-less objects</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cedric</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Yan Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Jennifer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><surname>Krause</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ELBO surgery: Yet another way to carve up the variational evidence lower bound</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Workshop</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Taeksoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<editor>
			<persName><forename type="first">Yee</forename><surname>Whye</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Teh</forename><surname>Doina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Precup</forename></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fast gradient-based inference with continuous latent variable models in auxiliary form</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.0733[cs.LG]</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName><forename type="first">Anders</forename><surname>Boesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lindbo</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Søren</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<editor>
			<persName><forename type="first">Maria</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition: Applications of neural network chips and automatic learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H P</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><surname>Hubbard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Hsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><forename type="middle">Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia Bin</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Hsuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europen Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Lectures on the Coupling Method</title>
		<author>
			<persName><forename type="first">Torgny</forename><surname>Lindvall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Dover</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A unified feature disentangler for multidomain image translation and manipulation</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen</forename><forename type="middle">Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">Ying</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName><forename type="first">Ming-Yu Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2005">2017. 1, 2, 4, 5</date>
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detach and adapt: Learning cross-domain disentangled deep representation</title>
		<author>
			<persName><forename type="first">Yen</forename><forename type="middle">Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">Ying</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tzu</forename><surname>Chien Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><forename type="middle">De</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><forename type="middle">Chen</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<editor>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Blei</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hanna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kristen</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicolò</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1640" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Latent normalizing flows for many-to-many cross-domain mappings</title>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Shweta Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fahad Shahbaz Khan, and Fatih Porikli. Cross-domain transferability of adversarial perturbations</title>
		<author>
			<persName><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeuIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image generation from small datasets via batch statistics adaptation</title>
		<author>
			<persName><forename type="first">Atsuhiro</forename><surname>Noguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2750" to="2758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge Data Engineering (TKDE)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Federated adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName><forename type="first">Yogesh</forename><surname>Swami Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">D</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generalized zero-and few-shot learning via aligned variational autoencoders</title>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Schonfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayna</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey on deep transfer learning</title>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunfang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks (ICANN)</title>
		<editor>
			<persName><forename type="first">Vera</forename><surname>Kurková</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yannis</forename><surname>Manolopoulos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Barbara</forename><surname>Hammer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lazaros</forename><forename type="middle">S</forename><surname>Iliadis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ilias</forename><surname>Maglogiannis</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">VAE with a vampprior</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<editor>
			<persName><forename type="first">Fernando</forename><surname>Perez-Cruz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="1214" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Diverse and accurate image description using a variational auto-encoder with an additive gaussian encoding space</title>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ad-vances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5757" to="5767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A survey of transfer learning</title>
		<author>
			<persName><forename type="first">Taghi</forename><forename type="middle">M</forename><surname>Karl R Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingding</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning descriptors for object recognition and 3d pose estimation</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747[cs.LG</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semantic jitter: Dense supervision for visual comparisons via synthetic images</title>
		<author>
			<persName><forename type="first">Aron</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-attribute transfer via disentangled representation</title>
		<author>
			<persName><forename type="first">Jianfu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9195" to="9202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Latent normalizing flows for discrete sequences</title>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7673" to="7682" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
